<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tog---282">TOG - 282</h2>
<ul>
<li><details>
<summary>
(2022). Nonlinear compliant modes for large-deformation analysis of
flexible structures. <em>TOG</em>, <em>42</em>(2), 21:1–11. (<a
href="https://doi.org/10.1145/3568952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many flexible structures are characterized by a small number of compliant modes , i.e., large-deformation paths that can be traversed with little mechanical effort, whereas resistance to other deformations is much stiffer. Predicting the compliant modes for a given flexible structure, however, is challenging. While linear eigenmodes capture the small-deformation behavior, they quickly divert into states of unrealistically high energy for larger displacements. Moreover, they are inherently unable to predict nonlinear phenomena such as buckling, stiffening, multistability, and contact. To address this limitation, we propose Nonlinear Compliant Modes —a physically principled extension of linear eigenmodes for large-deformation analysis. Instead of constraining the entire structure to deform along a given eigenmode, our method only prescribes the projection of the system’s state onto the linear mode while all other degrees of freedom follow through energy minimization. We evaluate the potential of our method on a diverse set of flexible structures, ranging from compliant mechanisms to topology-optimized joints and structured materials. As validated through experiments on physical prototypes, our method correctly predicts a broad range of nonlinear effects that linear eigenanalysis fails to capture.},
  archive      = {J_TOG},
  author       = {Simon Duenser and Bernhard Thomaszewski and Roi Poranne and Stelian Coros},
  doi          = {10.1145/3568952},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {21:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Nonlinear compliant modes for large-deformation analysis of flexible structures},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceptual visibility model for temporal contrast changes in
periphery. <em>TOG</em>, <em>42</em>(2), 20:1–16. (<a
href="https://doi.org/10.1145/3564241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling perception is critical for many applications and developments in computer graphics to optimize and evaluate content generation techniques. Most of the work to date has focused on central (foveal) vision. However, this is insufficient for novel wide-field-of-view display devices, such as virtual and augmented reality headsets. Furthermore, the perceptual models proposed for the fovea do not readily extend to the off-center, peripheral visual field, where human perception is drastically different. In this article, we focus on modeling the temporal aspect of visual perception in the periphery. We present new psychophysical experiments that measure the sensitivity of human observers to different spatio-temporal stimuli across a wide field of view. We use the collected data to build a perceptual model for the visibility of temporal changes at different eccentricities in complex video content. Finally, we discuss, demonstrate, and evaluate several problems that can be addressed using our technique. First, we show how our model enables injecting new content into the periphery without distracting the viewer, and we discuss the link between the model and human attention. Second, we demonstrate how foveated rendering methods can be evaluated and optimized to limit the visibility of temporal aliasing.},
  archive      = {J_TOG},
  author       = {Cara Tursun and Piotr Didyk},
  doi          = {10.1145/3564241},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {20:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Perceptual visibility model for temporal contrast changes in periphery},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planar panels and planar supporting beams in architectural
structures. <em>TOG</em>, <em>42</em>(2), 19:1–17. (<a
href="https://doi.org/10.1145/3561050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate geometric properties and modeling capabilities of quad meshes with planar faces whose mesh polylines enjoy the additional property of being contained in a single plane. This planarity is a major benefit in architectural design and building construction: If a structural element is contained in a plane, it can be manufactured on the ground without scaffolding and put into place as a whole. Further, the plane it is contained in serves as part of a so-called support structure. We discuss design of meshes under the requirement that one half of mesh polylines are planar (“P meshes”), and we also investigate the geometry and design of meshes where all polylines enjoy this property (“PP meshes”). We work in the space of planes and with appropriate transformations of that space. We also incorporate further properties relevant for architectural design, such as near-rectangular panels and repetitive nodes. We provide geometric insights, give explicit constructions, and show an approach to geometric modeling of both P meshes and PP meshes, in particular, the case of nearly rectangular panels.},
  archive      = {J_TOG},
  author       = {Caigui Jiang and Cheng Wang and Xavier Tellier and Johannes Wallner and Helmut Pottmann},
  doi          = {10.1145/3561050},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {19:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Planar panels and planar supporting beams in architectural structures},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interlocking spiral drawings inspired by m. C. Escher’s
print whirlpools. <em>TOG</em>, <em>42</em>(2), 18:1–17. (<a
href="https://doi.org/10.1145/3560711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whirlpools , by the Dutch graphic artist M. C. Escher, is a woodcut print in which fish interlock as a double spiral tessellation. Inspired by this print, in this article we extend the idea and present a general method to create Escher-like interlocking spiral drawings of N whirlpools. To this end, we first introduce an algorithm for constructing regular spiral tiling T . Then, we design a suitable spiral tiling T and use N copies of T to compose an interlocking spiral tiling K of N whirlpools. To create Escher-like drawings similar to the print, we next specify realization details of using wallpaper templates to decorate K . To enhance the aesthetic appeal, we propose several measures to minimize motif overlaps of the spiral drawings. Technologically, we develop algorithms for generating Escher-like drawings that can be implemented using shaders. The method established is thus able to generate a great variety of exotic Escher-like interlocking spiral drawings.},
  archive      = {J_TOG},
  author       = {Peichang Ouyang and Krzysztof Gdawiec and Alain Nicolas and David Bailey and Kwok Wai Chung},
  doi          = {10.1145/3560711},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {18:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interlocking spiral drawings inspired by m. c. escher’s print whirlpools},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented paths and reodesics for topologically-stable
matching. <em>TOG</em>, <em>42</em>(2), 17:1–15. (<a
href="https://doi.org/10.1145/3554978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fully-automatic method that computes from scratch point-to-point dense correspondences between isometric shapes under topological noise. While relying on pairwise distance preservation constraints is common and generally sufficient to handle isometric deformations, presence of topological noise needs further actions that we present as our main contributions. First, instead of comparing distances over two paths on two input surfaces, we cast fuzzy votes at the path endpoints based on topologically-robust heat diffusion from path vertices. Second, we make the matching even more stable to topological noise by introducing the so-called reodesics, which are locally shortest geodesics that go through robust matches. In addition to the five standard datasets for isometric shape correspondence with and without topological noise, we employ and release a sixth one geared specifically towards topological noise evaluation with ground-truth information. We demonstrate our qualitative and quantitative advantages over seven recent state-of-the-art methods on these six datasets.},
  archive      = {J_TOG},
  author       = {Yusuf Sahillioğlu and Devin Horsman},
  doi          = {10.1145/3554978},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {2},
  pages        = {17:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Augmented paths and reodesics for topologically-stable matching},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hex-mesh generation and processing: A survey. <em>TOG</em>,
<em>42</em>(2), 16:1–44. (<a
href="https://doi.org/10.1145/3554920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide a detailed survey of techniques for hexahedral mesh generation. We cover the whole spectrum of alternative approaches to mesh generation, as well as post-processing algorithms for connectivity editing and mesh optimization. For each technique, we highlight capabilities and limitations, also pointing out the associated unsolved challenges. Recent relaxed approaches, aiming to generate not pure-hex but hex-dominant meshes, are also discussed. The required background, pertaining to geometrical as well as combinatorial aspects, is introduced along the way.},
  archive      = {J_TOG},
  author       = {Nico Pietroni and Marcel Campen and Alla Sheffer and Gianmarco Cherchi and David Bommes and Xifeng Gao and Riccardo Scateni and Franck Ledoux and Jean Remacle and Marco Livesu},
  doi          = {10.1145/3554920},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {2},
  pages        = {16:1–44},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hex-mesh generation and processing: A survey},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Versatile control of fluid-directed solid objects using
multi-task reinforcement learning. <em>TOG</em>, <em>42</em>(2),
15:1–14. (<a href="https://doi.org/10.1145/3554731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning-based controller for high-dimensional dynamic systems with coupled fluid and solid objects. The dynamic behaviors of such systems can vary across different simulators and the control tasks subject to changing requirements from users. Our controller features high versatility and can adapt to changing dynamic behaviors and multiple tasks without re-training, which is achieved by combining two training strategies. We use meta-reinforcement learning to inform the controller of changing simulation parameters. We further design a novel task representation, which allows the controller to adapt to continually changing tasks via hindsight experience replay. We highlight the robustness and generality of our controller on a row of dynamic-rich tasks, including scooping up solid balls from a water pool, in-air ball acrobatics using fluid spouts, and zero-shot transferring to unseen simulators and constitutive models. In all the scenarios, our controller consistently outperforms the plain multi-task reinforcement-learning baseline.},
  archive      = {J_TOG},
  author       = {Bo Ren and Xiaohan Ye and Zherong Pan and Taiyuan Zhang},
  doi          = {10.1145/3554731},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {2},
  pages        = {15:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Versatile control of fluid-directed solid objects using multi-task reinforcement learning},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surface reconstruction from point clouds without normals by
parametrizing the gauss formula. <em>TOG</em>, <em>42</em>(2), 14:1–19.
(<a href="https://doi.org/10.1145/3554730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Parametric Gauss Reconstruction (PGR) for surface reconstruction from point clouds without normals. Our insight builds on the Gauss formula in potential theory, which represents the indicator function of a region as an integral over its boundary. By viewing surface normals and surface element areas as unknown parameters, the Gauss formula interprets the indicator as a member of some parametric function space. We can solve for the unknown parameters using the Gauss formula and simultaneously obtain the indicator function. Our method bypasses the need for accurate input normals as required by most existing non-data-driven methods, while also exhibiting superiority over data-driven methods, since no training is needed. Moreover, by modifying the Gauss formula and employing regularization, PGR also adapts to difficult cases such as noisy inputs, thin structures, sparse or nonuniform points, for which accurate normal estimation becomes quite difficult. Our code is publicly available at https://github.com/jsnln/ParametricGaussRecon .},
  archive      = {J_TOG},
  author       = {Siyou Lin and Dong Xiao and Zuoqiang Shi and Bin Wang},
  doi          = {10.1145/3554730},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {2},
  pages        = {14:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface reconstruction from point clouds without normals by parametrizing the gauss formula},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Singularity computation for rational parametric surfaces
using moving planes. <em>TOG</em>, <em>42</em>(1), 12:1–14. (<a
href="https://doi.org/10.1145/3551387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Singularity computation is a fundamental problem in Computer Graphics and Computer Aided Geometric Design, since it is closely related to topology determination, intersection, mesh generation, rendering, simulation, and modeling of curves and surfaces. In this article, we present an efficient and robust algorithm for computing all the singularities (including their orders) of rational parametric surfaces using the technique of moving planes. The main approach is first to construct a representation matrix whose columns correspond to moving planes following the parametric surface. Then, by substituting the parametric equation of the rational surface into this representation matrix, one can extract the singularity information from the corresponding matrix and return all the singular loci including self-intersection curves, cusp curves, and isolated singular points of the rational surface, together with the order of each singular locus. We present some examples to compare our algorithm with state-of-the-art methods from different perspectives including robustness, efficiency, order computation, and numerical stability, and the experimental results show that our method outperforms existing methods in all these aspects. Furthermore, applications of our algorithm in surface rendering, mesh generation and surface/surface intersections are provided to demonstrate that correctly computing the self-intersection curves of a surface is essential to generate high quality results for these applications.},
  archive      = {J_TOG},
  author       = {Xiaohong Jia and Falai Chen and Shanshan Yao},
  doi          = {10.1145/3551387},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {1},
  pages        = {12:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Singularity computation for rational parametric surfaces using moving planes},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DiffCloth: Differentiable cloth simulation with dry
frictional contact. <em>TOG</em>, <em>42</em>(1), 2:1–20. (<a
href="https://doi.org/10.1145/3527660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloth simulation has wide applications in computer animation, garment design, and robot-assisted dressing. This work presents a differentiable cloth simulator whose additional gradient information facilitates cloth-related applications. Our differentiable simulator extends a state-of-the-art cloth simulator based on Projective Dynamics (PD) and with dry frictional contact [Ly et al. 2020 ]. We draw inspiration from previous work [Du et al. 2021 ] to propose a fast and novel method for deriving gradients in PD-based cloth simulation with dry frictional contact. Furthermore, we conduct a comprehensive analysis and evaluation of the usefulness of gradients in contact-rich cloth simulation. Finally, we demonstrate the efficacy of our simulator in a number of downstream applications, including system identification, trajectory optimization for assisted dressing, closed-loop control, inverse design, and real-to-sim transfer. We observe a substantial speedup obtained from using our gradient information in solving most of these applications.},
  archive      = {J_TOG},
  author       = {Yifei Li and Tao Du and Kui Wu and Jie Xu and Wojciech Matusik},
  doi          = {10.1145/3527660},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {1},
  pages        = {2:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffCloth: Differentiable cloth simulation with dry frictional contact},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constraint-based simulation of passive suction cups.
<em>TOG</em>, <em>42</em>(1), 13:1–14. (<a
href="https://doi.org/10.1145/3551889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a physics-based model of suction phenomenon to achieve simulation of deformable objects like suction cups. Our model uses a constraint-based formulation to simulate the variations of pressure inside suction cups. The respective internal pressures are represented as pressure constraints which are coupled with anti-interpenetration and friction constraints. Furthermore, our method is able to detect multiple air cavities using information from collision detection. We solve the pressure constraints based on the ideal gas law while considering several cavity states. We test our model with a number of scenarios reflecting a variety of uses, for instance, a spring loaded jumping toy, a manipulator performing a pick and place task, and an octopus tentacle grasping a soda can. We also evaluate the ability of our model to reproduce the physics of suction cups of varying shapes, lifting objects of different masses, and sliding on a slippery surface. The results show promise for various applications such as the simulation in soft robotics and computer animation.},
  archive      = {J_TOG},
  author       = {Antonin Bernardin and Eulalie Coevoet and Paul Kry and Sheldon Andrews and Christian Duriez and Maud Marchal},
  doi          = {10.1145/3551889},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {13:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constraint-based simulation of passive suction cups},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HiGAN+: Handwriting imitation GAN with disentangled
representations. <em>TOG</em>, <em>42</em>(1), 11:1–17. (<a
href="https://doi.org/10.1145/3550070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans remain far better than machines at learning, where humans require fewer examples to learn new concepts and can use those concepts in richer ways. Take handwriting as an example, after learning from very limited handwriting scripts, a person can easily imagine what the handwritten texts would like with other arbitrary textual contents (even for unseen words or texts). Moreover, humans can also hallucinate to imitate calligraphic styles from just a single reference handwriting sample (that even have never seen before). Humans can do such hallucinations, perhaps because they can learn to disentangle the textual contents and calligraphic styles from handwriting images. Inspired by this, we propose a novel handwriting imitation generative adversarial network (HiGAN+) for realistic handwritten text synthesis based on disentangled representations. The proposed HiGAN+ can achieve a precise one-shot handwriting style transfer by introducing the writer-specific auxiliary loss and contextual loss, and it also attains a good global &amp; local consistency by refining local details of synthetic handwriting images. Extensive experiments, including human evaluations, on the benchmark dataset validate our superiority in terms of visual quality, scalability, compactness, and style transferability compared with the state-of-the-art GANs for handwritten text synthesis.},
  archive      = {J_TOG},
  author       = {Ji Gan and Weiqiang Wang and Jiaxu Leng and Xinbo Gao},
  doi          = {10.1145/3550070},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {11:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {HiGAN+: Handwriting imitation GAN with disentangled representations},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential frequency heterodyne time-of-flight imaging for
instantaneous depth and velocity estimation. <em>TOG</em>,
<em>42</em>(1), 9:1–13. (<a
href="https://doi.org/10.1145/3546939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we discuss the imaging of depth and velocity using heterodyne-mode time-of-flight (ToF) cameras. In particular, Doppler ToF (D-ToF) imaging utilizes heterodyne modulation to measure the velocity from the Doppler frequency shift, which uniquely facilitates the instantaneous radial velocity estimation. However, theoretical discussion on D-ToF is limited to orthogonal frequency and sinusoidal waveform modulation. This study extends the formulation of the D-ToF imaging, and proposes an arbitrary-frequency, arbitrary-waveform framework considering a phase-compensated, symmetrical two-dimensional correlation map. With the proposed framework, the optimal heterodyne frequency for frequency decoding is found. A differential frequency sampling and decoding method is then proposed, which computes the frequency and phase from as few as four simultaneously captured images. With an experiment platform we built, it is confirmed that the minimum velocity sensing error is half that of the orthogonal frequency method, and the sensible phase range is approximately 2.5 times larger. The conclusions in this study allow the ToF velocity imaging to be applied at the optimal sample frequencies for a wide range of ToF sensors. This pushes one step further to the practical use of ToF velocity imaging.},
  archive      = {J_TOG},
  author       = {Yunpu Hu and Leo Miyashita and Masatoshi Ishikawa},
  doi          = {10.1145/3546939},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {9:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differential frequency heterodyne time-of-flight imaging for instantaneous depth and velocity estimation},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpongeCake: A layered microflake surface appearance model.
<em>TOG</em>, <em>42</em>(1), 8:1–16. (<a
href="https://doi.org/10.1145/3546940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose SpongeCake: A layered BSDF model where each layer is a volumetric scattering medium, defined using microflake or other phase functions. We omit any reflecting and refracting interfaces between the layers. The first advantage of this formulation is that an exact and analytic solution for single scattering, regardless of the number of volumetric layers, can be derived. We propose to approximate multiple scattering by an additional single-scattering lobe with modified parameters and a Lambertian lobe. We use a parameter mapping neural network to find the parameters of the newly added lobes to closely approximate the multiple scattering effect. Despite the absence of layer interfaces, we demonstrate that many common material effects can be achieved with layers of SGGX microflake and other volumes with appropriate parameters. A normal mapping effect can also be achieved through mapping of microflake orientations, which avoids artifacts common in standard normal maps. Thanks to the analytical formulation, our model is very fast to evaluate and sample. Through various parameter settings, our model is able to handle many types of materials, like plastics, wood, cloth, and so on, opening a number of practical applications.},
  archive      = {J_TOG},
  author       = {Beibei Wang and Wenhua Jin and Miloš Hašan and Ling-Qi Yan},
  doi          = {10.1145/3546940},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {8:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {SpongeCake: A layered microflake surface appearance model},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pivotal tuning for latent-based editing of real images.
<em>TOG</em>, <em>42</em>(1), 6:1–13. (<a
href="https://doi.org/10.1145/3544777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous facial editing techniques have been proposed that leverage the generative power of a pretrained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pretrained generator’s domain. As it turns out, StyleGAN’s latent space induces an inherent tradeoff between distortion and editability, i.e., between maintaining the original appearance and convincingly altering its attributes. Hence, it remains challenging to apply ID-preserving edits to real facial images. In this article, we present an approach to bridge this gap. The idea is Pivotal Tuning —a brief training process that preserves editing quality, while surgically changing the portrayed identity and appearance. In Pivotal Tuning Inversion, an initial inverted latent code serves as a pivot, around which the generator is fine-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. We further show that Pivotal Tuning also applies to accommodating for a multitude of faces, while introducing negligible distortion on the rest of the domain. We validate our technique through inversion and editing metrics and show preferable scores to state-of-the-art methods. Last, we present successful editing for harder cases, including elaborate make-up or headwear.},
  archive      = {J_TOG},
  author       = {Daniel Roich and Ron Mokady and Amit H. Bermano and Daniel Cohen-Or},
  doi          = {10.1145/3544777},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {1},
  pages        = {6:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pivotal tuning for latent-based editing of real images},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-infrared imaging for information embedding and
extraction with layered structures. <em>TOG</em>, <em>42</em>(1),
4:1–26. (<a href="https://doi.org/10.1145/3533426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-invasive inspection and imaging techniques are used to acquire non-visible information embedded in samples. Typical applications include medical imaging, defect evaluation, and electronics testing. However, existing methods have specific limitations, including safety risks (e.g., X-ray), equipment costs (e.g., optical tomography), personnel training (e.g., ultrasonography), and material constraints (e.g., terahertz spectroscopy). Such constraints make these approaches impractical for everyday scenarios. In this article, we present a method that is low-cost and practical for non-invasive inspection in everyday settings. Our prototype incorporates a miniaturized near-infrared spectroscopy scanner driven by a computer-controlled 2D-plotter. Our work presents a method to optimize content embedding, as well as a wavelength selection algorithm to extract content without human supervision. We show that our method can successfully extract occluded text through a paper stack of up to 16 pages. In addition, we present a deep-learning-based image enhancement model that can further improve the image quality and simultaneously decompose overlapping content. Finally, we demonstrate how our method can be generalized to different inks and other layered materials beyond paper. Our approach enables a wide range of content embedding applications, including chipless information embedding, physical secret sharing, 3D print evaluations, and steganography.},
  archive      = {J_TOG},
  author       = {Weiwei Jiang and Difeng Yu and Chaofan Wang and Zhanna Sarsenbayeva and Niels van Berkel and Jorge Goncalves and Vassilis Kostakos},
  doi          = {10.1145/3533426},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {1},
  pages        = {4:1–26},
  shortjournal = {ACM Trans. Graph.},
  title        = {Near-infrared imaging for information embedding and extraction with layered structures},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DSG-net: Learning disentangled structure and geometry for 3D
shape generation. <em>TOG</em>, <em>42</em>(1), 1:1–17. (<a
href="https://doi.org/10.1145/3526212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D shape generation is a fundamental operation in computer graphics. While significant progress has been made, especially with recent deep generative models, it remains a challenge to synthesize high-quality shapes with rich geometric details and complex structures, in a controllable manner. To tackle this, we introduce DSG-Net, a deep neural network that learns a disentangled structured &amp; geometric mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with disentangled control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged. To achieve this, we simultaneously learn structure and geometry through variational autoencoders (VAEs) in a hierarchical manner for both, with bijective mappings at each level. In this manner, we effectively encode geometry and structure in separate latent spaces, while ensuring their compatibility: the structure is used to guide the geometry and vice versa. At the leaf level, the part geometry is represented using a conditional part VAE, to encode high-quality geometric details, guided by the structure context as the condition. Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes, outperforming state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Jie Yang and Kaichun Mo and Yu-Kun Lai and Leonidas J. Guibas and Lin Gao},
  doi          = {10.1145/3526212},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {1},
  pages        = {1:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {DSG-net: Learning disentangled structure and geometry for 3D shape generation},
  volume       = {42},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAD2Sketch: Generating concept sketches from CAD sequences.
<em>TOG</em>, <em>41</em>(6), 279:1–18. (<a
href="https://doi.org/10.1145/3550454.3555488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept sketches are ubiquitous in industrial design, as they allow designers to quickly depict imaginary 3D objects. To construct their sketches with accurate perspective, designers rely on longstanding drawing techniques, including the use of auxiliary construction lines to identify midpoints of perspective planes, to align points vertically and horizontally, and to project planar curves from one perspective plane to another. We present a method to synthesize such construction lines from CAD sequences. Importantly, our method balances the presence of construction lines with overall clutter, such that the resulting sketch is both well-constructed and readable, as professional designers are trained to do. In addition to generating sketches that are visually similar to real ones, we apply our method to synthesize a large quantity of paired sketches and normal maps, and show that the resulting dataset can be used to train a neural network to infer normals from concept sketches. 1},
  archive      = {J_TOG},
  author       = {Felix Hähnlein and Changjian Li and Niloy J. Mitra and Adrien Bousseau},
  doi          = {10.1145/3550454.3555488},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {279:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {CAD2Sketch: Generating concept sketches from CAD sequences},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assemble them all: Physics-based planning for generalizable
assembly by disassembly. <em>TOG</em>, <em>41</em>(6), 278:1–11. (<a
href="https://doi.org/10.1145/3550454.3555525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly planning is the core of automating product assembly, maintenance, and recycling for modern industrial manufacturing. Despite its importance and long history of research, planning for mechanical assemblies when given the final assembled state remains a challenging problem. This is due to the complexity of dealing with arbitrary 3D shapes and the highly constrained motion required for real-world assemblies. In this work, we propose a novel method to efficiently plan physically plausible assembly motion and sequences for real-world assemblies. Our method leverages the assembly-by-disassembly principle and physics-based simulation to efficiently explore a reduced search space. To evaluate the generality of our method, we define a large-scale dataset consisting of thousands of physically valid industrial assemblies with a variety of assembly motions required. Our experiments on this new benchmark demonstrate we achieve a state-of-the-art success rate and the highest computational efficiency compared to other baseline algorithms. Our method also generalizes to rotational assemblies (e.g., screws and puzzles) and solves 80-part assemblies within several minutes.},
  archive      = {J_TOG},
  author       = {Yunsheng Tian and Jie Xu and Yichen Li and Jieliang Luo and Shinjiro Sueda and Hui Li and Karl D. D. Willis and Wojciech Matusik},
  doi          = {10.1145/3550454.3555525},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {278:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Assemble them all: Physics-based planning for generalizable assembly by disassembly},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S3-slicer: A general slicing framework for multi-axis 3D
printing. <em>TOG</em>, <em>41</em>(6), 277:1–15. (<a
href="https://doi.org/10.1145/3550454.3555516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-axis motion introduces more degrees of freedom into the process of 3D printing to enable different objectives of fabrication by accumulating materials layers upon curved layers. An existing challenge is how to effectively generate the curved layers satisfying multiple objectives simultaneously. This paper presents a general slicing framework for achieving multiple fabrication objectives including support free, strength reinforcement and surface quality. These objectives are formulated as local printing directions varied in the volume of a solid, which are achieved by computing the rotation-driven deformation for the input model. The height field of a deformed model is mapped into a scalar field on its original shape, the isosurfaces of which give the curved layers of multi-axis 3D printing. The deformation can be effectively optimized with the help of quaternion fields to achieve the fabrication objectives. The effectiveness of our method has been verified on a variety of models.},
  archive      = {J_TOG},
  author       = {Tianyu Zhang and Guoxin Fang and Yuming Huang and Neelotpal Dutta and Sylvain Lefebvre and Zekai Murat Kilic and Charlie C. L. Wang},
  doi          = {10.1145/3550454.3555516},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {277:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {S3-slicer: A general slicing framework for multi-axis 3D printing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit conversion of manifold b-rep solids by neural
halfspace representation. <em>TOG</em>, <em>41</em>(6), 276:1–15. (<a
href="https://doi.org/10.1145/3550454.3555502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel implicit representation --- neural halfspace representation (NH-Rep), to convert manifold B-Rep solids to implicit representations. NH-Rep is a Boolean tree built on a set of implicit functions represented by the neural network, and the composite Boolean function is capable of representing solid geometry while preserving sharp features. We propose an efficient algorithm to extract the Boolean tree from a manifold B-Rep solid and devise a neural network-based optimization approach to compute the implicit functions. We demonstrate the high quality offered by our conversion algorithm on ten thousand manifold B-Rep CAD models that contain various curved patches including NURBS, and the superiority of our learning approach over other representative implicit conversion algorithms in terms of surface reconstruction, sharp feature preservation, signed distance field approximation, and robustness to various surface geometry, as well as a set of applications supported by NH-Rep.},
  archive      = {J_TOG},
  author       = {Hao-Xiang Guo and Yang Liu and Hao Pan and Baining Guo},
  doi          = {10.1145/3550454.3555502},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {276:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Implicit conversion of manifold B-rep solids by neural halfspace representation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Isotropic ARAP energy using cauchy-green invariants.
<em>TOG</em>, <em>41</em>(6), 275:1–14. (<a
href="https://doi.org/10.1145/3550454.3555507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Isotropic As-Rigid-As-Possible (ARAP) energy has been popular for shape editing, mesh parametrisation and soft-body simulation for almost two decades. However, a formulation using Cauchy-Green (CG) invariants has always been unclear, due to a rotation-polluted trace term that cannot be directly expressed using these invariants. We show how this incongruent trace term can be understood via an implicit relationship to the CG invariants. Our analysis reveals this relationship to be a polynomial where the roots equate to the trace term, and where the derivatives also give rise to closed-form expressions of the Hessian to guarantee positive semi-definiteness for a fast and concise Newton-type implicit time integration. A consequence of this analysis is a novel analytical formulation to compute rotations and singular values of deformation-gradient tensors without explicit/numerical factorization which is significant, resulting in up-to 3.5× speedup and benefits energy function evaluation for reducing solver time. We validate our energy formulation by experiments and comparison, demonstrating that our resulting eigendecomposition using the CG invariants is equivalent to existing ARAP formulations. We thus reveal isotropic ARAP energy to be a member of the &quot;Cauchy-Green club&quot;, meaning that it can indeed be defined using CG invariants and therefore that the closed-form expressions of the resulting Hessian are shared with other energies written in their terms.},
  archive      = {J_TOG},
  author       = {Huancheng Lin and Floyd M. Chitalu and Taku Komura},
  doi          = {10.1145/3550454.3555507},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {275:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Isotropic ARAP energy using cauchy-green invariants},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape from release: Inverse design and fabrication of
controlled release structures. <em>TOG</em>, <em>41</em>(6), 274:1–14.
(<a href="https://doi.org/10.1145/3550454.3555518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects with different shapes can dissolve in significantly different ways inside a solution. Predicting different shapes&#39; dissolution dynamics is an important problem especially in pharmaceutics. More important and challenging, however, is controlling the dissolution via shape, i.e. , designing shapes that lead to a desired release behavior of materials in a solvent over a specific time. Here, we tackle this challenge by introducing a computational inverse design pipeline. We begin by introducing a simple, physically-inspired differentiable forward model of dissolution. We then formulate our inverse design as a PDE-constrained topology optimization that has access to analytical derivatives obtained via sensitivity analysis. Furthermore, we incorporate fabricability terms in the optimization objective that enable physically realizing our designs. We thoroughly analyze our approach on a diverse set of examples via both simulation and fabrication.},
  archive      = {J_TOG},
  author       = {Julian Panetta and Haleh Mohammadian and Emiliano Luci and Vahid Babaei},
  doi          = {10.1145/3550454.3555518},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {274:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shape from release: Inverse design and fabrication of controlled release structures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation of hand anatomy using medical imaging.
<em>TOG</em>, <em>41</em>(6), 273:1–20. (<a
href="https://doi.org/10.1145/3550454.3555486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision modeling of the hand internal musculoskeletal anatomy has been largely limited to individual poses, and has not been connected into continuous volumetric motion of the hand anatomy actuating across the hand&#39;s entire range of motion. This is for a good reason, as hand anatomy and its motion are extremely complex and cannot be predicted merely from the anatomy in a single pose. We give a method to simulate the volumetric shape of hand&#39;s musculoskeletal organs to any pose in the hand&#39;s range of motion, producing external hand shapes and internal organ shapes that match ground truth optical scans and medical images (MRI) in multiple scanned poses. We achieve this by combining MRI images in multiple hand poses with FEM multibody nonlinear elastoplastic simulation. Our system models bones, muscles, tendons, joint ligaments and fat as separate volumetric organs that mechanically interact through contact and attachments, and whose shape matches medical images (MRI) in the MRI-scanned hand poses. The match to MRI is achieved by incorporating pose-space deformation and plastic strains into the simulation. We show how to do this in a non-intrusive manner that still retains all the simulation benefits, namely the ability to prescribe realistic material properties, generalize to arbitrary poses, preserve volume and obey contacts and attachments. We use our method to produce volumetric renders of the internal anatomy of the human hand in motion, and to compute and render highly realistic hand surface shapes. We evaluate our method by comparing it to optical scans, and demonstrate that we qualitatively and quantitatively substantially decrease the error compared to previous work. We test our method on five complex hand sequences, generated either using keyframe animation or performance animation using modern hand tracking techniques.},
  archive      = {J_TOG},
  author       = {Mianlun Zheng and Bohan Wang and Jingtao Huang and Jernej Barbič},
  doi          = {10.1145/3550454.3555486},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {273:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simulation of hand anatomy using medical imaging},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable simulation of inertial musculotendons.
<em>TOG</em>, <em>41</em>(6), 272:1–11. (<a
href="https://doi.org/10.1145/3550454.3555490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and practical approach for incorporating the effects of muscle inertia, which has been ignored by previous musculoskeletal simulators in both graphics and biomechanics. We approximate the inertia of the muscle by assuming that muscle mass is distributed along the centerline of the muscle. We express the motion of the musculotendons in terms of the motion of the skeletal joints using a chain of Jacobians, so that at the top level, only the reduced degrees of freedom of the skeleton are used to completely drive both bones and musculotendons. Our approach can handle all commonly used musculotendon path types, including those with multiple path points and wrapping surfaces. For muscle paths involving wrapping surfaces, we use neural networks to model the Jacobians, trained using existing wrapping surface libraries, which allows us to effectively handle the Jacobian discontinuities that occur when musculotendon paths collide with wrapping surfaces. We demonstrate support for higher-order time integrators, complex joints, inverse dynamics, Hill-type muscle models, and differentiability. In the limit, as the muscle mass is reduced to zero, our approach gracefully degrades to traditional simulators without support for muscle inertia. Finally, it is possible to mix and match inertial and non-inertial musculotendons, depending on the application.},
  archive      = {J_TOG},
  author       = {Ying Wang and Jasper Verheul and Sang-Hoon Yeo and Nima Khademi Kalantari and Shinjiro Sueda},
  doi          = {10.1145/3550454.3555490},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {272:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable simulation of inertial musculotendons},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuralMarker: A framework for learning general marker
correspondence. <em>TOG</em>, <em>41</em>(6), 271:1–10. (<a
href="https://doi.org/10.1145/3550454.3555468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of estimating correspondences from a general marker, such as a movie poster, to an image that captures such a marker. Conventionally, this problem is addressed by fitting a homography model based on sparse feature matching. However, they are only able to handle plane-like markers and the sparse features do not sufficiently utilize appearance information. In this paper, we propose a novel framework NeuralMarker, training a neural network estimating dense marker correspondences under various challenging conditions, such as marker deformation, harsh lighting, etc. Deep learning has presented an excellent performance in correspondence learning once provided with sufficient training data. However, annotating pixel-wise dense correspondence for training marker correspondence is too expensive. We observe that the challenges of marker correspondence estimation come from two individual aspects: geometry variation and appearance variation. We, therefore, design two components addressing these two challenges in NeuralMarker. First, we create a synthetic dataset FlyingMarkers containing marker-image pairs with ground truth dense correspondences. By training with FlyingMarkers, the neural network is encouraged to capture various marker motions. Second, we propose the novel Symmetric Epipolar Distance (SED) loss, which enables learning dense correspondence from posed images. Learning with the SED loss and the cross-lighting posed images collected by Structure-from-Motion (SfM), NeuralMarker is remarkably robust in harsh lighting environments and avoids synthetic image bias. Besides, we also propose a novel marker correspondence evaluation method circumstancing annotations on real marker-image pairs and create a new benchmark. We show that NeuralMarker significantly outperforms previous methods and enables new interesting applications, including Augmented Reality (AR) and video editing.},
  archive      = {J_TOG},
  author       = {Zhaoyang Huang and Xiaokun Pan and Weihong Pan and Weikang Bian and Yan Xu and Ka Chun Cheung and Guofeng Zhang and Hongsheng Li},
  doi          = {10.1145/3550454.3555468},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {271:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralMarker: A framework for learning general marker correspondence},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IDE-3D: Interactive disentangled editing for high-resolution
3D-aware portrait synthesis. <em>TOG</em>, <em>41</em>(6), 270:1–10. (<a
href="https://doi.org/10.1145/3550454.3555506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing 3D-aware facial generation methods face a dilemma in quality versus editability: they either generate editable results in low resolution, or high-quality ones with no editing flexibility. In this work, we propose a new approach that brings the best of both worlds together. Our system consists of three major components: (1) a 3D-semantics-aware generative model that produces view-consistent, disentangled face images and semantic masks; (2) a hybrid GAN inversion approach that initializes the latent codes from the semantic and texture encoder, and further optimizes them for faithful reconstruction; and (3) a canonical editor that enables efficient manipulation of semantic masks in canonical view and produces high-quality editing results. Our approach is competent for many applications, e.g. free-view face drawing, editing and style control. Both quantitative and qualitative results show that our method reaches the state-of-the-art in terms of photorealism, faithfulness and efficiency.},
  archive      = {J_TOG},
  author       = {Jingxiang Sun and Xuan Wang and Yichun Shi and Lizhen Wang and Jue Wang and Yebin Liu},
  doi          = {10.1145/3550454.3555506},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {270:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {IDE-3D: Interactive disentangled editing for high-resolution 3D-aware portrait synthesis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural brushstroke engine: Learning a latent style space of
interactive drawing tools. <em>TOG</em>, <em>41</em>(6), 269:1–18. (<a
href="https://doi.org/10.1145/3550454.3555472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Neural Brushstroke Engine, the first method to apply deep generative models to learn a distribution of interactive drawing tools. Our conditional GAN model learns the latent space of drawing styles from a small set (about 200) of unlabeled images in different media. Once trained, a single model can texturize stroke patches drawn by the artist, emulating a diverse collection of brush styles in the latent space. In order to enable interactive painting on a canvas of arbitrary size, we design a painting engine able to support real-time seamless patch-based generation, while allowing artists direct control of stroke shape, color and thickness. We show that the latent space learned by our model generalizes to unseen drawing and more experimental styles (e.g. beads) by embedding real styles into the latent space. We explore other applications of the continuous latent space, such as optimizing brushes to enable painting in the style of an existing artwork, automatic line drawing stylization, brush interpolation, and even natural language search over a continuous space of drawing tools. Our prototype received positive feedback from a small group of digital artists.},
  archive      = {J_TOG},
  author       = {Maria Shugrina and Chin-Ying Li and Sanja Fidler},
  doi          = {10.1145/3550454.3555472},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {269:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural brushstroke engine: Learning a latent style space of interactive drawing tools},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Force-aware interface via electromyography for natural VR/AR
interaction. <em>TOG</em>, <em>41</em>(6), 268:1–18. (<a
href="https://doi.org/10.1145/3550454.3555461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While tremendous advances in visual and auditory realism have been made for virtual and augmented reality (VR/AR), introducing a plausible sense of physicality into the virtual world remains challenging. Closing the gap between real-world physicality and immersive virtual experience requires a closed interaction loop: applying user-exerted physical forces to the virtual environment and generating haptic sensations back to the users. However, existing VR/AR solutions either completely ignore the force inputs from the users or rely on obtrusive sensing devices that compromise user experience. By identifying users&#39; muscle activation patterns while engaging in VR/AR, we design a learning-based neural interface for natural and intuitive force inputs. Specifically, we show that lightweight electromyography sensors, resting non-invasively on users&#39; forearm skin, inform and establish a robust understanding of their complex hand activities. Fuelled by a neural-network-based model, our interface can decode finger-wise forces in real-time with 3.3\% mean error, and generalize to new users with little calibration. Through an interactive psychophysical study, we show that human perception of virtual objects&#39; physical properties, such as stiffness, can be significantly enhanced by our interface. We further demonstrate that our interface enables ubiquitous control via finger tapping. Ultimately, we envision our findings to push forward research towards more realistic physicality in future VR/AR.},
  archive      = {J_TOG},
  author       = {Yunxiang Zhang and Benjamin Liang and Boyuan Chen and Paul M. Torrens and S. Farokh Atashzar and Dahua Lin and Qi Sun},
  doi          = {10.1145/3550454.3555461},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {268:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Force-aware interface via electromyography for natural VR/AR interaction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constant time median filter using 2D wavelet matrix.
<em>TOG</em>, <em>41</em>(6), 267:1–10. (<a
href="https://doi.org/10.1145/3550454.3555512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The median filter is a simple yet powerful noise reduction technique that is extensively applied in image, signal, and speech processing. It can effectively remove impulsive noise while preserving the content of the image by taking the median of neighboring pixels; thus, it has various applications, such as restoration of a damaged image and facial beautification. The median filter is typically implemented in one of two major approaches: the histogram-based method, which requires O (1) computation time per pixel when focusing on the kernel radius r , and the sorting-based method, which requires approximately O ( r 2 ) computation time per pixel but has a light constant factor. These are used differently depending on the kernel radius and the number of bits in the image. However, the computation time is still slow, particularly when the kernel radius is in the mid to large range. This paper introduces novel and efficient median filter with constant complexity O (1) for kernel size using the wavelet matrix data structure, which has been applied to query-based searches on one-dimensional data. We extended the original wavelet matrix to two-dimensional data for application to computer graphics problems. The objective of this study was to achieve high-speed median filter computation in parallel computing environment with many threads (i.e., GPUs). Our implementation for the GPU is an order of magnitude faster than the histogram method for 8-bit images. Unlike traditional histogram methods, which suffer from significant computational overhead, the proposed method can handle images with high pixel depth (e.g., 16- and 32-bit high dynamic range images). When the kernel radius is greater than 12 for 8-bit images, the proposed method outperforms the other median filter computation methods.},
  archive      = {J_TOG},
  author       = {Yuji Moroto and Nobuyuki Umetani},
  doi          = {10.1145/3550454.3555512},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {267:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constant time median filter using 2D wavelet matrix},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Look-ahead training with learned reflectance loss for
single-image SVBRDF estimation. <em>TOG</em>, <em>41</em>(6), 266:1–12.
(<a href="https://doi.org/10.1145/3550454.3555495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel optimization-based method to estimate the reflectance properties of a near planar surface from a single input image. Specifically, we perform test-time optimization by directly updating the parameters of a neural network to minimize the test error. Since single image SVBRDF estimation is a highly ill-posed problem, such an optimization is prone to overfitting. Our main contribution is to address this problem by introducing a training mechanism that takes the test-time optimization into account. Specifically, we train our network by minimizing the training loss after one or more gradient updates with the test loss. By training the network in this manner, we ensure that the network does not overfit to the input image during the test-time optimization process. Additionally, we propose a learned reflectance loss to augment the typically used rendering loss during the test-time optimization. We do so by using an auxiliary network that estimates pseudo ground truth reflectance parameters and train it in combination with the main network. Our approach is able to converge with a small number of iterations of the test-time optimization and produces better results compared to the state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Xilong Zhou and Nima Khademi Kalantari},
  doi          = {10.1145/3550454.3555495},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {266:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Look-ahead training with learned reflectance loss for single-image SVBRDF estimation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DifferSketching: How differently do people sketch 3D
objects? <em>TOG</em>, <em>41</em>(6), 264:1–16. (<a
href="https://doi.org/10.1145/3550454.3555493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple sketch datasets have been proposed to understand how people draw 3D objects. However, such datasets are often of small scale and cover a small set of objects or categories. In addition, these datasets contain freehand sketches mostly from expert users, making it difficult to compare the drawings by expert and novice users, while such comparisons are critical in informing more effective sketch-based interfaces for either user groups. These observations motivate us to analyze how differently people with and without adequate drawing skills sketch 3D objects. We invited 70 novice users and 38 expert users to sketch 136 3D objects, which were presented as 362 images rendered from multiple views. This leads to a new dataset of 3,620 freehand multi-view sketches, which are registered with their corresponding 3D objects under certain views. Our dataset is an order of magnitude larger than the existing datasets. We analyze the collected data at three levels, i.e., sketch-level, stroke-level, and pixel-level, under both spatial and temporal characteristics, and within and across groups of creators. We found that the drawings by professionals and novices show significant differences at stroke-level, both intrinsically and extrinsically. We demonstrate the usefulness of our dataset in two applications: (i) freehand-style sketch synthesis, and (ii) posing it as a potential benchmark for sketch-based 3D reconstruction. Our dataset and code are available at https://chufengxiao.github.io/DifferSketching/.},
  archive      = {J_TOG},
  author       = {Chufeng Xiao and Wanchao Su and Jing Liao and Zhouhui Lian and Yi-Zhe Song and Hongbo Fu},
  doi          = {10.1145/3550454.3555493},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {264:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DifferSketching: How differently do people sketch 3D objects?},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive exploration of tension-compression mixed shells.
<em>TOG</em>, <em>41</em>(6), 263:1–14. (<a
href="https://doi.org/10.1145/3550454.3555438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving a pure-compression stress state is considered central to the form-finding of shell structures. However, the pure-compression assumption restricts the geometry of the structure&#39;s plan in that any free boundary edges cannot bulge outward. Allowing both tension and compression is essential so that overhanging leaves can stretch out toward the sky. When performing tension-compression mixed form-finding, a problem with boundary condition (BC) compatibility arises. Since the form-finding equation is hyperbolic, boundary information propagates along the asymptotic lines of the stress function. If conflicting BC data is prescribed at either end of an asymptotic line, the problem becomes ill-posed. This requires a user of a form-finding method to know the solution in advance. By contrast, pure-tension or pure-compression problems are elliptic and always give solutions under any BCs sufficient to restrain rigid motion. To solve the form-finding problem for tension-compression mixed shells, we focus on the Airy&#39;s stress function, which describes the stress field in a shell. Rather than taking the stress function as given, we instead treat both the stress function and the shell as unknowns. This doubles the solution variables, turning the problem to one that has an infinity of different solutions. By enforcing equilibrium in the shell interior and prescribing the correct matching pairs of BCs to both the stress function and the shell, a stress function and shell can be simultaneously found such that equilibrium is satisfied everywhere in the shell interior and thus automatically has compatible BCs by construction. The problem of a potentially over-constrained form-finding is thus avoided by expanding the solution space and creating an under-determined problem. By varying inputs and repeatedly searching for stress function-shell pairs that fall within the solution space, a user is allowed to interactively explore the possible forms of tension-compression mixed shells under the given plan of the shell.},
  archive      = {J_TOG},
  author       = {Masaaki Miki and Toby Mitchell},
  doi          = {10.1145/3550454.3555438},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {263:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive exploration of tension-compression mixed shells},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural james-stein combiner for unbiased and biased
renderings. <em>TOG</em>, <em>41</em>(6), 262:1–14. (<a
href="https://doi.org/10.1145/3550454.3555496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unbiased rendering algorithms such as path tracing produce accurate images given a huge number of samples, but in practice, the techniques often leave visually distracting artifacts (i.e., noise) in their rendered images due to a limited time budget. A favored approach for mitigating the noise problem is applying learning-based denoisers to unbiased but noisy rendered images and suppressing the noise while preserving image details. However, such denoising techniques typically introduce a systematic error, i.e., the denoising bias, which does not decline as rapidly when increasing the sample size, unlike the other type of error, i.e., variance. It can technically lead to slow numerical convergence of the denoising techniques. We propose a new combination framework built upon the James-Stein (JS) estimator, which merges a pair of unbiased and biased rendering images, e.g., a path-traced image and its denoised result. Unlike existing post-correction techniques for image denoising, our framework helps an input denoiser have lower errors than its unbiased input without relying on accurate estimation of per-pixel denoising errors. We demonstrate that our framework based on the well-established JS theories allows us to improve the error reduction rates of state-of-the-art learning-based denoisers more robustly than recent post-denoisers.},
  archive      = {J_TOG},
  author       = {Jeongmin Gu and Jose A. Iglesias-Guitian and Bochang Moon},
  doi          = {10.1145/3550454.3555496},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {262:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural james-stein combiner for unbiased and biased renderings},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable multi-class sampling via filtered sliced optimal
transport. <em>TOG</em>, <em>41</em>(6), 261:1–14. (<a
href="https://doi.org/10.1145/3550454.3555484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-class point optimization formulation based on continuous Wasserstein barycenters. Our formulation is designed to handle hundreds to thousands of optimization objectives and comes with a practical optimization scheme. We demonstrate the effectiveness of our framework on various sampling applications like stippling, object placement, and Monte-Carlo integration. We a derive multi-class error bound for perceptual rendering error which can be minimized using our optimization. We provide source code at https://github.com/iribis/filtered-sliced-optimal-transport.},
  archive      = {J_TOG},
  author       = {Corentin Salaün and Iliyan Georgiev and Hans-Peter Seidel and Gurprit Singh},
  doi          = {10.1145/3550454.3555484},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {261:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scalable multi-class sampling via filtered sliced optimal transport},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian blue noise. <em>TOG</em>, <em>41</em>(6), 260:1–15.
(<a href="https://doi.org/10.1145/3550454.3555519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the various approaches for producing point distributions with blue noise spectrum, we argue for an optimization framework using Gaussian kernels. We show that with a wise selection of optimization parameters, this approach attains unprecedented quality, provably surpassing the current state of the art attained by the optimal transport (BNOT) approach. Further, we show that our algorithm scales smoothly and feasibly to high dimensions while maintaining the same quality, realizing unprecedented high-quality high-dimensional blue noise sets. Finally, we show an extension to adaptive sampling.},
  archive      = {J_TOG},
  author       = {Abdalla G. M. Ahmed and Jing Ren and Peter Wonka},
  doi          = {10.1145/3550454.3555519},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {260:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Gaussian blue noise},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep adaptive sampling and reconstruction using analytic
distributions. <em>TOG</em>, <em>41</em>(6), 259:1–16. (<a
href="https://doi.org/10.1145/3550454.3555515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an adaptive sampling and reconstruction method for offline Monte Carlo rendering. Our method produces sampling maps constrained by a user-defined budget that minimize the expected future denoising error. Compared to other state-of-the-art methods, which produce the necessary training data on the fly by composing pre-rendered images, our method samples from analytic noise distributions instead. These distributions are compact and closely approximate the pixel value distributions stemming from Monte Carlo rendering. Our method can efficiently sample training data by leveraging only a few per-pixel statistics of the target distribution, which provides several benefits over the current state of the art. Most notably, our analytic distributions&#39; modeling accuracy and sampling efficiency increase with sample count, essential for high-quality offline rendering. Although our distributions are approximate, our method supports joint end-to-end training of the sampling and denoising networks. Finally, we propose the addition of a global summary module to our architecture that accumulates valuable information from image regions outside of the network&#39;s receptive field. This information discourages sub-optimal decisions based on local information. Our evaluation against other state-of-the-art neural sampling methods demonstrates denoising quality and data efficiency improvements.},
  archive      = {J_TOG},
  author       = {Farnood Salehi and Marco Manzi and Gerhard Roethlin and Romann Weber and Christopher Schroers and Marios Papas},
  doi          = {10.1145/3550454.3555515},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {259:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep adaptive sampling and reconstruction using analytic distributions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable hybrid traffic simulation. <em>TOG</em>,
<em>41</em>(6), 258:1–10. (<a
href="https://doi.org/10.1145/3550454.3555492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel differentiable hybrid traffic simulator , which simulates traffic using a hybrid model of both macroscopic and microscopic models and can be directly integrated into a neural network for traffic control and flow optimization. This is the first differentiable traffic simulator for macroscopic and hybrid models that can compute gradients for traffic states across time steps and inhomogeneous lanes. To compute the gradient flow between two types of traffic models in a hybrid framework, we present a novel intermediate conversion component that bridges the lanes in a differentiable manner as well. We also show that we can use analytical gradients to accelerate the overall process and enhance scalability. Thanks to these gradients, our simulator can provide more efficient and scalable solutions for complex learning and control problems posed in traffic engineering than other existing algorithms. Refer to https://sites.google.com/umd.edu/diff-hybrid-traffic-sim for our project.},
  archive      = {J_TOG},
  author       = {Sanghyun Son and Yi-Ling Qiao and Jason Sewall and Ming C. Lin},
  doi          = {10.1145/3550454.3555492},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {258:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable hybrid traffic simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient neural style transfer for volumetric simulations.
<em>TOG</em>, <em>41</em>(6), 257:1–10. (<a
href="https://doi.org/10.1145/3550454.3555517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artistically controlling fluids has always been a challenging task. Recently, volumetric Neural Style Transfer (NST) techniques have been used to artistically manipulate smoke simulation data with 2D images. In this work, we revisit previous volumetric NST techniques for smoke, proposing a suite of upgrades that enable stylizations that are significantly faster, simpler, more controllable and less prone to artifacts. Moreover, the energy minimization solved by previous methods is camera dependent. To avoid that, a computationally expensive iterative optimization performed for multiple views sampled around the original simulation is needed, which can take up to several minutes per frame. We propose a simple feed-forward neural network architecture that is able to infer view-independent stylizations that are three orders of the magnitude faster than its optimization-based counterpart.},
  archive      = {J_TOG},
  author       = {Joshua Aurand and Raphael Ortiz and Silvia Nauer and Vinicius C. Azevedo},
  doi          = {10.1145/3550454.3555517},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {257:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient neural style transfer for volumetric simulations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hydrophobic and hydrophilic solid-fluid interaction.
<em>TOG</em>, <em>41</em>(6), 256:1–15. (<a
href="https://doi.org/10.1145/3550454.3555478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel solid-fluid coupling method to capture the subtle hydrophobic and hydrophilic interactions between liquid, solid, and air at their multi-phase junctions. The key component of our approach is a Lagrangian model that tackles the coupling, evolution, and equilibrium of dynamic contact lines evolving on the interface between surface-tension fluid and deformable objects. This contact-line model captures an ensemble of small-scale geometric and physical processes, including dynamic waterfront tracking, local momentum transfer and force balance, and interfacial tension calculation. On top of this contact-line model, we further developed a mesh-based level set method to evolve the three-phase T-junction on a deformable solid surface. Our dynamic contact-line model, in conjunction with its monolithic coupling system, unifies the simulation of various hydrophobic and hydrophilic solid-fluid-interaction phenomena and enables a broad range of challenging small-scale elastocapillary phenomena that were previously difficult or impractical to solve, such as the elastocapillary origami and self-assembly, dynamic contact angles of drops, capillary adhesion, as well as wetting and splashing on vibrating surfaces.},
  archive      = {J_TOG},
  author       = {Jinyuan Liu and Mengdi Wang and Fan Feng and Annie Tang and Qiqin Le and Bo Zhu},
  doi          = {10.1145/3550454.3555478},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {256:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hydrophobic and hydrophilic solid-fluid interaction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ElastoMonolith: A monolithic optimization-based liquid
solver for contact-aware elastic-solid coupling. <em>TOG</em>,
<em>41</em>(6), 255:1–19. (<a
href="https://doi.org/10.1145/3550454.3555474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous coupling of diverse physical systems poses significant computational challenges in terms of speed, quality, and stability. Rather than treating all components with a single discretization methodology (e.g., smoothed particles, material point method, Eulerian grid, etc.) that is ill-suited to some components, our solver, ElastoMonolith , addresses three-way interactions among standard particle-in-cell-based viscous and inviscid fluids, Lagrangian mesh-based deformable bodies, and rigid bodies. While prior methods often treat some terms explicitly or in a decoupled fashion for efficiency, often at the cost of robustness or stability, we demonstrate the effectiveness of a strong coupling approach that expresses all of the relevant physics within one consistent and unified optimization problem, including fluid pressure and viscosity, elasticity of the deformables, frictional solid-solid contact, and solid-fluid interface conditions. We further develop a numerical solver to tackle this difficult optimization problem, incorporating projected Newton, an active set method, and a transformation of the inner linear system matrix to ensure symmetric positive definiteness. Our experimental evaluations show that our framework can achieve high quality coupling results that avoid artifacts such as volume loss, instability, sticky contacts, and spurious interpenetrations.},
  archive      = {J_TOG},
  author       = {Tetsuya Takahashi and Christopher Batty},
  doi          = {10.1145/3550454.3555474},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {255:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {ElastoMonolith: A monolithic optimization-based liquid solver for contact-aware elastic-solid coupling},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-order directional fields. <em>TOG</em>, <em>41</em>(6),
254:1–17. (<a href="https://doi.org/10.1145/3550454.3555455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for representing face-based directional fields of an arbitrary piecewise-polynomial order. Our framework is based on a primal-dual decomposition of fields, where the exact component of a field is the gradient of piecewise-polynomial conforming function, and the coexact component is defined as the adjoint of a dimensionally-consistent discrete curl operator. Our novel formulation sidesteps the difficult problem of constructing high-order non-conforming function spaces, and makes it simple to harness the flexibility of higher-order finite elements for directional-field processing. Our representation is structure-preserving, and draws on principles from finite-element exterior calculus. We demonstrate its benefits for applications such as Helmholtz-Hodge decomposition, smooth PolyVector fields, the vector heat method, and seamless parameterization.},
  archive      = {J_TOG},
  author       = {Iwan Boksebeld and Amir Vaxman},
  doi          = {10.1145/3550454.3555455},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {254:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-order directional fields},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Globally injective flattening via a reduced harmonic
subspace. <em>TOG</em>, <em>41</em>(6), 253:1–17. (<a
href="https://doi.org/10.1145/3550454.3555449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a highly efficient-and-robust method for free-boundary flattening of disk-like triangle meshes in a globally injective manner. We show that by restricting the solution to a low-dimensional subspace of harmonic maps, we can dramatically accelerate the process while obtaining a low-distortion result. The algorithm consists of two main steps. A linear subspace construction, and a nonlinear nonconvex optimization for finding a low-distortion globally injective map within that subspace. The complexity of the first step dominates the algorithm&#39;s runtime and is merely that of solving a linear system. We combine recent results for computing locally-and-globally injective maps with that of harmonic maps into a conceptually simple algorithm that guarantees global injectivity. We demonstrate the great efficiency of our method over a dataset of 100 large scale models with more than 2M triangles each. Our algorithm is 10 times faster on average compared to the state-of-the-art Efficient Bijective Parameterizations (EBP) method [Su et al. 2020], on these high-resolution meshes, and more than 20 times faster on challenging examples (Figures 1,11). The speedup over [Jiang et al. 2017; Smith and Schaefer 2015] is even more dramatic.},
  archive      = {J_TOG},
  author       = {Guy Fargion and Ofir Weber},
  doi          = {10.1145/3550454.3555449},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {253:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Globally injective flattening via a reduced harmonic subspace},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MeshTaichi: A compiler for efficient mesh-based operations.
<em>TOG</em>, <em>41</em>(6), 252:1–17. (<a
href="https://doi.org/10.1145/3550454.3555430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meshes are an indispensable representation in many graphics applications because they provide conformal spatial discretizations. However, mesh-based operations are often slow due to unstructured memory access patterns. We propose MeshTaichi, a novel mesh compiler that provides an intuitive programming model for efficient mesh-based operations. Our programming model hides the complex indexing system from users and allows users to write mesh-based operations using reference-style neighborhood queries. Our compiler achieves its high performance by exploiting data locality. We partition input meshes and prepare the wanted relations by inspecting users&#39; code during compile time. During run time, we further utilize on-chip memory (shared memory on GPU and L1 cache on CPU) to access the wanted attributes of mesh elements efficiently. Our compiler decouples low-level optimization options with computations, so that users can explore different localized data attributes and different memory orderings without changing their computation code. As a result, users can write concise code using our programming model to generate efficient mesh-based computations on both CPU and GPU backends. We test MeshTaichi on a variety of physically-based simulation and geometry processing applications with both triangle and tetrahedron meshes. MeshTaichi achieves a consistent speedup ranging from 1.4× to 6×, compared to state-of-the-art mesh data structures and compilers.},
  archive      = {J_TOG},
  author       = {Chang Yu and Yi Xu and Ye Kuang and Yuanming Hu and Tiantian Liu},
  doi          = {10.1145/3550454.3555430},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {252:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {MeshTaichi: A compiler for efficient mesh-based operations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Declarative specification for unstructured mesh editing
algorithms. <em>TOG</em>, <em>41</em>(6), 251:1–14. (<a
href="https://doi.org/10.1145/3550454.3555513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel approach to describe mesh generation, mesh adaptation, and geometric modeling algorithms relying on changing mesh connectivity using a high-level abstraction. The main motivation is to enable easy customization and development of these algorithms via a declarative specification consisting of a set of per-element invariants, operation scheduling, and attribute transfer for each editing operation. We demonstrate that widely used algorithms editing surfaces and volumes can be compactly expressed with our abstraction, and their implementation within our framework is simple, automatically parallelizable on shared-memory architectures, and with guaranteed satisfaction of the prescribed invariants. These algorithms are readable and easy to customize for specific use cases. We introduce a software library implementing this abstraction and providing automatic shared-memory parallelization.},
  archive      = {J_TOG},
  author       = {Zhongshi Jiang and Jiacheng Dai and Yixin Hu and Yunfan Zhou and Jeremie Dumas and Qingnan Zhou and Gurkirat Singh Bajwa and Denis Zorin and Daniele Panozzo and Teseo Schneider},
  doi          = {10.1145/3550454.3555513},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {251:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Declarative specification for unstructured mesh editing algorithms},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SkinMixer: Blending 3D animated models. <em>TOG</em>,
<em>41</em>(6), 250:1–15. (<a
href="https://doi.org/10.1145/3550454.3555503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel technique to compose new 3D animated models, such as videogame characters, by combining pieces from existing ones. Our method works on production-ready rigged, skinned, and animated 3D models to reassemble new ones. We exploit mix-and-match operations on the skeletons to trigger the automatic creation of a new mesh, linked to the new skeleton by a set of skinning weights and complete with a set of animations. The resulting model preserves the quality of the input meshings (which can be quad-dominant and semi-regular), skinning weights (inducing believable deformation), and animations, featuring coherent movements of the new skeleton. Our method enables content creators to reuse valuable, carefully designed assets by assembling new ready-to-use characters while preserving most of the hand-crafted subtleties of models authored by digital artists. As shown in the accompanying video, it allows for drastically cutting the time needed to obtain the final result.},
  archive      = {J_TOG},
  author       = {Stefano Nuvoli and Nico Pietroni and Paolo Cignoni and Riccardo Scateni and Marco Tarini},
  doi          = {10.1145/3550454.3555503},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {250:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {SkinMixer: Blending 3D animated models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical layout blending with recursive optimal
correspondence. <em>TOG</em>, <em>41</em>(6), 249:1–15. (<a
href="https://doi.org/10.1145/3550454.3555446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for blending hierarchical layouts with semantic labels. The core of our method is a hierarchical structure correspondence algorithm, which recursively finds optimal substructure correspondences, achieving a globally optimal correspondence between a pair of hierarchical layouts. This correspondence is consistent with the structures of both layouts, allowing us to define the union of the layouts&#39; structures. The resulting compound structure helps extract intermediate layout structures, from which blended layouts can be generated via an optimization approach. The correspondence also defines a similarity measure between layouts in a hierarchically structured view. Our method provides a new way for novel layout creation. The introduced structural similarity measure regularizes the layouts in a hyperspace. We demonstrate two applications in this paper, i.e., exploratory design of novel layouts and sketch-based layout retrieval, and test them on a magazine layout dataset. The effectiveness and feasibility of these two applications are confirmed by the user feedback and the extensive results. The code is available at https://github.com/lyf7115/LayoutBlending.},
  archive      = {J_TOG},
  author       = {Pengfei Xu and Yifan Li and Zhijin Yang and Weiran Shi and Hongbo Fu and Hui Huang},
  doi          = {10.1145/3550454.3555446},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {249:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hierarchical layout blending with recursive optimal correspondence},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive and robust mesh booleans. <em>TOG</em>,
<em>41</em>(6), 248:1–14. (<a
href="https://doi.org/10.1145/3550454.3555460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean operations are among the most used paradigms to create and edit digital shapes. Despite being conceptually simple, the computation of mesh Booleans is notoriously challenging. Main issues come from numerical approximations that make the detection and processing of intersection points inconsistent and unreliable, exposing implementations based on floating point arithmetic to many kinds of degeneracy and failure. Numerical methods based on rational numbers or exact geometric predicates have the needed robustness guarantees, that are achieved at the cost of increased computation times that, as of today, has always restricted the use of robust mesh Booleans to offline applications. We introduce an algorithm for Boolean operations with robustness guarantees that is capable of operating at interactive frame rates on meshes with up to 200K triangles. We evaluate our tool thoroughly, considering not only interactive applications but also batch processing of large collections of meshes, processing of huge meshes containing millions of elements and variadic Booleans of hundreds of shapes altogether. In all these experiments, we consistently outperform prior robust floating point methods by at least one order of magnitude.},
  archive      = {J_TOG},
  author       = {Gianmarco Cherchi and Fabio Pellacini and Marco Attene and Marco Livesu},
  doi          = {10.1145/3550454.3555460},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {248:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive and robust mesh booleans},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BoolSurf: Boolean operations on surfaces. <em>TOG</em>,
<em>41</em>(6), 247:1–13. (<a
href="https://doi.org/10.1145/3550454.3555466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We port Boolean set operations between 2D shapes to surfaces of any genus, with any number of open boundaries. We combine shapes bounded by sets of freely intersecting loops, consisting of geodesic lines and cubic Bézier splines lying on a surface. We compute the arrangement of shapes directly on the surface and assign integer labels to the cells of such arrangement. Differently from the Euclidean case, some arrangements on a manifold may be inconsistent. We detect inconsistent arrangements and help the user to resolve them. Also, we extend to the manifold setting recent work on Boundary-Sampled Halfspaces, thus supporting operations more general than standard Booleans, which are well defined on inconsistent arrangements, too. Our implementation discretizes the input shapes into polylines at an arbitrary resolution, independent of the level of resolution of the underlying mesh. We resolve the arrangement inside each triangle of the mesh independently and combine the results to reconstruct both the boundaries and the interior of each cell in the arrangement. We reconstruct the control points of curves bounding cells, in order to free the result from discretization and provide an output in vector format. We support interactive usage, editing shapes consisting up to 100k line segments on meshes of up to 1M triangles.},
  archive      = {J_TOG},
  author       = {Marzia Riso and Giacomo Nazzaro and Enrico Puppo and Alec Jacobson and Qingnan Zhou and Fabio Pellacini},
  doi          = {10.1145/3550454.3555466},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {247:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {BoolSurf: Boolean operations on surfaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIPNet: Neural normal-to-anisotropic-roughness MIP mapping.
<em>TOG</em>, <em>41</em>(6), 246:1–12. (<a
href="https://doi.org/10.1145/3550454.3555487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MIPNet, a novel approach for SVBRDF mipmapping which preserves material appearance under varying view distances and lighting conditions. As in classical mipmapping, our method explicitly encodes the multiscale appearance of materials in a SVBRDF mipmap pyramid. To do so, we use a tensor-based representation, coping with gradient-based optimization, for encoding anisotropy which is compatible with existing real-time rendering engines. Instead of relying on a simple texture patch average for each channel independently, we propose a cascaded architecture of multilayer perceptrons to approximate the material appearance using only the fixed material channels. Our neural model learns simple mipmapping filters using a differentiable rendering pipeline based on a rendering loss and is able to transfer signal from normal to anisotropic roughness. As a result, we obtain a drop-in replacement for standard material mipmapping, offering a significant improvement in appearance preservation while still boiling down to a single per-pixel mipmap texture fetch. We report extensive experiments on two distinct BRDF models.},
  archive      = {J_TOG},
  author       = {Alban Gauthier and Robin Faury and Jérémy Levallois and Théo Thonat and Jean-Marc Thiery and Tamy Boubekeur},
  doi          = {10.1145/3550454.3555487},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {246:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MIPNet: Neural normal-to-anisotropic-roughness MIP mapping},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metappearance: Meta-learning for visual appearance
reproduction. <em>TOG</em>, <em>41</em>(6), 245:1–13. (<a
href="https://doi.org/10.1145/3550454.3555458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There currently exist two main approaches to reproducing visual appearance using Machine Learning (ML): The first is training models that generalize over different instances of a problem, e.g., different images of a dataset. As one-shot approaches, these offer fast inference, but often fall short in quality. The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem, e.g., a flash image of a material. These methods offer high quality, but take long to train. We suggest to combine both techniques end-to-end using meta-learning: We over-fit onto a single problem instance in an inner loop, while also learning how to do so efficiently in an outer-loop across many exemplars. To this end, we derive the required formalism that allows applying meta-learning to a wide range of visual appearance reproduction problems: textures, Bidirectional Reflectance Distribution Functions (BRDFs), spatially-varying BRDFs (svBRDFs), illumination or the entire light transport of a scene. The effects of meta-learning parameters on several different aspects of visual appearance are analyzed in our framework, and specific guidance for different tasks is provided. Metappearance enables visual quality that is similar to over-fit approaches in only a fraction of their runtime while keeping the adaptivity of general models.},
  archive      = {J_TOG},
  author       = {Michael Fischer and Tobias Ritschel},
  doi          = {10.1145/3550454.3555458},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {245:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Metappearance: Meta-learning for visual appearance reproduction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-based surface tension flow. <em>TOG</em>,
<em>41</em>(6), 244:1–12. (<a
href="https://doi.org/10.1145/3550454.3555476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to simulating surface tension flow within a position-based dynamics (PBD) framework. We enhance the conventional PBD fluid method in terms of its surface representation and constraint enforcement to furnish support for the simulation of interfacial phenomena driven by strong surface tension and contact dynamics. The key component of our framework is an on-the-fly local meshing algorithm to build the local geometry around each surface particle. Based on this local mesh structure, we devise novel surface constraints that can be integrated seamlessly into a PBD framework to model strong surface tension effects. We demonstrate the efficacy of our approach by simulating a multitude of surface tension flow examples exhibiting intricate interfacial dynamics of films and drops, which were all infeasible for a traditional PBD method.},
  archive      = {J_TOG},
  author       = {Jingrui Xing and Liangwang Ruan and Bin Wang and Bo Zhu and Baoquan Chen},
  doi          = {10.1145/3550454.3555476},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {244:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Position-based surface tension flow},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Curl-flow: Boundary-respecting pointwise incompressible
velocity interpolation for grid-based fluids. <em>TOG</em>,
<em>41</em>(6), 243:1–21. (<a
href="https://doi.org/10.1145/3550454.3555498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to augment standard grid-based fluid solvers with pointwise divergence-free velocity interpolation, thereby ensuring exact incompressibility down to the sub-cell level. Our method takes as input a discretely divergence-free velocity field generated by a staggered grid pressure projection, and first recovers a corresponding discrete vector potential. Instead of solving a costly vector Poisson problem for the potential, we develop a fast parallel sweeping strategy to find a candidate potential and apply a gauge transformation to enforce the Coulomb gauge condition and thereby make it numerically smooth. Interpolating this discrete potential generates a point-wise vector potential whose analytical curl is a pointwise incompressible velocity field. Our method further supports irregular solid geometry through the use of level set-based cut-cells and a novel Curl-Noise-inspired potential ramping procedure that simultaneously offers strictly non-penetrating velocities and incompressibility. Experimental comparisons demonstrate that the vector potential reconstruction procedure at the heart of our approach is consistently faster than prior such reconstruction schemes, especially those that solve vector Poisson problems. Moreover, in exchange for its modest extra cost, our overall Curl-Flow framework produces significantly improved particle trajectories that closely respect irregular obstacles, do not suffer from spurious sources or sinks, and yield superior particle distributions over time.},
  archive      = {J_TOG},
  author       = {Jumyung Chang and Ruben Partono and Vinicius C. Azevedo and Christopher Batty},
  doi          = {10.1145/3550454.3555498},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {243:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Curl-flow: Boundary-respecting pointwise incompressible velocity interpolation for grid-based fluids},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast octree neighborhood search for SPH simulations.
<em>TOG</em>, <em>41</em>(6), 242:1–13. (<a
href="https://doi.org/10.1145/3550454.3555523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new octree-based neighborhood search method for SPH simulation. A speedup of up to 1.9x is observed in comparison to state-of-the-art methods which rely on uniform grids. While our method focuses on maximizing performance in fixed-radius SPH simulations, we show that it can also be used in scenarios where the particle support radius is not constant thanks to the adaptive nature of the octree acceleration structure. Neighborhood search methods typically consist of an acceleration structure that prunes the space of possible particle neighbor pairs, followed by direct distance comparisons between the remaining particle pairs. Previous works have focused on minimizing the number of comparisons. However, in an effort to minimize the actual computation time, we find that distance comparisons exhibit very high throughput on modern CPUs. By permitting more comparisons than strictly necessary, the time spent on preparing and searching the acceleration structure can be reduced, yielding a net positive speedup. The choice of an octree acceleration structure, instead of the uniform grid typically used in fixed-radius methods, ensures balanced computational tasks. This benefits both parallelism and provides consistently high computational intensity for the distance comparisons. We present a detailed account of high-level considerations that, together with low-level decisions, enable high throughput for performance-critical parts of the algorithm. Finally, we demonstrate the high performance of our algorithm on a number of large-scale fixed-radius SPH benchmarks and show in experiments with a support radius ratio up to 3 that our method is also effective in multi-resolution SPH simulations.},
  archive      = {J_TOG},
  author       = {José Antonio Fernández-Fernández and Lukas Westhofen and Fabian Löschner and Stefan Rhys Jeske and Andreas Longva and Jan Bender},
  doi          = {10.1145/3550454.3555523},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {242:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast octree neighborhood search for SPH simulations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hidden degrees of freedom in implicit vortex filaments.
<em>TOG</em>, <em>41</em>(6), 241:1–14. (<a
href="https://doi.org/10.1145/3550454.3555459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new representation of curve dynamics, with applications to vortex filaments in fluid dynamics. Instead of representing these filaments with explicit curve geometry and Lagrangian equations of motion, we represent curves implicitly with a new co-dimensional 2 level set description. Our implicit representation admits several redundant mathematical degrees of freedom in both the configuration and the dynamics of the curves, which can be tailored specifically to improve numerical robustness, in contrast to naive approaches for implicit curve dynamics that suffer from overwhelming numerical stability problems. Furthermore, we note how these hidden degrees of freedom perfectly map to a Clebsch representation in fluid dynamics. Motivated by these observations, we introduce untwisted level set functions and non-swirling dynamics which successfully regularize sources of numerical instability, particularly in the twisting modes around curve filaments. A consequence is a novel simulation method which produces stable dynamics for large numbers of interacting vortex filaments and effortlessly handles topological changes and re-connection events.},
  archive      = {J_TOG},
  author       = {Sadashige Ishida and Chris Wojtan and Albert Chern},
  doi          = {10.1145/3550454.3555459},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {241:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hidden degrees of freedom in implicit vortex filaments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A monte carlo method for fluid simulation. <em>TOG</em>,
<em>41</em>(6), 240:1–16. (<a
href="https://doi.org/10.1145/3550454.3555450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel Monte Carlo-based fluid simulation approach capable of pointwise and stochastic estimation of fluid motion. Drawing on the Feynman-Kac representation of the vorticity transport equation, we propose a recursive Monte Carlo estimator of the Biot-Savart law and extend it with a stream function formulation that allows us to treat free-slip boundary conditions using a Walk-on-Spheres algorithm. Inspired by the Monte Carlo literature in rendering, we design and compare variance reduction schemes suited to a fluid simulation context for the first time, show its applicability to complex boundary settings, and detail a simple and practical implementation with temporal grid caching. We validate the correctness of our approach via quantitative and qualitative evaluations - across a range of settings and domain geometries - and thoroughly explore its parameters&#39; design space. Finally, we provide an in-depth discussion of several axes of future work building on this new numerical simulation modality.},
  archive      = {J_TOG},
  author       = {Damien Rioux-Lavoie and Ryusuke Sugimoto and Tümay Özdemir and Naoharu H. Shimada and Christopher Batty and Derek Nowrouzezahrai and Toshiya Hachisuka},
  doi          = {10.1145/3550454.3555450},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {240:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A monte carlo method for fluid simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fluidic topology optimization with an anisotropic mixture
model. <em>TOG</em>, <em>41</em>(6), 239:1–14. (<a
href="https://doi.org/10.1145/3550454.3555429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluidic devices are crucial components in many industrial applications involving fluid mechanics. Computational design of a high-performance fluidic system faces multifaceted challenges regarding its geometric representation and physical accuracy. We present a novel topology optimization method to design fluidic devices in a Stokes flow context. Our approach is featured by its capability in accommodating a broad spectrum of boundary conditions at the solid-fluid interface. Our key contribution is an anisotropic and differentiable constitutive model that unifies the representation of different phases and boundary conditions in a Stokes model, enabling a topology optimization method that can synthesize novel structures with accurate boundary conditions from a background grid discretization. We demonstrate the efficacy of our approach by conducting several fluidic system design tasks with over four million design parameters.},
  archive      = {J_TOG},
  author       = {Yifei Li and Tao Du and Sangeetha Grama Srinivasan and Kui Wu and Bo Zhu and Eftychios Sifakis and Wojciech Matusik},
  doi          = {10.1145/3550454.3555429},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {239:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluidic topology optimization with an anisotropic mixture model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural photo-finishing. <em>TOG</em>, <em>41</em>(6),
238:1–15. (<a href="https://doi.org/10.1145/3550454.3555526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing pipelines are ubiquitous and we rely on them either directly, by filtering or adjusting an image post-capture, or indirectly, as image signal processing (ISP) pipelines on broadly deployed camera systems. Used by artists, photographers, system engineers, and for downstream vision tasks, traditional image processing pipelines feature complex algorithmic branches developed over decades. Recently, image-to-image networks have made great strides in image processing, style transfer, and semantic understanding. The differentiable nature of these networks allows them to fit a large corpus of data; however, they do not allow for intuitive, fine-grained controls that photographers find in modern photo-finishing tools. This work closes that gap and presents an approach to making complex photo-finishing pipelines differentiable, allowing legacy algorithms to be trained akin to neural networks using first-order optimization methods. By concatenating tailored network proxy models of individual processing steps (e.g. white-balance, tone-mapping, color tuning), we can model a non-differentiable reference image finishing pipeline more faithfully than existing proxy image-to-image network models. We validate the method for several diverse applications, including photo and video style transfer, slider regression for commercial camera ISPs, photography-driven neural demosaicking, and adversarial photo-editing.},
  archive      = {J_TOG},
  author       = {Ethan Tseng and Yuxuan Zhang and Lars Jebe and Xuaner Zhang and Zhihao Xia and Yifei Fan and Felix Heide and Jiawen Chen},
  doi          = {10.1145/3550454.3555526},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {238:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural photo-finishing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Production-ready face re-aging for visual effects.
<em>TOG</em>, <em>41</em>(6), 237:1–12. (<a
href="https://doi.org/10.1145/3550454.3555520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic digital re-aging of faces in video is becoming increasingly common in entertainment and advertising. But the predominant 2D painting workflow often requires frame-by-frame manual work that can take days to accomplish, even by skilled artists. Although research on facial image re-aging has attempted to automate and solve this problem, current techniques are of little practical use as they typically suffer from facial identity loss, poor resolution, and unstable results across subsequent video frames. In this paper, we present the first practical, fully-automatic and production-ready method for re-aging faces in video images. Our first key insight is in addressing the problem of collecting longitudinal training data for learning to re-age faces over extended periods of time, a task that is nearly impossible to accomplish for a large number of real people. We show how such a longitudinal dataset can be constructed by leveraging the current state-of-the-art in facial re-aging that, although failing on real images, does provide photoreal re-aging results on synthetic faces. Our second key insight is then to leverage such synthetic data and formulate facial re-aging as a practical image-to-image translation task that can be performed by training a well-understood U-Net architecture, without the need for more complex network designs. We demonstrate how the simple U-Net, surprisingly, allows us to advance the state of the art for re-aging real faces on video, with unprecedented temporal stability and preservation of facial identity across variable expressions, viewpoints, and lighting conditions. Finally, our new face re-aging network (FRAN) incorporates simple and intuitive mechanisms that provides artists with localized control and creative freedom to direct and fine-tune the re-aging effect, a feature that is largely important in real production pipelines and often overlooked in related research work.},
  archive      = {J_TOG},
  author       = {Gaspard Zoss and Prashanth Chandran and Eftychios Sifakis and Markus Gross and Paulo Gotardo and Derek Bradley},
  doi          = {10.1145/3550454.3555520},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {237:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Production-ready face re-aging for visual effects},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural parameterization for dynamic human head editing.
<em>TOG</em>, <em>41</em>(6), 236:1–15. (<a
href="https://doi.org/10.1145/3550454.3555494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit radiance functions emerged as a powerful scene representation for reconstructing and rendering photo-realistic views of a 3D scene. These representations, however, suffer from poor editability. On the other hand, explicit representations such as polygonal meshes allow easy editing but are not as suitable for reconstructing accurate details in dynamic human heads, such as fine facial features, hair, teeth, and eyes. In this work, we present Neural Parameterization (NeP), a hybrid representation that provides the advantages of both implicit and explicit methods. NeP is capable of photo-realistic rendering while allowing fine-grained editing of the scene geometry and appearance. We first disentangle the geometry and appearance by parameterizing the 3D geometry into 2D texture space. We enable geometric editability by introducing an explicit linear deformation blending layer. The deformation is controlled by a set of sparse key points, which can be explicitly and intuitively displaced to edit the geometry. For appearance, we develop a hybrid 2D texture consisting of an explicit texture map for easy editing and implicit view and time-dependent residuals to model temporal and view variations. We compare our method to several reconstruction and editing baselines. The results show that the NeP achieves almost the same level of rendering accuracy while maintaining high editability.},
  archive      = {J_TOG},
  author       = {Li Ma and Xiaoyu Li and Jing Liao and Xuan Wang and Qi Zhang and Jue Wang and Pedro V. Sander},
  doi          = {10.1145/3550454.3555494},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {236:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural parameterization for dynamic human head editing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human performance modeling and rendering via neural animated
mesh. <em>TOG</em>, <em>41</em>(6), 235:1–17. (<a
href="https://doi.org/10.1145/3550454.3555451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have recently seen tremendous progress in the neural advances for photo-real human modeling and rendering. However, it&#39;s still challenging to integrate them into an existing mesh-based pipeline for downstream applications. In this paper, we present a comprehensive neural approach for high-quality reconstruction, compression, and rendering of human performances from dense multi-view videos. Our core intuition is to bridge the traditional animated mesh workflow with a new class of highly efficient neural techniques. We first introduce a neural surface reconstructor for high-quality surface generation in minutes. It marries the implicit volumetric rendering of the truncated signed distance field (TSDF) with multi-resolution hash encoding. We further propose a hybrid neural tracker to generate animated meshes, which combines explicit non-rigid tracking with implicit dynamic deformation in a self-supervised framework. The former provides the coarse warping back into the canonical space, while the latter implicit one further predicts the displacements using the 4D hash encoding as in our reconstructor. Then, we discuss the rendering schemes using the obtained animated meshes, ranging from dynamic texturing to lumigraph rendering under various bandwidth settings. To strike an intricate balance between quality and bandwidth, we propose a hierarchical solution by first rendering 6 virtual views covering the performer and then conducting occlusion-aware neural texture blending. We demonstrate the efficacy of our approach in a variety of mesh-based applications and photo-realistic free-view experiences on various platforms, i.e., inserting virtual human performances into real environments through mobile AR or immersively watching talent shows with VR headsets.},
  archive      = {J_TOG},
  author       = {Fuqiang Zhao and Yuheng Jiang and Kaixin Yao and Jiakai Zhang and Liao Wang and Haizhao Dai and Yuhui Zhong and Yingliang Zhang and Minye Wu and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3550454.3555451},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {235:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Human performance modeling and rendering via neural animated mesh},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ICARUS: A specialized architecture for neural radiance
fields rendering. <em>TOG</em>, <em>41</em>(6), 234:1–14. (<a
href="https://doi.org/10.1145/3550454.3555505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practical deployment of Neural Radiance Fields (NeRF) in rendering applications faces several challenges, with the most critical one being low rendering speed on even high-end graphic processing units (GPUs). In this paper, we present ICARUS, a specialized accelerator architecture tailored for NeRF rendering. Unlike GPUs using general purpose computing and memory architectures for NeRF, ICARUS executes the complete NeRF pipeline using dedicated plenoptic cores (PLCore) consisting of a positional encoding unit (PEU), a multi-layer perceptron (MLP) engine, and a volume rendering unit (VRU). A PLCore takes in positions &amp; directions and renders the corresponding pixel colors without any intermediate data going off-chip for temporary storage and exchange, which can be time and power consuming. To implement the most expensive component of NeRF, i.e., the MLP, we transform the fully connected operations to approximated reconfigurable multiple constant multiplications (MCMs), where common subexpressions are shared across different multiplications to improve the computation efficiency. We build a prototype ICARUS using Synopsys HAPS-80 S104, a field programmable gate array (FPGA)-based prototyping system for large-scale integrated circuits and systems design. We evaluate the power-performancearea (PPA) of a PLCore using 40nm LP CMOS technology. Working at 400 MHz, a single PLCore occupies 16.5 mm 2 and consumes 282.8 mW, translating to 0.105 uJ/sample. The results are compared with those of GPU and tensor processing unit (TPU) implementations.},
  archive      = {J_TOG},
  author       = {Chaolin Rao and Huangjie Yu and Haochuan Wan and Jindong Zhou and Yueyang Zheng and Minye Wu and Yu Ma and Anpei Chen and Binzhe Yuan and Pingqiang Zhou and Xin Lou and Jingyi Yu},
  doi          = {10.1145/3550454.3555505},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {234:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ICARUS: A specialized architecture for neural radiance fields rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QuadStream: A quad-based scene streaming architecture for
novel viewpoint reconstruction. <em>TOG</em>, <em>41</em>(6), 233:1–13.
(<a href="https://doi.org/10.1145/3550454.3555524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming rendered 3D content over a network to a thin client device, such as a phone or a VR/AR headset, brings high-fidelity graphics to platforms where it would not normally possible due to thermal, power, or cost constraints. Streamed 3D content must be transmitted with a representation that is both robust to latency and potential network dropouts. Transmitting a video stream and reprojecting to correct for changing viewpoints fails in the presence of disocclusion events; streaming scene geometry and performing high-quality rendering on the client is not possible on limited-power mobile GPUs. To balance the competing goals of disocclusion robustness and minimal client workload, we introduce QuadStream , a new streaming content representation that reduces motion-to-photon latency by allowing clients to efficiently render novel views without artifacts caused by disocclusion events. Motivated by traditional macroblock approaches to video codec design, we decompose the scene seen from positions in a view cell into a series of quad proxies , or view-aligned quads from multiple views. By operating on a rasterized G-Buffer, our approach is independent of the representation used for the scene itself; the resulting QuadStream is an approximate geometric representation of the scene that can be reconstructed by a thin client to render both the current view and nearby adjacent views. Our technical contributions are an efficient parallel quad generation, merging, and packing strategy for proxy views covering potential client movement in a scene; a packing and encoding strategy that allows masked quads with depth information to be transmitted as a frame-coherent stream; and an efficient rendering approach for rendering our QuadStream representation into entirely novel views on thin clients. We show that our approach achieves superior quality compared both to video data streaming methods, and to geometry-based streaming.},
  archive      = {J_TOG},
  author       = {Jozef Hladky and Michael Stengel and Nicholas Vining and Bernhard Kerbl and Hans-Peter Seidel and Markus Steinberger},
  doi          = {10.1145/3550454.3555524},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {233:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {QuadStream: A quad-based scene streaming architecture for novel viewpoint reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LuisaRender: A high-performance rendering framework with
layered and unified interfaces on stream architectures. <em>TOG</em>,
<em>41</em>(6), 232:1–19. (<a
href="https://doi.org/10.1145/3550454.3555463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in hardware have drawn more attention than ever to high-quality offline rendering with modern stream processors, both in the industry and in research fields. However, the graphics APIs are fragmented and existing shading languages lack high-level constructs such as polymorphism, which adds complexity to developing and maintaining cross-platform high-performance renderers. We present LuisaRender 1 , a high-performance rendering framework for modern stream-architecture hardware. Our main contribution is an expressive C++-embedded DSL for kernel programming with JIT code generation and compilation. We also implement a unified runtime layer with resource wrappers and an optimized Monte Carlo renderer. Experiments on test scenes show that LuisaRender achieves much higher performance than existing research renderers on modern graphics hardware, e.g., 5--11× faster than PBRT-v4 and 4--16× faster than Mitsuba 3.},
  archive      = {J_TOG},
  author       = {Shaokun Zheng and Zhiqian Zhou and Xin Chen and Difei Yan and Chuyan Zhang and Yuefeng Geng and Yan Gu and Kun Xu},
  doi          = {10.1145/3550454.3555463},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {232:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to relight portrait images via a virtual light
stage and synthetic-to-real adaptation. <em>TOG</em>, <em>41</em>(6),
231:1–21. (<a href="https://doi.org/10.1145/3550454.3555442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photorealistic. To meet the first condition, we propose to train the relighting network with training data generated by a virtual light stage that performs physically-based rendering on various 3D synthetic humans under different environment maps. To meet the second condition, we develop a novel synthetic-to-real approach to bring photorealism to the relighting network output. In addition to achieving SOTA results, our approach offers several advantages over the prior methods, including controllable glares on glasses and more temporally-consistent results for relighting videos.},
  archive      = {J_TOG},
  author       = {Yu-Ying Yeh and Koki Nagano and Sameh Khamis and Jan Kautz and Ming-Yu Liu and Ting-Chun Wang},
  doi          = {10.1145/3550454.3555442},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {231:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to relight portrait images via a virtual light stage and synthetic-to-real adaptation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepJoin: Learning a joint occupancy, signed distance, and
normal field function for shape repair. <em>TOG</em>, <em>41</em>(6),
230:1–10. (<a href="https://doi.org/10.1145/3550454.3555470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce DeepJoin, an automated approach to generate high-resolution repairs for fractured shapes using deep neural networks. Existing approaches to perform automated shape repair operate exclusively on symmetric objects, require a complete proxy shape, or predict restoration shapes using low-resolution voxels which are too coarse for physical repair. We generate a high-resolution restoration shape by inferring a corresponding complete shape and a break surface from an input fractured shape. We present a novel implicit shape representation for fractured shape repair that combines the occupancy function, signed distance function, and normal field. We demonstrate repairs using our approach for synthetically fractured objects from ShapeNet, 3D scans from the Google Scanned Objects dataset, objects in the style of ancient Greek pottery from the QP Cultural Heritage dataset, and real fractured objects. We outperform six baseline approaches in terms of chamfer distance and normal consistency. Unlike existing approaches and restorations generated using subtraction, DeepJoin restorations do not exhibit surface artifacts and join closely to the fractured region of the fractured shape. Our code is available at: https://github.com/Terascale-All-sensing-Research-Studio/DeepJoin.},
  archive      = {J_TOG},
  author       = {Nikolas Lamb and Sean Banerjee and Natasha Kholgade Banerjee},
  doi          = {10.1145/3550454.3555470},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {230:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepJoin: Learning a joint occupancy, signed distance, and normal field function for shape repair},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neural galerkin solver for accurate surface
reconstruction. <em>TOG</em>, <em>41</em>(6), 229:1–16. (<a
href="https://doi.org/10.1145/3550454.3555457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reconstruct meshes from the widely-available 3D point cloud data, implicit shape representation is among the primary choices as an intermediate form due to its superior representation power and robustness in topological optimizations. Although different parameterizations of the implicit fields have been explored to model the underlying geometry, there is no explicit mechanism to ensure the fitting tightness of the surface to the input. We present in response, NeuralGalerkin, a neural Galerkin-method-based solver designed for reconstructing highly-accurate surfaces from the input point clouds. NeuralGalerkin internally discretizes the target implicit field as a linear combination of a set of spatially-varying basis functions inferred by an adaptive sparse convolution neural network. It then solves differentiably for a variational problem that incorporates both positional and normal constraints from the data in closed form within a single forward pass, highly respecting the raw input points. The reconstructed surface extracted from the implicit interpolants is hence very accurate and incorporates useful inductive biases benefiting from the training data. Extensive evaluations on various datasets demonstrate our method&#39;s promising reconstruction performance and scalability.},
  archive      = {J_TOG},
  author       = {Jiahui Huang and Hao-Xiang Chen and Shi-Min Hu},
  doi          = {10.1145/3550454.3555457},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {229:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A neural galerkin solver for accurate surface reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFEPS: Reconstructing feature-line equipped polygonal
surface. <em>TOG</em>, <em>41</em>(6), 228:1–15. (<a
href="https://doi.org/10.1145/3550454.3555443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature lines are important geometric cues in characterizing the structure of a CAD model. Despite great progress in both explicit reconstruction and implicit reconstruction, it remains a challenging task to reconstruct a polygonal surface equipped with feature lines, especially when the input point cloud is noisy and lacks faithful normal vectors. In this paper, we develop a multistage algorithm, named RFEPS , to address this challenge. The key steps include (1) denoising the point cloud based on the assumption of local planarity, (2) identifying the feature-line zone by optimization of discrete optimal transport, (3) augmenting the point set so that sufficiently many additional points are generated on potential geometry edges, and (4) generating a polygonal surface that interpolates the augmented point set based on restricted power diagram. We demonstrate through extensive experiments that RFEPS, benefiting from the edge-point augmentation and the feature preserving explicit reconstruction, outperforms state of the art methods in terms of the reconstruction quality, especially in terms of the ability to reconstruct missing feature lines.},
  archive      = {J_TOG},
  author       = {Rui Xu and Zixiong Wang and Zhiyang Dou and Chen Zong and Shiqing Xin and Mingyan Jiang and Tao Ju and Changhe Tu},
  doi          = {10.1145/3550454.3555443},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {228:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {RFEPS: Reconstructing feature-line equipped polygonal surface},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic poisson surface reconstruction. <em>TOG</em>,
<em>41</em>(6), 227:1–12. (<a
href="https://doi.org/10.1145/3550454.3555441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a statistical extension of the classic Poisson Surface Reconstruction algorithm for recovering shapes from 3D point clouds. Instead of outputting an implicit function, we represent the reconstructed shape as a modified Gaussian Process, which allows us to conduct statistical queries (e.g., the likelihood of a point in space being on the surface or inside a solid). We show that this perspective: improves PSR&#39;s integration into the online scanning process, broadens its application realm, and opens the door to other lines of research such as applying task-specific priors.},
  archive      = {J_TOG},
  author       = {Silvia Sellán and Alec Jacobson},
  doi          = {10.1145/3550454.3555441},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {227:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stochastic poisson surface reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuralRoom: Geometry-constrained neural implicit surfaces
for indoor scene reconstruction. <em>TOG</em>, <em>41</em>(6), 226:1–15.
(<a href="https://doi.org/10.1145/3550454.3555514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel neural surface reconstruction method called NeuralRoom for reconstructing room-sized indoor scenes directly from a set of 2D images. Recently, implicit neural representations have become a promising way to reconstruct surfaces from multiview images due to their high-quality results and simplicity. However, implicit neural representations usually cannot reconstruct indoor scenes well because they suffer severe shape-radiance ambiguity. We assume that the indoor scene consists of texture-rich and flat texture-less regions. In texture-rich regions, the multiview stereo can obtain accurate results. In the flat area, normal estimation networks usually obtain a good normal estimation. Based on the above observations, we reduce the possible spatial variation range of implicit neural surfaces by reliable geometric priors to alleviate shape-radiance ambiguity. Specifically, we use multiview stereo results to limit the NeuralRoom optimization space and then use reliable geometric priors to guide NeuralRoom training. Then the NeuralRoom would produce a neural scene representation that can render an image consistent with the input training images. In addition, we propose a smoothing method called perturbation-residual restrictions to improve the accuracy and completeness of the flat region, which assumes that the sampling points in a local surface should have the same normal and similar distance to the observation center. Experiments on the ScanNet dataset show that our method can reconstruct the texture-less area of indoor scenes while maintaining the accuracy of detail. We also apply NeuralRoom to more advanced multiview reconstruction algorithms and significantly improve their reconstruction quality.},
  archive      = {J_TOG},
  author       = {Yusen Wang and Zongcheng Li and Yu Jiang and Kaixuan Zhou and Tuo Cao and Yanping Fu and Chunxia Xiao},
  doi          = {10.1145/3550454.3555514},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {226:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralRoom: Geometry-constrained neural implicit surfaces for indoor scene reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact 3D path generation via 3D cam-linkage mechanisms.
<em>TOG</em>, <em>41</em>(6), 225:1–13. (<a
href="https://doi.org/10.1145/3550454.3555431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exact 3D path generation is a fundamental problem of designing a mechanism to make a point exactly move along a prescribed 3D path , driven by a single actuator. Existing mechanisms are insufficient to address this problem. Planar linkages and their combinations with gears and/or plate cams can only generate 2D paths while 1-DOF spatial linkages can only generate 3D paths with rather simple shapes. In this paper, we present a new 3D cam-linkage mechanism, consisting of two 3D cams and five links, for exactly generating a continuous 3D path. To design a 3D cam-linkage mechanism, we first model a 3-DOF 5-bar spatial linkage to exactly generate a prescribed 3D path and then reduce the spatial linkage&#39;s DOFs from 3 to 1 by composing the linkage with two 3D cam-follower mechanisms. Our computational approach optimizes the 3D cam-linkage mechanism&#39;s topology and geometry to minimize the mechanism&#39;s total weight while ensuring smooth, collision-free, and singularity-free motion. We show that our 3D cam-linkage mechanism is able to exactly generate a continuous 3D path with arbitrary shape and a finite number of C 0 points, evaluate the mechanism&#39;s kinematic performance with 3D printed prototypes, and demonstrate that the mechanism can be generalized for exact 3D motion generation.},
  archive      = {J_TOG},
  author       = {Yingjie Cheng and Peng Song and Yukun Lu and Wen Jie Jeremy Chew and Ligang Liu},
  doi          = {10.1145/3550454.3555431},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {225:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Exact 3D path generation via 3D cam-linkage mechanisms},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to generate 3D shapes from a single example.
<em>TOG</em>, <em>41</em>(6), 224:1–19. (<a
href="https://doi.org/10.1145/3550454.3555480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing generative models for 3D shapes are typically trained on a large 3D dataset, often of a specific object category. In this paper, we investigate the deep generative model that learns from only a single reference 3D shape. Specifically, we present a multi-scale GAN-based model designed to capture the input shape&#39;s geometric features across a range of spatial scales. To avoid large memory and computational cost induced by operating on the 3D volume, we build our generator atop the tri-plane hybrid representation, which requires only 2D convolutions. We train our generative model on a voxel pyramid of the reference shape, without the need of any external supervision or manual annotation. Once trained, our model can generate diverse and high-quality 3D shapes possibly of different sizes and aspect ratios. The resulting shapes present variations across different scales, and at the same time retain the global structure of the reference shape. Through extensive evaluation, both qualitative and quantitative, we demonstrate that our model can generate 3D shapes of various types. 1},
  archive      = {J_TOG},
  author       = {Rundi Wu and Changxi Zheng},
  doi          = {10.1145/3550454.3555480},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {224:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to generate 3D shapes from a single example},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A biologically inspired hair aging model. <em>TOG</em>,
<em>41</em>(6), 223:1–9. (<a
href="https://doi.org/10.1145/3550454.3555444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hair rendering has been a focal point of attention in computer graphics for the last couple of decades. However, there have been few contributions to the modeling and rendering of the natural hair aging phenomenon. We present a new technique that simulates the process of hair graying and hair thinning on digital models due to aging. Given a 3D human head model with hair, we first compute a segmentation of the head using K-means since hair aging occurs at different rates in distinct head parts. Hair graying is simulated according to recent biological knowledge on aging factors for hairs, and hair thinning decreases hair diameters linearly with time. Our system is biologically inspired, supports facial hair, both genders and many ethnicities, and is compatible with different lengths of hair strands. Our real-time results resemble real-life hair aging, accomplished by simulating the stochastic nature of the process and the gradual decrease of melanin.},
  archive      = {J_TOG},
  author       = {Arthur E. Balbão and Marcelo Walter},
  doi          = {10.1145/3550454.3555444},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {223:1–9},
  shortjournal = {ACM Trans. Graph.},
  title        = {A biologically inspired hair aging model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dressing avatars: Deep photorealistic appearance for
physically simulated clothing. <em>TOG</em>, <em>41</em>(6), 222:1–15.
(<a href="https://doi.org/10.1145/3550454.3555456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent progress in developing animatable full-body avatars, realistic modeling of clothing - one of the core aspects of human self-expression - remains an open challenge. State-of-the-art physical simulation methods can generate realistically behaving clothing geometry at interactive rates. Modeling photorealistic appearance, however, usually requires physically-based rendering which is too expensive for interactive applications. On the other hand, data-driven deep appearance models are capable of efficiently producing realistic appearance, but struggle at synthesizing geometry of highly dynamic clothing and handling challenging body-clothing configurations. To this end, we introduce pose-driven avatars with explicit modeling of clothing that exhibit both photorealistic appearance learned from real-world data and realistic clothing dynamics. The key idea is to introduce a neural clothing appearance model that operates on top of explicit geometry: at training time we use high-fidelity tracking, whereas at animation time we rely on physically simulated geometry. Our core contribution is a physically-inspired appearance network, capable of generating photorealistic appearance with view-dependent and dynamic shadowing effects even for unseen body-clothing configurations. We conduct a thorough evaluation of our model and demonstrate diverse animation results on several subjects and different types of clothing. Unlike previous work on photorealistic full-body avatars, our approach can produce much richer dynamics and more realistic deformations even for many examples of loose clothing. We also demonstrate that our formulation naturally allows clothing to be used with avatars of different people while staying fully animatable, thus enabling, for the first time, photorealistic avatars with novel clothing.},
  archive      = {J_TOG},
  author       = {Donglai Xiang and Timur Bagautdinov and Tuur Stuyck and Fabian Prada and Javier Romero and Weipeng Xu and Shunsuke Saito and Jingfan Guo and Breannan Smith and Takaaki Shiratori and Yaser Sheikh and Jessica Hodgins and Chenglei Wu},
  doi          = {10.1145/3550454.3555456},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {222:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dressing avatars: Deep photorealistic appearance for physically simulated clothing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based bending stiffness parameter estimation by a
drape tester. <em>TOG</em>, <em>41</em>(6), 221:1–16. (<a
href="https://doi.org/10.1145/3550454.3555464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world fabrics often possess complicated nonlinear, anisotropic bending stiffness properties. Measuring the physical parameters of such properties for physics-based simulation is difficult yet unnecessary, due to the persistent existence of numerical errors in simulation technology. In this work, we propose to adopt a simulation-in-the-loop strategy: instead of measuring the physical parameters, we estimate the simulation parameters to minimize the discrepancy between reality and simulation. This strategy offers good flexibility in test setups, but the associated optimization problem is computationally expensive to solve by numerical methods. Our solution is to train a regression-based neural network for inferring bending stiffness parameters, directly from drape features captured in the real world. Specifically, we choose the Cusick drape test method and treat multiple-view depth images as the feature vector. To effectively and efficiently train our network, we develop a highly expressive and physically validated bending stiffness model, and we use the traditional cantilever test to collect the parameters of this model for 618 real-world fabrics. Given the whole parameter data set, we then construct a parameter subspace, generate new samples within the sub-space, and finally simulate and augment synthetic data for training purposes. The experiment shows that our trained system can replace cantilever tests for quick, reliable and effective estimation of simulation-ready parameters. Thanks to the use of the system, our simulator can now faithfully simulate bending effects comparable to those in the real world.},
  archive      = {J_TOG},
  author       = {Xudong Feng and Wenchao Huang and Weiwei Xu and Huamin Wang},
  doi          = {10.1145/3550454.3555464},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {221:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning-based bending stiffness parameter estimation by a drape tester},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural cloth simulation. <em>TOG</em>, <em>41</em>(6),
220:1–14. (<a href="https://doi.org/10.1145/3550454.3555491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general framework for the garment animation problem through unsupervised deep learning inspired in physically based simulation. Existing trends in the literature already explore this possibility. Nonetheless, these approaches do not handle cloth dynamics. Here, we propose the first methodology able to learn realistic cloth dynamics unsupervisedly, and henceforth, a general formulation for neural cloth simulation. The key to achieve this is to adapt an existing optimization scheme for motion from simulation based methodologies to deep learning. Then, analyzing the nature of the problem, we devise an architecture able to automatically disentangle static and dynamic cloth subspaces by design. We will show how this improves model performance. Additionally, this opens the possibility of a novel motion augmentation technique that greatly improves generalization. Finally, we show it also allows to control the level of motion in the predictions. This is a useful, never seen before, tool for artists. We provide of detailed analysis of the problem to establish the bases of neural cloth simulation and guide future research into the specifics of this domain.},
  archive      = {J_TOG},
  author       = {Hugo Bertiche and Meysam Madadi and Sergio Escalera},
  doi          = {10.1145/3550454.3555491},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {220:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural cloth simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion guided deep dynamic 3D garments. <em>TOG</em>,
<em>41</em>(6), 219:1–12. (<a
href="https://doi.org/10.1145/3550454.3555485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic dynamic garments on animated characters have many AR/VR applications. While authoring such dynamic garment geometry is still a challenging task, data-driven simulation provides an attractive alternative, especially if it can be controlled simply using the motion of the underlying character. In this work, we focus on motion guided dynamic 3D garments, especially for loose garments. In a data-driven setup, we first learn a generative space of plausible garment geometries. Then, we learn a mapping to this space to capture the motion dependent dynamic deformations, conditioned on the previous state of the garment as well as its relative position with respect to the underlying body. Technically, we model garment dynamics, driven using the input character motion, by predicting per-frame local displacements in a canonical state of the garment that is enriched with frame-dependent skinning weights to bring the garment to the global space. We resolve any remaining per-frame collisions by predicting residual local displacements. The resultant garment geometry is used as history to enable iterative roll-out prediction. We demonstrate plausible generalization to unseen body shapes and motion inputs, and show improvements over multiple state-of-the-art alternatives. Code and data is released in https://geometry.cs.ucl.ac.uk/projects/2022/MotionDeepGarment/},
  archive      = {J_TOG},
  author       = {Meng Zhang and Duygu Ceylan and Niloy J. Mitra},
  doi          = {10.1145/3550454.3555485},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {219:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Motion guided deep dynamic 3D garments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive simulation for cloth quasistatics. <em>TOG</em>,
<em>41</em>(6), 218:1–16. (<a
href="https://doi.org/10.1145/3550454.3555510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trade-off between speed and fidelity in cloth simulation is a fundamental computational problem in computer graphics and computational design. Coarse cloth models provide the interactive performance required by designers, but they can not be simulated at higher resolutions (&quot;up-resed&quot;) without introducing simulation artifacts and/or unpredicted outcomes, such as different folds, wrinkles and drapes. But how can a coarse simulation predict the result of an unconstrained, high-resolution simulation that has not yet been run? We propose Progressive Cloth Simulation (PCS), a new forward simulation method for efficient preview of cloth quasistatics on exceedingly coarse triangle meshes with consistent and progressive improvement over a hierarchy of increasingly higher-resolution models. PCS provides an efficient coarse previewing simulation method that predicts the coarse-scale folds and wrinkles that will be generated by a corresponding converged, high-fidelity C-IPC simulation of the cloth drape&#39;s equilibrium. For each preview PCS can generate an increasing-resolution sequence of consistent models that progress towards this converged solution. This successive improvement can then be interrupted at any point, for example, whenever design parameters are updated. PCS then ensures feasibility at all resolutions, so that predicted solutions remain intersection-free and capture the complex folding and buckling behaviors of frictionally contacting cloth.},
  archive      = {J_TOG},
  author       = {Jiayi Eris Zhang and Jérémie Dumas and Yun (Raymond) Fei and Alec Jacobson and Doug L. James and Danny M. Kaufman},
  doi          = {10.1145/3550454.3555510},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {218:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Progressive simulation for cloth quasistatics},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An implicit parametric morphable dental model. <em>TOG</em>,
<em>41</em>(6), 217:1–13. (<a
href="https://doi.org/10.1145/3550454.3555469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Morphable models of the human body capture variations among subjects and are useful in reconstruction and editing applications. Current dental models use an explicit mesh scene representation and model only the teeth, ignoring the gum. In this work, we present the first parametric 3D morphable dental model for both teeth and gum. Our model uses an implicit scene representation and is learned from rigidly aligned scans. It is based on a component-wise representation for each tooth and the gum, together with a learnable latent code for each of such components. It also learns a template shape thus enabling several applications such as segmentation, interpolation and tooth replacement. Our reconstruction quality is on par with the most advanced global implicit representations while enabling novel applications. The code will be available at https://github.com/cong-yi/DMM},
  archive      = {J_TOG},
  author       = {Congyi Zhang and Mohamed Elgharib and Gereon Fox and Min Gu and Christian Theobalt and Wenping Wang},
  doi          = {10.1145/3550454.3555469},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {217:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {An implicit parametric morphable dental model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LaplacianFusion: Detailed 3D clothed-human body
reconstruction. <em>TOG</em>, <em>41</em>(6), 216:1–14. (<a
href="https://doi.org/10.1145/3550454.3555511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose LaplacianFusion , a novel approach that reconstructs detailed and controllable 3D clothed-human body shapes from an input depth or 3D point cloud sequence. The key idea of our approach is to use Laplacian coordinates, well-known differential coordinates that have been used for mesh editing, for representing the local structures contained in the input scans, instead of implicit 3D functions or vertex displacements used previously. Our approach reconstructs a controllable base mesh using SMPL, and learns a surface function that predicts Laplacian coordinates representing surface details on the base mesh. For a given pose, we first build and subdivide a base mesh, which is a deformed SMPL template, and then estimate Laplacian coordinates for the mesh vertices using the surface function. The final reconstruction for the pose is obtained by integrating the estimated Laplacian coordinates as a whole. Experimental results show that our approach based on Laplacian coordinates successfully reconstructs more visually pleasing shape details than previous methods. The approach also enables various surface detail manipulations, such as detail transfer and enhancement.},
  archive      = {J_TOG},
  author       = {Hyomin Kim and Hyeonseo Nam and Jungeon Kim and Jaesik Park and Seungyong Lee},
  doi          = {10.1145/3550454.3555511},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {216:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {LaplacianFusion: Detailed 3D clothed-human body reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geo-metric: A perceptual dataset of distortions on faces.
<em>TOG</em>, <em>41</em>(6), 215:1–13. (<a
href="https://doi.org/10.1145/3550454.3555475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we take a novel perception-centered approach to quantify distortions on 3D geometry of faces, to which humans are particularly sensitive. We generated a dataset, composed of 100 high-quality and demographically-balanced face scans. We then subjected these meshes to distortions that cover relevant use cases in computer graphics, and conducted a large-scale perceptual study to subjectively evaluate them. Our dataset consists of over 84,000 quality comparisons, making it the largest ever psychophysical dataset for geometric distortions. Finally, we demonstrated how our data can be used for applications like metrics, compression, and level-of-detail rendering.},
  archive      = {J_TOG},
  author       = {Krzysztof Wolski and Laura Trutoiu and Zhao Dong and Zhengyang Shen and Kevin Mackenzie and Alexandre Chapiro},
  doi          = {10.1145/3550454.3555475},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {215:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Geo-metric: A perceptual dataset of distortions on faces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid face asset acquisition with recurrent feature
alignment. <em>TOG</em>, <em>41</em>(6), 214:1–17. (<a
href="https://doi.org/10.1145/3550454.3555509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Re current F eature A lignment (ReFA), an end-to-end neural network for the very rapid creation of production-grade face assets from multi-view images. ReFA is on par with the industrial pipelines in quality for producing accurate, complete, registered, and textured assets directly applicable to physically-based rendering, but produces the asset end-to-end, fully automatically at a significantly faster speed at 4.5 FPS, which is unprecedented among neural-based techniques. Our method represents face geometry as a position map in the UV space. The network first extracts per-pixel features in both the multi-view image space and the UV space. A recurrent module then iteratively optimizes the geometry by projecting the image-space features to the UV space and comparing them with a reference UV-space feature. The optimized geometry then provides pixel-aligned signals for the inference of high-resolution textures. Experiments have validated that ReFA achieves a median error of 0.603 mm in geometry reconstruction, is robust to extreme pose and expression, and excels in sparse-view settings. We believe that the progress achieved by our network enables lightweight, fast face assets acquisition that significantly boosts the downstream applications, such as avatar creation and facial performance capture. It will also enable massive database capturing for deep learning purposes.},
  archive      = {J_TOG},
  author       = {Shichen Liu and Yunxuan Cai and Haiwei Chen and Yichao Zhou and Yajie Zhao},
  doi          = {10.1145/3550454.3555509},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {214:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rapid face asset acquisition with recurrent feature alignment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SCULPTOR: Skeleton-consistent face creation using a learned
parametric generator. <em>TOG</em>, <em>41</em>(6), 213:1–17. (<a
href="https://doi.org/10.1145/3550454.3555462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen growing interest in 3D human face modeling due to its wide applications in digital human, character generation and animation. Existing approaches overwhelmingly emphasized on modeling the exterior shapes, textures and skin properties of faces, ignoring the inherent correlation between inner skeletal structures and appearance. In this paper, we present SCULPTOR, 3D face creations with Skeleton Consistency Using a Learned Parametric facial generaTOR , aiming to facilitate the easy creation of both anatomically correct and visually convincing face models via a hybrid parametric-physical representation. At the core of SCULPTOR is LUCY, the first large-scale shape-skeleton face dataset in collaboration with plastic surgeons. Named after the fossils of one of the oldest known human ancestors, our LUCY dataset contains high-quality Computed Tomography (CT) scans of the complete human head before and after orthognathic surgeries, which are critical for evaluating surgery results. LUCY consists of 144 scans of 72 subjects (31 male and 41 female), where each subject has two CT scans taken pre- and post-orthognathic operations. Based on our LUCY dataset, we learned a novel skeleton consistent parametric facial generator, SCULPTOR, which can create unique and nuanced facial features that help define a character and at the same time maintain physiological soundness. Our SCULPTOR jointly models the skull, face geometry and face appearance under a unified data-driven framework by separating the depiction of a 3D face into shape blend shape, pose blend shape and facial expression blend shape. SCULPTOR preserves both anatomic correctness and visual realism in facial generation tasks compared with existing methods. Finally, we showcase the robustness and effectiveness of SCULPTOR in various fancy applications unseen before, like archaeological skeletal facial completion, bone-aware character fusion, skull inference from images, face generation with lipo-Level change and facial animations, etc.},
  archive      = {J_TOG},
  author       = {Zesong Qiu and Yuwei Li and Dongming He and Qixuan Zhang and Longwen Zhang and Yinghao Zhang and Jingya Wang and Lan Xu and Xudong Wang and Yuyao Zhang and Jingyi Yu},
  doi          = {10.1145/3550454.3555462},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {213:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {SCULPTOR: Skeleton-consistent face creation using a learned parametric generator},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pupil-aware holography. <em>TOG</em>, <em>41</em>(6),
212:1–15. (<a href="https://doi.org/10.1145/3550454.3555508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays promise to deliver unprecedented display capabilities in augmented reality applications, featuring a wide field of view, wide color gamut, spatial resolution, and depth cues all in a compact form factor. While emerging holographic display approaches have been successful in achieving large étendue and high image quality as seen by a camera, the large étendue also reveals a problem that makes existing displays impractical: the sampling of the holographic field by the eye pupil. Existing methods have not investigated this issue due to the lack of displays with large enough étendue, and, as such, they suffer from severe artifacts with varying eye pupil size and location. We show that the holographic field as sampled by the eye pupil is highly varying for existing display setups, and we propose pupil-aware holography that maximizes the perceptual image quality irrespective of the size, location, and orientation of the eye pupil in a near-eye holographic display. We validate the proposed approach both in simulations and on a prototype holographic display and show that our method eliminates severe artifacts and significantly outperforms existing approaches.},
  archive      = {J_TOG},
  author       = {Praneeth Chakravarthula and Seung-Hwan Baek and Florian Schiffers and Ethan Tseng and Grace Kuo and Andrew Maimone and Nathan Matsuda and Oliver Cossairt and Douglas Lanman and Felix Heide},
  doi          = {10.1145/3550454.3555508},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {212:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pupil-aware holography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The continuity of locomotion: Rethinking conventions for
locomotion and its visualization in shared virtual reality spaces.
<em>TOG</em>, <em>41</em>(6), 211:1–14. (<a
href="https://doi.org/10.1145/3550454.3555522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural interaction between multiple users within a shared virtual environment (VE) relies on each other&#39;s awareness of the current position of the interaction partners. This, however, cannot be warranted when users employ noncontinuous locomotion techniques, such as teleportation, which may cause confusion among bystanders. In this paper, we pursue two approaches to create a pleasant experience for both the moving user and the bystanders observing that movement. First, we will introduce a Smart Avatar system that delivers continuous full-body human representations for noncontinuous locomotion in shared virtual reality (VR) spaces. Smart Avatars imitate their assigned user&#39;s real-world movements when close-by and autonomously navigate to their user when the distance between them exceeds a certain threshold, i.e., after the user teleports. As part of the Smart Avatar system, we implemented four avatar transition techniques and compared them to conventional avatar locomotion in a user study, revealing significant positive effects on the observers&#39; spatial awareness, as well as pragmatic and hedonic quality scores. Second, we introduce the concept of Stuttered Locomotion , which can be applied to any continuous locomotion method. By converting a continuous movement into short-interval teleport steps, we provide the merits of non-continuous locomotion for the moving user while observers can easily keep track of their path. Thus, while the experience for observers is similarly positive as with continuous motion, a user study confirmed that Stuttered Locomotion can significantly reduce the occurrence of cybersickness symptoms for the moving user, making it an attractive choice for shared VEs. We will discuss the potential of Smart Avatars and Stuttered Locomotion for shared VR experiences, both when applied individually and in combination.},
  archive      = {J_TOG},
  author       = {Jann Philipp Freiwald and Susanne Schmidt and Bernhard E. Riecke and Frank Steinicke},
  doi          = {10.1145/3550454.3555522},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {211:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {The continuity of locomotion: Rethinking conventions for locomotion and its visualization in shared virtual reality spaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Color-perception-guided display power reduction for virtual
reality. <em>TOG</em>, <em>41</em>(6), 210:1–16. (<a
href="https://doi.org/10.1145/3550454.3555473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Battery life is an increasingly urgent challenge for today&#39;s untethered VR and AR devices. However, the power efficiency of head-mounted displays is naturally at odds with growing computational requirements driven by better resolution, refresh rate, and dynamic ranges, all of which reduce the sustained usage time of untethered AR/VR devices. For instance, the Oculus Quest 2, under a fully-charged battery, can sustain only 2 to 3 hours of operation time. Prior display power reduction techniques mostly target smartphone displays. Directly applying smartphone display power reduction techniques, however, degrades the visual perception in AR/VR with noticeable artifacts. For instance, the &quot;power-saving mode&quot; on smartphones uniformly lowers the pixel luminance across the display and, as a result, presents an overall darkened visual perception to users if directly applied to VR content. Our key insight is that VR display power reduction must be cognizant of the gaze-contingent nature of high field-of-view VR displays. To that end, we present a gaze-contingent system that, without degrading luminance, minimizes the display power consumption while preserving high visual fidelity when users actively view immersive video sequences. This is enabled by constructing 1) a gaze-contingent color discrimination model through psychophysical studies, and 2) a display power model (with respect to pixel color) through real-device measurements. Critically, due to the careful design decisions made in constructing the two models, our algorithm is cast as a constrained optimization problem with a closed-form solution, which can be implemented as a real-time, image-space shader. We evaluate our system using a series of psychophysical studies and large-scale analyses on natural images. Experiment results show that our system reduces the display power by as much as 24\% (14\% on average) with little to no perceptual fidelity degradation.},
  archive      = {J_TOG},
  author       = {Budmonde Duinkharjav and Kenneth Chen and Abhishek Tyagi and Jiayi He and Yuhao Zhu and Qi Sun},
  doi          = {10.1145/3550454.3555473},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {210:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Color-perception-guided display power reduction for virtual reality},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rhythmic gesticulator: Rhythm-aware co-speech gesture
synthesis with hierarchical neural embeddings. <em>TOG</em>,
<em>41</em>(6), 209:1–19. (<a
href="https://doi.org/10.1145/3550454.3555435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech and the motion, resulting in rhythm- and semantics-aware gesture synthesis. Evaluations with existing objective metrics, a newly proposed rhythmic metric, and human feedback show that our method outperforms state-of-the-art systems by a clear margin.},
  archive      = {J_TOG},
  author       = {Tenglong Ao and Qingzhe Gao and Yuke Lou and Baoquan Chen and Libin Liu},
  doi          = {10.1145/3550454.3555435},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {209:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rhythmic gesticulator: Rhythm-aware co-speech gesture synthesis with hierarchical neural embeddings},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video-driven neural physically-based facial asset for
production. <em>TOG</em>, <em>41</em>(6), 208:1–16. (<a
href="https://doi.org/10.1145/3550454.3555445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production-level workflows for producing convincing 3D dynamic human faces have long relied on an assortment of labor-intensive tools for geometry and texture generation, motion capture and rigging, and expression synthesis. Recent neural approaches automate individual components but the corresponding latent representations cannot provide artists with explicit controls as in conventional tools. In this paper, we present a new learning-based, video-driven approach for generating dynamic facial geometries with high-quality physically-based assets. For data collection, we construct a hybrid multiview-photometric capture stage, coupling with ultra-fast video cameras to obtain raw 3D facial assets. We then set out to model the facial expression, geometry and physically-based textures using separate VAEs where we impose a global MLP based expression mapping across the latent spaces of respective networks, to preserve characteristics across respective attributes. We also model the delta information as wrinkle maps for the physically-based textures, achieving high-quality 4K dynamic textures. We demonstrate our approach in high-fidelity performer-specific facial capture and cross-identity facial motion retargeting. In addition, our multi-VAE-based neural asset, along with the fast adaptation schemes, can also be deployed to handle in-the-wild videos. Besides, we motivate the utility of our explicit facial disentangling strategy by providing various promising physically-based editing results with high realism. Comprehensive experiments show that our technique provides higher accuracy and visual fidelity than previous video-driven facial reconstruction and animation methods.},
  archive      = {J_TOG},
  author       = {Longwen Zhang and Chuxiao Zeng and Qixuan Zhang and Hongyang Lin and Ruixiang Cao and Wei Yang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3550454.3555445},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {208:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Video-driven neural physically-based facial asset for production},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reference based sketch extraction via attention mechanism.
<em>TOG</em>, <em>41</em>(6), 207:1–16. (<a
href="https://doi.org/10.1145/3550454.3555504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model that extracts a sketch from a colorized image in such a way that the extracted sketch has a line style similar to a given reference sketch while preserving the visual content identically to the colorized image. Authentic sketches drawn by artists have various sketch styles to add visual interest and contribute feeling to the sketch. However, existing sketch-extraction methods generate sketches with only one style. Moreover, existing style transfer models fail to transfer sketch styles because they are mostly designed to transfer textures of a source style image instead of transferring the sparse line styles from a reference sketch. Lacking the necessary volumes of data for standard training of translation systems, at the core of our GAN-based solution is a self-reference sketch style generator that produces various reference sketches with a similar style but different spatial layouts. We use independent attention modules to detect the edges of a colorized image and reference sketch as well as the visual correspondences between them. We apply several loss terms to imitate the style and enforce sparsity in the extracted sketches. Our sketch-extraction method results in a close imitation of a reference sketch style drawn by an artist and outperforms all baseline methods. Using our method, we produce a synthetic dataset representing various sketch styles and improve the performance of auto-colorization models, in high demand in comics. The validity of our approach is confirmed via qualitative and quantitative evaluations.},
  archive      = {J_TOG},
  author       = {Amirsaman Ashtari and Chang Wook Seo and Cholmin Kang and Sihun Cha and Junyong Noh},
  doi          = {10.1145/3550454.3555504},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {207:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reference based sketch extraction via attention mechanism},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MyStyle: A personalized generative prior. <em>TOG</em>,
<em>41</em>(6), 206:1–10. (<a
href="https://doi.org/10.1145/3550454.3555436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person&#39;s key facial characteristics. Given a small reference set of portrait images of a person (~ 100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.},
  archive      = {J_TOG},
  author       = {Yotam Nitzan and Kfir Aberman and Qiurui He and Orly Liba and Michal Yarom and Yossi Gandelsman and Inbar Mosseri and Yael Pritch and Daniel Cohen-Or},
  doi          = {10.1145/3550454.3555436},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {206:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {MyStyle: A personalized generative prior},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UniColor: A unified framework for multi-modal colorization
with transformer. <em>TOG</em>, <em>41</em>(6), 205:1–16. (<a
href="https://doi.org/10.1145/3550454.3555471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the first unified framework UniColor to support colorization in multiple modalities, including both unconditional and conditional ones, such as stroke, exemplar, text, and even a mix of them. Rather than learning a separate model for each type of condition, we introduce a two-stage colorization framework for incorporating various conditions into a single model. In the first stage, multi-modal conditions are converted into a common representation of hint points. Particularly, we propose a novel CLIP-based method to convert the text to hint points. In the second stage, we propose a Transformer-based network composed of Chroma-VQGAN and Hybrid-Transformer to generate diverse and high-quality colorization results conditioned on hint points. Both qualitative and quantitative comparisons demonstrate that our method outperforms state-of-the-art methods in every control modality and further enables multi-modal colorization that was not feasible before. Moreover, we design an interactive interface showing the effectiveness of our unified framework in practical usage, including automatic colorization, hybrid-control colorization, local recolorization, and iterative color editing. Our code and models are available at https://luckyhzt.github.io/unicolor .},
  archive      = {J_TOG},
  author       = {Zhitong Huang and Nanxuan Zhao and Jing Liao},
  doi          = {10.1145/3550454.3555471},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {205:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {UniColor: A unified framework for multi-modal colorization with transformer},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled image colorization via global anchors.
<em>TOG</em>, <em>41</em>(6), 204:1–13. (<a
href="https://doi.org/10.1145/3550454.3555432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorization is multimodal by nature and challenges existing frameworks to achieve colorful and structurally consistent results. Even the sophisticated autoregressive model struggles to maintain long-distance color consistency due to the fragility of sequential dependence. To overcome this challenge, we propose a novel colorization framework that disentangles color multimodality and structure consistency through global color anchors, so that both aspects could be learned effectively. Our key insight is that several carefully located anchors could approximately represent the color distribution of an image, and conditioned on the anchor colors, we can predict the image color in a deterministic manner by utilizing internal correlation. To this end, we construct a colorization model with dual branches, where the color modeler predicts the color distribution for anchor color representation, and the color generator predicts the pixel colors by referring the sampled anchor colors. Importantly, the anchors are located under two principles: color independence and global coverage, which is realized with clustering analysis on the deep color features. To simplify the computation, we creatively adopt soft superpixel segmentation to reduce the image primitives, which still nicely reserves the reversibility to pixel-wise representation. Extensive experiments show that our method achieves notable superiority over various mainstream frameworks in perceptual quality. Thanks to anchor-based color representation, our model has the flexibility to support diverse and controllable colorization as well.},
  archive      = {J_TOG},
  author       = {Menghan Xia and Wenbo Hu and Tien-Tsin Wong and Jue Wang},
  doi          = {10.1145/3550454.3555432},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {204:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Disentangled image colorization via global anchors},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VToonify: Controllable high-resolution portrait video style
transfer. <em>TOG</em>, <em>41</em>(6), 203:1–15. (<a
href="https://doi.org/10.1145/3550454.3555437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls. Code and pretrained models are available at our project page: www.mmlab-ntu.com/project/vtoonify/.},
  archive      = {J_TOG},
  author       = {Shuai Yang and Liming Jiang and Ziwei Liu and Chen Change Loy},
  doi          = {10.1145/3550454.3555437},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {203:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {VToonify: Controllable high-resolution portrait video style transfer},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient light probes for real-time global illumination.
<em>TOG</em>, <em>41</em>(6), 202:1–14. (<a
href="https://doi.org/10.1145/3550454.3555452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproducing physically-based global illumination (GI) effects has been a long-standing demand for many real-time graphical applications. In pursuit of this goal, many recent engines resort to some form of light probes baked in a precomputation stage. Unfortunately, the GI effects stemming from the precomputed probes are rather limited due to the constraints in the probe storage, representation or query. In this paper, we propose a new method for probe-based GI rendering which can generate a wide range of GI effects, including glossy reflection with multiple bounces, in complex scenes. The key contributions behind our work include a gradient-based search algorithm and a neural image reconstruction method. The search algorithm is designed to reproject the probes&#39; contents to any query viewpoint, without introducing parallax errors, and converges fast to the optimal solution. The neural image reconstruction method, based on a dedicated neural network and several G-buffers, tries to recover high-quality images from low-quality inputs due to limited resolution or (potential) low sampling rate of the probes. This neural method makes the generation of light probes efficient. Moreover, a temporal reprojection strategy and a temporal loss are employed to improve temporal stability for animation sequences. The whole pipeline runs in realtime (&gt;30 frames per second) even for high-resolution (1920×1080) outputs, thanks to the fast convergence rate of the gradient-based search algorithm and a light-weight design of the neural network. Extensive experiments on multiple complex scenes have been conducted to show the superiority of our method over the state-of-the-arts.},
  archive      = {J_TOG},
  author       = {Jie Guo and Zijing Zong and Yadong Song and Xihao Fu and Chengzhi Tao and Yanwen Guo and Ling-Qi Yan},
  doi          = {10.1145/3550454.3555452},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {202:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient light probes for real-time global illumination},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural point catacaustics for novel-view synthesis of
reflections. <em>TOG</em>, <em>41</em>(6), 201:1–15. (<a
href="https://doi.org/10.1145/3550454.3555497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/},
  archive      = {J_TOG},
  author       = {Georgios Kopanas and Thomas Leimkühler and Gilles Rainer and Clément Jambon and George Drettakis},
  doi          = {10.1145/3550454.3555497},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {201:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural point catacaustics for novel-view synthesis of reflections},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstructing personalized semantic facial NeRF models from
monocular video. <em>TOG</em>, <em>41</em>(6), 200:1–12. (<a
href="https://doi.org/10.1145/3550454.3555501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel semantic model for human head defined with neural radiance field. The 3D-consistent head model consist of a set of disentangled and interpretable bases, and can be driven by low-dimensional expression coefficients. Thanks to the powerful representation ability of neural radiance field, the constructed model can represent complex facial attributes including hair, wearings, which can not be represented by traditional mesh blendshape. To construct the personalized semantic facial model, we propose to define the bases as several multi-level voxel fields. With a short monocular RGB video as input, our method can construct the subject&#39;s semantic facial NeRF model with only ten to twenty minutes, and can render a photorealistic human head image in tens of miliseconds with a given expression coefficient and view direction. With this novel representation, we apply it to many tasks like facial retargeting and expression editing. Experimental results demonstrate its strong representation ability and training/inference speed. Demo videos and released code are provided in our project page: https://ustc3dv.github.io/NeRFBlendShape/},
  archive      = {J_TOG},
  author       = {Xuan Gao and Chenglai Zhong and Jun Xiang and Yang Hong and Yudong Guo and Juyong Zhang},
  doi          = {10.1145/3550454.3555501},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {200:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reconstructing personalized semantic facial NeRF models from monocular video},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affordable spectral measurements of translucent materials.
<em>TOG</em>, <em>41</em>(6), 199:1–13. (<a
href="https://doi.org/10.1145/3550454.3555499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a spectral measurement approach for the bulk optical properties of translucent materials using only low-cost components. We focus on the translucent inks used in full-color 3D printing, and develop a technique with a high spectral resolution, which is important for accurate color reproduction. We enable this by developing a new acquisition technique for the three unknown material parameters, namely, the absorption and scattering coefficients, and its phase function anisotropy factor, that only requires three point measurements with a spectrometer. In essence, our technique is based on us finding a three-dimensional appearance map , computed using Monte Carlo rendering, that allows the conversion between the three observables and the material parameters. Our measurement setup works without laboratory equipment or expensive optical components. We validate our results on a 3D printed color checker with various ink combinations. Our work paves a path for more accurate appearance modeling and fabrication even for low-budget environments or affordable embedding into other devices.},
  archive      = {J_TOG},
  author       = {Tomáš Iser and Tobias Rittig and Emilie Nogué and Thomas Klaus Nindel and Alexander Wilkie},
  doi          = {10.1145/3550454.3555499},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {199:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Affordable spectral measurements of translucent materials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous collaborative autoscanning with mode switching
for multi-robot scene reconstruction. <em>TOG</em>, <em>41</em>(6),
198:1–13. (<a href="https://doi.org/10.1145/3550454.3555483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When conducting autonomous scanning for the online reconstruction of unknown indoor environments, robots have to be competent at exploring scene structure and reconstructing objects with high quality. Our key observation is that different tasks demand specialized scanning properties of robots: rapid moving speed and far vision for global exploration and slow moving speed and narrow vision for local object reconstruction, which are referred as two different scanning modes: explorer and reconstructor , respectively. When requiring multiple robots to collaborate for efficient exploration and fine-grained reconstruction, the questions on when to generate and how to assign those tasks should be carefully answered. Therefore, we propose a novel asynchronous collaborative autoscanning method with mode switching, which generates two kinds of scanning tasks with associated scanning modes, i.e., exploration task with explorer mode and reconstruction task with reconstructor mode, and assign them to the robots to execute in an asynchronous collaborative manner to highly boost the scanning efficiency and reconstruction quality. The task assignment is optimized by solving a modified Multi-Depot Multiple Traveling Salesman Problem (MDMTSP). Moreover, to further enhance the collaboration and increase the efficiency, we propose a task-flow model that actives the task generation and assignment process immediately when any of the robots finish all its tasks with no need to wait for all other robots to complete the tasks assigned in the previous iteration. Extensive experiments have been conducted to show the importance of each key component of our method and the superiority over previous methods in scanning efficiency and reconstruction quality.},
  archive      = {J_TOG},
  author       = {Junfu Guo and Changhao Li and Xi Xia and Ruizhen Hu and Ligang Liu},
  doi          = {10.1145/3550454.3555483},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {198:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Asynchronous collaborative autoscanning with mode switching for multi-robot scene reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning reconstructability for drone aerial path planning.
<em>TOG</em>, <em>41</em>(6), 197:1–17. (<a
href="https://doi.org/10.1145/3550454.3555433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first learning-based reconstructability predictor to improve view and path planning for large-scale 3D urban scene acquisition using unmanned drones. In contrast to previous heuristic approaches, our method learns a model that explicitly predicts how well a 3D urban scene will be reconstructed from a set of viewpoints. To make such a model trainable and simultaneously applicable to drone path planning, we simulate the proxy-based 3D scene reconstruction during training to set up the prediction. Specifically, the neural network we design is trained to predict the scene reconstructability as a function of the proxy geometry , a set of viewpoints, and optionally a series of scene images acquired in flight. To reconstruct a new urban scene, we first build the 3D scene proxy, then rely on the predicted reconstruction quality and uncertainty measures by our network, based off of the proxy geometry, to guide the drone path planning. We demonstrate that our data-driven reconstructability predictions are more closely correlated to the true reconstruction quality than prior heuristic measures. Further, our learned predictor can be easily integrated into existing path planners to yield improvements. Finally, we devise a new iterative view planning framework, based on the learned reconstructability, and show superior performance of the new planner when reconstructing both synthetic and real scenes.},
  archive      = {J_TOG},
  author       = {Yilin Liu and Liqiang Lin and Yue Hu and Ke Xie and Chi-Wing Fu and Hao Zhang and Hui Huang},
  doi          = {10.1145/3550454.3555433},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {197:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning reconstructability for drone aerial path planning},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pattern-based cloth registration and sparse-view animation.
<em>TOG</em>, <em>41</em>(6), 196:1–17. (<a
href="https://doi.org/10.1145/3550454.3555448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel multi-view camera pipeline for the reconstruction and registration of dynamic clothing. Our proposed method relies on a specifically designed pattern that allows for precise video tracking in each camera view. We triangulate the tracked points and register the cloth surface in a fine-grained geometric resolution and low localization error. Compared to state-of-the-art methods, our registration exhibits stable correspondence, tracking the same points on the deforming cloth surface along the temporal sequence. As an application, we demonstrate how the use of our registration pipeline greatly improves state-of-the-art pose-based drivable cloth models. Furthermore, we propose a novel model, Garment Avatar , for driving cloth from a dense tracking signal which is obtained from two opposing camera views. The method produces realistic reconstructions which are faithful to the actual geometry of the deforming cloth. In this setting, the user wears a garment with our custom pattern which enables our driving model to reconstruct the geometry. Our code and data are available at https://github.com/HalimiOshri/Pattern-Based-Cloth-Registration-and-Sparse-View-Animation. The released data includes our pattern and registered mesh sequences containing four different subjects and 15k frames in total.},
  archive      = {J_TOG},
  author       = {Oshri Halimi and Tuur Stuyck and Donglai Xiang and Timur Bagautdinov and He Wen and Ron Kimmel and Takaaki Shiratori and Chenglei Wu and Yaser Sheikh and Fabian Prada},
  doi          = {10.1145/3550454.3555448},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {196:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pattern-based cloth registration and sparse-view animation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text2Light: Zero-shot text-driven HDR panorama generation.
<em>TOG</em>, <em>41</em>(6), 195:1–16. (<a
href="https://doi.org/10.1145/3550454.3555447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality HDRIs (High Dynamic Range Images), typically HDR panoramas, are one of the most popular ways to create photorealistic lighting and 360-degree reflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs, a versatile and controllable generative model is highly desired, where layman users can intuitively control the generation process. However, existing state-of-the-art methods still struggle to synthesize high-quality panoramas for complex scenes. In this work, we propose a zero-shot text-driven framework, Text2Light , to generate 4K+ resolution HDRIs without paired training data. Given a free-form text as the description of the scene, we synthesize the corresponding HDRI with two dedicated steps: 1 ) text-driven panorama generation in low dynamic range (LDR) and low resolution (LR), and 2 ) super-resolution inverse tone mapping to scale up the LDR panorama both in resolution and dynamic range. Specifically, to achieve zero-shot text-driven panorama generation, we first build dual codebooks as the discrete representation for diverse environmental textures. Then, driven by the pre-trained Contrastive Language-Image Pre-training (CLIP) model, a text-conditioned global sampler learns to sample holistic semantics from the global codebook according to the input text. Furthermore, a structure-aware local sampler learns to synthesize LDR panoramas patch-by-patch, guided by holistic semantics. To achieve super-resolution inverse tone mapping, we derive a continuous representation of 360-degree imaging from the LDR panorama as a set of structured latent codes anchored to the sphere. This continuous representation enables a versatile module to upscale the resolution and dynamic range simultaneously. Extensive experiments demonstrate the superior capability of Text2Light in generating high-quality HDR panoramas. In addition, we show the feasibility of our work in realistic rendering and immersive VR.},
  archive      = {J_TOG},
  author       = {Zhaoxi Chen and Guangcong Wang and Ziwei Liu},
  doi          = {10.1145/3550454.3555447},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {195:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Text2Light: Zero-shot text-driven HDR panorama generation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PopStage: The generation of stage cross-editing video based
on spatio-temporal matching. <em>TOG</em>, <em>41</em>(6), 194:1–13. (<a
href="https://doi.org/10.1145/3550454.3555467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StageMix is a mixed video that is created by concatenating the segments from various performance videos of an identical song in a visually smooth manner by matching the main subject&#39;s silhouette presented in the frame. We introduce PopStage , which allows users to generate a StageMix automatically. PopStage is designed based on the StageMix Editing Guideline that we established by interviewing creators as well as observing their workflows. PopStage consists of two main steps: finding an editing path and generating a transition effect at a transition point. Using a reward function that favors visual connection and the optimality of transition timing across the videos, we obtain the optimal path that maximizes the sum of rewards through dynamic programming. Given the optimal path, PopStage then aligns the silhouettes of the main subject from the transitioning video pair to enhance the visual connection at the transition point. The virtual camera view is next optimized to remove the black areas that are often created due to the transformation needed for silhouette alignment, while reducing pixel loss. In this process, we enforce the view to be the maximum size while maintaining the temporal continuity across the frames. Experimental results show that PopStage can generate a StageMix of a similar quality to those produced by professional creators in a highly reduced production time.},
  archive      = {J_TOG},
  author       = {Dawon Lee and Jung Eun Yoo and Kyungmin Cho and Bumki Kim and Gyeonghun Im and Junyong Noh},
  doi          = {10.1145/3550454.3555467},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {194:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {PopStage: The generation of stage cross-editing video based on spatio-temporal matching},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Make your own sprites: Aliasing-aware and cell-controllable
pixelization. <em>TOG</em>, <em>41</em>(6), 193:1–16. (<a
href="https://doi.org/10.1145/3550454.3555482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel art is a unique art style with the appearance of low resolution images. In this paper, we propose a data-driven pixelization method that can produce sharp and crisp cell effects with controllable cell sizes. Our approach overcomes the limitation of existing learning-based methods in cell size control by introducing a reference pixel art to explicitly regularize the cell structure. In particular, the cell structure features of the reference pixel art are used as an auxiliary input for the pixelization process, and for measuring the style similarity between the generated result and the reference pixel art. Furthermore, we disentangle the pixelization process into specific cell-aware and aliasing-aware stages, mitigating the ambiguities in joint learning of cell size, aliasing effect, and color assignment. To train our model, we construct a dedicated pixel art dataset and augment it with different cell sizes and different degrees of anti-aliasing effects. Extensive experiments demonstrate its superior performance over state-of-the-arts in terms of cell sharpness and perceptual expressiveness. We also show promising results of video game pixelization for the first time. Code and dataset are available at https://github.com/WuZongWei6/Pixelization.},
  archive      = {J_TOG},
  author       = {Zongwei Wu and Liangyu Chai and Nanxuan Zhao and Bailin Deng and Yongtuo Liu and Qiang Wen and Junle Wang and Shengfeng He},
  doi          = {10.1145/3550454.3555482},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {193:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Make your own sprites: Aliasing-aware and cell-controllable pixelization},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sprite-from-sprite: Cartoon animation decomposition with
self-supervised sprite estimation. <em>TOG</em>, <em>41</em>(6),
192:1–12. (<a href="https://doi.org/10.1145/3550454.3555439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to decompose cartoon animation videos into a set of &quot;sprites&quot; --- the basic units of digital cartoons that depict the contents and transforms of each animated object. The sprites in real-world cartoons are unique: artists may draw arbitrary sprite animations for expressiveness, where the animated content is often complicated, irregular, and challenging; alternatively, artists may also reduce their workload by tweening and adjusting sprites, or even reuse static sprites, in which case the transformations are relatively regular and simple. Based on these observations, we propose a sprite decomposition framework using Pixel Multilayer Perceptrons (Pixel MLPs) where the estimation of each sprite is conditioned on and guided by all other sprites. In this way, once those relatively regular and simple sprites are resolved, the decomposition of the remaining &quot;challenging&quot; sprites can simplified and eased with the guidance of other sprites. We call this method &quot;sprite-from-sprite&quot; cartoon decomposition. We study ablative architectures of our framework, and the user study demonstrates that our results are the most preferred ones in 19/20 cases.},
  archive      = {J_TOG},
  author       = {Lvmin Zhang and Tien-Tsin Wong and Yuxin Liu},
  doi          = {10.1145/3550454.3555439},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {192:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sprite-from-sprite: Cartoon animation decomposition with self-supervised sprite estimation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient differentiation of pixel reconstruction filters
for path-space differentiable rendering. <em>TOG</em>, <em>41</em>(6),
191:1–16. (<a href="https://doi.org/10.1145/3550454.3555500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel reconstruction filters play an important role in physics-based rendering and have been thoroughly studied. In physics-based differentiable rendering, however, the proper treatment of pixel filters remains largely under-explored. We present a new technique to efficiently differentiate pixel reconstruction filters based on the path-space formulation. Specifically, we formulate the pixel boundary integral that models discontinuities in pixel filters and introduce new antithetic sampling methods that support differentiable path sampling methods, such as adjoint particle tracing and bidirectional path tracing. We demonstrate both the need and efficacy of antithetic sampling when estimating this integral, and we evaluate its effectiveness across several differentiable- and inverse-rendering settings.},
  archive      = {J_TOG},
  author       = {Zihan Yu and Cheng Zhang and Derek Nowrouzezahrai and Zhao Dong and Shuang Zhao},
  doi          = {10.1145/3550454.3555500},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {191:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient differentiation of pixel reconstruction filters for path-space differentiable rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth of field aware differentiable rendering. <em>TOG</em>,
<em>41</em>(6), 190:1–18. (<a
href="https://doi.org/10.1145/3550454.3555521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cameras with a finite aperture diameter exhibit defocus for scene elements that are not at the focus distance, and have only a limited depth of field within which objects appear acceptably sharp. In this work we address the problem of applying inverse rendering techniques to input data that exhibits such defocus blurring. We present differentiable depth-of-field rendering techniques that are applicable to both rasterization-based methods using mesh representations, as well as ray-marching-based methods using either explicit [Yu et al. 2021] or implicit volumetric radiance fields [Mildenhall et al. 2020]. Our approach learns significantly sharper scene reconstructions on data containing blur due to depth of field, and recovers aperture and focus distance parameters that result in plausible forward-rendered images. We show applications to macro photography, where typical lens configurations result in a very narrow depth of field, and to multi-camera video capture, where maintaining sharp focus across a large capture volume for a moving subject is difficult.},
  archive      = {J_TOG},
  author       = {Stanislav Pidhorskyi and Timur Bagautdinov and Shugao Ma and Jason Saragih and Gabriel Schwartz and Yaser Sheikh and Tomas Simon},
  doi          = {10.1145/3550454.3555521},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {190:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Depth of field aware differentiable rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable rendering using RGBXY derivatives and optimal
transport. <em>TOG</em>, <em>41</em>(6), 189:1–13. (<a
href="https://doi.org/10.1145/3550454.3555479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional differentiable rendering approaches are usually hard to converge in inverse rendering optimizations, especially when initial and target object locations are not so close. Inspired by Lagrangian fluid simulation, we present a novel differentiable rendering method to address this problem. We associate each screen-space pixel with the visible 3D geometric point covered by the center of the pixel and compute derivatives on geometric points rather than on pixels. We refer to the associated geometric points as point proxies of pixels. For each point proxy, we compute its 5D RGBXY derivatives which measures how its 3D RGB color and 2D projected screen-space position change with respect to scene parameters. Furthermore, in order to capture global and long-range object motions, we utilize optimal transport based pixel matching to design a more sophisticated loss function. We have conducted experiments to evaluate the effectiveness of our proposed method on various inverse rendering applications and have demonstrated superior convergence behavior compared to state-of-the-art baselines.},
  archive      = {J_TOG},
  author       = {Jiankai Xing and Fujun Luan and Ling-Qi Yan and Xuejun Hu and Houde Qian and Kun Xu},
  doi          = {10.1145/3550454.3555479},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {189:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable rendering using RGBXY derivatives and optimal transport},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing medial axis transform with feature preservation
via restricted power diagram. <em>TOG</em>, <em>41</em>(6), 188:1–18.
(<a href="https://doi.org/10.1145/3550454.3555465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for computing the medial axis transform of 3D shapes while preserving their medial features via restricted power diagram (RPD). Medial features, including external features such as the sharp edges and corners of the input mesh surface and internal features such as the seams and junctions of medial axis, are important shape descriptors both topologically and geometrically. However, existing medial axis approximation methods fail to capture and preserve them due to the fundamentally under-sampling in the vicinity of medial features, and the difficulty to build their correct connections. In this paper we use the RPD of medial spheres and its affiliated structures to help solve these challenges. The dual structure of RPD provides the connectivity of medial spheres. The surfacic restricted power cell (RPC) of each medial sphere provides the tangential surface regions that these spheres have contact with. The connected components (CC) of surfacic RPC give us the classification of each sphere, to be on a medial sheet, a seam, or a junction. They allow us to detect insufficient sphere sampling around medial features and develop necessary conditions to preserve them. Using this RPD-based framework, we are able to construct high quality medial meshes with features preserved. Compared with existing sampling-based or voxel-based methods, our method is the first one that can preserve not only external features but also internal features of medial axes.},
  archive      = {J_TOG},
  author       = {Ningna Wang and Bin Wang and Wenping Wang and Xiaohu Guo},
  doi          = {10.1145/3550454.3555465},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {188:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computing medial axis transform with feature preservation via restricted power diagram},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3QNet: 3D point cloud geometry quantization compression
network. <em>TOG</em>, <em>41</em>(6), 187:1–13. (<a
href="https://doi.org/10.1145/3550454.3555481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the development of 3D applications, the point cloud, as a spatial description easily acquired by sensors, has been widely used in multiple areas such as SLAM and 3D reconstruction. Point Cloud Compression (PCC) has also attracted more attention as a primary step before point cloud transferring and saving, where the geometry compression is an important component of PCC to compress the points geometrical structures. However, existing non-learning-based geometry compression methods are often limited by manually pre-defined compression rules. Though learning-based compression methods can significantly improve the algorithm performances by learning compression rules from data, they still have some defects. Voxel-based compression networks introduce precision errors due to the voxelized operations, while point-based methods may have relatively weak robustness and are mainly designed for sparse point clouds. In this work, we propose a novel learning-based point cloud compression framework named 3D Point Cloud Geometry Quantiation Compression Network (3QNet), which overcomes the robustness limitation of existing point-based methods and can handle dense points. By learning a codebook including common structural features from simple and sparse shapes, 3QNet can efficiently deal with multiple kinds of point clouds. According to experiments on object models, indoor scenes, and outdoor scans, 3QNet can achieve better compression performances than many representative methods.},
  archive      = {J_TOG},
  author       = {Tianxin Huang and Jiangning Zhang and Jun Chen and Zhonggan Ding and Ying Tai and Zhenyu Zhang and Chengjie Wang and Yong Liu},
  doi          = {10.1145/3550454.3555481},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {187:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {3QNet: 3D point cloud geometry quantization compression network},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SHRED: 3D shape region decomposition with learned local
operations. <em>TOG</em>, <em>41</em>(6), 186:1–11. (<a
href="https://doi.org/10.1145/3550454.3555440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a 3D point cloud as input and uses learned local operations to produce a segmentation that approximates fine-grained part instances. We endow SHRED with three decomposition operations: splitting regions, fixing the boundaries between regions, and merging regions together. Modules are trained independently and locally, allowing SHRED to generate high-quality segmentations for categories not seen during training. We train and evaluate SHRED with fine-grained segmentations from PartNet; using its merge-threshold hyperparameter, we show that SHRED produces segmentations that better respect ground-truth annotations compared with baseline methods, at any desired decomposition granularity. Finally, we demonstrate that SHRED is useful for downstream applications, out-performing all baselines on zero-shot fine-grained part instance segmentation and few-shot finegrained semantic segmentation when combined with methods that learn to label shape regions.},
  archive      = {J_TOG},
  author       = {R. Kenny Jones and Aalia Habib and Daniel Ritchie},
  doi          = {10.1145/3550454.3555440},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {186:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {SHRED: 3D shape region decomposition with learned local operations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SurfaceVoronoi: Efficiently computing voronoi diagrams over
mesh surfaces with arbitrary distance solvers. <em>TOG</em>,
<em>41</em>(6), 185:1–12. (<a
href="https://doi.org/10.1145/3550454.3555453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to compute Voronoi diagrams over mesh surfaces driven by an arbitrary geodesic distance solver, assuming that the input is a triangle mesh as well as a collection of sites P = { Pi } m i =1 on the surface. We propose two key techniques to solve this problem. First, as the partition is determined by minimizing the m distance fields, each of which rooted at a source site, we suggest keeping one or more distance triples, for each triangle, that may help determine the Voronoi bisectors when one uses a mark-and-sweep geodesic algorithm to predict the multi-source distance field. Second, rather than keep the distance itself at a mesh vertex, we use the squared distance to characterize the linear change of distance field restricted in a triangle, which is proved to induce an exact VD when the base surface reduces to a planar triangle mesh. Specially, our algorithm also supports the Euclidean distance, which can handle thin-sheet models (e.g. leaf) and runs faster than the traditional restricted Voronoi diagram (RVD) algorithm. It is very extensible to deal with various variants of surface-based Voronoi diagrams including (1) surface-based power diagram, (2) constrained Voronoi diagram with curve-type breaklines, and (3) curve-type generators. We conduct extensive experimental results to validate the ability to approximate the exact VD in different distance-driven scenarios.},
  archive      = {J_TOG},
  author       = {Shiqing Xin and Pengfei Wang and Rui Xu and Dongming Yan and Shuangmin Chen and Wenping Wang and Caiming Zhang and Changhe Tu},
  doi          = {10.1145/3550454.3555453},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {185:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {SurfaceVoronoi: Efficiently computing voronoi diagrams over mesh surfaces with arbitrary distance solvers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion in-betweening via two-stage transformers.
<em>TOG</em>, <em>41</em>(6), 184:1–16. (<a
href="https://doi.org/10.1145/3550454.3555454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep learning-based framework to synthesize motion in-betweening in a two-stage manner. Given some context frames and a target frame, the system can generate plausible transitions with variable lengths in a non-autoregressive fashion. The framework consists of two Transformer Encoder-based networks operating in two stages: in the first stage a Context Transformer is designed to generate rough transitions based on the context and in the second stage a Detail Transformer is employed to refine motion details. Compared to existing Transformer-based methods which either use a complete Transformer Encoder-Decoder architecture or additional 1D convolutions to generate motion transitions, our framework achieves superior performance with less trainable parameters by only leveraging the Transformer Encoder and masked self-attention mechanism. To enhance the generalization of our transformer-based framework, we further introduce Keyframe Positional Encoding and Learned Relative Positional Encoding to make our method robust in synthesizing longer transitions exceeding the maximum transition length during training. Our framework is also artist-friendly by supporting full and partial pose constraints within the transition, giving artists fine control over the synthesized results. We benchmark our framework on the LAFAN1 dataset, and experiments show that our method outperforms the current state-of-the-art methods at a large margin (an average of 16\% for normal-length sequences and 55\% for excessive-length sequences). Our method trains faster than the RNN-based method and achieves a four-time speedup during inference. We implement our framework into a production-ready tool inside an animation authoring software and conduct a pilot study to validate the practical value of our method.},
  archive      = {J_TOG},
  author       = {Jia Qin and Youyi Zheng and Kun Zhou},
  doi          = {10.1145/3550454.3555454},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {184:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Motion in-betweening via two-stage transformers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ControlVAE: Model-based learning of generative controllers
for physics-based characters. <em>TOG</em>, <em>41</em>(6), 183:1–16.
(<a href="https://doi.org/10.1145/3550454.3555434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce ControlVAE, a novel model-based framework for learning generative motion control policies based on variational autoencoders (VAE). Our framework can learn a rich and flexible latent representation of skills and a skill-conditioned generative control policy from a diverse set of unorganized motion sequences, which enables the generation of realistic human behaviors by sampling in the latent space and allows high-level control policies to reuse the learned skills to accomplish a variety of downstream tasks. In the training of ControlVAE, we employ a learnable world model to realize direct supervision of the latent space and the control policy. This world model effectively captures the unknown dynamics of the simulation system, enabling efficient model-based learning of high-level downstream tasks. We also learn a state-conditional prior distribution in the VAE-based generative control policy, which generates a skill embedding that outperforms the non-conditional priors in downstream tasks. We demonstrate the effectiveness of ControlVAE using a diverse set of tasks, which allows realistic and interactive control of the simulated characters.},
  archive      = {J_TOG},
  author       = {Heyuan Yao and Zhenhua Song and Baoquan Chen and Libin Liu},
  doi          = {10.1145/3550454.3555434},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {183:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ControlVAE: Model-based learning of generative controllers for physics-based characters},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning virtual chimeras by dynamic motion reassembly.
<em>TOG</em>, <em>41</em>(6), 182:1–13. (<a
href="https://doi.org/10.1145/3550454.3555489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Chimera is a mythological hybrid creature composed of different animal parts. The chimera&#39;s movements are highly dependent on the spatial and temporal alignments of its composing parts. In this paper, we present a novel algorithm that creates and animates chimeras by dynamically reassembling source characters and their movements. Our algorithm exploits a two-network architecture: part assembler and dynamic controller. The part assembler is a supervised learning layer that searches for the spatial alignment among body parts, assuming that the temporal alignment is provided. The dynamic controller is a reinforcement learning layer that learns robust control policy for a wide variety of potential temporal alignments. These two layers are tightly intertwined and learned simultaneously. The chimera animation generated by our algorithm is energy efficient and expressive in terms of describing weight shifting, balancing, and full-body coordination. We demonstrate the versatility of our algorithm by generating the motor skills of a large variety of chimeras from limited source characters.},
  archive      = {J_TOG},
  author       = {Seyoung Lee and Jiye Lee and Jehee Lee},
  doi          = {10.1145/3550454.3555489},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {182:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning virtual chimeras by dynamic motion reassembly},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sparse non-parametric BRDF model. <em>TOG</em>,
<em>41</em>(5), 181:1–18. (<a
href="https://doi.org/10.1145/3533427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel sparse non-parametric Bidirectional Reflectance Distribution Function (BRDF) model derived using a machine learning approach to represent the space of possible BRDFs using a set of multidimensional sub-spaces, or dictionaries. By training the dictionaries under a sparsity constraint, the model guarantees high-quality representations with minimal storage requirements and an inherent clustering of the BDRF-space. The model can be trained once and then reused to represent a wide variety of measured BRDFs. Moreover, the proposed method is flexible to incorporate new unobserved data sets, parameterizations, and transformations. In addition, we show that any two, or more, BRDFs can be smoothly interpolated in the coefficient space of the model rather than the significantly higher-dimensional BRDF space. The proposed sparse BRDF model is evaluated using the MERL, DTU, and RGL-EPFL BRDF databases. Experimental results show that the proposed approach results in about 9.75dB higher signal-to-noise ratio on average for rendered images as compared to current state-of-the-art models.},
  archive      = {J_TOG},
  author       = {Tanaboon Tongbuasirilai and Jonas Unger and Christine Guillemot and Ehsan Miandji},
  doi          = {10.1145/3533427},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {181:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {A sparse non-parametric BRDF model},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted delaunay triangulation for explicit surface
reconstruction. <em>TOG</em>, <em>41</em>(5), 180:1–20. (<a
href="https://doi.org/10.1145/3533768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of explicit surface reconstruction is to generate a surface mesh by interpolating a given point cloud. Explicit surface reconstruction is necessary when the point cloud is required to appear exactly on the surface. However, for a non-perfect input, such as lack of normals, low density, irregular distribution, thin and tiny parts, and high genus, a robust explicit reconstruction method that can generate a high-quality manifold triangulation is missing. We propose a robust explicit surface reconstruction method that starts from an initial simple surface mesh, alternately performs a Filmsticking step and a Sculpting step of the initial mesh, and converges when the surface mesh interpolates all input points (except outliers) and remains stable. The Filmsticking is to minimize the geometric distance between the surface mesh and the point cloud through iteratively performing a restricted Voronoi diagram technique on the surface mesh, whereas the Sculpting is to bootstrap the Filmsticking iteration from local minima by applying appropriate geometric and topological changes of the surface mesh. Our algorithm is fully automatic and produces high-quality surface meshes for non-perfect inputs that are typically considered to be challenging for prior state of the art. We conducted extensive experiments on simulated scans and real scans to validate the effectiveness of our approach.},
  archive      = {J_TOG},
  author       = {Pengfei Wang and Zixiong Wang and Shiqing Xin and Xifeng Gao and Wenping Wang and Changhe Tu},
  doi          = {10.1145/3533768},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {180:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Restricted delaunay triangulation for explicit surface reconstruction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StyleFusion: Disentangling spatial segments in
StyleGAN-generated images. <em>TOG</em>, <em>41</em>(5), 179:1–15. (<a
href="https://doi.org/10.1145/3527168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present StyleFusion , a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion allows one to perform semantically-aware cross-image mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the user’s region of interest. Code is available at: https://github.com/OmerKafri/StyleFusion .},
  archive      = {J_TOG},
  author       = {Omer Kafri and Or Patashnik and Yuval Alaluf and Daniel Cohen-Or},
  doi          = {10.1145/3527168},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {179:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleFusion: Disentangling spatial segments in StyleGAN-generated images},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hogel-free holography. <em>TOG</em>, <em>41</em>(5),
178:1–16. (<a href="https://doi.org/10.1145/3516428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography is a promising avenue for high-quality displays without requiring bulky, complex optical systems. While recent work has demonstrated accurate hologram generation of 2D scenes, high-quality holographic projections of 3D scenes has been out of reach until now. Existing multiplane 3D holography approaches fail to model wavefronts in the presence of partial occlusion while holographic stereogram methods have to make a fundamental tradeoff between spatial and angular resolution. In addition, existing 3D holographic display methods rely on heuristic encoding of complex amplitude into phase-only pixels which results in holograms with severe artifacts. Fundamental limitations of the input representation, wavefront modeling, and optimization methods prohibit artifact-free 3D holographic projections in today’s displays. To lift these limitations, we introduce hogel-free holography which optimizes for true 3D holograms, supporting both depth- and view-dependent effects for the first time. Our approach overcomes the fundamental spatio-angular resolution tradeoff typical to stereogram approaches. Moreover, it avoids heuristic encoding schemes to achieve high image fidelity over a 3D volume. We validate that the proposed method achieves 10 dB PSNR improvement on simulated holographic reconstructions. We also validate our approach on an experimental prototype with accurate parallax and depth focus effects.},
  archive      = {J_TOG},
  author       = {Praneeth Chakravarthula and Ethan Tseng and Henry Fuchs and Felix Heide},
  doi          = {10.1145/3516428},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {178:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hogel-free holography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OptiTrap: Optimal trap trajectories for acoustic levitation
displays. <em>TOG</em>, <em>41</em>(5), 173:1–14. (<a
href="https://doi.org/10.1145/3517746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic levitation has recently demonstrated the ability to create volumetric content by trapping and quickly moving particles along reference paths to reveal shapes in mid-air. However, the problem of specifying physically feasible trap trajectories to display desired shapes remains unsolved. Even if only the final shape is of interest to the content creator, the trap trajectories need to determine where and when the traps need to be, for the particle to reveal the intended shape. We propose OptiTrap , the first structured numerical approach to compute trap trajectories for acoustic levitation displays. Our approach generates trap trajectories that are physically feasible and nearly time-optimal, and reveal generic mid-air shapes, given only a reference path (i.e., a shape with no time information). We provide a multi-dimensional model of the acoustic forces around a trap to model the trap-particle system dynamics and compute optimal trap trajectories by formulating and solving a non-linear path following problem. We formulate our approach and evaluate it, demonstrating how OptiTrap consistently produces feasible and nearly optimal paths, with increases in size, frequency, and accuracy of the shapes rendered, allowing us to demonstrate larger and more complex shapes than ever shown to date.},
  archive      = {J_TOG},
  author       = {Viktorija Paneva and Arthur Fleig and Diego MartíNez Plasencia and Timm Faulwasser and Jörg Müller},
  doi          = {10.1145/3517746},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {173:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {OptiTrap: Optimal trap trajectories for acoustic levitation displays},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational mirror cup and saucer art. <em>TOG</em>,
<em>41</em>(5), 174:1–15. (<a
href="https://doi.org/10.1145/3517120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mirror cup and saucer art created by artists Yul Cho and Sang-Ha Cho, part of the saucer is directly visible to the viewer, while the other part of the saucer is occluded and can only be seen as a reflection through a mirror cup. Thus, viewers see an image directly on the saucer and another image on the mirror cup; however, the existing art design is limited to wavelike saucers. In this work, we propose a general computational framework for mirror cup and saucer art design. As input, we take from the user one image for the direct view, one image for the reflected view, and the base shape of the saucer. Our algorithm then generates a suitable saucer shape by deforming the input shape. We formulate this problem as a constrained optimization for the saucer surface. Our framework solves for the fine geometry details on the base shape along with its texture, such that when a mirror cup is placed on the saucer, the user-specified images are observed as direct and reflected views. Through extensive experiments, we demonstrate the effectiveness of our framework and the great design flexibility that it offers to users. We further validate the produced art pieces by fabricating the colored saucers using three-dimensional printing.},
  archive      = {J_TOG},
  author       = {Kang Wu and Renjie Chen and Xiao-Ming Fu and Ligang Liu},
  doi          = {10.1145/3517120},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {174:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational mirror cup and saucer art},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulating brittle fracture with material points.
<em>TOG</em>, <em>41</em>(5), 177:1–20. (<a
href="https://doi.org/10.1145/3522573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale topological changes play a key role in capturing the fine debris of fracturing virtual brittle material. Real-world, tough brittle fractures have dynamic branching behaviour but numerical simulation of this phenomena is notoriously challenging. In order to robustly capture these visual characteristics, we simulate brittle fracture by combining elastodynamic continuum mechanical models with rigid-body methods: A continuum damage mechanics (CDM) problem is solved, following rigid-body impact, to simulate crack propagation by tracking a damage field. We combine the result of this elastostatic continuum model with a novel technique to approximate cracks as a non-manifold mid-surface, which enables accurate and robust modelling of material fragment volumes to compliment fast-and-rigid shatter effects. For enhanced realism, we add fracture detail, incorporating particle damage-time to inform localised perturbation of the crack surface with artistic control. We evaluate our method with numerous examples and comparisons, showing that it produces a breadth of brittle material fracture effects and with low simulation resolution to require much less time compared to fully elastodynamic simulations.},
  archive      = {J_TOG},
  author       = {Linxu Fan and Floyd M. Chitalu and Taku Komura},
  doi          = {10.1145/3522573},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {177:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simulating brittle fracture with material points},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics-based combustion simulation. <em>TOG</em>,
<em>41</em>(5), 176:1–21. (<a
href="https://doi.org/10.1145/3526213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a physics-based combustion simulation method for computer graphics that extends the mathematical models of previous efforts to automatically capture more realistic flames as well as temperature and soot distributions. Our method includes mathematical models for the thermodynamic properties of real-world fuels which enables, for example, the prediction of adiabatic flame temperatures. We couple this with a model of heat transfer that includes convection, conduction as well as both radiative cooling and heating. This facilitates among other things ignition at a distance without heating up the intermediate air. We model the combustion as infinitely fast chemistry and couple this with the thin flame model, spatially varying laminar burning velocities based on local species and empirical measurements, physically validated soot formation and oxidation as well as water vapor production and condensation. We implement this on adaptive octree-like grids with collocated state variables, a new SBDF2-derived semi-Lagrangian time integrator for velocity, and a multigrid scheme used for multiple solver components. In combination, these models enable us to simulate deflagration phenomena ranging from small scale premixed and diffusion flames to fireballs and subsonic explosions which we demonstrate by several examples. In addition, we validate several of the results based on reference footage and measurements and discuss the relation of prevalent heuristic techniques arising in visual effects production to some of the physics-based models we propose.},
  archive      = {J_TOG},
  author       = {Michael B. Nielsen and Morten Bojsen-Hansen and Konstantinos Stamatelos and Robert Bridson},
  doi          = {10.1145/3526213},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {176:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physics-based combustion simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparsity-specific code optimization using expression trees.
<em>TOG</em>, <em>41</em>(5), 175:1–19. (<a
href="https://doi.org/10.1145/3520484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a code generator that converts unoptimized C++ code operating on sparse data into vectorized and parallel CPU or GPU kernels. Our approach unrolls the computation into a massive expression graph, performs redundant expression elimination, grouping, and then generates an architecture-specific kernel to solve the same problem, assuming that the sparsity pattern is fixed, which is a common scenario in many applications in computer graphics and scientific computing. We show that our approach scales to large problems and can achieve speedups of two orders of magnitude on CPUs and three orders of magnitude on GPUs, compared to a set of manually optimized CPU baselines. To demonstrate the practical applicability of our approach, we employ it to optimize popular algorithms with applications to physical simulation and interactive mesh deformation.},
  archive      = {J_TOG},
  author       = {Philipp Herholz and Xuan Tang and Teseo Schneider and Shoaib Kamil and Daniele Panozzo and Olga Sorkine-Hornung},
  doi          = {10.1145/3520484},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {175:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sparsity-specific code optimization using expression trees},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Searching for fast demosaicking algorithms. <em>TOG</em>,
<em>41</em>(5), 172:1–18. (<a
href="https://doi.org/10.1145/3508461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to automatically synthesize efficient, high-quality demosaicking algorithms, across a range of computational budgets, given a loss function and training data. It performs a multi-objective, discrete-continuous optimization which simultaneously solves for the program structure and parameters that best tradeoff computational cost and image quality. We design the method to exploit domain-specific structure for search efficiency. We apply it to several tasks, including demosaicking both Bayer and Fuji X-Trans color filter patterns, as well as joint demosaicking and super-resolution. In a few days on 8 GPUs, it produces a family of algorithms that significantly improves image quality relative to the prior state-of-the-art across a range of computational budgets from 10 s to 1000 s of operations per pixel (1 dB–3 dB higher quality at the same cost, or 8.5–200× higher throughput at same or better quality). The resulting programs combine features of both classical and deep learning-based demosaicking algorithms into more efficient hybrid combinations, which are bandwidth-efficient and vectorizable by construction. Finally, our method automatically schedules and compiles all generated programs into optimized SIMD code for modern processors.},
  archive      = {J_TOG},
  author       = {Karima Ma and Michael Gharbi and Andrew Adams and Shoaib Kamil and Tzu-Mao Li and Connelly Barnes and Jonathan Ragan-Kelley},
  doi          = {10.1145/3508461},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {172:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Searching for fast demosaicking algorithms},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active exploration for neural global illumination of
variable scenes. <em>TOG</em>, <em>41</em>(5), 171:1–18. (<a
href="https://doi.org/10.1145/3522735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural rendering algorithms introduce a fundamentally new approach for photorealistic rendering, typically by learning a neural representation of illumination on large numbers of ground truth images. When training for a given variable scene, such as changing objects, materials, lights, and viewpoint, the space \( \mathcal {D} \) of possible training data instances quickly becomes unmanageable as the dimensions of variable parameters increase. We introduce a novel Active Exploration method using Markov Chain Monte Carlo, which explores \( \mathcal {D} \), generating samples (i.e., ground truth renderings) that best help training and interleaves training and on-the-fly sample data generation. We introduce a self-tuning sample reuse strategy to minimize the expensive step of rendering training samples. We apply our approach on a neural generator that learns to render novel scene instances given an explicit parameterization of the scene configuration. Our results show that Active Exploration trains our network much more efficiently than uniformly sampling and, together with our resolution enhancement approach, achieves better quality than uniform sampling at convergence. Our method allows interactive rendering of hard light transport paths (e.g., complex caustics), which require very high samples counts to be captured, and provides dynamic scene navigation and manipulation, after training for 5 to 18 hours depending on required quality and variations.},
  archive      = {J_TOG},
  author       = {Stavros Diolatzis and Julien Philip and George Drettakis},
  doi          = {10.1145/3522735},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {171:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Active exploration for neural global illumination of variable scenes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthesis of frame field-aligned multi-laminar structures.
<em>TOG</em>, <em>41</em>(5), 170:1–20. (<a
href="https://doi.org/10.1145/3516522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of topology optimization, the homogenization approach has been revived as an important alternative to the established, density-based methods. Homogenization can represent microstructures at length scales decoupled from the resolution of the computational grid. The optimal microstructure for a single load case is an orthogonal rank-3 laminate. Initially, we investigate where singularities occur in orthogonal rank-3 laminates and show that the laminar parts of the structures we seek are unaffected by the singularities. Based on this observation, we propose a method for generating multi-laminar structures from frame fields that describe rank-3 laminates. Rather than establishing a parametrization of the domain, we compute stream surfaces that align with the frame fields and solve an optimization problem to find a well-spaced collection of such stream surfaces. Since our method does not rely on a parametrization, we also do not need a combing of the frame fields to generate this collection. Finally, we provide a method for synthesizing multi-laminar structures from a stream surface collection. This method produces a volumetric solid for each surface and combines these to form the output. We demonstrate our method on several frame fields produced by the homogenization approach to topology optimization.},
  archive      = {J_TOG},
  author       = {Florian Cyril Stutz and Tim Felle Olsen and Jeroen Peter Groen and Tuan Nguyen Trung and Niels Aage and Ole Sigmund and Justin Solomon and Jakob Andreas Bærentzen},
  doi          = {10.1145/3516522},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {170:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Synthesis of frame field-aligned multi-laminar structures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient b-spline lagrangian/eulerian method for
compressible flow, shock waves, and fracturing solids. <em>TOG</em>,
<em>41</em>(5), 169:1–13. (<a
href="https://doi.org/10.1145/3519595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new method for modeling the interaction between compressible flow, shock waves, and deformable structures, emphasizing destructive dynamics. Extending advances in time-splitting compressible flow and the Material Point Methods (MPM), we develop a hybrid Eulerian and Lagrangian/Eulerian scheme for monolithic flow-structure interactions. We adopt the second-order WENO scheme to advance the continuity equation. To stably resolve deforming boundaries with sub-cell particles, we propose a blending treatment of reflective and passable boundary conditions inspired by the theory of porous media. The strongly coupled velocity-pressure system is discretized with a new mixed-order finite element formulation employing B-spline shape functions. Shock wave propagation, temperature/density-induced buoyancy effects, and topology changes in solids are unitedly captured.},
  archive      = {J_TOG},
  author       = {Yadi Cao and Yunuo Chen and Minchen Li and Yin Yang and Xinxin Zhang and Mridul Aanjaneya and Chenfanfu Jiang},
  doi          = {10.1145/3519595},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {169:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {An efficient B-spline Lagrangian/Eulerian method for compressible flow, shock waves, and fracturing solids},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local anatomically-constrained facial performance
retargeting. <em>TOG</em>, <em>41</em>(4), 168:1–14. (<a
href="https://doi.org/10.1145/3528223.3530114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating realistic facial animation for CG characters and digital doubles is one of the hardest tasks in animation. A typical production workflow involves capturing the performance of a real actor using mo-cap technology, and transferring the captured motion to the target digital character. This process, known as retargeting , has been used for over a decade, and typically relies on either large blendshape rigs that are expensive to create, or direct deformation transfer algorithms that operate on individual geometric elements and are prone to artifacts. We present a new method for high-fidelity offline facial performance retargeting that is neither expensive nor artifact-prone. Our two step method first transfers local expression details to the target, and is followed by a global face surface prediction that uses anatomical constraints in order to stay in the feasible shape space of the target character. Our method also offers artists with familiar blendshape controls to perform fine adjustments to the retargeted animation. As such, our method is ideally suited for the complex task of human-to-human 3D facial performance retargeting, where the quality bar is extremely high in order to avoid the uncanny valley, while also being applicable for more common human-to-creature settings. We demonstrate the superior performance of our method over traditional deformation transfer algorithms, while achieving a quality comparable to current blendshape-based techniques used in production while requiring significantly fewer input shapes at setup time. A detailed user study corroborates the realistic and artifact free animations generated by our method in comparison to existing techniques.},
  archive      = {J_TOG},
  author       = {Prashanth Chandran and Loïc Ciccone and Markus Gross and Derek Bradley},
  doi          = {10.1145/3528223.3530114},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {168:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Local anatomically-constrained facial performance retargeting},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepFaceVideoEditing: Sketch-based deep editing of face
videos. <em>TOG</em>, <em>41</em>(4), 167:1–16. (<a
href="https://doi.org/10.1145/3528223.3530056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches, which are simple and concise, have been used in recent deep image synthesis methods to allow intuitive generation and editing of facial images. However, it is nontrivial to extend such methods to video editing due to various challenges, ranging from appropriate manipulation propagation and fusion of multiple editing operations to ensure temporal coherence and visual quality. To address these issues, we propose a novel sketch-based facial video editing framework, in which we represent editing manipulations in latent space and propose specific propagation and fusion modules to generate high-quality video editing results based on StyleGAN3. Specifically, we first design an optimization approach to represent sketch editing manipulations by editing vectors, which are propagated to the whole video sequence using a proper strategy to cope with different editing needs. Specifically, input editing operations are classified into two categories: temporally consistent editing and temporally variant editing. The former (e.g., change of face shape) is applied to the whole video sequence directly, while the latter (e.g., change of facial expression or dynamics) is propagated with the guidance of expression or only affects adjacent frames in a given time window. Since users often perform different editing operations in multiple frames, we further present a region-aware fusion approach to fuse diverse editing effects. Our method supports video editing on facial structure and expression movement by sketch, which cannot be achieved by previous works. Both qualitative and quantitative evaluations show the superior editing ability of our system to existing and alternative solutions.},
  archive      = {J_TOG},
  author       = {Feng-Lin Liu and Shu-Yu Chen and Yu-Kun Lai and Chunpeng Li and Yue-Ren Jiang and Hongbo Fu and Lin Gao},
  doi          = {10.1145/3528223.3530056},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {167:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepFaceVideoEditing: Sketch-based deep editing of face videos},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EyeNeRF: A hybrid representation for photorealistic
synthesis, animation and relighting of human eyes. <em>TOG</em>,
<em>41</em>(4), 166:1–16. (<a
href="https://doi.org/10.1145/3528223.3530130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A unique challenge in creating high-quality animatable and relightable 3D avatars of real people is modeling human eyes, particularly in conjunction with the surrounding periocular face region. The challenge of synthesizing eyes is multifold as it requires 1) appropriate representations for the various components of the eye and the periocular region for coherent viewpoint synthesis, capable of representing diffuse, refractive and highly reflective surfaces, 2) disentangling skin and eye appearance from environmental illumination such that it may be rendered under novel lighting conditions, and 3) capturing eyeball motion and the deformation of the surrounding skin to enable re-gazing. These challenges have traditionally necessitated the use of expensive and cumbersome capture setups to obtain high-quality results, and even then, modeling of the full eye region holistically has remained elusive. We present a novel geometry and appearance representation that enables high-fidelity capture and photorealistic animation, view synthesis and relighting of the eye region using only a sparse set of lights and cameras. Our hybrid representation combines an explicit parametric surface model for the eyeball surface with implicit deformable volumetric representations for the periocular region and the interior of the eye. This novel hybrid model has been designed specifically to address the various parts of that exceptionally challenging facial area - the explicit eyeball surface allows modeling refraction and high frequency specular reflection at the cornea, whereas the implicit representation is well suited to model lower frequency skin reflection via spherical harmonics and can represent non-surface structures such as hair (i.e. eyebrows) or highly diffuse volumetric bodies (i.e. sclera), both of which are a challenge for explicit surface models. Tightly integrating the two representations in a joint framework allows controlled photoreal image synthesis and joint optimization of both the geometry parameters of the eyeball and the implicit neural network in continuous 3D space. We show that for high-resolution close-ups of the human eye, our model can synthesize high-fidelity animated gaze from novel views under unseen illumination conditions, allowing to generate visually rich eye imagery.},
  archive      = {J_TOG},
  author       = {Gengyan Li and Abhimitra Meka and Franziska Mueller and Marcel C. Buehler and Otmar Hilliges and Thabo Beeler},
  doi          = {10.1145/3528223.3530130},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {166:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {EyeNeRF: A hybrid representation for photorealistic synthesis, animation and relighting of human eyes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial hair tracking for high fidelity performance capture.
<em>TOG</em>, <em>41</em>(4), 165:1–12. (<a
href="https://doi.org/10.1145/3528223.3530116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial hair is a largely overlooked topic in facial performance capture. Most production pipelines in the entertainment industry do not have a way to automatically capture facial hair or track the skin underneath it. Thus, actors are asked to shave clean before face capture, which is very often undesirable. Capturing the geometry of individual facial hairs is very challenging, and their presence makes it harder to capture the deforming shape of the underlying skin surface. Some attempts have already been made at automating this task, but only for static faces with relatively sparse 3D hair reconstructions. In particular, current methods lack the temporal correspondence needed when capturing a sequence of video frames depicting facial performance. The problem of robustly tracking the skin underneath also remains unaddressed. In this paper, we propose the first multiview reconstruction pipeline that tracks both the dense 3D facial hair, as well as the underlying 3D skin for entire performances. Our method operates with standard setups for face photogrammetry, without requiring dense camera arrays. For a given capture subject, our algorithm first reconstructs a dense, high-quality neutral 3D facial hairstyle by registering sparser hair reconstructions over multiple frames that depict a neutral face under quasi-rigid motion. This custom-built, reference facial hairstyle is then tracked throughout a variety of changing facial expressions in a captured performance, and the result is used to constrain the tracking of the 3D skin surface underneath. We demonstrate the proposed capture pipeline on a variety of different facial hairstyles and lengths, ranging from sparse and short to dense full-beards.},
  archive      = {J_TOG},
  author       = {Sebastian Winberg and Gaspard Zoss and Prashanth Chandran and Paulo Gotardo and Derek Bradley},
  doi          = {10.1145/3528223.3530116},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {165:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Facial hair tracking for high fidelity performance capture},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artemis: Articulated neural pets with appearance and motion
synthesis. <em>TOG</em>, <em>41</em>(4), 164:1–19. (<a
href="https://doi.org/10.1145/3528223.3530086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We, humans, are entering into a virtual era and indeed want to bring animals to the virtual world as well for companion. Yet, computer-generated (CGI) furry animals are limited by tedious off-line rendering, let alone interactive motion control. In this paper, we present ARTEMIS, a novel neural modeling and rendering pipeline for generating ARTiculated neural pets with appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time animation, and photo-realistic rendering of furry animals. The core of our ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient octree-based representation for animal animation and fur rendering. The animation then becomes equivalent to voxel-level deformation based on explicit skeletal warping. We further use a fast octree indexing and efficient volumetric rendering scheme to generate appearance and density features maps. Finally, we propose a novel shading network to generate high-fidelity details of appearance and opacity under novel poses from appearance and density feature maps. For the motion control module in ARTEMIS, we combine state-of-the-art animal motion capture approach with recent neural character control scheme. We introduce an effective optimization scheme to reconstruct the skeletal motion of real animals captured by a multi-view RGB and Vicon camera array. We feed all the captured motion into a neural character control scheme to generate abstract control signals with motion styles. We further integrate ARTEMIS into existing engines that support VR headsets, providing an unprecedented immersive experience where a user can intimately interact with a variety of virtual animals with vivid movements and photo-realistic appearance. Extensive experiments and showcases demonstrate the effectiveness of our ARTEMIS system in achieving highly realistic rendering of NGI animals in real-time, providing daily immersive and interactive experiences with digital animals unseen before. We make available our ARTEMIS model and dynamic furry animal dataset at https://haiminluo.github.io/publication/artemis/.},
  archive      = {J_TOG},
  author       = {Haimin Luo and Teng Xu and Yuheng Jiang and Chenglin Zhou and Qiwei Qiu and Yingliang Zhang and Wei Yang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3528223.3530086},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {164:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Artemis: Articulated neural pets with appearance and motion synthesis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Authentic volumetric avatars from a phone scan.
<em>TOG</em>, <em>41</em>(4), 163:1–19. (<a
href="https://doi.org/10.1145/3528223.3530143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating photorealistic avatars of existing people currently requires extensive person-specific data capture, which is usually only accessible to the VFX industry and not the general public. Our work aims to address this drawback by relying only on a short mobile phone capture to obtain a drivable 3D head avatar that matches a person&#39;s likeness faithfully. In contrast to existing approaches, our architecture avoids the complex task of directly modeling the entire manifold of human appearance, aiming instead to generate an avatar model that can be specialized to novel identities using only small amounts of data. The model dispenses with low-dimensional latent spaces that are commonly employed for hallucinating novel identities, and instead, uses a conditional representation that can extract person-specific information at multiple scales from a high resolution registered neutral phone scan. We achieve high quality results through the use of a novel universal avatar prior that has been trained on high resolution multi-view video captures of facial performances of hundreds of human subjects. By fine-tuning the model using inverse rendering we achieve increased realism and personalize its range of motion. The output of our approach is not only a high-fidelity 3D head avatar that matches the person&#39;s facial shape and appearance, but one that can also be driven using a jointly discovered shared global expression space with disentangled controls for gaze direction. Via a series of experiments we demonstrate that our avatars are faithful representations of the subject&#39;s likeness. Compared to other state-of-the-art methods for lightweight avatar creation, our approach exhibits superior visual quality and animateability.},
  archive      = {J_TOG},
  author       = {Chen Cao and Tomas Simon and Jin Kyu Kim and Gabe Schwartz and Michael Zollhoefer and Shun-Suke Saito and Stephen Lombardi and Shih-En Wei and Danielle Belko and Shoou-I Yu and Yaser Sheikh and Jason Saragih},
  doi          = {10.1145/3528223.3530143},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {163:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Authentic volumetric avatars from a phone scan},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text2Human: Text-driven controllable human image generation.
<em>TOG</em>, <em>41</em>(4), 162:1–11. (<a
href="https://doi.org/10.1145/3528223.3530104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural representations for each type of texture. The codebook at the coarse level includes the structural representations of textures, while the codebook at the fine level focuses on the details of textures. To make use of the learned hierarchical codebook to synthesize desired images, a diffusion-based transformer sampler with mixture of experts is firstly employed to sample indices from the coarsest level of the codebook, which then is used to predict the indices of the codebook at finer levels. The predicted indices at different levels are translated to human images by the decoder learned accompanied with hierarchical codebooks. The use of mixture-of-experts allows for the generated image conditioned on the fine-grained text input. The prediction for finer level indices refines the quality of clothing textures. Extensive quantitative and qualitative evaluations demonstrate that our proposed Text2Human framework can generate more diverse and realistic human images compared to state-of-the-art methods. Our project page is https://yumingj.github.io/projects/Text2Human.html. Code and pretrained models are available at https://github.com/yumingj/Text2Human.},
  archive      = {J_TOG},
  author       = {Yuming Jiang and Shuai Yang and Haonan Qiu and Wayne Wu and Chen Change Loy and Ziwei Liu},
  doi          = {10.1145/3528223.3530104},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {162:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Text2Human: Text-driven controllable human image generation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AvatarCLIP: Zero-shot text-driven generation and animation
of 3D avatars. <em>TOG</em>, <em>41</em>(4), 161:1–19. (<a
href="https://doi.org/10.1145/3528223.3530094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D avatar creation plays a crucial role in the digital age. However, the whole production process is prohibitively time-consuming and labor-intensive. To democratize this technology to a larger audience, we propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages. Our key insight is to take advantage of the powerful vision-language model CLIP for supervising neural human generation, in terms of 3D geometry, texture and animation. Specifically, driven by natural language descriptions, we initialize 3D human geometry generation with a shape VAE network. Based on the generated 3D human shapes, a volume rendering model is utilized to further facilitate geometry sculpting and texture generation. Moreover, by leveraging the priors learned in the motion VAE, a CLIP-guided reference-based motion synthesis method is proposed for the animation of the generated 3D avatar. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of AvatarCLIP on a wide range of avatars. Remarkably, AvatarCLIP can generate unseen 3D avatars with novel animations, achieving superior zero-shot capability. Codes are available at https://github.com/hongfz16/AvatarCLIP.},
  archive      = {J_TOG},
  author       = {Fangzhou Hong and Mingyuan Zhang and Liang Pan and Zhongang Cai and Lei Yang and Ziwei Liu},
  doi          = {10.1145/3528223.3530094},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {161:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {AvatarCLIP: Zero-shot text-driven generation and animation of 3D avatars},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). As-locally-uniform-as-possible reshaping of vector clip-art.
<em>TOG</em>, <em>41</em>(4), 160:1–10. (<a
href="https://doi.org/10.1145/3528223.3530098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector clip-art images consist of regions bounded by a network of vector curves. Users often wish to reshape , or rescale, existing clip-art images by changing the locations, proportions, or scales of different image elements. When reshaping images depicting synthetic content they seek to preserve global and local structures. These structures are best preserved when the gradient of the mapping between the original and the reshaped curve networks is locally as close as possible to a uniform scale; mappings that satisfy this property maximally preserve the input curve orientations and minimally change the shape of the input&#39;s geometric details, while allowing changes in the relative scales of the different features. The expectation of approximate scale uniformity is local ; while reshaping operations are typically expected to change the relative proportions of a subset of network regions, users expect the change to be minimal away from the directly impacted regions and expect such changes to be gradual and distributed as evenly as possible. Unfortunately, existing methods for editing 2D curve networks do not satisfy these criteria. We propose a targeted As-Locally-Uniform-as-Possible (ALUP) vector clip-art reshaping method that satisfies the properties above. We formulate the computation of the desired output network as the solution of a constrained variational optimization problem. We effectively compute the desired solution by casting this continuous problem as a minimization of a non-linear discrete energy function, and obtain the desired minimizer by using a custom iterative solver. We validate our method via perceptual studies comparing our results to those created via algorithmic alternatives and manually generated ones. Participants preferred our results over the closest alternative by a ratio of 6 to 1.},
  archive      = {J_TOG},
  author       = {Chrystiano Araújo and Nicholas Vining and Enrique Rosales and Giorgio Gori and Alla Sheffer},
  doi          = {10.1145/3528223.3530098},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {160:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {As-locally-uniform-as-possible reshaping of vector clip-art},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustered vector textures. <em>TOG</em>, <em>41</em>(4),
159:1–23. (<a href="https://doi.org/10.1145/3528223.3530062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive vector patterns are common in a variety of applications but can be challenging and tedious to create. Existing automatic synthesis methods target relatively simple, unstructured patterns such as discrete elements and continuous Bézier curves. This paper proposes an algorithm for generating vector patterns with diverse shapes and structured local interactions via a sample-based representation. Our main idea is adding explicit clustering as part of neighborhood similarity and iterative sample optimization for more robust sample synthesis and pattern reconstruction. The results indicate that our method can outperform existing methods on synthesizing a variety of structured vector textures. Our project page is available at https://phtu-cs.github.io/cvt-sig22/.},
  archive      = {J_TOG},
  author       = {Peihan Tu and Li-Yi Wei and Matthias Zwicker},
  doi          = {10.1145/3528223.3530062},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {159:1–23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Clustered vector textures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuralTailor: Reconstructing sewing pattern structures from
3D point clouds of garments. <em>TOG</em>, <em>41</em>(4), 158:1–16. (<a
href="https://doi.org/10.1145/3528223.3530179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fields of SocialVR, performance capture, and virtual try-on are often faced with a need to faithfully reproduce real garments in the virtual world. One critical task is the disentanglement of the intrinsic garment shape from deformations due to fabric properties, physical forces, and contact with the body. We propose to use a garment sewing pattern, a realistic and compact garment descriptor, to facilitate the intrinsic garment shape estimation. Another major challenge is a high diversity of shapes and designs in the domain. The most common approach for Deep Learning on 3D garments is to build specialized models for individual garments or garment types. We argue that building a unified model for various garment designs has the benefit of generalization to novel garment types, hence covering a larger design domain than individual models would. We introduce NeuralTailor, a novel architecture based on point-level attention for set regression with variable cardinality, and apply it to the task of reconstructing 2D garment sewing patterns from the 3D point cloud garment models. Our experiments show that NeuralTailor successfully reconstructs sewing patterns and generalizes to garment types with pattern topologies unseen during training.},
  archive      = {J_TOG},
  author       = {Maria Korosteleva and Sung-Hee Lee},
  doi          = {10.1145/3528223.3530179},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {158:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralTailor: Reconstructing sewing pattern structures from 3D point clouds of garments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational pattern making from 3D garment models.
<em>TOG</em>, <em>41</em>(4), 157:1–14. (<a
href="https://doi.org/10.1145/3528223.3530145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for computing a sewing pattern of a given 3D garment model. Our algorithm segments an input 3D garment shape into patches and computes their 2D parameterization, resulting in pattern pieces that can be cut out of fabric and sewn together to manufacture the garment. Unlike the general state-of-the-art approaches for surface cutting and flattening, our method explicitly targets garment fabrication. It accounts for the unique properties and constraints of tailoring, such as seam symmetry, the usage of darts, fabric grain alignment, and a flattening distortion measure that models woven fabric deformation, respecting its anisotropic behavior. We bootstrap a recent patch layout approach developed for quadrilateral remeshing and adapt it to the purpose of computational pattern making, ensuring that the deformation of each pattern piece stays within prescribed bounds of cloth stress. While our algorithm can automatically produce the sewing patterns, it is fast enough to admit user input to creatively iterate on the pattern design. Our method can take several target poses of the 3D garment into account and integrate them into the sewing pattern design. We demonstrate results on both skintight and loose garments, showcasing the versatile application possibilities of our approach.},
  archive      = {J_TOG},
  author       = {Nico Pietroni and Corentin Dumery and Raphael Falque and Mark Liu and Teresa Vidal-Calleja and Olga Sorkine-Hornung},
  doi          = {10.1145/3528223.3530145},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {157:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational pattern making from 3D garment models},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified many-worlds browsing of arbitrary physics-based
animations. <em>TOG</em>, <em>41</em>(4), 156:1–15. (<a
href="https://doi.org/10.1145/3528223.3530082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually tuning physics-based animation parameters to explore a simulation outcome space or achieve desired motion outcomes can be notoriously tedious. This problem has motivated many sophisticated and specialized optimization-based methods for fine-grained (keyframe) control, each of which are typically limited to specific animation phenomena, usually complicated, and, unfortunately, not widely used. In this paper, we propose Unified Many-Worlds Browsing (UMWB), a practical method for sample-level control and exploration of physics-based animations. Our approach supports browsing of large simulation ensembles of arbitrary animation phenomena by using a unified volumetric WORLDPACK representation based on spatiotemporally compressed voxel data associated with geometric occupancy and other low-fidelity animation state. Beyond memory reduction, the WORLDPACK representation also enables unified query support for interactive browsing: it provides fast evaluation of approximate spatiotemporal queries, such as occupancy tests that find ensemble samples (&quot;worlds&quot;) where material is either IN or NOT IN a user-specified spacetime region. WORLDPACKS also support real-time hardware-accelerated voxel rendering by exploiting the spatially hierarchical and temporal RLE raster data structure. Our UMWB implementation supports interactive browsing (and offline refinement) of ensembles containing thousands of simulation samples, and fast spatiotemporal queries and ranking. We show UMWB results using a wide variety of physics-based animation phenomena---not just JELL-O ® .},
  archive      = {J_TOG},
  author       = {Purvi Goel and Doug L. James},
  doi          = {10.1145/3528223.3530082},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {156:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unified many-worlds browsing of arbitrary physics-based animations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ecoclimates: Climate-response modeling of vegetation.
<em>TOG</em>, <em>41</em>(4), 155:1–19. (<a
href="https://doi.org/10.1145/3528223.3530146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the greatest challenges to mankind is understanding the underlying principles of climate change. Over the last years, the role of forests in climate change has received increased attention. This is due to the observation that not only the atmosphere has a principal impact on vegetation growth but also that vegetation is contributing to local variations of weather resulting in diverse microclimates. The interconnection of plant ecosystems and weather is described and studied as ecoclimates. In this work we take steps towards simulating ecoclimates by modeling the feedback loops between vegetation, soil, and atmosphere. In contrast to existing methods that only describe the climate at a global scale, our model aims at simulating local variations of climate. Specifically, we model tree growth interactively in response to gradients of water, temperature and light. As a result, we are able to capture a range of ecoclimate phenomena that have not been modeled before, including geomorphic controls, forest edge effects, the Foehn effect and spatial vegetation patterning. To validate the plausibility of our method we conduct a comparative analysis to studies from ecology and climatology. Consequently, our method advances the state-of-the-art of generating highly realistic outdoor landscapes of vegetation.},
  archive      = {J_TOG},
  author       = {Wojtek Pałubicki and Miłosz Makowski and Weronika Gajda and Torsten Hädrich and Dominik L. Michels and Sören Pirk},
  doi          = {10.1145/3528223.3530146},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {155:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ecoclimates: Climate-response modeling of vegetation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A moving eulerian-lagrangian particle method for thin film
and foam simulation. <em>TOG</em>, <em>41</em>(4), 154:1–17. (<a
href="https://doi.org/10.1145/3528223.3530174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Moving Eulerian-Lagrangian Particles (MELP), a novel mesh-free method for simulating incompressible fluid on thin films and foams. Employing a bi-layer particle structure, MELP jointly simulates detailed, vigorous flow and large surface deformation at high stability and efficiency. In addition, we design multi-MELP: a mechanism that facilitates the physically-based interaction between multiple MELP systems, to simulate bubble clusters and foams with non-manifold topological evolution. We showcase the efficacy of our method with a broad range of challenging thin film phenomena, including the Rayleigh-Taylor instability across double-bubbles, foam fragmentation with rim surface tension, recovery of the Plateau borders, Newton black films, as well as cyclones on bubble clusters.},
  archive      = {J_TOG},
  author       = {Yitong Deng and Mengdi Wang and Xiangxin Kong and Shiying Xiong and Zangyueyang Xian and Bo Zhu},
  doi          = {10.1145/3528223.3530174},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {154:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A moving eulerian-lagrangian particle method for thin film and foam simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Filament based plasma. <em>TOG</em>, <em>41</em>(4),
153:1–14. (<a href="https://doi.org/10.1145/3528223.3530102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation of stellar atmospheres, such as that of our own sun, is a common task in CGI for scientific visualization, movies and games. A fibrous volumetric texture is a visually dominant feature of the solar corona---the plasma that extends from the solar surface into space. These coronal fibers can be modeled as magnetic filaments whose shape is governed by the magnetohydrostatic equation. The magnetic filaments provide a Lagrangian curve representation and their initial configuration can be prescribed by an artist or generated from magnetic flux given as a scalar texture on the sun&#39;s surface. Subsequently, the shape of the filaments is determined based on a variational formulation. The output is a visual rendering of the whole sun. We demonstrate the fidelity of our method by comparing the resulting renderings with actual images of our sun&#39;s corona.},
  archive      = {J_TOG},
  author       = {Marcel Padilla and Oliver Gross and Felix Knöppel and Albert Chern and Ulrich Pinkall and Peter Schröder},
  doi          = {10.1145/3528223.3530102},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {153:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Filament based plasma},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Umbrella meshes: Elastic mechanisms for freeform shape
deployment. <em>TOG</em>, <em>41</em>(4), 152:1–15. (<a
href="https://doi.org/10.1145/3528223.3530089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems. We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure&#39;s strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.},
  archive      = {J_TOG},
  author       = {Yingying Ren and Uday Kusupati and Julian Panetta and Florin Isvoranu and Davide Pellis and Tian Chen and Mark Pauly},
  doi          = {10.1145/3528223.3530089},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {152:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Umbrella meshes: Elastic mechanisms for freeform shape deployment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed integer neural inverse design. <em>TOG</em>,
<em>41</em>(4), 151:1–14. (<a
href="https://doi.org/10.1145/3528223.3530083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational design and fabrication, neural networks are becoming important surrogates for bulky forward simulations. A long-standing, intertwined question is that of inverse design: how to compute a design that satisfies a desired target performance? Here, we show that the piecewise linear property, very common in everyday neural networks, allows for an inverse design formulation based on mixed-integer linear programming. Our mixed-integer inverse design uncovers globally optimal or near optimal solutions in a principled manner. Furthermore, our method significantly facilitates emerging, but challenging, combinatorial inverse design tasks, such as material selection. For problems where finding the optimal solution is intractable, we develop an efficient yet near-optimal hybrid approach. Eventually, our method is able to find solutions provably robust to possible fabrication perturbations among multiple designs with similar performances. Our code and data are available at https://gitlab.mpi-klsb.mpg.de/nansari/mixed-integer-neural-inverse-design.},
  archive      = {J_TOG},
  author       = {Navid Ansari and Hans-Peter Seidel and Vahid Babaei},
  doi          = {10.1145/3528223.3530083},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {151:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mixed integer neural inverse design},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational design of high-level interlocking puzzles.
<em>TOG</em>, <em>41</em>(4), 150:1–15. (<a
href="https://doi.org/10.1145/3528223.3530071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interlocking puzzles are intriguing geometric games where the puzzle pieces are held together based on their geometric arrangement, preventing the puzzle from falling apart. High-level-of-difficulty , or simply high-level , interlocking puzzles are a subclass of interlocking puzzles that require multiple moves to take out the first subassembly from the puzzle. Solving a high-level interlocking puzzle is a challenging task since one has to explore many different configurations of the puzzle pieces until reaching a configuration where the first subassembly can be taken out. Designing a high-level interlocking puzzle with a user-specified level of difficulty is even harder since the puzzle pieces have to be interlocking in all the configurations before the first subassembly is taken out. In this paper, we present a computational approach to design high-level interlocking puzzles. The core idea is to represent all possible configurations of an interlocking puzzle as well as transitions among these configurations using a rooted, undirected graph called a disassembly graph and leverage this graph to find a disassembly plan that requires a minimal number of moves to take out the first subassembly from the puzzle. At the design stage, our algorithm iteratively constructs the geometry of each puzzle piece to expand the disassembly graph incrementally, aiming to achieve a user-specified level of difficulty. We show that our approach allows efficient generation of high-level interlocking puzzles of various shape complexities, including new solutions not attainable by state-of-the-art approaches.},
  archive      = {J_TOG},
  author       = {Rulin Chen and Ziqi Wang and Peng Song and Bernd Bickel},
  doi          = {10.1145/3528223.3530071},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {150:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of high-level interlocking puzzles},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational design of passive grippers. <em>TOG</em>,
<em>41</em>(4), 149:2–12. (<a
href="https://doi.org/10.1145/3528223.3530162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel generative design tool for passive grippers---robot end effectors that have no additional actuation and instead leverage the existing degrees of freedom in a robotic arm to perform grasping tasks. Passive grippers are used because they offer interesting trade-offs between cost and capabilities. However, existing designs are limited in the types of shapes that can be grasped. This work proposes to use rapid-manufacturing and design optimization to expand the space of shapes that can be passively grasped. Our novel generative design algorithm takes in an object and its positioning with respect to a robotic arm and generates a 3D printable passive gripper that can stably pick the object up. To achieve this, we address the key challenge of jointly optimizing the shape and the insert trajectory to ensure a passively stable grasp. We evaluate our method on a testing suite of 22 objects (23 experiments), all of which were evaluated with physical experiments to bridge the virtual-to-real gap. Code and data are at https://homes.cs.washington.edu/~milink/passive-gripper/},
  archive      = {J_TOG},
  author       = {Milin Kodnongbua and Ian Good and Yu Lou and Jeffrey Lipton and Adriana Schulz},
  doi          = {10.1145/3528223.3530162},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {149:2–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of passive grippers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face deblurring using dual camera fusion on mobile phones.
<em>TOG</em>, <em>41</em>(4), 148:1–16. (<a
href="https://doi.org/10.1145/3528223.3530131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion blur of fast-moving subjects is a longstanding problem in photography and very common on mobile phones due to limited light collection efficiency, particularly in low-light conditions. While we have witnessed great progress in image deblurring in recent years, most methods require significant computational power and have limitations in processing high-resolution photos with severe local motions. To this end, we develop a novel face deblurring system based on the dual camera fusion technique for mobile phones. The system detects subject motion to dynamically enable a reference camera, e.g., ultrawide angle camera commonly available on recent premium phones, and captures an auxiliary photo with faster shutter settings. While the main shot is low noise but blurry (Figure 1(a)), the reference shot is sharp but noisy (Figure 1(b)). We learn ML models to align and fuse these two shots and output a clear photo without motion blur (Figure 1(c)). Our algorithm runs efficiently on Google Pixel 6, which takes 463 ms overhead per shot. Our experiments demonstrate the advantage and robustness of our system against alternative single-image, multi-frame, face-specific, and video deblurring algorithms as well as commercial products. To the best of our knowledge, our work is the first mobile solution for face motion deblurring that works reliably and robustly over thousands of images in diverse motion and lighting conditions.},
  archive      = {J_TOG},
  author       = {Wei-Sheng Lai and Yichang Shih and Lun-Cheng Chu and Xiaotong Wu and Sung-Fang Tsai and Michael Krainin and Deqing Sun and Chia-Kai Liang},
  doi          = {10.1145/3528223.3530131},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {148:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Face deblurring using dual camera fusion on mobile phones},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perception of letter glyph parameters for InfoTypography.
<em>TOG</em>, <em>41</em>(4), 147:1–21. (<a
href="https://doi.org/10.1145/3528223.3530111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of variable font technologies---where typographic parameters such as weight, x-height and slant are easily adjusted across a range---enables encoding ordinal, interval or ratio data into text that is still readable. This is potentially valuable to represent additional information in text labels in visualizations (e.g., font weight can indicate city size in a geographical visualization) or in text itself (e.g., the intended reading speed of a sentence can be encoded with the font width). However, we do not know how different parameters, which are complex variations of shape, are perceived by the human visual system. Without this information it is difficult to select appropriate parameters and mapping functions that maximize perception of differences within the parameter range. We provide an empirical characterization of seven typographical parameters of Latin fonts in terms of absolute perception and just noticeable differences (JNDs) to help visualization designers to choose typographic parameters for visualizations that contain text, as well as support typographers and type designers when selecting which levels of these parameters to implement to achieve differentiability between normal text, emphasized text and different headings.},
  archive      = {J_TOG},
  author       = {Johannes Lang and Miguel A. Nacenta},
  doi          = {10.1145/3528223.3530111},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {147:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Perception of letter glyph parameters for InfoTypography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dark stereo: Improving depth perception under low luminance.
<em>TOG</em>, <em>41</em>(4), 146:1–12. (<a
href="https://doi.org/10.1145/3528223.3530136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often desirable or unavoidable to display Virtual Reality (VR) or stereoscopic content at low brightness. For example, a dimmer display reduces the flicker artefacts that are introduced by low-persistence VR headsets. It also saves power, prolongs battery life, and reduces the cost of a display or projection system. Additionally, stereo movies are usually displayed at relatively low luminance due to polarization filters or other optical elements necessary to separate two views. However, the binocular depth cues become less reliable at low luminance. In this paper, we propose a model of stereo constancy that predicts the precision of binocular depth cues for a given contrast and luminance. We use the model to design a novel contrast enhancement algorithm that compensates for the deteriorated depth perception to deliver good-quality stereoscopic images even for displays of very low brightness.},
  archive      = {J_TOG},
  author       = {Krzysztof Wolski and Fangcheng Zhong and Karol Myszkowski and Rafał K. Mantiuk},
  doi          = {10.1145/3528223.3530136},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {146:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dark stereo: Improving depth perception under low luminance},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StelaCSF: A unified model of contrast sensitivity as the
function of spatio-temporal frequency, eccentricity, luminance and area.
<em>TOG</em>, <em>41</em>(4), 145:1–16. (<a
href="https://doi.org/10.1145/3528223.3530115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A contrast sensitivity function, or CSF, is a cornerstone of many visual models. It explains whether a contrast pattern is visible to the human eye. The existing CSFs typically account for a subset of relevant dimensions describing a stimulus, limiting the use of such functions to either static or foveal content but not both. In this paper, we propose a unified CSF, stelaCSF, which accounts for all major dimensions of the stimulus: spatial and temporal frequency, eccentricity, luminance, and area. To model the 5-dimensional space of contrast sensitivity, we combined data from 11 papers, each of which studied a subset of this space. While previously proposed CSFs were fitted to a single dataset, stelaCSF can predict the data from all these studies using the same set of parameters. The predictions are accurate in the entire domain, including low frequencies. In addition, stelaCSF relies on psychophysical models and experimental evidence to explain the major interactions between the 5 dimensions of the CSF. We demonstrate the utility of our new CSF in a flicker detection metric and in foveated rendering.},
  archive      = {J_TOG},
  author       = {Rafał K. Mantiuk and Maliha Ashraf and Alexandre Chapiro},
  doi          = {10.1145/3528223.3530115},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {145:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {StelaCSF: A unified model of contrast sensitivity as the function of spatio-temporal frequency, eccentricity, luminance and area},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image features influence reaction time: A learned
probabilistic perceptual model for saccade latency. <em>TOG</em>,
<em>41</em>(4), 144:1–15. (<a
href="https://doi.org/10.1145/3528223.3530055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to ask and answer an essential question &quot; how quickly do we react after observing a displayed visual target?&quot; To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.},
  archive      = {J_TOG},
  author       = {Budmonde Duinkharjav and Praneeth Chakravarthula and Rachel Brown and Anjul Patney and Qi Sun},
  doi          = {10.1145/3528223.3530055},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {144:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Image features influence reaction time: A learned probabilistic perceptual model for saccade latency},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noise-based enhancement for foveated rendering.
<em>TOG</em>, <em>41</em>(4), 143:1–14. (<a
href="https://doi.org/10.1145/3528223.3530101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human visual sensitivity to spatial details declines towards the periphery. Novel image synthesis techniques, so-called foveated rendering, exploit this observation and reduce the spatial resolution of synthesized images for the periphery, avoiding the synthesis of high-spatial-frequency details that are costly to generate but not perceived by a viewer. However, contemporary techniques do not make a clear distinction between the range of spatial frequencies that must be reproduced and those that can be omitted. For a given eccentricity, there is a range of frequencies that are detectable but not resolvable. While the accurate reproduction of these frequencies is not required, an observer can detect their absence if completely omitted. We use this observation to improve the performance of existing foveated rendering techniques. We demonstrate that this specific range of frequencies can be efficiently replaced with procedural noise whose parameters are carefully tuned to image content and human perception. Consequently, these frequencies do not have to be synthesized during rendering, allowing more aggressive foveation, and they can be replaced by noise generated in a less expensive post-processing step, leading to improved performance of the rendering system. Our main contribution is a perceptually-inspired technique for deriving the parameters of the noise required for the enhancement and its calibration. The method operates on rendering output and runs at rates exceeding 200 FPS at 4K resolution, making it suitable for integration with real-time foveated rendering systems for VR and AR devices. We validate our results and compare them to the existing contrast enhancement technique in user experiments.},
  archive      = {J_TOG},
  author       = {Taimoor Tariq and Cara Tursun and Piotr Didyk},
  doi          = {10.1145/3528223.3530101},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {143:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Noise-based enhancement for foveated rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SNeRF: Stylized neural implicit representations for 3D
scenes. <em>TOG</em>, <em>41</em>(4), 142:1–11. (<a
href="https://doi.org/10.1145/3528223.3530107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a stylized novel view synthesis method. Applying state-of-the-art stylization methods to novel views frame by frame often causes jittering artifacts due to the lack of cross-view consistency. Therefore, this paper investigates 3D scene stylization that provides a strong inductive bias for consistent novel view synthesis. Specifically, we adopt the emerging neural radiance fields (NeRF) as our choice of 3D scene representation for their capability to render high-quality novel views for a variety of scenes. However, as rendering a novel view from a NeRF requires a large number of samples, training a stylized NeRF requires a large amount of GPU memory that goes beyond an off-the-shelf GPU capacity. We introduce a new training method to address this problem by alternating the NeRF and stylization optimization steps. Such a method enables us to make full use of our hardware memory capacity to both generate images at higher resolution and adopt more expressive image style transfer methods. Our experiments show that our method produces stylized NeRFs for a wide range of content, including indoor, outdoor and dynamic scenes, and synthesizes high-quality novel views with cross-view consistency.},
  archive      = {J_TOG},
  author       = {Thu Nguyen-Phuoc and Feng Liu and Lei Xiao},
  doi          = {10.1145/3528223.3530107},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {142:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {SNeRF: Stylized neural implicit representations for 3D scenes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StyleGAN-NADA: CLIP-guided domain adaptation of image
generators. <em>TOG</em>, <em>41</em>(4), 141:1–13. (<a
href="https://doi.org/10.1145/3528223.3530164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can a generative model be trained to produce images from a specific domain, guided only by a text prompt, without seeing any image? In other words: can an image generator be trained &quot;blindly&quot;? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or infeasible to reach with existing methods. We conduct an extensive set of experiments across a wide range of domains. These demonstrate the effectiveness of our approach, and show that our models preserve the latent-space structure that makes generative models appealing for downstream tasks. Code and videos available at: stylegan-nada.github.io/},
  archive      = {J_TOG},
  author       = {Rinon Gal and Or Patashnik and Haggai Maron and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or},
  doi          = {10.1145/3528223.3530164},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {141:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleGAN-NADA: CLIP-guided domain adaptation of image generators},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DCT-net: Domain-calibrated translation for portrait
stylization. <em>TOG</em>, <em>41</em>(4), 140:1–9. (<a
href="https://doi.org/10.1145/3528223.3530159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces DCT-Net, a novel image translation architecture for few-shot portrait stylization. Given limited style exemplars (~100), the new architecture can produce high-quality style transfer results with advanced ability to synthesize high-fidelity contents and strong generality to handle complicated scenes (e.g., occlusions and accessories). Moreover, it enables full-body image translation via one elegant evaluation network trained by partial observations (i.e., stylized heads). Few-shot learning based style transfer is challenging since the learned model can easily become overfitted in the target domain, due to the biased distribution formed by only a few training examples. This paper aims to handle the challenge by adopting the key idea of &quot;calibration first, translation later&quot; and exploring the augmented global structure with locally-focused translation. Specifically, the proposed DCT-Net consists of three modules: a content adapter borrowing the powerful prior from source photos to calibrate the content distribution of target samples; a geometry expansion module using affine transformations to release spatially semantic constraints; and a texture translation module leveraging samples produced by the calibrated distribution to learn a fine-grained conversion. Experimental results demonstrate the proposed method&#39;s superiority over the state of the art in head stylization and its effectiveness on full image translation with adaptive deformations. Our code is publicly available at https://github.com/menyifang/DCT-Net.},
  archive      = {J_TOG},
  author       = {Yifang Men and Yuan Yao and Miaomiao Cui and Zhouhui Lian and Xuansong Xie},
  doi          = {10.1145/3528223.3530159},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {140:1–9},
  shortjournal = {ACM Trans. Graph.},
  title        = {DCT-net: Domain-calibrated translation for portrait stylization},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Character articulation through profile curves. <em>TOG</em>,
<em>41</em>(4), 139:1–14. (<a
href="https://doi.org/10.1145/3528223.3530060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer animation relies heavily on rigging setups that articulate character surfaces through a broad range of poses. Although many deformation strategies have been proposed over the years, constructing character rigs is still a cumbersome process that involves repetitive authoring of point weights and corrective sculpts with limited and indirect shaping controls. This paper presents a new approach for character articulation that produces detail-preserving deformations fully controlled by 3D curves that profile the deforming surface. Our method starts with a spline-based rigging system in which artists can draw and articulate sparse curvenets that describe surface profiles. By analyzing the layout of the rigged curvenets, we quantify the deformation along each curve side independent of the mesh connectivity, thus separating the articulation controllers from the underlying surface representation. To propagate the curvenet articulation over the character surface, we formulate a deformation optimization that reconstructs surface details while conforming to the rigged curvenets. In this process, we introduce a cut-cell algorithm that binds the curvenet to the surface mesh by cutting mesh elements into smaller polygons possibly with cracks, and then derive a cut-aware numerical discretization that provides harmonic interpolations with curve discontinuities. We demonstrate the expressiveness and flexibility of our method using a series of animation clips.},
  archive      = {J_TOG},
  author       = {Fernando De Goes and William Sheffler and Kurt Fleischer},
  doi          = {10.1145/3528223.3530060},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {139:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Character articulation through profile curves},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GANimator: Neural motion synthesis from a single sequence.
<em>TOG</em>, <em>41</em>(4), 138:1–12. (<a
href="https://doi.org/10.1145/3528223.3530157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. Existing data-driven techniques for motion synthesis require a large motion dataset which contains the desired and specific skeletal structure. By contrast, GANimator only requires training on a single motion sequence, enabling novel motion synthesis for a variety of skeletal structures e.g. , bipeds, quadropeds, hexapeds, and more. Our framework contains a series of generative and adversarial neural networks, each responsible for generating motions in a specific frame rate. The framework progressively learns to synthesize motion from random noise, enabling hierarchical control over the generated motion content across varying levels of detail. We show a number of applications, including crowd simulation, key-frame editing, style transfer, and interactive control, which all learn from a single input sequence. Code and data for this paper are at https://peizhuoli.github.io/ganimator.},
  archive      = {J_TOG},
  author       = {Peizhuo Li and Kfir Aberman and Zihan Zhang and Rana Hanocka and Olga Sorkine-Hornung},
  doi          = {10.1145/3528223.3530157},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {138:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {GANimator: Neural motion synthesis from a single sequence},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time controllable motion transition for characters.
<em>TOG</em>, <em>41</em>(4), 137:1–10. (<a
href="https://doi.org/10.1145/3528223.3530090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time in-between motion generation is universally required in games and highly desirable in existing animation pipelines. Its core challenge lies in the need to satisfy three critical conditions simultaneously: quality, controllability and speed , which renders any methods that need offline computation (or post-processing) or cannot incorporate (often unpredictable) user control undesirable. To this end, we propose a new real-time transition method to address the aforementioned challenges. Our approach consists of two key components: motion manifold and conditional transitioning. The former learns the important low-level motion features and their dynamics; while the latter synthesizes transitions conditioned on a target frame and the desired transition duration. We first learn a motion manifold that explicitly models the intrinsic transition stochasticity in human motions via a multi-modal mapping mechanism. Then, during generation, we design a transition model which is essentially a sampling strategy to sample from the learned manifold, based on the target frame and the aimed transition duration. We validate our method on different datasets in tasks where no post-processing or offline computation is allowed. Through exhaustive evaluation and comparison, we show that our method is able to generate high-quality motions measured under multiple metrics. Our method is also robust under various target frames (with extreme cases).},
  archive      = {J_TOG},
  author       = {Xiangjun Tang and He Wang and Bo Hu and Xu Gong and Ruifan Yi and Qilong Kou and Xiaogang Jin},
  doi          = {10.1145/3528223.3530090},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {137:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time controllable motion transition for characters},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepPhase: Periodic autoencoders for learning motion phase
manifolds. <em>TOG</em>, <em>41</em>(4), 136:1–13. (<a
href="https://doi.org/10.1145/3528223.3530178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases.},
  archive      = {J_TOG},
  author       = {Sebastian Starke and Ian Mason and Taku Komura},
  doi          = {10.1145/3528223.3530178},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {136:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepPhase: Periodic autoencoders for learning motion phase manifolds},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aδ: Autodiff for discontinuous programs - applied to
shaders. <em>TOG</em>, <em>41</em>(4), 135:1–24. (<a
href="https://doi.org/10.1145/3528223.3530125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, automatic differentiation (AD) has profoundly impacted graphics and vision applications --- both broadly via deep learning and specifically for inverse rendering. Traditional AD methods ignore gradients at discontinuities, instead treating functions as continuous. Rendering algorithms intrinsically rely on discontinuities, crucial at object silhouettes and in general for any branching operation. Researchers have proposed fully- automatic differentiation approaches for handling discontinuities by restricting to affine functions, or semi- automatic processes restricted either to invertible functions or to specialized applications like vector graphics. This paper describes a compiler-based approach to extend reverse mode AD so as to accept arbitrary programs involving discontinuities. Our novel gradient rules generalize differentiation to work correctly, assuming there is a single discontinuity in a local neighborhood, by approximating the prefiltered gradient over a box kernel oriented along a 1D sampling axis. We describe when such approximation rules are first-order correct, and show that this correctness criterion applies to a relatively broad class of functions. Moreover, we show that the method is effective in practice for arbitrary programs, including features for which we cannot prove correctness. We evaluate this approach on procedural shader programs, where the task is to optimize unknown parameters in order to match a target image, and our method outperforms baselines in terms of both convergence and efficiency. Our compiler outputs gradient programs in TensorFlow, PyTorch (for quick prototypes) and Halide with an optional auto-scheduler (for efficiency). The compiler also outputs GLSL that renders the target image, allowing users to interactively modify and animate the shader, which would otherwise be cumbersome in other representations such as triangle meshes or vector art.},
  archive      = {J_TOG},
  author       = {Yuting Yang and Connelly Barnes and Andrew Adams and Adam Finkelstein},
  doi          = {10.1145/3528223.3530125},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {135:1–24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Aδ: Autodiff for discontinuous programs - applied to shaders},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-free multiple-bounce computations for smith
microfacet BSDFs. <em>TOG</em>, <em>41</em>(4), 134:1–14. (<a
href="https://doi.org/10.1145/3528223.3530112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidirectional Scattering Distribution Functions (BSDFs) encode how a material reflects or transmits the incoming light. The most commonly used model is the microfacet BSDF. It computes the material response from the microgeometry of the surface assuming a single bounce on specular microfacets. The original model ignores multiple bounces on the microgeometry, resulting in an energy loss, especially for rough materials. In this paper, we present a new method to compute the multiple bounces inside the microgeometry, eliminating this energy loss. Our method relies on a position-free formulation of multiple bounces inside the microgeometry. We use an explicit mathematical definition of the path space that describes single and multiple bounces in a uniform way. We then study the behavior of light on the different vertices and segments in the path space, leading to a reciprocal multiple-bounce description of BSDFs. Furthermore, we present practical, unbiased Monte Carlo estimators to compute multiple scattering. Our method is less noisy than existing algorithms for computing multiple scattering. It is almost noise-free with a very-low sampling rate, from 2 to 4 samples per pixel (spp).},
  archive      = {J_TOG},
  author       = {Beibei Wang and Wenhua Jin and Jiahui Fan and Jian Yang and Nicolas Holzschuch and Ling-Qi Yan},
  doi          = {10.1145/3528223.3530112},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {134:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Position-free multiple-bounce computations for smith microfacet BSDFs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse ellipsometry: Portable acquisition of polarimetric
SVBRDF and shape with unstructured flash photography. <em>TOG</em>,
<em>41</em>(4), 133:1–14. (<a
href="https://doi.org/10.1145/3528223.3530075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ellipsometry techniques allow to measure polarization information of materials, requiring precise rotations of optical components with different configurations of lights and sensors. This results in cumbersome capture devices, carefully calibrated in lab conditions, and in very long acquisition times, usually in the order of a few days per object. Recent techniques allow to capture polarimetric spatially-varying reflectance information, but limited to a single view, or to cover all view directions, but limited to spherical objects made of a single homogeneous material. We present sparse ellipsometry , a portable polarimetric acquisition method that captures both polarimetric SVBRDF and 3D shape simultaneously. Our handheld device consists of off-the-shelf, fixed optical components. Instead of days, the total acquisition time varies between twenty and thirty minutes per object. We develop a complete polarimetric SVBRDF model that includes diffuse and specular components, as well as single scattering, and devise a novel polarimetric inverse rendering algorithm with data augmentation of specular reflection samples via generative modeling. Our results show a strong agreement with a recent ground-truth dataset of captured polarimetric BRDFs of real-world objects.},
  archive      = {J_TOG},
  author       = {Inseung Hwang and Daniel S. Jeon and Adolfo Muñoz and Diego Gutierrez and Xin Tong and Min H. Kim},
  doi          = {10.1145/3528223.3530075},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {133:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sparse ellipsometry: Portable acquisition of polarimetric SVBRDF and shape with unstructured flash photography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards practical physical-optics rendering. <em>TOG</em>,
<em>41</em>(4), 132:1–24. (<a
href="https://doi.org/10.1145/3528223.3530119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical light transport (PLT) algorithms can represent the wave nature of light globally in a scene, and are consistent with Maxwell&#39;s theory of electromagnetism. As such, they are able to reproduce the wave-interference and diffraction effects of real physical optics. However, the recent works that have proposed PLT are too expensive to apply to real-world scenes with complex geometry and materials. To address this problem, we propose a novel framework for physical light transport based on several key ideas that actually makes PLT practical for complex scenes. First, we restrict the spatial coherence shape of light to an anisotropic Gaussian and justify this restriction with general arguments based on entropy. This restriction serves to simplify the rest of the derivations, without practical loss of generality. To describe partially-coherent light, we present new rendering primitives that generalize the radiometric radiance and irradiance, and are based on the well-known Stokes parameters. We are able to represent light of arbitrary spectral content and states of polarization, and with any coherence volume and anisotropy. We also present the wave BSDF to accurately render diffractions and wave-interference effects. Furthermore, we present an approach to importance sample this wave BSDF to facilitate bi-directional path tracing, which has been previously impossible. We show good agreement with state-of-the-art methods, but unlike them we are able to render complex scenes where all the materials are new, coherence-aware physical optics materials, and with performance approaching that of &quot;classical&quot; rendering methods.},
  archive      = {J_TOG},
  author       = {Shlomi Steinberg and Pradeep Sen and Ling-Qi Yan},
  doi          = {10.1145/3528223.3530119},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {132:1–24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards practical physical-optics rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Photo-to-shape material transfer for diverse structures.
<em>TOG</em>, <em>41</em>(4), 131:1–14. (<a
href="https://doi.org/10.1145/3528223.3530088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for assigning photorealistic relightable materials to 3D shapes in an automatic manner. Our method takes as input a photo exemplar of a real object and a 3D object with segmentation, and uses the exemplar to guide the assignment of materials to the parts of the shape, so that the appearance of the resulting shape is as similar as possible to the exemplar. To accomplish this goal, our method combines an image translation neural network with a material assignment neural network. The image translation network translates the color from the exemplar to a projection of the 3D shape and the part segmentation from the projection to the exemplar. Then, the material prediction network assigns materials from a collection of realistic materials to the projected parts, based on the translated images and perceptual similarity of the materials. One key idea of our method is to use the translation network to establish a correspondence between the exemplar and shape projection, which allows us to transfer materials between objects with diverse structures. Another key idea of our method is to use the two pairs of (color, segmentation) images provided by the image translation to guide the material assignment, which enables us to ensure the consistency in the assignment. We demonstrate that our method allows us to assign materials to shapes so that their appearances better resemble the input exemplars, improving the quality of the results over the state-of-the-art method, and allowing us to automatically create thousands of shapes with high-quality photorealistic materials. Code and data for this paper are available at https://github.com/XiangyuSu611/TMT.},
  archive      = {J_TOG},
  author       = {Ruizhen Hu and Xiangyu Su and Xiangkai Chen and Oliver Van Kaick and Hui Huang},
  doi          = {10.1145/3528223.3530088},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {131:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Photo-to-shape material transfer for diverse structures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Moving level-of-detail surfaces. <em>TOG</em>,
<em>41</em>(4), 130:1–10. (<a
href="https://doi.org/10.1145/3528223.3530151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a simple, fast, and smooth scheme to approximate Algebraic Point Set Surfaces using non-compact kernels, which is particularly suited for filtering and reconstructing point sets presenting large missing parts. Our key idea is to consider a moving level-of-detail of the input point set which is adaptive w.r.t. to the evaluation location, just such as the samples weights are output sensitive in the traditional moving least squares scheme. We also introduce an adaptive progressive octree refinement scheme, driven by the resulting implicit surface, to properly capture the modeled geometry even far away from the input samples. Similarly to typical compactly-supported approximations, our operator runs in logarithmic time while defining high quality surfaces even on challenging inputs for which only global optimizations achieve reasonable results. We demonstrate our technique on a variety of point sets featuring geometric noise as well as large holes.},
  archive      = {J_TOG},
  author       = {Corentin Mercier and Thibault Lescoat and Pierre Roussillon and Tamy Boubekeur and Jean-Marc Thiery},
  doi          = {10.1145/3528223.3530151},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {130:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Moving level-of-detail surfaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ComplexGen: CAD reconstruction by b-rep chain complex
generation. <em>TOG</em>, <em>41</em>(4), 129:1–18. (<a
href="https://doi.org/10.1145/3528223.3530078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We view the reconstruction of CAD models in the boundary representation (B-Rep) as the detection of geometric primitives of different orders, i.e. , vertices, edges and surface patches, and the correspondence of primitives, which are holistically modeled as a chain complex, and show that by modeling such comprehensive structures more complete and regularized reconstructions can be achieved. We solve the complex generation problem in two steps. First, we propose a novel neural framework that consists of a sparse CNN encoder for input point cloud processing and a tri-path transformer decoder for generating geometric primitives and their mutual relationships with estimated probabilities. Second, given the probabilistic structure predicted by the neural network, we recover a definite B-Rep chain complex by solving a global optimization maximizing the likelihood under structural validness constraints and applying geometric refinements. Extensive tests on large scale CAD datasets demonstrate that the modeling of B-Rep chain complex structure enables more accurate detection for learning and more constrained reconstruction for optimization, leading to structurally more faithful and complete CAD B-Rep models than previous results.},
  archive      = {J_TOG},
  author       = {Haoxiang Guo and Shilin Liu and Hao Pan and Yang Liu and Xin Tong and Baining Guo},
  doi          = {10.1145/3528223.3530078},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {129:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {ComplexGen: CAD reconstruction by B-rep chain complex generation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative poisson surface reconstruction (iPSR) for
unoriented points. <em>TOG</em>, <em>41</em>(4), 128:1–13. (<a
href="https://doi.org/10.1145/3528223.3530096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poisson surface reconstruction (PSR) remains a popular technique for reconstructing watertight surfaces from 3D point samples thanks to its efficiency, simplicity, and robustness. Yet, the existing PSR method and subsequent variants work only for oriented points. This paper intends to validate that an improved PSR, called iPSR, can completely eliminate the requirement of point normals and proceed in an iterative manner. In each iteration, iPSR takes as input point samples with normals directly computed from the surface obtained in the preceding iteration, and then generates a new surface with better quality. Extensive quantitative evaluation confirms that the new iPSR algorithm converges in 5--30 iterations even with randomly initialized normals. If initialized with a simple visibility based heuristic, iPSR can further reduce the number of iterations. We conduct comprehensive comparisons with PSR and other powerful implicit-function based methods. Finally, we confirm iPSR&#39;s effectiveness and scalability on the AIM@SHAPE dataset and challenging (indoor and outdoor) scenes. Code and data for this paper are at https://github.com/houfei0801/ipsr.},
  archive      = {J_TOG},
  author       = {Fei Hou and Chiyu Wang and Wencheng Wang and Hong Qin and Chen Qian and Ying He},
  doi          = {10.1145/3528223.3530096},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {128:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Iterative poisson surface reconstruction (iPSR) for unoriented points},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alpha wrapping with an offset. <em>TOG</em>, <em>41</em>(4),
127:1–22. (<a href="https://doi.org/10.1145/3528223.3530152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an input 3D geometry such as a triangle soup or a point set, we address the problem of generating a watertight and orientable surface triangle mesh that strictly encloses the input. The output mesh is obtained by greedily refining and carving a 3D Delaunay triangulation on an offset surface of the input, while carving with empty balls of radius alpha. The proposed algorithm is controlled via two user-defined parameters: alpha and offset. Alpha controls the size of cavities or holes that cannot be traversed during carving, while offset controls the distance between the vertices of the output mesh and the input. Our algorithm is guaranteed to terminate and to yield a valid and strictly enclosing mesh, even for defect-laden inputs. Genericity is achieved using an abstract interface probing the input, enabling any geometry to be used, provided a few basic geometric queries can be answered. We benchmark the algorithm on large public datasets such as Thingi10k, and compare it to state-of-the-art approaches in terms of robustness, approximation, output complexity, speed, and peak memory consumption. Our implementation is available through the CGAL library.},
  archive      = {J_TOG},
  author       = {Cédric Portaneri and Mael Rouxel-Labbé and Michael Hemmer and David Cohen-Steiner and Pierre Alliez},
  doi          = {10.1145/3528223.3530152},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {127:1–22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Alpha wrapping with an offset},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adjoint nonlinear ray tracing. <em>TOG</em>, <em>41</em>(4),
126:1–13. (<a href="https://doi.org/10.1145/3528223.3530077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing and designing media with continuously-varying refractive index fields remains a challenging problem in computer graphics. A core difficulty in trying to tackle this inverse problem is that light travels inside such media along curves, rather than straight lines. Existing techniques for this problem make strong assumptions on the shape of the ray inside the medium, and thus limit themselves to media where the ray deflection is relatively small. More recently, differentiable rendering techniques have relaxed this limitation, by making it possible to differentiably simulate curved light paths. However, the automatic differentiation algorithms underlying these techniques use large amounts of memory, restricting existing differentiable rendering techniques to relatively small media and low spatial resolutions. We present a method for optimizing refractive index fields that both accounts for curved light paths and has a small, constant memory footprint. We use the adjoint state method to derive a set of equations for computing derivatives with respect to the refractive index field of optimization objectives that are subject to nonlinear ray tracing constraints. We additionally introduce discretization schemes to numerically evaluate these equations, without the need to store nonlinear ray trajectories in memory, significantly reducing the memory requirements of our algorithm. We use our technique to optimize high-resolution refractive index fields for a variety of applications, including creating different types of displays (multiview, lightfield, caustic), designing gradient-index optics, and reconstructing gas flows.},
  archive      = {J_TOG},
  author       = {Arjun Teh and Matthew O&#39;Toole and Ioannis Gkioulekas},
  doi          = {10.1145/3528223.3530077},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {126:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adjoint nonlinear ray tracing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable signed distance function rendering.
<em>TOG</em>, <em>41</em>(4), 125:1–18. (<a
href="https://doi.org/10.1145/3528223.3530139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically-based differentiable rendering has recently emerged as an attractive new technique for solving inverse problems that recover complete 3D scene representations from images. The inversion of shape parameters is of particular interest but also poses severe challenges: shapes are intertwined with visibility, whose discontinuous nature introduces severe bias in computed derivatives unless costly precautions are taken. Shape representations like triangle meshes suffer from additional difficulties, since the continuous optimization of mesh parameters cannot introduce topological changes. One common solution to these difficulties entails representing shapes using signed distance functions (SDFs) and gradually adapting their zero level set during optimization. Previous differentiable rendering of SDFs did not fully account for visibility gradients and required the use of mask or silhouette supervision, or discretization into a triangle mesh. In this article, we show how to extend the commonly used sphere tracing algorithm so that it additionally outputs a reparameterization that provides the means to compute accurate shape parameter derivatives. At a high level, this resembles techniques for differentiable mesh rendering, though we show that the SDF representation admits a particularly efficient reparameterization that outperforms prior work. Our experiments demonstrate the reconstruction of (synthetic) objects without complex regularization or priors, using only a per-pixel RGB loss.},
  archive      = {J_TOG},
  author       = {Delio Vicini and Sébastien Speierer and Wenzel Jakob},
  doi          = {10.1145/3528223.3530139},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {125:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable signed distance function rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DR.JIT: A just-in-time compiler for differentiable
rendering. <em>TOG</em>, <em>41</em>(4), 124:1–19. (<a
href="https://doi.org/10.1145/3528223.3530099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DR.JIT is a new just-in-time compiler for physically based rendering and its derivative. DR.JIT expedites research on these topics in two ways: first, it traces high-level simulation code (e.g., written in Python) and aggressively simplifies and specializes the resulting program representation, producing data-parallel kernels with state-of-the-art performance on CPUs and GPUs. Second, it simplifies the development of differentiable rendering algorithms. Efficient methods in this area turn the derivative of a simulation into a simulation of the derivative. DR.JIT provides fine-grained control over the process of automatic differentiation to help with this transformation. Specialization is particularly helpful in the context of differentiation, since large parts of the simulation ultimately do not influence the computed gradients. DR.JIT tracks data dependencies globally to find and remove redundant computation.},
  archive      = {J_TOG},
  author       = {Wenzel Jakob and Sébastien Speierer and Nicolas Roussel and Delio Vicini},
  doi          = {10.1145/3528223.3530099},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {124:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {DR.JIT: A just-in-time compiler for differentiable rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient estimation of boundary integrals for path-space
differentiable rendering. <em>TOG</em>, <em>41</em>(4), 123:1–13. (<a
href="https://doi.org/10.1145/3528223.3530080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary integrals are unique to physics-based differentiable rendering and crucial for differentiating with respect to object geometry. Under the differential path integral framework---which has enabled the development of sophisticated differentiable rendering algorithms---the boundary components are themselves path integrals. Previously, although the mathematical formulation of boundary path integrals have been established, efficient estimation of these integrals remains challenging. In this paper, we introduce a new technique to efficiently estimate boundary path integrals. A key component of our technique is a primary-sample-space guiding step for importance sampling of boundary segments. Additionally, we show multiple importance sampling can be used to combine multiple guided samplings. Lastly, we introduce an optional edge sorting step to further improve the runtime performance. We evaluate the effectiveness of our method using several differentiable-rendering and inverse-rendering examples and provide comparisons with existing methods for reconstruction as well as gradient quality.},
  archive      = {J_TOG},
  author       = {Kai Yan and Christoph Lassner and Brian Budge and Zhao Dong and Shuang Zhao},
  doi          = {10.1145/3528223.3530080},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {123:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient estimation of boundary integrals for path-space differentiable rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit neural representation for physics-driven actuated
soft bodies. <em>TOG</em>, <em>41</em>(4), 122:1–10. (<a
href="https://doi.org/10.1145/3528223.3530156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal&#39;s dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.},
  archive      = {J_TOG},
  author       = {Lingchen Yang and Byungsoo Kim and Gaspard Zoss and Baran Gözcü and Markus Gross and Barbara Solenthaler},
  doi          = {10.1145/3528223.3530156},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {122:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Implicit neural representation for physics-driven actuated soft bodies},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuralSound: Learning-based modal sound synthesis with
acoustic transfer. <em>TOG</em>, <em>41</em>(4), 121:1–15. (<a
href="https://doi.org/10.1145/3528223.3530184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel learning-based modal sound synthesis approach that includes a mixed vibration solver for modal analysis and a radiation network for acoustic transfer. Our mixed vibration solver consists of a 3D sparse convolution network and a Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) module for iterative optimization. Moreover, we highlight the correlation between a standard numerical vibration solver and our network architecture. Our radiation network predicts the Far-Field Acoustic Transfer maps (FFAT Maps) from the surface vibration of the object. The overall running time of our learning-based approach for most new objects is less than one second on a RTX 3080 Ti GPU while maintaining a high sound quality close to the ground truth solved by standard numerical methods. We also evaluate the numerical and perceptual accuracy of our approach on different objects with various shapes and materials.},
  archive      = {J_TOG},
  author       = {Xutong Jin and Sheng Li and Guoping Wang and Dinesh Manocha},
  doi          = {10.1145/3528223.3530184},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {121:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuralSound: Learning-based modal sound synthesis with acoustic transfer},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NIMBLE: A non-rigid hand model with bones and muscles.
<em>TOG</em>, <em>41</em>(4), 120:1–16. (<a
href="https://doi.org/10.1145/3528223.3530079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Metaverse applications demand reliable, accurate, and photorealistic reproductions of human hands to perform sophisticated operations as if in the physical world. While real human hand represents one of the most intricate coordination between bones, muscle, tendon, and skin, state-of-the-art techniques unanimously focus on modeling only the skeleton of the hand. In this paper, we present NIMBLE, a novel parametric hand model that includes the missing key components, bringing 3D hand model to a new level of realism. We first annotate muscles, bones and skins on the recent Magnetic Resonance Imaging hand (MRI-Hand) dataset [Li et al. 2021] and then register a volumetric template hand onto individual poses and subjects within the dataset. NIMBLE consists of 20 bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin mesh. Via iterative shape registration and parameter learning, it further produces shape blend shapes, pose blend shapes, and a joint regressor. We demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks. By enforcing the inner bones and muscles to match anatomic and kinematic rules, NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the appearance of skin, we further construct a photometric HandStage to acquire high-quality textures and normal maps to model wrinkles and palm print. Finally, NIMBLE also benefits learning-based hand pose and shape estimation by either synthesizing rich data or acting directly as a differentiable layer in the inference network.},
  archive      = {J_TOG},
  author       = {Yuwei Li and Longwen Zhang and Zesong Qiu and Yingwenqi Jiang and Nianyi Li and Yuexin Ma and Yuyao Zhang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3528223.3530079},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {120:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {NIMBLE: A non-rigid hand model with bones and muscles},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics informed neural fields for smoke reconstruction with
sparse data. <em>TOG</em>, <em>41</em>(4), 119:1–14. (<a
href="https://doi.org/10.1145/3528223.3530169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity reconstruction of dynamic fluids from sparse multiview RGB videos remains a formidable challenge, due to the complexity of the underlying physics as well as the severe occlusion and complex lighting in the captured data. Existing solutions either assume knowledge of obstacles and lighting, or only focus on simple fluid scenes without obstacles or complex lighting, and thus are unsuitable for real-world scenes with unknown lighting conditions or arbitrary obstacles. We present the first method to reconstruct dynamic fluid phenomena by leveraging the governing physics (ie, Navier -Stokes equations) in an end-to-end optimization from a mere set of sparse video frames without taking lighting conditions, geometry information, or boundary conditions as input. Our method provides a continuous spatio-temporal scene representation using neural networks as the ansatz of density and velocity solution functions for fluids as well as the radiance field for static objects. With a hybrid architecture that separates static and dynamic contents apart, fluid interactions with static obstacles are reconstructed for the first time without additional geometry input or human labeling. By augmenting time-varying neural radiance fields with physics-informed deep learning, our method benefits from the supervision of images and physical priors. Our progressively growing model with regularization further disentangles the density-color ambiguity in the radiance field, which allows for a more robust optimization from the given input of sparse views. A pretrained density-to-velocity fluid model is leveraged in addition as the data prior to avoid suboptimal velocity solutions which underestimate vorticity but trivially fulfill physical equations. Our method exhibits high-quality results with relaxed constraints and strong flexibility on a representative set of synthetic and real flow captures. Code and sample tests are at https://people.mpi-inf.mpg.de/~mchu/projects/PI-NeRF/.},
  archive      = {J_TOG},
  author       = {Mengyu Chu and Lingjie Liu and Quan Zheng and Erik Franz and Hans-Peter Seidel and Christian Theobalt and Rhaleb Zayer},
  doi          = {10.1145/3528223.3530169},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {119:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physics informed neural fields for smoke reconstruction with sparse data},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The power particle-in-cell method. <em>TOG</em>,
<em>41</em>(4), 118:1–13. (<a
href="https://doi.org/10.1145/3528223.3530066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new weighting scheme for particle-grid transfers that generates hybrid Lagrangian/Eulerian fluid simulations with uniform particle distributions and precise volume control. At its core, our approach reformulates the construction of Power Particles [de Goes et al. 2015] by computing volume-constrained density kernels. We employ these optimized kernels as particle domains within the Generalized Interpolation Material Point method (GIMP) in order to incorporate Power Particles into the Particle-In-Cell framework, hence the name the Power Particle-In-Cell method. We address the construction of volume-constrained density kernels as a regularized optimal transportation problem and describe an iterative solver based on localized Gaussian convolutions that leads to a significant performance speedup compared to [de Goes et al. 2015]. We also present novel extensions for handling free surfaces and solid obstacles that bypass the need for cell clipping and ghost particles. We demonstrate the advantages of our transfer weights by improving hybrid schemes for fluid simulation such as the Fluid Implicit Particle (FLIP) method and the Affine Particle-In-Cell (APIC) method with volume preservation and robustness to varying particle-per-cell ratio, while retaining low numerical dissipation, conserving linear and angular momenta, and avoiding particle reseeding or post-process relaxations.},
  archive      = {J_TOG},
  author       = {Ziyin Qu and Minchen Li and Fernando De Goes and Chenfanfu Jiang},
  doi          = {10.1145/3528223.3530066},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {118:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {The power particle-in-cell method},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guided bubbles and wet foam for realistic whitewater
simulation. <em>TOG</em>, <em>41</em>(4), 117:1–16. (<a
href="https://doi.org/10.1145/3528223.3530059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for enhancing fluid simulations with realistic bubble and foam detail. We treat bubbles as discrete air particles, two-way coupled with a sparse volumetric Euler flow, as first suggested in [Stomakhin et al. 2020]. We elaborate further on their scheme and introduce a bubble inertia correction term for improved convergence. We also show how one can add bubbles to an already existing fluid simulation using our novel guiding technique, which performs local re-simulation of fluid to achieve more interesting bubble dynamics through coupling. As bubbles reach the surface, they are converted into foam and simulated separately. Our foam is discretized with smoothed particle hydrodynamics (SPH), and we replace forces normal to the fluid surface with a fluid surface manifold advection constraint to achieve more robust and stable results. The SPH forces are derived through proper constitutive modeling of an incompressible viscous liquid, and we explain why this choice is appropriate for &quot;wet&quot; types of foam. This allows us to produce believable dynamics from close-up scenarios to large oceans, with just a few parameters that work intuitively across a variety of scales. Additionally, we present relevant research on air entrainment metrics and bubble distributions that have been used in this work.},
  archive      = {J_TOG},
  author       = {Joel Wretborn and Sean Flynn and Alexey Stomakhin},
  doi          = {10.1145/3528223.3530059},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {117:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Guided bubbles and wet foam for realistic whitewater simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A clebsch method for free-surface vortical flow simulation.
<em>TOG</em>, <em>41</em>(4), 116:1–13. (<a
href="https://doi.org/10.1145/3528223.3530150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Clebsch method to simulate the free-surface vortical flow. At the center of our approach lies a level-set method enhanced by a wave-function correction scheme and a wave-function extrapolation algorithm to tackle the Clebsch method&#39;s numerical instabilities near a dynamic interface. By combining the Clebsch wave function&#39;s expressiveness in representing vortical structures and the level-set function&#39;s ability on tracking interfacial dynamics, we can model complex vortex-interface interaction problems that exhibit rich free-surface flow details on a Cartesian grid. We showcase the efficacy of our approach by simulating a wide range of new free-surface flow phenomena that were impractical for previous methods, including horseshoe vortex, sink vortex, bubble rings, and free-surface wake vortices.},
  archive      = {J_TOG},
  author       = {Shiying Xiong and Zhecheng Wang and Mengdi Wang and Bo Zhu},
  doi          = {10.1145/3528223.3530150},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {116:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A clebsch method for free-surface vortical flow simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VEMPIC: Particle-in-polyhedron fluid simulation for
intricate solid boundaries. <em>TOG</em>, <em>41</em>(4), 115:1–22. (<a
href="https://doi.org/10.1145/3528223.3530138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comprehensive visual modeling of fluid motion has historically been a challenging task, due in no small part to the difficulties inherent in geometries that are non-manifold, open, or thin. Modern geometric cut-cell mesh generators have been shown to produce, both robustly and quickly, workable volumetric elements in the presence of these problematic geometries, and the resulting volumetric representation would seem to offer an ideal infrastructure with which to perform fluid simulations. However, cut-cell mesh elements are general polyhedra that often contain holes and are non-convex; it is therefore difficult to construct the explicit function spaces required to employ standard functional discretizations, such as the Finite Element Method. The Virtual Element Method (VEM) has recently emerged as a functional discretization that successfully operates with complex polyhedral elements through a weak formulation of its function spaces. We present a novel cut-cell fluid simulation framework that exactly represents boundary geometry during the simulation. Our approach enables, for the first time, detailed fluid simulation with &quot;in-the-wild&quot; obstacles, including ones that contain non-manifold parts, self-intersections, and extremely thin features. Our key technical contribution is the generalization of the Particle-In-Cell fluid simulation methodology to arbitrary polyhedra using VEM. Coupled with a robust cut-cell generation scheme, this produces a fluid simulation algorithm that can operate on previously infeasible geometries without requiring any additional mesh modification or repair.},
  archive      = {J_TOG},
  author       = {Michael Tao and Christopher Batty and Mirela Ben-Chen and Eugene Fiume and David I. W. Levin},
  doi          = {10.1145/3528223.3530138},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {115:1–22},
  shortjournal = {ACM Trans. Graph.},
  title        = {VEMPIC: Particle-in-polyhedron fluid simulation for intricate solid boundaries},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient kinetic simulation of two-phase flows.
<em>TOG</em>, <em>41</em>(4), 114:1–17. (<a
href="https://doi.org/10.1145/3528223.3530132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-life multiphase flows exhibit a number of complex and visually appealing behaviors, involving bubbling, wetting, splashing, and glugging. However, most state-of-the-art simulation techniques in graphics can only demonstrate a limited range of multiphase flow phenomena, due to their inability to handle the real water-air density ratio and to the large amount of numerical viscosity introduced in the flow simulation and its coupling with the interface. Recently, kinetic-based methods have achieved success in simulating large density ratios and high Reynolds numbers efficiently; but their memory overhead, limited stability, and numerically-intensive treatment of coupling with immersed solids remain enduring obstacles to their adoption in movie productions. In this paper, we propose a new kinetic solver to couple the incompressible Navier-Stokes equations with a conservative phase-field equation which remedies these major practical hurdles. The resulting two-phase immiscible fluid solver is shown to be efficient due to its massively-parallel nature and GPU implementation, as well as very versatile and reliable because of its enhanced stability to large density ratios, high Reynolds numbers, and complex solid boundaries. We highlight the advantages of our solver through various challenging simulation results that capture intricate and turbulent air-water interaction, including comparisons to previous work and real footage.},
  archive      = {J_TOG},
  author       = {Wei Li and Yihui Ma and Xiaopei Liu and Mathieu Desbrun},
  doi          = {10.1145/3528223.3530132},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {114:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient kinetic simulation of two-phase flows},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covector fluids. <em>TOG</em>, <em>41</em>(4), 113:1–16. (<a
href="https://doi.org/10.1145/3528223.3530120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The animation of delicate vortical structures of gas and liquids has been of great interest in computer graphics. However, common velocity-based fluid solvers can damp the vortical flow, while vorticity-based fluid solvers suffer from performance drawbacks. We propose a new velocity-based fluid solver derived from a reformulated Euler equation using covectors. Our method generates rich vortex dynamics by an advection process that respects the Kelvin circulation theorem. The numerical algorithm requires only a small local adjustment to existing advection-projection methods and can easily leverage recent advances therein. The resulting solver emulates a vortex method without the expensive conversion between vortical variables and velocities. We demonstrate that our method preserves vorticity in both vortex filament dynamics and turbulent flows significantly better than previous methods, while also improving preservation of energy.},
  archive      = {J_TOG},
  author       = {Mohammad Sina Nabizadeh and Stephanie Wang and Ravi Ramamoorthi and Albert Chern},
  doi          = {10.1145/3528223.3530120},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {113:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Covector fluids},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Closed-loop control of direct ink writing via reinforcement
learning. <em>TOG</em>, <em>41</em>(4), 112:1–10. (<a
href="https://doi.org/10.1145/3528223.3530144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling additive manufacturing to employ a wide range of novel, functional materials can be a major boost to this technology. However, making such materials printable requires painstaking trial-and-error by an expert operator, as they typically tend to exhibit peculiar rheological or hysteresis properties. Even in the case of successfully finding the process parameters, there is no guarantee of print-to-print consistency due to material differences between batches. These challenges make closed-loop feedback an attractive option where the process parameters are adjusted on-the-fly. There are several challenges for designing an efficient controller: the deposition parameters are complex and highly coupled, artifacts occur after long time horizons, simulating the deposition is computationally costly, and learning on hardware is intractable. In this work, we demonstrate the feasibility of learning a closed-loop control policy for additive manufacturing using reinforcement learning. We show that approximate, but efficient, numerical simulation is sufficient as long as it allows learning the behavioral patterns of deposition that translate to real-world experiences. In combination with reinforcement learning, our model can be used to discover control policies that outperform baseline controllers. Furthermore, the recovered policies have a minimal sim-to-real gap. We showcase this by applying our control policy in-vivo on a single-layer printer using low and high viscosity materials.},
  archive      = {J_TOG},
  author       = {Michal Piovarči and Michael Foshey and Jie Xu and Timmothy Erps and Vahid Babaei and Piotr Didyk and Szymon Rusinkiewicz and Wojciech Matusik and Bernd Bickel},
  doi          = {10.1145/3528223.3530144},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {112:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Closed-loop control of direct ink writing via reinforcement learning},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accommodative holography: Improving accommodation response
for perceptually realistic holographic displays. <em>TOG</em>,
<em>41</em>(4), 111:1–15. (<a
href="https://doi.org/10.1145/3528223.3530147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays have gained unprecedented attention as next-generation virtual and augmented reality applications with recent achievements in the realization of a high-contrast image through computer-generated holograms (CGHs). However, these holograms show a high energy concentration in a limited angular spectrum, whereas the holograms with uniformly distributed angular spectrum suffer from a severe speckle noise in the reconstructed images. In this study, we claim that these two physical phenomena attributed to the existing CGHs significantly limit the support of accommodation cues, which is known as one of the biggest advantages of holographic displays. To support the statement, we analyze and evaluate various CGH algorithms with contrast gradients - a change of contrast over the change of the focal diopter of the eye - simulated based on the optical configuration of the display system and human visual perception models. We first introduce two approaches to improve monocular accommodation response in holographic viewing experience; optical and computational approaches to provide holographic images with sufficient contrast gradients. We design and conduct user experiments with our prototype of holographic near-eye displays, validating the deficient support of accommodation cues in the existing CGH algorithms and demonstrating the feasibility of the proposed solutions with significant improvements on accommodative gains.},
  archive      = {J_TOG},
  author       = {Dongyeon Kim and Seung-Woo Nam and Byounghyo Lee and Jong-Mo Seo and Byoungho Lee},
  doi          = {10.1145/3528223.3530147},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {111:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Accommodative holography: Improving accommodation response for perceptually realistic holographic displays},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint neural phase retrieval and compression for energy- and
computation-efficient holography on the edge. <em>TOG</em>,
<em>41</em>(4), 110:1–16. (<a
href="https://doi.org/10.1145/3528223.3530070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76\% computation and consequently 83\% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.},
  archive      = {J_TOG},
  author       = {Yujie Wang and Praneeth Chakravarthula and Qi Sun and Baoquan Chen},
  doi          = {10.1145/3528223.3530070},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {110:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Joint neural phase retrieval and compression for energy- and computation-efficient holography on the edge},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural jacobian fields: Learning intrinsic mappings of
arbitrary meshes. <em>TOG</em>, <em>41</em>(4), 109:1–17. (<a
href="https://doi.org/10.1145/3528223.3530141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a framework designed to accurately predict piecewise linear mappings of arbitrary meshes via a neural network, enabling training and evaluating over heterogeneous collections of meshes that do not share a triangulation, as well as producing highly detail-preserving maps whose accuracy exceeds current state of the art. The framework is based on reducing the neural aspect to a prediction of a matrix for a single given point, conditioned on a global shape descriptor. The field of matrices is then projected onto the tangent bundle of the given mesh, and used as candidate jacobians for the predicted map. The map is computed by a standard Poisson solve, implemented as a differentiable layer with cached pre-factorization for efficient training. This construction is agnostic to the triangulation of the input, thereby enabling applications on datasets with varying triangulations. At the same time, by operating in the intrinsic gradient domain of each individual mesh, it allows the framework to predict highly-accurate mappings. We validate these properties by conducting experiments over a broad range of scenarios, from semantic ones such as morphing, registration, and deformation transfer, to optimization-based ones, such as emulating elastic deformations and contact correction, as well as being the first work, to our knowledge, to tackle the task of learning to compute UV parameterizations of arbitrary meshes. The results exhibit the high accuracy of the method as well as its versatility, as it is readily applied to the above scenarios without any changes to the framework.},
  archive      = {J_TOG},
  author       = {Noam Aigerman and Kunal Gupta and Vladimir G. Kim and Siddhartha Chaudhuri and Jun Saito and Thibault Groueix},
  doi          = {10.1145/3528223.3530141},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {109:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural jacobian fields: Learning intrinsic mappings of arbitrary meshes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DEF: Deep estimation of sharp geometric features in 3D
shapes. <em>TOG</em>, <em>41</em>(4), 108:1–22. (<a
href="https://doi.org/10.1145/3528223.3530140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Deep Estimators of Features (DEFs), a learning-based framework for predicting sharp geometric features in sampled 3D shapes. Differently from existing data-driven methods, which reduce this problem to feature classification, we propose to regress a scalar field representing the distance from point samples to the closest feature line on local patches. Our approach is the first that scales to massive point clouds by fusing distance-to-feature estimates obtained on individual patches. We extensively evaluate our approach against related state-of-the-art methods on newly proposed synthetic and real-world 3D CAD model benchmarks. Our approach not only outperforms these (with improvements in Recall and False Positives Rates), but generalizes to real-world scans after training our model on synthetic data and fine-tuning it on a small dataset of scanned data. We demonstrate a downstream application, where we reconstruct an explicit representation of straight and curved sharp feature lines from range scan data. We make code, pre-trained models, and our training and evaluation datasets available at https://github.com/artonson/def.},
  archive      = {J_TOG},
  author       = {Albert Matveev and Ruslan Rakhimov and Alexey Artemov and Gleb Bobrovskikh and Vage Egiazarian and Emil Bogomolov and Daniele Panozzo and Denis Zorin and Evgeny Burnaev},
  doi          = {10.1145/3528223.3530140},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {108:1–22},
  shortjournal = {ACM Trans. Graph.},
  title        = {DEF: Deep estimation of sharp geometric features in 3D shapes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spelunking the deep: Guaranteed queries on general neural
implicit surfaces via range analysis. <em>TOG</em>, <em>41</em>(4),
107:1–16. (<a href="https://doi.org/10.1145/3528223.3530155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.},
  archive      = {J_TOG},
  author       = {Nicholas Sharp and Alec Jacobson},
  doi          = {10.1145/3528223.3530155},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {107:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spelunking the deep: Guaranteed queries on general neural implicit surfaces via range analysis},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPAGHETTI: Editing implicit shapes through part aware
generation. <em>TOG</em>, <em>41</em>(4), 106:1–20. (<a
href="https://doi.org/10.1145/3528223.3530084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for E diting I mplicit S hapes T hrough P art A ware G enera T ion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes. Our code, editing user interface demo and pre-trained models are available at github.com/amirhertz/spaghetti.},
  archive      = {J_TOG},
  author       = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
  doi          = {10.1145/3528223.3530084},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {106:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {SPAGHETTI: Editing implicit shapes through part aware generation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeltaConv: Anisotropic operators for geometric deep learning
on point clouds. <em>TOG</em>, <em>41</em>(4), 105:1–10. (<a
href="https://doi.org/10.1145/3528223.3530166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from 3D point-cloud data has rapidly gained momentum, motivated by the success of deep learning on images and the increased availability of 3D data. In this paper, we aim to construct anisotropic convolution layers that work directly on the surface derived from a point cloud. This is challenging because of the lack of a global coordinate system for tangential directions on surfaces. We introduce DeltaConv, a convolution layer that combines geometric operators from vector calculus to enable the construction of anisotropic filters on point clouds. Because these operators are defined on scalar- and vector-fields, we separate the network into a scalar- and a vector-stream, which are connected by the operators. The vector stream enables the network to explicitly represent, evaluate, and process directional information. Our convolutions are robust and simple to implement and match or improve on state-of-the-art approaches on several benchmarks, while also speeding up training and inference.},
  archive      = {J_TOG},
  author       = {Ruben Wiersma and Ahmad Nasikun and Elmar Eisemann and Klaus Hildebrandt},
  doi          = {10.1145/3528223.3530166},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {105:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeltaConv: Anisotropic operators for geometric deep learning on point clouds},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural dual contouring. <em>TOG</em>, <em>41</em>(4),
104:1–13. (<a href="https://doi.org/10.1145/3528223.3530108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce neural dual contouring (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface. During experiments with five prominent datasets, we find that NDC, when trained on one of the datasets, generalizes well to the others. Furthermore, NDC provides better surface reconstruction accuracy, feature preservation, output complexity, triangle quality, and inference time in comparison to previous learned (e.g., neural marching cubes, convolutional occupancy networks) and traditional (e.g., Poisson) methods. Code and data are available at https://github.com/czq142857/NDC.},
  archive      = {J_TOG},
  author       = {Zhiqin Chen and Andrea Tagliasacchi and Thomas Funkhouser and Hao Zhang},
  doi          = {10.1145/3528223.3530108},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {104:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural dual contouring},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual octree graph networks for learning adaptive volumetric
shape representations. <em>TOG</em>, <em>41</em>(4), 103:1–15. (<a
href="https://doi.org/10.1145/3528223.3530087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an adaptive deep representation of volumetric fields of 3D shapes and an efficient approach to learn this deep representation for high-quality 3D shape reconstruction and auto-encoding. Our method encodes the volumetric field of a 3D shape with an adaptive feature volume organized by an octree and applies a compact multilayer perceptron network for mapping the features to the field value at each 3D position. An encoder-decoder network is designed to learn the adaptive feature volume based on the graph convolutions over the dual graph of octree nodes. The core of our network is a new graph convolution operator defined over a regular grid of features fused from irregular neighboring octree nodes at different levels, which not only reduces the computational and memory cost of the convolutions over irregular neighboring octree nodes, but also improves the performance of feature learning. Our method effectively encodes shape details, enables fast 3D shape reconstruction, and exhibits good generality for modeling 3D shapes out of training categories. We evaluate our method on a set of reconstruction tasks of 3D shapes and scenes and validate its superiority over other existing approaches. Our code, data, and trained models are available at https://wang-ps.github.io/dualocnn.},
  archive      = {J_TOG},
  author       = {Peng-Shuai Wang and Yang Liu and Xin Tong},
  doi          = {10.1145/3528223.3530087},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {103:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dual octree graph networks for learning adaptive volumetric shape representations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instant neural graphics primitives with a multiresolution
hash encoding. <em>TOG</em>, <em>41</em>(4), 102:1–15. (<a
href="https://doi.org/10.1145/3528223.3530127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
  archive      = {J_TOG},
  author       = {Thomas Müller and Alex Evans and Christoph Schied and Alexander Keller},
  doi          = {10.1145/3528223.3530127},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {102:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Instant neural graphics primitives with a multiresolution hash encoding},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural rendering in a room: Amodal 3D understanding and
free-viewpoint rendering for the closed scene composed of pre-captured
objects. <em>TOG</em>, <em>41</em>(4), 101:1–10. (<a
href="https://doi.org/10.1145/3528223.3530163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We, as human beings, can understand and picture a familiar scene from arbitrary viewpoints given a single image, whereas this is still a grand challenge for computers. We hereby present a novel solution to mimic such human perception capability based on a new paradigm of amodal 3D scene understanding with neural rendering for a closed scene. Specifically, we first learn the prior knowledge of the objects in a closed scene via an offline stage, which facilitates an online stage to understand the room with unseen furniture arrangement. During the online stage, given a panoramic image of the scene in different layouts, we utilize a holistic neural-rendering-based optimization framework to efficiently estimate the correct 3D scene layout and deliver realistic free-viewpoint rendering. In order to handle the domain gap between the offline and online stage, our method exploits compositional neural rendering techniques for data augmentation in the offline training. The experiments on both synthetic and real datasets demonstrate that our two-stage design achieves robust 3D scene understanding and outperforms competing methods by a large margin, and we also show that our realistic free-viewpoint rendering enables various applications, including scene touring and editing. Code and data are available on the project webpage: https://zju3dv.github.io/nr_in_a_room/.},
  archive      = {J_TOG},
  author       = {Bangbang Yang and Yinda Zhang and Yijin Li and Zhaopeng Cui and Sean Fanello and Hujun Bao and Guofeng Zhang},
  doi          = {10.1145/3528223.3530163},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {101:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural rendering in a room: Amodal 3D understanding and free-viewpoint rendering for the closed scene composed of pre-captured objects},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Egocentric scene reconstruction from an omnidirectional
video. <em>TOG</em>, <em>41</em>(4), 100:1–12. (<a
href="https://doi.org/10.1145/3528223.3530074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional videos capture environmental scenes effectively, but they have rarely been used for geometry reconstruction. In this work, we propose an egocentric 3D reconstruction method that can acquire scene geometry with high accuracy from a short egocentric omnidirectional video. To this end, we first estimate per-frame depth using a spherical disparity network. We then fuse per-frame depth estimates into a novel spherical binoctree data structure that is specifically designed to tolerate spherical depth estimation errors. By subdividing the spherical space into binary tree and octree nodes that represent spherical frustums adaptively, the spherical binoctree effectively enables egocentric surface geometry reconstruction for environmental scenes while simultaneously assigning high-resolution nodes for closely observed surfaces. This allows to reconstruct an entire scene from a short video captured with a small camera trajectory. Experimental results validate the effectiveness and accuracy of our approach for reconstructing the 3D geometry of environmental scenes from short egocentric omnidirectional video inputs. We further demonstrate various applications using a conventional omnidirectional camera, including novel-view synthesis, object insertion, and relighting of scenes using reconstructed 3D models with texture.},
  archive      = {J_TOG},
  author       = {Hyeonjoong Jang and Andréas Meuleman and Dahyun Kang and Donggun Kim and Christian Richardt and Min H. Kim},
  doi          = {10.1145/3528223.3530074},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {100:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Egocentric scene reconstruction from an omnidirectional video},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADOP: Approximate differentiable one-pixel point rendering.
<em>TOG</em>, <em>41</em>(4), 99:1–14. (<a
href="https://doi.org/10.1145/3528223.3530122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present ADOP, a novel point-based, differentiable neural rendering pipeline. Like other neural renderers, our system takes as input calibrated camera images and a proxy geometry of the scene, in our case a point cloud. To generate a novel view, the point cloud is rasterized with learned feature vectors as colors and a deep neural network fills the remaining holes and shades each output pixel. The rasterizer renders points as one-pixel splats, which makes it very fast and allows us to compute gradients with respect to all relevant input parameters efficiently. Furthermore, our pipeline contains a fully differentiable physically-based photometric camera model, including exposure, white balance, and a camera response function. Following the idea of inverse rendering, we use our renderer to refine its input in order to reduce inconsistencies and optimize the quality of its output. In particular, we can optimize structural parameters like the camera pose, lens distortions, point positions and features, and a neural environment map, but also photometric parameters like camera response function, vignetting, and per-image exposure and white balance. Because our pipeline includes photometric parameters, e.g. exposure and camera response function, our system can smoothly handle input images with varying exposure and white balance, and generates high-dynamic range output. We show that due to the improved input, we can achieve high render quality, also for difficult input, e.g. with imperfect camera calibrations, inaccurate proxy geometry, or varying exposure. As a result, a simpler and thus faster deep neural network is sufficient for reconstruction. In combination with the fast point rasterization, ADOP achieves real-time rendering rates even for models with well over 100M points. https://github.com/darglein/ADOP},
  archive      = {J_TOG},
  author       = {Darius Rückert and Linus Franke and Marc Stamminger},
  doi          = {10.1145/3528223.3530122},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {99:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ADOP: Approximate differentiable one-pixel point rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable neural indoor scene rendering. <em>TOG</em>,
<em>41</em>(4), 98:1–16. (<a
href="https://doi.org/10.1145/3528223.3530153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m 2 . Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.},
  archive      = {J_TOG},
  author       = {Xiuchao Wu and Jiamin Xu and Zihan Zhu and Hujun Bao and Qixing Huang and James Tompkin and Weiwei Xu},
  doi          = {10.1145/3528223.3530153},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {98:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scalable neural indoor scene rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning high-DOF reaching-and-grasping via dynamic
representation of gripper-object interaction. <em>TOG</em>,
<em>41</em>(4), 97:1–14. (<a
href="https://doi.org/10.1145/3528223.3530091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We approach the problem of high-DOF reaching-and-grasping via learning joint planning of grasp and motion with deep reinforcement learning. To resolve the sample efficiency issue in learning the high-dimensional and complex control of dexterous grasping, we propose an effective representation of grasping state characterizing the spatial interaction between the gripper and the target object. To represent gripper-object interaction, we adopt Interaction Bisector Surface (IBS) which is the Voronoi diagram between two close by 3D geometric objects and has been successfully applied in characterizing spatial relations between 3D objects. We found that IBS is surprisingly effective as a state representation since it well informs the finegrained control of each finger with spatial relation against the target object. This novel grasp representation, together with several technical contributions including a fast IBS approximation, a novel vector-based reward and an effective training strategy, facilitate learning a strong control model of high-DOF grasping with good sample efficiency, dynamic adaptability, and cross-category generality. Experiments show that it generates high-quality dexterous grasp for complex shapes with smooth grasping motions. Code and data for this paper are at https://github.com/qijinshe/IBS-Grasping.},
  archive      = {J_TOG},
  author       = {Qijin She and Ruizhen Hu and Juzhan Xu and Min Liu and Kai Xu and Hui Huang},
  doi          = {10.1145/3528223.3530091},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {97:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning high-DOF reaching-and-grasping via dynamic representation of gripper-object interaction},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics-based character controllers using conditional VAEs.
<em>TOG</em>, <em>41</em>(4), 96:1–12. (<a
href="https://doi.org/10.1145/3528223.3530067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality motion capture datasets are now publicly available, and researchers have used them to create kinematics-based controllers that can generate plausible and diverse human motions without conditioning on specific goals (i.e., a task-agnostic generative model). In this paper, we present an algorithm to build such controllers for physically simulated characters having many degrees of freedom. Our physics-based controllers are learned by using conditional VAEs, which can perform a variety of behaviors that are similar to motions in the training dataset. The controllers are robust enough to generate more than a few minutes of motion without conditioning on specific goals and to allow many complex downstream tasks to be solved efficiently. To show the effectiveness of our method, we demonstrate controllers learned from several different motion capture databases and use them to solve a number of downstream tasks that are challenging to learn controllers that generate natural-looking motions from scratch. We also perform ablation studies to demonstrate the importance of the elements of the algorithm. Code and data for this paper are available at: https://github.com/facebookresearch/PhysicsVAE},
  archive      = {J_TOG},
  author       = {Jungdam Won and Deepak Gopinath and Jessica Hodgins},
  doi          = {10.1145/3528223.3530067},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {96:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physics-based character controllers using conditional VAEs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to use chopsticks in diverse gripping styles.
<em>TOG</em>, <em>41</em>(4), 95:1–17. (<a
href="https://doi.org/10.1145/3528223.3530057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning dexterous manipulation skills is a long-standing challenge in computer graphics and robotics, especially when the task involves complex and delicate interactions between the hands, tools and objects. In this paper, we focus on chopsticks-based object relocation tasks, which are common yet demanding. The key to successful chopsticks skills is steady gripping of the sticks that also supports delicate maneuvers. We automatically discover physically valid chopsticks holding poses by Bayesian Optimization (BO) and Deep Reinforcement Learning (DRL), which works for multiple gripping styles and hand morphologies without the need of example data. Given as input the discovered gripping poses and desired objects to be moved, we build physics-based hand controllers to accomplish relocation tasks in two stages. First, kinematic trajectories are synthesized for the chopsticks and hand in a motion planning stage. The key components of our motion planner include a grasping model to select suitable chopsticks configurations for grasping the object, and a trajectory optimization module to generate collision-free chopsticks trajectories. Then we train physics-based hand controllers through DRL again to track the desired kinematic trajectories produced by the motion planner. We demonstrate the capabilities of our framework by relocating objects of various shapes and sizes, in diverse gripping styles and holding positions for multiple hand morphologies. Our system achieves faster learning speed and better control robustness, when compared to vanilla systems that attempt to learn chopstick-based skills without a gripping pose optimization module and/or without a kinematic motion planner. Our code and models are available at this link. 1},
  archive      = {J_TOG},
  author       = {Zeshi Yang and Kangkang Yin and Libin Liu},
  doi          = {10.1145/3528223.3530057},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {95:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to use chopsticks in diverse gripping styles},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASE: Large-scale reusable adversarial skill embeddings for
physically simulated characters. <em>TOG</em>, <em>41</em>(4), 94:1–17.
(<a href="https://doi.org/10.1145/3528223.3530110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.},
  archive      = {J_TOG},
  author       = {Xue Bin Peng and Yunrong Guo and Lina Halper and Sergey Levine and Sanja Fidler},
  doi          = {10.1145/3528223.3530110},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {94:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {ASE: Large-scale reusable adversarial skill embeddings for physically simulated characters},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Free2CAD: Parsing freehand drawings into CAD commands.
<em>TOG</em>, <em>41</em>(4), 93:1–16. (<a
href="https://doi.org/10.1145/3528223.3530133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CAD modeling, despite being the industry-standard, remains restricted to usage by skilled practitioners due to two key barriers. First, the user must be able to mentally parse a final shape into a valid sequence of supported CAD commands; and second, the user must be sufficiently conversant with CAD software packages to be able to execute the corresponding CAD commands. As a step towards addressing both these challenges, we present Free2CAD wherein the user can simply sketch the final shape and our system parses the input strokes into a sequence of commands expressed in a simplified CAD language. When executed, these commands reproduce the sketched object. Technically, we cast sketch-based CAD modeling as a sequence-to-sequence translation problem, for which we leverage the powerful Transformers neural network architecture. Given the sequence of pen strokes as input, we introduce the new task of grouping strokes that correspond to individual CAD operations. We combine stroke grouping with geometric fitting of the operation parameters, such that intermediate groups are geometrically corrected before being reused, as context, for subsequent steps in the sequence inference. Although trained on synthetically-generated data, we demonstrate that Free2CAD generalizes to sketches created from real-world CAD models as well as to sketches drawn by novice users. Code and data are at https://github.com/Enigma-li/Free2CAD.},
  archive      = {J_TOG},
  author       = {Changjian Li and Hao Pan and Adrien Bousseau and Niloy J. Mitra},
  doi          = {10.1145/3528223.3530133},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {93:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Free2CAD: Parsing freehand drawings into CAD commands},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WallPlan: Synthesizing floorplans by learning to generate
wall graphs. <em>TOG</em>, <em>41</em>(4), 92:1–14. (<a
href="https://doi.org/10.1145/3528223.3530135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floorplan generation has drawn widespread interest in the community. Recent learning-based methods for generating realistic floorplans have made significant progress while a complex heuristic post-processing is still necessary to obtain desired results. In this paper, we propose a novel wall-oriented method, called WallPlan , for automatically and efficiently generating plausible floorplans from various design constraints. We pioneer the representation of the floorplan as a wall graph with room labels and consider the floorplan generation as a graph generation. Given the boundary as input, we first initialize the boundary with windows predicted by WinNet. Then a graph generation network GraphNet and semantics prediction network LabelNet are coupled to generate the wall graph progressively by imitating graph traversal. WallPlan can be applied for practical architectural designs, especially the wall-based constraints. We conduct ablation experiments, qualitative evaluations, quantitative comparisons, and perceptual studies to evaluate our method&#39;s feasibility, efficacy, and versatility. Intensive experiments demonstrate our method requires no post-processing, producing higher quality floorplans than state-of-the-art techniques.},
  archive      = {J_TOG},
  author       = {Jiahui Sun and Wenming Wu and Ligang Liu and Wenjie Min and Gaofeng Zhang and Liping Zheng},
  doi          = {10.1145/3528223.3530135},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {92:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {WallPlan: Synthesizing floorplans by learning to generate wall graphs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive augmented reality storytelling guided by scene
semantics. <em>TOG</em>, <em>41</em>(4), 91:1–15. (<a
href="https://doi.org/10.1145/3528223.3530061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel interactive augmented reality (AR) storytelling approach guided by indoor scene semantics. Our approach automatically populates virtual contents in real-world environments to deliver AR stories, which match both the story plots and scene semantics. During the storytelling process, a player can participate as a character in the story. Meanwhile, the behaviors of the virtual characters and the placement of the virtual items adapt to the player&#39;s actions. An input raw story is represented as a sequence of events, which contain high-level descriptions of the characters&#39; states, and is converted into a graph representation with automatically supplemented low-level spatial details. Our hierarchical story sampling approach samples realistic character behaviors that fit the story contexts through optimizations; and an animator, which estimates and prioritizes the player&#39;s actions, animates the virtual characters to tell the story in AR. Through experiments and a user study, we validated the effectiveness of our approach for AR storytelling in different environments.},
  archive      = {J_TOG},
  author       = {Changyang Li and Wanwan Li and Haikun Huang and Lap-Fai Yu},
  doi          = {10.1145/3528223.3530061},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {91:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive augmented reality storytelling guided by scene semantics},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic optimal space partitioning for redirected walking in
multi-user environment. <em>TOG</em>, <em>41</em>(4), 90:1–14. (<a
href="https://doi.org/10.1145/3528223.3530113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-user Redirected Walking (RDW), the space subdivision method divides a shared physical space into sub-spaces and allocates a sub-space to each user. While this approach has the advantage of precluding any collisions between users, the conventional space subdivision method suffers from frequent boundary resets due to the reduction of available space per user. To address this challenge, in this study, we propose a space subdivision method called Optimal Space Partitioning (OSP) that dynamically divides the shared physical space in real-time. By exploiting spatial information of the physical and virtual environment, OSP predicts the movement of users and divides the shared physical space into optimal sub-spaces separated with shutters. Our OSP framework is trained using deep reinforcement learning to allocate optimal sub-space to each user and provide optimal steering. Our experiments demonstrate that OSP provides higher sense of immersion to users by minimizing the total number of reset counts, while preserving the advantage of the existing space subdivision strategy: ensuring better safety to users by completely eliminating the possibility of any collisions between users beforehand. Our project is available at https://github.com/AppleParfait/OSP-Archive.},
  archive      = {J_TOG},
  author       = {Sang-Bin Jeon and Soon-Uk Kwon and June-Young Hwang and Yong-Hun Cho and Hayeon Kim and Jinhyung Park and In-Kwon Lee},
  doi          = {10.1145/3528223.3530113},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {90:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic optimal space partitioning for redirected walking in multi-user environment},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid design of articulated objects. <em>TOG</em>,
<em>41</em>(4), 89:1–8. (<a
href="https://doi.org/10.1145/3528223.3530092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing articulated objects is challenging because, unlike with static objects, it requires complex decisions to be made regarding the form, parts, rig, poses, and motion. We present a novel 3D sketching system for rapidly authoring concepts of articulated objects for the early stages of design, when designers make such decisions. Compared to existing CAD software, which focuses on slowly but elaborately producing models consisting of precise surfaces and volumes, our system focuses on quickly but roughly producing models consisting of key curves through a small set of coherent pen and multi-touch gestures. We found that professional designers could easily learn and use our system and author compelling concepts in a short time, showing that 3D sketching can be extended to designing articulated objects and is generally applicable in film, animation, game, and product design.},
  archive      = {J_TOG},
  author       = {Joon Hyub Lee and Hanbit Kim and Seok-Hyung Bae},
  doi          = {10.1145/3528223.3530092},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {89:1–8},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rapid design of articulated objects},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Piecewise-smooth surface fitting onto unstructured 3D
sketches. <em>TOG</em>, <em>41</em>(4), 88:1–16. (<a
href="https://doi.org/10.1145/3528223.3530100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to transform unstructured 3D sketches into piecewise smooth surfaces that preserve sketched geometric features. Immersive 3D drawing and sketch-based 3D modeling applications increasingly produce imperfect and unstructured collections of 3D strokes as design output. These 3D sketches are readily perceived as piecewise smooth surfaces by viewers, but are poorly handled by existing 3D surface techniques tailored to well-connected curve networks or sparse point sets. Our algorithm is aligned with human tendency to imagine the strokes as a small set of simple smooth surfaces joined along stroke boundaries. Starting with an initial proxy surface, we iteratively segment the surface into smooth patches joined sharply along some strokes, and optimize these patches to fit surrounding strokes. Our evaluation is fourfold: we demonstrate the impact of various algorithmic parameters, we evaluate our method on synthetic sketches with known ground truth surfaces, we compare to prior art, and we show compelling results on more than 50 designs from a diverse set of 3D sketch sources.},
  archive      = {J_TOG},
  author       = {Emilie Yu and Rahul Arora and J. Andreas Bærentzen and Karan Singh and Adrien Bousseau},
  doi          = {10.1145/3528223.3530100},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {88:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Piecewise-smooth surface fitting onto unstructured 3D sketches},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting viewer-perceived intended vector sketch
connectivity. <em>TOG</em>, <em>41</em>(4), 87:1–11. (<a
href="https://doi.org/10.1145/3528223.3530097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many sketch processing applications target precise vector drawings with accurately specified stroke intersections, yet free-form artist drawn sketches are typically inexact: strokes that are intended to intersect often stop short of doing so. While human observers easily perceive the artist intended stroke connectivity, manually, or even semi-manually, correcting drawings to generate correctly connected outputs is tedious and highly time consuming. We propose a novel, robust algorithm that extracts viewer-perceived stroke connectivity from inexact free-form vector drawings by leveraging observations about local and global factors that impact human perception of inter-stroke connectivity. We employ the identified local cues to train classifiers that assess the likelihood that pairs of strokes are perceived as forming end-to-end or T- junctions based on local context. We then use these classifiers within an incremental framework that combines classifier provided likelihoods with a more global, contextual and closure-based, analysis. We demonstrate our method on over 95 diversely sourced inputs, and validate it via a series of perceptual studies; participants prefer our outputs over the closest alternative by a factor of 9 to 1.},
  archive      = {J_TOG},
  author       = {Jerry Yin and Chenxi Liu and Rebecca Lin and Nicholas Vining and Helge Rhodin and Alla Sheffer},
  doi          = {10.1145/3528223.3530097},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {87:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Detecting viewer-perceived intended vector sketch connectivity},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CLIPasso: Semantically-aware object sketching. <em>TOG</em>,
<em>41</em>(4), 86:1–11. (<a
href="https://doi.org/10.1145/3528223.3530068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of Bézier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.},
  archive      = {J_TOG},
  author       = {Yael Vinker and Ehsan Pajouheshgar and Jessica Y. Bo and Roman Christian Bachmann and Amit Haim Bermano and Daniel Cohen-Or and Amir Zamir and Ariel Shamir},
  doi          = {10.1145/3528223.3530068},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {86:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {CLIPasso: Semantically-aware object sketching},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sketch2Pose: Estimating a 3D character pose from a bitmap
sketch. <em>TOG</em>, <em>41</em>(4), 85:1–15. (<a
href="https://doi.org/10.1145/3528223.3530106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist&#39;s intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.},
  archive      = {J_TOG},
  author       = {Kirill Brodt and Mikhail Bessmeltsev},
  doi          = {10.1145/3528223.3530106},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {85:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sketch2Pose: Estimating a 3D character pose from a bitmap sketch},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MatBuilder: Mastering sampling uniformity over projections.
<em>TOG</em>, <em>41</em>(4), 84:1–13. (<a
href="https://doi.org/10.1145/3528223.3530063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications ranging from quasi-Monte Carlo integration over optimal control to neural networks benefit from high-dimensional, highly uniform samples. In the case of computer graphics, and more particularly in rendering, despite the need for uniformity, several sub-problems expose a low-dimensional structure. In this context, mastering sampling uniformity over projections while preserving high-dimensional uniformity has been intrinsically challenging. This difficulty may explain the relatively small number of mathematical constructions for such samplers. We propose a novel approach by showing that uniformity constraints can be expressed as an integer linear program that results in a sampler with the desired properties. As it turns out, complex constraints are easy to describe by means of stratification and sequence properties of digital nets. Formalized using generator matrix determinants, our new MatBuilder software solves the set of constraints by iterating the linear integer program solver in a greedy fashion to compute a problem-specific set of generator matrices that can be used as a drop-in replacement in the popular digital net samplers. The samplers created by MatBuilder achieve the uniformity of classic low discrepancy sequences. More importantly, we demonstrate the benefit of the unprecedented versatility of our constraint approach with respect to low-dimensional problem structure for several applications.},
  archive      = {J_TOG},
  author       = {Loïs Paulin and Nicolas Bonneel and David Coeurjolly and Jean-Claude Iehl and Alexander Keller and Victor Ostromoukhov},
  doi          = {10.1145/3528223.3530063},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {84:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {MatBuilder: Mastering sampling uniformity over projections},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantically supervised appearance decomposition for virtual
staging from a single panorama. <em>TOG</em>, <em>41</em>(4), 83:1–15.
(<a href="https://doi.org/10.1145/3528223.3530148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel approach to decompose a single panorama of an empty indoor environment into four appearance components: specular, direct sunlight, diffuse and diffuse ambient without direct sunlight. Our system is weakly supervised by automatically generated semantic maps (with floor, wall, ceiling, lamp, window and door labels) that have shown success on perspective views and are trained for panoramas using transfer learning without any further annotations. A GAN-based approach supervised by coarse information obtained from the semantic map extracts specular reflection and direct sunlight regions on the floor and walls. These lighting effects are removed via a similar GAN-based approach and a semantic-aware inpainting step. The appearance decomposition enables multiple applications including sun direction estimation, virtual furniture insertion, floor material replacement, and sun direction change, providing an effective tool for virtual home staging. We demonstrate the effectiveness of our approach on a large and recently released dataset of panoramas of empty homes.},
  archive      = {J_TOG},
  author       = {Tiancheng Zhi and Bowei Chen and Ivaylo Boyadzhiev and Sing Bing Kang and Martial Hebert and Srinivasa G. Narasimhan},
  doi          = {10.1145/3528223.3530148},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {83:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semantically supervised appearance decomposition for virtual staging from a single panorama},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape dithering for 3D printing. <em>TOG</em>,
<em>41</em>(4), 82:1–12. (<a
href="https://doi.org/10.1145/3528223.3530129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient, purely geometric, algorithmic, and parameter free approach to improve surface quality and accuracy in voxel-controlled 3D printing by counteracting quantization artifacts. Such artifacts arise due to the discrete voxel sampling of the continuous shape used to control the 3D printer, and are characterized by low-frequency geometric patterns on surfaces of any orientation. They are visually disturbing, particularly on small prints or smooth surfaces, and adversely affect the fatigue behavior of printed parts. We use implicit shape dithering, displacing the part&#39;s signed distance field with a high-frequent signal whose amplitude is adapted to the (anisotropic) print resolution. We expand the reverse generalized Fourier slice theorem by shear transforms, which we leverage to optimize a 3D blue-noise mask to generate the anisotropic dither signal. As a point process it is efficient and does not adversely affect 3D halftoning. We evaluate our approach for efficiency, geometric accuracy and show its advantages over the state of the art.},
  archive      = {J_TOG},
  author       = {Mostafa Morsy Abdelkader Morsy and Alan Brunton and Philipp Urban},
  doi          = {10.1145/3528223.3530129},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {82:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shape dithering for 3D printing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EARS: Efficiency-aware russian roulette and splitting.
<em>TOG</em>, <em>41</em>(4), 81:1–14. (<a
href="https://doi.org/10.1145/3528223.3530168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Russian roulette and splitting are widely used techniques to increase the efficiency of Monte Carlo estimators. But, despite their popularity, there is little work on how to best apply them. Most existing approaches rely on simple heuristics based on, e.g., surface albedo and roughness. Their efficiency often hinges on user-controlled parameters. We instead iteratively learn optimal Russian roulette and splitting factors during rendering, using a simple and lightweight data structure. Given perfect estimates of variance and cost, our fixed-point iteration provably converges to the optimal Russian roulette and splitting factors that maximize the rendering efficiency. In our application to unidirectional path tracing, we achieve consistent and significant speed-ups over the state of the art.},
  archive      = {J_TOG},
  author       = {Alexander Rath and Pascal Grittmann and Sebastian Herholz and Philippe Weier and Philipp Slusallek},
  doi          = {10.1145/3528223.3530168},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {81:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {EARS: Efficiency-aware russian roulette and splitting},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiency-aware multiple importance sampling for
bidirectional rendering algorithms. <em>TOG</em>, <em>41</em>(4),
80:1–12. (<a href="https://doi.org/10.1145/3528223.3530126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple importance sampling (MIS) is an indispensable tool in light-transport simulation. It enables robust Monte Carlo integration by combining samples from several techniques. However, it is well understood that such a combination is not always more efficient than using a single sampling technique. Thus a major criticism of complex combined estimators, such as bidirectional path tracing, is that they can be significantly less efficient on common scenes than simpler algorithms like forward path tracing. We propose a general method to improve MIS efficiency: By cheaply estimating the efficiencies of various technique and sample-count combinations, we can pick the best one. The key ingredient is a numerically robust and efficient scheme that uses the samples of one MIS combination to compute the efficiency of multiple other combinations. For example, we can run forward path tracing and use its samples to decide which subset of VCM to enable, and at what sampling rates. The sample count for each technique can be controlled per-pixel or globally. Applied to VCM, our approach enables robust rendering of complex scenes with caustics, without compromising efficiency on simpler scenes.},
  archive      = {J_TOG},
  author       = {Pascal Grittmann and Ömercan Yazici and Iliyan Georgiev and Philipp Slusallek},
  doi          = {10.1145/3528223.3530126},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {80:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficiency-aware multiple importance sampling for bidirectional rendering algorithms},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression-based monte carlo integration. <em>TOG</em>,
<em>41</em>(4), 79:1–14. (<a
href="https://doi.org/10.1145/3528223.3530095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo integration is typically interpreted as an estimator of the expected value using stochastic samples. There exists an alternative interpretation in calculus where Monte Carlo integration can be seen as estimating a constant function---from the stochastic evaluations of the integrand---that integrates to the original integral. The integral mean value theorem states that this constant function should be the mean (or expectation) of the integrand. Since both interpretations result in the same estimator, little attention has been devoted to the calculus-oriented interpretation. We show that the calculus-oriented interpretation actually implies the possibility of using a more complex function than a constant one to construct a more efficient estimator for Monte Carlo integration. We build a new estimator based on this interpretation and relate our estimator to control variates with least-squares regression on the stochastic samples of the integrand. Unlike prior work, our resulting estimator is provably better than or equal to the conventional Monte Carlo estimator. To demonstrate the strength of our approach, we introduce a practical estimator that can act as a simple drop-in replacement for conventional Monte Carlo integration. We experimentally validate our framework on various light transport integrals. The code is available at https://github.com/iribis/regressionmc.},
  archive      = {J_TOG},
  author       = {Corentin Salaün and Adrien Gruson and Binh-Son Hua and Toshiya Hachisuka and Gurprit Singh},
  doi          = {10.1145/3528223.3530095},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {79:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Regression-based monte carlo integration},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and rendering non-euclidean spaces approximated
with concatenated polytopes. <em>TOG</em>, <em>41</em>(4), 78:1–13. (<a
href="https://doi.org/10.1145/3528223.3530186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A non-Euclidean space is characterized as a manifold with a specific structure that violates Euclid&#39;s postulates. This paper proposes to approximate a manifold with polytopes. Based on the scene designer&#39;s specification, the polytopes are automatically concatenated and embedded in a higher-dimensional Euclidean space. Then, the scene is navigated and rendered via novel methods tailored to concatenated polytopes. The proof-of-concept implementation and experiments with it show that the proposed methods bring the virtual-world users unusual and fascinating experiences, which cannot be provided in Euclidean-space applications.},
  archive      = {J_TOG},
  author       = {Seung-Wook Kim and Jaehyung Doh and Junghyun Han},
  doi          = {10.1145/3528223.3530186},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {78:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modeling and rendering non-euclidean spaces approximated with concatenated polytopes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPCBPT: Subspace-based probabilistic connections for
bidirectional path tracing. <em>TOG</em>, <em>41</em>(4), 77:1–14. (<a
href="https://doi.org/10.1145/3528223.3530183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidirectional path tracing (BDPT) can be accelerated by selecting appropriate light sub-paths for connection. However, existing algorithms need to perform frequent distribution reconstruction and have expensive overhead. We present a novel approach, SPCBPT, for probabilistic connections that constructs the light selection distribution in sub-path space. Our approach bins the sub-paths into multiple subspaces and keeps the sub-paths in the same subspace of low discrepancy, wherein the light sub-paths can be selected by a subspace-based two-stage sampling method, i.e., first sampling the light subspace and then resampling the light sub-paths within this subspace. The subspace-based distribution is free of reconstruction and provides efficient light selection at a very low cost. We also propose a method that considers the Multiple Importance Sampling (MIS) term in the light selection and thus obtain an MIS-aware distribution that can minimize the upper bound of variance of the combined estimator. Prior methods typically omit this MIS weights term. We evaluate our algorithm using various benchmarks, and the results show that our approach has superior performance and can significantly reduce the noise compared with the state-of-the-art method.},
  archive      = {J_TOG},
  author       = {Fujia Su and Sheng Li and Guoping Wang},
  doi          = {10.1145/3528223.3530183},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {77:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {SPCBPT: Subspace-based probabilistic connections for bidirectional path tracing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R2E2: Low-latency path tracing of terabyte-scale scenes
using thousands of cloud CPUs. <em>TOG</em>, <em>41</em>(4), 76:1–12.
(<a href="https://doi.org/10.1145/3528223.3530171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we explore the viability of path tracing massive scenes using a &quot;supercomputer&quot; constructed on-the-fly from thousands of small, serverless cloud computing nodes. We present R2E2 (Really Elastic Ray Engine) a scene decomposition-based parallel renderer that rapidly acquires thousands of cloud CPU cores, loads scene geometry from a pre-built scene BVH into the aggregate memory of these nodes in parallel, and performs full path traced global illumination using an inter-node messaging service designed for communicating ray data. To balance ray tracing work across many nodes, R2E2 adopts a service-oriented design that statically replicates geometry and texture data from frequently traversed scene regions onto multiple nodes based on estimates of load, and dynamically assigns ray tracing work to lightly loaded nodes holding the required data. We port pbrt&#39;s ray-scene intersection components to the R2E2 architecture, and demonstrate that scenes with up to a terabyte of geometry and texture data (where as little as 1/250th of the scene can fit on any one node) can be path traced at 4K resolution, in tens of seconds using thousands of tiny serverless nodes on the AWS Lambda platform.},
  archive      = {J_TOG},
  author       = {Sadjad Fouladi and Brennan Shacklett and Fait Poms and Arjun Arora and Alex Ozdemir and Deepti Raghavan and Pat Hanrahan and Kayvon Fatahalian and Keith Winstein},
  doi          = {10.1145/3528223.3530171},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {76:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {R2E2: Low-latency path tracing of terabyte-scale scenes using thousands of cloud CPUs},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized resampled importance sampling: Foundations of
ReSTIR. <em>TOG</em>, <em>41</em>(4), 75:1–23. (<a
href="https://doi.org/10.1145/3528223.3530158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As scenes become ever more complex and real-time applications embrace ray tracing, path sampling algorithms that maximize quality at low sample counts become vital. Recent resampling algorithms building on Talbot et al.&#39;s [2005] resampled importance sampling (RIS) reuse paths spatiotemporally to render surprisingly complex light transport with a few samples per pixel. These reservoir-based spatiotemporal importance resamplers (ReSTIR) and their underlying RIS theory make various assumptions, including sample independence. But sample reuse introduces correlation , so ReSTIR-style iterative reuse loses most convergence guarantees that RIS theoretically provides. We introduce generalized resampled importance sampling (GRIS) to extend the theory, allowing RIS on correlated samples, with unknown PDFs and taken from varied domains. This solidifies the theoretical foundation, allowing us to derive variance bounds and convergence conditions in ReSTIR-based samplers. It also guides practical algorithm design and enables advanced path reuse between pixels via complex shift mappings. We show a path-traced resampler (ReSTIR PT) running interactively on complex scenes, capturing many-bounce diffuse and specular lighting while shading just one path per pixel. With our new theoretical foundation, we can also modify the algorithm to guarantee convergence for offline renderers.},
  archive      = {J_TOG},
  author       = {Daqi Lin and Markus Kettunen and Benedikt Bitterli and Jacopo Pantaleoni and Cem Yuksel and Chris Wyman},
  doi          = {10.1145/3528223.3530158},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {75:1–23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generalized resampled importance sampling: Foundations of ReSTIR},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASSET: Autoregressive semantic scene editing with
transformers at high resolutions. <em>TOG</em>, <em>41</em>(4), 74:1–12.
(<a href="https://doi.org/10.1145/3528223.3530172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user&#39;s edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer&#39;s attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or fora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches. We present qualitative and quantitative results, along with user studies, demonstrating the effectiveness of our method. Our code and dataset are available at our project page: https://github.com/DifanLiu/ASSET},
  archive      = {J_TOG},
  author       = {Difan Liu and Sandesh Shetty and Tobias Hinz and Matthew Fisher and Richard Zhang and Taesung Park and Evangelos Kalogerakis},
  doi          = {10.1145/3528223.3530172},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {74:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {ASSET: Autoregressive semantic scene editing with transformers at high resolutions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rewriting geometric rules of a GAN. <em>TOG</em>,
<em>41</em>(4), 73:1–16. (<a
href="https://doi.org/10.1145/3528223.3530065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process - the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to &quot;warp&quot; a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing.},
  archive      = {J_TOG},
  author       = {Sheng-Yu Wang and David Bau and Jun-Yan Zhu},
  doi          = {10.1145/3528223.3530065},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {73:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rewriting geometric rules of a GAN},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangling random and cyclic effects in time-lapse
sequences. <em>TOG</em>, <em>41</em>(4), 72:1–13. (<a
href="https://doi.org/10.1145/3528223.3530170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-lapse image sequences offer visually compelling insights into dynamic processes that are too slow to observe in real time. However, playing a long time-lapse sequence back as a video often results in distracting flicker due to random effects, such as weather, as well as cyclic effects, such as the day-night cycle. We introduce the problem of disentangling time-lapse sequences in a way that allows separate, after-the-fact control of overall trends, cyclic effects, and random effects in the images, and describe a technique based on data-driven generative models that achieves this goal. This enables us to &quot;re-render&quot; the sequences in ways that would not be possible with the input images alone. For example, we can stabilize a long sequence to focus on plant growth over many months, under selectable, consistent weather. Our approach is based on Generative Adversarial Networks (GAN) that are conditioned with the time coordinate of the time-lapse sequence. Our architecture and training procedure are designed so that the networks learn to model random variations, such as weather, using the GAN&#39;s latent space, and to disentangle overall trends and cyclic variations by feeding the conditioning time label to the model using Fourier features with specific frequencies. We show that our models are robust to defects in the training data, enabling us to amend some of the practical difficulties in capturing long time-lapse sequences, such as temporary occlusions, uneven frame spacing, and missing frames.},
  archive      = {J_TOG},
  author       = {Erik Härkönen and Miika Aittala and Tuomas Kynkäänniemi and Samuli Laine and Timo Aila and Jaakko Lehtinen},
  doi          = {10.1145/3528223.3530170},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {72:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Disentangling random and cyclic effects in time-lapse sequences},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive rigidification of elastic solids. <em>TOG</em>,
<em>41</em>(4), 71:1–11. (<a
href="https://doi.org/10.1145/3528223.3530124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for reducing the computational cost of elastic solid simulation by treating connected sets of non-deforming elements as rigid bodies. Non-deforming elements are identified as those where the strain rate squared Frobenius norm falls below a threshold for several frames. Rigidification uses a breadth first search to identify connected components while avoiding connections that would form hinges between rigid components. Rigid elements become elastic again when their approximate strain velocity rises above a threshold, which is fast to compute using a single iteration of conjugate gradient with a fixed Laplacian-based incomplete Cholesky preconditioner. With rigidification, the system size to solve at each time step can be greatly reduced, and if all elastic element become rigid, it reduces to solving the rigid body system. We demonstrate our results on a variety of 2D and 3D examples, and show that our method is likewise especially beneficial in contact rich examples.},
  archive      = {J_TOG},
  author       = {Alexandre Mercier-Aubin and Paul G. Kry and Alexandre Winter and David I. W. Levin},
  doi          = {10.1145/3528223.3530124},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {71:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive rigidification of elastic solids},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-centric deformation learning. <em>TOG</em>,
<em>41</em>(4), 70:1–11. (<a
href="https://doi.org/10.1145/3528223.3530182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to machine-learn highly detailed, nonlinear contact deformations for real-time dynamic simulation. We depart from previous deformation-learning strategies, and model contact deformations in a contact-centric manner. This strategy shows excellent generalization with respect to the object&#39;s configuration space, and it allows for simple and accurate learning. We complement the contact-centric learning strategy with two additional key ingredients: learning a continuous vector field of contact deformations, instead of a discrete approximation; and sparsifying the mapping between the contact configuration and contact deformations. These two ingredients further contribute to the accuracy, efficiency, and generalization of the method. We integrate our learning-based contact deformation model with subspace dynamics, showing real-time dynamic simulations with fine contact deformation detail.},
  archive      = {J_TOG},
  author       = {Cristian Romero and Dan Casas and Maurizio M. Chiaramonte and Miguel A. Otaduy},
  doi          = {10.1145/3528223.3530182},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {70:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Contact-centric deformation learning},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penetration-free projective dynamics on the GPU.
<em>TOG</em>, <em>41</em>(4), 69:1–16. (<a
href="https://doi.org/10.1145/3528223.3530069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a GPU algorithm for deformable simulation. Our method offers good computational efficiency and penetration-free guarantee at the same time, which are not common with existing techniques. The main idea is an algorithmic integration of projective dynamics (PD) and incremental potential contact (IPC). PD is a position-based simulation framework, favored for its robust convergence and convenient implementation. We show that PD can be employed to handle the variational optimization with the interior point method e.g., IPC. While conceptually straightforward, this requires a dedicated rework over the collision resolution and the iteration modality to avoid incorrect collision projection with improved numerical convergence. IPC exploits a barrier-based formulation, which yields an infinitely large penalty when the constraint is on the verge of being violated. This mechanism guarantees intersection-free trajectories of deformable bodies during the simulation, as long as they are apart at the rest configuration. On the downside, IPC brings a large amount of nonlinearity to the system, making PD slower to converge. To mitigate this issue, we propose a novel GPU algorithm named A-Jacobi for faster linear solve at the global step of PD. A-Jacobi is based on Jacobi iteration, but it better harvests the computation capacity on modern GPUs by lumping several Jacobi steps into a single iteration. In addition, we also re-design the CCD root finding procedure by using a new minimum-gradient Newton algorithm. Those saved time budgets allow more iterations to accommodate stiff IPC barriers so that the result is both realistic and collision-free. Putting together, our algorithm simulates complicated models of both solids and shells on the GPU at an interactive rate or even in real time.},
  archive      = {J_TOG},
  author       = {Lei Lan and Guanqun Ma and Yin Yang and Changxi Zheng and Minchen Li and Chenfanfu Jiang},
  doi          = {10.1145/3528223.3530069},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {69:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Penetration-free projective dynamics on the GPU},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast evaluation of smooth distance constraints on
co-dimensional geometry. <em>TOG</em>, <em>41</em>(4), 68:1–17. (<a
href="https://doi.org/10.1145/3528223.3530093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method for computing a smooth minimum distance function based on the LogSumExp function for point clouds, edge meshes, triangle meshes, and combinations of all three. We derive blending weights and a modified Barnes-Hut acceleration approach that ensure our method approximates the true distance, and is conservative (points outside the zero isosurface are guaranteed to be outside the surface) and efficient to evaluate for all the above data types. This, in combination with its ability to smooth sparsely sampled and noisy data, like point clouds, shortens the gap between data acquisition and simulation, and thereby enables new applications such as direct, co-dimensional rigid body simulation using unprocessed lidar data.},
  archive      = {J_TOG},
  author       = {Abhishek Madan and David I. W. Levin},
  doi          = {10.1145/3528223.3530093},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {68:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast evaluation of smooth distance constraints on co-dimensional geometry},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affine body dynamics: Fast, stable and intersection-free
simulation of stiff materials. <em>TOG</em>, <em>41</em>(4), 67:1–14.
(<a href="https://doi.org/10.1145/3528223.3530064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating stiff materials in applications where deformations are either not significant or else can safely be ignored is a fundamental task across fields. Rigid body modeling has thus long remained a critical tool and is, by far, the most popular simulation strategy currently employed for modeling stiff solids. At the same time, rigid body methods continue to pose a number of well known challenges and trade-offs including intersections, instabilities, inaccuracies, and/or slow performances that grow with contact-problem complexity. In this paper we revisit the stiff body problem and present ABD, a simple and highly effective affine body dynamics framework, which significantly improves state-of-the-art for simulating stiff-body dynamics. We trace the challenges in rigid-body methods to the necessity of linearizing piecewise-rigid trajectories and subsequent constraints. ABD instead relaxes the unnecessary (and unrealistic) constraint that each body&#39;s motion be exactly rigid with a stiff orthogonality potential, while preserving the rigid body model&#39;s key feature of a small coordinate representation. In doing so ABD replaces piecewise linearization with piecewise linear trajectories. This, in turn, combines the best of both worlds: compact coordinates ensure small, sparse system solves, while piecewise-linear trajectories enable efficient and accurate constraint (contact and joint) evaluations. Beginning with this simple foundation, ABD preserves all guarantees of the underlying IPC model we build it upon, e.g., solution convergence, guaranteed non-intersection, and accurate frictional contact. Over a wide range and scale of simulation problems we demonstrate that ABD brings orders of magnitude performance gains (two- to three-orders on the CPU and an order more when utilizing the GPU, obtaining 10, 000× speedups) over prior IPC-based methods, while maintaining simulation quality and nonintersection of trajectories. At the same time ABD has comparable or faster timings when compared to state-of-the-art rigid body libraries optimized for performance without guarantees, and successfully and efficiently solves challenging simulation problems where both classes of prior rigid body simulation methods fail altogether.},
  archive      = {J_TOG},
  author       = {Lei Lan and Danny M. Kaufman and Minchen Li and Chenfanfu Jiang and Yin Yang},
  doi          = {10.1145/3528223.3530064},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {67:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Affine body dynamics: Fast, stable and intersection-free simulation of stiff materials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified newton barrier method for multibody dynamics.
<em>TOG</em>, <em>41</em>(4), 66:1–14. (<a
href="https://doi.org/10.1145/3528223.3530076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a simulation framework for multibody dynamics via a universal variational integration. Our method naturally supports mixed rigid-deformables and mixed codimensional geometries, while providing guaranteed numerical convergence and accurate resolution of contact, friction, and a wide range of articulation constraints. We unify (1) the treatment of simulation degrees of freedom for rigid and soft bodies by formulating them both in terms of Lagrangian nodal displacements, (2) the handling of general linear equality joint constraints through an efficient change-of-variable strategy, (3) the enforcement of nonlinear articulation constraints based on novel distance potential energies, (4) the resolution of frictional contact between mixed dimensions and bodies with a variational Incremental Potential Contact formulation, and (5) the modeling of generalized restitution through semi-implicit Rayleigh damping. We conduct extensive unit tests and benchmark studies to demonstrate the efficacy of our method.},
  archive      = {J_TOG},
  author       = {Yunuo Chen and Minchen Li and Lei Lan and Hao Su and Yin Yang and Chenfanfu Jiang},
  doi          = {10.1145/3528223.3530076},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {66:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A unified newton barrier method for multibody dynamics},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of yarn-level simulation models for production
fabrics. <em>TOG</em>, <em>41</em>(4), 65:1–15. (<a
href="https://doi.org/10.1145/3528223.3530167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a methodology for inverse-modeling of yarn-level mechanics of cloth, based on the mechanical response of fabrics in the real world. We compiled a database from physical tests of several different knitted fabrics used in the textile industry. These data span different types of complex knit patterns, yarn compositions, and fabric finishes, and the results demonstrate diverse physical properties like stiffness, nonlinearity, and anisotropy. We then develop a system for approximating these mechanical responses with yarn-level cloth simulation. To do so, we introduce an efficient pipeline for converting between fabric-level data and yarn-level simulation, including a novel swatch-level approximation for speeding up computation, and some small-but-necessary extensions to yarn-level models used in computer graphics. The dataset used for this paper can be found at http://mslab.es/projects/YarnLevelFabrics.},
  archive      = {J_TOG},
  author       = {Georg Sperl and Rosa M. Sánchez-Banderas and Manwen Li and Chris Wojtan and Miguel A. Otaduy},
  doi          = {10.1145/3528223.3530167},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {65:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Estimation of yarn-level simulation models for production fabrics},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general two-stage initialization for sag-free deformable
simulations. <em>TOG</em>, <em>41</em>(4), 64:1–13. (<a
href="https://doi.org/10.1145/3528223.3530165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Initializing simulations of deformable objects involves setting the rest state of all internal forces at the rest shape of the object. However, often times the rest shape is not explicitly provided. In its absence, it is common to initialize by treating the given initial shape as the rest shape. This leads to sagging, the undesirable deformation under gravity as soon as the simulation begins. Prior solutions to sagging are limited to specific simulation systems and material models, most of them cannot handle frictional contact, and they require solving expensive global nonlinear optimization problems. We introduce a novel solution to the sagging problem that can be applied to a variety of simulation systems and materials. The key feature of our approach is that we avoid solving a global nonlinear optimization problem by performing the initialization in two stages. First, we use a global linear optimization for static equilibrium. Any nonlinearity of the material definition is handled in the local stage, which solves many small local problems efficiently and in parallel. Notably, our method can properly handle frictional contact orders of magnitude faster than prior work. We show that our approach can be applied to various simulation systems by presenting examples with mass-spring systems, cloth simulations, the finite element method, the material point method, and position-based dynamics.},
  archive      = {J_TOG},
  author       = {Jerry Hsu and Nghia Truong and Cem Yuksel and Kui Wu},
  doi          = {10.1145/3528223.3530165},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {64:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A general two-stage initialization for sag-free deformable simulations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A GPU-based multilevel additive schwarz preconditioner for
cloth and deformable body simulation. <em>TOG</em>, <em>41</em>(4),
63:1–14. (<a href="https://doi.org/10.1145/3528223.3530085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we wish to push the limit of real-time cloth and deformable body simulation to a higher level with 50K to 500K vertices, based on the development of a novel GPU-based multilevel additive Schwarz (MAS) pre-conditioner. Similar to other preconditioners under the MAS framework, our preconditioner naturally adopts multilevel and domain decomposition concepts. But contrary to previous works, we advocate the use of small, non-overlapping domains that can well explore the parallel computing power on a GPU. Based on this idea, we investigate and invent a series of algorithms for our preconditioner, including multilevel domain construction using Morton codes, low-cost matrix precomputation by one-way Gauss-Jordan elimination, and conflict-free symmetric-matrix-vector multiplication in runtime preconditioning. The experiment shows that our preconditioner is effective, fast, cheap to precompute and scalable with respect to stiffness and problem size. It is compatible with many linear and nonlinear solvers used in cloth and deformable body simulation with dynamic contacts, such as PCG, accelerated gradient descent and L-BFGS. On a GPU, our preconditioner speeds up a PCG solver by approximately a factor of four, and its CPU version outperforms a number of competitors, including ILU0 and ILUT.},
  archive      = {J_TOG},
  author       = {Botao Wu and Zhendong Wang and Huamin Wang},
  doi          = {10.1145/3528223.3530085},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {63:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A GPU-based multilevel additive schwarz preconditioner for cloth and deformable body simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). True seams: Modeling seams in digital garments.
<em>TOG</em>, <em>41</em>(4), 62:1–16. (<a
href="https://doi.org/10.1145/3528223.3530128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seams play a fundamental role in the way a garment looks, fits, feels and behaves. Seams can have very different shapes and mechanical properties depending on how fabric is overlapped, folded and stitched together, with garment designers often choosing specific seam and stitch type combinations depending on the appearance and behavior they want for the garment. Yet, virtually all 3D CAD tools for fashion and visual effects ignore most of the visual and mechanical complexity of seams, and just treat them as joint edges, their simplest possible form, drastically limiting the fidelity of digital garments. In this paper, we present a method that models seams following their true, real-life construction. Each seam brings together and overlaps the fabric pieces to be sewn, folds the fabric according to the type of seam, and stitches the resulting assembly following the type of stitch. To avoid dealing with the complexities of folding in 3D space, we cast the problem into a sequence of simpler 2D problems where we can easily shape the seam and produce a result free of self-intersections, before lifting the folded geometry back to 3D space. We run a series of constrained optimizations to enforce spatial properties in these 2D settings, allowing us to treat asymmetric seams, gatherings and overlapping construction orders. Using a variety of common seams and stitches, we show how our approach substantially improves the visual appearance of full garments, for a better and more predictive digital replica.},
  archive      = {J_TOG},
  author       = {Alejandro Rodríguez and Gabriel Cirio},
  doi          = {10.1145/3528223.3530128},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {62:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {True seams: Modeling seams in digital garments},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation and optimization of magnetoelastic thin shells.
<em>TOG</em>, <em>41</em>(4), 61:1–18. (<a
href="https://doi.org/10.1145/3528223.3530142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetoelastic thin shells exhibit great potential in realizing versatile functionalities through a broad range of combination of material stiffness, remnant magnetization intensity, and external magnetic stimuli. In this paper, we propose a novel computational method for forward simulation and inverse design of magnetoelastic thin shells. Our system consists of two key components of forward simulation and backward optimization. On the simulation side, we have developed a new continuum mechanics model based on the Kirchhoff-Love thin-shell model to characterize the behaviors of a megnetolelastic thin shell under external magnetic stimuli. Based on this model, we proposed an implicit numerical simulator facilitated by the magnetic energy Hessian to treat the elastic and magnetic stresses within a unified framework, which is versatile to incorporation with other thin shell models. On the optimization side, we have devised a new differentiable simulation framework equipped with an efficient adjoint formula to accommodate various PDE-constraint, inverse design problems of magnetoelastic thin-shell structures, in both static and dynamic settings. It also encompasses applications of magnetoelastic soft robots, functional Origami, artworks, and meta-material designs. We demonstrate the efficacy of our framework by designing and simulating a broad array of magnetoelastic thin-shell objects that manifest complicated interactions between magnetic fields, materials, and control policies.},
  archive      = {J_TOG},
  author       = {Xuwen Chen and Xingyu Ni and Bo Zhu and Bin Wang and Baoquan Chen},
  doi          = {10.1145/3528223.3530142},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {61:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simulation and optimization of magnetoelastic thin shells},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Volume parametrization quantization for hexahedral meshing.
<em>TOG</em>, <em>41</em>(4), 60:1–19. (<a
href="https://doi.org/10.1145/3528223.3530123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developments in the field of parametrization-based quad mesh generation on surfaces have been impactful over the past decade. In this context, an important advance has been the replacement of error-prone rounding in the generation of integer-grid maps, by robust quantization methods. In parallel, parametrization-based hex mesh generation for volumes has been advanced. In this volumetric context, however, the state-of-the-art still relies on fragile rounding, not rarely producing defective meshes, especially when targeting a coarse mesh resolution. We present a method to robustly quantize volume parametrizations, i.e., to determine guaranteed valid choices of integers for 3D integer-grid maps. Inspired by the 2D case, we base our construction on a non-conforming cell decomposition of the volume, a 3D analogue of a T-mesh. In particular, we leverage the motorcycle complex, a recent generalization of the motorcycle graph, for this purpose. Integer values are expressed in a differential manner on the edges of this complex, enabling the efficient formulation of the conditions required to strictly prevent forcing the map into degeneration. Applying our method in the context of hexahedral meshing, we demonstrate that hexahedral meshes can be generated with significantly improved flexibility.},
  archive      = {J_TOG},
  author       = {Hendrik Brückler and David Bommes and Marcel Campen},
  doi          = {10.1145/3528223.3530123},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {60:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Volume parametrization quantization for hexahedral meshing},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Which cross fields can be quadrangulated?: Global
parameterization from prescribed holonomy signatures. <em>TOG</em>,
<em>41</em>(4), 59:1–12. (<a
href="https://doi.org/10.1145/3528223.3530187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a method for the generation of seamless surface parametrizations with guaranteed local injectivity and full control over holonomy. Previous methods guarantee only one of the two. Local injectivity is required to enable these parametrizations&#39; use in applications such as surface quadrangulation and spline construction. Holonomy control is crucial to enable guidance or prescription of the parametrization&#39;s isocurves based on directional information, in particular from cross-fields or feature curves, and more generally to constrain the parametrization topologically. To this end we investigate the relation between cross-field topology and seamless parametrization topology. Leveraging previous results on locally injective parametrization and combining them with insights on this relation in terms of holonomy, we propose an algorithm that meets these requirements. A key component relies on the insight that arbitrary surface cut graphs, as required for global parametrization, can be homeomorphically modified to assume almost any set of turning numbers with respect to a given target cross-field.},
  archive      = {J_TOG},
  author       = {Hanxiao Shen and Leyi Zhu and Ryan Capouellez and Daniele Panozzo and Marcel Campen and Denis Zorin},
  doi          = {10.1145/3528223.3530187},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {59:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Which cross fields can be quadrangulated?: Global parameterization from prescribed holonomy signatures},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing sparse integer-constrained cones for conformal
parameterizations. <em>TOG</em>, <em>41</em>(4), 58:1–13. (<a
href="https://doi.org/10.1145/3528223.3530118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to generate sparse integer-constrained cone singularities with low distortion constraints for conformal parameterizations. Inspired by [Fang et al. 2021; Soliman et al. 2018], the cone computation is formulated as a constrained optimization problem, where the objective is the number of cones measured by the ℓ 0 -norm of Gaussian curvature of vertices, and the constraint is to restrict the cone angles to be multiples of π /2 and control the distortion while ensuring that the Yamabe equation holds. Besides, the holonomy angles for the non-contractible homology loops are additionally required to be multiples of π /2 for achieving rotationally seamless conformal parameterizations. The Douglas-Rachford (DR) splitting algorithm is used to solve this challenging optimization problem, and our success relies on two key components. First, replacing each integer constraint with the intersection of a box set and a sphere enables us to manage the subproblems in DR splitting update steps in the continuous domain. Second, a novel solver is developed to optimize the ℓ 0 -norm without any approximation. We demonstrate the effectiveness and feasibility of our algorithm on a data set containing 3885 models. Compared to state-of-the-art methods, our method achieves a better tradeoff between the number of cones and the parameterization distortion.},
  archive      = {J_TOG},
  author       = {Mo Li and Qing Fang and Wenqing Ouyang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3528223.3530118},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {58:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computing sparse integer-constrained cones for conformal parameterizations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compatible intrinsic triangulations. <em>TOG</em>,
<em>41</em>(4), 57:1–12. (<a
href="https://doi.org/10.1145/3528223.3530175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding distortion-minimizing homeomorphisms between surfaces of arbitrary genus is a fundamental task in computer graphics and geometry processing. We propose a simple method utilizing intrinsic triangulations, operating directly on the original surfaces without going through any intermediate domains such as a plane or a sphere. Given two models A and B as triangle meshes, our algorithm constructs a Compatible Intrinsic Triangulation (CIT), a pair of intrinsic triangulations over A and B with full correspondences in their vertices, edges and faces. Such a tessellation allows us to establish consistent images of edges and faces of A&#39;s input mesh over B (and vice versa) by tracing piecewise-geodesic paths over A and B. Our algorithm for constructing CITs, primarily consisting of carefully designed edge flipping schemes, is empirical in nature without any guarantee of success, but turns out to be robust enough to be used within a similar second-order optimization framework as was used previously in the literature. The utility of our method is demonstrated through comparisons and evaluation on a standard benchmark dataset.},
  archive      = {J_TOG},
  author       = {Kenshi Takayama},
  doi          = {10.1145/3528223.3530175},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {57:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Compatible intrinsic triangulations},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeROIC: Neural rendering of objects from online image
collections. <em>TOG</em>, <em>41</em>(4), 56:1–12. (<a
href="https://doi.org/10.1145/3528223.3530177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method to acquire object representations from online image collections, capturing high-quality geometry and material properties of arbitrary objects from photographs with varying cameras, illumination, and backgrounds. This enables various object-centric rendering applications such as novel-view synthesis, relighting, and harmonized background composition from challenging in-the-wild input. Using a multi-stage approach extending neural radiance fields, we first infer the surface geometry and refine the coarsely estimated initial camera parameters, while leveraging coarse foreground object masks to improve the training efficiency and geometry quality. We also introduce a robust normal estimation technique which eliminates the effect of geometric noise while retaining crucial details. Lastly, we extract surface material properties and ambient illumination, represented in spherical harmonics with extensions that handle transient elements, e.g. sharp shadows. The union of these components results in a highly modular and efficient object acquisition framework. Extensive evaluations and comparisons demonstrate the advantages of our approach in capturing high-quality geometry and appearance properties useful for rendering applications.},
  archive      = {J_TOG},
  author       = {Zhengfei Kuang and Kyle Olszewski and Menglei Chai and Zeng Huang and Panos Achlioptas and Sergey Tulyakov},
  doi          = {10.1145/3528223.3530177},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {56:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeROIC: Neural rendering of objects from online image collections},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeAT: Neural adaptive tomography. <em>TOG</em>,
<em>41</em>(4), 55:1–13. (<a
href="https://doi.org/10.1145/3528223.3530121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present Neural Adaptive Tomography (NeAT), the first adaptive, hierarchical neural rendering pipeline for tomography. Through a combination of neural features with an adaptive explicit representation, we achieve reconstruction times far superior to existing neural inverse rendering methods. The adaptive explicit representation improves efficiency by facilitating empty space culling and concentrating samples in complex regions, while the neural features act as a neural regularizer for the 3D reconstruction. The NeAT framework is designed specifically for the tomographic setting, which consists only of semi-transparent volumetric scenes instead of opaque objects. In this setting, NeAT outperforms the quality of existing optimization-based tomography solvers while being substantially faster. https://github.com/darglein/NeAT},
  archive      = {J_TOG},
  author       = {Darius Rückert and Yuanhao Wang and Rui Li and Ramzi Idoughi and Wolfgang Heidrich},
  doi          = {10.1145/3528223.3530121},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {55:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeAT: Neural adaptive tomography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational quadratic shape functions for polygons and
polyhedra. <em>TOG</em>, <em>41</em>(4), 54:1–14. (<a
href="https://doi.org/10.1145/3528223.3530137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving partial differential equations (PDEs) on geometric domains is an important component of computer graphics, geometry processing, and many other fields. Typically, the given discrete mesh is the geometric representation and should not be altered for simulation purposes. Hence, accurately solving PDEs on general meshes is a central goal and has been considered for various differential operators over the last years. While it is known that using higher-order basis functions on simplicial meshes can substantially improve accuracy and convergence, extending these benefits to general surface or volume tessellations in an efficient fashion remains an open problem. Our work proposes variationally optimized piecewise quadratic shape functions for polygons and polyhedra, which generalize quadratic P 2 elements, exactly reproduce them on simplices, and inherit their beneficial numerical properties. To mitigate the associated cost of increased computation time, particularly for volumetric meshes, we introduce a custom two-level multigrid solver which significantly improves computational performance.},
  archive      = {J_TOG},
  author       = {Astrid Bunge and Philipp Herholz and Olga Sorkine-Hornung and Mario Botsch and Michael Kazhdan},
  doi          = {10.1145/3528223.3530137},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {54:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational quadratic shape functions for polygons and polyhedra},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grid-free monte carlo for PDEs with spatially varying
coefficients. <em>TOG</em>, <em>41</em>(4), 53:1–17. (<a
href="https://doi.org/10.1145/3528223.3530134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial differential equations (PDEs) with spatially varying coefficients arise throughout science and engineering, modeling rich heterogeneous material behavior. Yet conventional PDE solvers struggle with the immense complexity found in nature, since they must first discretize the problem---leading to spatial aliasing, and global meshing/sampling that is costly and error-prone. We describe a method that approximates neither the domain geometry, the problem data, nor the solution space, providing the exact solution (in expectation) even for problems with extremely detailed geometry and intricate coefficients. Our main contribution is to extend the walk on spheres (WoS) algorithm from constant- to variable-coefficient problems, by drawing on techniques from volumetric rendering. In particular, an approach inspired by null-scattering yields unbiased Monte Carlo estimators for a large class of 2nd order elliptic PDEs, which share many attractive features with Monte Carlo rendering: no meshing, trivial parallelism, and the ability to evaluate the solution at any point without solving a global system of equations.},
  archive      = {J_TOG},
  author       = {Rohan Sawhney and Dario Seyb and Wojciech Jarosz and Keenan Crane},
  doi          = {10.1145/3528223.3530134},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {53:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Grid-free monte carlo for PDEs with spatially varying coefficients},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energetically consistent inelasticity for optimization time
integration. <em>TOG</em>, <em>41</em>(4), 52:1–16. (<a
href="https://doi.org/10.1145/3528223.3530072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Energetically Consistent Inelasticity (ECI), a new formulation for modeling and discretizing finite strain elastoplasticity/viscoelasticity in a way that is compatible with optimization-based time integrators. We provide an in-depth analysis for allowing plasticity to be implicitly integrated through an augmented strain energy density function. We develop ECI on the associative von-Mises J2 plasticity, the non-associative Drucker-Prager plasticity, and the finite strain viscoelasticity. We demonstrate the resulting scheme on both the Finite Element Method (FEM) and the Material Point Method (MPM). Combined with a custom Newton-type optimization integration scheme, our method enables simulating stiff and large-deformation inelastic dynamics of metal, sand, snow, and foam with larger time steps, improved stability, higher efficiency, and better accuracy than existing approaches.},
  archive      = {J_TOG},
  author       = {Xuan Li and Minchen Li and Chenfanfu Jiang},
  doi          = {10.1145/3528223.3530072},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {52:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Energetically consistent inelasticity for optimization time integration},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic quantization for physics-based simulation.
<em>TOG</em>, <em>41</em>(4), 51:1–16. (<a
href="https://doi.org/10.1145/3528223.3530154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization has proven effective in high-resolution and large-scale simulations, which benefit from bit-level memory saving. However, identifying a quantization scheme that meets the requirement of both precision and memory efficiency requires trial and error. In this paper, we propose a novel framework to allow users to obtain a quantization scheme by simply specifying either an error bound or a memory compression rate. Based on the error propagation theory, our method takes advantage of auto-diff to estimate the contributions of each quantization operation to the total error. We formulate the task as a constrained optimization problem, which can be efficiently solved with analytical formulas derived for the linearized objective function. Our workflow extends the Taichi compiler and introduces dithering to improve the precision of quantized simulations. We demonstrate the generality and efficiency of our method via several challenging examples of physics-based simulation, which achieves up to 2.5× memory compression without noticeable degradation of visual quality in the results. Our code and data are available at https://github.com/Hanke98/AutoQantizer.},
  archive      = {J_TOG},
  author       = {Jiafeng Liu and Haoyang Shi and Siyuan Zhang and Yin Yang and Chongyang Ma and Weiwei Xu},
  doi          = {10.1145/3528223.3530154},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {51:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Automatic quantization for physics-based simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loki: A unified multiphysics simulation framework for
production. <em>TOG</em>, <em>41</em>(4), 50:1–20. (<a
href="https://doi.org/10.1145/3528223.3530058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Loki, a new framework for robust simulation of fluid, rigid, and deformable objects with non-compromising fidelity on any single element, and capabilities for coupling and representation transitions across multiple elements. Loki adapts multiple best-in-class solvers into a unified framework driven by a declarative state machine where users declare &#39;what&#39; is simulated but not &#39;when,&#39; so an automatic scheduling system takes care of mixing any combination of objects. This leads to intuitive setups for coupled simulations such as hair in the wind or objects transitioning from one representation to another, for example bulk water FLIP particles to SPH spray particles to volumetric mist. We also provide a consistent treatment for components used in several domains, such as unified collision and attachment constraints across 1D, 2D, 3D deforming and rigid objects. Distribution over MPI, custom linear equation solvers, and aggressive application of sparse techniques keep performance within production requirements. We demonstrate a variety of solvers within the framework and their interactions, including FLIPstyle liquids, spatially adaptive volumetric fluids, SPH, MPM, and mesh-based solids, including but not limited to discrete elastic rods, elastons, and FEM with state-of-the-art constitutive models. Our framework has proven powerful and intuitive enough for voluntary artist adoption and has delivered creature and FX simulations for multiple major movie productions in the preceding four years.},
  archive      = {J_TOG},
  author       = {Steve Lesser and Alexey Stomakhin and Gilles Daviet and Joel Wretborn and John Edholm and Noh-Hoon Lee and Eston Schweickart and Xiao Zhai and Sean Flynn and Andrew Moffat},
  doi          = {10.1145/3528223.3530058},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {50:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Loki: A unified multiphysics simulation framework for production},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast unsmoothed aggregation algebraic multigrid framework
for the large-scale simulation of incompressible flow. <em>TOG</em>,
<em>41</em>(4), 49:1–18. (<a
href="https://doi.org/10.1145/3528223.3530109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multigrid methods are quite efficient for solving the pressure Poisson equation in simulations of incompressible flow. However, for viscous liquids, geometric multigrid turned out to be less efficient for solving the variational viscosity equation. In this contribution, we present an Unsmoothed Aggregation Algebraic MultiGrid (UAAMG) method with a multi-color Gauss-Seidel smoother, which consistently solves the variational viscosity equation in a few iterations for various material parameters. Moreover, we augment the OpenVDB data structure with Intel SIMD intrinsic functions to perform sparse matrix-vector multiplications efficiently on all multigrid levels. Our framework is 2.0 to 14.6 times faster compared to the state-of-the-art adaptive octree solver in commercial software for the large-scale simulation of both non-viscous and viscous flow. The code is available at http://computationalsciences.org/publications/shao-2022-multigrid.html.},
  archive      = {J_TOG},
  author       = {Han Shao and Libo Huang and Dominik L. Michels},
  doi          = {10.1145/3528223.3530109},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {49:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {A fast unsmoothed aggregation algebraic multigrid framework for the large-scale simulation of incompressible flow},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unbiased and consistent rendering using biased estimators.
<em>TOG</em>, <em>41</em>(4), 48:1–13. (<a
href="https://doi.org/10.1145/3528223.3530160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general framework for transforming biased estimators into unbiased and consistent estimators for the same quantity. We show how several existing unbiased and consistent estimation strategies in rendering are special cases of this framework, and are part of a broader debiasing principle. We provide a recipe for constructing estimators using our generalized framework and demonstrate its applicability by developing novel unbiased forms of transmittance estimation, photon mapping, and finite differences.},
  archive      = {J_TOG},
  author       = {Zackary Misso and Benedikt Bitterli and Iliyan Georgiev and Wojciech Jarosz},
  doi          = {10.1145/3528223.3530160},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {48:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unbiased and consistent rendering using biased estimators},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical level-of-detail aggregation of fur appearance.
<em>TOG</em>, <em>41</em>(4), 47:1–17. (<a
href="https://doi.org/10.1145/3528223.3530105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fur appearance rendering is crucial for the realism of computer generated imagery, but is also a challenge in computer graphics for many years. Much effort has been made to accurately simulate the multiple-scattered light transport among fur fibers, but the computation cost is still very high, since the number of fur fibers is usually extremely large. In this paper, we aim at reducing the number of fur fibers while preserving realistic fur appearance. We present an aggregated fur appearance model, using one thick cylinder to accurately describe the aggregated optical behavior of a bunch of fur fibers, including the multiple scattering of light among them. Then, to acquire the parameters of our aggregated model, we use a lightweight neural network to map individual fur fiber&#39;s optical properties to those in our aggregated model. Finally, we come up with a practical heuristic that guides the simplification process of fur dynamically at different bounces of the light, leading to a practical level-of-detail rendering scheme. Our method achieves nearly the same results as the ground truth, but performs 3.8×-13.5× faster.},
  archive      = {J_TOG},
  author       = {Junqiu Zhu and Sizhe Zhao and Lu Wang and Yanning Xu and Ling-Qi Yan},
  doi          = {10.1145/3528223.3530105},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {47:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Practical level-of-detail aggregation of fur appearance},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MatFormer: A generative model for procedural materials.
<em>TOG</em>, <em>41</em>(4), 46:1–12. (<a
href="https://doi.org/10.1145/3528223.3530173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural material graphs are a compact, parameteric, and resolution-independent representation that are a popular choice for material authoring. However, designing procedural materials requires significant expertise and publicly accessible libraries contain only a few thousand such graphs. We present MatFormer, a generative model that can produce a diverse set of high-quality procedural materials with complex spatial patterns and appearance. While procedural materials can be modeled as directed (operation) graphs, they contain arbitrary numbers of heterogeneous nodes with unstructured, often long-range node connections, and functional constraints on node parameters and connections. MatFormer addresses these challenges with a multi-stage transformer-based model that sequentially generates nodes, node parameters, and edges, while ensuring the semantic validity of the graph. In addition to generation, MatFormer can be used for the auto-completion and exploration of partial material graphs. We qualitatively and quantitatively demonstrate that our method outperforms alternative approaches, in both generated graph and material quality.},
  archive      = {J_TOG},
  author       = {Paul Guerrero and Miloš Hašan and Kalyan Sunkavalli and Radomír Měch and Tamy Boubekeur and Niloy J. Mitra},
  doi          = {10.1145/3528223.3530173},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {46:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MatFormer: A generative model for procedural materials},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Procedural texturing of solid wood with knots. <em>TOG</em>,
<em>41</em>(4), 45:1–10. (<a
href="https://doi.org/10.1145/3528223.3530081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a procedural framework for modeling the annual ring pattern of solid wood with knots. Although wood texturing is a well-studied topic, there have been few previous attempts at modeling knots inside the wood texture. Our method takes the skeletal structure of a tree log as input and produces a three-dimensional scalar field representing the time of added growth, which defines the volumetric annual ring pattern. First, separate fields are computed around each strand of the skeleton, i.e., the stem and each knot. The strands are then merged into a single field using smooth minimums. We further suggest techniques for controlling the smooth minimum to adjust the balance of smoothness and reproduce the distortion effects observed around dead knots. Our method is implemented as a shader program running on a GPU with computation times of approximately 0.5 s per image and an input data size of 600 KB. We present rendered images of solid wood from pine and spruce as well as plywood and cross-laminated timber (CLT). Our results were evaluated by wood experts, who confirmed the plausibility of the rendered annual ring patterns. Link to code: https://github.com/marialarsson/procedural_knots.},
  archive      = {J_TOG},
  author       = {Maria Larsson and Takashi Ijiri and Hironori Yoshida and Johannes A. J. Huber and Magnus Fredriksson and Olof Broman and Takeo Igarashi},
  doi          = {10.1145/3528223.3530081},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {45:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Procedural texturing of solid wood with knots},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unbiased inverse volume rendering with differential
trackers. <em>TOG</em>, <em>41</em>(4), 44:1–20. (<a
href="https://doi.org/10.1145/3528223.3530073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric representations are popular in inverse rendering because they have a simple parameterization, are smoothly varying, and transparently handle topology changes. However, incorporating the full volumetric transport of light is costly and challenging, often leading practitioners to implement simplified models, such as purely emissive and absorbing volumes with &quot;baked&quot; lighting. One such challenge is the efficient estimation of the gradients of the volume&#39;s appearance with respect to its scattering and absorption parameters. We show that the straightforward approach---differentiating a volumetric free-flight sampler---can lead to biased and high-variance gradients, hindering optimization. Instead, we propose using a new sampling strategy: differential ratio tracking , which is unbiased, yields low-variance gradients, and runs in linear time. Differential ratio tracking combines ratio tracking and reservoir sampling to estimate gradients by sampling distances proportional to the unweighted transmittance rather than the usual extinction-weighted transmittance. In addition, we observe local minima when optimizing scattering parameters to reproduce dense volumes or surfaces. We show that these local minima can be overcome by bootstrapping the optimization from nonphysical emissive volumes that are easily optimized.},
  archive      = {J_TOG},
  author       = {Merlin Nimier-David and Thomas Müller and Alexander Keller and Wenzel Jakob},
  doi          = {10.1145/3528223.3530073},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {44:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unbiased inverse volume rendering with differential trackers},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developability-driven piecewise approximations for
triangular meshes. <em>TOG</em>, <em>41</em>(4), 43:1–13. (<a
href="https://doi.org/10.1145/3528223.3530117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to compute a piecewise mesh with a few developable patches and a small approximation error for an input triangular mesh. Our key observation is that a deformed mesh after enforcing discrete developability is easily partitioned into nearly developable patches. To obtain the nearly developable mesh, we present a new edge-oriented notion of discrete developability to define a developability-encouraged deformation energy, which is further optimized by the block nonlinear Gauss-Seidel method. The key to successfully applying this optimizer is three types of auxiliary variables. Then, a coarse-to-fine segmentation technique is developed to partition the deformed mesh into a small set of nearly discrete developable patches. Finally, we refine the segmented mesh to reduce the discrete Gaussian curvature while keeping the patches smooth and the approximation error small. In practice, our algorithm achieves a favorable tradeoff between the number of developable patches and the approximation error. We demonstrate the feasibility and practicability of our method over various examples, including seventeen physical manufacturing models with paper.},
  archive      = {J_TOG},
  author       = {Zheng-Yu Zhao and Qing Fang and Wenqing Ouyang and Zheng Zhang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3528223.3530117},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {43:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Developability-driven piecewise approximations for triangular meshes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate convex decomposition for 3D meshes with
collision-aware concavity and tree search. <em>TOG</em>, <em>41</em>(4),
42:1–18. (<a href="https://doi.org/10.1145/3528223.3530103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate convex decomposition aims to decompose a 3D shape into a set of almost convex components, whose convex hulls can then be used to represent the input shape. It thus enables efficient geometry processing algorithms specifically designed for convex shapes and has been widely used in game engines, physics simulations, and animation. While prior works can capture the global structure of input shapes, they may fail to preserve fine-grained details (e.g., filling a toaster&#39;s slots), which are critical for retaining the functionality of objects in interactive environments. In this paper, we propose a novel method that addresses the limitations of existing approaches from three perspectives: (a) We introduce a novel collision-aware concavity metric that examines the distance between a shape and its convex hull from both the boundary and the interior. The proposed concavity preserves collision conditions and is more robust to detect various approximation errors. (b) We decompose shapes by directly cutting meshes with 3D planes. It ensures generated convex hulls are intersection-free and avoids voxelization errors. (c) Instead of using a one-step greedy strategy, we propose employing a multi-step tree search to determine the cutting planes, which leads to a globally better solution and avoids unnecessary cuttings. Through extensive evaluation on a large-scale articulated object dataset, we show that our method generates decompositions closer to the original shape with fewer components. It thus supports delicate and efficient object interaction in downstream applications.},
  archive      = {J_TOG},
  author       = {Xinyue Wei and Minghua Liu and Zhan Ling and Hao Su},
  doi          = {10.1145/3528223.3530103},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {42:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Approximate convex decomposition for 3D meshes with collision-aware concavity and tree search},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust computation of implicit surface networks for
piecewise linear functions. <em>TOG</em>, <em>41</em>(4), 41:1–16. (<a
href="https://doi.org/10.1145/3528223.3530176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit surface networks, such as arrangements of implicit surfaces and materials interfaces, are used for modeling piecewise smooth or partitioned shapes. However, accurate and numerically robust algorithms for discretizing either structure on a grid are still lacking. We present a unified approach for computing both types of surface networks for piecewise linear functions defined on a tetrahedral grid. Both algorithms are guaranteed to produce a correct combinatorial structure for any number of functions. Our main contribution is an exact and efficient method for partitioning a tetrahedron using the level sets of linear functions defined by barycentric interpolation. To further improve performance, we designed look-up tables to speed up processing of tetrahedra involving few functions and introduced an efficient algorithm for identifying nested 3D regions.},
  archive      = {J_TOG},
  author       = {Xingyi Du and Qingnan Zhou and Nathan Carr and Tao Ju},
  doi          = {10.1145/3528223.3530176},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {41:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust computation of implicit surface networks for piecewise linear functions},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TopoCut: Fast and robust planar cutting of arbitrary
domains. <em>TOG</em>, <em>41</em>(4), 40:1–15. (<a
href="https://doi.org/10.1145/3528223.3530149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a complex three-dimensional domain delimited by a closed and non-degenerate input triangle mesh without any self-intersection, a common geometry processing task consists in cutting up the domain into cells through a set of planar cuts, creating a &quot;cut-cell mesh&quot;, i.e., a volumetric decomposition of the domain amenable to visualization (e.g., exploded views), animation (e.g., virtual surgery), or simulation (finite volume computations). A large number of methods have proposed either efficient or robust solutions, sometimes restricting the cuts to form a regular or adaptive grid for simplicity; yet, none can guarantee both properties, severely limiting their usefulness in practice. At the core of the difficulty is the determination of topological relationships among large numbers of vertices, edges, faces and cells in order to assemble a proper cut-cell mesh: while exact geometric computations provide a robust solution to this issue, their high computational cost has prompted a number of faster solutions based on, e.g., local floating-point angle sorting to significantly accelerate the process --- but losing robustness in doing so. In this paper, we introduce a new approach to planar cutting of 3D domains that substitutes topological inference for numerical ordering through a novel mesh data structure, and revert to exact numerical evaluations only in the few rare cases where it is strictly necessary. We show that our novel concept of topological cuts exploits the inherent structure of cut-cell mesh generation to save computational time while still guaranteeing exactness for, and robustness to, arbitrary cuts and surface geometry. We demonstrate the superiority of our approach over state-of-the-art methods on almost 10,000 meshes with a wide range of geometric and topological complexity. We also provide an open source implementation.},
  archive      = {J_TOG},
  author       = {Xianzhong Fang and Mathieu Desbrun and Hujun Bao and Jin Huang},
  doi          = {10.1145/3528223.3530149},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {40:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {TopoCut: Fast and robust planar cutting of arbitrary domains},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMBER: Exact mesh booleans via efficient &amp; robust local
arrangements. <em>TOG</em>, <em>41</em>(4), 39:1–15. (<a
href="https://doi.org/10.1145/3528223.3530181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean operators are an essential tool in a wide range of geometry processing and CAD/CAM tasks. We present a novel method, EMBER, to compute Boolean operations on polygon meshes which is exact, reliable, and highly performant at the same time. Exactness is guaranteed by using a plane-based representation for the input meshes along with recently introduced homogeneous integer coordinates. Reliability and robustness emerge from a formulation of the algorithm via generalized winding numbers and mesh arrangements. High performance is achieved by avoiding the (pre-)construction of a global acceleration structure. Instead, our algorithm performs an adaptive recursive subdivision of the scene&#39;s bounding box while generating and tracking all required data on the fly. By leveraging a number of early-out termination criteria, we can avoid the generation and inspection of regions that do not contribute to the output. With a careful implementation and a work-stealing multi-threading architecture, we are able to compute Boolean operations between meshes with millions of triangles at interactive rates. We run an extensive evaluation on the Thingi10K dataset to demonstrate that our method outperforms state-of-the-art algorithms, even inexact ones like QuickCSG, by orders of magnitude.},
  archive      = {J_TOG},
  author       = {Philip Trettner and Julius Nehring-Wirxel and Leif Kobbelt},
  doi          = {10.1145/3528223.3530181},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {39:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {EMBER: Exact mesh booleans via efficient &amp; robust local arrangements},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High dynamic range and super-resolution from raw image
bursts. <em>TOG</em>, <em>41</em>(4), 38:1–21. (<a
href="https://doi.org/10.1145/3528223.3530180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of highresolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with exposure bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image restoration, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to ×4 on real photographs taken in the wild with hand-held cameras, and high robustness to low-light conditions, noise, camera shake, and moderate object motion.},
  archive      = {J_TOG},
  author       = {Bruno Lecouat and Thomas Eboli and Jean Ponce and Julien Mairal},
  doi          = {10.1145/3528223.3530180},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {38:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {High dynamic range and super-resolution from raw image bursts},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeing through obstructions with diffractive cloaking.
<em>TOG</em>, <em>41</em>(4), 37:1–15. (<a
href="https://doi.org/10.1145/3528223.3530185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unwanted camera obstruction can severely degrade captured images, including both scene occluders near the camera and partial occlusions of the camera cover glass. Such occlusions can cause catastrophic failures for various scene understanding tasks such as semantic segmentation, object detection, and depth estimation. Existing camera arrays capture multiple redundant views of a scene to see around thin occlusions. Such multi-camera systems effectively form a large synthetic aperture, which can suppress nearby occluders with a large defocus blur, but significantly increase the overall form factor of the imaging setup. In this work, we propose a monocular single-shot imaging approach that optically cloaks obstructions by emulating a large array. Instead of relying on different camera views, we learn a diffractive optical element (DOE) that performs depth-dependent optical encoding, scattering nearby occlusions while allowing paraxial wavefronts to be focused. We computationally reconstruct unobstructed images from these superposed measurements with a neural network that is trained jointly with the optical layer of the proposed imaging system. We assess the proposed method in simulation and with an experimental prototype, validating that the proposed computational camera is capable of recovering occluded scene information in the presence of severe camera obstruction.},
  archive      = {J_TOG},
  author       = {Zheng Shi and Yuval Bahat and Seung-Hwan Baek and Qiang Fu and Hadi Amata and Xiao Li and Praneeth Chakravarthula and Wolfgang Heidrich and Felix Heide},
  doi          = {10.1145/3528223.3530185},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {37:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Seeing through obstructions with diffractive cloaking},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blending camera and 77 GHz radar sensing for equitable,
robust plethysmography. <em>TOG</em>, <em>41</em>(4), 36:1–14. (<a
href="https://doi.org/10.1145/3528223.3530161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the resurgence of non-contact vital sign sensing due to the COVID-19 pandemic, remote heart-rate monitoring has gained significant prominence. Many existing methods use cameras; however previous work shows a performance loss for darker skin tones. In this paper, we show through light transport analysis that the camera modality is fundamentally biased against darker skin tones. We propose to reduce this bias through multi-modal fusion with a complementary and fairer modality - radar. Through a novel debiasing oriented fusion framework, we achieve performance gains over all tested baselines and achieve skin tone fairness improvements over the RGB modality. That is, the associated Pareto frontier between performance and fairness is improved when compared to the RGB modality. In addition, performance improvements are obtained over the radar-based method, with small trade-offs in fairness. We also open-source the largest multi-modal remote heart-rate estimation dataset of paired camera and radar measurements with a focus on skin tone representation.},
  archive      = {J_TOG},
  author       = {Alexander Vilesov and Pradyumna Chari and Adnan Armouti and Anirudh Bindiganavale Harish and Kimaya Kulkarni and Ananya Deoghare and Laleh Jalilian and Achuta Kadambi},
  doi          = {10.1145/3528223.3530161},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {36:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Blending camera and 77 GHz radar sensing for equitable, robust plethysmography},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TCB-spline-based image vectorization. <em>TOG</em>,
<em>41</em>(3), 34:1–17. (<a
href="https://doi.org/10.1145/3513132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector image representation methods that can faithfully reconstruct objects and color variations in a raster image are desired in many practical applications. This article presents triangular configuration B-spline (referred to as TCB-spline)-based vector graphics for raster image vectorization. Based on this new representation, an automatic raster image vectorization paradigm is proposed. The proposed framework first detects sharp curvilinear features in the image and constructs knot meshes based on the detected feature lines. It iteratively optimizes color and position of control points and updates the knot meshes. By using collinear knots at feature lines, both smooth and discontinuous color variations can be efficiently modeled by the same set of quadratic TCB-splines. A variational knot mesh generation method is designed to adaptively introduce knots and update their connectivity to satisfy the local reconstruction quality. Experiments and comparisons show that our framework outperforms the existing state-of-the-art methods in providing more faithful reconstruction results. In particular, our method is able to model undetected features and subtle or complicated color variations in-between features, which the previous methods cannot handle efficiently. Our vectorization representation also facilitates a variety of editing operations performed directly over vector images.},
  archive      = {J_TOG},
  author       = {Haikuan Zhu and Juan Cao and Yanyang Xiao and Zhonggui Chen and Zichun Zhong and Yongjie Jessica Zhang},
  doi          = {10.1145/3513132},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {34:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {TCB-spline-based image vectorization},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion puzzle: Arbitrary motion style transfer by body part.
<em>TOG</em>, <em>41</em>(3), 33:1–16. (<a
href="https://doi.org/10.1145/3516429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Motion Puzzle, a novel motion style transfer network that advances the state-of-the-art in several important respects. The Motion Puzzle is the first that can control the motion style of individual body parts, allowing for local style editing and significantly increasing the range of stylized motions. Designed to keep the human’s kinematic structure, our framework extracts style features from multiple style motions for different body parts and transfers them locally to the target body parts. Another major advantage is that it can transfer both global and local traits of motion style by integrating the adaptive instance normalization and attention modules while keeping the skeleton topology. Thus, it can capture styles exhibited by dynamic movements, such as flapping and staggering, significantly better than previous work. In addition, our framework allows for arbitrary motion style transfer without datasets with style labeling or motion pairing, making many publicly available motion datasets available for training. Our framework can be easily integrated with motion generation frameworks to create many applications, such as real-time motion transfer. We demonstrate the advantages of our framework with a number of examples and comparisons with previous work.},
  archive      = {J_TOG},
  author       = {Deok-Kyeong Jang and Soomin Park and Sung-Hee Lee},
  doi          = {10.1145/3516429},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {33:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Motion puzzle: Arbitrary motion style transfer by body part},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HRBF-fusion: Accurate 3D reconstruction from RGB-d data
using on-the-fly implicits. <em>TOG</em>, <em>41</em>(3), 35:1–19. (<a
href="https://doi.org/10.1145/3516521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstruction of high-fidelity 3D objects or scenes is a fundamental research problem. Recent advances in RGB-D fusion have demonstrated the potential of producing 3D models from consumer-level RGB-D cameras. However, due to the discrete nature and limited resolution of their surface representations (e.g., point or voxel based), existing approaches suffer from the accumulation of errors in camera tracking and distortion in the reconstruction, which leads to an unsatisfactory 3D reconstruction. In this article, we present a method using on-the-fly implicits of Hermite Radial Basis Functions (HRBFs) as a continuous surface representation for camera tracking in an existing RGB-D fusion framework. Furthermore, curvature estimation and confidence evaluation are coherently derived from the inherent surface properties of the on-the-fly HRBF implicits, which are devoted to a data fusion with better quality. We argue that our continuous but on-the-fly surface representation can effectively mitigate the impact of noise with its robustness and constrain the reconstruction with inherent surface smoothness when being compared with discrete representations. Experimental results on various real-world and synthetic datasets demonstrate that our HRBF-fusion outperforms the state-of-the-art approaches in terms of tracking robustness and reconstruction accuracy.},
  archive      = {J_TOG},
  author       = {Yabin Xu and Liangliang Nan and Laishui Zhou and Jun Wang and Charlie C. L. Wang},
  doi          = {10.1145/3516521},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {35:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {HRBF-fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StrokeStyles: Stroke-based segmentation and stylization of
fonts. <em>TOG</em>, <em>41</em>(3), 28:1–21. (<a
href="https://doi.org/10.1145/3505246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method to automatically segment a font’s glyphs into a set of overlapping and intersecting strokes with the aim of generating artistic stylizations. The segmentation method relies on a geometric analysis of the glyph’s outline, its interior, and the surrounding areas and is grounded in perceptually informed principles and measures. Our method does not require training data or templates and applies to glyphs in a large variety of input languages, writing systems, and styles. It uses the medial axis, curvilinear shape features that specify convex and concave outline parts, links that connect concavities, and seven junction types. We show that the resulting decomposition in strokes can be used to create variations, stylizations, and animations in different artistic or design-oriented styles while remaining recognizably similar to the input font.},
  archive      = {J_TOG},
  author       = {Daniel Berio and Frederic Fol Leymarie and Paul Asente and Jose Echevarria},
  doi          = {10.1145/3505246},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {28:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {StrokeStyles: Stroke-based segmentation and stylization of fonts},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-optimization of design and fabrication plans for
carpentry. <em>TOG</em>, <em>41</em>(3), 32:1–13. (<a
href="https://doi.org/10.1145/3508499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Past work on optimizing fabrication plans given a carpentry design can provide Pareto-optimal plans trading off between material waste, fabrication time, precision, and other considerations. However, when developing fabrication plans, experts rarely restrict to a single design , instead considering families of design variations , sometimes adjusting designs to simplify fabrication. Jointly exploring the design and fabrication plan spaces for each design is intractable using current techniques. We present a new approach to jointly optimize design and fabrication plans for carpentered objects. To make this bi-level optimization tractable, we adapt recent work from program synthesis based on equality graphs (e-graphs), which encode sets of equivalent programs. Our insight is that subproblems within our bi-level problem share significant substructures. By representing both designs and fabrication plans in a new bag of parts (BOP) e-graph, we amortize the cost of optimizing design components shared among multiple candidates. Even using BOP e-graphs, the optimization space grows quickly in practice. Hence, we also show how a feedback-guided search strategy dubbed Iterative Contraction and Expansion on E-graphs (ICEE) can keep the size of the e-graph manageable and direct the search towards promising candidates. We illustrate the advantages of our pipeline through examples from the carpentry domain.},
  archive      = {J_TOG},
  author       = {Haisen Zhao and Max Willsey and Amy Zhu and Chandrakana Nandi and Zachary Tatlock and Justin Solomon and Adriana Schulz},
  doi          = {10.1145/3508499},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {32:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Co-optimization of design and fabrication plans for carpentry},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A practical model for realistic butterfly flight simulation.
<em>TOG</em>, <em>41</em>(3), 31:1–12. (<a
href="https://doi.org/10.1145/3510459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Butterflies are not only ubiquitous around the world but are also widely known for inspiring thrill resonance, with their elegant and peculiar flights. However, realistically modeling and simulating butterfly flights—in particular, for real-time graphics and animation applications—remains an under-explored problem. In this article, we propose an efficient and practical model to simulate butterfly flights. We first model a butterfly with parametric maneuvering functions, including wing-abdomen interaction. Then, we simulate dynamic maneuvering control of the butterfly through our force-based model, which includes both the aerodynamics force and the vortex force. Through many simulation experiments and comparisons, we demonstrate that our method can efficiently simulate realistic butterfly flight motions in various real-world settings.},
  archive      = {J_TOG},
  author       = {Qiang Chen and Tingsong Lu and Yang Tong and Guoliang Luo and Xiaogang Jin and Zhigang Deng},
  doi          = {10.1145/3510459},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {31:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {A practical model for realistic butterfly flight simulation},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LookOut! Interactive camera gimbal controller for filming
long takes. <em>TOG</em>, <em>41</em>(3), 30:1–16. (<a
href="https://doi.org/10.1145/3506693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job of a camera operator is challenging, and potentially dangerous, when filming long moving camera shots. Broadly, the operator must keep the actors in frame while safely navigating around obstacles and while fulfilling an artistic vision. We propose a unified hardware and software system that distributes some of the camera operator’s burden, freeing the operator up to focus on safety and aesthetics during a take. Our real-time system provides solo operators with end-to-end control so that they can balance on-set responsiveness to action against planned storyboards and framing while looking where they are going. By default, we film without a field monitor. Our LookOut system is built around a lightweight commodity camera gimbal mechanism, with heavy modifications to the controller, which would normally just provide active stabilization. Our control algorithm reacts to speech commands, video, and a premade script. Specifically, our automatic monitoring of the live video feed saves the operator from distractions. In preproduction, an artist uses our graphical user interface (GUI) to design a sequence of high-level camera “behaviors.” Those can be specific, based on a storyboard, or looser objectives, such as “frame both actors.” Then, during filming, a machine-readable script, exported from the GUI, ties together with the sensor readings to drive the gimbal. To validate our algorithm, we compared tracking strategies, interfaces, and hardware protocols and collected impressions from (a) filmmakers who used all aspects of our system and (b) filmmakers who watched footage filmed using LookOut.},
  archive      = {J_TOG},
  author       = {Mohamed Sayed and Robert Cinca and Enrico Costanza and Gabriel Brostow},
  doi          = {10.1145/3506693},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {30:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {LookOut! interactive camera gimbal controller for filming long takes},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dev2PQ: Planar quadrilateral strip remeshing of developable
surfaces. <em>TOG</em>, <em>41</em>(3), 29:1–18. (<a
href="https://doi.org/10.1145/3510002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an algorithm to remesh triangle meshes representing developable surfaces to planar quad dominant meshes. The output of our algorithm consists of planar quadrilateral (PQ) strips that are aligned to principal curvature directions and closely approximate the curved parts of the input developable, and planar polygons representing the flat parts of the input that connect the PQ strips. Developable PQ-strip meshes are useful in many areas of shape modeling, thanks to the simplicity of fabrication from flat sheet material. Unfortunately, they are difficult to model due to their restrictive combinatorics. Other representations of developable surfaces, such as arbitrary triangle or quad meshes, are more suitable for interactive freeform modeling but generally have non-planar faces or are not aligned to principal curvatures. Our method leverages the modeling flexibility of non-ruling-based representations of developable surfaces while still obtaining developable, curvature-aligned PQ-strip meshes. Our algorithm optimizes for a scalar function on the input mesh, such that its isolines are extrinsically straight and align well to the locally estimated ruling directions. The condition that guarantees straight isolines is non-linear of high order and numerically difficult to enforce in a straightforward manner. We devise an alternating optimization method that makes our problem tractable and practical to compute. Our method works automatically on any developable input, including multiple patches and curved folds, without explicit domain decomposition. We demonstrate the effectiveness of our approach on a variety of developable surfaces and show how our remeshing can be used alongside handle-based interactive freeform modeling of developable shapes.},
  archive      = {J_TOG},
  author       = {Floor Verhoeven and Amir Vaxman and Tim Hoffmann and Olga Sorkine-Hornung},
  doi          = {10.1145/3510002},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {29:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dev2PQ: Planar quadrilateral strip remeshing of developable surfaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DiffusionNet: Discretization agnostic learning on surfaces.
<em>TOG</em>, <em>41</em>(3), 27:1–16. (<a
href="https://doi.org/10.1145/3507905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new general-purpose approach to deep learning on three-dimensional surfaces based on the insight that a simple diffusion layer is highly effective for spatial communication. The resulting networks are automatically robust to changes in resolution and sampling of a surface—a basic property that is crucial for practical applications. Our networks can be discretized on various geometric representations, such as triangle meshes or point clouds, and can even be trained on one representation and then applied to another. We optimize the spatial support of diffusion as a continuous network parameter ranging from purely local to totally global, removing the burden of manually choosing neighborhood sizes. The only other ingredients in the method are a multi-layer perceptron applied independently at each point and spatial gradient features to support directional filters. The resulting networks are simple, robust, and efficient. Here, we focus primarily on triangle mesh surfaces and demonstrate state-of-the-art results for a variety of tasks, including surface classification, segmentation, and non-rigid correspondence.},
  archive      = {J_TOG},
  author       = {Nicholas Sharp and Souhaib Attaiki and Keenan Crane and Maks Ovsjanikov},
  doi          = {10.1145/3507905},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {27:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffusionNet: Discretization agnostic learning on surfaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceptual error optimization for monte carlo rendering.
<em>TOG</em>, <em>41</em>(3), 26:1–17. (<a
href="https://doi.org/10.1145/3504002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing realistic images involves computing high-dimensional light-transport integrals. In practice, these integrals are numerically estimated via Monte Carlo integration. The error of this estimation manifests itself as conspicuous aliasing or noise. To ameliorate such artifacts and improve image fidelity, we propose a perception-oriented framework to optimize the error of Monte Carlo rendering. We leverage models based on human perception from the halftoning literature. The result is an optimization problem whose solution distributes the error as visually pleasing blue noise in image space. To find solutions, we present a set of algorithms that provide varying trade-offs between quality and speed, showing substantial improvements over prior state of the art. We perform evaluations using quantitative and error metrics and provide extensive supplemental material to demonstrate the perceptual improvements achieved by our methods.},
  archive      = {J_TOG},
  author       = {Vassillen Chizhov and Iliyan Georgiev and Karol Myszkowski and Gurprit Singh},
  doi          = {10.1145/3504002},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {26:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Perceptual error optimization for monte carlo rendering},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subdivision-based mesh convolution networks. <em>TOG</em>,
<em>41</em>(3), 25:1–16. (<a
href="https://doi.org/10.1145/3506694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutionalneural networks (CNNs) have made great breakthroughs in two-dimensional (2D) computer vision. However, their irregular structure makes it hard to harness the potential of CNNs directly on meshes. A subdivision surface provides a hierarchical multi-resolution structure in which each face in a closed 2-manifold triangle mesh is exactly adjacent to three faces. Motivated by these two observations, this article presents SubdivNet , an innovative and versatile CNN framework for three-dimensional (3D) triangle meshes with Loop subdivision sequence connectivity. Making an analogy between mesh faces and pixels in a 2D image allows us to present a mesh convolution operator to aggregate local features from nearby faces. By exploiting face neighborhoods, this convolution can support standard 2D convolutional network concepts, e.g., variable kernel size, stride, and dilation. Based on the multi-resolution hierarchy, we make use of pooling layers that uniformly merge four faces into one and an upsampling method that splits one face into four. Thereby, many popular 2D CNN architectures can be easily adapted to process 3D meshes. Meshes with arbitrary connectivity can be remeshed to have Loop subdivision sequence connectivity via self-parameterization, making SubdivNet a general approach. Extensive evaluation and various applications demonstrate SubdivNet’s effectiveness and efficiency.},
  archive      = {J_TOG},
  author       = {Shi-Min Hu and Zheng-Ning Liu and Meng-Hao Guo and Jun-Xiong Cai and Jiahui Huang and Tai-Jiang Mu and Ralph R. Martin},
  doi          = {10.1145/3506694},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {25:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Subdivision-based mesh convolution networks},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TextureMe: High-quality textured scene reconstruction in
real time. <em>TOG</em>, <em>41</em>(3), 24:1–18. (<a
href="https://doi.org/10.1145/3503926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) reconstruction using an RGB-D camera has been widely adopted for realistic content creation. However, high-quality texture mapping onto the reconstructed geometry is often treated as an offline step that should run after geometric reconstruction. In this article, we propose TextureMe , a novel approach that jointly recovers 3D surface geometry and high-quality texture in real time. The key idea is to create triangular texture patches that correspond to zero-crossing triangles of truncated signed distance function (TSDF) progressively in a global texture atlas. Our approach integrates color details into the texture patches in parallel with the depth map integration to a TSDF. It also actively updates a pool of texture patches to adapt TSDF changes and minimizes misalignment artifacts that occur due to camera drift and image distortion. Our global texture atlas representation is fully compatible with conventional texture mapping. As a result, our approach produces high-quality textures without utilizing additional texture map optimization, mesh parameterization, or heavy post-processing. High-quality scenes produced by our real-time approach are even comparable to the results from state-of-the-art methods that run offline.},
  archive      = {J_TOG},
  author       = {Jungeon Kim and Hyomin Kim and Hyeonseo Nam and Jaesik Park and Seungyong Lee},
  doi          = {10.1145/3503926},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {24:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {TextureMe: High-quality textured scene reconstruction in real time},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A large-scale comparison of tetrahedral and hexahedral
elements for solving elliptic PDEs with the finite element method.
<em>TOG</em>, <em>41</em>(3), 23:1–14. (<a
href="https://doi.org/10.1145/3508372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Finite Element Method (FEM) is widely used to solve discrete Partial Differential Equations (PDEs) in engineering and graphics applications. The popularity of FEM led to the development of a large family of variants, most of which require a tetrahedral or hexahedral mesh to construct the basis. While the theoretical properties of FEM basis (such as convergence rate, stability, etc.) are well understood under specific assumptions on the mesh quality, their practical performance, influenced both by the choice of the basis construction and quality of mesh generation, have not been systematically documented for large collections of automatically meshed 3D geometries. We introduce a set of benchmark problems involving most commonly solved elliptic PDEs, starting from simple cases with an analytical solution, moving to commonly used test problem setups, and using manufactured solutions for thousands of real-world, automatically meshed geometries. For all these cases, we use state-of-the-art meshing tools to create both tetrahedral and hexahedral meshes, and compare the performance of different element types for common elliptic PDEs. The goal of this benchmark is to enable comparison of complete FEM pipelines, from mesh generation to algebraic solver, and exploration of relative impact of different factors on the overall system performance. As a specific application of our geometry and benchmark dataset, we explore the question of relative advantages of unstructured (triangular/ tetrahedral) and structured (quadrilateral/hexahedral) discretizations. We observe that for Lagrange-type elements, while linear tetrahedral elements perform poorly, quadratic tetrahedral elements perform equally well or outperform hexahedral elements for our set of problems and currently available mesh generation algorithms. This observation suggests that for common problems in structural analysis, thermal analysis, and low Reynolds number flows, high-quality results can be obtained with unstructured tetrahedral meshes, which can be created robustly and automatically. We release the description of the benchmark problems, meshes, and reference implementation of our testing infrastructure to enable statistically significant comparisons between different FE methods, which we hope will be helpful in the development of new meshing and FEA techniques.},
  archive      = {J_TOG},
  author       = {Teseo Schneider and Yixin Hu and Xifeng Gao and Jérémie Dumas and Denis Zorin and Daniele Panozzo},
  doi          = {10.1145/3508372},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {23:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A large-scale comparison of tetrahedral and hexahedral elements for solving elliptic PDEs with the finite element method},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Procedural urban forestry. <em>TOG</em>, <em>41</em>(2),
20:1–18. (<a href="https://doi.org/10.1145/3502220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The placement of vegetation plays a central role in the realism of virtual scenes. We introduce procedural placement models (PPMs) for vegetation in urban layouts. PPMs are environmentally sensitive to city geometry and allow identifying plausible plant positions based on structural and functional zones in an urban layout. PPMs can either be directly used by defining their parameters or learned from satellite images and land register data. This allows us to populate urban landscapes with complex 3D vegetation and enhance existing approaches for generating urban landscapes. Our framework’s effectiveness is shown through examples of large-scale city scenes and close-ups of individually grown tree models. We validate the results generated with our framework with a perceptual user study and its usability based on urban scene design sessions with expert users.},
  archive      = {J_TOG},
  author       = {Till Niese and Sören Pirk and Matthias Albrecht and Bedrich Benes and Oliver Deussen},
  doi          = {10.1145/3502220},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {20:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Procedural urban forestry},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constant-cost spatio-angular prefiltering of glinty
appearance using tensor decomposition. <em>TOG</em>, <em>41</em>(2),
22:1–17. (<a href="https://doi.org/10.1145/3507915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detailed glinty appearance from complex surface microstructures enhances the level of realism but is both - and time-consuming to render, especially when viewed from far away (large spatial coverage) and/or illuminated by area lights (large angular coverage). In this article, we formulate the glinty appearance rendering process as a spatio-angular range query problem of the Normal Distribution Functions (NDFs), and introduce an efficient spatio-angular prefiltering solution to it. We start by exhaustively precomputing all possible NDFs with differently sized positional coverages. Then we compress the precomputed data using tensor rank decomposition, which enables accurate and fast angular range queries. With our spatio-angular prefiltering scheme, we are able to solve both the storage and performance issues at the same time, leading to efficient rendering of glinty appearance with both constant storage and constant performance, regardless of the range of spatio-angular queries. Finally, we demonstrate that our method easily applies to practical rendering applications that were traditionally considered difficult. For example, efficient bidirectional reflection distribution function evaluation accurate NDF importance sampling, fast global illumination between glinty objects, high-frequency preserving rendering with environment lighting, and tile-based synthesis of glinty appearance.},
  archive      = {J_TOG},
  author       = {Hong Deng and Yang Liu and Beibei Wang and Jian Yang and Lei Ma and Nicolas Holzschuch and Ling-Qi Yan},
  doi          = {10.1145/3507915},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {22:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constant-cost spatio-angular prefiltering of glinty appearance using tensor decomposition},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A primitive for manual hatching. <em>TOG</em>,
<em>41</em>(2), 21:1–17. (<a
href="https://doi.org/10.1145/3503460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In art, hatching means drawing patterns of roughly parallel lines. Even with skill and time, an artist can find these patterns difficult to create and edit. Our new artistic primitive—the hatching shape—facilitates hatching for an artist drawing from imagination. A hatching shape comprises a mask and three fields: width, spacing, and direction. Streamline advection uses these fields to create hatching marks. A hatching shape also contains barrier curves: deliberate discontinuities useful for drawing complex forms. We explain several operations on hatching shapes, such as the multi-dir operation, an easy way to depict 3D form using a hatching shape’s direction field. We also explain the modifications to streamline advection necessary to produce hatching marks from a hatching shape.},
  archive      = {J_TOG},
  author       = {Greg Philbrick and Craig S. Kaplan},
  doi          = {10.1145/3503460},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {21:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A primitive for manual hatching},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PAVEL: Decorative patterns with packed volumetric elements.
<em>TOG</em>, <em>41</em>(2), 19:1–15. (<a
href="https://doi.org/10.1145/3502802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world hand-crafted objects are decorated with elements that are packed onto the object’s surface and deformed to cover it as much as possible. Examples are artisanal ceramics and metal jewelry. Inspired by these objects, we present a method to enrich surfaces with packed volumetric decorations. Our algorithm works by first determining the locations in which to add the decorative elements and then removing the non-physical overlap between them while preserving the decoration volume. For the placement, we support several strategies depending on the desired overall motif. To remove the overlap, we use an approach based on implicit deformable models creating the qualitative effect of plastic warping while avoiding expensive and hard-to-control physical simulations. Our decorative elements can be used to enhance virtual surfaces, as well as 3D-printed pieces, by assembling the decorations onto real surfaces to obtain tangible reproductions.},
  archive      = {J_TOG},
  author       = {Filippo Andrea Fanni and Fabio Pellacini and Riccardo Scateni and Andrea Giachetti},
  doi          = {10.1145/3502802},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {19:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {PAVEL: Decorative patterns with packed volumetric elements},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inverse procedural modeling pipeline for SVBRDF maps.
<em>TOG</em>, <em>41</em>(2), 18:1–17. (<a
href="https://doi.org/10.1145/3502431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural modeling is now the de facto standard of material modeling in industry. Procedural models can be edited and are easily extended, unlike pixel-based representations of captured materials. In this article, we present a semi-automatic pipeline for general material proceduralization. Given Spatially Varying Bidirectional Reflectance Distribution Functions (SVBRDFs) represented as sets of pixel maps, our pipeline decomposes them into a tree of sub-materials whose spatial distributions are encoded by their associated mask maps. This semi-automatic decomposition of material maps progresses hierarchically, driven by our new spectrum-aware material matting and instance-based decomposition methods. Each decomposed sub-material is proceduralized by a novel multi-layer noise model to capture local variations at different scales. Spatial distributions of these sub-materials are modeled either by a by-example inverse synthesis method recovering Point Process Texture Basis Functions (PPTBF) [ 30 ] or via random sampling. To reconstruct procedural material maps, we propose a differentiable rendering-based optimization that recomposes all generated procedures together to maximize the similarity between our procedural models and the input material pixel maps. We evaluate our pipeline on a variety of synthetic and real materials. We demonstrate our method’s capacity to process a wide range of material types, eliminating the need for artist designed material graphs required in previous work [ 38 , 53 ]. As fully procedural models, our results expand to arbitrary resolution and enable high-level user control of appearance.},
  archive      = {J_TOG},
  author       = {Yiwei Hu and Chengan He and Valentin Deschaintre and Julie Dorsey and Holly Rushmeier},
  doi          = {10.1145/3502431},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {18:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {An inverse procedural modeling pipeline for SVBRDF maps},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The hierarchical subspace iteration method for
laplace–beltrami eigenproblems. <em>TOG</em>, <em>41</em>(2), 17:1–14.
(<a href="https://doi.org/10.1145/3495208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse eigenproblems are important for various applications in computer graphics. The spectrum and eigenfunctions of the Laplace–Beltrami operator, for example, are fundamental for methods in shape analysis and mesh processing. The Subspace Iteration Method is a robust solver for these problems. In practice, however, Lanczos schemes are often faster. In this article, we introduce the Hierarchical Subspace Iteration Method (HSIM) , a novel solver for sparse eigenproblems that operates on a hierarchy of nested vector spaces. The hierarchy is constructed such that on the coarsest space all eigenpairs can be computed with a dense eigensolver. HSIM uses these eigenpairs as initialization and iterates from coarse to fine over the hierarchy. On each level, subspace iterations, initialized with the solution from the previous level, are used to approximate the eigenpairs. This approach substantially reduces the number of iterations needed on the finest grid compared to the non-hierarchical Subspace Iteration Method. Our experiments show that HSIM can solve Laplace–Beltrami eigenproblems on meshes faster than state-of-the-art methods based on Lanczos iterations, preconditioned conjugate gradients, and subspace iterations.},
  archive      = {J_TOG},
  author       = {Ahmad Nasikun and Klaus Hildebrandt},
  doi          = {10.1145/3495208},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {17:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {The hierarchical subspace iteration method for Laplace–Beltrami eigenproblems},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCN-denoiser: Mesh denoising with graph convolutional
networks. <em>TOG</em>, <em>41</em>(1), 8:1–14. (<a
href="https://doi.org/10.1145/3480168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present GCN-Denoiser, a novel feature-preserving mesh denoising method based on graph convolutional networks ( GCNs ). Unlike previous learning-based mesh denoising methods that exploit handcrafted or voxel-based representations for feature learning, our method explores the structure of a triangular mesh itself and introduces a graph representation followed by graph convolution operations in the dual space of triangles. We show such a graph representation naturally captures the geometry features while being lightweight for both training and inference. To facilitate effective feature learning, our network exploits both static and dynamic edge convolutions, which allow us to learn information from both the explicit mesh structure and potential implicit relations among unconnected neighbors. To better approximate an unknown noise function, we introduce a cascaded optimization paradigm to progressively regress the noise-free facet normals with multiple GCNs. GCN-Denoiser achieves the new state-of-the-art results in multiple noise datasets, including CAD models often containing sharp features and raw scan models with real noise captured from different devices. We also create a new dataset called PrintData containing 20 real scans with their corresponding ground-truth meshes for the research community. Our code and data are available at https://github.com/Jhonve/GCN-Denoiser.},
  archive      = {J_TOG},
  author       = {Yuefan Shen and Hongbo Fu and Zhongshuo Du and Xiang Chen and Evgeny Burnaev and Denis Zorin and Kun Zhou and Youyi Zheng},
  doi          = {10.1145/3480168},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {1},
  pages        = {8:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {GCN-denoiser: Mesh denoising with graph convolutional networks},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rendering of subjective speckle formed by rough statistical
surfaces. <em>TOG</em>, <em>41</em>(1), 2:1–23. (<a
href="https://doi.org/10.1145/3472293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous effort has been extended by the computer graphics community to advance the level of realism of material appearance reproduction by incorporating increasingly more advanced techniques. We are now able to re-enact the complicated interplay between light and microscopic surface features—scratches, bumps and other imperfections—in a visually convincing fashion. However, diffractive patterns arise even when no explicitly defined features are present: Any random surface will act as a diffracting aperture and its statistics heavily influence the statistics of the diffracted wave fields. Nonetheless, the problem of rendering diffraction effects induced by surfaces that are defined purely statistically remains wholly unexplored. We present a thorough derivation, from core optical principles, of the intensity of the scattered fields that arise when a natural, partially coherent light source illuminates a random surface. We follow with a probability theory analysis of the statistics of those fields and present our rendering algorithm. All of our derivations are formally proven and verified numerically as well. Our method is the first to render diffraction effects produced by a surface described statistically only, and bridges the theoretical gap between contemporary surface modelling and rendering.},
  archive      = {J_TOG},
  author       = {Shlomi Steinberg and Ling-Qi Yan},
  doi          = {10.1145/3472293},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {1},
  pages        = {2:1–23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rendering of subjective speckle formed by rough statistical surfaces},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SofGAN: A portrait image generator with dynamic styling.
<em>TOG</em>, <em>41</em>(1), 1:1–26. (<a
href="https://doi.org/10.1145/3470848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Generative Adversarial Networks (GANs) have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a SofGAN image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a semantic occupancy field (SOF). The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texturemaps and stylized to a portrait photo using our semantic instance-wise module. Through extensive experiments, we show that our system can generate high-quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications, such as appearance-consistent facial animation and dynamic styling.},
  archive      = {J_TOG},
  author       = {Anpei Chen and Ruiyang Liu and Ling Xie and Zhang Chen and Hao Su and Jingyi Yu},
  doi          = {10.1145/3470848},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {1},
  pages        = {1:1–26},
  shortjournal = {ACM Trans. Graph.},
  title        = {SofGAN: A portrait image generator with dynamic styling},
  volume       = {41},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
