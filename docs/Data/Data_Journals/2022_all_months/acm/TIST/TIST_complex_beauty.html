<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIST_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tist---109">TIST - 109</h2>
<ul>
<li><details>
<summary>
(2022). Describing UI screenshots in natural language.
<em>TIST</em>, <em>14</em>(1), 19:1–28. (<a
href="https://doi.org/10.1145/3564702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to describe any user interface (UI) screenshot in natural language can promote understanding of the main purpose of the UI, yet currently it cannot be accomplished with state-of-the-art captioning systems. We introduce XUI, a novel method inspired by the global precedence effect to create informative descriptions of UIs, starting with an overview and then providing fine-grained descriptions about the most salient elements. XUI builds upon computational models for topic classification, visual saliency prediction, and natural language generation (NLG). XUI provides descriptions with up to three different granularity levels that, together, describe what is in the interface and what the user can do with it. We found that XUI descriptions are highly readable, are perceived to accurately describe the UI, and score similarly to human-generated UI descriptions. XUI is available as open-source software.},
  archive      = {J_TIST},
  author       = {Luis A. Leiva and Asutosh Hota and Antti Oulasvirta},
  doi          = {10.1145/3564702},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {19:1–28},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Describing UI screenshots in natural language},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multitask balanced and recalibrated network for medical code
prediction. <em>TIST</em>, <em>14</em>(1), 17:1–20. (<a
href="https://doi.org/10.1145/3563041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human coders assign standardized medical codes to clinical documents generated during patients’ hospitalization, which is error prone and labor intensive. Automated medical coding approaches have been developed using machine learning methods, such as deep neural networks. Nevertheless, automated medical coding is still challenging because of complex code association, noise in lengthy documents, and the imbalanced class problem. We propose a novel neural network, called the Multitask Balanced and Recalibrated Neural Network, to solve these issues. Significantly, the multitask learning scheme shares the relationship knowledge between different coding branches to capture code association. A recalibrated aggregation module is developed by cascading convolutional blocks to extract high-level semantic features that mitigate the impact of noise in documents. Also, the cascaded structure of the recalibrated module can benefit learning from lengthy notes. To solve the imbalanced class problem, we deploy focal loss to redistribute the attention on low- and high-frequency medical codes. Experimental results show that our proposed model outperforms competitive baselines on a real-world clinical dataset called the Medical Information Mart for Intensive Care (MIMIC-III).},
  archive      = {J_TIST},
  author       = {Wei Sun and Shaoxiong Ji and Erik Cambria and Pekka Marttinen},
  doi          = {10.1145/3563041},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {17:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Multitask balanced and recalibrated network for medical code prediction},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new similarity space tailored for supervised deep metric
learning. <em>TIST</em>, <em>14</em>(1), 16:1–25. (<a
href="https://doi.org/10.1145/3559766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel deep metric learning method. Differently from many works in this area, we define a novel latent space obtained through an autoencoder. The new space, namely S-space, is divided into different regions describing positions where pairs of objects are similar/dissimilar. We locate makers to identify these regions and estimate the similarities between objects through a kernel-based Cauchy distribution to measure the markers’ distance and the new data representation. In our approach, we simultaneously estimate the markers’ position in the S-space and represent the objects in the same space. Moreover, we propose a new regularization function to prevent similar markers from collapsing altogether. Our method emphasizes the group property (separability) while preserving instance representativity. We present evidence that our proposal can represent complex spaces, for instance, when groups of similar objects are located in disjoint regions. We compare our proposal to nine different distance metric learning approaches (four of them are based on deep learning) on 28 real-world heterogeneous datasets. According to the four quantitative metrics used, our method overcomes all of the nine strategies from the literature.},
  archive      = {J_TIST},
  author       = {Pedro Barros and Fabiane Queiroz and Flávio Figueiredo and Jefersson A. Dos Santos and Heitor Ramos},
  doi          = {10.1145/3559766},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {16:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A new similarity space tailored for supervised deep metric learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Redundant label learning via subspace representation and
global disambiguation. <em>TIST</em>, <em>14</em>(1), 15:1–19. (<a
href="https://doi.org/10.1145/3558547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundant Label Learning (RLL) aims at inducing a robust model from training data, where each example is associated with a set of candidate labels, among which some of them are incorrect. Most existing approaches deal with such problem by disambiguating the candidate labels first and then inducing the predictive model from the disambiguated data. However, these approaches only focus on disambiguation for each instance’ candidate label set, while the global label context tends to be ignored. Meanwhile, these approaches usually induce the objective model by directly utilizing the original feature information, which may lead to the model overfitting due to high-dimensional redundant features. To tackle the above issues, we propose a novel feature S ubspac E R epresentation and label G lobal Disambiguat IO n ( SERGIO ) approach, which improves the generalization ability of the learning system from the perspective of both feature space and label space. Specifically, we project the original high-dimensional feature space into a low-dimensional subspace, where the projection matrix is regularized with an orthogonality constraint to make the subspace more compact. Meanwhile, we introduce a label confidence matrix and constrain it with ℓ 1 -norm and trace-norm regularization simultaneously, which are utilized to explore global label correlations and further well in accordance with the nature of single-label classification and multi-label classification problem, respectively. Extensive experiments on both single-label and multi-label RLL datasets demonstrate that our proposed method achieves competitive performance against state-of-the-art approaches.},
  archive      = {J_TIST},
  author       = {Gengyu Lyu and Songhe Feng and Wei Liu and Shuoyan Liu and Congyan Lang},
  doi          = {10.1145/3558547},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {15:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Redundant label learning via subspace representation and global disambiguation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLAG: A feedback-aware local and global model for
heterogeneous sequential recommendation. <em>TIST</em>, <em>14</em>(1),
14:1–22. (<a href="https://doi.org/10.1145/3557046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous sequential recommendation that models sequences of items associated with more than one type of feedback such as examinations and purchases is an emerging topic in the research community, which is also an important problem in many real-world applications. Though there are some methods proposed to exploit different types of feedback in item sequences such as RLBL, RIB, and BINN, they are based on RNN and may not be very competitive in capturing users’ complex and dynamic preferences. And most existing advanced sequential recommendation methods such as the CNN- and attention-based methods are often designed for making use of item sequences with one single type of feedback, which thus can not be applied to the studied problem directly. As a response, we propose a novel feedback-aware local and global (FLAG) preference learning model for heterogeneous sequential recommendation. Our FLAG contains four modules, including (i) a local preference learning module for capturing a user’s short-term interest, which adopts a novel feedback-aware self-attention block to distinguish different types of feedback; (ii) a global preference learning module for modeling a user’s global preference; (iii) a local intention learning module, which takes a user’s real feedback in the next step, i.e., the user’s intention at the current step, as the query vector in a self-attention block to figure out the items that match the user’s intention well; and (iv) a prediction module for preference integration and final prediction. We then conduct extensive experiments on three public datasets and find that our FLAG significantly outperforms 13 very competitive baselines in terms of two commonly used ranking-oriented metrics in most cases. We also include ablation studies and sensitivity analysis of our FLAG to have more in-depth insights.},
  archive      = {J_TIST},
  author       = {Mingkai He and Jing Lin and Jinwei Luo and Weike Pan and Zhong Ming},
  doi          = {10.1145/3557046},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {14:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FLAG: A feedback-aware local and global model for heterogeneous sequential recommendation},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized online learning: Take benefits from others’
data without sharing your own to track global trend. <em>TIST</em>,
<em>14</em>(1), 13:1–22. (<a
href="https://doi.org/10.1145/3559765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized online learning (online learning in decentralized networks) has been attracting more and more attention, since it is believed that decentralized online learning can help data providers cooperatively better solve their online problems without sharing their private data to a third party or other providers. Typically, the cooperation is achieved by letting the data providers exchange their models between neighbors, e.g., recommendation model. However, the best regret bound for a decentralized online learning algorithm is 𝒪( n √ T ), where n is the number of nodes (or users) and T is the number of iterations. This is clearly insignificant, since this bound can be achieved without any communication in the networks. This reminds us to ask a fundamental question: Can people really get benefit from the decentralized online learning by exchanging information? In this article, we studied when and why the communication can help the decentralized online learning to reduce the regret. Specifically, each loss function is characterized by two components: the adversarial component and the stochastic component. Under this characterization, we show that decentralized online gradient enjoys a regret bound \( {\mathcal {O}(\sqrt {n^2TG^2 + n T \sigma ^2})} \), where G measures the magnitude of the adversarial component in the private data (or equivalently the local loss function) and σ measures the randomness within the private data. This regret suggests that people can get benefits from the randomness in the private data by exchanging private information. Another important contribution of this article is to consider the dynamic regret—a more practical regret to track users’ interest dynamics. Empirical studies are also conducted to validate our analysis.},
  archive      = {J_TIST},
  author       = {Wendi Wu and Zongren Li and Yawei Zhao and Chen Yu and Peilin Zhao and Ji Liu and Kunlun He},
  doi          = {10.1145/3559765},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {13:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Decentralized online learning: Take benefits from others’ data without sharing your own to track global trend},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised discriminative representation learning by
fuzzy autoencoder. <em>TIST</em>, <em>14</em>(1), 11:1–18. (<a
href="https://doi.org/10.1145/3555777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning based on autoencoders has received great concern for its potential ability to capture valuable latent information. Conventional autoencoders pursue minimal reconstruction error, but in most machine learning tasks such as classification and clustering, the discrimination of feature representation is also important. To address this limitation, an enhanced self-supervised discriminative fuzzy autoencoder (FAE) is innovatively proposed, which focuses on exploring information within data to guide the unsupervised training process and enhancing feature discrimination in a self-supervised manner. In FAE, fuzzy membership is applied to provide a means of self-supervised, which allows FAE can not only utilize AE’s outstanding representation learning capabilities but can also transform the original data into another space with improved discrimination. First, the objective function corresponding to FAE is proposed by reconstruction loss and clustering oriented loss simultaneously. Subsequently, Mini-Batch Gradient Descent is applied to infer the objective function and the detailed process is illustrated step by step. Finally, empirical studies on clustering tasks have demonstrated the superiority of FAE over the state of the art.},
  archive      = {J_TIST},
  author       = {Wenlu Yang and Hongjun Wang and Yinghui Zhang and Zehao Liu and Tianrui Li},
  doi          = {10.1145/3555777},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {11:1–18},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Self-supervised discriminative representation learning by fuzzy autoencoder},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning embedded into smart traps for fruit insect
pests detection. <em>TIST</em>, <em>14</em>(1), 10:1–24. (<a
href="https://doi.org/10.1145/3552435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel approach to identify two species of fruit insect pests as part of a network of intelligent traps designed to monitor the population of these insects in a plantation. The proposed approach uses a simple Digital Image Processing technique to detect regions in the image that are likely the monitored pests and an Artificial Neural Network to classify the regions into the right class given their characteristics. This identification is done essentially by a Convolutional Neural Network (CNN), which learns the characteristics of the insects based on their images made from the adhesive floor inside a trap. We have trained several CNN architectures, with different configurations, through a data set of images collected in the field. We aimed to find the model with the highest precision and the lowest time needed for the classification. The best performance in classification was achieved by ResNet18, with a precision of 93.55\% and 91.28\% for the classification of the pests focused on this study, named Ceratitis capitata and Grapholita molesta , respectively, and a 90.72\%overall accuracy. Yet, the classification must be embedded on a resource-constrained system inside the trap, then we exploited SqueezeNet, MobileNet, and MNASNet architectures to achieve a model with lesser inference time and small losses in accuracy when compared to the models we assessed. We also attempted to quantize our highest precision model to reduce even more inference time in embedded systems, which achieved a precision of 88.76\% and 89.73\% for C. capitata and G. molesta , respectively; notwithstanding, a decrease of roughly 2\% on the overall accuracy was endured. According to the expertise of our partner company, our results are worthwhile for a real-world application, since general human laborers have a precision of about 85\%.},
  archive      = {J_TIST},
  author       = {Lucas Freitas and Valter Martins and Marilton de Aguiar and Lisane de Brisolara and Paulo Ferreira},
  doi          = {10.1145/3552435},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {10:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Deep learning embedded into smart traps for fruit insect pests detection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain generalization for activity recognition via adaptive
feature fusion. <em>TIST</em>, <em>14</em>(1), 9:1–21. (<a
href="https://doi.org/10.1145/3552434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition requires the efforts to build a generalizable model using the training datasets with the hope to achieve good performance in test datasets. However, in real applications, the training and testing datasets may have totally different distributions due to various reasons such as different body shapes, acting styles, and habits, damaging the model’s generalization performance. While such a distribution gap can be reduced by existing domain adaptation approaches, they typically assume that the test data can be accessed in the training stage, which is not realistic. In this article, we consider a more practical and challenging scenario: domain-generalized activity recognition (DGAR) where the test dataset cannot be accessed during training. To this end, we propose Adaptive Feature Fusion for Activity Recognition (AFFAR) , a domain generalization approach that learns to fuse the domain-invariant and domain-specific representations to improve the model’s generalization performance. AFFAR takes the best of both worlds where domain-invariant representations enhance the transferability across domains and domain-specific representations leverage the model discrimination power from each domain. Extensive experiments on three public HAR datasets show its effectiveness. Furthermore, we apply AFFAR to a real application, i.e., the diagnosis of Children’s Attention Deficit Hyperactivity Disorder (ADHD), which also demonstrates the superiority of our approach.},
  archive      = {J_TIST},
  author       = {Xin Qin and Jindong Wang and Yiqiang Chen and Wang Lu and Xinlong Jiang},
  doi          = {10.1145/3552434},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {9:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Domain generalization for activity recognition via adaptive feature fusion},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical multi-agent model for reinforced medical
resource allocation with imperfect information. <em>TIST</em>,
<em>14</em>(1), 8:1–27. (<a
href="https://doi.org/10.1145/3552436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the COVID-19 pandemic, the shortage in medical resources became increasingly more evident. Therefore, efficient strategies for medical resource allocation are urgently needed. However, conventional rule-based methods employed by public health experts have limited capability in dealing with the complex and dynamic pandemic-spreading situation. In addition, model-based optimization methods such as dynamic programming (DP) fail to work since we cannot obtain a precise model in real-world situations most of the time. Model-free reinforcement learning (RL) is a powerful tool for decision-making; however, three key challenges exist in solving this problem via RL: (1) complex situations and countless choices for decision-making in the real world; (2) imperfect information due to the latency of pandemic spreading; and (3) limitations on conducting experiments in the real world since we cannot set up pandemic outbreaks arbitrarily. In this article, we propose a hierarchical RL framework with several specially designed components. We design a decomposed action space with a corresponding training algorithm to deal with the countless choices, ensuring efficient and real-time strategies. We design a recurrent neural network–based framework to utilize the imperfect information obtained from the environment. We also design a multi-agent voting method, which modifies the decision-making process considering the randomness during model training and, thus, improves the performance. We build a pandemic-spreading simulator based on real-world data, serving as the experimental platform. We then conduct extensive experiments. The results show that our method outperforms all baselines, which reduces infections and deaths by 14.25\% on average without the multi-agent voting method and up to 15.44\% with it.},
  archive      = {J_TIST},
  author       = {Qianyue Hao and Fengli Xu and Lin Chen and Pan Hui and Yong Li},
  doi          = {10.1145/3552436},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {8:1–27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Hierarchical multi-agent model for reinforced medical resource allocation with imperfect information},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How is the stroke? Inferring shot influence in badminton
matches via long short-term dependencies. <em>TIST</em>, <em>14</em>(1),
7:1–22. (<a href="https://doi.org/10.1145/3551391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying significant shots in a rally is important for evaluating players’ performance in badminton matches. While there are several studies that have quantified player performance in other sports, analyzing badminton data has remained untouched. In this article, we introduce a badminton language to fully describe the process of the shot, and we propose a deep-learning model composed of a novel short-term extractor and a long-term encoder for capturing a shot-by-shot sequence in a badminton rally by framing the problem as predicting a rally result. Our model incorporates an attention mechanism to enable the transparency between the action sequence and the rally result, which is essential for badminton experts to gain interpretable predictions. Experimental evaluation based on a real-world dataset demonstrates that our proposed model outperforms the strong baselines. We also conducted case studies to show the ability to enhance players’ decision-making confidence and to provide advanced insights for coaching, which benefits the badminton analysis community and bridges the gap between the field of badminton and computer science.},
  archive      = {J_TIST},
  author       = {Wei-Yao Wang and Teng-Fong Chan and Wen-Chih Peng and Hui-Kuo Yang and Chih-Chuan Wang and Yao-Chung Fan},
  doi          = {10.1145/3551391},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {7:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {How is the stroke? inferring shot influence in badminton matches via long short-term dependencies},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporation of data-mined knowledge into black-box SVM for
interpretability. <em>TIST</em>, <em>14</em>(1), 6:1–22. (<a
href="https://doi.org/10.1145/3548775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of interpretability often makes black-box models challenging to be applied in many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-mined knowledge into the black-box soft-margin SVM model to enhance accuracy and interpretability. The concept and incorporation mechanism of data-mined knowledge are successively developed, based on which a partially interpretable soft-margin SVM ( pTsm -SVM) optimization model is designed and then solved through reformulating the optimization problem as standard quadratic programming. An algorithm for mining linear positive (negative) class knowledge from general data sets is also proposed, which generates a linear two-dimensional discriminative rule with specificity (sensitivity) equal to 1 and the highest possible sensitivity (specificity) among all two-dimensional feature spaces. The knowledge-integrated pTsm -SVM works by achieving a good trade-off among the “large margin”, “high specificity”, and “high sensitivity”. Our experimental results on eight UCI datasets demonstrate the superiority of the proposed pTsm -SVM over the standard soft-margin SVM both in terms of accuracy and interpretability.},
  archive      = {J_TIST},
  author       = {Shaohan Chen and Chuanhou Gao and Ping Zhang},
  doi          = {10.1145/3548775},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {6:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Incorporation of data-mined knowledge into black-box SVM for interpretability},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial learning for cross domain recommendations.
<em>TIST</em>, <em>14</em>(1), 5:1–25. (<a
href="https://doi.org/10.1145/3548776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cross domain recommender systems typically assume homogeneous user preferences across multiple domains to capture similarities of user-item interactions and to provide cross domain recommendations accordingly. Meanwhile, the heterogeneity of user behaviors is usually not well studied and captured during the recommendation process, where users might have vastly different interests in different domains. In addition, previous models focus primarily on recommendation tasks between domain pairs, and cannot be naturally extended to serve for multiple domain recommendation applications. To address these challenges, we propose to utilize the idea of adversarial learning to intelligently incorporate global user preferences and domain-specific user preferences for providing satisfying cross domain recommendations. In particular, our proposed Adversarial Cross Domain Recommendation (ACDR) model first obtains the latent representations of global user preferences from their explicit feature information, and then transforms them into domain-specific user embeddings, where we take into account user behaviors and their heterogeneous preferences among different domains. By doing so, we address the differences among user representations in the domain-specific latent space while also preserving global user preferences, as we effectively segment the distributions of domain-specific user embeddings in the shared latent space. The convergence of our proposed model is theoretically guaranteed. The proposed ACDR model leads to significant and consistent improvements in cross domain recommendation performance over the state-of-the-art baseline models, which we demonstrate through extensive experiments on three real-world datasets. In addition, we show that the improvements are greater on those datasets that are smaller and more sparse, on those users that have fewer interaction records in the dataset, and when user interactions from more product domains are included in the cross domain recommendation model.},
  archive      = {J_TIST},
  author       = {Pan Li and Brian Brost and Alexander Tuzhilin},
  doi          = {10.1145/3548776},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {5:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Adversarial learning for cross domain recommendations},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trustworthy AI: A computational perspective. <em>TIST</em>,
<em>14</em>(1), 4:1–59. (<a
href="https://doi.org/10.1145/3546872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone’s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety &amp; Robustness, (ii) Nondiscrimination &amp; Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability &amp; Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.},
  archive      = {J_TIST},
  author       = {Haochen Liu and Yiqi Wang and Wenqi Fan and Xiaorui Liu and Yaxin Li and Shaili Jain and Yunhao Liu and Anil Jain and Jiliang Tang},
  doi          = {10.1145/3546872},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {4:1–59},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Trustworthy AI: A computational perspective},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). No free lunch theorem for security and utility in federated
learning. <em>TIST</em>, <em>14</em>(1), 1:1–35. (<a
href="https://doi.org/10.1145/3563219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a federated learning scenario where multiple parties jointly learn a model from their respective data, there exist two conflicting goals for the choice of appropriate algorithms. On one hand, private and sensitive training data must be kept secure as much as possible in the presence of semi-honest partners; on the other hand, a certain amount of information has to be exchanged among different parties for the sake of learning utility. Such a challenge calls for the privacy-preserving federated learning solution, which maximizes the utility of the learned model and maintains a provable privacy guarantee of participating parties’ private data. This article illustrates a general framework that (1) formulates the trade-off between privacy loss and utility loss from a unified information-theoretic point of view, and (2) delineates quantitative bounds of the privacy-utility trade-off when different protection mechanisms including randomization, sparsity, and homomorphic encryption are used. It was shown that in general there is no free lunch for the privacy-utility trade-off , and one has to trade the preserving of privacy with a certain degree of degraded utility. The quantitative analysis illustrated in this article may serve as the guidance for the design of practical federated learning algorithms.},
  archive      = {J_TIST},
  author       = {Xiaojin Zhang and Hanlin Gu and Lixin Fan and Kai Chen and Qiang Yang},
  doi          = {10.1145/3563219},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {1:1–35},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {No free lunch theorem for security and utility in federated learning},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shifting capsule networks from the cloud to the deep edge.
<em>TIST</em>, <em>13</em>(6), 105:1–25. (<a
href="https://doi.org/10.1145/3544562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule networks (CapsNets) are an emerging trend in image processing. In contrast to a convolutional neural network, CapsNets are not vulnerable to object deformation, as the relative spatial information of the objects is preserved across the network. However, their complexity is mainly related to the capsule structure and the dynamic routing mechanism, which makes it almost unreasonable to deploy a CapsNet, in its original form, in a resource-constrained device powered by a small microcontroller (MCU). In an era where intelligence is rapidly shifting from the cloud to the edge, this high complexity imposes serious challenges to the adoption of CapsNets at the very edge. To tackle this issue, we present an API for the execution of quantized CapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm CMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers as operands. Along with it, we propose a framework to perform post-training quantization of a CapsNet. Results show a reduction in memory footprint of almost 75\%, with accuracy loss ranging from 0.07\% to 0.18\%. In terms of throughput, our Arm Cortex-M API enables the execution of primary capsule and capsule layers with medium-sized kernels in just 119.94 and 90.60 ms, respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC (RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms, respectively.},
  archive      = {J_TIST},
  author       = {Miguel Costa and Diogo Costa and Tiago Gomes and Sandro Pinto},
  doi          = {10.1145/3544562},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {105:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Shifting capsule networks from the cloud to the deep edge},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AggEnhance: Aggregation enhancement by class interior points
in federated learning with non-IID data. <em>TIST</em>, <em>13</em>(6),
104:1–25. (<a href="https://doi.org/10.1145/3544495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a privacy-preserving paradigm for multi-institutional collaborations, where the aggregation is an essential procedure after training on the local datasets. Conventional aggregation algorithms often apply a weighted averaging of the updates generated from distributed machines to update the global model. However, while the data distributions are non-IID, the large discrepancy between the local updates might lead to a poor averaged result and a lower convergence speed, i.e., more iterations required to achieve a certain performance. To solve this problem, this article proposes a novel method named AggEnhance for enhancing the aggregation, where we synthesize a group of reliable samples from the local models and tune the aggregated result on them. These samples, named class interior points (CIPs) in this work, bound the relevant decision boundaries that ensure the performance of aggregated result. To the best of our knowledge, this is the first work to explicitly design an enhancing method for the aggregation in prevailing FL pipelines. A series of experiments on real data demonstrate that our method has noticeable improvements of the convergence in non-IID scenarios. In particular, our approach reduces the iterations by 31.87\% on average for the CIFAR10 dataset and 43.90\% for the PASCAL VOC dataset. Since our method does not modify other procedures of FL pipelines, it is easy to apply to most existing FL frameworks. Furthermore, it does not require additional data transmitted from the local clients to the global server, thus holding the same security level as the original FL algorithms.},
  archive      = {J_TIST},
  author       = {Jinxiang Ou and Yunheng Shen and Feng Wang and Qiao Liu and Xuegong Zhang and Hairong Lv},
  doi          = {10.1145/3544495},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {104:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {AggEnhance: Aggregation enhancement by class interior points in federated learning with non-IID data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling continuous time sequences with intermittent
observations using marked temporal point processes. <em>TIST</em>,
<em>13</em>(6), 103:1–26. (<a
href="https://doi.org/10.1145/3545118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large fraction of data generated via human activities such as online purchases, health records, spatial mobility, etc. can be represented as a sequence of events over a continuous-time. Learning deep learning models over these continuous-time event sequences is a non-trivial task as it involves modeling the ever-increasing event timestamps, inter-event time gaps, event types, and the influences between different events within and across different sequences. In recent years, neural enhancements to marked temporal point processes (MTPP) have emerged as a powerful framework to model the underlying generative mechanism of asynchronous events localized in continuous time. However, most existing models and inference methods in the MTPP framework consider only the complete observation scenario i.e., the event sequence being modeled is completely observed with no missing events – an ideal setting that is rarely applicable in real-world applications. A recent line of work which considers missing events while training MTPP utilizes supervised learning techniques that require additional knowledge of missing or observed label for each event in a sequence, which further restricts its practicability as in several scenarios the details of missing events is not known a priori . In this work, we provide a novel unsupervised model and inference method for learning MTPP in presence of event sequences with missing events. Specifically, we first model the generative processes of observed events and missing events using two MTPP, where the missing events are represented as latent random variables. Then, we devise an unsupervised training method that jointly learns both the MTPP by means of variational inference. Such a formulation can effectively impute the missing data among the observed events, which in turn enhances its predictive prowess, and can identify the optimal position of missing events in a sequence. Experiments with eight real-world datasets show that IMTPP outperforms the state-of-the-art MTPP frameworks for event prediction and missing data imputation, and provides stable optimization.},
  archive      = {J_TIST},
  author       = {Vinayak Gupta and Srikanta Bedathur and Sourangshu Bhattacharya and Abir De},
  doi          = {10.1145/3545118},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {103:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Modeling continuous time sequences with intermittent observations using marked temporal point processes},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving collaborative filtering by distributed
mediation. <em>TIST</em>, <em>13</em>(6), 102:1–26. (<a
href="https://doi.org/10.1145/3542950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have become very influential in our everyday decision making, e.g., helping us choose a movie from a content platform, or offering us suitable products on e-commerce websites. While most vendors who utilize recommender systems rely exclusively on training data consisting of past transactions that took place through them, it would be beneficial to base recommendations on the rating data of more than one vendor. However, enlarging the training data by means of sharing information between different vendors may jeopardize the privacy of users. We devise here secure multi-party protocols that enable the practice of Collaborative Filtering (CF) in a manner that preserves the privacy of the vendors and users. Shmueli and Tassa [ 38 ] introduced privacy-preserving protocols of CF that involved a mediator; namely, an external entity that assists in performing the computations. They demonstrated the significant advantages of mediation in that context. We take here the mediation approach into the next level by using several independent mediators. Such distributed mediation maintains all of the advantages that were identified by Shmueli and Tassa, and offers additional ones, in comparison with the single-mediator protocols: stronger security and dramatically shorter runtimes. In addition, while all prior art assumed limited and unrealistic settings, in which each user can purchase any given item through only one vendor, we consider here a general and more realistic setting, which encompasses all previously considered settings, where users can choose between different competing vendors. We demonstrate the appealing performance of our protocols through extensive experimentation.},
  archive      = {J_TIST},
  author       = {Tamir Tassa and Alon Ben Horin},
  doi          = {10.1145/3542950},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {102:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Privacy-preserving collaborative filtering by distributed mediation},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting extreme traffic events via a context augmented
graph autoencoder. <em>TIST</em>, <em>13</em>(6), 101:1–23. (<a
href="https://doi.org/10.1145/3539735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely detection of large events on urban transportation networks enables informed mobility management. This work tackles the problem of extreme event detection on large-scale transportation networks using origin-destination mobility data, which is now widely available. Such data is highly structured in time and space, but high dimensional and sparse. Current multivariate time series anomaly detection methods cannot fully address these challenges. To exploit the structure of mobility data, we formulate the event detection problem in a novel way, as detecting anomalies in a set of time-dependent directed weighted graphs. We further propose a Context augmented Graph Autoencoder (Con-GAE) model to solve the problem, which leverages graph embedding and context embedding techniques to capture the spatial and temporal patterns. Con-GAE adopts an autoencoder framework and detects anomalies via semi-supervised learning. The performance of the method is assessed on several city-scale travel-time datasets from Uber Movement, New York taxis, and Chicago taxis and compared to state-of-the-art approaches. The proposed Con-GAE can achieve an improvement in the area under the curve score as large as 0.15 over the second best method. We also discuss real-world traffic anomalies detected by Con-GAE.},
  archive      = {J_TIST},
  author       = {Yue Hu and Ao Qu and Dan Work},
  doi          = {10.1145/3539735},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {101:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Detecting extreme traffic events via a context augmented graph autoencoder},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Budget distributed support vector machine for non-ID
federated learning scenarios. <em>TIST</em>, <em>13</em>(6), 100:1–25.
(<a href="https://doi.org/10.1145/3539734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been remarkable growth in Federated Learning (FL) approaches because they have proven to be very effective in training large Machine Learning (ML) models and also serve to preserve data confidentiality, as recommended by the GDPR or other business confidentiality restrictions that may apply. Despite the success of FL, performance is greatly reduced when data is not distributed identically (non-ID) across participants, as local model updates tend to diverge from the optimal global solution and thus the model averaging procedure in the aggregator is less effective. Kernel methods such as Support Vector Machines (SVMs) have not seen an equivalent evolution in the area of privacy preserving edge computing because they suffer from inherent computational, privacy and scalability issues. Furthermore, non-linear SVMs do not naturally lead to federated schemes, since locally trained models cannot be passed to the aggregator because they reveal training data (they are built on Support Vectors), and the global model cannot be updated at every worker using gradient descent. In this article, we explore the use of a particular controlled complexity (“Budget”) Distributed SVM (BDSVM) in the FL scenario with non-ID data, which is the least favorable situation, but very common in practice. The proposed BDSVM algorithm is as follows: model weights are broadcasted to workers, which locally update some kernel Gram matrices computed according to a common architectural base and send them back to the aggregator, which finally combines them, updates the global model, and repeats the procedure until a convergence criterion is met. Experimental results using synthetic 2D datasets show that the proposed method can obtain maximal margin decision boundaries even when the data is non-ID distributed. Further experiments using real-world datasets with non-ID data distribution show that the proposed algorithm provides better performance with less communication requirements than a comparable Multilayer Perceptron (MLP) trained using FedAvg. The advantage is more remarkable for a larger number of edge devices. We have also demonstrated the robustness of the proposed method against information leakage, membership inference attacks, and situations with dropout or straggler participants. Finally, in experiments run on separate processes/machines interconnected via the cloud messaging service developed in the context of the EU-H2020 MUSKETEER project, BDSVM is able to train better models than FedAvg in about half the time.},
  archive      = {J_TIST},
  author       = {A. Navia-Vázquez and R. Díaz-Morales and M. Fernández-Díaz},
  doi          = {10.1145/3539734},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {100:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Budget distributed support vector machine for non-ID federated learning scenarios},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-silo federated learning for multi-tier networks with
vertical and horizontal data partitioning. <em>TIST</em>,
<em>13</em>(6), 99:1–27. (<a
href="https://doi.org/10.1145/3543433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider federated learning in tiered communication networks. Our network model consists of a set of silos, each holding a vertical partition of the data. Each silo contains a hub and a set of clients, with the silo’s vertical data shard partitioned horizontally across its clients. We propose Tiered Decentralized Coordinate Descent (TDCD), a communication-efficient decentralized training algorithm for such two-tiered networks. The clients in each silo perform multiple local gradient steps before sharing updates with their hub to reduce communication overhead. Each hub adjusts its coordinates by averaging its workers’ updates, and then hubs exchange intermediate updates with one another. We present a theoretical analysis of our algorithm and show the dependence of the convergence rate on the number of vertical partitions and the number of local updates. We further validate our approach empirically via simulation-based experiments using a variety of datasets and objectives.},
  archive      = {J_TIST},
  author       = {Anirban Das and Timothy Castiglia and Shiqiang Wang and Stacy Patterson},
  doi          = {10.1145/3543433},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {99:1–27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Cross-silo federated learning for multi-tier networks with vertical and horizontal data partitioning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of aggregation-based group
recommender systems for ephemeral groups. <em>TIST</em>, <em>13</em>(6),
98:1–26. (<a href="https://doi.org/10.1145/3542804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems ( RecSys ) provide suggestions in many decision-making processes. Given that groups of people can perform many real-world activities (e.g., a group of people attending a conference looking for a place to dine), the need for recommendations for groups has increased. A wide range of Group Recommender Systems ( GRecSys ) has been developed to aggregate individual preferences to group preferences. We analyze 175 studies related to GRecSys . Previous works evaluate their systems using different types of groups (sizes and cohesiveness), and most of such works focus on testing their systems using only one type of item, called Experience Goods (EG). As a consequence, it is hard to get consistent conclusions about the performance of GRecSys . We present the aggregation strategies and aggregation functions that GRecSys commonly use to aggregate group members’ preferences. This study experimentally compares the performance (i.e., accuracy, ranking quality, and usefulness) using four metrics (Hit Ratio, Normalize Discounted Cumulative Gain, Diversity, and Coverage) of eight representative RecSys for group recommendations on ephemeral groups. Moreover, we use two different aggregation strategies, 10 different aggregation functions, and two different types of items on two types of datasets (EG and Search Goods (SG)) containing real-life datasets. The results show that the evaluation of GRecSys needs to use both EG and SG types of data, because the different characteristics of datasets lead to different performance. GRecSys using Singular Value Decomposition or Neural Collaborative Filtering methods work better than others. It is observed that the Average aggregation function is the one that produces better results.},
  archive      = {J_TIST},
  author       = {Edgar Ceh-Varela and Huiping Cao and Hady W. Lauw},
  doi          = {10.1145/3542804},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {98:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Performance evaluation of aggregation-based group recommender systems for ephemeral groups},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation learning on variable length and incomplete
wearable-sensory time series. <em>TIST</em>, <em>13</em>(6), 97:1–21.
(<a href="https://doi.org/10.1145/3531228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of wearable sensors (e.g., smart wristband) is creating unprecedented opportunities to not only inform health and wellness states of individuals, but also assess and infer personal attributes, including demographic and personality attributes. However, the data captured from wearables, such as heart rate or number of steps, present two key challenges: (1) the time series is often of variable length and incomplete due to different data collection periods (e.g., wearing behavior varies by person); and (2) there is inter-individual variability to external factors like stress and environment. This article addresses these challenges and brings us closer to the potential of personalized insights about an individual, taking the leap from quantified self to qualified self. Specifically, HeartSpace proposed in this article learns embedding of the time-series data with variable length and missing values via the integration of a time-series encoding module and a pattern aggregation network. Additionally, HeartSpace implements a Siamese-triplet network to optimize representations by jointly capturing intra- and inter-series correlations during the embedding learning process. The empirical evaluation over two different real-world data presents significant performance gains over state-of-the-art baselines in a variety of applications, including user identification, personality prediction, demographics inference, job performance prediction, and sleep duration estimation.},
  archive      = {J_TIST},
  author       = {Xian Wu and Chao Huang and Pablo Robles-Granda and Nitesh V. Chawla},
  doi          = {10.1145/3531228},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {97:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Representation learning on variable length and incomplete wearable-sensory time series},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Steering-by-example for progressive visual analytics.
<em>TIST</em>, <em>13</em>(6), 96:1–26. (<a
href="https://doi.org/10.1145/3531229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progressive visual analytics allows users to interact with early, partial results of long-running computations on large datasets. In this context, computational steering is often brought up as a means to prioritize the progressive computation. This is meant to focus computational resources on data subspaces of interest so as to ensure their computation is completed before all others. Yet, current approaches to select a region of the view space and then to prioritize its corresponding data subspace either require a one-to-one mapping between view and data space, or they need to establish and maintain computationally costly index structures to trace complex mappings between view and data space. We present steering-by-example, a novel interactive steering approach for progressive visual analytics, which allows prioritizing data subspaces for the progression by generating a relaxed query from a set of selected data items. Our approach works independently of the particular visualization technique and without additional index structures. First benchmark results show that steering-by-example considerably improves Precision and Recall for prioritizing unprocessed data for a selected view region, clearly outperforming random uniform sampling.},
  archive      = {J_TIST},
  author       = {Marius Hogräfer and Marco Angelini and Giuseppe Santucci and Hans-Jörg Schulz},
  doi          = {10.1145/3531229},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {96:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Steering-by-example for progressive visual analytics},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A holistic approach for role inference and action
anticipation in human teams. <em>TIST</em>, <em>13</em>(6), 95:1–24. (<a
href="https://doi.org/10.1145/3531230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to anticipate human actions is critical to many cyber-physical systems, such as robots and autonomous vehicles. Computer vision and sensing algorithms to date have focused on extracting and predicting visual features that are explicit in the scene, such as color, appearance, actions, positions, and velocities, using video and physical measurements, such as object depth and motion. Human actions, however, are intrinsically influenced and motivated by many implicit factors such as context, human roles and interactions, past experience, and inner goals or intentions. For example, in a sport team, the team strategy, player role, and dynamic circumstances driven by the behavior of the opponents, all influence the actions of each player. This article proposes a holistic framework for incorporating visual features, as well as hidden information, such as social roles, and domain knowledge. The approach, relying on a novel dynamic Markov random field (DMRF) model, infers the instantaneous team strategy and, subsequently, the players’ roles that are temporally evolving throughout the game. The results from the DMRF inference stage are then integrated with instantaneous visual features, such as individual actions and position, in order to perform holistic action anticipation using a multi-layer perceptron (MLP). The approach is demonstrated on the team sport of volleyball, by first training the DMRF and MLP offline with past videos, and, then, by applying them to new volleyball videos online. These results show that the method is able to infer the players’ roles with an average accuracy of 86.99\%, and anticipate future actions over a sequence of up to 46 frames with an average accuracy of 80.50\%. Additionally, the method predicts the onset and duration of each action achieving a mean relative error of 14.57\% and 15.67\%, respectively.},
  archive      = {J_TIST},
  author       = {Junyi Dong and Qingze Huo and Silvia Ferrari},
  doi          = {10.1145/3531230},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {95:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A holistic approach for role inference and action anticipation in human teams},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAFE and SOUP: Toward adaptive VDI workload prediction.
<em>TIST</em>, <em>13</em>(6), 94:1–28. (<a
href="https://doi.org/10.1145/3529536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Virtual Desktop Infrastructure (VDI) system, effective resource management is rather important where turning off spare virtual machines would help save running cost while maintaining sufficient virtual machines is essential to secure satisfactory user experience. Current VDI resource management strategy works in a passive manner by either reactively driving available capacity based on user demands or following manually configured schedules, which may lead to unnecessary running costs or unsatisfactory user experience. In this article, we propose a first attempt toward proactive VDI resource management, where two adaptive learning approaches for VDI workload prediction are proposed by learning from multi-grained historical features. For non-persistent desktop pool, based on the aggregation session count of pool-sharing users, the CAFE approach induces a pool-level workload predictive model by utilizing coarse-to-fine historical features extracted from aggregation workload data. For persistent desktop pool, based on the session connection status of individual users within the same pool, the SOUP approach induces user-level workload predictive model by incorporating encoded multi-grained features extracted from the logon behavior of individual users into an aggregation pool-level model. Extensive experiments on datasets of real VDI customers and electricity load evidently verify the effectiveness of the proposed adaptive approaches for VDI workload prediction as well as other workload prediction tasks.},
  archive      = {J_TIST},
  author       = {Yao Zhang and Wenping Fan and Qichen Hao and Xinya Wu and Min-Ling Zhang},
  doi          = {10.1145/3529536},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {94:1–28},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {CAFE and SOUP: Toward adaptive VDI workload prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaDetector: Meta event knowledge transfer for fake news
detection. <em>TIST</em>, <em>13</em>(6), 93:1–25. (<a
href="https://doi.org/10.1145/3532851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blooming of fake news on social networks has devastating impacts on society, the economy, and public security. Although numerous studies are conducted for the automatic detection of fake news, the majority tend to utilize deep neural networks to learn event-specific features for superior detection performance on specific datasets. However, the trained models heavily rely on the training datasets and are infeasible to apply to upcoming events due to the discrepancy between event distributions. Inspired by domain adaptation theories, we propose an end-to-end adversarial adaptation network, dubbed as MetaDetector , to transfer meta knowledge (event-shared features) between different events. Specifically, MetaDetector pushes the feature extractor and event discriminator to eliminate event-specific features and preserve required meta knowledge by adversarial training. Furthermore, the pseudo-event discriminator is utilized to evaluate the importance of news records in historical events to obtain partial knowledge that are discriminative for detecting fake news. Under the coordinated optimization among all the submodules, MetaDetector accurately transfers the meta knowledge of historical events to the upcoming event for fact checking. We conduct extensive experiments on two real-world datasets collected from Sina Weibo and Twitter. The experimental results demonstrate that MetaDetector outperforms the state-of-the-art methods, especially when the distribution discrepancy between events is significant.},
  archive      = {J_TIST},
  author       = {Yasan Ding and Bin Guo and Yan Liu and Yunji Liang and Haocheng Shen and Zhiwen Yu},
  doi          = {10.1145/3532851},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {93:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {MetaDetector: Meta event knowledge transfer for fake news detection},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoPhy -PGNN: Learning physics-guided neural networks with
competing loss functions for solving eigenvalue problems. <em>TIST</em>,
<em>13</em>(6), 92:1–23. (<a
href="https://doi.org/10.1145/3530911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-guided Neural Networks (PGNNs) represent an emerging class of neural networks that are trained using physics-guided (PG) loss functions (capturing violations in network outputs with known physics), along with the supervision contained in data. Existing work in PGNNs has demonstrated the efficacy of adding single PG loss functions in the neural network objectives, using constant tradeoff parameters, to ensure better generalizability. However, in the presence of multiple PG functions with competing gradient directions, there is a need to adaptively tune the contribution of different PG loss functions during the course of training to arrive at generalizable solutions. We demonstrate the presence of competing PG losses in the generic neural network problem of solving for the lowest (or highest) eigenvector of a physics-based eigenvalue equation, which is commonly encountered in many scientific problems. We present a novel approach to handle competing PG losses and demonstrate its efficacy in learning generalizable solutions in two motivating applications of quantum mechanics and electromagnetic propagation. All the code and data used in this work are available at https://github.com/jayroxis/Cophy-PGNN.},
  archive      = {J_TIST},
  author       = {Mohannad Elhamod and Jie Bu and Christopher Singh and Matthew Redell and Abantika Ghosh and Viktor Podolskiy and Wei-Cheng Lee and Anuj Karpatne},
  doi          = {10.1145/3530911},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {92:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {CoPhy -PGNN: Learning physics-guided neural networks with competing loss functions for solving eigenvalue problems},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-level optimization to reduce waiting time at locks in
inland waterway transportation. <em>TIST</em>, <em>13</em>(6), 91:1–30.
(<a href="https://doi.org/10.1145/3527822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inland vessels often have to cross numerous locks before reaching their final destination, which leads to a significant delay and sometimes represents as much as half of the total travel time. The delay affects shipment costs and can affect other parts of the transport chain, adversely impacting this transportation mode’s growth. Therefore, this work presents a two-level solution to ensure a shorter waiting time at locks and improve inland waterway transport. On the one hand, the first level focuses on making infrastructural modifications by proposing an efficient Lock Automation Decision Making (Lock-ADM) method. The problem modeling consists of using a three-stage algorithm. Firstly, we calculate the optimal number of locks while minimizing the investment costs using the exact solver, CPLEX. Secondly, we measure the importance of locks in the network, and finally, we select the best locks to automate using the Genetic Algorithm (GA) metaheuristic. Based on real data, we achieved an average reduction of 33.7\% in overall lock waiting time at a low cost. On the other hand, the second level proposes a Dynamic Lock Scheduling (Lock-DS) to efficiently manage vessels scheduling at locks by minimizing their waiting time and optimizing their speed. We achieve an average reduction of 69.9\% in vessel waiting time and a reduction of 48.03\% in total fuel consumption compared to existing scheduling methods. Automating the most important locks with Lock-ADM and managing their crossing with Lock-DS ensure shorter vessels’ waiting time and represent a significant first step towards the automation of inland navigation.},
  archive      = {J_TIST},
  author       = {Wided Hammedi and Sidi Mohammed Senouci and Philippe Brunet and Metzli Ramirez-Martinez},
  doi          = {10.1145/3527822},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {91:1–30},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Two-level optimization to reduce waiting time at locks in inland waterway transportation},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jointly optimizing expressional and residual models for 3D
facial expression removal. <em>TIST</em>, <em>13</em>(6), 90:1–17. (<a
href="https://doi.org/10.1145/3533312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a facial expression removal method to recover a 3D neutral face from a single 3D expressional or non-neutral face. We treat a 3D non-neutral face as the sum of its neutral one and the residual. This can be satisfied if the correspondence between 3D vertices of expressional faces and those of neutral faces is established. We propose a non-rigid deformation method to establish the correspondence between 3D faces. Then, according to algebra inequality, the minimization of a neutral face model can be replaced by the minimization of its upper bound, i.e., the errors of an expressional face model and a residual model. Thus, we co-optimize the representation errors of the latter two models and build the relationship between the representation coefficients of the two models. Given an expressional face as the input, its corresponding neutral face can be inferred by the associative representation parameters in these two models. In the testing stage, we use an iterative joint fitting scheme to obtain a more accurate recovery. Extensive experiments are conducted to evaluate our method. The results show that our method obtains considerably better performance than existing methods in terms of average root mean square errors and recognition rates, and also better visual effects.},
  archive      = {J_TIST},
  author       = {Qian Zheng and Yueming Wang and Zhenfang Hu and Xiaobo Zhang and Zhaohui Wu and Gang Pan},
  doi          = {10.1145/3533312},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {90:1–17},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Jointly optimizing expressional and residual models for 3D facial expression removal},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepExpress: Heterogeneous and coupled sequence modeling for
express delivery prediction. <em>TIST</em>, <em>13</em>(6), 89:1–22. (<a
href="https://doi.org/10.1145/3526087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of express delivery sequence, i.e., modeling and estimating the volumes of daily incoming and outgoing parcels for delivery, is critical for online business, logistics, and positive customer experience, and specifically for resource allocation optimization and promotional activity arrangement. A precise estimate of consumer delivery requests has to involve sequential factors such as shopping behaviors, weather conditions, events, business campaigns, and their couplings. Despite that various methods have integrated external features to enhance the effects, extant works fail to address complex feature-sequence couplings in the following aspects: weaken the inter-dependencies when processing heterogeneous data and ignore the cumulative and evolving situation of coupling relationships. To address these issues, we propose DeepExpress—a deep-learning-based express delivery sequence prediction model, which extends the classic seq2seq framework to learn feature-sequence couplings. DeepExpress leverages an express delivery seq2seq learning, a carefully designed heterogeneous feature representation, and a novel joint training attention mechanism to adaptively handle heterogeneity issues and capture feature-sequence couplings for accurate prediction. Experimental results on real-world data demonstrate that the proposed method outperforms both shallow and deep baseline models.},
  archive      = {J_TIST},
  author       = {Siyuan Ren and Bin Guo and Longbing Cao and Ke Li and Jiaqi Liu and Zhiwen Yu},
  doi          = {10.1145/3526087},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {89:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {DeepExpress: Heterogeneous and coupled sequence modeling for express delivery prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsic performance influence-based participant
contribution estimation for horizontal federated learning.
<em>TIST</em>, <em>13</em>(6), 88:1–24. (<a
href="https://doi.org/10.1145/3523059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust, and efficient manner. To achieve this goal, we propose a novel contribution estimation method: Intrinsic Performance Influence-based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets, and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data and thus prevent them from participating and deteriorating the learning ecosystem.},
  archive      = {J_TIST},
  author       = {Lin Zhang and Lixin Fan and Yong Luo and Ling-Yu Duan},
  doi          = {10.1145/3523059},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {88:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Intrinsic performance influence-based participant contribution estimation for horizontal federated learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of driving behavior on commuter’s comfort during cab
rides: Towards a new perspective of driver rating. <em>TIST</em>,
<em>13</em>(6), 87:1–25. (<a
href="https://doi.org/10.1145/3523063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commuter comfort in cab rides affects driver rating as well as the reputation of ride-hailing firms like Uber/Lyft. Existing research has revealed that commuter comfort not only varies at a personalized level but also is perceived differently on different trips for the same commuter. Furthermore, there are several factors, including driving behavior and driving environment, affecting the perception of comfort. Automatically extracting the perceived comfort level of a commuter due to the impact of the driving behavior is crucial for a timely feedback to the drivers, which can help them to meet the commuter’s satisfaction. In light of this, we surveyed around 200 commuters who usually take such cab rides and obtained a set of features that impact comfort during cab rides. Following this, we develop a system Ridergo which collects smartphone sensor data from a commuter, extracts the spatial time series feature from the data, and then computes the level of commuter comfort on a five-point scale with respect to the driving. Ridergo uses a Hierarchical Temporal Memory model-based approach to observe anomalies in the feature distribution and then trains a multi-task learning-based neural network model to obtain the comfort level of the commuter at a personalized level. The model also intelligently queries the commuter to add new data points to the available dataset and, in turn, improve itself over periodic training. Evaluation of Ridergo on 30 participants shows that the system could provide efficient comfort score with high accuracy when the driving impacts the perceived comfort.},
  archive      = {J_TIST},
  author       = {Rohit Verma and Sugandh Pargal and Debasree Das and Tanusree Parbat and Sai Shankar Kambalapalli and Bivas Mitra and Sandip Chakraborty},
  doi          = {10.1145/3523063},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {6},
  pages        = {87:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Impact of driving behavior on commuter’s comfort during cab rides: Towards a new perspective of driver rating},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic probabilistic graphical model for progressive fake
news detection on social media platform. <em>TIST</em>, <em>13</em>(5),
86:1–24. (<a href="https://doi.org/10.1145/3523060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fake news has been readily spread by massive amounts of users in social media, and automatic fake news detection has become necessary. The existing works need to prepare the overall data to perform detection, losing important information about the dynamic evolution of crowd opinions, and usually neglect the issue of uneven arrival of data in the real world. To address these issues, in this article, we focus on a kind of approach for fake news detection, namely progressive detection , which can be achieved by the dynamic Probabilistic Graphical Model . Based on the observation on real-world datasets, we adaptively improve the Kalman Filter to the Labeled Variable Dimension Kalman Filter (LVDKF) that learns two universal patterns from true and fake news, respectively, which can capture the temporal information of time-series data that arrive unevenly. It can take sequential data as input, distill the dynamic evolution knowledge regarding a post, and utilize crowd wisdom from users’ responses to achieve progressive detection. Then we derive the formulas using the Forward, Backward, and EM Algorithm, and we design a dynamic detection algorithm using Bayes’ theorem. Finally, we design experimental scenarios simulating progressive detection and evaluate LVDKF on two public datasets. It outperforms the baseline methods in these experimental scenarios, which indicates that it is adequate for progressive detection.},
  archive      = {J_TIST},
  author       = {Ke Li and Bin Guo and Jiaqi Liu and Jiangtao Wang and Haoyang Ren and Fei Yi and Zhiwen Yu},
  doi          = {10.1145/3523060},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {86:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Dynamic probabilistic graphical model for progressive fake news detection on social media platform},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). INN: An interpretable neural network for AI incubation in
manufacturing. <em>TIST</em>, <em>13</em>(5), 85:1–23. (<a
href="https://doi.org/10.1145/3519313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both artificial intelligence (AI) and domain knowledge from human experts play an important role in manufacturing decision making. Smart manufacturing emphasizes a fully automated data-driven decision-making; however, the AI incubation process involves human experts to enhance AI systems by integrating domain knowledge for modeling, data collection and annotation, and feature extraction. Such an AI incubation process not only enhances the domain knowledge discovery but also improves the interpretability and trustworthiness of AI methods. In this article, we focus on the knowledge transfer from human experts to a supervised learning problem by learning domain knowledge as interpretable features and rules, which can be used to construct rule-based systems to support manufacturing decision making, such as process modeling and quality inspection. Although many advanced statistical and machine learning methods have shown promising modeling accuracy and efficiency, rule-based systems are still highly preferred and widely adopted due to their interpretability for human experts to comprehend. However, most of the existing rule-based systems are constructed based on deterministic human-crafted rules, whose parameters, such as thresholds of decision rules, are suboptimal. Yet the machine learning methods, such as tree models or neural networks, can learn a decision rule based structure without much interpretation or agreement with domain knowledge. Therefore, the traditional machine learning models and human experts’ domain knowledge cannot be directly improved by learning from data. In this research, we propose an interpretable neural network (INN) model with a center-adjustable sigmoid activation function to efficiently optimize the rule-based systems. Using the rule-based system from domain knowledge to regulate the INN architecture not only improves the prediction accuracy with optimized parameters but also ensures the interpretability by adopting the interpretable rule-based systems from domain knowledge. The proposed INN will be effective for supervised learning problems when rule-based systems are available. The merits of the INN model are demonstrated via a simulation study and a real case study in the quality modeling of a semiconductor manufacturing process. The source code of this work is hosted here: https://github.com/XiaoyuChenUofL/Interpretable-Neural-Network .},
  archive      = {J_TIST},
  author       = {Xiaoyu Chen and Yingyan Zeng and Sungku Kang and Ran Jin},
  doi          = {10.1145/3519313},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {85:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {INN: An interpretable neural network for AI incubation in manufacturing},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on source code documentation. <em>TIST</em>,
<em>13</em>(5), 84:1–44. (<a
href="https://doi.org/10.1145/3519312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context: Coding is an incremental activity where a developer may need to understand a code before making suitable changes in the code. Code documentation is considered one of the best practices in software development but requires significant efforts from developers. Recent advances in natural language processing and machine learning have provided enough motivation to devise automated approaches for source code documentation at multiple levels. Objective: The review aims to study current code documentation practices and analyze the existing literature to provide a perspective on their preparedness to address the stated problem and the challenges that lie ahead. Methodology: We provide a detailed account of the literature in the area of automated source code documentation at different levels and critically analyze the effectiveness of the proposed approaches. This also allows us to infer gaps and challenges to address the problem at different levels. Findings: (1) The research community focused on method-level summarization. (2) Deep learning has dominated the past five years of this research field. (3) Researchers are regularly proposing bigger corpora for source code documentation. (4) Java and Python are the widely used programming languages as corpus. (5) Bilingual Evaluation Understudy is the most favored evaluation metric for the research persons.},
  archive      = {J_TIST},
  author       = {Sawan Rai and Ramesh Chandra Belwal and Atul Gupta},
  doi          = {10.1145/3519312},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {84:1–44},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A review on source code documentation},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A foraging strategy with risk response for individual robots
in adversarial environments. <em>TIST</em>, <em>13</em>(5), 83:1–29. (<a
href="https://doi.org/10.1145/3514499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential problem in robotics, foraging means that robots collect objects from a given environment and return them to a specified location. On many occasions, robots are required to perform foraging tasks in adversarial environments, such as battlefield rescue, where potential adversaries may damage robots with a certain probability. The longer an individual robot moves through adversarial environments, the higher the probability of being damaged by adversaries. The robot system can gain utility only when the robot brings carried objects back to a predetermined home station. Such a risk of being damaged makes returning home at different locations potentially relevant to the expected utility produced by the robot. Thus, the individual robot faces a dilemma when it responds to the potential risks in adversarial environments: whether to return the carried resources home or continue foraging tasks. In this article, two fundamental environment settings are discussed, homogeneous cases and heterogeneous cases. The former is analyzed as having both the optimal substructure property and the non-aftereffect property. Then, we present a dynamic programming (DP) algorithm that can find an optimal solution with polynomial time complexity. For the latter, it is proven that finding an optimal solution is \( \mathcal {NP} \)-hard. We then propose a heuristic algorithm: A division hierarchical path planning (DHPP) algorithm that is based on the idea of dividing the foraging routes generated initially into a certain number of subroutes to dilute risks. Finally, these algorithms are extensively evaluated in simulations, concluding that in adversarial environments, they can significantly improve the productivity of an individual robot before it is damaged.},
  archive      = {J_TIST},
  author       = {Kai Di and Yifeng Zhou and Fuhan Yan and Jiuchuan Jiang and Shaofu Yang and Yichuan Jiang},
  doi          = {10.1145/3514499},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {83:1–29},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A foraging strategy with risk response for individual robots in adversarial environments},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gray-box shilling attack: An adversarial learning approach.
<em>TIST</em>, <em>13</em>(5), 82:1–21. (<a
href="https://doi.org/10.1145/3512352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are essential components of many information services, which aim to find relevant items that match user preferences. Several studies have shown that shilling attacks can significantly weaken the robustness of recommender systems by injecting fake user profiles. Traditional shilling attacks focus on creating hand-engineered fake user profiles, but these profiles can be detected effortlessly by advanced detection methods. Adversarial learning, which has emerged in recent years, can be leveraged to generate powerful and intelligent attack models. To this end, in this article we explore potential risks of recommender systems and shed light on a gray-box shilling attack model based on generative adversarial networks, named GSA-GANs . Specifically, we aim to generate fake user profiles that can achieve two goals: unnoticeable and offensive. Toward these goals, there are several challenges that we need to address: (1) learning complex user behaviors from user-item rating data, and (2) adversely influencing the recommendation results without knowing the underlying recommendation algorithms. To tackle these challenges, two essential GAN modules are respectively designed to make generated fake profiles more similar to real ones and harmful to recommendation results. Experimental results on three public datasets demonstrate that the proposed GSA-GANs framework outperforms baseline models in attack effectiveness, transferability, and camouflage. In the end, we also provide several possible defensive strategies against GSA-GANs. The exploration and analysis in our work will contribute to the defense research of recommender systems.},
  archive      = {J_TIST},
  author       = {Zongwei Wang and Min Gao and Jundong Li and Junwei Zhang and Jiang Zhong},
  doi          = {10.1145/3512352},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {82:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Gray-box shilling attack: An adversarial learning approach},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crowd flow prediction for irregular regions with semantic
graph attention network. <em>TIST</em>, <em>13</em>(5), 81:1–14. (<a
href="https://doi.org/10.1145/3501805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is essential to predict crowd flow precisely in a city, which is practically partitioned into irregular regions based on road networks and functionality. However, prior works mainly focus on grid-based crowd flow prediction, where a city is divided into many regular grids. Although Convolutional Neural Netwok (CNN) is powerful to capture spatial dependence from grid-based Euclidean data, it fails to tackle non-Euclidean data, which reflect the correlations among irregular regions. Besides, prior works fail to jointly capture the hierarchical spatio-temporal dependence from both regular and irregular regions. Finally, the correlations among regions are time-varying and functionality-related. However, the combination of dynamic and semantic attributes of regions are ignored by related works. To address the above challenges, in this article, we propose a novel model to tackle the flow prediction task for irregular regions. First, we employ CNN and Graph Neural Network (GNN) to capture micro and macro spatial dependence among grid-based regions and irregular regions, respectively. Further, we think highly of the dynamic inter-region correlations and propose a location-aware and time-aware graph attention mechanism named Semantic Graph Attention Network (Semantic-GAT), based on dynamic node attribute embedding and multi-view graph reconstruction. Extensive experimental results based on two real-life datasets demonstrate that our model outperforms 10 baselines by reducing the prediction error around 8\%.},
  archive      = {J_TIST},
  author       = {Fuxian Li and Jie Feng and Huan Yan and Depeng Jin and Yong Li},
  doi          = {10.1145/3501805},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {81:1–14},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Crowd flow prediction for irregular regions with semantic graph attention network},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated multi-task graph learning. <em>TIST</em>,
<em>13</em>(5), 80:1–27. (<a
href="https://doi.org/10.1145/3527622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed processing and analysis of large-scale graph data remain challenging because of the high-level discrepancy among graphs. This study investigates a novel subproblem: the distributed multi-task learning on the graph, which jointly learns multiple analysis tasks from decentralized graphs. We propose a federated multi-task graph learning (FMTGL) framework to solve the problem within a privacy-preserving and scalable scheme. Its core is an innovative data-fusion mechanism and a low-latency distributed optimization method. The former captures multi-source data relatedness and generates universal task representation for local task analysis. The latter enables the quick update of our framework with gradients sparsification and tree-based aggregation. As a theoretical result, the proposed optimization method has a convergence rate interpolates between \( \mathcal {O}(1/T) \) and \( \mathcal {O}(1/\sqrt {T}) \), up to logarithmic terms. Unlike previous studies, our work analyzes the convergence behavior with adaptive stepsize selection and non-convex assumption. Experimental results on three graph datasets verify the effectiveness and scalability of FMTGL.},
  archive      = {J_TIST},
  author       = {Yijing Liu and Dongming Han and Jianwei Zhang and Haiyang Zhu and Mingliang Xu and Wei Chen},
  doi          = {10.1145/3527622},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {80:1–27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated multi-task graph learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLeet: Online federated learning via staleness awareness and
performance prediction. <em>TIST</em>, <em>13</em>(5), 79:1–30. (<a
href="https://doi.org/10.1145/3527621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is very appealing for its privacy benefits: essentially, a global model is trained with updates computed on mobile devices while keeping the data of users local. Standard FL infrastructures are however designed to have no energy or performance impact on mobile devices, and are therefore not suitable for applications that require frequent ( online ) model updates, such as news recommenders. This article presents FLeet , the first Online FL system, acting as a middleware between the Android operating system and the machine learning application. FLeet combines the privacy of Standard FL with the precision of online learning thanks to two core components: (1) I-Prof , a new lightweight profiler that predicts and controls the impact of learning tasks on mobile devices, and (2) AdaSGD , a new adaptive learning algorithm that is resilient to delayed updates. Our extensive evaluation shows that Online FL, as implemented by FLeet , can deliver a 2.3× quality boost compared to Standard FL while only consuming 0.036\% of the battery per day. I-Prof can accurately control the impact of learning tasks by improving the prediction accuracy by up to 3.6× in terms of computation time, and by up to 19× in terms of energy. AdaSGD outperforms alternative FL approaches by 18.4\% in terms of convergence speed on heterogeneous data.},
  archive      = {J_TIST},
  author       = {Georgios Damaskinos and Rachid Guerraoui and Anne-Marie Kermarrec and Vlad Nitu and Rhicheek Patra and Francois Taiani},
  doi          = {10.1145/3527621},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {79:1–30},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FLeet: Online federated learning via staleness awareness and performance prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-synchronous federated learning for energy-efficient
training and accelerated convergence in cross-silo settings.
<em>TIST</em>, <em>13</em>(5), 78:1–29. (<a
href="https://doi.org/10.1145/3524885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are situations where data relevant to machine learning problems are distributed across multiple locations that cannot share the data due to regulatory, competitiveness, or privacy reasons. Machine learning approaches that require data to be copied to a single location are hampered by the challenges of data sharing. Federated Learning (FL) is a promising approach to learn a joint model over all the available data across silos. In many cases, the sites participating in a federation have different data distributions and computational capabilities. In these heterogeneous environments existing approaches exhibit poor performance: synchronous FL protocols are communication efficient, but have slow learning convergence and high energy cost; conversely, asynchronous FL protocols have faster convergence with lower energy cost, but higher communication. In this work, we introduce a novel energy-efficient Semi-Synchronous Federated Learning protocol that mixes local models periodically with minimal idle time and fast convergence. We show through extensive experiments over established benchmark datasets in the computer-vision domain as well as in real-world biomedical settings that our approach significantly outperforms previous work in data and computationally heterogeneous environments .},
  archive      = {J_TIST},
  author       = {Dimitris Stripelis and Paul M. Thompson and José Luis Ambite},
  doi          = {10.1145/3524885},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {78:1–29},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Semi-synchronous federated learning for energy-efficient training and accelerated convergence in cross-silo settings},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient learning framework for federated XGBoost using
secret sharing and distributed optimization. <em>TIST</em>,
<em>13</em>(5), 77:1–28. (<a
href="https://doi.org/10.1145/3523061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {XGBoost is one of the most widely used machine learning models in the industry due to its superior learning accuracy and efficiency. Targeting at data isolation issues in the big data problems, it is crucial to deploy a secure and efficient federated XGBoost (FedXGB) model. Existing FedXGB models either have data leakage issues or are only applicable to the two-party setting with heavy communication and computation overheads. In this article, a lossless multi-party federated XGB learning framework is proposed with a security guarantee, which reshapes the XGBoost’s split criterion calculation process under a secret sharing setting and solves the leaf weight calculation problem by leveraging distributed optimization. Remarkably, a thorough analysis of model security is provided as well, and multiple numerical results showcase the superiority of the proposed FedXGB compared with the state-of-the-art models on benchmark datasets.},
  archive      = {J_TIST},
  author       = {Lunchen Xie and Jiaqi Liu and Songtao Lu and Tsung-Hui Chang and Qingjiang Shi},
  doi          = {10.1145/3523061},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {77:1–28},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {An efficient learning framework for federated XGBoost using secret sharing and distributed optimization},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defending against poisoning backdoor attacks on federated
meta-learning. <em>TIST</em>, <em>13</em>(5), 76:1–25. (<a
href="https://doi.org/10.1145/3523062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks : a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this article, we analyze the effects of backdoor attacks on federated meta-learning , where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principle, make federated learning frameworks more robust to backdoor attacks (when new training examples are benign), we find that even one-shot attacks can be very successful and persist after additional training. To address these vulnerabilities, we propose a defense mechanism inspired by matching networks , where the class of an input is predicted from the similarity of its features with a support set of labeled examples. By removing the decision logic from the model shared with the federation, the success and persistence of backdoor attacks are greatly reduced.},
  archive      = {J_TIST},
  author       = {Chien-Lun Chen and Sara Babakniya and Marco Paolieri and Leana Golubchik},
  doi          = {10.1145/3523062},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {76:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Defending against poisoning backdoor attacks on federated meta-learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CLC: A consensus-based label correction approach in
federated learning. <em>TIST</em>, <em>13</em>(5), 75:1–23. (<a
href="https://doi.org/10.1145/3519311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a novel distributed learning framework where multiple participants collaboratively train a global model without sharing any raw data to preserve privacy. However, data quality may vary among the participants, the most typical of which is label noise. The incorrect label would significantly damage the performance of the global model. In FL, the inaccessibility of raw data makes this issue more challenging. Previously published studies are limited to using a task-specific benchmark-trained model to evaluate the relevance between the benchmark dataset in the server and the local one on the participants’ side. However, such approaches have failed to exploit the cooperative nature of FL itself and are not practical. This paper proposes a Consensus-based Label Correction approach (CLC) in FL, which tries to correct the noisy labels using the developed consensus method among the FL participants. The consensus-defined class-wise information is used to identify the noisy labels and correct them with pseudo-labels. Extensive experiments are conducted on several public datasets in various settings. The experimental results prove the advantage over the state-of-art methods. The link to the source code is https://github.com/bixiao-zeng/CLC.git .},
  archive      = {J_TIST},
  author       = {Bixiao Zeng and Xiaodong Yang and Yiqiang Chen and Hanchao Yu and Yingwei Zhang},
  doi          = {10.1145/3519311},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {75:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {CLC: A consensus-based label correction approach in federated learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SignDS-FL: Local differentially private federated learning
with sign-based dimension selection. <em>TIST</em>, <em>13</em>(5),
74:1–22. (<a href="https://doi.org/10.1145/3517820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) [ 31 ] is a decentralized learning mechanism that has attracted increasing attention due to its achievements in computational efficiency and privacy preservation. However, recent research highlights that the original FL framework may still reveal sensitive information of clients’ local data from the exchanged local updates and the global model parameters. Local Differential Privacy (LDP), as a rigorous definition of privacy, has been applied to Federated Learning to provide formal privacy guarantees and prevent potential privacy leakage. However, previous LDP-FL solutions suffer from considerable utility loss with an increase of model dimensionality. Recent work [ 29 ] proposed a two-stage framework that mitigates the dimension-dependency problem by first selecting one “important” dimension for each local update and then perturbing the dimension value to construct the sparse privatized update. However, the framework may still suffer from utility loss because of the insufficient per-stage privacy budget and slow model convergence. In this article, we propose an improved framework, SignDS-FL , which shares the concept of dimension selection with Reference [ 29 ], but saves the privacy cost for the value perturbation stage by assigning random sign values to the selected dimensions. Besides using the single-dimension selection algorithms in Reference [ 29 ], we propose an Exponential Mechanism-based Multi-Dimension Selection algorithm that further improves model convergence and accuracy. We evaluate the framework on a number of real-world datasets with both simple logistic regression models and deep neural networks. For training logistic regression models on structured datasets, our framework yields only a \( \sim \)1\%–2\% accuracy loss in comparison to a \( \sim \)5\%–15\% decrease of accuracy for the baseline methods. For training deep neural networks on image datasets, the accuracy loss of our framework is less than \( 8\% \) and at best only \( 2\% \). Extensive experimental results show that our framework significantly outperforms the previous LDP-FL solutions and enjoys an advanced utility-privacy balance.},
  archive      = {J_TIST},
  author       = {Xue Jiang and Xuebing Zhou and Jens Grossklags},
  doi          = {10.1145/3517820},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {74:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {SignDS-FL: Local differentially private federated learning with sign-based dimension selection},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-weighted robust federated learning with corrupted data
sources. <em>TIST</em>, <em>13</em>(5), 73:1–20. (<a
href="https://doi.org/10.1145/3517821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning provides a communication-efficient and privacy-preserving training process by enabling learning statistical models with massive participants without accessing their local data. Standard federated learning techniques that naively minimize an average loss function are vulnerable to data corruptions from outliers, systematic mislabeling, or even adversaries. In this article, we address this challenge by proposing Auto-weighted Robust Federated Learning ( ARFL ), a novel approach that jointly learns the global model and the weights of local updates to provide robustness against corrupted data sources. We prove a learning bound on the expected loss with respect to the predictor and the weights of clients, which guides the definition of the objective for robust federated learning. We present an objective that minimizes the weighted sum of empirical risk of clients with a regularization term, where the weights can be allocated by comparing the empirical risk of each client with the average empirical risk of the best \( p \) clients. This method can downweight the clients with significantly higher losses, thereby lowering their contributions to the global model. We show that this approach achieves robustness when the data of corrupted clients is distributed differently from the benign ones. To optimize the objective function, we propose a communication-efficient algorithm based on the blockwise minimization paradigm. We conduct extensive experiments on multiple benchmark datasets, including CIFAR-10, FEMNIST, and Shakespeare, considering different neural network models. The results show that our solution is robust against different scenarios, including label shuffling, label flipping, and noisy features, and outperforms the state-of-the-art methods in most scenarios.},
  archive      = {J_TIST},
  author       = {Shenghui Li and Edith Ngai and Fanghua Ye and Thiemo Voigt},
  doi          = {10.1145/3517821},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {73:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Auto-weighted robust federated learning with corrupted data sources},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning for electronic health records.
<em>TIST</em>, <em>13</em>(5), 72:1–17. (<a
href="https://doi.org/10.1145/3514500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data-driven medical research, multi-center studies have long been preferred over single-center ones due to a single institute sometimes not having enough data to obtain sufficient statistical power for certain hypothesis testings as well as predictive and subgroup studies. The wide adoption of electronic health records (EHRs) has made multi-institutional collaboration much more feasible. However, concerns over infrastructures, regulations, privacy, and data standardization present a challenge to data sharing across healthcare institutions. Federated Learning (FL), which allows multiple sites to collaboratively train a global model without directly sharing data, has become a promising paradigm to break the data isolation. In this study, we surveyed existing works on FL applications in EHRs and evaluated the performance of current state-of-the-art FL algorithms on two EHR machine learning tasks of significant clinical importance on a real world multi-center EHR dataset.},
  archive      = {J_TIST},
  author       = {Trung Kien Dang and Xiang Lan and Jianshu Weng and Mengling Feng},
  doi          = {10.1145/3514500},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {72:1–17},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated learning for electronic health records},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FLEE: A hierarchical federated learning framework for
distributed deep neural network over cloud, edge, and end device.
<em>TIST</em>, <em>13</em>(5), 71:1–24. (<a
href="https://doi.org/10.1145/3514501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of smart devices, the computing capabilities of portable end devices such as mobile phones have been greatly enhanced. Meanwhile, traditional cloud computing faces great challenges caused by privacy-leakage and time-delay problems, there is a trend to push models down to edges and end devices. However, due to the limitation of computing resource, it is difficult for end devices to complete complex computing tasks alone. Therefore, this article divides the model into two parts and deploys them on multiple end devices and edges, respectively. Meanwhile, an early exit is set to reduce computing resource overhead, forming a hierarchical distributed architecture. In order to enable the distributed model to continuously evolve by using new data generated by end devices, we comprehensively consider various data distributions on end devices and edges, proposing a hierarchical federated learning framework FLEE , which can realize dynamical updates of models without redeploying them. Through image and sentence classification experiments, we verify that it can improve model performances under all kinds of data distributions, and prove that compared with other frameworks, the models trained by FLEE consume less global computing resource in the inference stage.},
  archive      = {J_TIST},
  author       = {Zhengyi Zhong and Weidong Bao and Ji Wang and Xiaomin Zhu and Xiongtao Zhang},
  doi          = {10.1145/3514501},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {71:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FLEE: A hierarchical federated learning framework for distributed deep neural network over cloud, edge, and end device},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PSDF: Privacy-aware IoV service deployment with federated
learning in cloud-edge computing. <em>TIST</em>, <em>13</em>(5),
70:1–22. (<a href="https://doi.org/10.1145/3501810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through the collaboration of cloud and edge, cloud-edge computing allows the edge that approximates end-users undertakes those non-computationally intensive service processing of the cloud, reducing the communication overhead and satisfying the low latency requirement of Internet of Vehicle (IoV). With cloud-edge computing, the computing tasks in IoV is able to be delivered to the edge servers (ESs) instead of the cloud and rely on the deployed services of ESs for a series of processing. Due to the storage and computing resource limits of ESs, how to dynamically deploy partial services to the edge is still a puzzle. Moreover, the decision of service deployment often requires the transmission of local service requests from ESs to the cloud, which increases the risk of privacy leakage. In this article, a method for privacy-aware IoV service deployment with federated learning in cloud-edge computing, named PSDF, is proposed. Technically, federated learning secures the distributed training of deployment decision network on each ES by the exchange and aggregation of model weights, avoiding the original data transmission. Meanwhile, homomorphic encryption is adopted for the uploaded weights before the model aggregation on the cloud. Besides, a service deployment scheme based on deep deterministic policy gradient is proposed. Eventually, the performance of PSDF is evaluated by massive experiments.},
  archive      = {J_TIST},
  author       = {Xiaolong Xu and Wentao Liu and Yulan Zhang and Xuyun Zhang and Wanchun Dou and Lianyong Qi and Md Zakirul Alam Bhuiyan},
  doi          = {10.1145/3501810},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {70:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {PSDF: Privacy-aware IoV service deployment with federated learning in cloud-edge computing},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Preface to federated learning: Algorithms, systems, and
applications: Part 2. <em>TIST</em>, <em>13</em>(5), 69:1–2. (<a
href="https://doi.org/10.1145/3536420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIST},
  author       = {Qiang Yang and Yongxin Tong and Yang Liu and Yangqiu Song and Hao Peng and Boi Faltings},
  doi          = {10.1145/3536420},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {5},
  pages        = {69:1–2},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Preface to federated learning: algorithms, systems, and applications: part 2},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning for personalized humor recognition.
<em>TIST</em>, <em>13</em>(4), 68:1–18. (<a
href="https://doi.org/10.1145/3511710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational understanding of humor is an important topic under creative language understanding and modeling. It can play a key role in complex human-AI interactions. The challenge here is that human perception of humorous content is highly subjective. The same joke may receive different funniness ratings from different readers. This makes it highly challenging for humor recognition models to achieve personalization in practical scenarios. Existing approaches are generally designed based on the assumption that users have a consensus on whether a given text is humorous or not. Thus, they cannot handle diverse humor preferences well. In this article, we propose the FedHumor approach for the recognition of humorous content in a personalized manner through Federated Learning (FL). Extending a pre-trained language model, FedHumor guides the fine-tuning process by considering diverse distributions of humor preferences from individuals. It incorporates a diversity adaptation strategy into the FL paradigm to train a personalized humor recognition model. To the best of our knowledge, FedHumor is the first text-based personalized humor recognition model through federated learning. Extensive experiments demonstrate the advantage of FedHumor in recognizing humorous texts compared to nine state-of-the-art humor recognition approaches with superior capability for handling the diversity in humor labels produced by users with diverse preferences.},
  archive      = {J_TIST},
  author       = {Xu Guo and Han Yu and Boyang Li and Hao Wang and Pengwei Xing and Siwei Feng and Zaiqing Nie and Chunyan Miao},
  doi          = {10.1145/3511710},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {68:1–18},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated learning for personalized humor recognition},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-efficient federated learning with adaptive
quantization. <em>TIST</em>, <em>13</em>(4), 67:1–26. (<a
href="https://doi.org/10.1145/3510587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted tremendous attentions in recent years due to its privacy-preserving measures and great potential in some distributed but privacy-sensitive applications, such as finance and health. However, high communication overloads for transmitting high-dimensional networks and extra security masks remain a bottleneck of FL. This article proposes a communication-efficient FL framework with an Adaptive Quantized Gradient (AQG), which adaptively adjusts the quantization level based on a local gradient’s update to fully utilize the heterogeneity of local data distribution for reducing unnecessary transmissions. In addition, client dropout issues are taken into account and an Augmented AQG is developed, which could limit the dropout noise with an appropriate amplification mechanism for transmitted gradients. Theoretical analysis and experiment results show that the proposed AQG leads to 18\% to 50\% of additional transmission reduction as compared with existing popular methods, including Quantized Gradient Descent (QGD) and Lazily Aggregated Quantized (LAQ) gradient-based methods without deteriorating convergence properties. Experiments with heterogenous data distributions corroborate a more significant transmission reduction compared with independent identical data distributions. The proposed AQG is robust to a client dropping rate up to 90\% empirically, and the Augmented AQG manages to further improve the FL system’s communication efficiency with the presence of moderate-scale client dropouts commonly seen in practical FL scenarios.},
  archive      = {J_TIST},
  author       = {Yuzhu Mao and Zihao Zhao and Guangfeng Yan and Yang Liu and Tian Lan and Linqi Song and Wenbo Ding},
  doi          = {10.1145/3510587},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {67:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Communication-efficient federated learning with adaptive quantization},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FedBERT: When federated learning meets pre-training.
<em>TIST</em>, <em>13</em>(4), 66:1–26. (<a
href="https://doi.org/10.1145/3510033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast growth of pre-trained models (PTMs) has brought natural language processing to a new era, which has become a dominant technique for various natural language processing (NLP) applications. Every user can download the weights of PTMs, then fine-tune the weights for a task on the local side. However, the pre-training of a model relies heavily on accessing a large-scale of training data and requires a vast amount of computing resources. These strict requirements make it impossible for any single client to pre-train such a model. To grant clients with limited computing capability to participate in pre-training a large model, we propose a new learning approach, FedBERT , that takes advantage of the federated learning and split learning approaches, resorting to pre-training BERT in a federated way. FedBERT can prevent sharing the raw data information and obtain excellent performance. Extensive experiments on seven GLUE tasks demonstrate that FedBERT can maintain its effectiveness without communicating to the sensitive local data of clients.},
  archive      = {J_TIST},
  author       = {Yuanyishu Tian and Yao Wan and Lingjuan Lyu and Dezhong Yao and Hai Jin and Lichao Sun},
  doi          = {10.1145/3510033},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {66:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FedBERT: When federated learning meets pre-training},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GRNN: Generative regression neural network—a data leakage
attack for federated learning. <em>TIST</em>, <em>13</em>(4), 65:1–24.
(<a href="https://doi.org/10.1145/3510032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data privacy has become an increasingly important issue in Machine Learning (ML) , where many approaches have been developed to tackle this challenge, e.g., cryptography ( Homomorphic Encryption (HE) , Differential Privacy (DP) ) and collaborative training (Secure Multi-Party Computation (MPC) , Distributed Learning, and Federated Learning (FL) ). These techniques have a particular focus on data encryption or secure local computation. They transfer the intermediate information to the third party to compute the final result. Gradient exchanging is commonly considered to be a secure way of training a robust model collaboratively in Deep Learning (DL) . However, recent researches have demonstrated that sensitive information can be recovered from the shared gradient. Generative Adversarial Network (GAN) , in particular, has shown to be effective in recovering such information. However, GAN based techniques require additional information, such as class labels that are generally unavailable for privacy-preserved learning. In this article, we show that, in the FL system, image-based privacy data can be easily recovered in full from the shared gradient only via our proposed Generative Regression Neural Network (GRNN) . We formulate the attack to be a regression problem and optimize two branches of the generative model by minimizing the distance between gradients. We evaluate our method on several image classification tasks. The results illustrate that our proposed GRNN outperforms state-of-the-art methods with better stability, stronger robustness, and higher accuracy. It also has no convergence requirement to the global FL model. Moreover, we demonstrate information leakage using face re-identification. Some defense strategies are also discussed in this work.},
  archive      = {J_TIST},
  author       = {Hanchi Ren and Jingjing Deng and Xianghua Xie},
  doi          = {10.1145/3510032},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {65:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {GRNN: Generative regression neural Network—A data leakage attack for federated learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FedCVT: Semi-supervised vertical federated learning with
cross-view training. <em>TIST</em>, <em>13</em>(4), 64:1–16. (<a
href="https://doi.org/10.1145/3510031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows multiple parties to build machine learning models collaboratively without exposing data. In particular, vertical federated learning (VFL) enables participating parties to build a joint machine learning model based upon distributed features of aligned samples. However, VFL requires all parties to share a sufficient amount of aligned samples. In reality, the set of aligned samples may be small, leaving the majority of the non-aligned data unused. In this article, we propose Federated Cross-view Training (FedCVT), a semi-supervised learning approach that improves the performance of the VFL model with limited aligned samples. More specifically, FedCVT estimates representations for missing features, predicts pseudo-labels for unlabeled samples to expand the training set, and trains three classifiers jointly based upon different views of the expanded training set to improve the VFL model’s performance. FedCVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on NUS-WIDE, Vehicle, and CIFAR10 datasets. The experimental results demonstrate that FedCVT significantly outperforms vanilla VFL that only utilizes aligned samples. Finally, we perform ablation studies to investigate the contribution of each component of FedCVT to the performance of FedCVT.},
  archive      = {J_TIST},
  author       = {Yan Kang and Yang Liu and Xinle Liang},
  doi          = {10.1145/3510031},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {64:1–16},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FedCVT: Semi-supervised vertical federated learning with cross-view training},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The OARF benchmark suite: Characterization and implications
for federated learning systems. <em>TIST</em>, <em>13</em>(4), 63:1–32.
(<a href="https://doi.org/10.1145/3510540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning (FL) have focused mainly on synthetic datasets and use a limited number of applications. OARF mimics more realistic application scenarios with publicly available datasets as different data silos in image, text, and structured data. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution, and learning task complexity. The extensive evaluations with reference implementations show the future research opportunities for important aspects of FL systems. We have developed reference implementations, and evaluated the important aspects of FL, including model accuracy, communication cost, throughput, and convergence time. Through these evaluations, we discovered some interesting findings such as FL can effectively increase end-to-end throughput. The code of OARF is publicly available on GitHub. 1},
  archive      = {J_TIST},
  author       = {Sixu Hu and Yuan Li and Xu Liu and Qinbin Li and Zhaomin Wu and Bingsheng He},
  doi          = {10.1145/3510540},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {63:1–32},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {The OARF benchmark suite: Characterization and implications for federated learning systems},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FedCTR: Federated native ad CTR prediction with
cross-platform user behavior data. <em>TIST</em>, <em>13</em>(4),
62:1–19. (<a href="https://doi.org/10.1145/3506715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Native ad is a popular type of online advertisement that has similar forms with the native content displayed on websites. Native ad click-through rate (CTR) prediction is useful for improving user experience and platform revenue. However, it is challenging due to the lack of explicit user intent, and user behaviors on the platform with native ads may be insufficient to infer users’ interest in ads. Fortunately, user behaviors exist on many online platforms that can provide complementary information for user-interest mining. Thus, leveraging multi-platform user behaviors is useful for native ad CTR prediction. However, user behaviors are highly privacy-sensitive, and the behavior data on different platforms cannot be directly aggregated due to user privacy concerns and data protection regulations. Existing CTR prediction methods usually require centralized storage of user behavior data for user modeling, which cannot be directly applied to the CTR prediction task with multi-platform user behaviors. In this article, we propose a federated native ad CTR prediction method named FedCTR, which can learn user-interest representations from cross-platform user behaviors in a privacy-preserving way. On each platform a local user model learns user embeddings from the local user behaviors on that platform. The local user embeddings from different platforms are uploaded to a server for aggregation, and the aggregated ones are sent to the ad platform for CTR prediction. Besides, we apply local differential privacy and differential privacy to the local and aggregated user embeddings, respectively, for better privacy protection. Moreover, we propose a federated framework for collaborative model training with distributed models and user behaviors. Extensive experiments on real-world dataset show that FedCTR can effectively leverage multi-platform user behaviors for native ad CTR prediction in a privacy-preserving manner.},
  archive      = {J_TIST},
  author       = {Chuhan Wu and Fangzhao Wu and Lingjuan Lyu and Yongfeng Huang and Xing Xie},
  doi          = {10.1145/3506715},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {62:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FedCTR: Federated native ad CTR prediction with cross-platform user behavior data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated multi-view learning for private medical data
integration and analysis. <em>TIST</em>, <em>13</em>(4), 61:1–23. (<a
href="https://doi.org/10.1145/3501816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the rapid expansion of information technology and digitalization of health data, there is an increasing concern on maintaining data privacy while garnering the benefits in the medical field. Two critical challenges are identified: First, medical data is naturally distributed across multiple local sites, making it difficult to collectively train machine learning models without data leakage. Second, in medical applications, data are often collected from different sources and views, resulting in heterogeneity and complexity that requires reconciliation. In this article, we present a generic Federated Multi-view Learning (FedMV) framework for multi-view data leakage prevention. Specifically, we apply this framework to two types of problems based on local data availability: Vertical Federated Multi-view Learning (V-FedMV) and Horizontal Federated Multi-view Learning (H-FedMV). We experimented with real-world keyboard data collected from BiAffect study. Our results demonstrated that the proposed approach can make full use of multi-view data in a privacy-preserving way, and both V-FedMV and H-FedMV perform better than their single-view and pairwise counterparts. Besides, the framework can be easily adapted to deal with multi-view sequential data. We have developed a sequential model (S-FedMV) that takes sequence of multi-view data as input and demonstrated it experimentally. To the best of our knowledge, this framework is the first to consider both vertical and horizontal diversification in the multi-view setting, as well as their sequential federated learning.},
  archive      = {J_TIST},
  author       = {Sicong Che and Zhaoming Kong and Hao Peng and Lichao Sun and Alex Leow and Yong Chen and Lifang He},
  doi          = {10.1145/3501816},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {61:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated multi-view learning for private medical data integration and analysis},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GTG-shapley: Efficient and accurate participant contribution
evaluation in federated learning. <em>TIST</em>, <em>13</em>(4),
60:1–21. (<a href="https://doi.org/10.1145/3501811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) bridges the gap between collaborative machine learning and preserving data privacy. To sustain the long-term operation of an FL ecosystem, it is important to attract high-quality data owners with appropriate incentive schemes. As an important building block of such incentive schemes, it is essential to fairly evaluate participants’ contribution to the performance of the final FL model without exposing their private data. Shapley Value (SV)–based techniques have been widely adopted to provide a fair evaluation of FL participant contributions. However, existing approaches incur significant computation costs, making them difficult to apply in practice. In this article, we propose the Guided Truncation Gradient Shapley (GTG-Shapley) approach to address this challenge. It reconstructs FL models from gradient updates for SV calculation instead of repeatedly training with different combinations of FL participants. In addition, we design a guided Monte Carlo sampling approach combined with within-round and between-round truncation to further reduce the number of model reconstructions and evaluations required. This is accomplished through extensive experiments under diverse realistic data distribution settings. The results demonstrate that GTG-Shapley can closely approximate actual Shapley values while significantly increasing computational efficiency compared with the state-of-the-art, especially under non-i.i.d. settings.},
  archive      = {J_TIST},
  author       = {Zelei Liu and Yuanyuan Chen and Han Yu and Yang Liu and Lizhen Cui},
  doi          = {10.1145/3501811},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {60:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {GTG-shapley: Efficient and accurate participant contribution evaluation in federated learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient federated matrix factorization against inference
attacks. <em>TIST</em>, <em>13</em>(4), 59:1–20. (<a
href="https://doi.org/10.1145/3501812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems typically require the revelation of users’ ratings to the recommender server, which will subsequently use these ratings to provide personalized services. However, such revelations make users vulnerable to a broader set of inference attacks, allowing the recommender server to learn users’ private attributes, e.g., age and gender. Therefore, in this paper, we propose an efficient federated matrix factorization method that protects users against inference attacks. The key idea is that we obfuscate one user’s rating to another such that the private attribute leakage is minimized under the given distortion budget, which bounds the recommending loss and overhead of system efficiency. During the obfuscation, we apply differential privacy to control the information leakage between the users. We also adopt homomorphic encryption to protect the intermediate results during training. Our framework is implemented and tested on real-world datasets. The result shows that our method can reduce up to 16.7\% of inference attack accuracy compared to using no privacy protections.},
  archive      = {J_TIST},
  author       = {Di Chai and Leye Wang and Kai Chen and Qiang Yang},
  doi          = {10.1145/3501812},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {59:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Efficient federated matrix factorization against inference attacks},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving availability of vertical federated learning:
Relaxing inference on non-overlapping data. <em>TIST</em>,
<em>13</em>(4), 58:1–20. (<a
href="https://doi.org/10.1145/3501817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) enables multiple parties to collaboratively train a machine learning model over vertically distributed datasets without data privacy leakage. However, there is a limitation of the current VFL solutions: current VFL models fail to conduct inference on non-overlapping samples during inference. This limitation seriously damages the VFL model’s availability because, in practice, overlapping samples may only take up a small portion of the whole data at each party which means a large part of inference tasks will fail. In this article, we propose a novel VFL framework which enables federated inference on non-overlapping data. Our framework regards the distributed features as privileged information which is available in the training period but disappears during inference. We distill the knowledge of such privileged features and transfer them to the parties’ local model which only processes local features. Furthermore, we adopt Oblivious Transfer (OT) to preserve data ID privacy during training and inference. Empirically, we evaluate the model on the real-world dataset collected from Criteo and Taobao. Besides, we also provide a security analysis of the proposed framework.},
  archive      = {J_TIST},
  author       = {Zhenghang Ren and Liu Yang and Kai Chen},
  doi          = {10.1145/3501817},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {58:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Improving availability of vertical federated learning: Relaxing inference on non-overlapping data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic-aware federated learning for face forgery video
detection. <em>TIST</em>, <em>13</em>(4), 57:1–25. (<a
href="https://doi.org/10.1145/3501814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of face forgery videos is a serious threat to information credibility, calling for effective detection algorithms to identify them. Most existing methods have assumed a shared or centralized training set. However, in practice, data may be distributed on devices of different enterprises that cannot be centralized to share due to security and privacy restrictions. In this article, we propose a Federated Learning face forgery detection framework to train a global model collaboratively while keeping data on local devices. In order to make the detection model more robust, we propose a novel Inconsistency-Capture module (ICM) to capture the dynamic inconsistencies between adjacent frames of face forgery videos. The ICM contains two parallel branches. The first branch takes the whole face of adjacent frames as input to calculate a global inconsistency representation. The second branch focuses only on the inter-frame variation of critical regions to capture the local inconsistency. To the best of our knowledge, this is the first work to apply federated learning to face forgery video detection, which is trained with decentralized data. Extensive experiments show that the proposed framework achieves competitive performance compared with existing methods that are trained with centralized data, with higher-level security and privacy guarantee.},
  archive      = {J_TIST},
  author       = {Ziheng Hu and Hongtao Xie and Lingyun Yu and Xingyu Gao and Zhihua Shang and Yongdong Zhang},
  doi          = {10.1145/3501814},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {57:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Dynamic-aware federated learning for face forgery video detection},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated dynamic graph neural networks with secure
aggregation for video-based distributed surveillance. <em>TIST</em>,
<em>13</em>(4), 56:1–23. (<a
href="https://doi.org/10.1145/3501808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed surveillance systems have the ability to detect, track, and snapshot objects moving around in a certain space. The systems generate video data from multiple personal devices or street cameras. Intelligent video-analysis models are needed to learn dynamic representation of the objects for detection and tracking. Can we exploit the structural and dynamic information without storing the spatiotemporal video data at a central server that leads to a violation of user privacy? In this work, we introduce Federated Dynamic Graph Neural Network (Feddy), a distributed and secured framework to learn the object representations from graph sequences: (1) It aggregates structural information from nearby objects in the current graph as well as dynamic information from those in the previous graph. It uses a self-supervised loss of predicting the trajectories of objects. (2) It is trained in a federated learning manner. The centrally located server sends the model to user devices. Local models on the respective user devices learn and periodically send their learning to the central server without ever exposing the user’s data to server. (3) Studies showed that the aggregated parameters could be inspected though decrypted when broadcast to clients for model synchronizing, after the server performed a weighted average. We design an appropriate aggregation mechanism of secure aggregation primitives that can protect the security and privacy in federated learning with scalability. Experiments on four video camera datasets as well as simulation demonstrate that Feddy achieves great effectiveness and security.},
  archive      = {J_TIST},
  author       = {Meng Jiang and Taeho Jung and Ryan Karl and Tong Zhao},
  doi          = {10.1145/3501808},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {56:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated dynamic graph neural networks with secure aggregation for video-based distributed surveillance},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated social recommendation with graph neural network.
<em>TIST</em>, <em>13</em>(4), 55:1–24. (<a
href="https://doi.org/10.1145/3501815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have become prosperous nowadays, designed to predict users’ potential interests in items by learning embeddings. Recent developments of the Graph Neural Networks (GNNs) also provide recommender systems (RSs) with powerful backbones to learn embeddings from a user-item graph. However, only leveraging the user-item interactions suffers from the cold-start issue due to the difficulty in data collection. Hence, current endeavors propose fusing social information with user-item interactions to alleviate it, which is the social recommendation problem. Existing work employs GNNs to aggregate both social links and user-item interactions simultaneously. However, they all require centralized storage of the social links and item interactions of users, which leads to privacy concerns. Additionally, according to strict privacy protection under General Data Protection Regulation, centralized data storage may not be feasible in the future, urging a decentralized framework of social recommendation. As a result, we design a federated learning recommender system for the social recommendation task, which is rather challenging because of its heterogeneity, personalization, and privacy protection requirements. To this end, we devise a novel framework Fe drated So cial recommendation with G raph neural network ( FeSoG ). Firstly, FeSoG adopts relational attention and aggregation to handle heterogeneity. Secondly, FeSoG infers user embeddings using local data to retain personalization. Last but not least, the proposed model employs pseudo-labeling techniques with item sampling to protect the privacy and enhance training. Extensive experiments on three real-world datasets justify the effectiveness of FeSoG in completing social recommendation and privacy protection. We are the first work proposing a federated learning framework for social recommendation to the best of our knowledge.},
  archive      = {J_TIST},
  author       = {Zhiwei Liu and Liangwei Yang and Ziwei Fan and Hao Peng and Philip S. Yu},
  doi          = {10.1145/3501815},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {55:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated social recommendation with graph neural network},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning for healthcare: Systematic review and
architecture proposal. <em>TIST</em>, <em>13</em>(4), 54:1–23. (<a
href="https://doi.org/10.1145/3501813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of machine learning (ML) with electronic health records (EHR) is growing in popularity as a means to extract knowledge that can improve the decision-making process in healthcare. Such methods require training of high-quality learning models based on diverse and comprehensive datasets, which are hard to obtain due to the sensitive nature of medical data from patients. In this context, federated learning (FL) is a methodology that enables the distributed training of machine learning models with remotely hosted datasets without the need to accumulate data and, therefore, compromise it. FL is a promising solution to improve ML-based systems, better aligning them to regulatory requirements, improving trustworthiness and data sovereignty. However, many open questions must be addressed before the use of FL becomes widespread. This article aims at presenting a systematic literature review on current research about FL in the context of EHR data for healthcare applications. Our analysis highlights the main research topics, proposed solutions, case studies, and respective ML methods. Furthermore, the article discusses a general architecture for FL applied to healthcare data based on the main insights obtained from the literature review. The collected literature corpus indicates that there is extensive research on the privacy and confidentiality aspects of training data and model sharing, which is expected given the sensitive nature of medical data. Studies also explore improvements to the aggregation mechanisms required to generate the learning model from distributed contributions and case studies with different types of medical data.},
  archive      = {J_TIST},
  author       = {Rodolfo Stoffel Antunes and Cristiano André da Costa and Arne Küderle and Imrana Abdullahi Yari and Björn Eskofier},
  doi          = {10.1145/3501813},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {54:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Federated learning for healthcare: Systematic review and architecture proposal},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward scalable and privacy-preserving deep neural network
via algorithmic-cryptographic co-design. <em>TIST</em>, <em>13</em>(4),
53:1–21. (<a href="https://doi.org/10.1145/3501809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have achieved remarkable progress in various real-world applications, especially when abundant training data are provided. However, data isolation has become a serious problem currently. Existing works build privacy-preserving DNN models from either algorithmic perspective or cryptographic perspective. The former mainly splits the DNN computation graph between data holders or between data holders and server, which demonstrates good scalability but suffers from accuracy loss and potential privacy risks. In contrast, the latter leverages time-consuming cryptographic techniques, which has strong privacy guarantee but poor scalability. In this article, we propose SPNN—a Scalable and Privacy-preserving deep Neural Network learning framework, from an algorithmic-cryptographic co-perspective. From algorithmic perspective, we split the computation graph of DNN models into two parts, i.e., the private-data-related computations that are performed by data holders and the rest heavy computations that are delegated to a semi-honest server with high computation ability. From cryptographic perspective, we propose using two types of cryptographic techniques, i.e., secret sharing and homomorphic encryption, for the isolated data holders to conduct private-data-related computations privately and cooperatively. Furthermore, we implement SPNN in a decentralized setting and introduce user-friendly APIs. Experimental results conducted on real-world datasets demonstrate the superiority of our proposed SPNN.},
  archive      = {J_TIST},
  author       = {Jun Zhou and Longfei Zheng and Chaochao Chen and Yan Wang and Xiaolin Zheng and Bingzhe Wu and Cen Chen and Li Wang and Jianwei Yin},
  doi          = {10.1145/3501809},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {53:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Toward scalable and privacy-preserving deep neural network via algorithmic-cryptographic co-design},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Introduction to the special issue on the federated
learning: Algorithms, systems, and applications: Part 1. <em>TIST</em>,
<em>13</em>(4), 52:1–3. (<a
href="https://doi.org/10.1145/3514223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIST},
  author       = {Qiang Yang and Yongxin Tong and Yang Liu and Yangqiu Song and Hao Peng and Boi Faltings},
  doi          = {10.1145/3514223},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {4},
  pages        = {52:1–3},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Introduction to the special issue on the federated learning: algorithms, systems, and applications: part 1},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised short-text modeling through auxiliary
context generation. <em>TIST</em>, <em>13</em>(3), 51:1–21. (<a
href="https://doi.org/10.1145/3511712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text is ambiguous and often relies predominantly on the domain and context at hand in order to attain semantic relevance. Existing classification models perform poorly on short text due to data sparsity and inadequate context. Auxiliary context, which can often provide sufficient background regarding the domain, is typically available in several application scenarios. While some of the existing works aim to leverage real-world knowledge to enhance short-text representations, they fail to place appropriate emphasis on the auxiliary context. Such models do not harness the full potential of the available context in auxiliary sources. To address this challenge, we reformulate short-text classification as a dual channel self-supervised learning problem (that leverages auxiliary context) with a generation network and a corresponding prediction model. We propose a self-supervised framework, Pseudo-Auxiliary Context generation network for Short-text Modeling (PACS) , to comprehensively leverage auxiliary context and it is jointly learned with a prediction network in an end-to-end manner. Our PACS model consists of two sub-networks: a Context Generation Network (CGN) that models the auxiliary context’s distribution and a Prediction Network (PN) to map the short-text features and auxiliary context distribution to the final class label. Our experimental results on diverse datasets demonstrate that PACS outperforms formidable state-of-the-art baselines. We also demonstrate the performance of our model on cold-start scenarios (where contextual information is non-existent) during prediction. Furthermore, we perform interpretability and ablation studies to analyze various representational features captured by our model and the individual contribution of its modules to the overall performance of PACS, respectively.},
  archive      = {J_TIST},
  author       = {Nurendra Choudhary and Charu C. Aggarwal and Karthik Subbian and Chandan K. Reddy},
  doi          = {10.1145/3511712},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {51:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Self-supervised short-text modeling through auxiliary context generation},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doing more with less: Overcoming data scarcity for POI
recommendation via cross-region transfer. <em>TIST</em>, <em>13</em>(3),
50:1–24. (<a href="https://doi.org/10.1145/3511711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variability in social app usage across regions results in a high skew of the quantity and the quality of check-in data collected, which in turn is a challenge for effective location recommender systems. In this article, we present Axolotl ( Automated cross Location-network Transfer Learning ), a novel method aimed at transferring location preference models learned in a data-rich region to significantly boost the quality of recommendations in a data-scarce region. Axolotl predominantly deploys two channels for information transfer: (1) a meta-learning based procedure learned using location recommendation as well as social predictions, and (2) a lightweight unsupervised cluster-based transfer across users and locations with similar preferences. Both of these work together synergistically to achieve improved accuracy of recommendations in data-scarce regions without any prerequisite of overlapping users and with minimal fine-tuning. We build Axolotl on top of a twin graph-attention neural network model used for capturing the user- and location-conditioned influences in a user-mobility graph for each region. We conduct extensive experiments on 12 user mobility datasets across the US, Japan, and Germany, using three as source regions and nine of them (that have much sparsely recorded mobility data) as target regions. Empirically, we show that Axolotl achieves up to 18\% better recommendation performance than the existing state-of-the-art methods across all metrics.},
  archive      = {J_TIST},
  author       = {Vinayak Gupta and Srikanta Bedathur},
  doi          = {10.1145/3511711},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {50:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Doing more with less: Overcoming data scarcity for POI recommendation via cross-region transfer},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traveling transporter problem: Arranging a new circular
route in a public transportation system based on heterogeneous
non-monotonic urban data. <em>TIST</em>, <em>13</em>(3), 49:1–25. (<a
href="https://doi.org/10.1145/3510034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid computational intelligent systems that synergize learning-based inference models and route planning strategies have thrived in recent years. In this article, we focus on the non-monotonicity originated from heterogeneous urban data, as well as heuristics based on neural networks, and thereafter formulate the traveling transporter problem (TTP). TTP is a multi-criteria optimization problem and may be applied to the circular route deployment in public transportation. In particular, TTP aims to find an optimized route that maximizes passenger flow according to a neural-network-based inference model and minimizes the length of the route given several constraints, including must-visit stations and the requirement for additional ones. As a variation of the traveling salesman problem (TSP), we propose a framework that first recommends new stations’ location while considering the herding effect between stations, and thereafter combines state-of-the-art TSP solvers and a metaheuristic named Trembling Hand , which is inspired by self-efficacy for solving TTP. Precisely, the proposed Trembling Hand enhances the spatial exploration considering the structural patterns, previous actions, and aging factors. Evaluation conducted on two real-world mass transit systems, Tainan and Chicago, shows that the proposed framework can outperform other state-of-the-art methods by securing the Pareto-optimal toward the objectives of TTP among comparative methods under various constrained settings.},
  archive      = {J_TIST},
  author       = {Fandel Lin and Hsun-Ping Hsieh},
  doi          = {10.1145/3510034},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {49:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Traveling transporter problem: Arranging a new circular route in a public transportation system based on heterogeneous non-monotonic urban data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What can knowledge bring to machine learning?—a survey of
low-shot learning for structured data. <em>TIST</em>, <em>13</em>(3),
48:1–45. (<a href="https://doi.org/10.1145/3510030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include heavy reliance on massive training data, limited generalizability, and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of human. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.},
  archive      = {J_TIST},
  author       = {Yang Hu and Adriane Chapman and Guihua Wen and Dame Wendy Hall},
  doi          = {10.1145/3510030},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {48:1–45},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {What can knowledge bring to machine Learning?—A survey of low-shot learning for structured data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A computational framework for modeling biobehavioral rhythms
from mobile and wearable data streams. <em>TIST</em>, <em>13</em>(3),
47:1–27. (<a href="https://doi.org/10.1145/3510029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a computational framework for modeling biobehavioral rhythms - the repeating cycles of physiological, psychological, social, and environmental events - from mobile and wearable data streams. The framework incorporates four main components: mobile data processing, rhythm discovery, rhythm modeling, and machine learning. We evaluate the framework with two case studies using datasets of smartphone, Fitbit, and OURA smart ring to evaluate the framework’s ability to (1) detect cyclic biobehavior, (2) model commonality and differences in rhythms of human participants in the sample datasets, and (3) predict their health and readiness status using models of biobehavioral rhythms. Our evaluation demonstrates the framework’s ability to generate new knowledge and findings through rigorous micro- and macro-level modeling of human rhythms from mobile and wearable data streams collected in the wild and using them to assess and predict different life and health outcomes.},
  archive      = {J_TIST},
  author       = {Runze Yan and Xinwen Liu and Janine Dutcher and Michael Tumminia and Daniella Villalba and Sheldon Cohen and David Creswell and Kasey Creswell and Jennifer Mankoff and Anind Dey and Afsaneh Doryab},
  doi          = {10.1145/3510029},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {47:1–27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A computational framework for modeling biobehavioral rhythms from mobile and wearable data streams},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised video object segmentation via
dual-attention cross-branch fusion. <em>TIST</em>, <em>13</em>(3),
46:1–20. (<a href="https://doi.org/10.1145/3506716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, concerning the challenge of collecting large-scale explicitly annotated videos, weakly supervised video object segmentation (WSVOS) using video tags has attracted much attention. Existing WSVOS approaches follow a general pipeline including two phases, i.e., a pseudo masks generation phase and a refinement phase. To explore the intrinsic property and correlation buried in the video frames, most of them focus on the later phase by introducing optical flow as temporal information to provide more supervision. However, these optical flow-based studies are greatly affected by illumination and distortion and lack consideration of the discriminative capacity of multi-level deep features. In this article, with the goal of capturing more effective temporal information and investigating a temporal information fusion strategy accordingly, we propose a unified WSVOS model by adopting a two-branch architecture with a multi-level cross-branch fusion strategy, named as dual-attention cross-branch fusion network (DACF-Net). Concretely, the two branches of DACF-Net, i.e., a temporal prediction subnetwork (TPN) and a spatial segmentation subnetwork (SSN), are used for extracting temporal information and generating predicted segmentation masks, respectively. To perform the cross-branch fusion between TPN and SSN, we propose a dual-attention fusion module that can be plugged into the SSN flexibly. We also pose a cross-frame coherence loss (CFCL) to achieve smooth segmentation results by exploiting the coherence of masks produced by TPN and SSN. Extensive experiments demonstrate the effectiveness of proposed approach compared with the state-of-the-arts on two challenging datasets, i.e., Davis-2016 and YouTube-Objects.},
  archive      = {J_TIST},
  author       = {Lili Wei and Congyan Lang and Liqian Liang and Songhe Feng and Tao Wang and Shidi Chen},
  doi          = {10.1145/3506716},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {46:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Weakly supervised video object segmentation via dual-attention cross-branch fusion},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NEAR: Neighborhood edge AggregatoR for graph classification.
<em>TIST</em>, <em>13</em>(3), 45:1–17. (<a
href="https://doi.org/10.1145/3506714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning graph-structured data with graph neural networks (GNNs) has been recently emerging as an important field because of its wide applicability in bioinformatics, chemoinformatics, social network analysis, and data mining. Recent GNN algorithms are based on neural message passing, which enables GNNs to integrate local structures and node features recursively. However, past GNN algorithms based on 1-hop neighborhood neural message passing are exposed to a risk of loss of information on local structures and relationships. In this article, we propose Neighborhood Edge AggregatoR (NEAR), a framework that aggregates relations between the nodes in the neighborhood via edges. NEAR, which can be orthogonally combined with Graph Isomorphism Network (GIN), gives integrated information that describes which nodes in the neighborhood are connected. Therefore, NEAR can reflect additional information of a local structure of each node beyond the nodes themselves in 1-hop neighborhood. Experimental results on multiple graph classification tasks show that our algorithm makes a good improvement over other existing 1-hop based GNN-based algorithms.},
  archive      = {J_TIST},
  author       = {Cheolhyeong Kim and Haeseong Moon and Hyung Ju Hwang},
  doi          = {10.1145/3506714},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {45:1–17},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {NEAR: Neighborhood edge AggregatoR for graph classification},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utility-aware and privacy-preserving trajectory synthesis
model that resists social relationship privacy attacks. <em>TIST</em>,
<em>13</em>(3), 44:1–28. (<a
href="https://doi.org/10.1145/3495160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For academic research and business intelligence, trajectory data has been widely collected and analyzed. Releasing trajectory data to a third party may lead to serious privacy leakage, which has spawned considerable researches on trajectory privacy protection technology. However, existing work suffers from several shortcomings. They either focus on point-based location privacy, ignoring the spatio-temporal correlations among locations within a trajectory, or they protect the privacy of each user separately without considering privacy leakage of the social relationship between trajectories of different users. Besides, they fail to balance privacy protection and data utility. Motivated by these limitations, in this article, we propose S 3 T -Trajectory, which is a utility-aware and privacy-preserving trajectory synthesis model that Resists social relationship privacy attacks. Specifically, we first develop a time-dependent Markov chain based on an adaptive spatio-temporal discrete grid to efficiently and accurately capture human mobility behavior. Then, we propose three mobility feature metrics from spatio-temporal, semantic, and social dimensions. On the basis of the metrics, we construct a bi-level optimization problem to accomplish the utility-aware and privacy-preserving trajectory synthesizing. The upper-level objective guarantees data utility and the lower-level optimization problems (or upper-level constraints) provides two-layer privacy protection for S 3 T -Trajectory, i.e., resisting location inference attacks and social relationship privacy attacks. We conduct extensive experiments on large-scale real-world datasets loc-Gowalla and loc-Brightkite. The experimental results demonstrate the effectiveness and robustness of S 3 T Trajectory. Compared with the baseline models, S 3 T Trajectory achieves between 7.8\% and 23.8\% performance improvement in resisting social relationship privacy attacks and achieves at least 5.19\% improvement regarding data utility.},
  archive      = {J_TIST},
  author       = {Zhirun Zheng and Zhetao Li and Jie Li and Hongbo Jiang and Tong Li and Bin Guo},
  doi          = {10.1145/3495160},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {44:1–28},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Utility-aware and privacy-preserving trajectory synthesis model that resists social relationship privacy attacks},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithms for trajectory points clustering in
location-based social networks. <em>TIST</em>, <em>13</em>(3), 43:1–29.
(<a href="https://doi.org/10.1145/3480972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in localization techniques have fundamentally enhanced social networking services, allowing users to share their locations and location-related contents. This has further increased the popularity of location-based social networks (LBSNs) and produces a huge amount of trajectories composed of continuous and complex spatio-temporal points from people’s daily lives. How to accurately aggregate large-scale trajectories is an important and challenging task. Conventional clustering algorithms (e.g., k -means or k -mediods) cannot be directly employed to process trajectory data due to their serialization, triviality and redundancy. Aiming to overcome the drawbacks of traditional k -means algorithm and k -mediods, including their sensitivity to the selection of the initial k value, the cluster centers and easy convergence to a locally optimal solution, we first propose an optimized k -means algorithm (namely OKM ) to obtain k optimal initial clustering centers based on the density of trajectory points. Second, because k -means is sensitive to noisy points, we propose an improved k -mediods algorithm called IKMD based on an acceptable radius r by considering users’ geographic location in LBSNs. The value of k can be calculated based on r , and the optimal k points are selected as the initial clustering centers with high densities to reduce the cost of distance calculation. Thirdly, we thoroughly analyze the advantages of IKMD by comparing it with the commonly used clustering approaches through illustrative examples. Last, we conduct extensive experiments to evaluate the performance of IKMD against seven clustering approaches including the proposed optimized k -means algorithm, k -mediods algorithm, traditional density-based k -mediods algorithm and the state-of-the-arts trajectory clustering methods. The results demonstrate that IKMD significantly outperforms existing algorithms in the cost of distance calculation and the convergence speed. The methods proposed is proved to contribute to a larger effort targeted at advancing the study of intelligent trajectory data analytics.},
  archive      = {J_TIST},
  author       = {Nan Han and Shaojie Qiao and Kun Yue and Jianbin Huang and Qiang He and Tingting Tang and Faliang Huang and Chunlin He and Chang-An Yuan},
  doi          = {10.1145/3480972},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {43:1–29},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Algorithms for trajectory points clustering in location-based social networks},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy preservation for trajectory publication based on
differential privacy. <em>TIST</em>, <em>13</em>(3), 42:1–21. (<a
href="https://doi.org/10.1145/3474839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of location-aware devices, trajectory data have been used widely in real-life applications. However, trajectory data are often associated with sensitive labels, such as users’ purchase transactions and planned activities. As such, inappropriate sharing or publishing of these data could threaten users’ privacy, especially when an adversary has sufficient background knowledge about a trajectory through other data sources, such as social media (check-in tags). Though differential privacy has been used to address the privacy of trajectory data, no existing method can protect the privacy of both trajectory data and sensitive labels. In this article, we propose a comprehensive trajectory publishing algorithm with three effective procedures. First, we apply density-based clustering to determine hotspots and outliers and then blur their locations by generalization. Second, we propose a graph-based model to efficiently capture the relationship among sensitive labels and trajectory points in all records and leverage Laplace noise to achieve differential privacy. Finally, we generate and publish trajectories by traversing and updating this graph until we travel all vertexes. Our experiments on synthetic and real-life datasets demonstrate that our algorithm effectively protects the privacy of both sensitive labels and location data in trajectory publication. Compared with existing works on trajectory publishing, our algorithm can also achieve higher data utility.},
  archive      = {J_TIST},
  author       = {Lin Yao and Zhenyu Chen and Haibo Hu and Guowei Wu and Bin Wu},
  doi          = {10.1145/3474839},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {42:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Privacy preservation for trajectory publication based on differential privacy},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning-based trajectory pricing on
ride-hailing platforms. <em>TIST</em>, <em>13</em>(3), 41:1–19. (<a
href="https://doi.org/10.1145/3474841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic pricing plays an important role in solving the problems such as traffic load reduction, congestion control, and revenue improvement. Efficient dynamic pricing strategies can increase capacity utilization, total revenue of service providers, and the satisfaction of both passengers and drivers. Many proposed dynamic pricing technologies focus on short-term optimization and face poor scalability in modeling long-term goals for the limitations of solution optimality and prohibitive computation. In this article, a deep reinforcement learning framework is proposed to tackle the dynamic pricing problem for ride-hailing platforms. A soft actor-critic (SAC) algorithm is adopted in the reinforcement learning framework. First, the dynamic pricing problem is translated into a Markov Decision Process (MDP) and is set up in continuous action spaces, which is no need for the discretization of action space. Then, a new reward function is obtained by the order response rate and the KL-divergence between supply distribution and demand distribution. Experiments and case studies demonstrate that the proposed method outperforms the baselines in terms of order response rate and total revenue.},
  archive      = {J_TIST},
  author       = {Jianbin Huang and Longji Huang and Meijuan Liu and He Li and Qinglin Tan and Xiaoke Ma and Jiangtao Cui and De-Shuang Huang},
  doi          = {10.1145/3474841},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {41:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Deep reinforcement learning-based trajectory pricing on ride-hailing platforms},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPSClean: A framework for cleaning and repairing GPS data.
<em>TIST</em>, <em>13</em>(3), 40:1–22. (<a
href="https://doi.org/10.1145/3469088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of GPS-equipped mobile devices has led to the emergence of big trajectory data. The collected raw data usually contain errors and anomalies information caused by device failure, sensor error, and environment influence. Low-quality data fails to support application requirements and therefore raw data will be comprehensively cleaned before usage. Existing methods are suboptimal to detect GPS data errors and do the repairing. To solve the problem, we propose a framework called GPSClean to analyze the anomalies data and develop effective methods to repair the data. There are primarily four modules in GPSClean : (i) data preprocessing, (ii) data filling, (iii) data repairing, and (iv) data conversion. For (i), we propose an approach named MDSort (Maximum Disorder Sorting) to efficiently solve the issue of data disorder. For (ii), we propose a method named NNF (Nearest Neighbor Filling) to fill missing data. For (iii), we design an approach named RCSWS (Range Constraints and Sliding Window Statistics) to repair anomalies and also improve the accuracy of data repairing by mak7ing use of driving direction. We use 45 million real trajectory data to evaluate our proposal in a prototype database system SECONDO. Experimental results show that the accuracy of RCSWS is three times higher than an alternative method SCREEN and nearly an order of magnitude higher than an alternative method EWMA.},
  archive      = {J_TIST},
  author       = {Chenglong Fang and Feng Wang and Bin Yao and Jianqiu Xu},
  doi          = {10.1145/3469088},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {40:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {GPSClean: A framework for cleaning and repairing GPS data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating algorithmic sampling-based motion planning with
learning in autonomous driving. <em>TIST</em>, <em>13</em>(3), 39:1–27.
(<a href="https://doi.org/10.1145/3469086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling-based motion planning (SBMP) is a major algorithmic trajectory planning approach in autonomous driving given its high efficiency and outstanding performance in practice. However, driving safety still calls for further refinement of SBMP. In this article we organically integrate algorithmic motion planning with learning models to improve SBMP in highway traffic scenarios from the following two perspectives. First, given the number of points to be sampled, we develop a new model to sample “important” points for SBMP by predicting the intention of surrounding vehicles and learning the distribution of human drivers’ trajectory. Second, we empirically study the relationship between the number of sample points and the environment, which is largely ignored in conventional SBMP. Then, we provide a guideline to select the appropriate number of points to be sampled under different scenarios to guarantee efficiency. The simulation experiments are conducted based on the vehicle trajectory dataset NGSIM. The results show that the proposed sampling strategy outperforms existing sampling strategies in terms of the computing time, traveling time, and smoothness of the trajectory.},
  archive      = {J_TIST},
  author       = {Yifan Zhang and Jinghuai Zhang and Jindi Zhang and Jianping Wang and Kejie Lu and Jeff Hong},
  doi          = {10.1145/3469086},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {39:1–27},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Integrating algorithmic sampling-based motion planning with learning in autonomous driving},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate correlation-aware spatio-temporal graph
convolutional networks for multi-scale traffic prediction.
<em>TIST</em>, <em>13</em>(3), 38:1–22. (<a
href="https://doi.org/10.1145/3469087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction based on vehicle trajectories collected from the installed GPS devices is critically important to Intelligent Transportation Systems (ITS). One limitation of existing traffic prediction models is that they mostly focus on predicting road-segment level traffic conditions, which can be considered as a fine-grained prediction. In many scenarios, however, a coarse-grained prediction, such as predicting the traffic flows among different urban areas covering multiple road links, is also required to help government have a better understanding on traffic conditions from the macroscopic point of view. This is especially useful in the applications of urban planning and public transportation planning. Another limitation is that the correlations among different types of traffic-related features are largely ignored. For example, the traffic flow and traffic speed are usually negatively correlated. Existing works regard these traffic-related features as independent features without considering their correlations. In this article, we for the first time study the novel problem of multivariate correlation-aware multi-scale traffic flow predicting, and we propose a feature correlation-aware spatio-temporal graph convolutional networks named MC-STGCN to effectively address it. Specifically, given a road graph, we first construct a coarse-grained road graph based on both the topology closeness and the traffic flow similarity among the nodes (road links). Then a cross-scale spatial-temporal feature learning and fusion technique is proposed for dealing with both the fine- and coarse-grained traffic data. In the spatial domain, a cross-scale GCN is proposed to learn the multi-scale spatial features jointly and fuse them together. In the temporal domain, a cross-scale temporal network that is composed of a hierarchical attention is designed for effectively capturing intra- and inter-scale temporal correlations. To effectively capture the feature correlations, a feature correlation learning component is also designed. Finally, a structural constraint is introduced to make the predictions on the two scale traffic data consistent. We conduct extensive evaluations over two real traffic datasets, and the results demonstrate the superior performance of the proposal on both fine- and coarse-grained traffic predictions.},
  archive      = {J_TIST},
  author       = {Senzhang Wang and Meiyue Zhang and Hao Miao and Zhaohui Peng and Philip S. Yu},
  doi          = {10.1145/3469087},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {38:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Multivariate correlation-aware spatio-temporal graph convolutional networks for multi-scale traffic prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supply-demand-aware deep reinforcement learning for dynamic
fleet management. <em>TIST</em>, <em>13</em>(3), 37:1–19. (<a
href="https://doi.org/10.1145/3467979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online ride-hailing platforms have reduced significantly the amounts of the time that taxis are idle and that passengers spend on waiting. As a key component of these platforms, the fleet management problem can be naturally modeled as a Markov Decision Process, which enables us to use the deep reinforcement learning. However, existing studies are proposed based on simplified problem settings that fail to model the complicated supply-dynamics and restrict the performance in the real traffic environment. In this article, we propose a supply-demand-aware deep reinforcement learning algorithm for taxi dispatching, where we use a deep Q-network with action sampling policy, called AS-DQN, to learn an optimal dispatching policy. Furthermore, we utilize a dueling network architecture, called AS-DDQN, to improve the performance of AS-DQN. Extensive experiments on real-world datasets offer insight into the performance of our model and show that it is capable of outperforming the baseline approaches.},
  archive      = {J_TIST},
  author       = {Bolong Zheng and Lingfeng Ming and Qi Hu and Zhipeng Lü and Guanfeng Liu and Xiaofang Zhou},
  doi          = {10.1145/3467979},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {37:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Supply-demand-aware deep reinforcement learning for dynamic fleet management},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing trajectory gaps to find possible rendezvous
region. <em>TIST</em>, <em>13</em>(3), 36:1–23. (<a
href="https://doi.org/10.1145/3467977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given trajectory data with gaps, we investigate methods to identify possible rendezvous regions. The problem has societal applications such as improving maritime safety and regulatory enforcement. The challenges come from two aspects. First, gaps in trajectory data make it difficult to identify regions where moving objects may have rendezvoused for nefarious reasons. Hence, traditional linear or shortest path interpolation methods may not be able to detect such activities, since objects in a rendezvous may have traveled away from their usual routes to meet. Second, user detecting a rendezvous regions involve a large number of gaps and associated trajectories, making the task computationally very expensive. In preliminary work, we proposed a more effective way of handling gaps and provided examples to illustrate potential rendezvous regions. In this article, we are providing detailed experiments with both synthetic and real-world data. Experiments on synthetic data show that the accuracy improved by 50 percent, which is substantial as compared to the baseline approach. In this article, we propose a refined algorithm Temporal Selection Search for finding a potential rendezvous region and finding an optimal temporal range to improve computational efficiency. We also incorporate two novel spatial filters: (i) a Static Ellipse Intersection Filter and (ii) a Dynamic Circle Intersection Spatial Filter. Both the baseline and proposed approaches account for every possible rendezvous pattern. We provide a theoretical evaluation of the algorithms correctness and completeness along with a time complexity analysis. Experimental results on synthetic and real-world maritime trajectory data show that the proposed approach substantially improves the area pruning effectiveness and computation time over the baseline technique. We also performed experiments based on accuracy and precision on synthetic dataset on both proposed and baseline techniques.},
  archive      = {J_TIST},
  author       = {Arun Sharma and Shashi Shekhar},
  doi          = {10.1145/3467977},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {36:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Analyzing trajectory gaps to find possible rendezvous region},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and effective similar subtrajectory search: A
spatial-aware comprehension approach. <em>TIST</em>, <em>13</em>(3),
35:1–22. (<a href="https://doi.org/10.1145/3456723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many applications take subtrajectories as basic units for analysis, there is little research on the similar subtrajectory search problem aiming to return a portion of a trajectory (i.e., subtrajectory), which is the most similar to a query trajectory. We find that in some special cases, when a grid-based metric is used, this problem can be formulated as a reading comprehension problem, which has been studied extensively in the field of natural language processing (NLP). By this formulation, we can obtain faster models with better performance than existing methods. However, due to the difference between natural language and trajectory (e.g., spatial relationship), it is impossible to directly apply NLP models to this problem. Therefore, we propose a Similar Subtrajectory Search with a Graph Neural Networks framework. This framework contains four modules including a spatial-aware grid embedding module, a trajectory embedding module, a query-context trajectory fusion module, and a span prediction module. Specifically, in the spatial-aware grid embedding module, the spatial-based grid adjacency is constructed and delivered to the graph neural network to learn spatial-aware grid embedding. The trajectory embedding module aims to model the sequential information of trajectories. The purpose of the query-context trajectory fusion module is to fuse the information of the query trajectory to each grid of the context trajectories. Finally, the span prediction module aims to predict the start and the end of a subtrajectory for the context trajectory, which is the most similar to the query trajectory. We conduct comprehensive experiments on two real world datasets, where the proposed framework outperforms the state-of-the-art baselines consistently and significantly.},
  archive      = {J_TIST},
  author       = {Liwei Deng and Hao Sun and Rui Sun and Yan Zhao and Han Su},
  doi          = {10.1145/3456723},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {35:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Efficient and effective similar subtrajectory search: A spatial-aware comprehension approach},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introduction to the special issue on intelligent trajectory
analytics: Part II. <em>TIST</em>, <em>13</em>(3), 34:1–2. (<a
href="https://doi.org/10.1145/3510021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIST},
  author       = {Kai Zheng and Yong Li and Cyrus Shahabi and Hongzhi Yin},
  doi          = {10.1145/3510021},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {3},
  pages        = {34:1–2},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Introduction to the special issue on intelligent trajectory analytics: Part II},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CrimeTensor: Fine-scale crime prediction via tensor learning
with spatiotemporal consistency. <em>TIST</em>, <em>13</em>(2), 33:1–24.
(<a href="https://doi.org/10.1145/3501807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime poses a major threat to human life and property, which has been recognized as one of the most crucial problems in our society. Predicting the number of crime incidents in each region of a city before they happen is of great importance to fight against crime. There has been a great deal of research focused on crime prediction, ranging from introducing diversified data sources to exploring various prediction models. However, most of the existing approaches fail to offer fine-scale prediction results and take little notice of the intricate spatial-temporal-categorical correlations contained in crime incidents. In this article, we propose a tailor-made framework called CrimeTensor to predict the number of crime incidents belonging to different categories within each target region via tensor learning with spatiotemporal consistency. In particular, we model the crime data as a tensor and present an objective function which tries to take full advantage of the spatial, temporal, and categorical correlations contained in crime incidents. Moreover, a well-designed optimization algorithm which transforms the objective into a compact form and then applies CP decomposition to find the optimal solution is elaborated to solve the objective function. Furthermore, we develop an enhanced framework which takes a set of pre-selected regions to conduct prediction so as to further improve the computational efficiency of the optimization algorithm. Finally, extensive experiments are performed on both proprietary and public datasets and our framework significantly outperforms all the baselines in terms of each evaluation metric.},
  archive      = {J_TIST},
  author       = {Weichao Liang and Zhiang Wu and Zhe Li and Yong Ge},
  doi          = {10.1145/3501807},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {33:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {CrimeTensor: Fine-scale crime prediction via tensor learning with spatiotemporal consistency},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Location-centered house price prediction: A multi-task
learning approach. <em>TIST</em>, <em>13</em>(2), 32:1–25. (<a
href="https://doi.org/10.1145/3501806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate house prediction is of great significance to various real estate stakeholders such as house owners, buyers, and investors. We propose a location-centered prediction framework that differs from existing work in terms of data profiling and prediction model. Regarding data profiling, we make an important observation as follows – besides the in-house features such as floor area, the location plays a critical role in house price prediction. Unfortunately, existing work either overlooked it or had a coarse grained measurement of locations. Thereby, we define and capture a fine-grained location profile powered by a diverse range of location data sources, including transportation profile, education profile, suburb profile based on census data, and facility profile. Regarding the choice of prediction model, we observe that a variety of approaches either consider the entire data for modeling, or split the entire house data and model each partition independently. However, such modeling ignores the relatedness among partitions, and for all prediction scenarios, there may not be sufficient training samples per partition for the latter approach. We address this problem by conducting a careful study of exploiting the Multi-Task Learning (MTL) model. Specifically, we map the strategies for splitting the entire house data to the ways the tasks are defined in MTL, and select specific MTL-based methods with different regularization terms to capture and exploit the relatedness among tasks. Based on real-world house transaction data collected in Melbourne, Australia, we design extensive experimental evaluations, and the results indicate a significant superiority of MTL-based methods over state-of-the-art approaches. Meanwhile, we conduct an in-depth analysis on the impact of task definitions and method selections in MTL on the prediction performance, and demonstrate that the impact of task definitions on prediction performance far exceeds that of method selections.},
  archive      = {J_TIST},
  author       = {Guangliang Gao and Zhifeng Bao and Jie Cao and A. K. Qin and Timos Sellis},
  doi          = {10.1145/3501806},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {32:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Location-centered house price prediction: A multi-task learning approach},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on text classification: From traditional to deep
learning. <em>TIST</em>, <em>13</em>(2), 31:1–41. (<a
href="https://doi.org/10.1145/3495162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
  archive      = {J_TIST},
  author       = {Qian Li and Hao Peng and Jianxin Li and Congying Xia and Renyu Yang and Lichao Sun and Philip S. Yu and Lifang He},
  doi          = {10.1145/3495162},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {31:1–41},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {A survey on text classification: From traditional to deep learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian attribute bagging-based extreme learning machine
for high-dimensional classification and regression. <em>TIST</em>,
<em>13</em>(2), 30:1–26. (<a
href="https://doi.org/10.1145/3495164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a Bayesian attribute bagging-based extreme learning machine (BAB-ELM) to handle high-dimensional classification and regression problems. First, the decision-making degree (DMD) of a condition attribute is calculated based on the Bayesian decision theory, i.e., the conditional probability of the condition attribute given the decision attribute. Second, the condition attribute with the highest DMD is put into the condition attribute group (CAG) corresponding to the specific decision attribute. Third, the bagging attribute groups (BAGs) are used to train an ensemble learning model of extreme learning machines (ELMs). Each base ELM is trained on a BAG which is composed of condition attributes that are randomly selected from the CAGs. Fourth, the information amount ratios of bagging condition attributes to all condition attributes is used as the weights to fuse the predictions of base ELMs in BAB-ELM. Exhaustive experiments have been conducted to compare the feasibility and effectiveness of BAB-ELM with seven other ELM models, i.e., ELM, ensemble-based ELM (EN-ELM), voting-based ELM (V-ELM), ensemble ELM (E-ELM), ensemble ELM based on multi-activation functions (MAF-EELM), bagging ELM, and simple ensemble ELM. Experimental results show that BAB-ELM is convergent with the increase of base ELMs and also can yield higher classification accuracy and lower regression error for high-dimensional classification and regression problems.},
  archive      = {J_TIST},
  author       = {Yulin He and Xuan Ye and Joshua Zhexue Huang and Philippe Fournier-Viger},
  doi          = {10.1145/3495164},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {30:1–26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Bayesian attribute bagging-based extreme learning machine for high-dimensional classification and regression},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven targeted advertising recommendation system for
outdoor billboard. <em>TIST</em>, <em>13</em>(2), 29:1–23. (<a
href="https://doi.org/10.1145/3495159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose and study a novel data-driven framework for Targeted Outdoor Advertising Recommendation (TOAR) with a special consideration of user profiles and advertisement topics. Given an advertisement query and a set of outdoor billboards with different spatial locations and rental prices, our goal is to find a subset of billboards, such that the total targeted influence is maximum under a limited budget constraint. To achieve this goal, we are facing two challenges: (1) it is difficult to estimate targeted advertising influence in physical world; (2) due to NP hardness, many common search techniques fail to provide a satisfied solution with an acceptable time, especially for large-scale problem settings. Taking into account the exposure strength, advertisement matching degree, and advertising repetition effect, we first build a targeted influence model that can characterize that the advertising influence spreads along with users mobility. Subsequently, based on a divide-and-conquer strategy, we develop two effective approaches, i.e., a master–slave-based sequential optimization method, TOAR-MSS, and a cooperative co-evolution-based optimization method, TOAR-CC, to solve our studied problem. Extensive experiments on two real-world datasets clearly validate the effectiveness and efficiency of our proposed approaches.},
  archive      = {J_TIST},
  author       = {Liang Wang and Zhiwen Yu and Bin Guo and Dingqi Yang and Lianbo Ma and Zhidan Liu and Fei Xiong},
  doi          = {10.1145/3495159},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {29:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Data-driven targeted advertising recommendation system for outdoor billboard},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Make more connections: Urban traffic flow forecasting with
spatiotemporal adaptive gated graph convolution network. <em>TIST</em>,
<em>13</em>(2), 28:1–25. (<a
href="https://doi.org/10.1145/3488902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic flow forecasting is a critical issue in intelligent transportation systems. Due to the complexity and uncertainty of urban road conditions, how to capture the dynamic spatiotemporal correlation and make accurate predictions is very challenging. In most of existing works, urban road network is often modeled as a fixed graph based on local proximity. However, such modeling is not sufficient to describe the dynamics of the road network and capture the global contextual information. In this paper, we consider constructing the road network as a dynamic weighted graph through attention mechanism. Furthermore, we propose to seek both spatial neighbors and semantic neighbors to make more connections between road nodes. We propose a novel Spatiotemporal Adaptive Gated Graph Convolution Network ( STAG-GCN ) to predict traffic conditions for several time steps ahead. STAG-GCN mainly consists of two major components: (1) multivariate self-attention Temporal Convolution Network ( TCN ) is utilized to capture local and long-range temporal dependencies across recent, daily-periodic and weekly-periodic observations; (2) mix-hop AG-GCN extracts selective spatial and semantic dependencies within multi-layer stacking through adaptive graph gating mechanism and mix-hop propagation mechanism. The output of different components are weighted fused to generate the final prediction results. Extensive experiments on two real-world large scale urban traffic dataset have verified the effectiveness, and the multi-step forecasting performance of our proposed models outperforms the state-of-the-art baselines.},
  archive      = {J_TIST},
  author       = {Bin Lu and Xiaoying Gan and Haiming Jin and Luoyi Fu and Xinbing Wang and Haisong Zhang},
  doi          = {10.1145/3488902},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {28:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Make more connections: Urban traffic flow forecasting with spatiotemporal adaptive gated graph convolution network},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-GAN+: Estimating human mobility responses to COVID-19
through spatio-temporal generative adversarial networks with enhanced
features. <em>TIST</em>, <em>13</em>(2), 27:1–23. (<a
href="https://doi.org/10.1145/3481617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human mobility responses to the large-scale spreading of the COVID-19 pandemic is crucial, since its significance guides policymakers to give Non-pharmaceutical Interventions, such as closure or reopening of businesses. It is challenging to model due to complex social contexts and limited training data. Recently, we proposed a conditional generative adversarial network (COVID-GAN) to estimate human mobility response under a set of social and policy conditions integrated from multiple data sources. Although COVID-GAN achieves a good average estimation accuracy under real-world conditions, it produces higher errors in certain regions due to the presence of spatial heterogeneity and outliers. To address these issues, in this article, we extend our prior work by introducing a new spatio-temporal deep generative model, namely, COVID-GAN+. COVID-GAN+ deals with the spatial heterogeneity issue by introducing a new spatial feature layer that utilizes the local Moran statistic to model the spatial heterogeneity strength in the data. In addition, we redesign the training objective to learn the estimated mobility changes from historical average levels to mitigate the effects of spatial outliers. We perform comprehensive evaluations using urban mobility data derived from cell phone records and census data. Results show that COVID-GAN+ can better approximate real-world human mobility responses than prior methods, including COVID-GAN.},
  archive      = {J_TIST},
  author       = {Han Bao and Xun Zhou and Yiqun Xie and Yingxue Zhang and Yanhua Li},
  doi          = {10.1145/3481617},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {27:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {COVID-GAN+: Estimating human mobility responses to COVID-19 through spatio-temporal generative adversarial networks with enhanced features},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Earth imagery segmentation on terrain surface with limited
training labels: A semi-supervised approach based on physics-guided
graph co-training. <em>TIST</em>, <em>13</em>(2), 26:1–22. (<a
href="https://doi.org/10.1145/3481043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given earth imagery with spectral features on a terrain surface, this paper studies surface segmentation based on both explanatory features and surface topology. The problem is important in many spatial and spatiotemporal applications such as flood extent mapping in hydrology. The problem is uniquely challenging for several reasons: first, the size of earth imagery on a terrain surface is often much larger than the input of popular deep convolutional neural networks; second, there exists topological structure dependency between pixel classes on the surface, and such dependency can follow an unknown and non-linear distribution; third, there are often limited training labels. Existing methods for earth imagery segmentation often divide the imagery into patches and consider the elevation as an additional feature channel. These methods do not fully incorporate the spatial topological structural constraint within and across surface patches and thus often show poor results, especially when training labels are limited. Existing methods on semi-supervised and unsupervised learning for earth imagery often focus on learning representation without explicitly incorporating surface topology. In contrast, we propose a novel framework that explicitly models the topological skeleton of a terrain surface with a contour tree from computational topology, which is guided by the physical constraint (e.g., water flow direction on terrains). Our framework consists of two neural networks: a convolutional neural network (CNN) to learn spatial contextual features on a 2D image grid, and a graph neural network (GNN) to learn the statistical distribution of physics-guided spatial topological dependency on the contour tree. The two models are co-trained via variational EM. Evaluations on the real-world flood mapping datasets show that the proposed models outperform baseline methods in classification accuracy, especially when training labels are limited.},
  archive      = {J_TIST},
  author       = {Wenchong He and Arpan Man Sainju and Zhe Jiang and Da Yan and Yang Zhou},
  doi          = {10.1145/3481043},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {26:1–22},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Earth imagery segmentation on terrain surface with limited training labels: A semi-supervised approach based on physics-guided graph co-training},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised spatial deep learning for earth image
segmentation based on imperfect polyline labels. <em>TIST</em>,
<em>13</em>(2), 25:1–20. (<a
href="https://doi.org/10.1145/3480970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has achieved tremendous success in image segmentation for computer vision applications. The performance of these models heavily relies on the availability of large-scale high-quality training labels (e.g., PASCAL VOC 2012). Unfortunately, such large-scale high-quality training data are often unavailable in many real-world spatial or spatiotemporal problems in earth science and remote sensing (e.g., mapping the nationwide river streams for water resource management). Although extensive efforts have been made to reduce the reliance on labeled data (e.g., semi-supervised or unsupervised learning, few-shot learning), the complex nature of geographic data such as spatial heterogeneity still requires sufficient training labels when transferring a pre-trained model from one region to another. On the other hand, it is often much easier to collect lower-quality training labels with imperfect alignment with earth imagery pixels (e.g., through interpreting coarse imagery by non-expert volunteers). However, directly training a deep neural network on imperfect labels with geometric annotation errors could significantly impact model performance. Existing research that overcomes imperfect training labels either focuses on errors in label class semantics or characterizes label location errors at the pixel level. These methods do not fully incorporate the geometric properties of label location errors in the vector representation. To fill the gap, this article proposes a weakly supervised learning framework to simultaneously update deep learning model parameters and infer hidden true vector label locations. Specifically, we model label location errors in the vector representation to partially reserve geometric properties (e.g., spatial contiguity within line segments). Evaluations on real-world datasets in the National Hydrography Dataset (NHD) refinement application illustrate that the proposed framework outperforms baseline methods in classification accuracy.},
  archive      = {J_TIST},
  author       = {Zhe Jiang and Wenchong He and Marcus Stephen Kirby and Arpan Man Sainju and Shaowen Wang and Lawrence V. Stanislawski and Ethan J. Shavers and E. Lynn Usery},
  doi          = {10.1145/3480970},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {25:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Weakly supervised spatial deep learning for earth image segmentation based on imperfect polyline labels},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepRoute+: Modeling couriers’ spatial-temporal behaviors
and decision preferences for package pick-up route prediction.
<em>TIST</em>, <em>13</em>(2), 24:1–23. (<a
href="https://doi.org/10.1145/3481006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over 10 billion packages are picked up every day in China. A fundamental task raised in the emerging intelligent logistics systems is the couriers’ package pick-up route prediction, which is beneficial for package dispatching, arrival-time estimation and overdue-risk evaluation, by leveraging the predicted routes to improve those downstream tasks. In the package pick-up scene, the decision-making of a courier is affected by strict spatial-temporal constraints (e.g., package location, promised pick-up time, current time, and courier’s current location). Furthermore, couriers have different decision preferences on various factors (e.g., time factor, distance factor, and balance of both), based on their own perception of the environments and work experience. In this article, we propose a novel model, named DeepRoute+, to predict couriers’ future package pick-up routes according to the couriers’ decision experience and preference learned from the historical behaviors. Specifically, DeepRoute+ consists of three layers: (1) The representation layer produces experience- and preference-aware representations for the unpicked-up packages, in which a decision preference module can dynamically adjust the importance of factors that affects the courier’s decision under the current situation. (2) The transformer encoder layer encodes the representations of packages while considering the spatial-temporal correlations among them. (3) The attention-based decoder layer uses the attention mechanism to generate the whole pick-up route recurrently. Experiments on a real-world logistics dataset demonstrate the state-of-the-art performance of our model.},
  archive      = {J_TIST},
  author       = {Haomin Wen and Youfang Lin and Huaiyu Wan and Shengnan Guo and Fan Wu and Lixia Wu and Chao Song and Yinghui Xu},
  doi          = {10.1145/3481006},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {24:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {DeepRoute+: Modeling couriers’ spatial-temporal behaviors and decision preferences for package pick-up route prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Urban traffic dynamics prediction—a continuous
spatial-temporal meta-learning approach. <em>TIST</em>, <em>13</em>(2),
23:1–19. (<a href="https://doi.org/10.1145/3474837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic status (e.g., traffic speed and volume) is highly dynamic in nature, namely, varying across space and evolving over time. Thus, predicting such traffic dynamics is of great importance to urban development and transportation management. However, it is very challenging to solve this problem due to spatial-temporal dependencies and traffic uncertainties. In this article, we solve the traffic dynamics prediction problem from Bayesian meta-learning perspective and propose a novel continuous spatial-temporal meta-learner (cST-ML), which is trained on a distribution of traffic prediction tasks segmented by historical traffic data with the goal of learning a strategy that can be quickly adapted to related but unseen traffic prediction tasks. cST-ML tackles the traffic dynamics prediction challenges by advancing the Bayesian black-box meta-learning framework through the following new points: (1) cST-ML captures the dynamics of traffic prediction tasks using variational inference, and to better capture the temporal uncertainties within tasks, cST-ML performs as a rolling window within each task; (2) cST-ML has novel designs in architecture, where CNN and LSTM are embedded to capture the spatial-temporal dependencies between traffic status and traffic-related features; (3) novel training and testing algorithms for cST-ML are designed. We also conduct experiments on two real-world traffic datasets (taxi inflow and traffic speed) to evaluate our proposed cST-ML. The experimental results verify that cST-ML can significantly improve the urban traffic prediction performance and outperform all baseline models especially when obvious traffic dynamics and temporal uncertainties are presented.},
  archive      = {J_TIST},
  author       = {Yingxue Zhang and Yanhua Li and Xun Zhou and Jun Luo and Zhi-Li Zhang},
  doi          = {10.1145/3474837},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {23:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Urban traffic dynamics Prediction—A continuous spatial-temporal meta-learning approach},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative adversarial networks for spatio-temporal data: A
survey. <em>TIST</em>, <em>13</em>(2), 22:1–25. (<a
href="https://doi.org/10.1145/3474838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have shown remarkable success in producing realistic-looking images in the computer vision area. Recently, GAN-based techniques are shown to be promising for spatio-temporal-based applications such as trajectory prediction, events generation, and time-series data imputation. While several reviews for GANs in computer vision have been presented, no one has considered addressing the practical applications and challenges relevant to spatio-temporal data. In this article, we have conducted a comprehensive review of the recent developments of GANs for spatio-temporal data. We summarise the application of popular GAN architectures for spatio-temporal data and the common practices for evaluating the performance of spatio-temporal applications with GANs. Finally, we point out future research directions to benefit researchers in this area.},
  archive      = {J_TIST},
  author       = {Nan Gao and Hao Xue and Wei Shao and Sichen Zhao and Kyle Kai Qin and Arian Prabowo and Mohammad Saiedur Rahaman and Flora D. Salim},
  doi          = {10.1145/3474838},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {22:1–25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Generative adversarial networks for spatio-temporal data: A survey},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting citywide crowd dynamics at big events: A deep
learning system. <em>TIST</em>, <em>13</em>(2), 21:1–24. (<a
href="https://doi.org/10.1145/3472300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event crowd management has been a significant research topic with high social impact. When some big events happen such as an earthquake, typhoon, and national festival, crowd management becomes the first priority for governments (e.g., police) and public service operators (e.g., subway/bus operator) to protect people’s safety or maintain the operation of public infrastructures. However, under such event situations, human behavior will become very different from daily routines, which makes prediction of crowd dynamics at big events become highly challenging, especially at a citywide level. Therefore in this study, we aim to extract the “deep” trend only from the current momentary observations and generate an accurate prediction for the trend in the short future, which is considered to be an effective way to deal with the event situations. Motivated by these, we build an online system called DeepUrbanEvent, which can iteratively take citywide crowd dynamics from the current one hour as input and report the prediction results for the next one hour as output. A novel deep learning architecture built with recurrent neural networks is designed to effectively model these highly complex sequential data in an analogous manner to video prediction tasks. Experimental results demonstrate the superior performance of our proposed methodology to the existing approaches. Lastly, we apply our prototype system to multiple big real-world events and show that it is highly deployable as an online crowd management system.},
  archive      = {J_TIST},
  author       = {Renhe Jiang and Zekun Cai and Zhaonan Wang and Chuang Yang and Zipei Fan and Quanjun Chen and Xuan Song and Ryosuke Shibasaki},
  doi          = {10.1145/3472300},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {21:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Predicting citywide crowd dynamics at big events: A deep learning system},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph sequence neural network with an attention mechanism
for traffic speed prediction. <em>TIST</em>, <em>13</em>(2), 20:1–24.
(<a href="https://doi.org/10.1145/3470889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the emerging success of Graph Neural Networks (GNNs) for modeling graphical data. A GNN can model the spatial dependencies of nodes in a graph based on message passing through node aggregation. However, in many application scenarios, these spatial dependencies can change over time, and a basic GNN model cannot capture these changes. In this article, we propose a G raph S eq uence neural network with an A tt ention mechanism (GSeqAtt) for processing graph sequences. More specifically, two attention mechanisms are combined: a horizontal mechanism and a vertical mechanism. GTransformer, which is a horizontal attention mechanism for handling time series, is used to capture the correlations between graphs in the input time sequence. The vertical attention mechanism, a Graph Network (GN) block structure with an attention mechanism (GNAtt), acts within the graph structure in each frame of the time series. Experiments show that our proposed model is able to handle information propagation for graph sequences accurately and efficiently. Moreover, results on real-world data from three road intersections show that our GSeqAtt outperforms state-of-the-art baselines on the traffic speed prediction task.},
  archive      = {J_TIST},
  author       = {Zhilong Lu and Weifeng Lv and Zhipu Xie and Bowen Du and Guixi Xiong and Leilei Sun and Haiquan Wang},
  doi          = {10.1145/3470889},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {20:1–24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Graph sequence neural network with an attention mechanism for traffic speed prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep spatio-temporal adaptive 3D convolutional neural
networks for traffic flow prediction. <em>TIST</em>, <em>13</em>(2),
19:1–21. (<a href="https://doi.org/10.1145/3510829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is the upstream problem of path planning, intelligent transportation system, and other tasks. Many studies have been carried out on the traffic flow prediction of the spatio-temporal network, but the effects of spatio-temporal flexibility (historical data of the same type of time intervals in the same location will change flexibly) and spatio-temporal correlation (different road conditions have different effects at different times) have not been considered at the same time. We propose the Deep Spatio-temporal Adaptive 3D Convolution Neural Network (ST-A3DNet), which is a new scheme to solve both spatio-temporal correlation and flexibility, and consider spatio-temporal complexity (complex external factors, such as weather and holidays). Different from other traffic forecasting models, ST-A3DNet captures the spatio-temporal relationship at the same time through the Adaptive 3D convolution module, assigns different weights flexibly according to the influence of historical data, and obtains the impact of external factors on the flow through the ex-mask module. Considering the holidays and weather conditions, we train our model for experiments in Xi’an and Chengdu. We evaluate the ST-A3DNet and the results show that we have better results than the other 11 baselines.},
  archive      = {J_TIST},
  author       = {He Li and Xuejiao Li and Liangcai Su and Duo Jin and Jianbin Huang and Deshuang Huang},
  doi          = {10.1145/3510829},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {19:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Deep spatio-temporal adaptive 3D convolutional neural networks for traffic flow prediction},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal spatio-temporal prediction with stochastic
adversarial networks. <em>TIST</em>, <em>13</em>(2), 18:1–23. (<a
href="https://doi.org/10.1145/3458025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal (ST) data is a collection of multiple time series data with different spatial locations and is inherently stochastic and unpredictable. An accurate prediction over such data is an important building block for several urban applications, such as taxi demand prediction, traffic flow prediction, and so on. Existing deep learning based approaches assume that outcome is deterministic and there is only one plausible future; therefore, cannot capture the multimodal nature of future contents and dynamics. In addition, existing approaches learn spatial and temporal data separately as they assume weak correlation between them. To handle these issues, in this article, we propose a stochastic spatio-temporal generative model (named D-GAN) which adopts Generative Adversarial Networks (GANs)-based structure for more accurate ST prediction in multiple time steps. D-GAN consists of two components: (1) spatio-temporal correlation network which models spatio-temporal joint distribution of pixels and supports a stochastic sampling of latent variables for multiple plausible futures; (2) a stochastic adversarial network to jointly learn generation and variational inference of data through implicit distribution modeling. D-GAN also supports fusion of external factors through explicit objective to improve the model learning. Extensive experiments performed on two real-world datasets show that D-GAN achieves significant improvements and outperforms baseline models.},
  archive      = {J_TIST},
  author       = {Divya Saxena and Jiannong Cao},
  doi          = {10.1145/3458025},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {18:1–23},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Multimodal spatio-temporal prediction with stochastic adversarial networks},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introduction to the special issue on deep learning for
spatio-temporal data: Part 2. <em>TIST</em>, <em>13</em>(2), 17:1–4. (<a
href="https://doi.org/10.1145/3510023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIST},
  author       = {Senzhang Wang and Junbo Zhang and Yanjie Fu and Yong Li},
  doi          = {10.1145/3510023},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {2},
  pages        = {17:1–4},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Introduction to the special issue on deep learning for spatio-temporal data: Part 2},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FairSR: Fairness-aware sequential recommendation through
multi-task learning with preference graph embeddings. <em>TIST</em>,
<em>13</em>(1), 16:1–21. (<a
href="https://doi.org/10.1145/3495163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This article aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness , is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning-based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users’ and items’ attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness.},
  archive      = {J_TIST},
  author       = {Cheng-Te Li and Cheng Hsu and Yang Zhang},
  doi          = {10.1145/3495163},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {16:1–21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {FairSR: Fairness-aware sequential recommendation through multi-task learning with preference graph embeddings},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural networks: Taxonomy, advances, and trends.
<em>TIST</em>, <em>13</em>(1), 15:1–54. (<a
href="https://doi.org/10.1145/3495161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks provide a powerful toolkit for embedding real-world graphs into low-dimensional spaces according to specific tasks. Up to now, there have been several surveys on this topic. However, they usually lay emphasis on different angles so that the readers cannot see a panorama of the graph neural networks. This survey aims to overcome this limitation and provide a systematic and comprehensive review on the graph neural networks. First of all, we provide a novel taxonomy for the graph neural networks, and then refer to up to 327 relevant literatures to show the panorama of the graph neural networks. All of them are classified into the corresponding categories. In order to drive the graph neural networks into a new stage, we summarize four future research directions so as to overcome the challenges faced. It is expected that more and more scholars can understand and exploit the graph neural networks and use them in their research community.},
  archive      = {J_TIST},
  author       = {Yu Zhou and Haixia Zheng and Xin Huang and Shufeng Hao and Dengao Li and Jumin Zhao},
  doi          = {10.1145/3495161},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {15:1–54},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Graph neural networks: Taxonomy, advances, and trends},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining willing-to-pay behavior patterns from payment
datasets. <em>TIST</em>, <em>13</em>(1), 14:1–19. (<a
href="https://doi.org/10.1145/3485848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The customer base is the most valuable resource to E-commerce companies. A comprehensive understanding of customers’ preferences and behavior is crucial to developing good marketing strategies, in order to achieve optimal customer lifetime values (CLVs). For example, by exploring customer behavior patterns, given a marketing plan with a limited budget, a set of potential customers is able to be identified to maximize profit. In other words, personalized campaigns at the right time and in the right place can be treated as the last stage of consumption. Moreover, effective future purchase estimation and recommendation help guide the customer to the up-selling stage. The proposed willing-to-pay prediction model (W2P) exploits the transaction data to predict customer payment behavior based on a probabilistic graphical model, which provides semantic explanation of the estimated results and deals with the sparsity of payment data from each customer. Existing work in this domain ranks the customers by their probabilities of purchase in different conditions. However, the customer with the highest purchase probability does not necessarily spend the most. Therefore, we propose a CLV maximization algorithm based on the prediction results. In addition, we improve the model by behavioral segmentation wherein we group the customers by payment behaviors to reduce the size of the offline models and enhance the accuracy for low-frequency customers. The experiment results show that our model outperforms the state-of-the-art methods in purchase behavior prediction.},
  archive      = {J_TIST},
  author       = {Yu-Ting Wen and Hui-Kuo Yang and Wen-Chih Peng},
  doi          = {10.1145/3485848},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {14:1–19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Mining willing-to-pay behavior patterns from payment datasets},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-adaptive feature transformation networks for object
detection in low luminance images. <em>TIST</em>, <em>13</em>(1),
13:1–11. (<a href="https://doi.org/10.1145/3480973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent improvement of object detection techniques, many of them fail to detect objects in low-luminance images. The blurry and dimmed nature of low-luminance images results in the extraction of vague features and failure to detect objects. In addition, many existing object detection methods are based on models trained on both sufficient- and low-luminance images, which also negatively affect the feature extraction process and detection results. In this article, we propose a framework called Self-adaptive Feature Transformation Network (SFT-Net) to effectively detect objects in low-luminance conditions. The proposed SFT-Net consists of the following three modules: (1) feature transformation module, (2) self-adaptive module, and (3) object detection module. The purpose of the feature transformation module is to enhance the extracted feature through unsupervisely learning a feature domain projection procedure. The self-adaptive module is utilized as a probabilistic module producing appropriate features either from the transformed or the original features to further boost the performance and generalization ability of the proposed framework. Finally, the object detection module is designed to accurately detect objects in both low- and sufficient- luminance images by using the appropriate features produced by the self-adaptive module. The experimental results demonstrate that the proposed SFT-Net framework significantly outperforms the state-of-the-art object detection techniques, achieving an average precision (AP) of up to 6.35 and 11.89 higher on the sufficient- and low- luminance domain, respectively.},
  archive      = {J_TIST},
  author       = {Shih-Chia Huang and Quoc-Viet Hoang and Da-Wei Jaw},
  doi          = {10.1145/3480973},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {13:1–11},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Self-adaptive feature transformation networks for object detection in low luminance images},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting future locations with semantic trajectories.
<em>TIST</em>, <em>13</em>(1), 7:1–20. (<a
href="https://doi.org/10.1145/3465060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location prediction has attracted much attention due to its important role in many location-based services, including taxi services, route navigation, traffic planning, and location-based advertisements. Traditional methods only use spatial-temporal trajectory data to predict where a user will go next. The divorce of semantic knowledge from the spatial-temporal one inhibits our better understanding of users’ activities. Inspired by the architecture of Long Short Term Memory (LSTM), we design ST-LSTM, which draws on semantic trajectories to predict future locations. Semantic data add a new dimension to our study, increasing the accuracy of prediction. Since semantic trajectories are sparser than the spatial-temporal ones, we propose a strategic filling algorithm to solve this problem. In addition, as the prediction is based on the historical trajectories of users, the cold-start problem arises. We build a new virtual social network for users to resolve the issue. Experiments on two real-world datasets show that the performance of our method is superior to those of the baselines.},
  archive      = {J_TIST},
  author       = {Heli Sun and Xianglan Guo and Zhou Yang and Xuguang Chu and Xinwang Liu and Liang He},
  doi          = {10.1145/3465060},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  number       = {1},
  pages        = {7:1–20},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Predicting future locations with semantic trajectories},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
