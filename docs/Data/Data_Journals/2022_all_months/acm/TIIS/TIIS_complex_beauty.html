<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis---34">TIIS - 34</h2>
<ul>
<li><details>
<summary>
(2022). Detection and recognition of driver distraction using
multimodal signals. <em>TIIS</em>, <em>12</em>(4), 1–28. (<a
href="https://doi.org/10.1145/3519267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distracted driving is a leading cause of accidents worldwide. The tasks of distraction detection and recognition have been traditionally addressed as computer vision problems. However, distracted behaviors are not always expressed in a visually observable way. In this work, we introduce a novel multimodal dataset of distracted driver behaviors, consisting of data collected using twelve information channels coming from visual, acoustic, near-infrared, thermal, physiological and linguistic modalities. The data were collected from 45 subjects while being exposed to four different distractions (three cognitive and one physical). For the purposes of this paper, we performed experiments with visual, physiological, and thermal information to explore potential of multimodal modeling for distraction recognition. In addition, we analyze the value of different modalities by identifying specific visual, physiological, and thermal groups of features that contribute the most to distraction characterization. Our results highlight the advantage of multimodal representations and reveal valuable insights for the role played by the three modalities on identifying different types of driving distractions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519267},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Detection and recognition of driver distraction using multimodal signals},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the importance of user backgrounds and impressions:
Lessons learned from interactive AI applications. <em>TIIS</em>,
<em>12</em>(4), 1–29. (<a
href="https://doi.org/10.1145/3531066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this article, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video activity recognition tool in cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. However, those who encountered weaknesses earlier made significantly fewer errors, since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Motivated by these findings and similar existing work, we formalize and present a conceptual model of user’s past experiences that examine the relations between user’s backgrounds, experiences, and human factors in XAI systems based on usage time. Our work presents strong findings and implications, aiming to raise the awareness of AI designers toward biases associated with user impressions and backgrounds.},
  archive      = {J_TIIS},
  doi          = {10.1145/3531066},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {On the importance of user backgrounds and impressions: Lessons learned from interactive AI applications},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Textflow: Toward supporting screen-free manipulation of
situation-relevant smart messages. <em>TIIS</em>, <em>12</em>(4), 1–29.
(<a href="https://doi.org/10.1145/3519263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce TextFlow , a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users’ location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519263},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Textflow: Toward supporting screen-free manipulation of situation-relevant smart messages},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to support users in understanding intelligent systems?
An analysis and conceptual framework of user questions considering user
mindsets, involvement, and knowledge outcomes. <em>TIIS</em>,
<em>12</em>(4), 1–27. (<a
href="https://doi.org/10.1145/3519264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets, user involvement, and knowledge outcomes to reveal, differentiate and classify current notions in prior work. This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work. We further discuss related aspects such as stakeholders and trust, and also provide material to apply our framework in practice (e.g., ideation/design sessions). We thus hope to advance and structure the dialogue on supporting users in understanding intelligent systems.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519264},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {How to support users in understanding intelligent systems? an analysis and conceptual framework of user questions considering user mindsets, involvement, and knowledge outcomes},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating user-centred explanations via illocutionary
question answering: From philosophy to interfaces. <em>TIIS</em>,
<em>12</em>(4), 1–32. (<a
href="https://doi.org/10.1145/3519265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. With this work, we aim to prove that the philosophical theory of explanations presented by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive and illocutionary process of answering questions. Specifically, our contribution is an approach to frame illocution in a computer-friendly way, to achieve user-centrality with statistical question answering. Indeed, we frame the illocution of an explanatory process as that mechanism responsible for anticipating the needs of the explainee in the form of unposed, implicit, archetypal questions, hence improving the user-centrality of the underlying explanatory process. Therefore, we hypothesise that if an explanatory process is an illocutionary act of providing content-giving answers to questions, and illocution is as we defined it, the more explicit and implicit questions can be answered by an explanatory tool, the more usable (as per ISO 9241-210) its explanations. We tested our hypothesis with a user-study involving more than 60 participants, on two XAI-based systems, one for credit approval (finance) and one for heart disease prediction (healthcare). The results showed that increasing the illocutionary power of an explanatory tool can produce statistically significant improvements (hence with a P value lower than .05) on effectiveness. This, combined with a visible alignment between the increments in effectiveness and satisfaction, suggests that our understanding of illocution can be correct, giving evidence in favour of our theory.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519265},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Generating user-centred explanations via illocutionary question answering: From philosophy to interfaces},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of explanations in AI-assisted decision making:
Principles and comparisons. <em>TIIS</em>, <em>12</em>(4), 1–36. (<a
href="https://doi.org/10.1145/3519266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy — improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519266},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Effects of explanations in AI-assisted decision making: Principles and comparisons},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GO-finder: A registration-free wearable system for assisting
users in finding lost hand-held objects. <em>TIIS</em>, <em>12</em>(4),
1–29. (<a href="https://doi.org/10.1145/3519268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People spend an enormous amount of time and effort looking for lost objects. To help remind people of the location of lost objects, various computational systems that provide information on their locations have been developed. However, prior systems for assisting people in finding objects require users to register the target objects in advance. This requirement imposes a cumbersome burden on the users, and the system cannot help remind them of unexpectedly lost objects. We propose GO-Finder (“Generic Object Finder”), a registration-free wearable camera-based system for assisting people in finding an arbitrary number of objects based on two key features: automatic discovery of hand-held objects and image-based candidate selection. Given a video taken from a wearable camera, GO-Finder automatically detects and groups hand-held objects to form a visual timeline of the objects. Users can retrieve the last appearance of the object by browsing the timeline through a smartphone app. We conducted user studies to investigate how users benefit from using GO-Finder. In the first study, we asked participants to perform an object retrieval task and confirmed improved accuracy and reduced mental load in the object search task by providing clear visual cues on object locations. In the second study, the system’s usability on a longer and more realistic scenario was verified, accompanied by an additional feature of context-based candidate filtering. Participant feedback suggested the usefulness of GO-Finder also in realistic scenarios where more than one hundred objects appear.},
  archive      = {J_TIIS},
  doi          = {10.1145/3519268},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {GO-finder: A registration-free wearable system for assisting users in finding lost hand-held objects},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PEACE: A model of key social and emotional qualities of
conversational chatbots. <em>TIIS</em>, <em>12</em>(4), 1–29. (<a
href="https://doi.org/10.1145/3531064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain chatbots engage with users in natural conversations to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of a chatbot most correspond to users’ expectations and preferences. Even though existing work has considered a wide range of aspects, some key components are still missing. For example, the role of chatbots’ ability to communicate with humans at the emotional level remains an open subject of study. Furthermore, these trait qualities are likely to cover several dimensions. It is crucial to understand how the different qualities relate and interact with each other and what the core aspects would be. For this purpose, we first designed an exploratory user study aimed at gaining a basic understanding of the desired qualities of chatbots with a special focus on their emotional intelligence. Using the findings from the first study, we constructed a model of the desired traits by carefully selecting a set of features. With the help of a large-scale survey and structural equation modeling, we further validated the model using data collected from the survey. The final outcome is called the PEACE model (Politeness, Entertainment, Attentive Curiosity, and Empathy) . By analyzing the dependencies between the different PEACE constructs, we shed light on the importance of and interplay between the chatbots’ qualities and the effect of users’ attitudes and concerns on their expectations of the technology. Not only PEACE defines the key ingredients of the social qualities of a chatbot, it also helped us derive a set of design implications useful for the development of socially adequate and emotionally aware open-domain chatbots.},
  archive      = {J_TIIS},
  doi          = {10.1145/3531064},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {PEACE: A model of key social and emotional qualities of conversational chatbots},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-icon+: An automated end-to-end code generation tool for
icon designs in UI development. <em>TIIS</em>, <em>12</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3531065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximately 50% of development resources are devoted to user interface (UI) development tasks [ 9 ]. Occupying a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this article, we present Auto-Icon+ , an approach for automatically generating readable and efficient code for icons from design artifacts. According to our interviews to understand the gap between designers (icons are assembled from multiple components) and developers (icons as single images), we apply a heuristic clustering algorithm to compose the components into an icon image. We then propose an approach based on a deep learning model and computer vision methods to convert the composed icon image to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real-world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon designs, in terms of saving 65.2% implementing time.},
  archive      = {J_TIIS},
  doi          = {10.1145/3531065},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Auto-icon+: An automated end-to-end code generation tool for icon designs in UI development},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ForSense: Accelerating online research through sensemaking
integration and machine research support. <em>TIIS</em>, <em>12</em>(4),
1–23. (<a href="https://doi.org/10.1145/3532853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences. Guided by sensemaking theory, we present ForSense, a browser extension for accelerating people’s online research experience. The two primary sources of novelty of ForSense  are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading. We use ForSense  as a design probe to explore (1) the benefits of integrating multiple stages of online research, (2) the opportunities to accelerate online research using current advances in machine reading, (3) the opportunities to support online research tasks in the presence of imprecise machine suggestions, and (4) insights about the behaviors people exhibit when performing online research, the pages they visit, and the artifacts they create. Through our design probe, we observe people performing online research tasks, and see that they benefit from ForSense’s integration and machine support for online research. From the information and insights we collected, we derive and share key recommendations for designing and supporting imprecise machine assistance for research tasks.},
  archive      = {J_TIIS},
  doi          = {10.1145/3532853},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ForSense: Accelerating online research through sensemaking integration and machine research support},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning semantically rich network-based multi-modal mobile
user interface embeddings. <em>TIIS</em>, <em>12</em>(4), 1–29. (<a
href="https://doi.org/10.1145/3533856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantically rich information from multiple modalities—text, code, images, categorical and numerical data—co-exist in the user interface (UI) design of mobile applications. Moreover, each UI design is composed of inter-linked UI entities that support different functions of an application, e.g., a UI screen comprising a UI taskbar, a menu, and multiple button elements. Existing UI representation learning methods unfortunately are not designed to capture multi-modal and linkage structure between UI entities. To support effective search and recommendation applications over mobile UIs, we need UI representations that integrate latent semantics present in both multi-modal information and linkages between UI entities. In this article, we present a novel self-supervised model—Multi-modal Attention-based Attributed Network Embedding (MAAN) model. MAAN is designed to capture structural network information present within the linkages between UI entities, as well as multi-modal attributes of the UI entity nodes. Based on the variational autoencoder framework, MAAN learns semantically rich UI embeddings in a self-supervised manner by reconstructing the attributes of UI entities and the linkages between them. The generated embeddings can be applied to a variety of downstream tasks: predicting UI elements associated with UI screens, inferring missing UI screen and element attributes, predicting UI user ratings, and retrieving UIs. Extensive experiments, including user evaluations, conducted on datasets from RICO, a rich real-world mobile UI repository, demonstrate that MAAN out-performs other state-of-the-art models. The number of linkages between UI entities can provide further information on the role of different UI entities in UI designs. However, MAAN does not capture edge attributes. To extend and generalize MAAN to learn even richer UI embeddings, we further propose EMAAN to capture edge attributes. We conduct additional extensive experiments on EMAAN, which show that it improves the performance of MAAN and similarly out-performs state-of-the-art models.},
  archive      = {J_TIIS},
  doi          = {10.1145/3533856},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {11},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Learning semantically rich network-based multi-modal mobile user interface embeddings},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving office workers’ workspace using a self-adjusting
computer screen. <em>TIIS</em>, <em>12</em>(3), 1–32. (<a
href="https://doi.org/10.1145/3545993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid evolution of technology, computers and their users’ workspaces have become an essential part of our life in general. Today, many people use computers both for work and for personal needs, spending long hours sitting at a desk in front of a computer screen, changing their pose slightly from time to time. This phenomenon impacts people’s health negatively, adversely affecting their musculoskeletal and ocular systems. To mitigate these risks, several different ergonomic solutions have been suggested. This study proposes, demonstrates, and evaluates a technological solution that automatically adjusts the computer screen position and orientation to its user’s current pose, using a simple RGB camera and robotic arm. The automatic adjustment will reduce the physical load on users and better fit their changing poses. The user’s pose is extracted from images continuously acquired by the system’s camera. The most suitable screen position is calculated according to the user’s pose and ergonomic guidelines. Thereafter, the robotic arm adjusts the screen accordingly. The evaluation was done through a user study with 35 users who rated both the idea and the prototype system itself highly.},
  archive      = {J_TIIS},
  doi          = {10.1145/3545993},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Improving office workers’ workspace using a self-adjusting computer screen},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An agile new research framework for hybrid human-AI teaming:
Trust, transparency, and transferability. <em>TIIS</em>, <em>12</em>(3),
1–36. (<a href="https://doi.org/10.1145/3514257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations.},
  archive      = {J_TIIS},
  doi          = {10.1145/3514257},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {An agile new research framework for hybrid human-AI teaming: Trust, transparency, and transferability},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward involving end-users in interactive human-in-the-loop
AI fairness. <em>TIIS</em>, <em>12</em>(3), 1–30. (<a
href="https://doi.org/10.1145/3514258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning experts in making their AI models fairer. Drawing inspiration from an Explainable AI approach called explanatory debugging used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any technical or domain background to identify potential fairness issues and possibly fix them in the context of loan decisions. Through workshops with end-users, we co-designed and implemented a prototype system that allowed end-users to see why predictions were made, and then to change weights on features to “debug” fairness issues. We evaluated the use of this prototype system through an online study. To investigate the implications of diverse human values about fairness around the globe, we also explored how cultural dimensions might play a role in using this prototype. Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.},
  archive      = {J_TIIS},
  doi          = {10.1145/3514258},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Toward involving end-users in interactive human-in-the-loop AI fairness},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ClioQuery: Interactive query-oriented text analytics for
comprehensive investigation of historical news archives. <em>TIIS</em>,
<em>12</em>(3), 1–49. (<a
href="https://doi.org/10.1145/3524025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historians and archivists often find and analyze the occurrences of query words in newspaper archives to help answer fundamental questions about society. But much work in text analytics focuses on helping people investigate other textual units, such as events, clusters, ranked documents, entity relationships, or thematic hierarchies. Informed by a study into the needs of historians and archivists, we thus propose ClioQuery , a text analytics system uniquely organized around the analysis of query words in context. ClioQuery applies text simplification techniques from natural language processing to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. It also pairs these new NLP methods with more traditional features like linked views and in-text highlighting to help engender trust in summarization techniques. We evaluate ClioQuery with two separate user studies, in which historians explain how ClioQuery ’s novel text simplification features can help facilitate historical research. We also evaluate with a separate quantitative comparison study, which shows that ClioQuery helps crowdworkers find and remember historical information. Such results suggest possible new directions for text analytics in other query-oriented settings.},
  archive      = {J_TIIS},
  doi          = {10.1145/3524025},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-49},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {ClioQuery: Interactive query-oriented text analytics for comprehensive investigation of historical news archives},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Expressive latent feature modelling for explainable matrix
factorisation-based recommender systems. <em>TIIS</em>, <em>12</em>(3),
1–30. (<a href="https://doi.org/10.1145/3530299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional matrix factorisation (MF)-based recommender system methods, despite their success in making the recommendation, lack explainable recommendations as the produced latent features are meaningless and cannot explain the recommendation. This article introduces an MF-based explainable recommender system framework that utilises the user-item rating data and the available item information to model meaningful user and item latent features. These features are exploited to enhance the rating prediction accuracy and the recommendation explainability. Our proposed feature-based explainable recommender system framework utilises these meaningful user and item latent features to explain the recommendation without relying on private or outer data. The recommendations are explained to the user using text message and bar chart. Our proposed model has been evaluated in terms of the rating prediction accuracy and the reasonableness of the explanation using six real-world benchmark datasets for movies, books, video games, and fashion recommendation systems. The results show that the proposed model can produce accurate explainable recommendations.},
  archive      = {J_TIIS},
  doi          = {10.1145/3530299},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Expressive latent feature modelling for explainable matrix factorisation-based recommender systems},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of a multi-agent “human-in-the-loop” game design
system. <em>TIIS</em>, <em>12</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3531009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing games is a complicated and time-consuming process, where developing new levels for existing games can take weeks. Procedural content generation offers the potential to shorten this timeframe, however, automated design tools are not adopted widely in the game industry. This article presents an expert evaluation of a human-in-the-loop generative design approach for commercial game maps that incorporates multiple computational agents. The evaluation aims to gauge the extent to which such an approach could support and be accepted by human game designers and to determine whether the computational agents improve the overall design. To evaluate the approach, 11 game designers utilized the approach to design game levels with the computational agents both active and inactive. Eye-tracking, observational, and think-aloud data was collected to determine whether designers favored levels suggested by the computational agents. This data was triangulated with qualitative data from semi-structured interviews that were used to gather overall opinions of the approach. The eye-tracking data indicates that the participating game level designers showed a clear preference for levels suggested by the computational agents, however, expert designers in particular appeared to reject the idea that the computational agents are helpful. The perception of computational tools not being useful needs to be addressed if procedural content generation approaches are to fulfill their potential for the game industry.},
  archive      = {J_TIIS},
  doi          = {10.1145/3531009},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Evaluation of a multi-agent “Human-in-the-loop” game design system},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SketchMaker: Sketch extraction and reuse for interactive
scene sketch composition. <em>TIIS</em>, <em>12</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3543956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching is an intuitive and simple way to depict sciences with various object form and appearance characteristics. In the past few years, widely available touchscreen devices have increasingly made sketch-based human-AI co-creation applications popular. One key issue of sketch-oriented interaction is to prepare input sketches efficiently by non-professionals because it is usually difficult and time-consuming to draw an ideal sketch with appropriate outlines and rich details, especially for novice users with no sketching skills. Thus, sketching brings great obstacles for sketch applications in daily life. On the other hand, hand-drawn sketches are scarce and hard to collect. Given the fact that there are several large-scale sketch datasets providing sketch data resources, but they usually have a limited number of objects and categories in sketch, and do not support users to collect new sketch materials according to their personal preferences. In addition, few sketch-related applications support the reuse of existing sketch elements. Thus, knowing how to extract sketches from existing drawings and effectively re-use them in interactive scene sketch composition will provide an elegant way for sketch-based image retrieval (SBIR) applications, which are widely used in various touch screen devices. In this study, we first conduct a study on current SBIR to better understand the main requirements and challenges in sketch-oriented applications. Then we develop the SketchMaker as an interactive sketch extraction and composition system to help users generate scene sketches via reusing object sketches in existing scene sketches with minimal manual intervention. Moreover, we demonstrate how SBIR improves from composited scene sketches to verify the performance of our interactive sketch processing system. We also include a sketch-based video localization task as an alternative application of our sketch composition scheme. Our pilot study shows that our system is effective and efficient, and provides a way to promote practical applications of sketches.},
  archive      = {J_TIIS},
  doi          = {10.1145/3543956},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {SketchMaker: Sketch extraction and reuse for interactive scene sketch composition},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive driving assistant model (ADAM) for advising drivers
of autonomous vehicles. <em>TIIS</em>, <em>12</em>(3), 1–28. (<a
href="https://doi.org/10.1145/3545994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully autonomous driving is on the horizon; vehicles with advanced driver assistance systems (ADAS) such as Tesla&#39;s Autopilot are already available to consumers. However, all currently available ADAS applications require a human driver to be alert and ready to take control if needed. Partially automated driving introduces new complexities to human interactions with cars and can even increase collision risk. A better understanding of drivers’ trust in automation may help reduce these complexities. Much of the existing research on trust in ADAS has relied on use of surveys and physiological measures to assess trust and has been conducted using driving simulators. There have been relatively few studies that use telemetry data from real automated vehicles to assess trust in ADAS. In addition, although some ADAS technologies provide alerts when, for example, drivers’ hands are not on the steering wheel, these systems are not personalized to individual drivers. Needed are adaptive technologies that can help drivers of autonomous vehicles avoid crashes based on multiple real-time data streams. In this paper, we propose an architecture for adaptive autonomous driving assistance. Two layers of multiple sensory fusion models are developed to provide appropriate voice reminders to increase driving safety based on predicted driving status. Results suggest that human trust in automation can be quantified and predicted with 80% accuracy based on vehicle data, and that adaptive speech-based advice can be provided to drivers with 90 to 95% accuracy. With more data, these models can be used to evaluate trust in driving assistance tools, which can ultimately lead to safer and appropriate use of these features.},
  archive      = {J_TIIS},
  doi          = {10.1145/3545994},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Adaptive driving assistant model (ADAM) for advising drivers of autonomous vehicles},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of older adult’s voice assistant use for
health information seeking. <em>TIIS</em>, <em>12</em>(2), 1–32. (<a
href="https://doi.org/10.1145/3484507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although voice assistants are increasingly being adopted by older adults, we lack empirical research on how they interact with these devices for health information seeking. Also, prior work shows how voice assistant responses can provide misleading or inaccurate information and be harmful particularly in health contexts. Because of increased health needs while aging, this paper studies older adult’s (ages 65+) health-related voice assistant interactions. Motivated by a lack of empirical evidence for how older adults approach information seeking with emerging technologies, we first conducted a survey of n = 201 older adults to understand how they engage voice assistants compared to a range of offline and digital sources for health information seeking. Findings show how voice assistants were used for confirmatory health queries, with users showing signs of distrust. As much prior work focuses on perceptions of voice assistant use, we conducted scenario-based interviews with n = 35 older adults to study health-related voice assistant behavior. In interviews, participants engaged with different health topics (flu, migraine, high blood pressure) and scenario types (symptom-driven, behavior-driven) using a voice assistant. Findings show how conversational and human-like expectations with voice assistants lead to information breakdowns between the older adult and voice assistant. This paper contributes a nuanced query-level analysis of older adults’ voice-based health information seeking behaviors. Further, data provide evidence for how query reformulation happens with complex topics in voice-based information seeking. We use our findings to discuss how voice interfaces can better support older adults’ health information seeking behaviors and expectations.},
  archive      = {J_TIIS},
  doi          = {10.1145/3484507},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {An empirical study of older adult’s voice assistant use for health information seeking},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experiences of a speech-enabled conversational agent for the
self-report of well-being among people living with affective disorders:
An in-the-wild study. <em>TIIS</em>, <em>12</em>(2), 1–29. (<a
href="https://doi.org/10.1145/3484508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing commercial success of smart speaker devices following recent advancements in speech recognition technology has surfaced new opportunities for collecting self-reported health and well-being data. Speech-enabled conversational agents (CAs) in particular, deployed in home environments using just such systems, may offer increasingly intuitive and engaging means of self-report. To date, however, few real-world studies have examined users’ experiences of engaging in the self-report of mental health using such devices or the challenges of deploying these systems in the home context. With these aims in mind, this article recounts findings from a 4-week “in-the-wild” study during which 20 individuals with depression or bipolar disorder used a speech-enabled CA named “Sofia” to maintain a daily diary log, responding also to the World Health Organization–Five Well-Being Index WHO-5 scale every 2 weeks. Thematic analysis of post-study interviews highlights actions taken by participants to overcome CAs’ limitations, diverse personifications of a speech-enabled agent, and unique forms of valuing of this system among users’ personal and social circles. These findings serve as initial evidence for the potential of CAs to support the self-report of mental health and well-being, while highlighting the need to address outstanding technical limitations in addition to design challenges of conversational pattern matching, filling unmet interpersonal gaps, and the use of self-report CAs in the at-home social context. Based on these insights, we discuss implications for the future design of CAs to support the self-report of mental health and well-being.},
  archive      = {J_TIIS},
  doi          = {10.1145/3484508},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Experiences of a speech-enabled conversational agent for the self-report of well-being among people living with affective disorders: An in-the-wild study},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discourse behavior of older adults interacting with a
dialogue agent competent in multiple topics. <em>TIIS</em>,
<em>12</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3484510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a conversational agent designed to provide realistic conversational practice to older adults at risk of isolation or social anxiety, and show the results of a content analysis on a corpus of data collected from experiments with elderly patients interacting with our system. The conversational agent, represented by a virtual avatar, is designed to hold multiple sessions of casual conversation with older adults. Throughout each interaction, the system analyzes the prosodic and nonverbal behavior of users and provides feedback to the user in the form of periodic comments and suggestions on how to improve. Our avatar is unique in its ability to hold natural dialogues on a wide range of everyday topics—27 topics in three groups, developed in collaboration with a team of gerontologists. The three groups vary in “degrees of intimacy,” and as such in degrees of cognitive difficulty for the user. After collecting data from nine participants who interacted with the avatar for seven to nine sessions over a period of 3 to 4 weeks, we present results concerning dialogue behavior and inferred sentiment of the users. Analysis of the dialogues reveals correlations such as greater elaborateness for more difficult topics, increasing elaborateness with successive sessions, stronger sentiments in topics concerned with life goals rather than routine activities, and stronger self-disclosure for more intimate topics. In addition to their intrinsic interest, these results also reflect positively on the sophistication and practical applicability of our dialogue system.},
  archive      = {J_TIIS},
  doi          = {10.1145/3484510},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Discourse behavior of older adults interacting with a dialogue agent competent in multiple topics},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chatbots to support young adults’ mental health: An
exploratory study of acceptability. <em>TIIS</em>, <em>12</em>(2), 1–39.
(<a href="https://doi.org/10.1145/3485874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the prevalence of mental health conditions, stigma, lack of awareness, and limited resources impede access to care, creating a need to improve mental health support. The recent surge in scientific and commercial interest in conversational agents and their potential to improve diagnosis and treatment seems a potentially fruitful area in this respect, particularly for young adults who widely use such systems in other contexts. Yet, there is little research that considers the acceptability of conversational agents in mental health. This study, therefore, presents three research activities that explore whether conversational agents and, in particular, chatbots can be an acceptable solution in mental healthcare for young adults. First, a survey of young adults (in a university setting) provides an understanding of the landscape of mental health in this age group and of their views around mental health technology, including chatbots. Second, a literature review synthesises current evidence relating to the acceptability of mental health conversational agents and points to future research priorities. Third, interviews with counsellors who work with young adults, supported by a chatbot prototype and user-centred design techniques, reveal the perceived benefits and potential roles of mental health chatbots from the perspective of mental health professionals, while suggesting preconditions for the acceptability of the technology. Taken together, these research activities: provide evidence that chatbots are an acceptable solution to offering mental health support for young adults; identify specific challenges relating to both the technology and environment; and argue for the application of user-centred approaches during development of mental health chatbots and more systematic and rigorous evaluations of the resulting solutions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3485874},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Chatbots to support young adults’ mental health: An exploratory study of acceptability},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multilingual neural coaching model with enhanced long-term
dialogue structure. <em>TIIS</em>, <em>12</em>(2), 1–47. (<a
href="https://doi.org/10.1145/3487066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.},
  archive      = {J_TIIS},
  doi          = {10.1145/3487066},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-47},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A multilingual neural coaching model with enhanced long-term dialogue structure},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relational agents for the homeless with tuberculosis
experience: Providing social support through human–agent relationships.
<em>TIIS</em>, <em>12</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3488056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–computer interaction (HCI) research, relational agents (RAs) are increasingly used to improve social support for vulnerable groups including people exposed to stigmas, alienation, and isolation. However, technical support for tuberculosis (TB) patients, one such vulnerable group, remains insufficient due to the nature of the infectious disease and difficulties in accessing the homeless community. To derive design considerations for developing RAs targeting homeless TB patients, we conducted an empirical study on the patients. Data were collected through participatory observations and interviews and were processed using deductive thematic analysis. The patients’ environmental and behavioral characteristics were classified, which showed that understanding these factors in the design of an RA is important because the patients’ perception, attitudes, and expectations towards the agent are shaped by (and also shape) their environmental and behavioral characteristics, which consequently affect the nature of relationships formed between them. Therefore, we drew the following design considerations: (1) protection of privacy is a prerequisite to the use of an RA for homeless TB patients and can be addressed from both short-term (technical) and long-term (sociotechnical) perspectives; (2) the homeless group emphasized affective support from the agent, suggesting that relationships per se are already valuable to people who have been socially isolated and stigmatized; (3) consideration of the past memories in selecting social cues can facilitate the exchange of affective expressions in user–agent interaction; and (4) an RA should clarify to its interlocuters its identity as a machine to avoid confusing people with low technological literacy.},
  archive      = {J_TIIS},
  doi          = {10.1145/3488056},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Relational agents for the homeless with tuberculosis experience: Providing social support through human–agent relationships},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “I don’t know what you mean by ‘i am anxious’”: A new method
for evaluating conversational agent responses to standardized mental
health inputs for anxiety and depression. <em>TIIS</em>, <em>12</em>(2),
1–23. (<a href="https://doi.org/10.1145/3488057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational agents (CAs) are increasingly ubiquitous and are now commonly used to access medical information. However, we lack systematic data about the quality of advice such agents provide. This paper evaluates CA advice for mental health (MH) questions, a pressing issue given that we are undergoing a mental health crisis. Building on prior work, we define a new method to systematically evaluate mental health responses from CAs. We develop multi-utterance conversational probes derived from two widely used mental health diagnostic surveys, the PHQ-9 (Depression) and the GAD-7 (Anxiety). We evaluate the responses of two text-based chatbots and four voice assistants to determine whether CAs provide relevant responses and treatments. Evaluations were conducted both by clinicians and immersively by trained raters, yielding consistent results across all raters. Although advice and recommendations were generally low quality, they were better for Crisis probes and for probes concerning symptoms of Anxiety rather than Depression. Responses were slightly improved for text versus speech-based agents, and when CAs had access to extended dialogue context. Design implications include suggestions for improved responses through clarification sub-dialogues. Responses may also be improved by the incorporation of empathy although this needs to be combined with effective treatments or advice.},
  archive      = {J_TIIS},
  doi          = {10.1145/3488057},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {“I don’t know what you mean by `I am anxious&#39;”: A new method for evaluating conversational agent responses to standardized mental health inputs for anxiety and depression},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on conversational agents for healthcare and
wellbeing. <em>TIIS</em>, <em>12</em>(2), 1–3. (<a
href="https://doi.org/10.1145/3532860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  doi          = {10.1145/3532860},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-3},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Special issue on conversational agents for healthcare and wellbeing},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive cognitive training with reinforcement learning.
<em>TIIS</em>, <em>12</em>(1), 1–29. (<a
href="https://doi.org/10.1145/3476777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-assisted cognitive training can help patients affected by several illnesses alleviate their cognitive deficits or healthy people improve their mental performance. In most computer-based systems, training sessions consist of graded exercises, which should ideally be able to gradually improve the trainee’s cognitive functions. Indeed, adapting the difficulty of the exercises to how individuals perform in their execution is crucial to improve the effectiveness of cognitive training activities. In this article, we propose the use of reinforcement learning (RL) to learn how to automatically adapt the difficulty of computerized exercises for cognitive training. In our approach, trainees’ performance in performed exercises is used as a reward to learn a policy that changes over time the values of the parameters that determine exercise difficulty. We illustrate a method to be initially used to learn difficulty-variation policies tailored for specific categories of trainees, and then to refine these policies for single individuals. We present the results of two user studies that provide evidence for the effectiveness of our method: a first study, in which a student category policy obtained via RL was found to have better effects on the cognitive function than a standard baseline training that adopts a mechanism to vary the difficulty proposed by neuropsychologists, and a second study, demonstrating that adding an RL-based individual customization further improves the training process.},
  archive      = {J_TIIS},
  doi          = {10.1145/3476777},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Adaptive cognitive training with reinforcement learning},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Initial responses to false positives in AI-supported
continuous interactions: A colonoscopy case study. <em>TIIS</em>,
<em>12</em>(1), 1–18. (<a
href="https://doi.org/10.1145/3480247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of artificial intelligence (AI) in clinical support systems is increasing. In this article, we focus on AI support for continuous interaction scenarios. A thorough understanding of end-user behaviour during these continuous human-AI interactions, in which user input is sustained over time and during which AI suggestions can appear at any time, is still missing. We present a controlled lab study involving 21 endoscopists and an AI colonoscopy support system. Using a custom-developed application and an off-the-shelf videogame controller, we record participants’ navigation behaviour and clinical assessment across 14 endoscopic videos. Each video is manually annotated to mimic an AI recommendation, being either true positive or false positive in nature. We find that time between AI recommendation and clinical assessment is significantly longer for incorrect assessments. Further, the type of medical content displayed significantly affects decision time. Finally, we discover that the participant’s clinical role plays a large part in the perception of clinical AI support systems. Our study presents a realistic assessment of the effects of imperfect and continuous AI support in a clinical scenario.},
  archive      = {J_TIIS},
  doi          = {10.1145/3480247},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Initial responses to false positives in AI-supported continuous interactions: A colonoscopy case study},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SmartShots: An optimization approach for generating videos
with data visualizations embedded. <em>TIIS</em>, <em>12</em>(1), 1–21.
(<a href="https://doi.org/10.1145/3484506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videos are well-received methods for storytellers to communicate various narratives. To further engage viewers, we introduce a novel visual medium where data visualizations are embedded into videos to present data insights. However, creating such data-driven videos requires professional video editing skills, data visualization knowledge, and even design talents. To ease the difficulty, we propose an optimization method and develop SmartShots, which facilitates the automatic integration of in-video visualizations. For its development, we first collaborated with experts from different backgrounds, including information visualization, design, and video production. Our discussions led to a design space that summarizes crucial design considerations along three dimensions: visualization, embedded layout, and rhythm. Based on that, we formulated an optimization problem that aims to address two challenges: (1) embedding visualizations while considering both contextual relevance and aesthetic principles and (2) generating videos by assembling multi-media materials. We show how SmartShots solves this optimization problem and demonstrate its usage in three cases. Finally, we report the results of semi-structured interviews with experts and amateur users on the usability of SmartShots.},
  archive      = {J_TIIS},
  doi          = {10.1145/3484506},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {SmartShots: An optimization approach for generating videos with data visualizations embedded},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tribe or not? Critical inspection of group differences using
TribalGram. <em>TIIS</em>, <em>12</em>(1), 1–34. (<a
href="https://doi.org/10.1145/3484509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains, including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group’s shared characteristics; in others, the group-level analysis can lead to problems, including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram , a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of “groups” mined from the data.},
  archive      = {J_TIIS},
  doi          = {10.1145/3484509},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Tribe or not? critical inspection of group differences using TribalGram},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding AI’s faults with AAR/AI: An empirical study.
<em>TIIS</em>, <em>12</em>(1), 1–33. (<a
href="https://doi.org/10.1145/3487065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Would you allow an AI agent to make decisions on your behalf? If the answer is “not always,” the next question becomes “in what circumstances”? Answering this question requires human users to be able to assess an AI agent—and not just with overall pass/fail assessments or statistics. Here users need to be able to localize an agent’s bugs so that they can determine when they are willing to rely on the agent and when they are not. After-Action Review for AI (AAR/AI), a new AI assessment process for integration with Explainable AI systems, aims to support human users in this endeavor, and in this article we empirically investigate AAR/AI’s effectiveness with domain-knowledgeable users. Our results show that AAR/AI participants not only located significantly more bugs than non-AAR/AI participants did (i.e., showed greater recall) but also located them more precisely (i.e., with greater precision). In fact, AAR/AI participants outperformed non-AAR/AI participants on every bug and were, on average, almost six times as likely as non-AAR/AI participants to find any particular bug. Finally, evidence suggests that incorporating labeling into the AAR/AI process may encourage domain-knowledgeable users to abstract above individual instances of bugs; we hypothesize that doing so may have contributed further to AAR/AI participants’ effectiveness.},
  archive      = {J_TIIS},
  doi          = {10.1145/3487065},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-33},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Finding AI’s faults with AAR/AI: An empirical study},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning GUI completions with user-defined constraints.
<em>TIIS</em>, <em>12</em>(1), 1–40. (<a
href="https://doi.org/10.1145/3490034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key objective in the design of graphical user interfaces (GUIs) is to ensure consistency across screens of the same product. However, designing a compliant layout is time-consuming and can distract designers from creative thinking. This paper studies layout recommendation methods that fulfill such consistency requirements using machine learning. Given a desired element type and size, the methods suggest element placements following real-world GUI design processes. Consistency requirements are given implicitly through previous layouts from which patterns are to be learned, comparable to existing screens of a software product. We adopt two recently proposed methods for this task, a Graph Neural Network (GNN) and a Transformer model, and compare them with a custom approach based on sequence alignment and nearest neighbor search (kNN) . The methods were tested on handcrafted datasets with explicit layout patterns, as well as large-scale public datasets of diverse mobile design layouts. Our results show that our instance-based learning algorithm outperforms both neural network approaches. Ultimately, this work contributes to establishing smarter design tools for professional designers with explainable algorithms that increase their efficacy.},
  archive      = {J_TIIS},
  doi          = {10.1145/3490034},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-40},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Learning GUI completions with user-defined constraints},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAtiMA toolkit: Toward an accessible tool for the
development of socio-emotional agents. <em>TIIS</em>, <em>12</em>(1),
1–30. (<a href="https://doi.org/10.1145/3510822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this article, we describe the FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers, and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects, and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, the FAtiMA Toolkit’s library-based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components, and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of the FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.},
  archive      = {J_TIIS},
  doi          = {10.1145/3510822},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {FAtiMA toolkit: Toward an accessible tool for the development of socio-emotional agents},
  volume       = {12},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
