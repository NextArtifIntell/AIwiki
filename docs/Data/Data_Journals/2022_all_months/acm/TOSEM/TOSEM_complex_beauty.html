<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOSEM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tosem---54">TOSEM - 54</h2>
<ul>
<li><details>
<summary>
(2022). Women’s participation in open source software: A survey of
the literature. <em>TOSEM</em>, <em>31</em>(4), 81:1–37. (<a
href="https://doi.org/10.1145/3510460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on women’s participation in OSS. It focuses on women contributors’ representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated women’s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5\% of projects were reported to have women as core developers, and women authored less than 5\% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.},
  archive      = {J_TOSEM},
  author       = {Bianca Trinkenreich and Igor Wiese and Anita Sarma and Marco Gerosa and Igor Steinmacher},
  doi          = {10.1145/3510460},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {81:1–37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Women’s participation in open source software: A survey of the literature},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing the plasticity of reinforcement learning-based
systems. <em>TOSEM</em>, <em>31</em>(4), 80:1–46. (<a
href="https://doi.org/10.1145/3511701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dataset available for pre-release training of a machine-learning based system is often not representative of all possible execution contexts that the system will encounter in the field. Reinforcement Learning (RL) is a prominent approach among those that support continual learning, i.e., learning continually in the field, in the post-release phase. No study has so far investigated any method to test the plasticity of RL-based systems, i.e., their capability to adapt to an execution context that may deviate from the training one. We propose an approach to test the plasticity of RL-based systems. The output of our approach is a quantification of the adaptation and anti-regression capabilities of the system, obtained by computing the adaptation frontier of the system in a changed environment. We visualize such frontier as an adaptation/anti-regression heatmap in two dimensions, or as a clustered projection when more than two dimensions are involved. In this way, we provide developers with information on the amount of changes that can be accommodated by the continual learning component of the system, which is key to decide if online, in-the-field learning can be safely enabled or not.},
  archive      = {J_TOSEM},
  author       = {Matteo Biagiola and Paolo Tonella},
  doi          = {10.1145/3511701},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {80:1–46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Testing the plasticity of reinforcement learning-based systems},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-aware prediction validator in deep learning
models for cyber-physical system data. <em>TOSEM</em>, <em>31</em>(4),
79:1–31. (<a href="https://doi.org/10.1145/3527451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Deep learning in Cyber-Physical Systems (CPSs) is gaining popularity due to its ability to bring intelligence to CPS behaviors. However, both CPSs and deep learning have inherent uncertainty. Such uncertainty, if not handled adequately, can lead to unsafe CPS behavior. The first step toward addressing such uncertainty in deep learning is to quantify uncertainty. Hence, we propose a novel method called NIRVANA (uNcertaInty pRediction ValidAtor iN Ai) for prediction validation based on uncertainty metrics. To this end, we first employ prediction-time Dropout-based Neural Networks to quantify uncertainty in deep learning models applied to CPS data. Second, such quantified uncertainty is taken as the input to predict wrong labels using a support vector machine, with the aim of building a highly discriminating prediction validator model with uncertainty values. In addition, we investigated the relationship between uncertainty quantification and prediction performance and conducted experiments to obtain optimal dropout ratios. We conducted all the experiments with four real-world CPS datasets. Results show that uncertainty quantification is negatively correlated to prediction performance of a deep learning model of CPS data. Also, our dropout ratio adjustment approach is effective in reducing uncertainty of correct predictions while increasing uncertainty of wrong predictions.},
  archive      = {J_TOSEM},
  author       = {Ferhat Ozgur Catak and Tao Yue and Shaukat Ali},
  doi          = {10.1145/3527451},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {79:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Uncertainty-aware prediction validator in deep learning models for cyber-physical system data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study on data distribution-aware test selection
for deep learning enhancement. <em>TOSEM</em>, <em>31</em>(4), 78:1–30.
(<a href="https://doi.org/10.1145/3511598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to traditional software that is constantly under evolution, deep neural networks need to evolve upon the rapid growth of test data for continuous enhancement (e.g., adapting to distribution shift in a new environment for deployment). However, it is labor intensive to manually label all of the collected test data. Test selection solves this problem by strategically choosing a small set to label. Via retraining with the selected set, deep neural networks will achieve competitive accuracy. Unfortunately, existing selection metrics involve three main limitations: (1) using different retraining processes, (2) ignoring data distribution shifts, and (3) being insufficiently evaluated. To fill this gap, we first conduct a systemically empirical study to reveal the impact of the retraining process and data distribution on model enhancement. Then based on our findings, we propose DAT, a novel distribution-aware test selection metric. Experimental results reveal that retraining using both the training and selected data outperforms using only the selected data. None of the selection metrics perform the best under various data distributions. By contrast, DAT effectively alleviates the impact of distribution shifts and outperforms the compared metrics by up to five times and 30.09\% accuracy improvement for model enhancement on simulated and in-the-wild distribution shift scenarios, respectively.},
  archive      = {J_TOSEM},
  author       = {Qiang Hu and Yuejun Guo and Maxime Cordy and Xiaofei Xie and Lei Ma and Mike Papadakis and Yves Le Traon},
  doi          = {10.1145/3511598},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {78:1–30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study on data distribution-aware test selection for deep learning enhancement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting patch correctness based on the similarity of
failing test cases. <em>TOSEM</em>, <em>31</em>(4), 77:1–30. (<a
href="https://doi.org/10.1145/3511096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches . We then propose BATS , an unsupervised learning-based approach to predict patch correctness by checking patch B ehavior A gainst failing T est S pecification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets—as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools.},
  archive      = {J_TOSEM},
  author       = {Haoye Tian and Yinghua Li and Weiguo Pian and Abdoul Kader Kaboré and Kui Liu and Andrew Habib and Jacques Klein and Tegawendé F. Bissyandé},
  doi          = {10.1145/3511096},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {77:1–30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Predicting patch correctness based on the similarity of failing test cases},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Just-in-time defect prediction on JavaScript projects: A
replication study. <em>TOSEM</em>, <em>31</em>(4), 76:1–38. (<a
href="https://doi.org/10.1145/3508479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-level defect prediction is widely referred to as just-in-time (JIT) defect prediction since it identifies a defect-inducing change at the check-in time, and researchers have proposed many approaches based on the language-independent change-level features. These approaches can be divided into two types: supervised approaches and unsupervised approaches, and their effectiveness has been verified on Java or C++ projects. However, whether the language-independent change-level features can effectively identify the defects of JavaScript projects is still unknown. Additionally, many researches have confirmed that supervised approaches outperform unsupervised approaches on Java or C++ projects when considering inspection effort. However, whether supervised JIT defect prediction approaches can still perform best on JavaScript projects is still unknown. Lastly, prior proposed change-level features are programming language–independent, whether programming language–specific change-level features can further improve the performance of JIT approaches on identifying defect-prone changes is also unknown. To address the aforementioned gap in knowledge, in this article, we collect and label the top-20 most starred JavaScript projects on GitHub. JavaScript is an extremely popular and widely used programming language in the industry. We propose five JavaScript-specific change-level features and conduct a large-scale empirical study (i.e., involving a total of 176,902 changes) and find that (1) supervised JIT defect prediction approaches (i.e., CBS+) still statistically significantly outperform unsupervised approaches on JavaScript projects when considering inspection effort; (2) JavaScript-specific change-level features can further improve the performance of approach built with language-independent features on identifying defect-prone changes; (3) the change-level features in the dimension of size (i.e., LT), diffusion (i.e., NF), and JavaScript-specific (i.e., SO and TC) are the most important features for indicating the defect-proneness of a change on JavaScript projects; and (4) project-related features (i.e., Stars, Branches, Def Ratio, Changes, Files, Defective, and Forks) have a high association with the probability of a change to be a defect-prone one on JavaScript projects.},
  archive      = {J_TOSEM},
  author       = {Chao Ni and Xin Xia and David Lo and Xiaohu Yang and Ahmed E. Hassan},
  doi          = {10.1145/3508479},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {76:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Just-in-time defect prediction on JavaScript projects: A replication study},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Turnover of companies in OpenStack: Prevalence and
rationale. <em>TOSEM</em>, <em>31</em>(4), 75:1–24. (<a
href="https://doi.org/10.1145/3510849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve commercial goals, companies have made substantial contributions to large open-source software (OSS) ecosystems such as OpenStack and have become the main contributors. However, they often withdraw their employees for a variety of reasons, which may affect the sustainability of OSS projects. While the turnover of individual contributors has been extensively investigated, there is a lack of knowledge about the nature of companies’ withdrawal. To this end, we conduct a mixed-methods empirical study on OpenStack to reveal how common company withdrawals were, to what degree withdrawn companies made contributions, and what the rationale behind withdrawals was. By analyzing the commit data of 18 versions of OpenStack, we find that the number of companies that have left is increasing and even surpasses the number of companies that have joined in later versions. Approximately 12\% of the companies in each version have exited by the next version. Compared to the sustaining companies that joined in the same version, the withdrawn companies tend to have a weaker contribution intensity but contribute to a similar scope of repositories in OpenStack. Through conducting a developer survey, we find four aspects of reasons for companies’ withdrawal from OpenStack: company, community, developer, and project. The most common reasons lie in the company aspect, i.e., the company either achieved its goals or failed to do so. By fitting the survival analysis model, we find that commercial goals are associated with the probability of the company’s withdrawal, and that a company’s contribution intensity and scale are positively correlated with its retention. Maintaining good retention is important but challenging for OSS ecosystems, and our results may shed light on potential approaches to improve company retention and reduce the negative impact of company withdrawal.},
  archive      = {J_TOSEM},
  author       = {Yuxia Zhang and Hui Liu and Xin Tan and Minghui Zhou and Zhi Jin and Jiaxin Zhu},
  doi          = {10.1145/3510849},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {75:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Turnover of companies in OpenStack: Prevalence and rationale},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Verifix: Verified repair of programming assignments.
<em>TOSEM</em>, <em>31</em>(4), 74:1–31. (<a
href="https://doi.org/10.1145/3510418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructor’s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.},
  archive      = {J_TOSEM},
  author       = {Umair Z. Ahmed and Zhiyu Fan and Jooyong Yi and Omar I. Al-Bataineh and Abhik Roychoudhury},
  doi          = {10.1145/3510418},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {74:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Verifix: Verified repair of programming assignments},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive mutation analysis via the natural language
channel in source code. <em>TOSEM</em>, <em>31</em>(4), 73:1–27. (<a
href="https://doi.org/10.1145/3510417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutation analysis can provide valuable insights into both the system under test and its test suite. However, it is not scalable due to the cost of building and testing a large number of mutants. Predictive Mutation Testing (PMT) has been proposed to reduce the cost of mutation testing, but it can only provide statistical inference about whether a mutant will be killed or not by the entire test suite. We propose Seshat, a Predictive Mutation Analysis (PMA) technique that can accurately predict the entire kill matrix , not just the Mutation Score (MS) of the given test suite. Seshat exploits the natural language channel in code, and learns the relationship between the syntactic and semantic concepts of each test case and the mutants it can kill, from a given kill matrix. The learnt model can later be used to predict the kill matrices for subsequent versions of the program, even after both the source and test code have changed significantly. Empirical evaluation using the programs in Defects4J shows that Seshat can predict kill matrices with an average F-score of 0.83 for versions that are up to years apart. This is an improvement in F-score by 0.14 and 0.45 points over the state-of-the-art PMT technique and a simple coverage-based heuristic, respectively. Seshat also performs as well as PMT for the prediction of the MS only. When applied to a mutant-based fault localisation technique, the predicted kill matrix by Seshat is successfully used to locate faults within the top 10 position, showing its usefulness beyond prediction of MS. Once Seshat trains its model using a concrete mutation analysis, the subsequent predictions made by Seshat are on average 39 times faster than actual test-based analysis. We also show that Seshat can be successfully applied to automatically generated test cases with an experiment using EvoSuite.},
  archive      = {J_TOSEM},
  author       = {Jinhan Kim and Juyoung Jeon and Shin Hong and Shin Yoo},
  doi          = {10.1145/3510417},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {73:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Predictive mutation analysis via the natural language channel in source code},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting compiler testing via compiler optimization
exploration. <em>TOSEM</em>, <em>31</em>(4), 72:1–33. (<a
href="https://doi.org/10.1145/3508362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: (1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and (2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g., -O0 , -O1 , -O2 , -O3 , -Os in GCC). To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that (1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting ), while 83.54\% of bugs are only detected under the latter; (2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing. We then propose the first approach, called COTest , by considering both factors to test compilers. Specifically, COTest first adopts machine-learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers.},
  archive      = {J_TOSEM},
  author       = {Junjie Chen and Chenyao Suo},
  doi          = {10.1145/3508362},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {72:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Boosting compiler testing via compiler optimization exploration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Verification of programs sensitive to heap layout.
<em>TOSEM</em>, <em>31</em>(4), 71:1–27. (<a
href="https://doi.org/10.1145/3508363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most C and C++ programs use dynamically allocated memory (often known as a heap) to store and organize their data. In practice, it can be useful to compare addresses of different heap objects, for instance, to store them in a binary search tree or a sorted array. However, comparisons of pointers to distinct objects are inherently ambiguous: The address order of two objects can be reversed in different executions of the same program, due to the nature of the allocation algorithm and other external factors. This poses a significant challenge to program verification, since a sound verifier must consider all possible behaviors of a program, including an arbitrary reordering of the heap. A naive verification of all possibilities, of course, leads to a combinatorial explosion of the state space: For this reason, we propose an under-approximating abstract domain that can be soundly refined to consider all relevant heap orderings. We have implemented the proposed abstract domain and evaluated it against several existing software verification tools on a collection of pointer-manipulating programs. In many cases, existing tools only consider a single fixed heap order, which is a source of unsoundness. We demonstrate that using our abstract domain, this unsoundness can be repaired at only a very modest performance cost. Additionally, we show that, even though many verifiers ignore it, ambiguous behavior is present in a considerable fraction of programs from software verification competition ( sv-comp ).},
  archive      = {J_TOSEM},
  author       = {Henrich Lauko and Lukáš Korenčik and Petr Ročkai},
  doi          = {10.1145/3508363},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {71:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Verification of programs sensitive to heap layout},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Super-optimization of smart contracts. <em>TOSEM</em>,
<em>31</em>(4), 70:1–29. (<a
href="https://doi.org/10.1145/3506800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are programs deployed on a blockchain. They are executed for a monetary fee paid in gas —a clear optimization target for smart contract compilers. Because smart contracts are a young, fast-moving field without (manually) fine-tuned compilers, they highly benefit from automated and adaptable approaches, especially as smart contracts are effectively immutable, and as such need a high level of assurance. This makes them an ideal domain for applying formal methods. Super-optimization is a technique to find the best translation of a block of instructions by trying all possible sequences of instructions that produce the same result. We present a framework for super-optimizing smart contracts based on Max-SMT with two main ingredients: (1) a stack functional specification extracted from the basic blocks of a smart contract, which is simplified using rules capturing the semantics of arithmetic, bit-wise, and relational operations, and (2) the synthesis of optimized blocks , which finds—by means of an efficient SMT encoding—basic blocks with minimal gas cost whose stack functional specification is equal (modulo commutativity) to the extracted one. We implemented our framework in the tool syrup 2.0 . Through large-scale experiments on real-world smart contracts, we analyze performance improvements for different SMT encodings, as well as tradeoffs between quality of optimizations and required optimization time.},
  archive      = {J_TOSEM},
  author       = {Elvira Albert and Pablo Gordillo and Alejandro Hernández-Cerezo and Albert Rubio and Maria A. Schett},
  doi          = {10.1145/3506800},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {70:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Super-optimization of smart contracts},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mutant reduction evaluation: What is there and what is
missing? <em>TOSEM</em>, <em>31</em>(4), 69:1–46. (<a
href="https://doi.org/10.1145/3522578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background. Mutation testing is a commonly used defect injection technique for evaluating the effectiveness of a test suite. However, it is usually computationally expensive. Therefore, many mutation reduction strategies, which aim to reduce the number of mutants, have been proposed. Problem. It is important to measure the ability of a mutation reduction strategy to maintain test suite effectiveness evaluation. However, existing evaluation indicators are unable to measure the “order-preserving ability”, i.e., to what extent the mutation score order among test suites is maintained before and after mutation reduction. As a result, misleading conclusions can be achieved when using existing indicators to evaluate the reduction effectiveness. Objective. We aim to propose evaluation indicators to measure the “order-preserving ability” of a mutation reduction strategy, which is important but missing in our community. Method. Given a test suite on a Software Under Test (SUT) with a set of original mutants, we leverage the test suite to generate a group of test suites that have a partial order relationship in defect detecting ability. When evaluating a reduction strategy, we first construct two partial order relationships among the generated test suites in terms of mutation score, one with the original mutants and another with the reduced mutants. Then, we measure the extent to which the partial order under the original mutants remains unchanged in the partial order under the reduced mutants. The more partial order is unchanged, the stronger the Order Preservation ( OP ) of the mutation reduction strategy is, and the more effective the reduction strategy is. Furthermore, we propose Effort-aware Relative Order Preservation ( EROP ) to measure how much gain a mutation reduction strategy can provide compared with a random reduction strategy. Result. The experimental results show that OP and EROP are able to efficiently measure the “order-preserving ability” of a mutation reduction strategy. As a result, they have a better ability to distinguish various mutation reduction strategies compared with the existing evaluation indicators. In addition, we find that Subsuming Mutant Selection (SMS) and Clustering Mutant Selection (CMS) are more effective than the other strategies under OP and EROP. Conclusion. We suggest, for the researchers, that OP and EROP should be used to measure the effectiveness of a mutant reduction strategy, and for the practitioners, that SMS and CMS should be given priority in practice.},
  archive      = {J_TOSEM},
  author       = {Peng Zhang and Yang Wang and Xutong Liu and Yanhui Li and Yibiao Yang and Ziyuan Wang and Xiaoyu Zhou and Lin Chen and Yuming Zhou},
  doi          = {10.1145/3522578},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {69:1–46},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Mutant reduction evaluation: What is there and what is missing?},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monitoring constraints and metaconstraints with temporal
logics on finite traces. <em>TOSEM</em>, <em>31</em>(4), 68:1–44. (<a
href="https://doi.org/10.1145/3506799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Runtime monitoring is a central operational decision support task in business process management. It helps process executors to check on-the-fly whether a running process instance satisfies business constraints of interest, providing an immediate feedback when deviations occur. We study runtime monitoring of properties expressed in ltl f , a variant of the classical ltl (Linear-time Temporal Logic) that is interpreted over finite traces, and in its extension ldl f , a powerful logic obtained by combining ltl f with regular expressions. We show that ldl f is able to declaratively express, in the logic itself, not only the constraints to be monitored, but also the de facto standard rv -LTL monitors. On the one hand, this enables us to directly employ the standard characterization of ldl f based on finite-state automata to monitor constraints in a fine-grained way. On the other hand, it provides the basis for declaratively expressing sophisticated metaconstraints that predicate on the monitoring state of other constraints, and to check them by relying on standard logical services instead of ad hoc algorithms. We then report on how this approach has been effectively implemented using Java to manipulate ldl f formulae and their corresponding monitors, and the RuM rule mining suite as underlying infrastructure.},
  archive      = {J_TOSEM},
  author       = {Giuseppe De Giacomo and Riccardo De Masellis and Fabrizio Maria Maggi and Marco Montali},
  doi          = {10.1145/3506799},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {68:1–44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Monitoring constraints and metaconstraints with temporal logics on finite traces},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How do successful and failed projects differ? A
socio-technical analysis. <em>TOSEM</em>, <em>31</em>(4), 67:1–24. (<a
href="https://doi.org/10.1145/3504003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development is at the intersection of the social realm , involving people who develop the software, and the technical realm , involving artifacts (code, docs, etc.) that are being produced. It has been shown that a socio-technical perspective provides rich information about the state of a software project. In particular, we are interested in socio-technical factors that are associated with project success . For this purpose, we frame the task as a network classification problem. We show how a set of heterogeneous networks composed of social and technical entities can be jointly embedded in a single vector space enabling mathematically sound comparisons between distinct software projects. Our approach is specifically designed using intuitive metrics stemming from network analysis and statistics to ease the interpretation of results in the context of software engineering wisdom. Based on a selection of 32 open source projects, we perform an empirical study to validate our approach considering three prediction scenarios to test the classification model’s ability generalizing to (1) randomly held-out project snapshots, (2) future project states, and (3) entirely new projects. Our results provide evidence that a socio-technical perspective is superior to a pure social or technical perspective when it comes to early indicators of future project success. To our surprise, the methodology proposed here even shows evidence of being able to generalize to entirely novel (project hold-out set) software projects reaching predication accuracies of 80\%, which is a further testament to the efficacy of our approach and beyond what has been possible so far. In addition, we identify key features that are strongly associated with project success. Our results indicate that even relatively simple socio-technical networks capture highly relevant and interpretable information about the early indicators of future project success.},
  archive      = {J_TOSEM},
  author       = {Mitchell Joblin and Sven Apel},
  doi          = {10.1145/3504003},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {67:1–24},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {How do successful and failed projects differ? a socio-technical analysis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accessibility in software practice: A practitioner’s
perspective. <em>TOSEM</em>, <em>31</em>(4), 66:1–26. (<a
href="https://doi.org/10.1145/3503508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners’ viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.},
  archive      = {J_TOSEM},
  author       = {Tingting Bi and Xin Xia and David Lo and John Grundy and Thomas Zimmermann and Denae Ford},
  doi          = {10.1145/3503508},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {66:1–26},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Accessibility in software practice: A practitioner’s perspective},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning for black-box testing of android
apps. <em>TOSEM</em>, <em>31</em>(4), 65:1–29. (<a
href="https://doi.org/10.1145/3502868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state space of Android apps is huge, and its thorough exploration during testing remains a significant challenge. The best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as one of Android apps. However, state-of-the-art, publicly available tools only support basic, Tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, including state-of-the-art tools, such as TimeMachine and Q-Testing. We also investigated the reasons behind such performance qualitatively, and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities. Moreover, we have developed FATE to fine-tune the hyperparameters of Deep RL algorithms on simulated apps, since it is computationally expensive to carry it out on real apps.},
  archive      = {J_TOSEM},
  author       = {Andrea Romdhana and Alessio Merlo and Mariano Ceccato and Paolo Tonella},
  doi          = {10.1145/3502868},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {65:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Deep reinforcement learning for black-box testing of android apps},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guaranteeing timed opacity using parametric timed model
checking. <em>TOSEM</em>, <em>31</em>(4), 64:1–36. (<a
href="https://doi.org/10.1145/3502851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information leakage can have dramatic consequences on systems security. Among harmful information leaks, the timing information leakage occurs whenever an attacker successfully deduces confidential internal information. In this work, we consider that the attacker has access (only) to the system execution time. We address the following timed opacity problem: given a timed system, a private location and a final location, synthesize the execution times from the initial location to the final location for which one cannot deduce whether the system went through the private location. We also consider the full timed opacity problem, asking whether the system is opaque for all execution times. We show that these problems are decidable for timed automata (TAs) but become undecidable when one adds parameters, yielding parametric timed automata (PTAs). We identify a subclass with some decidability results. We then devise an algorithm for synthesizing PTAs parameter valuations guaranteeing that the resulting TA is opaque. We finally show that our method can also apply to program analysis.},
  archive      = {J_TOSEM},
  author       = {Étienne André and Didier Lime and Dylan Marinho and Jun Sun},
  doi          = {10.1145/3502851},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {64:1–36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Guaranteeing timed opacity using parametric timed model checking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlating automated and human evaluation of code
documentation generation quality. <em>TOSEM</em>, <em>31</em>(4),
63:1–28. (<a href="https://doi.org/10.1145/3502853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic code documentation generation has been a crucial task in the field of software engineering. It not only relieves developers from writing code documentation but also helps them to understand programs better. Specifically, deep-learning-based techniques that leverage large-scale source code corpora have been widely used in code documentation generation. These works tend to use automatic metrics (such as BLEU, METEOR, ROUGE, CIDEr, and SPICE) to evaluate different models. These metrics compare generated documentation to reference texts by measuring the overlapping words. Unfortunately, there is no evidence demonstrating the correlation between these metrics and human judgment. We conduct experiments on two popular code documentation generation tasks, code comment generation and commit message generation, to investigate the presence or absence of correlations between these metrics and human judgments. For each task, we replicate three state-of-the-art approaches and the generated documentation is evaluated automatically in terms of BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. We also ask 24 participants to rate the generated documentation considering three aspects (i.e., language, content, and effectiveness). Each participant is given Java methods or commit diffs along with the target documentation to be rated. The results show that the ranking of generated documentation from automatic metrics is different from that evaluated by human annotators. Thus, these automatic metrics are not reliable enough to replace human evaluation for code documentation generation tasks. In addition, METEOR shows the strongest correlation (with moderate Pearson correlation r about 0.7) to human evaluation metrics. However, it is still much lower than the correlation observed between different annotators (with a high Pearson correlation r about 0.8) and correlations that are reported in the literature for other tasks (e.g., Neural Machine Translation  [ 39 ]). Our study points to the need to develop specialized automated evaluation metrics that can correlate more closely to human evaluation metrics for code generation tasks.},
  archive      = {J_TOSEM},
  author       = {Xing Hu and Qiuyuan Chen and Haoye Wang and Xin Xia and David Lo and Thomas Zimmermann},
  doi          = {10.1145/3502853},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {63:1–28},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Correlating automated and human evaluation of code documentation generation quality},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing and improving an evaluation dataset for detecting
semantic code clones via deep learning. <em>TOSEM</em>, <em>31</em>(4),
62:1–25. (<a href="https://doi.org/10.1145/3502852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.},
  archive      = {J_TOSEM},
  author       = {Hao Yu and Xing Hu and Ge Li and Ying Li and Qianxiang Wang and Tao Xie},
  doi          = {10.1145/3502852},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {62:1–25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Assessing and improving an evaluation dataset for detecting semantic code clones via deep learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated, cost-effective, and update-driven app testing.
<em>TOSEM</em>, <em>31</em>(4), 61:1–51. (<a
href="https://doi.org/10.1145/3502297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apps’ pervasive role in our society led to the definition of test automation approaches to ensure their dependability. However, state-of-the-art approaches tend to generate large numbers of test inputs and are unlikely to achieve more than 50\% method coverage. In this article, we propose a strategy to achieve significantly higher coverage of the code affected by updates with a much smaller number of test inputs, thus alleviating the test oracle problem. More specifically, we present ATUA, a model-based approach that synthesizes App models with static analysis, integrates a dynamically refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval. Its model-based strategy enables ATUA to generate a small set of inputs that exercise only the code affected by the updates. In turn, this makes common test oracle solutions more cost-effective, as they tend to involve human effort. A large empirical evaluation, conducted with 72 App versions belonging to nine popular Android Apps, has shown that ATUA is more effective and less effort-intensive than state-of-the-art approaches when testing App updates.},
  archive      = {J_TOSEM},
  author       = {Chanh Duc Ngo and Fabrizio Pastore and Lionel Briand},
  doi          = {10.1145/3502297},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {61:1–51},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Automated, cost-effective, and update-driven app testing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial robustness of deep code comment generation.
<em>TOSEM</em>, <em>31</em>(4), 60:1–30. (<a
href="https://doi.org/10.1145/3501256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT (Adversarial Code Comment gENeraTor) , an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method.},
  archive      = {J_TOSEM},
  author       = {Yu Zhou and Xiaoqing Zhang and Juanjuan Shen and Tingting Han and Taolue Chen and Harald Gall},
  doi          = {10.1145/3501256},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {60:1–30},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Adversarial robustness of deep code comment generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A common terminology for software risk management.
<em>TOSEM</em>, <em>31</em>(4), 59:1–47. (<a
href="https://doi.org/10.1145/3498539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation&#39;s processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology) , which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Protégé and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management.},
  archive      = {J_TOSEM},
  author       = {Jhon Masso and Félix García and César Pardo and Francisco J. Pino and Mario Piattini},
  doi          = {10.1145/3498539},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {59:1–47},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A common terminology for software risk management},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiRD: Race detection in software binaries under relaxed
memory models. <em>TOSEM</em>, <em>31</em>(4), 58:1–29. (<a
href="https://doi.org/10.1145/3498538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction reordering and interleavings in program execution under relaxed memory semantics result in non-intuitive behaviors, making it difficult to provide assurances about program correctness. Studies have shown that up to 90\% of the concurrency bugs reported by state-of-the-art static analyzers are false alarms. As a result, filtering false alarms and detecting real concurrency bugs is a challenging problem. Unsurprisingly, this problem has attracted the interest of the research community over the past few decades. Nonetheless, many of the existing techniques rely on analyzing source code, rarely consider the effects introduced by compilers, and assume a sequentially consistent memory model. In a practical setting, however, developers often do not have access to the source code, and even commodity architectures such as x86 and ARM are not sequentially consistent. In this work, we present B i rd , a prototype tool, to dynamically detect harmful data races in x86 binaries under relaxed memory models, TSO and PSO. B i rd employs source-DPOR to explore all distinct feasible interleavings for a multithreaded application. Our evaluation of B i rd on 42 publicly available benchmarks and its comparison with the state-of-the-art tools indicate B i rd ’s potential in effectively detecting data races in software binaries.},
  archive      = {J_TOSEM},
  author       = {Ridhi Jain and Rahul Purandare and Subodh Sharma},
  doi          = {10.1145/3498538},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {4},
  pages        = {58:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {BiRD: Race detection in software binaries under relaxed memory models},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive models in software engineering: Challenges and
opportunities. <em>TOSEM</em>, <em>31</em>(3), 56:1–72. (<a
href="https://doi.org/10.1145/3503509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
  archive      = {J_TOSEM},
  author       = {Yanming Yang and Xin Xia and David Lo and Tingting Bi and John Grundy and Xiaohu Yang},
  doi          = {10.1145/3503509},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {56:1–72},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Predictive models in software engineering: Challenges and opportunities},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Examining penetration tester behavior in the collegiate
penetration testing competition. <em>TOSEM</em>, <em>31</em>(3),
55:1–25. (<a href="https://doi.org/10.1145/3514040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penetration testing is a key practice toward engineering secure software. Malicious actors have many tactics at their disposal, and software engineers need to know what tactics attackers will prioritize in the first few hours of an attack. Projects like MITRE ATT&amp;CK™ provide knowledge, but how do people actually deploy this knowledge in real situations? A penetration testing competition provides a realistic, controlled environment with which to measure and compare the efficacy of attackers. In this work, we examine the details of vulnerability discovery and attacker behavior with the goal of improving existing vulnerability assessment processes using data from the 2019 Collegiate Penetration Testing Competition (CPTC). We constructed 98 timelines of vulnerability discovery and exploits for 37 unique vulnerabilities discovered by 10 teams of penetration testers. We grouped related vulnerabilities together by mapping to Common Weakness Enumerations and MITRE ATT&amp;CK™. We found that (1) vulnerabilities related to improper resource control (e.g., session fixation) are discovered faster and more often, as well as exploited faster, than vulnerabilities related to improper access control (e.g., weak password requirements), (2) there is a clear process followed by penetration testers of discovery/collection to lateral movement/pre-attack. Our methodology facilitates quicker analysis of vulnerabilities in future CPTC events.},
  archive      = {J_TOSEM},
  author       = {Benjamin S. Meyers and Sultan Fahad Almassari and Brandon N. Keller and Andrew Meneely},
  doi          = {10.1145/3514040},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {55:1–25},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Examining penetration tester behavior in the collegiate penetration testing competition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-travel investigation: Toward building a scalable attack
detection framework on ethereum. <em>TOSEM</em>, <em>31</em>(3),
54:1–33. (<a href="https://doi.org/10.1145/3505263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage. In this article, we propose a scalable attack detection framework named EthScope , which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around \( \text{2,300}\times \) when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.},
  archive      = {J_TOSEM},
  author       = {Siwei Wu and Lei Wu and Yajin Zhou and Runhuai Li and Zhi Wang and Xiapu Luo and Cong Wang and Kui Ren},
  doi          = {10.1145/3505263},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {54:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Time-travel investigation: Toward building a scalable attack detection framework on ethereum},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of the impact of hyperparameter tuning
and model optimization on the performance properties of deep neural
networks. <em>TOSEM</em>, <em>31</em>(3), 53:1–40. (<a
href="https://doi.org/10.1145/3506695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) models typically have many hyperparameters that can be configured to achieve optimal performance on a particular dataset. Practitioners usually tune the hyperparameters of their DNN models by training a number of trial models with different configurations of the hyperparameters, to find the optimal hyperparameter configuration that maximizes the training accuracy or minimizes the training loss. As such hyperparameter tuning usually focuses on the model accuracy or the loss function, it is not clear and remains under-explored how the process impacts other performance properties of DNN models, such as inference latency and model size. On the other hand, standard DNN models are often large in size and computing-intensive, prohibiting them from being directly deployed in resource-bounded environments such as mobile devices and Internet of Things (IoT) devices. To tackle this problem, various model optimization techniques (e.g., pruning or quantization) are proposed to make DNN models smaller and less computing-intensive so that they are better suited for resource-bounded environments. However, it is neither clear how the model optimization techniques impact other performance properties of DNN models such as inference latency and battery consumption, nor how the model optimization techniques impact the effect of hyperparameter tuning (i.e., the compounding effect). Therefore, in this paper, we perform a comprehensive study on four representative and widely-adopted DNN models, i.e., CNN image classification , Resnet-50 , CNN text classification , and LSTM sentiment classification , to investigate how different DNN model hyperparameters affect the standard DNN models, as well as how the hyperparameter tuning combined with model optimization affect the optimized DNN models, in terms of various performance properties (e.g., inference latency or battery consumption). Our empirical results indicate that tuning specific hyperparameters has heterogeneous impact on the performance of DNN models across different models and different performance properties. In particular, although the top tuned DNN models usually have very similar accuracy, they may have significantly different performance in terms of other aspects (e.g., inference latency). We also observe that model optimization has a confounding effect on the impact of hyperparameters on DNN model performance. For example, two sets of hyperparameters may result in standard models with similar performance but their performance may become significantly different after they are optimized and deployed on the mobile device. Our findings highlight that practitioners can benefit from paying attention to a variety of performance properties and the confounding effect of model optimization when tuning and optimizing their DNN models.},
  archive      = {J_TOSEM},
  author       = {Lizhi Liao and Heng Li and Weiyi Shang and Lei Ma},
  doi          = {10.1145/3506695},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {53:1–40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study of the impact of hyperparameter tuning and model optimization on the performance properties of deep neural networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). XCode: Towards cross-language code representation with
large-scale pre-training. <em>TOSEM</em>, <em>31</em>(3), 52:1–44. (<a
href="https://doi.org/10.1145/3506696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture. To address these issues, in this article, we propose a novel Cross -language Code representation with a large-scale pre-training ( XCode ) method. Concretely, we propose to use several abstract syntax trees and ELMo-enhanced variational autoencoders to obtain multiple pre-trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder (SED) architecture which uses the multi-teacher single-student method to transfer knowledge from the aforementioned pre-trained models to the distilled SED. The pre-trained models and SED will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-to-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.},
  archive      = {J_TOSEM},
  author       = {Zehao Lin and Guodun Li and Jingfeng Zhang and Yue Deng and Xiangji Zeng and Yin Zhang and Yao Wan},
  doi          = {10.1145/3506696},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {52:1–44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {XCode: Towards cross-language code representation with large-scale pre-training},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware code change embedding for better patch
correctness assessment. <em>TOSEM</em>, <em>31</em>(3), 51:1–29. (<a
href="https://doi.org/10.1145/3505247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the capability in successfully fixing more and more real-world bugs, existing Automated Program Repair (APR) techniques are still challenged by the long-standing overfitting problem (i.e., a generated patch that passes all tests is actually incorrect). Plenty of approaches have been proposed for automated patch correctness assessment (APCA ). Nonetheless, dynamic ones (i.e., those that needed to execute tests) are time-consuming while static ones (i.e., those built on top of static code features) are less precise. Therefore, embedding techniques have been proposed recently, which assess patch correctness via embedding token sequences extracted from the changed code of a generated patch. However, existing techniques rarely considered the context information and program structures of a generated patch, which are crucial for patch correctness assessment as revealed by existing studies. In this study, we explore the idea of context-aware code change embedding considering program structures for patch correctness assessment. Specifically, given a patch, we not only focus on the changed code but also take the correlated unchanged part into consideration, through which the context information can be extracted and leveraged. We then utilize the AST path technique for representation where the structure information from AST node can be captured. Finally, based on several pre-defined heuristics, we build a deep learning based classifier to predict the correctness of the patch. We implemented this idea as Cache and performed extensive experiments to assess its effectiveness. Our results demonstrate that Cache can (1) perform better than previous representation learning based techniques (e.g., Cache relatively outperforms existing techniques by \( \approx \)6\%, \( \approx \)3\%, and \( \approx \)16\%, respectively under three diverse experiment settings), and (2) achieve overall higher performance than existing APCA techniques while even being more precise than certain dynamic ones including PATCH-SIM (92.9\% vs. 83.0\%). Further results reveal that the context information and program structures leveraged by Cache contributed significantly to its outstanding performance.},
  archive      = {J_TOSEM},
  author       = {Bo Lin and Shangwen Wang and Ming Wen and Xiaoguang Mao},
  doi          = {10.1145/3505247},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {51:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Context-aware code change embedding for better patch correctness assessment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards robustness of deep program processing
models—detection, estimation, and enhancement. <em>TOSEM</em>,
<em>31</em>(3), 50:1–40. (<a
href="https://doi.org/10.1145/3511887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROT A to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROT M , which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROT T ) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.},
  archive      = {J_TOSEM},
  author       = {Huangzhao Zhang and Zhiyi Fu and Ge Li and Lei Ma and Zhehao Zhao and Hua’an Yang and Yizhe Sun and Yang Liu and Zhi Jin},
  doi          = {10.1145/3511887},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {50:1–40},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Towards robustness of deep program processing Models—Detection, estimation, and enhancement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting and augmenting missing key aspects in
vulnerability descriptions. <em>TOSEM</em>, <em>31</em>(3), 49:1–27. (<a
href="https://doi.org/10.1145/3498537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56\%, 85\%, 38\%, and 28\% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD) , which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem. To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88\%, 71\%, 61\%, and 81\% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 “future” CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.},
  archive      = {J_TOSEM},
  author       = {Hao Guo and Sen Chen and Zhenchang Xing and Xiaohong Li and Yude Bai and Jiamou Sun},
  doi          = {10.1145/3498537},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {49:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Detecting and augmenting missing key aspects in vulnerability descriptions},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of the effectiveness of an ensemble of
stand-alone sentiment detection tools for software engineering datasets.
<em>TOSEM</em>, <em>31</em>(3), 48:1–38. (<a
href="https://doi.org/10.1145/3491211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis in software engineering (SE) has shown promise to analyze and support diverse development activities. Recently, several tools are proposed to detect sentiments in software artifacts. While the tools improve accuracy over off-the-shelf tools, recent research shows that their performance could still be unsatisfactory. A more accurate sentiment detector for SE can help reduce noise in analysis of software scenarios where sentiment analysis is required. Recently, combinations, i.e., hybrids of stand-alone classifiers are found to offer better performance than the stand-alone classifiers for fault detection. However, we are aware of no such approach for sentiment detection for software artifacts. We report the results of an empirical study that we conducted to determine the feasibility of developing an ensemble engine by combining the polarity labels of stand-alone SE-specific sentiment detectors. Our study has two phases. In the first phase, we pick five SE-specific sentiment detection tools from two recently published papers by Lin et al. [ 29 , 30 ], who first reported negative results with stand alone sentiment detectors and then proposed an improved SE-specific sentiment detector, POME [ 29 ]. We report the study results on 17,581 units (sentences/documents) coming from six currently available sentiment benchmarks for software engineering. We find that the existing tools can be complementary to each other in 85-95\% of the cases, i.e., one is wrong but another is right. However, a majority voting-based ensemble of those tools fails to improve the accuracy of sentiment detection. We develop Sentisead, a supervised tool by combining the polarity labels and bag of words as features. Sentisead improves the performance (F1-score) of the individual tools by 4\% (over Senti4SD [ 5 ]) – 100\% (over POME [ 29 ]). The initial development of Sentisead occurred before we observed the use of deep learning models for SE-specific sentiment detection. In particular, recent papers show the superiority of advanced language-based pre-trained transformer models (PTM) over rule-based and shallow learning models. Consequently, in a second phase, we compare and improve Sentisead infrastructure using the PTMs. We find that a Sentisead infrastructure with RoBERTa as the ensemble of the five stand-alone rule-based and shallow learning SE-specific tools from Lin et al. [ 29 , 30 ] offers the best F1-score of 0.805 across the six datasets, while a stand-alone RoBERTa shows an F1-score of 0.801.},
  archive      = {J_TOSEM},
  author       = {Gias Uddin and Yann-Gaël Guéhénuc and Foutse Khomh and Chanchal K. Roy},
  doi          = {10.1145/3491211},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {48:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {An empirical study of the effectiveness of an ensemble of stand-alone sentiment detection tools for software engineering datasets},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NPC: Neuron path coverage via characterizing decision logic
of deep neural networks. <em>TOSEM</em>, <em>31</em>(3), 47:1–27. (<a
href="https://doi.org/10.1145/3490489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be. In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.},
  archive      = {J_TOSEM},
  author       = {Xiaofei Xie and Tianlin Li and Jian Wang and Lei Ma and Qing Guo and Felix Juefei-Xu and Yang Liu},
  doi          = {10.1145/3490489},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {47:1–27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {NPC: Neuron path coverage via characterizing decision logic of deep neural networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous and proactive software architecture evaluation:
An IoT case. <em>TOSEM</em>, <em>31</em>(3), 46:1–54. (<a
href="https://doi.org/10.1145/3492762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
  archive      = {J_TOSEM},
  author       = {Dalia Sobhy and Leandro Minku and Rami Bahsoon and Rick Kazman},
  doi          = {10.1145/3492762},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {46:1–54},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Continuous and proactive software architecture evaluation: An IoT case},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Industry–academia research collaboration and knowledge
co-creation: Patterns and anti-patterns. <em>TOSEM</em>, <em>31</em>(3),
45:1–52. (<a href="https://doi.org/10.1145/3494519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry–academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research–practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry–academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.},
  archive      = {J_TOSEM},
  author       = {Dusica Marijan and Sagar Sen},
  doi          = {10.1145/3494519},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {45:1–52},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Industry–Academia research collaboration and knowledge co-creation: Patterns and anti-patterns},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do developers really know how to use git commands? A
large-scale study using stack overflow. <em>TOSEM</em>, <em>31</em>(3),
44:1–29. (<a href="https://doi.org/10.1145/3494518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&amp;A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers.},
  archive      = {J_TOSEM},
  author       = {Wenhua Yang and Chong Zhang and Minxue Pan and Chang Xu and Yu Zhou and Zhiqiu Huang},
  doi          = {10.1145/3494518},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {44:1–29},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Do developers really know how to use git commands? a large-scale study using stack overflow},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All in one: Design, verification, and implementation of
SNOW-optimal read atomic transactions. <em>TOSEM</em>, <em>31</em>(3),
43:1–44. (<a href="https://doi.org/10.1145/3494517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed read atomic transactions are important building blocks of modern cloud databases that magnificently bridge the gap between data availability and strong data consistency. The performance of their transactional reads is particularly critical to the overall system performance, as many real-world database workloads are dominated by reads. Following the SNOW design principle for optimal reads, we develop LORA, a novel SNOW-optimal algorithm for distributed read atomic transactions. LORA completes its reads in exactly one round trip, even in the presence of conflicting writes, without imposing additional overhead to the communication, and it outperforms the state-of-the-art read atomic algorithms. To guide LORA’s development, we present a rewriting-logic-based framework and toolkit for design, verification, implementation, and evaluation of distributed databases. Within the framework, we formalize LORA and mathematically prove its data consistency guarantees. We also apply automatic model checking and statistical verification to validate our proofs and to estimate LORA’s performance. We additionally generate from the formal model a correct-by-construction distributed implementation for testing and performance evaluation under realistic deployments. Our design-level and implementation-based experimental results are consistent, which together demonstrate LORA’s promising data consistency and performance achievement.},
  archive      = {J_TOSEM},
  author       = {Si Liu},
  doi          = {10.1145/3494517},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {43:1–44},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {All in one: Design, verification, and implementation of SNOW-optimal read atomic transactions},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using personality detection tools for software engineering
research: How far can we go? <em>TOSEM</em>, <em>31</em>(3), 42:1–48.
(<a href="https://doi.org/10.1145/3491039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-the-shelf solutions trained on non-technical corpora, which might not be readily applicable to technical domains like software engineering. In this article, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers’ e-mails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers’ personalities from pull-request discussions and e-mails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.},
  archive      = {J_TOSEM},
  author       = {Fabio Calefato and Filippo Lanubile},
  doi          = {10.1145/3491039},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {42:1–48},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Using personality detection tools for software engineering research: How far can we go?},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the faults found in REST APIs by automated test
generation. <em>TOSEM</em>, <em>31</em>(3), 41:1–43. (<a
href="https://doi.org/10.1145/3491038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RESTful web services are often used for building a wide variety of enterprise applications. The diversity and increased number of applications using RESTful APIs means that increasing amounts of resources are spent developing and testing these systems. Automation in test data generation provides a useful way of generating test data in a fast and efficient manner. However, automated test generation often results in large test suites that are hard to evaluate and investigate manually. This article proposes a taxonomy of the faults we have found using search-based software testing techniques applied on RESTful APIs. The taxonomy is a first step in understanding, analyzing, and ultimately fixing software faults in web services and enterprise applications. We propose to apply a density-based clustering algorithm to the test cases evolved during the search to allow a better separation between different groups of faults. This is needed to enable engineers to highlight and focus on the most serious faults. Tests were automatically generated for a set of eight case studies, seven open-source and one industrial. The test cases generated during the search are clustered based on the reported last executed line and based on the error messages returned, when such error messages were available. The tests were manually evaluated to determine their root causes and to obtain additional information. The article presents a taxonomy of the faults found based on the manual analysis of 415 faults in the eight case studies and proposes a method to support the classification using clustering of the resulting test cases.},
  archive      = {J_TOSEM},
  author       = {Bogdan Marculescu and Man Zhang and Andrea Arcuri},
  doi          = {10.1145/3491038},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {41:1–43},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the faults found in REST APIs by automated test generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applying bayesian analysis guidelines to empirical software
engineering data: The case of programming languages and code quality.
<em>TOSEM</em>, <em>31</em>(3), 40:1–38. (<a
href="https://doi.org/10.1145/3490953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis is the tool of choice to turn data into information and then information into empirical knowledge. However, the process that goes from data to knowledge is long, uncertain, and riddled with pitfalls. To be valid, it should be supported by detailed, rigorous guidelines that help ferret out issues with the data or model and lead to qualified results that strike a reasonable balance between generality and practical relevance. Such guidelines are being developed by statisticians to support the latest techniques for Bayesian data analysis. In this article, we frame these guidelines in a way that is apt to empirical research in software engineering. To demonstrate the guidelines in practice, we apply them to reanalyze a GitHub dataset about code quality in different programming languages. The dataset’s original analysis [Ray et al. 55 ] and a critical reanalysis [Berger et al. 6 ] have attracted considerable attention—in no small part because they target a topic (the impact of different programming languages) on which strong opinions abound. The goals of our reanalysis are largely orthogonal to this previous work, as we are concerned with demonstrating, on data in an interesting domain, how to build a principled Bayesian data analysis and to showcase its benefits. In the process, we will also shed light on some critical aspects of the analyzed data and of the relationship between programming languages and code quality—such as the impact of project-specific characteristics other than the used programming language. The high-level conclusions of our exercise will be that Bayesian statistical techniques can be applied to analyze software engineering data in a way that is principled, flexible, and leads to convincing results that inform the state-of-the-art while highlighting the boundaries of its validity. The guidelines can support building solid statistical analyses and connecting their results. Thus, they can help buttress continued progress in empirical software engineering research.},
  archive      = {J_TOSEM},
  author       = {Carlo A. Furia and Richard Torkar and Robert Feldt},
  doi          = {10.1145/3490953},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {40:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Applying bayesian analysis guidelines to empirical software engineering data: The case of programming languages and code quality},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stateful serverless computing with crucial. <em>TOSEM</em>,
<em>31</em>(3), 39:1–38. (<a
href="https://doi.org/10.1145/3490386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial , a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k -means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18\%–40\% faster). We also use Crucial to port (part of) a state-of-the-art multi-threaded ML library to serverless. The ported application is up to 30\% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6\% of changes in the code bases of the evaluated applications.},
  archive      = {J_TOSEM},
  author       = {Daniel Barcelona-Pons and Pierre Sutra and Marc Sánchez-Artigas and Gerard París and Pedro García-López},
  doi          = {10.1145/3490386},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {39:1–38},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Stateful serverless computing with crucial},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Opinion mining for software development: A systematic
literature review. <em>TOSEM</em>, <em>31</em>(3), 38:1–41. (<a
href="https://doi.org/10.1145/3490388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opinion mining, sometimes referred to as sentiment analysis, has gained increasing attention in software engineering (SE) studies. SE researchers have applied opinion mining techniques in various contexts, such as identifying developers’ emotions expressed in code comments and extracting users’ critics toward mobile apps. Given the large amount of relevant studies available, it can take considerable time for researchers and developers to figure out which approaches they can adopt in their own studies and what perils these approaches entail. We conducted a systematic literature review involving 185 papers. More specifically, we present (1) well-defined categories of opinion mining-related software development activities, (2) available opinion mining approaches, whether they are evaluated when adopted in other studies, and how their performance is compared, (3) available datasets for performance evaluation and tool customization, and (4) concerns or limitations SE researchers might need to take into account when applying/customizing these opinion mining techniques. The results of our study serve as references to choose suitable opinion mining tools for software development activities and provide critical insights for the further development of opinion mining techniques in the SE domain.},
  archive      = {J_TOSEM},
  author       = {Bin Lin and Nathan Cassee and Alexander Serebrenik and Gabriele Bavota and Nicole Novielli and Michele Lanza},
  doi          = {10.1145/3490388},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {38:1–41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Opinion mining for software development: A systematic literature review},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Verification of distributed systems via sequential
emulation. <em>TOSEM</em>, <em>31</em>(3), 37:1–41. (<a
href="https://doi.org/10.1145/3490387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential emulation is a semantics-based technique to automatically reduce property checking of distributed systems to the analysis of sequential programs. An automated procedure takes as input a formal specification of a distributed system, a property of interest, and the structural operational semantics of the specification language and generates a sequential program whose execution traces emulate the possible evolutions of the considered system. The problem as to whether the property of interest holds for the system can then be expressed either as a reachability or as a termination query on the program. This allows to immediately adapt mature verification techniques developed for general-purpose languages to domain-specific languages, and to effortlessly integrate new techniques as soon as they become available. We test our approach on a selection of concurrent systems originated from different contexts from population protocols to models of flocking behaviour. By combining a comprehensive range of program verification techniques, from traditional symbolic execution to modern inductive-based methods such as property-directed reachability, we are able to draw consistent and correct verification verdicts for the considered systems.},
  archive      = {J_TOSEM},
  author       = {Luca Di Stefano and Rocco De Nicola and Omar Inverso},
  doi          = {10.1145/3490387},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {37:1–41},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Verification of distributed systems via sequential emulation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReCDroid+: Automated end-to-end crash reproduction from bug
reports for android apps. <em>TOSEM</em>, <em>31</em>(3), 36:1–33. (<a
href="https://doi.org/10.1145/3488244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially given that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid+, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid+ uses a combination of natural language processing (NLP) , deep learning, and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid+ on 66 original bug reports from 37 Android apps. The results show that ReCDroid+ successfully reproduced 42 crashes (63.6\% success rate) directly from the textual description of the manually reproduced bug reports. A user study involving 12 participants demonstrates that ReCDroid+ can improve the productivity of developers when resolving crash bug reports.},
  archive      = {J_TOSEM},
  author       = {Yu Zhao and Ting Su and Yang Liu and Wei Zheng and Xiaoxue Wu and Ramakanth Kavuluru and William G. J. Halfond and Tingting Yu},
  doi          = {10.1145/3488244},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {36:1–33},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {ReCDroid+: Automated end-to-end crash reproduction from bug reports for android apps},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context- and fairness-aware in-process crowdworker
recommendation. <em>TOSEM</em>, <em>31</em>(3), 35:1–31. (<a
href="https://doi.org/10.1145/3487571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to improve bug detection efficiency, i.e., detect more bugs with fewer workers. However, there are a couple of limitations in existing work. First, these studies mainly focus on one-time recommendations based on expertise matching at the beginning of a new task. Second, the recommendation results suffer from severe popularity bias, i.e., highly experienced workers are recommended in almost all the tasks, while less experienced workers rarely get recommended. This article argues the need for context- and fairness-aware in-process crowdworker recommendation in order to address these limitations. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. Besides, motivated by the popularity bias in existing crowdworker recommendation approach, this study also aims at alleviating the unfairness in recommendations. Driven by these observations, this article proposes a context- and fairness-aware in-process crowdworker recommendation approach, iRec2.0, to detect more bugs earlier, shorten the non-yielding windows, and alleviate the unfairness in recommendations. It consists of three main components: (1) the modeling of dynamic testing context, (2) the learning-based ranking component, and (3) the multi-objective optimization-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec2.0 in improving the cost-effectiveness of crowdtesting by saving the cost, shortening the testing process, and alleviating the unfairness among workers. In detail, iRec2.0 could shorten the non-yielding window by a median of 50\%–66\% in different application scenarios, and consequently have potential of saving testing cost by a median of 8\%–12\%. Meanwhile, the recommendation frequency of the crowdworker drop from 34\%–60\% to 5\%–26\% under different scenarios, indicating its potential in alleviating the unfairness among crowdworkers.},
  archive      = {J_TOSEM},
  author       = {Junjie Wang and Ye Yang and Song Wang and Jun Hu and Qing Wang},
  doi          = {10.1145/3487571},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {35:1–31},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Context- and fairness-aware in-process crowdworker recommendation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). L2S: A framework for synthesizing the most probable program
under a specification. <em>TOSEM</em>, <em>31</em>(3), 34:1–45. (<a
href="https://doi.org/10.1145/3487570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scenarios, we need to find the most likely program that meets a specification under a local context, where the local context can be an incomplete program, a partial specification, natural language description, and so on. We call such a problem program estimation . In this article, we propose a framework, LingLong Synthesis Framework (L2S) , to address this problem. Compared with existing work, our work is novel in the following aspects. (1) We propose a theory of expansion rules to describe how to decompose a program into choices. (2) We propose an approach based on abstract interpretation to efficiently prune off the program sub-space that does not satisfy the specification. (3) We prove that the probability of a program is the product of the probabilities of choosing expansion rules, regardless of the choosing order. (4) We reduce the program estimation problem to a pathfinding problem, enabling existing pathfinding algorithms to solve this problem. L2S has been applied to program generation and program repair. In this article, we report our instantiation of this framework for synthesizing conditional expressions (L2S-Cond) and repairing conditional statements (L2S-Hanabi). The experiments on L2S-Cond show that each option enabled by L2S, including the expansion rules, the pruning technique, and the use of different pathfinding algorithms, plays a major role in the performance of the approach. The default configuration of L2S-Cond correctly predicts nearly 60\% of the conditional expressions in the top 5 candidates. Moreover, we evaluate L2S-Hanabi on 272 bugs from two real-world Java defects benchmarks, namely Defects4J and Bugs.jar. L2S-Hanabi correctly fixes 32 bugs with a high precision of 84\%. In terms of repairing conditional statement bugs, L2S-Hanabi significantly outperforms all existing approaches in both precision and recall.},
  archive      = {J_TOSEM},
  author       = {Yingfei Xiong and Bo Wang},
  doi          = {10.1145/3487570},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {3},
  pages        = {34:1–45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {L2S: A framework for synthesizing the most probable program under a specification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Software engineering for AI-based systems: A survey.
<em>TOSEM</em>, <em>31</em>(2), 37e:1–59. (<a
href="https://doi.org/10.1145/3487043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
  archive      = {J_TOSEM},
  author       = {Silverio Martínez-Fernández and Justus Bogner and Xavier Franch and Marc Oriol and Julien Siebert and Adam Trendowicz and Anna Maria Vollmer and Stefan Wagner},
  doi          = {10.1145/3487043},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {37e:1–59},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Software engineering for AI-based systems: A survey},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study on blockchain architecture design decisions and
their security attacks and threats. <em>TOSEM</em>, <em>31</em>(2),
36e:1–45. (<a href="https://doi.org/10.1145/3502740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITRE’s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.},
  archive      = {J_TOSEM},
  author       = {Sabreen Ahmadjee and Carlos Mera-Gómez and Rami Bahsoon and Rick Kazman},
  doi          = {10.1145/3502740},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {36e:1–45},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A study on blockchain architecture design decisions and their security attacks and threats},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Buddy stacks: Protecting return addresses with efficient
thread-local storage and runtime re-randomization. <em>TOSEM</em>,
<em>31</em>(2), 35e:1–37. (<a
href="https://doi.org/10.1145/3494516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS) , which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB) , suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks. In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk ), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk . First, Bustk places a parallel shadow stack just below a thread’s call stack (as each other’s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS , to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each other’s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR . This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker. Our evaluation using web servers, Nginx and Apache Httpd , shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox , Python , LLVM , JDK and SPEC CPU2006 , to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).},
  archive      = {J_TOSEM},
  author       = {Changwei Zou and Xudong Wang and Yaoqing Gao and Jingling Xue},
  doi          = {10.1145/3494516},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {35e:1–37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Buddy stacks: Protecting return addresses with efficient thread-local storage and runtime re-randomization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SemMT: A semantic-based testing approach for machine
translation systems. <em>TOSEM</em>, <em>31</em>(2), 34e:1–36. (<a
href="https://doi.org/10.1145/3490488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2\% and 15.4\% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.},
  archive      = {J_TOSEM},
  author       = {Jialun Cao and Meiziniu Li and Yeting Li and Ming Wen and Shing-Chi Cheung and Haiming Chen},
  doi          = {10.1145/3490488},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {34e:1–36},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SemMT: A semantic-based testing approach for machine translation systems},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OSS effort estimation using software features similarity and
developer activity-based metrics. <em>TOSEM</em>, <em>31</em>(2),
33:1–35. (<a href="https://doi.org/10.1145/3485819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development effort estimation (SDEE) generally involves leveraging the information about the effort spent in developing similar software in the past. Most organizations do not have access to sufficient and reliable forms of such data from past projects. As such, the existing SDEE methods suffer from low usage and accuracy. We propose an efficient SDEE method for open source software, which provides accurate and fast effort estimates. The significant contributions of our article are (i) novel SDEE software metrics derived from developer activity information of various software repositories, (ii) an SDEE dataset comprising the SDEE metrics’ values derived from approximately 13,000 GitHub repositories from 150 different software categories, and (iii) an effort estimation tool based on SDEE metrics and a software description similarity model . Our software description similarity model is basically a machine learning model trained using the PVA on the software product descriptions of GitHub repositories. Given the software description of a newly envisioned software, our tool yields an effort estimate for developing it. Our method achieves the highest standardized accuracy score of 87.26\% (with Cliff’s δ = 0.88 at 99.999\% confidence level) and 42.7\% with the automatically transformed linear baseline model. Our software artifacts are available at https://doi.org/10.5281/zenodo.5095723.},
  archive      = {J_TOSEM},
  author       = {Ritu Kapur and Balwinder Sodhi},
  doi          = {10.1145/3485819},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {33:1–35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {OSS effort estimation using software features similarity and developer activity-based metrics},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A systematic literature review on the use of deep learning
in software engineering research. <em>TOSEM</em>, <em>31</em>(2),
32:1–58. (<a href="https://doi.org/10.1145/3485275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning , a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.},
  archive      = {J_TOSEM},
  author       = {Cody Watson and Nathan Cooper and David Nader Palacio and Kevin Moran and Denys Poshyvanyk},
  doi          = {10.1145/3485275},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {32:1–58},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {A systematic literature review on the use of deep learning in software engineering research},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-IDE code generation from natural language: Promise and
challenges. <em>TOSEM</em>, <em>31</em>(2), 29:1–47. (<a
href="https://doi.org/10.1145/3487569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code , especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries , but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, “At the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?” To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.},
  archive      = {J_TOSEM},
  author       = {Frank F. Xu and Bogdan Vasilescu and Graham Neubig},
  doi          = {10.1145/3487569},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  number       = {2},
  pages        = {29:1–47},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {In-IDE code generation from natural language: Promise and challenges},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
