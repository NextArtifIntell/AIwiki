<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---122">KIS - 122</h2>
<ul>
<li><details>
<summary>
(2022). Iterative sliding window aggregation for generating
length-scale-specific fractal features. <em>KIS</em>, <em>64</em>(12),
3463–3489. (<a
href="https://doi.org/10.1007/s10115-022-01754-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of high-resolution geospatial raster data is rapidly increasing, with potentially far-reaching applications in the area of food, energy, and water. The added resolution allows shifting the focus from data science at the level of individual pixels to working with windows of pixels that characterize a region. We propose a sliding-window-based approach that allows extracting derived features on a spectrum of well-defined length scales. The resulting image has the same resolution as the input image, albeit with slightly smaller size, and the fractal dimension measures are consistent with the definition of the conventional global feature. The sliding windows can be large since the dependence of the computational cost on the window size is logarithmic. We demonstrate the success of the approach for geometric examples and for land use data and show that the resulting features can aid in a downstream classification task. Overall, this work fits the broadly recognized need in agricultural data science of transforming raw data into multi-modal representations that capture application-relevant features.},
  archive      = {J_KIS},
  author       = {Denton, Anne M. and Goetze, Jordan and Dusek, Nicholas S.},
  doi          = {10.1007/s10115-022-01754-w},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3463-3489},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Iterative sliding window aggregation for generating length-scale-specific fractal features},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiresolution hierarchical support vector machine for
classification of large datasets. <em>KIS</em>, <em>64</em>(12),
3447–3462. (<a
href="https://doi.org/10.1007/s10115-022-01755-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is a popular supervised learning algorithm based on margin maximization. It has a high training cost and does not scale well to a large number of data points. We propose a multiresolution algorithm MRH-SVM that trains SVM on a hierarchical data aggregation structure, which also serves as a common data input to other learning algorithms. The proposed algorithm learns SVM models using high-level data aggregates and only visits data aggregates at more detailed levels where support vectors reside. In addition to performance improvements, the algorithm has advantages such as the ability to handle data streams and datasets with imbalanced classes. Experimental results show significant performance improvements in comparison with existing SVM algorithms.},
  archive      = {J_KIS},
  author       = {Alwajidi, Safaa and Yang, Li},
  doi          = {10.1007/s10115-022-01755-9},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3447-3462},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multiresolution hierarchical support vector machine for classification of large datasets},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed real-time ETL architecture for unstructured big
data. <em>KIS</em>, <em>64</em>(12), 3419–3445. (<a
href="https://doi.org/10.1007/s10115-022-01757-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time extract transform load (ETL) is the integral part of increasing demand of faster business decisions targeting large number of modern applications. Multi-source unstructured data stream extraction and transformation using disk data in distributed environment are the building blocks of real-time ETL due to volume and velocity of data. Therefore designing an architecture for basic building blocks for real-time ETL remains a major challenge. In this paper, we focus primarily to expedite stream-disk joins during transformation phase of ETL that is considered most expensive operator in stream processing due to frequent disk access. We propose an architecture for real-time ETL to ingest unstructured stream of data from multi-sources, without having to worry about the structure of data sources, and transform them after joining with distributed disk data. We also present a novel data pipeline stream-disk join that uses partition-based input and best-effort in-memory database technique reducing frequent disk access. The proposed architecture addresses the challenges of stream data loss, ignored un-matching streams, disk overhead and real-time processing for distributed environment. The experimental results obtained using stream generator and real-world datasets on local and distributed machines show that proposed architecture yields significantly improved throughput especially for large number of stream tuples with large datasets.},
  archive      = {J_KIS},
  author       = {Mehmood, Erum and Anees, Tayyaba},
  doi          = {10.1007/s10115-022-01757-7},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3419-3445},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Distributed real-time ETL architecture for unstructured big data},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Euler common spatial pattern modulated with cross-frequency
coupling. <em>KIS</em>, <em>64</em>(12), 3401–3418. (<a
href="https://doi.org/10.1007/s10115-022-01750-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electroencephalogram (EEG)-based brain–computer interfaces (BCIs), the method of common spatial pattern (CSP) is formulated as a problem of eigen-decomposition of covariance matrices. Each sample value was mapped into a complex space using Euler representation, and the Euler common spatial pattern (e-CSP) approach was developed. Cross-frequency coupling (CFC) represents the interaction between different frequency bands, which can better control the complex brain network than a single frequency band and provides a new idea for research on EEG signals. In this paper, we apply amplitude–amplitude coupling (AAC) to reformulate the covariance matrices in e-CSP; as a result, the AAC-modulated e-CSP is proposed. More discriminative features are discovered with the proposed approach. The proposed method is validated based on the Cho’s dataset. The experimental results illustrate the discrimination ability of the proposed method.},
  archive      = {J_KIS},
  author       = {Sun, Jing and Wang, Haixian and Jiang, Jiuchuan},
  doi          = {10.1007/s10115-022-01750-0},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3401-3418},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Euler common spatial pattern modulated with cross-frequency coupling},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-aware temporal cascade reconstruction to detect
asymptomatic cases. <em>KIS</em>, <em>64</em>(12), 3373–3399. (<a
href="https://doi.org/10.1007/s10115-022-01748-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of detecting asymptomatic cases in a temporal contact network in which multiple outbreaks have occurred. We show that the key to detecting asymptomatic cases well is taking into account both individual risk and the likelihood of disease-flow along edges. We consider both aspects by formulating the asymptomatic case detection problem as a directed prize-collecting Steiner tree (Directed PCST) problem. We present an approximation-preserving reduction from this problem to the directed Steiner tree problem and obtain scalable algorithms for the Directed PCST problem on instances with more than 1.5M edges obtained from both synthetic and fine-grained hospital data. On synthetic data, we demonstrate that our detection methods significantly outperform various baselines (with a gain of $$3.6 \times $$ ). We apply our method to the infectious disease prediction task by using an additional feature set that captures exposure to detected asymptomatic cases and show that our method outperforms all baselines. We further use our method to detect infection sources (“patient zero”) of outbreaks that outperform baselines. We also demonstrate that the solutions returned by our approach are clinically meaningful by presenting case studies.},
  archive      = {J_KIS},
  author       = {Jang, Hankyu and Pai, Shreyas and Adhikari, Bijaya and Pemmaraju, Sriram V.},
  doi          = {10.1007/s10115-022-01748-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3373-3399},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Risk-aware temporal cascade reconstruction to detect asymptomatic cases},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust multi-label feature selection with shared label
enhancement. <em>KIS</em>, <em>64</em>(12), 3343–3372. (<a
href="https://doi.org/10.1007/s10115-022-01747-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has attracted considerable attention due to the wide application of multi-label learning. However, previous methods do not fully consider the relationship between feature sets and label sets but devote attention to either of them. Furthermore, conventional multi-label learning utilizes logical labels to estimate relevance between feature sets and label sets so that the importance of corresponding labels cannot be well reflected. Additionally, numerous irrelevant and redundant labels degrade the classification performance of models. To this end, we propose a multi-label feature selection method named Robust multi-label Feature Selection with shared Label Enhanced (RLEFS). First, we obtain a robust label enhancement term by reconstructing labels from logical labels to numerical labels and imposing $$l_{2,1}$$ -norm onto the label enhancement term. Second, RLEFS utilizes the robust label enhancement term to share the similar latent semantic structure between feature matrix and label matrix. Third, local structure is considered to ensure the consistency of label information during the feature selection process. Finally, we integrate the above terms into one joint learning framework, and then, a simple but effective optimization method with provable convergence is proposed to solve RLEFS. Experimental results demonstrate the classification superiority of RLEFS in comparison with seven state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Li, Yonghao and Hu, Juncheng and Gao, Wanfu},
  doi          = {10.1007/s10115-022-01747-9},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3343-3372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Robust multi-label feature selection with shared label enhancement},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A learned index for approximate kNN queries in
high-dimensional spaces. <em>KIS</em>, <em>64</em>(12), 3325–3342. (<a
href="https://doi.org/10.1007/s10115-022-01742-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate k-Nearest Neighbor (kNN) search in high-dimensional spaces is a fundamental problem in computer systems and applications. However, traditional indexes for kNN search do not scale gracefully to massive high-dimensional datasets. As the dimension and data size grows, both the time complexity and space complexity would cost a considerable amount. Motivated by the recent research advancements of learned indexes, we present a learned index for approximate kNN search in high-dimensional spaces, named HKC $$^{+}$$ -index. First, a traditional tree-based index is constructed and used for query processing. Then, a deep neural network is trained as the learned index based on incoming queries and the original tree index. Extensive experiments on a variety of real-world high-dimensional datasets demonstrate that HKC $$^{+}$$ -index achieves up to 7 times in running time and 8 times smaller over the original tree index, while preserving the high accuracy performance.},
  archive      = {J_KIS},
  author       = {Li, Lingli and Cai, Jingwen and Xu, Jie},
  doi          = {10.1007/s10115-022-01742-0},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3325-3342},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A learned index for approximate kNN queries in high-dimensional spaces},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study into patient similarity through representation
learning from medical records. <em>KIS</em>, <em>64</em>(12), 3293–3324.
(<a href="https://doi.org/10.1007/s10115-022-01740-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient similarity assessment, which identifies patients similar to a given patient, is a fundamental component of many secondary uses of medical data. The assessment can be performed using electronic medical records (EMRs). Patient similarity measurement requires converting heterogeneous EMRs into comparable formats to calculate distance. This study presents a new data representation method for EMRs that considers the information in clinical narratives. To address the limitations of previous approaches in handling complex parts of EMR data, an unsupervised manner is proposed for building a patient representation, which integrates unstructured and structured data extracted from patients&#39; EMRs. We employed a tree structure to model the extracted data that capture the temporal relations of multiple medical events from EMR. We processed clinical notes to extract medical concepts using Python libraries such as MedspaCy and ScispaCy and mapped entities to the Unified Medical Language System (UMLS). To capture temporal aspects of the extracted events, we developed two new relabeling methods for the non-leaf nodes of the tree. To create an embedding vector for each patient, we traversed the tree to generate sequences that the Doc2vec algorithm would use. The comprehensive evaluation of the proposed method for patient similarity and mortality prediction tasks demonstrated that our proposed model leads to lower mean-squared error (MSE), higher precision, and normalized discounted cumulative gain (NDCG) relative to baselines.},
  archive      = {J_KIS},
  author       = {Memarzadeh, Hoda and Ghadiri, Nasser and Samwald, Matthias and Lotfi Shahreza, Maryam},
  doi          = {10.1007/s10115-022-01740-2},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3293-3324},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A study into patient similarity through representation learning from medical records},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Open dataset discovery using context-enhanced similarity
search. <em>KIS</em>, <em>64</em>(12), 3265–3291. (<a
href="https://doi.org/10.1007/s10115-022-01751-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, open data catalogs enable users to search for datasets with full-text queries in metadata records combined with simple faceted filtering. Using this combination, a user is able to discover a significant number of the datasets relevant to a user’s search intent. However, there still remain relevant datasets that are hard to find because of the enormous sparsity of their metadata (e.g., several keywords). As an alternative, in this paper, we propose an approach to dataset discovery based on similarity search over metadata descriptions enhanced by various semantic contexts. In general, the semantic contexts enrich the dataset metadata in a way that enables the identification of additional relevant datasets to a query that could not be retrieved using just the keyword or full-text search. In experimental evaluation we show that context-enhanced similarity retrieval methods increase the findability of relevant datasets, improving thus the retrieval recall that is critical in dataset discovery scenarios. As a part of the evaluation, we created a catalog-like user interface for dataset discovery and recorded streams of user actions that served us to create the ground truth. For the sake of reproducibility, we published the entire evaluation testbed.},
  archive      = {J_KIS},
  author       = {Bernhauer, David and Nečaský, Martin and Škoda, Petr and Klímek, Jakub and Skopal, Tomáš},
  doi          = {10.1007/s10115-022-01751-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3265-3291},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Open dataset discovery using context-enhanced similarity search},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-paced annotations of crowd workers. <em>KIS</em>,
<em>64</em>(12), 3235–3263. (<a
href="https://doi.org/10.1007/s10115-022-01759-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing can harness human intelligence to handle computer-hard tasks in a relatively economic way. The collected answers from various crowd workers are of different qualities, due to the task difficulty, worker capability, incentives and other factors. To maintain high-quality answers while reducing the cost, various strategies have been developed by modeling tasks, workers, or both. Nevertheless, they typically deem that the capability of workers is static when assigning/completing all the tasks. However, in actual fact, crowd workers can improve their capability by gradually completing easy to hard tasks, alike human beings’ intrinsic self-paced learning ability. In this paper, we study crowdsourcing with self-paced workers, whose capability can be progressively improved as they scrutinize and complete tasks from to easy to hard. We introduce a Self-paced Crowd-worker model (SPCrowder). In SPCrowder, workers firstly do a set of golden tasks with known truths, which serve as feedbacks to assist workers capturing the raw modes of tasks and to stimulate the self-paced learning. This also helps to estimate workers’ quality and tasks’ difficulty. SPCrowder then uses a task difficulty model to dynamically measure the difficulty of tasks and rank them from easy to hard and assign tasks to self-paced workers by maximizing a benefit criterion. By doing so, a normal worker can be capable to handle hard tasks after completing some easier and related tasks. We conducted extensive experiments on semi-simulated and real crowdsourcing datasets, SPCrowder outperforms competitive methods in quality control and budget saving. Crowd workers indeed hold the self-paced learning ability, which boosts the quality and save the budget.},
  archive      = {J_KIS},
  author       = {Kang, Xiangping and Yu, Guoxian and Domeniconi, Carlotta and Wang, Jun and Guo, Wei and Ren, Yazhou and Zhang, Xiayan and Cui, Lizhen},
  doi          = {10.1007/s10115-022-01759-5},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3235-3263},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-paced annotations of crowd workers},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpretable deep learning: Interpretation,
interpretability, trustworthiness, and beyond. <em>KIS</em>,
<em>64</em>(12), 3197–3234. (<a
href="https://doi.org/10.1007/s10115-022-01756-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been well-known for their superb handling of various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal how deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we first introduce and clarify two basic concepts—interpretations and interpretability—that people usually get confused about. To address the research efforts in interpretations, we elaborate the designs of a number of interpretation algorithms, from different perspectives, by proposing a new taxonomy. Then, to understand the interpretation results, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the current works in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and learning from interpretations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.},
  archive      = {J_KIS},
  author       = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
  doi          = {10.1007/s10115-022-01756-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3197-3234},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conversational question answering: A survey. <em>KIS</em>,
<em>64</em>(12), 3151–3195. (<a
href="https://doi.org/10.1007/s10115-022-01744-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on conversational question answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user’s information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
  archive      = {J_KIS},
  author       = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
  doi          = {10.1007/s10115-022-01744-y},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3151-3195},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Conversational question answering: A survey},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved optimization parameters prediction using the
modified mega trend diffusion function for a small dataset problem.
<em>KIS</em>, <em>64</em>(11), 3129–3149. (<a
href="https://doi.org/10.1007/s10115-022-01727-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a modified mega trend diffusion (MTD) function based on the K-means clustering algorithm to generate artificial samples for a training dataset. This would improve the prediction accuracy in a backpropagation neural network (BPNN) algorithm used in small dataset problems. The main contribution of this paper is in solving the attributes redundancy problem in an mega trend diffusion (MTD) function construction when there are two and three overlapped regions in the functions, using the K-means clustering algorithm. When used in predicting the parameters of an optimization algorithm, significant improvements in the prediction errors were observed, compared to the previous MTD method. The improvements were achieved by clustering the membership function (MF) for each attribute from the overlapped regions in the MF triangle. In this work, this algorithm is used to predict the control parameters of the artificial bee colony optimization (ABCO) ( $${N}_{i}$$ and $${L}_{i}$$ ), which was then used in finding the optimal exit door locations of building layouts. For a case study, six samples of multi-room building layouts were considered. Each layout consists of information on the number of rooms ( $${n}_{i}$$ ), room sizes ( $${s}_{i}$$ ) and corridor width ( $${w}_{i}$$ ). The performance of the model was evaluated against the conventional MTD method. The superiority of the proposed method over the conventional MTD was confirmed by the 17.67% and 28.68% improvements in the prediction error for twofold and threefold cross-validations, respectively. It is envisaged that the method can be very useful in improving the prediction error of data samples of various scales and with different sizes of artificial data.},
  archive      = {J_KIS},
  author       = {Khamis, Nurulaqilla and Selamat, Hazlina and Ismail, Fatimah Sham},
  doi          = {10.1007/s10115-022-01727-z},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3129-3149},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved optimization parameters prediction using the modified mega trend diffusion function for a small dataset problem},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge distillation for BERT unsupervised domain
adaptation. <em>KIS</em>, <em>64</em>(11), 3113–3128. (<a
href="https://doi.org/10.1007/s10115-022-01736-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pre-trained language model, BERT, has brought significant performance improvements across a range of natural language processing tasks. Since the model is trained on a large corpus of diverse topics, it shows robust performance for domain shift problems in which data distributions at training (source data) and testing (target data) differ while sharing similarities. Despite its great improvements compared to previous models, it still suffers from performance degradation due to domain shifts. To mitigate such problems, we propose a simple but effective unsupervised domain adaptation method, adversarial adaptation with distillation (AAD), which combines the adversarial discriminative domain adaptation (ADDA) framework with knowledge distillation. We evaluate our approach in the task of cross-domain sentiment classification on 30 domain pairs, advancing the state-of-the-art performance for unsupervised domain adaptation in text sentiment classification.},
  archive      = {J_KIS},
  author       = {Ryu, Minho and Lee, Geonseok and Lee, Kichun},
  doi          = {10.1007/s10115-022-01736-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3113-3128},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge distillation for BERT unsupervised domain adaptation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic modeling and intuitionistic fuzzy set-based approach
for efficient software bug triaging. <em>KIS</em>, <em>64</em>(11),
3081–3111. (<a
href="https://doi.org/10.1007/s10115-022-01735-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern software development involves multiple developers working remotely in a distributed manner around the world. Software bugs are continuously generated for multiple reasons across various modules. It is possible that one software bug can affect multiple modules, and there can be multiple developers associated with it. Furthermore, many software bug reports are unlabeled, vague, and noisy. The triager faces significant challenges in identifying multiple causes of software bugs and finding expert developers for bug fixing. In this paper, the fuzzy set is extended to Intuitionistic Fuzzy Sets (IFS), and a novel bug triaging approach based on Intuitionistic Fuzzy Similarity (IFSim) measures is presented to overcome the aforementioned problems. The topic model is used to discover multiple relationships between developers and software bugs. IFS is used to separate developers based on their degree of membership and non-membership in a particular software category, with a degree of hesitation for some developers. For a new bug, 15 different IFSim measure techniques are investigated to compute the similarity with the existing software bugs. Finally, a fuzzy $$\alpha $$ -cut is applied to find expert developers to repair it. The best results are obtained by considering the number of topics of 15 and 12 taxonomic terms for each topic. Among all the IFSim measure techniques, the similarity techniques proposed by Ye outperform other techniques. Experiments are carried out on available benchmark data sets, and the results are compared to traditional machine learning algorithms and the fuzzy logic-based Bugzie model.},
  archive      = {J_KIS},
  author       = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
  doi          = {10.1007/s10115-022-01735-z},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3081-3111},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Topic modeling and intuitionistic fuzzy set-based approach for efficient software bug triaging},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Which is better? A modularized evaluation for topic
popularity prediction. <em>KIS</em>, <em>64</em>(11), 3043–3080. (<a
href="https://doi.org/10.1007/s10115-022-01733-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of topic popularity prediction is to predict whether a topic on the Internet will become popular. Various elegant models have been proposed for this problem. However, different datasets and evaluation metrics they use lead to low comparability. In this paper, we conduct a comprehensive survey, propose a modularized evaluation scheme for evaluating the models and apply it to existing methods. Our scheme has four modules: categorization; qualitative evaluation on several metrics; quantitative experiment on real world data; and final ranking with risk matrix and MinDis to reflect performances under different scenarios. Furthermore, we analyze the efficiency and contribution of features used in feature-oriented methods. Our work helps users compare models and select appropriate ones for different requirements.},
  archive      = {J_KIS},
  author       = {Luo, Jiacheng and Xu, Wenyi and Gao, Xiaofeng and Chen, Guihai},
  doi          = {10.1007/s10115-022-01733-1},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3043-3080},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Which is better? a modularized evaluation for topic popularity prediction},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MICAR: Nonlinear association rule mining based on maximal
information coefficient. <em>KIS</em>, <em>64</em>(11), 3017–3042. (<a
href="https://doi.org/10.1007/s10115-022-01730-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association rule mining (ARM) is an important research issue in data mining and knowledge discovery. Existing ARM methods cannot discover nonlinear association rules, despite nonlinearity being common and significant in engineering practice. Besides, negative association rules are less researched, although they can effectively reflect widely existing negative associations in practical complex systems. Consequently, we propose MICAR, a nonlinear ARM method based on the maximal information coefficient (MIC). MICAR can extract nonlinear association rules in positive and negative forms from transactional or continuous databases. MICAR is realized in three steps: data preprocessing, candidate itemset mining and association rule generation. MIC is used to identify the type of association rules and find potential nonlinear correlations. MICAR can also control the redundancy in itemsets and association rules by restricting their quantity and forms. Experiments on authentic and simulation datasets show that MICAR can extract high-quality positive and negative association rules more effectively and efficiently than existing methods, especially has the unique ability to extract nonlinear association rules.},
  archive      = {J_KIS},
  author       = {Liu, Maidi and Yang, Zhiwei and Guo, Yong and Jiang, Jiang and Yang, Kewei},
  doi          = {10.1007/s10115-022-01730-4},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3017-3042},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MICAR: Nonlinear association rule mining based on maximal information coefficient},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RDF-gen: Generating RDF triples from big data sources.
<em>KIS</em>, <em>64</em>(11), 2985–3015. (<a
href="https://doi.org/10.1007/s10115-022-01729-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transforming disparate and heterogeneous data sources that provide large volumes of data in high velocity into a common form allows integrated and enriched views on data and thus provides further opportunities to advance the effectiveness and accuracy of data analysis and prediction tasks. This paper presents the RDF-Gen approach for transforming data provided by archival and streaming data sources, provided in various formats, into RDF triples, according to a set of ontological specifications. RDF-Gen introduces a generic mechanism which supports the transformation of data efficiently (i.e., with high throughput and low latency), even in cases where the velocity of data presents high peaks, offering facilities for discovering associations between data from different sources, and supporting transformation of modular data sets. This paper presents a parallel implementation of RDF-Gen, also presenting data transformation workflows that allow variations incorporating RDF-Gen instances, adjusting to the needs of data sources, application areas and performance requirements. RDF-Gen is experimentally evaluated against state of the art, in both archival and streaming settings: Experimental results show RDF-Gen efficiency and highlight key contributions.},
  archive      = {J_KIS},
  author       = {Santipantakis, Georgios M. and Kotis, Konstantinos I. and Glenis, Apostolos and Vouros, George A. and Doulkeridis, Christos and Vlachou, Akrivi},
  doi          = {10.1007/s10115-022-01729-x},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2985-3015},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RDF-gen: Generating RDF triples from big data sources},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of high utility itemsets mining methods based on
intelligent optimization algorithms. <em>KIS</em>, <em>64</em>(11),
2945–2984. (<a
href="https://doi.org/10.1007/s10115-022-01741-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high utility itemsets from massive data is one of the most active research directions in data mining at present. Intelligent optimization algorithms have been applied to the high utility itemsets mining because of their flexibility and intelligence, and have achieved good results. In this paper, high utility itemsets mining strategies based on swarm intelligence optimization algorithms are mainly analyzed and summarized comprehensively, and the strategies based on the evolutionary algorithms and other intelligence optimization algorithms are introduced in detail. The method based on swarm intelligence optimization algorithm is summarized and compared from the aspects of update strategy, pruning strategy, comparison algorithms, dataset, parameter settings, advantages, disadvantages, etc. The methods based on particle swarm optimization are classified in terms of particle update, which are traditional update strategies, sigmoid function-based strategies, greed-based strategies, roulette mechanism-based strategies, and set-based strategies. The experimental comparative analysis of the algorithms is carried out in terms of the operational efficiency of the algorithms and the number of high utility itemsets mined by the algorithms under the conditions of the same dataset. The experimental analysis shows that the strategy based on the swarm intelligence optimization algorithm is optimal, especially the high utility itemsets mining algorithm based on the bionic algorithm, which has a shorter running time and less number of high utility itemsets lost, and the least efficient strategy based on the genetic algorithm, which will lose a large number of itemsets.},
  archive      = {J_KIS},
  author       = {Han, Meng and Gao, Zhihui and Li, Ang and Liu, Shujuan and Mu, Dongliang},
  doi          = {10.1007/s10115-022-01741-1},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2945-2984},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An overview of high utility itemsets mining methods based on intelligent optimization algorithms},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Powered dirichlet–hawkes process: Challenging textual
clustering using a flexible temporal prior. <em>KIS</em>,
<em>64</em>(11), 2921–2944. (<a
href="https://doi.org/10.1007/s10115-022-01731-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The textual content of a document and its publication date are intertwined. For example, the publication of a news article on a topic is influenced by previous publications on similar issues, according to underlying temporal dynamics. However, it can be challenging to retrieve meaningful information when textual information conveys little information or when temporal dynamics are hard to unveil. Furthermore, the textual content of a document is not always linked to its temporal dynamics. We develop a flexible method to create clusters of textual documents according to both their content and publication time, the powered Dirichlet–Hawkes process (PDHP). We show PDHP yields significantly better results than state-of-the-art models when temporal information or textual content is weakly informative. The PDHP also alleviates the hypothesis that textual content and temporal dynamics are always perfectly correlated. PDHP retrieves textual clusters, temporal clusters, or a mixture of both with high accuracy. We demonstrate that PDHP generalizes previous work –the Dirichlet–Hawkes process (DHP) and uniform process (UP). Finally, we illustrate the changes induced by PDHP over DHP and UP with a real-world application using Reddit data. We detail how PDHP recovers bursty dynamics and show that its limit case accounts for daily and weekly publication cycles.},
  archive      = {J_KIS},
  author       = {Poux-Médard, Gaël and Velcin, Julien and Loudcher, Sabine},
  doi          = {10.1007/s10115-022-01731-3},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2921-2944},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Powered Dirichlet–Hawkes process: Challenging textual clustering using a flexible temporal prior},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Growth patterns and models of real-world hypergraphs.
<em>KIS</em>, <em>64</em>(11), 2883–2920. (<a
href="https://doi.org/10.1007/s10115-022-01739-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What kind of macroscopic structural and dynamical patterns can we observe in real-world hypergraphs? What can be underlying local dynamics on individuals, which ultimately lead to the observed patterns, beyond apparently random evolution? Graphs, which provide effective ways to represent pairwise interactions among entities, fail to represent group interactions (e.g., collaborations of three or more researchers, etc.). Regarded as a generalization of graphs, hypergraphs allowing for various sizes of edges prove fruitful in addressing this limitation. However, the increased complexity makes it challenging to understand hypergraphs as thoroughly as graphs. In this work, we closely examine seven structural and dynamical properties of real hypergraphs from six domains. To this end, we define new measures, extend notions of common graph properties to hypergraphs, and assess the significance of observed patterns by comparison with a null model and statistical tests. We also propose HyperFF, a stochastic model for generating realistic hypergraphs. Its merits are threefold: (a) Realistic: it successfully reproduces all seven patterns, in addition to five patterns established in previous studies, (b) Self-contained: unlike previously proposed models, it does not rely on oracles (i.e., unexplainable external information) at all, and it is parameterized by just two scalars, and (c) Emergent: it relies on simple and interpretable mechanisms on individual entities, which do not trivially enforce but surprisingly lead to macroscopic properties. While HyperFF is mathematically intractable, we provide theoretical justifications and mathematical analysis based on its simplified version.},
  archive      = {J_KIS},
  author       = {Ko, Jihoon and Kook, Yunbum and Shin, Kijung},
  doi          = {10.1007/s10115-022-01739-9},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2883-2920},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Growth patterns and models of real-world hypergraphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of approaches to answering complex questions over
knowledge bases. <em>KIS</em>, <em>64</em>(11), 2849–2881. (<a
href="https://doi.org/10.1007/s10115-022-01737-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering (QA) systems retrieve the most relevant answer to a natural language question. Knowledge base question answering (KBQA) systems explore entities and relations from knowledge bases to generate answers. Currently, QA systems achieve better results when answering simple questions, but complex QA systems are receiving great attention nowadays. However, there is a lack of studies that analyzes complex questions inside the KBQA field and how it has been addressed. This work aims to fill this gap, presenting a systematic mapping on the complex knowledge base question answering (C-KBQA). The main contributions of this work are: (i) the use of a systematic method to provide an overview of C-KBQA; (ii) a collection of 54 papers systematically selected from 894 papers; (iii) the identification of the most frequent venues, domains, and knowledge bases used in the literature; (iv) a mapping of methods, datasets, and metrics used in the C-KBQA scenario; (v) future directions and the main gaps in the C-KBQA field. The authors show that the C-KBQA system aims to solve two question types: multi-hop and constraint questions. Also, it was possible to identify three main steps to construct a C-KBQA system and the use of two main approaches in this process. It was also noticed that datasets for C-KBQA are still an open challenge.},
  archive      = {J_KIS},
  author       = {Gomes, Jorão and de Mello, Rômulo Chrispim and Ströele, Victor and de Souza, Jairo Francisco},
  doi          = {10.1007/s10115-022-01737-x},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2849-2881},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A study of approaches to answering complex questions over knowledge bases},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid online–offline learning to rank using simulated
annealing strategy based on dependent click model. <em>KIS</em>,
<em>64</em>(10), 2833–2847. (<a
href="https://doi.org/10.1007/s10115-022-01726-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to rank (LTR) is the process of constructing a model for ranking documents or objects. It is useful for many applications such as Information retrieval (IR) and recommendation systems. This paper introduces a comparison between Offline and Online (LTR) for IR. It also proposes a novel Offline (1 + 1)-Simulated Annealing Strategy (SAS-Rank) and introduces the first Hybrid Online–Offline LTR techniques using SAS-Rank and ES-Rank with Online Dependent Click Model (DCM). SAS-Rank is a combination of Simulated Annealing method and Evolutionary Strategy. From the obtained experimental results, we can conclude that the Offline LTR techniques outperformed the well-known Online Dependent Click Model (DCM) technique. Moreover, the Hybrid Online–Offline SAS-Click outperformed the predictive ranking results on unseen data in most evaluation fitness metrics using LETOR 4 dataset compared to other approaches. On the other hand, Hybrid ES-Click is a competitive approach with SAS-Click in evolving ranking models for training and validation data. Regarding Offline LTR, the SAS-Rank outperformed the well-known ES-Rank which has been compared in previous studies with fourteen machine learning techniques. This research uses the best available Linear LTR approaches existing in the literature which are offline ES-Rank with Online DCM. The linear LTR approach output is a linear ranking model which can be represented as a vector of feature importance weights. This paper demonstrated the results and findings obtained using the LETOR 4 dataset, and Java Archive Package is provided for facilitating reproducible research.},
  archive      = {J_KIS},
  author       = {Ibrahim, Osman Ali Sadek and Younis, Eman M. G.},
  doi          = {10.1007/s10115-022-01726-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2833-2847},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid online–offline learning to rank using simulated annealing strategy based on dependent click model},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic distances for time-series classification: Itakura
versus sakoe-chiba constraints. <em>KIS</em>, <em>64</em>(10),
2797–2832. (<a
href="https://doi.org/10.1007/s10115-022-01725-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of time series data mining, the accuracy of the simple, but very successful nearest neighbor (NN) classifier directly depends on the chosen similarity measure. To improve the efficiency of elastic measures introduced to overcome the shortcomings of Euclidean distance, the Sakoe-Chiba band is usually applied as a constraint. In this paper, we provide a detailed analysis of the influence of the alternative Itakura parallelogram constraint on the accuracy of the NN classifier in combination with four well-known elastic measures, compared to the Sakoe-Chiba constraint and the unconstrained variants of these measures. The findings suggest that, although the Sakoe-Chiba band generally produces better results, for certain types of datasets the Itakura parallelogram represents a better choice.},
  archive      = {J_KIS},
  author       = {Geler, Zoltan and Kurbalija, Vladimir and Ivanović, Mirjana and Radovanović, Miloš},
  doi          = {10.1007/s10115-022-01725-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2797-2832},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Elastic distances for time-series classification: Itakura versus sakoe-chiba constraints},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NEAWalk: Inferring missing social interactions via
topological-temporal embeddings of social groups. <em>KIS</em>,
<em>64</em>(10), 2771–2795. (<a
href="https://doi.org/10.1007/s10115-022-01724-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world network data consisting of social interactions can be incomplete due to deliberately erased or unsuccessful data collection, which cause the misleading of social interaction analysis for many various time-aware applications. Naturally, the link prediction task has drawn much research interest to predict the missing edges in the incomplete social network. However, existing studies of link prediction cannot effectively capture the entangling topological and temporal dynamics already residing in the social network, thus cannot effectively reasoning the missing interactions in dynamic networks. In this paper, we propose the NEAWalk, a novel model to infer the missing social interaction based on topological-temporal features of patterns in the social group. NEAWalk samples the query-relevant walks containing both the historical and evolving information by focusing on the temporal constraint and designs a dual-view anonymization procedure for extracting both topological and temporal features from the collected walks to conduct the inference. Two-track experiments on several well-known network datasets demonstrate that the NEAWalk stably achieves superior performance against several state-of-the-art baseline methods.},
  archive      = {J_KIS},
  author       = {Shen, Yinghan and Jiang, Xuhui and Li, Zijian and Wang, Yuanzhuo and Jin, Xiaolong and Ma, Shengjie and Cheng, Xueqi},
  doi          = {10.1007/s10115-022-01724-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2771-2795},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {NEAWalk: Inferring missing social interactions via topological-temporal embeddings of social groups},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parity-based cumulative fairness-aware boosting.
<em>KIS</em>, <em>64</em>(10), 2737–2770. (<a
href="https://doi.org/10.1007/s10115-022-01723-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven AI systems can lead to discrimination on the basis of protected attributes like gender or race. One cause for this is the encoded societal biases in the training data (e.g., under-representation of females in the tech workforce), which is aggravated in the presence of unbalanced class distributions (e.g., when “hired” is the minority class in a hiring application). State-of-the-art fairness-aware machine learning approaches focus on preserving the overall classification accuracy while mitigating discrimination. In the presence of class-imbalance, such methods may further aggravate the problem of discrimination by denying an already underrepresented group (e.g., females) the fundamental rights of equal social privileges (e.g., equal access to employment). To this end, we propose AdaFair, a fairness-aware boosting ensemble that changes the data distribution at each round, taking into account not only the class errors but also the fairness-related performance of the model defined cumulatively based on the partial ensemble. Except for the in-training boosting of the group discriminated over each round, AdaFair directly tackles imbalance during the post-training phase by optimizing the number of ensemble learners for balanced error performance. AdaFair can facilitate different parity-based fairness notions and mitigate effectively discriminatory outcomes.},
  archive      = {J_KIS},
  author       = {Iosifidis, Vasileios and Roy, Arjun and Ntoutsi, Eirini},
  doi          = {10.1007/s10115-022-01723-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2737-2770},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Parity-based cumulative fairness-aware boosting},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-objective optimization approach to package delivery
by the crowd of occupied taxis. <em>KIS</em>, <em>64</em>(10),
2713–2736. (<a
href="https://doi.org/10.1007/s10115-022-01722-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taxi crowdsourcing has gained great interest from the logistics industry and academe due to its significant economic and environmental impact. However, existing approaches have several limitations and focus solely on single objective optimization problem. In this paper, we propose a three-stage framework, namely MOOP4PD to improve the existing approaches. Firstly, we propose a DesCloser* pruning algorithm with no limitation on taxi capacity and use A* algorithm to further optimize the delivery routes. Then, a novel multi-objective pruning algorithm, named MDesCloser*, is presented to find the non-dominated set, which contains waiting time window MaxWT and taxi capacity MaxC constraints. Finally, we develop a constraint solving approach to obtain the ideal solution (i.e., MaxWT equals 11 and MaxC equals 6). We evaluate the performance using the data set generated by Brinkhoff road network generator in the city of Luoyang, China. Results show that our approach improve the objectives of success rate, average number of participating taxis, average delivery distance and average delivery time. Especially, MDesCloser* have best performance on the success rate with more than 0.88 and minimize the total waiting time of all packages to 14916.6 time slices if failure in delivering and maximize the average transshipping rate of interchange stations.},
  archive      = {J_KIS},
  author       = {Zhou, Zhifeng and Chen, Rong and Gao, Jian and Xing, Hu},
  doi          = {10.1007/s10115-022-01722-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2713-2736},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A multi-objective optimization approach to package delivery by the crowd of occupied taxis},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection in the context of long-term cloud resource
usage planning. <em>KIS</em>, <em>64</em>(10), 2689–2711. (<a
href="https://doi.org/10.1007/s10115-022-01721-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a new approach to automatic long-term cloud resource usage planning with a novel hybrid anomaly detection mechanism. It analyzes existing anomaly detection solutions, possible improvements and the impact on the accuracy of resource usage planning. The proposed anomaly detection solution is an important part of the research, since it allows greater accuracy to be achieved in the long term. The proposed approach dynamically adjusts reservation plans in order to reduce the unnecessary load on resources and prevent the cloud from running out of them. The predictions are based on cloud analysis conducted using machine learning algorithms, which made it possible to reduce costs by about 50%. The solution was evaluated on real-life data from over 1700 virtual machines.},
  archive      = {J_KIS},
  author       = {Nawrocki, Piotr and Sus, Wiktor},
  doi          = {10.1007/s10115-022-01721-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2689-2711},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly detection in the context of long-term cloud resource usage planning},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Missing value estimation of microarray data using sim-GAN.
<em>KIS</em>, <em>64</em>(10), 2661–2687. (<a
href="https://doi.org/10.1007/s10115-022-01718-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray data analysis needs utmost care as it plays a significant role in cancer study. Due to the excessive complexity of the data extraction process, it loses some relevant information (missing values) which leads to a significant irrecoverable disruption from the actual scenario. The imputation of missing values is a crucial preprocessing step in analyzing microarray data. Currently, numerous methodologies have been designed to resolve the problem, but the unsatisfactory outcome is obtained with high missing rates of data. In order to estimate the missing expression to complete the dataset, a novel method has been proposed based on the similarity index and generative adversarial network (Sim-GAN). Firstly, the raw dataset has been divided into two subsets, i.e., the target set (which contains genes with missing expression values) and the candidate set (contains without missing values). In the next step, the similarity index between target genes and candidate genes has been obtained. As microarray data represents several biological factors, three similarity matrices (structural similarity, functional similarity, and semantic similarity) have been derived to find the small subset of candidate genes for each target gene. In structural similarity, a novel approach has been used to reduce the time complexity is O(1) as well as tackle the nonlinearity. Now, the obtained subsets are fed into a generative adversarial network to compute the missing values of the targeted genomes. The experimental outcomes consolidate the claim that the proposed methodology gives a satisfactory performance in terms of meaningful expression values. A detailed comparative study based on several statistical (i.e., NRMSE, AUROC, etc.) and biological (i.e., CPP, BLCI) metrics to confirm that the proposed Sim-GAN outperforms the existing missing value estimation techniques.},
  archive      = {J_KIS},
  author       = {Pati, Soumen Kumar and Gupta, Manan Kumar and Shai, Rinita and Banerjee, Ayan and Ghosh, Arijit},
  doi          = {10.1007/s10115-022-01718-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2661-2687},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Missing value estimation of microarray data using sim-GAN},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for personalized recommendation with conditional
generative adversarial networks. <em>KIS</em>, <em>64</em>(10),
2637–2660. (<a
href="https://doi.org/10.1007/s10115-022-01719-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems suffer from interaction data sparsity in reality. Recently, generative adversarial network-based recommender systems have shown the potential to solve the problem. The negative sampling methods use the generator to collect signals from unlabeled data, while they suffer from sparse rewards in the policy gradient training process. The vector reconstruction methods generate user-related vectors for data augmentation to enhance robustness, but they lead to redundant calculation and ignore information conveyed by items. To alleviate the limitations of these methods, we propose a novel framework termed Personalized Recommendation with Conditional Generative Adversarial Networks to consider both of the user and the item subset as conditions. The sparsity and the dimension of conditional rating vectors can be controlled in our method, which simplifies both the generator’s reconstruction task and the discriminator’s learning task. In addition, the proposed method formulates conditional rating vector generation as a user-item matching problem, which allows a more flexible model selection for the generator. Experiments are conducted on three datasets to evaluate the effectiveness of the proposed framework.},
  archive      = {J_KIS},
  author       = {Wen, Jing and Zhu, Xi-Ran and Wang, Chang-Dong and Tian, Zhihong},
  doi          = {10.1007/s10115-022-01719-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2637-2660},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A framework for personalized recommendation with conditional generative adversarial networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning dataset representation for automatic machine
learning algorithm selection. <em>KIS</em>, <em>64</em>(10), 2599–2635.
(<a href="https://doi.org/10.1007/s10115-022-01716-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The algorithm selection problem is defined as identifying the best-performing machine learning (ML) algorithm for a given combination of dataset, task, and evaluation measure. The human expertise required to evaluate the increasing number of ML algorithms available has resulted in the need to automate the algorithm selection task. Various approaches have emerged to handle the automatic algorithm selection challenge, including meta-learning. Meta-learning is a popular approach that leverages accumulated experience for future learning and typically involves dataset characterization. Existing meta-learning methods often represent a dataset using predefined features and thus cannot be generalized across different ML tasks, or alternatively, learn a dataset’s representation in a supervised manner and therefore are unable to deal with unsupervised tasks. In this study, we propose a novel learning-based task-agnostic method for producing dataset representations. Then, we introduce TRIO, a meta-learning approach, that utilizes the proposed dataset representations to accurately recommend top-performing algorithms for previously unseen datasets. TRIO first learns graphical representations for the datasets, using four tools to learn the latent interactions among dataset instances and then utilizes a graph convolutional neural network technique to extract embedding representations from the graphs obtained. We extensively evaluate the effectiveness of our approach on 337 datasets and 195 ML algorithms, demonstrating that TRIO significantly outperforms state-of-the-art methods for algorithm selection for both supervised (classification and regression) and unsupervised (clustering) tasks.},
  archive      = {J_KIS},
  author       = {Cohen-Shapira, Noy and Rokach, Lior},
  doi          = {10.1007/s10115-022-01716-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2599-2635},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning dataset representation for automatic machine learning algorithm selection},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to hash: A comprehensive survey of deep
learning-based hashing methods. <em>KIS</em>, <em>64</em>(10),
2565–2597. (<a
href="https://doi.org/10.1007/s10115-022-01734-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explosive growth of big data demands efficient and fast algorithms for nearest neighbor search. Deep learning-based hashing methods have proved their efficacy to learn advanced hash functions that suit the desired goal of nearest neighbor search in large image-based data-sets. In this work, we present a comprehensive review of different deep learning-based supervised hashing methods particularly for image data-sets suggested by various researchers till date to generate advanced hash functions. We categorize prior works into a five-tier taxonomy based on: (i) the design of network architecture, (ii) training strategy based on nature of data-set, (iii) the type of loss function, (iv) the similarity measure and, (v) the nature of quantization. Further, different data-sets used in prior works are reported and compared based on various challenges in the characteristics of images that are part of the data-sets. Lastly, different future directions such as incremental hashing, cross-modality hashing and guidelines to improve design of hash functions are discussed. Based on our comparative review, it has been observed that generative adversarial networks-based hashing models outperform other methods. This is due to the fact that they leverage more data in the form of both real world and synthetically generated data. Furthermore, it has been perceived that triplet-loss-based loss functions learn better discriminative representations by pushing similar patterns together and dis-similar patterns away from each other. This study and its observations shall be useful for the researchers and practitioners working in this emerging research field.},
  archive      = {J_KIS},
  author       = {Singh, Avantika and Gupta, Shaifu},
  doi          = {10.1007/s10115-022-01734-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2565-2597},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning to hash: A comprehensive survey of deep learning-based hashing methods},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph relation embedding network for click-through rate
prediction. <em>KIS</em>, <em>64</em>(9), 2543–2564. (<a
href="https://doi.org/10.1007/s10115-022-01714-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep click-through rate (CTR) prediction models utilize a mainstream framework, which consists of the embedding layer and the feature interaction layer. Embeddings rich in semantic information directly benefit the downstream frameworks to mine potential information and achieve better performance. However, the embedding layer is rarely optimized in the CTR field. Although mapped into a low-dimensional embedding space, discrete features are still sparse. To solve this problem, we build graph structures to mine the similar interest of users and the co-occurrence relationship of items from click behavior sequences, and regard them as prior information for embedding optimization. For interpretable graph structures, we further propose graph relation embedding networks (GREENs), which utilize adapted order-wise graph convolution to alleviate the problems of data sparsity and over-smoothing. Moreover, we also propose a graph contrastive regularization module, which further normalizes graph embedding by maintaining certain graph structure information. Extensive experiments have proved that by introducing our embedding optimization methods, significant performance improvement is achieved.},
  archive      = {J_KIS},
  author       = {Wu, Yixuan and Hu, Youpeng and Xiong, Xin and Li, Xunkai and Guo, Ronghui and Deng, Shuiguang},
  doi          = {10.1007/s10115-022-01714-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2543-2564},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph relation embedding network for click-through rate prediction},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-bounded reinforcement learning for revenue
optimization in air cargo: A prescriptive learning approach.
<em>KIS</em>, <em>64</em>(9), 2515–2541. (<a
href="https://doi.org/10.1007/s10115-022-01713-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a prescriptive learning approach for revenue management in air-cargo that combines machine learning prediction with decision making using deep reinforcement learning. This approach, named RL-Cargo, addresses a problem that is unique to the air-cargo business, namely the wide discrepancy between the quantity (weight or volume) that a shipper will book and the actual amount received at departure time by the airline. The discrepancy results in sub-optimal and inefficient behavior by both the shipper and the airline resulting in an overall loss of potential revenue for the airline. In the proposed approach, booking features and extracted disguised missing values are exploited to provide a prediction on the received volume, while a DQN method using uncertainty bounds from the prediction intervals is proposed for decision making. We have validated the benefits of RL-Cargo using a real dataset of 1000 flights to compare classical Dynamic Programming and Deep Reinforcement Learning techniques on offloading costs and revenue generation. Our results suggest that prescriptive learning which combines prediction with decision making provides a principled approach for managing the air cargo revenue ecosystem. Furthermore, the proposed approach can be abstracted to many other application domains where decision making needs to be carried out in face of both data and behavioral uncertainty.},
  archive      = {J_KIS},
  author       = {Rizzo, Stefano Giovanni and Chen, Yixian and Pang, Linsey and Lucas, Ji and Kaoudi, Zoi and Quiane, Jorge and Chawla, Sanjay},
  doi          = {10.1007/s10115-022-01713-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2515-2541},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Uncertainty-bounded reinforcement learning for revenue optimization in air cargo: A prescriptive learning approach},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cooperative treatment of the plethoric answers problem in
RDF. <em>KIS</em>, <em>64</em>(9), 2481–2514. (<a
href="https://doi.org/10.1007/s10115-022-01710-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When querying Knowledge Bases, users are faced with large sets of data, often without knowing their underlying structures. It follows that users may make mistakes when formulating their queries, therefore receiving an unhelpful response. In this paper, we address the plethoric answers problem, the situation where the user query produces significantly more results than the user was expecting. The common approach to solving this problem, i.e. the top-K approach, reduces the query’s result size by applying various criteria to select only some answers. This selection is performed without considering the causes producing plethoric answers, and can therefore miss an underlying issue within the query. We deal with this problem by proposing an approach that identifies the parts of the failing query, called Minimal Failure Inducing Subqueries (MFIS), that cause plethoric answers. As long as the query contains an MFIS, it will fail to reach a sufficiently low amount of answers. Thus, thanks to these MFIS, interactive and automatic approaches can be set up to help the user in reformulating their query. The dual notion of MFIS, called Maximal Succeeding Subqueries (XSS), is also useful. They provide queries with a maximal number of parts of the original query that return non-plethoric answers. Our goal is to compute MFIS and XSS efficiently, so that they may be used to solve the plethoric answers problem. We show that computing this information is an $$\texttt {NP}$$ -hard problem. Thus, a baseline exhaustive search method cannot be used for most queries. We propose two algorithms that leverage properties of queries and data to compute MFIS and XSS efficiently for queries of reasonable size. We show experimentally that our two algorithms clearly outperform a baseline method on generated queries as well as real user-submitted queries.},
  archive      = {J_KIS},
  author       = {Parkin, Louise and Chardin, Brice and Jean, Stéphane and Hadjali, Allel and Baron, Mickael},
  doi          = {10.1007/s10115-022-01710-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2481-2514},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A cooperative treatment of the plethoric answers problem in RDF},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph attention-based collaborative filtering for
user-specific recommender system using knowledge graph and deep neural
networks. <em>KIS</em>, <em>64</em>(9), 2457–2480. (<a
href="https://doi.org/10.1007/s10115-022-01709-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering suffers from the issues of data sparsity and cold start. Due to which recommendation models that only rely on the user–item interaction graph are insufficient to model the latent relationship between complex interaction of users and items. Existing methods utilizing knowledge graphs for recommendation explicitly model the multi-hop neighbors of an entity while ignoring the relation-specific as well as user-specific information. Moreover, a collaborative signal is also crucial to be modeled explicitly besides knowledge graph information. In this work, a novel end-to-end recommendation scenario is presented which jointly learns the collaborative signal and knowledge graph context. The knowledge graph is utilized to provide supplementary information in the recommendation scenario. To have personalized recommendation for each user, user-specific attention mechanism is also utilized. The user and item triple sets are constructed which are then propagated in the knowledge graph to enrich their representation. Extensive experiments are carried out on three benchmark datasets to show the effectiveness of the proposed framework. Empirical results show that the proposed model performs better than the state-of-the-art KG-based recommendation models.},
  archive      = {J_KIS},
  author       = {Elahi, Ehsan and Halim, Zahid},
  doi          = {10.1007/s10115-022-01709-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2457-2480},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph attention-based collaborative filtering for user-specific recommender system using knowledge graph and deep neural networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complex attributed network embedding for medical
complication prediction. <em>KIS</em>, <em>64</em>(9), 2435–2456. (<a
href="https://doi.org/10.1007/s10115-022-01712-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assure the development of effective treatment plans, it is crucial for understanding the complication relationships among diseases. In practice, traditional statistical methods are widely used to find the complications of diseases despite the potential errors introduced by the discrepancies in medical records. Recently, with the advances of network embedding techniques, it is promising to predict medical complications in properly constructed biomedical networks. However, due to the variety and sparsity of disease attributes, it is challenging to measure the similarity between attributes of different disease nodes, which seriously interferes the medical complication prediction task. To deal with this problem, in this paper, we propose a novel data-driven Complex Attributed Network Embedding (CANE) method to learn representation for each disease, which can better solve the variety and sparsity. Specifically, we first estimate the initial low-level representations of disease attributes via a matrix factorization technique and then refine the representations via several well-designed attribute modeling modules. Along this line, we introduce aggregation functions to preserve local structure information in the representations of diseases and apply them for complication prediction task. Finally, comprehensive experiments on real-world biomedical data clearly validate the effectiveness of CANE.},
  archive      = {J_KIS},
  author       = {Zhang, Zhe and Xiong, Hui and Xu, Tong and Qin, Chuan and Zhang, Le and Chen, Enhong},
  doi          = {10.1007/s10115-022-01712-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2435-2456},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Complex attributed network embedding for medical complication prediction},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel approach for optimization of convolution neural
network with hybrid particle swarm and grey wolf algorithm for
classification of indian classical dances. <em>KIS</em>, <em>64</em>(9),
2411–2434. (<a
href="https://doi.org/10.1007/s10115-022-01707-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is the most dominant area to perform the complex challenging tasks such as image classification and recognition. Earlier researchers have been proposed various convolution neural network (CNN) with different architectures to improve the performance accuracy for the classification and recognition of images. However, the fine-tuning of hyper parameters, resulting the optimal network, regularization of parameters is the difficult task. The metaheuristic optimization algorithms are used for solving such kind of problems. In this paper we proffer a fine tune automate CNN with Hybrid Particle Swarm Grey Wolf (HPSGW). This novel algorithm used to discover the optimal parameters of the CNN like batch size, number of hidden layers, number of epochs and size of filters. The proffered optimized architecture is implemented on MNIST, CIFAR are two bench mark datasets and Indian Classical Dance (ICD) for the classification of 8 Indian Classical Dances. The Proffered method improves the model performance accuracy of 97.3% on ICD Dataset, and other benchmark datasets MNIST, CIFAR with an improved accuracy of 99.4% and 91.1%. This auto-tuned network improved the performance by 5.6% for Indian Classical Dance Forms Classification compared to earlier methods and also reduces the computational cost.},
  archive      = {J_KIS},
  author       = {Challapalli, Jhansi Rani and Devarakonda, Nagaraju},
  doi          = {10.1007/s10115-022-01707-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2411-2434},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel approach for optimization of convolution neural network with hybrid particle swarm and grey wolf algorithm for classification of indian classical dances},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy entropy functions based on perceived uncertainty.
<em>KIS</em>, <em>64</em>(9), 2389–2409. (<a
href="https://doi.org/10.1007/s10115-022-01700-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents fuzzy entropy functions based on perceived vagueness. The proposed entropy functions are based on the principle that different agents may perceive a membership grade differently. The perceived uncertainty for a membership grade is determined through a gain function. In this light, new variants of the popular fuzzy entropy functions are developed. Inspired by these variants, a novel fuzzy entropy function is also developed. The proposed functions are extended to the probabilistic fuzzy domain. A case study is included to illustrate applicability of the work.},
  archive      = {J_KIS},
  author       = {Aggarwal, Manish},
  doi          = {10.1007/s10115-022-01700-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2389-2409},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fuzzy entropy functions based on perceived uncertainty},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate posterior inference for bayesian models:
Black-box expectation propagation. <em>KIS</em>, <em>64</em>(9),
2361–2387. (<a
href="https://doi.org/10.1007/s10115-022-01705-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectation propagation (EP) is a widely successful way to approximate the posteriors of complex Bayesian models. However, it suffers from expensive memory and time overheads, since it involves local approximations with locally specific messages. A recent art, namely averaged EP (AEP), upgrades EP by leveraging the average message effect on the posterior distribution, instead of the locally specific ones, so as to simultaneously reduce memory and time costs. In this paper, we extend AEP to a novel black-box expectation propagation (abbr. BBEP) algorithm, which can be directly applied to many Bayesian models without model-specific derivations. We leverage three ideas of black-box learning, leading to three versions of BBEP, referred to as BBEP $$^{{\varvec{m}}}$$ , BBEP $$^{{\varvec{g}}}$$ and BBEP $$^{{\varvec{o}}}$$ with Monte Carlo moment matching, Monte Carlo gradients and objective of AEP, respectively. For variance reduction, the importance sampling is used, and the proposal distribution selection as well as high dimensionality setting is discussed. Furthermore, we develop online versions of BBEP for optimization speedup given large-scale data sets. We empirically compare BBEP against the state-of-the-art black-box baseline algorithms on both synthetic and real-world data sets. Experimental results demonstrate that BBEP outperforms the baseline algorithms and it is even on a par with analytical solutions in some settings.},
  archive      = {J_KIS},
  author       = {Li, Ximing and Li, Changchun and Chi, Jinjin and Ouyang, Jihong},
  doi          = {10.1007/s10115-022-01705-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2361-2387},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Approximate posterior inference for bayesian models: Black-box expectation propagation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GRosSo: Mining statistically robust patterns from a sequence
of datasets. <em>KIS</em>, <em>64</em>(9), 2329–2359. (<a
href="https://doi.org/10.1007/s10115-022-01689-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern mining is a fundamental data mining task with applications in several domains. In this work, we consider the scenario in which we have a sequence of datasets generated by potentially different underlying generative processes, and we study the problem of mining statistically robust patterns, which are patterns whose probabilities of appearing in transactions drawn from such generative processes respect well-defined conditions. Such conditions define the patterns of interest, describing the evolution of their probabilities through the datasets in the sequence, which may, for example, increase, decrease, or stay stable, through the sequence. Due to the stochastic nature of the data, one cannot identify the exact set of the statistically robust patterns by analyzing a sequence of samples, i.e., the datasets, taken from the generative processes, and has to resort to approximations. We then propose gRosSo, an algorithm to find rigorous approximations of the statistically robust patterns that do not contain false positives or false negatives with high probability. We apply our framework to the mining of statistically robust sequential patterns and statistically robust itemsets. Our extensive evaluation on pseudo-artificial and real data shows that gRosSo provides high-quality approximations for the problem of mining statistically robust sequential patterns and statistically robust itemsets.},
  archive      = {J_KIS},
  author       = {Tonon, Andrea and Vandin, Fabio},
  doi          = {10.1007/s10115-022-01689-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2329-2359},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GRosSo: Mining statistically robust patterns from a sequence of datasets},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on review summarization and sentiment
classification. <em>KIS</em>, <em>64</em>(9), 2289–2327. (<a
href="https://doi.org/10.1007/s10115-022-01728-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasingly more people using online services, purchasing products, and reviewing them, it becomes crucial to have a system that can provide a crisp representation of thousands of reviews written by them. This representation must depict the sentiment that a user has toward the product. This notion comes under review summarization (RS). An even more concise and generous representation to a customer would be a label: positive, negative, or neutral, depicting the reviewer’s opinion toward the product/service. This comes under the domain of sentiment classification (SC). There have been several advancements in both RS and review SC techniques through the years. Some very recent techniques have tried to perform these two tasks jointly, and their results have depicted that these tasks can, in fact, mutually benefit one another in improving their performance. This paper presents contemporary and some earlier techniques used in both RS and SC and the more recent techniques where both these tasks are performed jointly. We have also performed experiments on joint models and devised a model with a combination of deep learning, rule-based systems, and evolutionary algorithms.},
  archive      = {J_KIS},
  author       = {Komwad, Nagsen and Tiwari, Paras and Praveen, Banoth and Chowdary, C. Ravindranath},
  doi          = {10.1007/s10115-022-01728-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2289-2327},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey on review summarization and sentiment classification},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DSIM: Dynamic and static interest mining for sequential
recommendation. <em>KIS</em>, <em>64</em>(8), 2267–2288. (<a
href="https://doi.org/10.1007/s10115-022-01715-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation aims to predict the next interaction by mining users’ evolving interest from their historical behaviors. Through comprehensive study, we argue that the evolving interest consists of dynamic interest and static interest, both of which are important for recommendation tasks. However, it is still a challenging task to model the dynamic interest accurately since it may change rapidly. Besides, existing approaches cannot fully exploit users’ static interest. In this paper, we propose a novel dynamic and static interest mining approach called DSIM for sequential recommendation. Specifically, DSIM takes advantage of the time information and learn the dynamic interest precisely with a time-aware neural Hawkes process. Furthermore, a side information-aware self-attention mechanism is designed to leverage side information in a non-invasive way to learn static interest. Finally, DSIM fuses the learned dynamic and static interest adaptively with the gating mechanism and generates the hybrid interest for sequential recommendation. Extensive experiments on three real-world datasets demonstrate that DSIM can effectively mine and model users’ interest, and it outperforms the state-of-the-art baselines in sequential recommendation tasks.},
  archive      = {J_KIS},
  author       = {Yu, Dongjin and Chen, Jianjiang and Wang, Dongjing and Xu, Yueshen and Xiang, Zhengzhe and Deng, Shuiguang},
  doi          = {10.1007/s10115-022-01715-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2267-2288},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DSIM: Dynamic and static interest mining for sequential recommendation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CDARL: A contrastive discriminator-augmented reinforcement
learning framework for sequential recommendations. <em>KIS</em>,
<em>64</em>(8), 2239–2265. (<a
href="https://doi.org/10.1007/s10115-022-01711-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendations play a crucial role in many real-world applications. Due to the sequential nature, reinforcement learning has been employed to iteratively produce recommendations based on an observed stream of user behavior. In this setting, a recommendation agent interacts with the environments (users) by sequentially recommending items (actions) to maximize users’ overall long-term cumulative rewards. However, most reinforcement learning-based recommendation models only focus on extrinsic rewards based on user feedback, leading to sub-optimal policies if user-item interactions are sparse and fail to obtain the dynamic rewards based on the users’ preferences. As a remedy, we propose a dynamic intrinsic reward signal integrated with a contrastive discriminator-augmented reinforcement learning framework. Concretely, our framework contains two modules: (1) a contrastive learning module is employed to learn the representation of item sequences; (2) an intrinsic reward learning function to imitate the user’s internal dynamics. Furthermore, we combine static extrinsic reward and dynamic intrinsic reward to train a sequential recommender system based on double Q-learning. We integrate our framework with five representative sequential recommendation models. Specifically, our framework augments these recommendation models with two output layers: the supervised layer that applies cross-entropy loss to perform ranking and the other for reinforcement learning. Experimental results on two real-world datasets demonstrate that the proposed framework outperforms several sequential recommendation baselines and exploration with intrinsic reward baselines.},
  archive      = {J_KIS},
  author       = {Liu, Zhuang and Ma, Yunpu and Hildebrandt, Marcel and Ouyang, Yuanxin and Xiong, Zhang},
  doi          = {10.1007/s10115-022-01711-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2239-2265},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CDARL: A contrastive discriminator-augmented reinforcement learning framework for sequential recommendations},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive analysis of the diverse aspects inherent to
image data stream classification. <em>KIS</em>, <em>64</em>(8),
2215–2238. (<a
href="https://doi.org/10.1007/s10115-022-01717-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image data stream classification presents several challenges, for example, the evolution of concepts of known classes (concept drift) and the emergence of new classes (open set). Many studies conducted on image data stream classification investigate the classifier, but do not explore other important issues, such as specific evaluation methods for data stream scenarios, evolution of the image feature descriptor and the updating of the decision model, while considering characteristics of real application environments. This article thus aims at making contributions that aid in closing these gaps through the incorporation of an experimental study, which considers a new evaluation method for the classification of image streams, while deliberating on important issues connected to this task. To this end, algorithms from the literature were considered, in order to identify how such algorithms lose performance when evaluated in real-world scenarios. Experiments were carried out exploring the refinement of the feature descriptor, updating the model in the presence of concept drift and open set, in addition to the use of latency and active learning strategies. The results obtained show that the greater the reality considered in the experiments, the greater the degradation of the results.},
  archive      = {J_KIS},
  author       = {de Lima, Mateus C. and Souza, YanStivalettie and Faria, Elaine R. and Barioni, Maria Camila N.},
  doi          = {10.1007/s10115-022-01717-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2215-2238},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comprehensive analysis of the diverse aspects inherent to image data stream classification},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BertHANK: Hierarchical attention networks with enhanced
knowledge and pre-trained model for answer selection. <em>KIS</em>,
<em>64</em>(8), 2189–2213. (<a
href="https://doi.org/10.1007/s10115-022-01703-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Community Question Answering (CQA) becomes increasingly prevalent, because it provides platforms for users to collect information and share knowledge. However, given a question in a CQA system, there are often many different paired answers. It is almost impossible for users to view them item by item and select the most relevant one. Hence, answer selection becomes an important task of CQA. In this paper, we propose a novel solution - BertHANK, which is a hierarchical attention networks with enhanced knowledge and pre-trained model for answer selection. Specifically, in the encoding stage, knowledge enhancement and pre-training model are used for questions and answers, respectively. Further, we adopt multi-attention mechanism, including the cross-attention on question-answer pairs, the inner attention on questions at word level, and the hierarchical inner attention on answers at both word and sentence level, to capture more subtle semantic features. In more details, the cross-attention focuses on capturing interactive information among encoded questions and answers. While the hierarchical inner attention assigns different weights to words in sentences, and sentences in answers, thereby obtaining both global and local information of question-answer pairs. The hierarchical inner attention contributes to select out best-matched answers for specific questions. Finally, we integrate attention-questions and attention-answers to make prediction. The results show that our model achieves state-of-the-art performance on two corpora, SemEval-2015 and SemEval-2017 CQA datasets, outperforming the advanced baselines by a large margin.},
  archive      = {J_KIS},
  author       = {Yang, Haitian and Zhao, Xuan and Wang, Yan and Sun, Degang and Chen, Wei and Huang, Weiqing},
  doi          = {10.1007/s10115-022-01703-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2189-2213},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {BertHANK: Hierarchical attention networks with enhanced knowledge and pre-trained model for answer selection},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theoretical backgrounds of boolean reasoning-based binary
n-clustering. <em>KIS</em>, <em>64</em>(8), 2171–2188. (<a
href="https://doi.org/10.1007/s10115-022-01708-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering is a two-dimensional data analysis technique, where submatrices of a given data matrix are looked for. Its extension into three-dimensional data is called triclustering. In the paper, a new generalized look into n-dimensional binary data n-clustering is presented. The searching is performed in terms of the Boolean reasoning paradigm, where the original case (the data) is coded into the Boolean formula and its prime implicants are equivalent to the solutions of the original issue. The correctness (finding n-clusters containing only 0s or 1s) and maximality (the n-cluster cannot be expanded in any dimension without the correctness requirement violation) of such an approach have strong mathematical foundations. The paper also shows the application of Boolean reasoning-based n-clustering for small three- and four-dimensional artificial data as well as for some biomedical ones.},
  archive      = {J_KIS},
  author       = {Michalak, Marcin},
  doi          = {10.1007/s10115-022-01708-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2171-2188},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Theoretical backgrounds of boolean reasoning-based binary n-clustering},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based PU learning for binary and multiclass
classification without class prior. <em>KIS</em>, <em>64</em>(8),
2141–2169. (<a
href="https://doi.org/10.1007/s10115-022-01702-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we classify graph-structured data only with positive labels? Graph-based positive-unlabeled (PU) learning is to train a binary classifier given only the positive labels when the relationship between examples is given as a graph. The problem is of great importance for various tasks such as detecting malicious accounts in a social network, which are difficult to be modeled by supervised learning when the true negative labels are absent. Previous works for graph-based PU learning assume that the prior distribution of positive nodes is known in advance, which is not true in many real-world cases. In this work, we propose GRAB (Graph-based Risk minimization with iterAtive Belief propagation), a novel end-to-end approach for graph-based PU learning that requires no class prior. GRAB runs marginalization and update steps iteratively. The marginalization step models the given graph as a Markov network and estimates the marginals of latent variables. The update step trains the binary classifier by utilizing the computed marginals in the objective function. We then generalize GRAB to multi-positive unlabeled (MPU) learning, where multiple positive classes exist in a dataset. Extensive experiments on five real-world datasets show that GRAB achieves the state-of-the-art performance, even when the true prior is given only to the competitors.},
  archive      = {J_KIS},
  author       = {Yoo, Jaemin and Kim, Junghun and Yoon, Hoyoung and Kim, Geonsoo and Jang, Changwon and Kang, U},
  doi          = {10.1007/s10115-022-01702-8},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2141-2169},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph-based PU learning for binary and multiclass classification without class prior},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from crowds with decision trees. <em>KIS</em>,
<em>64</em>(8), 2123–2140. (<a
href="https://doi.org/10.1007/s10115-022-01701-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing systems provide an efficient way to collect labeled data by employing non-expert crowd workers. In practice, each instance obtains a multiple noisy label set from different workers. Ground truth inference algorithms are designed to infer the unknown true labels of data from multiple noisy label sets. Since there is substantial variation among different workers, evaluating the qualities of workers is crucial for ground truth inference. This paper proposes a novel algorithm called decision tree-based weighted majority voting (DTWMV). DTWMV directly takes the multiple noisy label set of each instance as its feature vector; that is, each worker is a feature of instances. Then sequential decision trees are built to calculate the weight of each feature (worker). Finally weighted majority voting is used to infer the integrated labels of instances. In DTWMV, evaluating the qualities of workers is converted to calculating the weights of features, which provides a new perspective for solving the ground truth inference problem. Then, a novel feature weight measurement based on decision trees is proposed. Our experimental results show that DTWMV can effectively evaluate the qualities of workers and improve the label quality of data.},
  archive      = {J_KIS},
  author       = {Yang, Wenjun and Li, Chaoqun and Jiang, Liangxiao},
  doi          = {10.1007/s10115-022-01701-9},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2123-2140},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning from crowds with decision trees},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised role learning for graph neural networks.
<em>KIS</em>, <em>64</em>(8), 2091–2121. (<a
href="https://doi.org/10.1007/s10115-022-01694-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present InfoMotif, a new semi-supervised, motif-regularized, learning framework over graphs. We overcome two key limitations of message passing in popular graph neural networks (GNNs): localization (a k-layer GNN cannot utilize features outside the k-hop neighborhood of the labeled training nodes) and over-smoothed (structurally indistinguishable) representations. We formulate attributed structural roles of nodes based on their occurrence in different network motifs, independent of network proximity. Network motifs are higher-order structures indicating connectivity patterns between nodes and are crucial to the organization of complex networks. Two nodes share attributed structural roles if they participate in topologically similar motif instances over covarying sets of attributes. InfoMotif achieves architecture-agnostic regularization of arbitrary GNNs through novel self-supervised learning objectives based on mutual information maximization. Our training curriculum dynamically prioritizes multiple motifs in the learning process without relying on distributional assumptions in the underlying graph or the learning task. We integrate three state-of-the-art GNNs in our framework, to show notable performance gains (3–10% accuracy) across nine diverse real-world datasets spanning homogeneous and heterogeneous networks. Notably, we see stronger gains for nodes with sparse training labels and diverse attributes in local neighborhood structures.},
  archive      = {J_KIS},
  author       = {Sankar, Aravind and Wang, Junting and Krishnan, Adit and Sundaram, Hari},
  doi          = {10.1007/s10115-022-01694-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2091-2121},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised role learning for graph neural networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging transfer learning in reinforcement learning to
tackle competitive influence maximization. <em>KIS</em>, <em>64</em>(8),
2059–2090. (<a
href="https://doi.org/10.1007/s10115-022-01696-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive influence maximization (CIM) is a key problem that seeks highly influential users to maximize the party’s reward than the competitor. Heuristic and game theory-based approaches are proposed to tackle the CIM problem. However, these approaches consider a selection of key influential users at the first round after knowing the competitor’s seed nodes. To overcome the first round seed selection, reinforcement learning (RL)-based models are proposed to tackle the competitive influence maximization allowing parties to select seed nodes in multiple rounds without explicitly knowing the competitor’s decision. Despite the successful application of RL-based models for CIM, the proposed RL-based models take extensive training time to train the model for finding an optimal strategy whenever the networks or settings of the agent change. To address the RL model’s efficiency, we extend transfer learning in reinforcement learning-based methods to reduce the training time and utilize the knowledge gained on a source network to a target network. Our objective is twofold; the first one is the appropriate state representation of the source and target networks to efficiently avail the knowledge gained on a source network to a target network. The second is to find an optimal transfer learning (TL) in the reinforcement learning method, which is more suitable to tackle the competitive influence maximization problem. We validate our proposed TL methods under two different settings of the agent. Experimental results demonstrate that our proposed TL methods achieve better or similar performance compared with the baseline model while reducing significant training time on target networks.},
  archive      = {J_KIS},
  author       = {Ali, Khurshed and Wang, Chih-Yu and Chen, Yi-Shin},
  doi          = {10.1007/s10115-022-01696-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2059-2090},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Leveraging transfer learning in reinforcement learning to tackle competitive influence maximization},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of clique-based overlapping community detection
algorithms. <em>KIS</em>, <em>64</em>(8), 2023–2058. (<a
href="https://doi.org/10.1007/s10115-022-01704-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of communities is one of the prominent characteristics of vast and complex networks like social networks, collaborative networks, and web graphs. In the modern era, new users get added to these complex networks, which results in an expansion of application-generated networks. Extracting relevant information from these large networks has become one of the most prominent research areas. Community detection tries to reduce the application-generated graph into smaller communities in which nodes within the community are similar. Most of the recent proposals are focused on detecting overlapping communities in the network with higher accuracy. An integral issue in graph theory is the enumeration of cliques in a larger graph. As clique is a group of completely connected nodes which shows the explicit communities means these nodes share the same types of information. Clique-based community detection algorithm utilizing the clique property of the graph also identifies the implicit communities, which is not directly shown in the graph. Many overlapping community detection algorithms are proposed by researchers that rely on cliques. The goal of this paper is to offer a comparative analysis of clique-based community detection algorithms. This paper provides a pervasive survey on research works identifying the cliques in a network for detecting overlapping communities. We bring together most of the state-of-the-art clique-based community detection algorithms into a single article with their accessible benchmark data sets. It presents a detailed description of methods based on K-cliques, maximal cliques, and triad percolation methods and addresses these approaches’ challenges. Finally, the comparative analysis of overlapping community detection methodologies is also reported.},
  archive      = {J_KIS},
  author       = {Gupta, Sumit Kumar and Singh, Dhirendra Pratap and Choudhary, Jaytrilok},
  doi          = {10.1007/s10115-022-01704-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2023-2058},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review of clique-based overlapping community detection algorithms},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-RAP: Relevance-aware QoS prediction in web-services and
user contexts. <em>KIS</em>, <em>64</em>(7), 1997–2022. (<a
href="https://doi.org/10.1007/s10115-022-01699-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With quick advancement in web technology, web-services offered on internet are growing quickly, making it challenging for users to choose a web-service fit to their needs. Recommender systems save users the hassle of going through a range of products by product recommendations through analytical techniques on historical data of user experiences of the available items/products. Research efforts provide several methods for web-service recommendation in which QoS-related attributes play primary role such as response-time, throughput, security, privacy and web-service-delivery. Derivable attributes including, user-trustworthiness and web-services reputation in contexts of users and web-services can also affect the QoS prediction. The proposed research focuses on a web-service recommendation model, S-RAP, for QoS prediction based on derivable attributes to predict QoS of a web-service that a user who has not invoked it before would experience. Services-Relevance attribute is proposed in this publication, which emphasizes on employing the historical data and extracting the degree of relevance in the users and web-services context to predict the QoS values for a user. The proposed system produces satisfactorily accurate rating predictions in the experiments evaluated by the Mean Absolute Error and Normalized Mean Absolute Error metrics. The results compared with state-of-the-art models show a relative improvement by 4.0%.},
  archive      = {J_KIS},
  author       = {Muslim, Hafiz Syed Muhammad and Rubab, Saddaf and Khan, Malik M. and Iltaf, Naima and Bashir, Ali Kashif and Javed, Kashif},
  doi          = {10.1007/s10115-022-01699-0},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1997-2022},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {S-RAP: Relevance-aware QoS prediction in web-services and user contexts},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On measuring network robustness for weighted networks.
<em>KIS</em>, <em>64</em>(7), 1967–1996. (<a
href="https://doi.org/10.1007/s10115-022-01670-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network robustness measures how well network structure is strong and healthy when it is under attack, such as vertices joining and leaving. It has been widely used in many applications, such as information diffusion, disease transmission, and network security. However, existing metrics, including node connectivity, edge connectivity, and graph expansion, can be suboptimal for measuring network robustness since they are inefficient to be computed and cannot directly apply to the weighted networks or disconnected networks. In this paper, we define the $${\mathcal {R}}$$ -energy as a new robustness measurement for weighted networks based on the method of spectral analysis. $${\mathcal {R}}$$ -energy can cope with disconnected networks and is efficient to compute with a time complexity of $$O(|V|+|E|)$$ , where V and E are sets of vertices and edges in the network, respectively. Our experiments illustrate the rationality and efficiency of computing $${\mathcal {R}}$$ -energy: (1) Removal of high degree vertices reduces network robustness more than that of random or small degree vertices; (2) it takes as little as 120 s to compute for a network with about 6M vertices and 33M edges. We can further detect events occurring in a dynamic Twitter network with about 130K users and discover interesting weekly tweeting trends by tracking changes to $${\mathcal {R}}$$ -energy.},
  archive      = {J_KIS},
  author       = {Zheng, Jianbing and Gao, Ming and Lim, Ee-Peng and Lo, David and Jin, Cheqing and Zhou, Aoying},
  doi          = {10.1007/s10115-022-01670-z},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1967-1996},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On measuring network robustness for weighted networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapter-based fine-tuning of pre-trained multilingual
language models for code-mixed and code-switched text classification.
<em>KIS</em>, <em>64</em>(7), 1937–1966. (<a
href="https://doi.org/10.1007/s10115-022-01698-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code-mixing and code-switching are frequent features in online conversations. Classification of such text is challenging if one of the languages is low-resourced. Fine-tuning pre-trained multilingual language models is a promising avenue for code-mixed text classification. In this paper, we explore adapter-based fine-tuning of PMLMs for CMCS text classification. We introduce sequential and parallel stacking of adapters, continuous fine-tuning of adapters, and training adapters without freezing the original model as novel techniques with respect to single-task CMCS text classification. We also present a newly annotated dataset for the classification of Sinhala–English code-mixed and code-switched text data, where Sinhala is a low-resourced language. Our dataset of 10000 user comments has been manually annotated for five classification tasks: sentiment analysis, humor detection, hate speech detection, language identification, and aspect identification, thus making it the first publicly available Sinhala–English CMCS dataset with the largest number of task annotation types. In addition to this dataset, we also tested our proposed techniques on Kannada–English and Hindi–English datasets. These experiments confirm that our adapter-based PMLM fine-tuning techniques outperform or are on par with the basic fine-tuning of PMLM models.},
  archive      = {J_KIS},
  author       = {Rathnayake, Himashi and Sumanapala, Janani and Rukshani, Raveesha and Ranathunga, Surangika},
  doi          = {10.1007/s10115-022-01698-1},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1937-1966},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adapter-based fine-tuning of pre-trained multilingual language models for code-mixed and code-switched text classification},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random walk on node cliques for high-quality samples to
estimate large graphs with high accuracies and low costs. <em>KIS</em>,
<em>64</em>(7), 1909–1935. (<a
href="https://doi.org/10.1007/s10115-022-01691-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random-walk-based sampling is an efficient way to extract and analyze the properties of large and complex graphs representing social networks. However, it is almost impractical for existing random-walk-based sampling schemes to reach the desired node distribution because of the indeterministic sampling budget (i.e., the number of samples or sampling steps) required for doing so with large volumes of data in graphs. On the other hand, under a small sampling budget, these methods produce low-quality samples with many repeats and high correlations (i.e., many common attributes), which leads to a large deviation from the desired node distribution and large estimation errors. In this paper, we propose a new random-walk sampling scheme based on node cliques (a subset of cliques), called node-clique random walk, or NCRW, to strike a good balance between the estimation error and the sampling budget, by producing unique samples with low correlations. Meanwhile, both the deviation from the desired node distribution and the estimation errors under the constraint of the sampling budget are reduced both theoretically and experimentally. Thus, the sampling costs which are closely related to the sampling budget are reduced. Our extensive experimental evaluation driven by real-world datasets further confirms that NCRW significantly increases the quality of samples and accuracy of estimations with much lower costs than those of existing random-walk-based sampling schemes especially in estimating the higher-order node attributes.},
  archive      = {J_KIS},
  author       = {Zhang, Lingling and Wang, Fang and Jiang, Hong and Feng, Dan and Xie, Yanwen and Zhang, Zhiwei and Wang, Guoren},
  doi          = {10.1007/s10115-022-01691-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1909-1935},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Random walk on node cliques for high-quality samples to estimate large graphs with high accuracies and low costs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trusting deep learning natural-language models via local and
global explanations. <em>KIS</em>, <em>64</em>(7), 1863–1907. (<a
href="https://doi.org/10.1007/s10115-022-01690-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the high accuracy offered by state-of-the-art deep natural-language models (e.g., LSTM, BERT), their application in real-life settings is still widely limited, as they behave like a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental requirement of future-generation data-driven systems based on deep-learning approaches. Several attempts to fulfill the existing gap between accuracy and interpretability have been made. However, robust and specialized eXplainable Artificial Intelligence solutions, tailored to deep natural-language models, are still missing. We propose a new framework, named T-EBAnO, which provides innovative prediction-local and class-based model-global explanation strategies tailored to deep learning natural-language models. Given a deep NLP model and the textual input data, T-EBAnO provides an objective, human-readable, domain-specific assessment of the reasons behind the automatic decision-making process. Specifically, the framework extracts sets of interpretable features mining the inner knowledge of the model. Then, it quantifies the influence of each feature during the prediction process by exploiting the normalized Perturbation Influence Relation index at the local level and the novel Global Absolute Influence and Global Relative Influence indexes at the global level. The effectiveness and the quality of the local and global explanations obtained with T-EBAnO are proved on an extensive set of experiments addressing different tasks, such as a sentiment-analysis task performed by a fine-tuned BERT model and a toxic-comment classification task performed by an LSTM model. The quality of the explanations proposed by T-EBAnO, and, specifically, the correlation between the influence index and human judgment, has been evaluated by humans in a survey with more than 4000 judgments. To prove the generality of T-EBAnO and its model/task-independent methodology, experiments with other models (ALBERT, ULMFit) on popular public datasets (Ag News and Cola) are also discussed in detail.},
  archive      = {J_KIS},
  author       = {Ventura, Francesco and Greco, Salvatore and Apiletti, Daniele and Cerquitelli, Tania},
  doi          = {10.1007/s10115-022-01690-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1863-1907},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trusting deep learning natural-language models via local and global explanations},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aspect-based sentiment analysis with enhanced
aspect-sensitive word embeddings. <em>KIS</em>, <em>64</em>(7),
1845–1861. (<a
href="https://doi.org/10.1007/s10115-022-01688-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect term sentiment classification (ATSC) aims at identifying sentiment polarities towards some aspect terms described in a text. One of the challenges in the ATSC is that the same word may express different sentiment polarities for distinct aspects. For instance, if the word “high” is used to describe the quality of a product, this word is most likely used to express a positive opinion towards the product. However, if the aspect term is about the price of the product, the same word “high” is quite likely used to represent a negative sentiment polarity. Such aspect-sensitive word features are also useful for the ATSC when the comparative forms are used to express opinions. What sentiment or opinion is expressed largely depends on who we compare with and how we compare. We describe a weakly supervised method to create an aspect-sensitive lexicon for each aspect, which is a relatively accurate representation of the sentiments that are related to that aspect. We also propose a sentiment analysis model enhanced with the learned aspect-sensitive word embeddings, and extensive experiments show that this model achieved state-of-the-art performances on multiple datasets.},
  archive      = {J_KIS},
  author       = {Qi, Yusi and Zheng, Xiaoqing and Huang, Xuanjing},
  doi          = {10.1007/s10115-022-01688-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1845-1861},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Aspect-based sentiment analysis with enhanced aspect-sensitive word embeddings},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CustRE: A rule based system for family relations extraction
from english text. <em>KIS</em>, <em>64</em>(7), 1817–1844. (<a
href="https://doi.org/10.1007/s10115-022-01687-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is an important information extraction task that must be solved in order to transform data into Knowledge Graph (KG), as semantic relations between entities form KG edges of the graph. Although much effort has been devoted to solve this task during the last three decades, but the results achieved are not as good yet. For instance, winner at Text Analysis Conference’s (TAC) Knowledge Base Population (KBP) 2015 slot filling task, the Stanford’s system, achieves F1 score of 60.5% on standard Relation Extraction (RE) dataset (Zhang et al., in: Position-aware attention and supervised data improve slot_lling. In: EMNLP 2017-Conference on Empirical Methods in Natural Language Processing, Proceedings, (2017). https://doi.org/10.18653/v1/d17-1004 ). The RE task therefore needs better solutions. This paper presents our system, CustRE, for better identification and classification of family relations from English text. CustRE is a rule based system, that uses regular expressions for pattern matching to extract family relations explicitly mentioned in text, and uses co-reference and propagation rules to extract family relations implicitly implied in the text. The proposed system, its implementation and the results obtained are presented in this paper. The results show that our approach makes a great improvement over existing methods by achieving F1 scores of 79.7% and 76.6% on TACRED family relations and CustFRE datasets respectively, which are 6.3 and 18.5 points higher than LUKE, the best score reporter on TACRED.},
  archive      = {J_KIS},
  author       = {Mumtaz, Raabia and Qadir, Muhammad Abdul},
  doi          = {10.1007/s10115-022-01687-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1817-1844},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CustRE: A rule based system for family relations extraction from english text},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging the gap between expressivity and efficiency in
stream reasoning: A structural caching approach for IoT streams.
<em>KIS</em>, <em>64</em>(7), 1781–1815. (<a
href="https://doi.org/10.1007/s10115-022-01686-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s data landscape, data streams are well represented. This is mainly due to the rise of data-intensive domains such as the Internet of Things (IoT), Smart Industries, Pervasive Health, and Social Media. To extract meaningful insights from these streams, they should be processed in real time, while solving an integration problem as these streams need to be combined with more static data and their domain knowledge. Ontologies are ideal for modeling this domain knowledge and facilitate the integration of heterogeneous data within data-intensive domains such as the IoT. Expressive reasoning techniques, such as OWL2 DL reasoning, are needed to completely interpret the domain knowledge and for the extraction of meaningful decisions. Expressive reasoning techniques have mainly focused on static data environments, as it tends to become slow with growing datasets. There is thus a mismatch between expressive reasoning and the real-time requirements of data-intensive domains. In this paper, we take a first step towards bridging the gap between expressivity and efficiency while reasoning over high-velocity IoT data streams for the task of event enrichment. We present a structural caching technique that eliminates reoccurring reasoning steps by exploiting the characteristics of most IoT streams, i.e., streams typically produce events that are similar in structure and size. Our caching technique speeds up reasoning time up to thousands of times for fully fledged OWL2 DL reasoners and even tenths and hundreds of times for less expressive OWL2 RL and OWL2 EL reasoners.},
  archive      = {J_KIS},
  author       = {Bonte, Pieter and Turck, Filip De and Ongenae, Femke},
  doi          = {10.1007/s10115-022-01686-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1781-1815},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Bridging the gap between expressivity and efficiency in stream reasoning: A structural caching approach for IoT streams},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural TV program recommendation with heterogeneous
attention. <em>KIS</em>, <em>64</em>(7), 1759–1779. (<a
href="https://doi.org/10.1007/s10115-022-01695-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TV program recommendation is very important to avoid confusing users with large amounts of information. The existing methods are mainly based on collaborative filtering to utilize the interaction between users and items. However, they ignore auxiliary information that contains rich semantic information. In this paper, we propose a neural TV program recommendation with heterogeneous attention, which incorporates the multi-level features of auxiliary information and neural networks based on attention mechanism to obtain accurate program and user representations. In the program encoder module, we learn the different semantic information of labels and titles contained in each program through a neural network with heterogeneous attention to identify multi-hierarchical program information. In the user encoder module, we incorporate auxiliary information and interactions between users and programs. In addition, we utilize a personalized attention mechanism to learn the importance of different programs for each user to reveal user preferences. Specifically, we collect and process user viewing data in the capital of China to provide a real scenario for personalized recommendation. Experiments on real dataset show that our method can effectively improve the effectiveness of TV program recommendations than other existing models.},
  archive      = {J_KIS},
  author       = {Yin, Fulian and Ji, Meiqi and Li, Sitong and Wang, Yanyan},
  doi          = {10.1007/s10115-022-01695-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1759-1779},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Neural TV program recommendation with heterogeneous attention},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical interactive multi-channel graph neural
network for technological knowledge flow forecasting. <em>KIS</em>,
<em>64</em>(7), 1723–1757. (<a
href="https://doi.org/10.1007/s10115-022-01697-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancement can provide new and more cost-effective solutions to challenges in critical areas. Therefore, as one of the important sources for technological progress, technological knowledge flow (TKF) forecasting, i.e., predicting the directional flows of knowledge from one technological field to another, has become a hot issue of widespread concern. However, existing researches either rely on labor-intensive empirical analysis or ignore the intrinsic characteristics inherent in TKF. To this end, we present a data-driven solution in this article, namely a hierarchical interactive multi-channel graph neural network (HIMTKF). Specifically, HIMTKF generates final predictions using two types of vector representations for each technology node (a diffusion vector and an absorption vector), which is realized by four components: high-order interaction module (HOI), co-occurrence module (CO), improved hierarchical delivery module (IHD) and technological knowledge flow tracing module (TFT). For one thing, HOI and CO are designed to represent high-order network relationships and co-occurrence relationships between technologies on the same hierarchy level. For another, IHD is aimed to model the hierarchical relationships between technologies while also taking their personalities into account. Then, TFT is intended for capturing the dynamic feature evolution of technologies with the above relations involved. Additionally, we develop a hybrid loss function and propose a new evaluation metric for better forecasting the unprecedented knowledge flows between technologies. Finally, we conduct extensive experiments on a large dataset of real-world patents. The results validate the effectiveness of our approach and shed light on several intriguing phenomena about technological knowledge flow trends.},
  archive      = {J_KIS},
  author       = {Liu, Huijie and Wu, Han and Zhang, Le and Yu, Runlong and Liu, Ye and Liu, Chunli and Li, Minglei and Liu, Qi and Chen, Enhong},
  doi          = {10.1007/s10115-022-01697-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1723-1757},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hierarchical interactive multi-channel graph neural network for technological knowledge flow forecasting},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagnostic captioning: A survey. <em>KIS</em>,
<em>64</em>(7), 1691–1722. (<a
href="https://doi.org/10.1007/s10115-022-01684-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic captioning (DC) concerns the automatic generation of a diagnostic text from a set of medical images of a patient collected during an examination. DC can assist inexperienced physicians, reducing clinical errors. It can also help experienced physicians produce diagnostic reports faster. Following the advances of deep learning, especially in generic image captioning, DC has recently attracted more attention, leading to several systems and datasets. This article is an extensive overview of DC. It presents relevant datasets, evaluation measures, and up-to-date systems. It also highlights shortcomings that hinder DC’s progress and proposes future directions.},
  archive      = {J_KIS},
  author       = {Pavlopoulos, John and Kougia, Vasiliki and Androutsopoulos, Ion and Papamichail, Dimitris},
  doi          = {10.1007/s10115-022-01684-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1691-1722},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diagnostic captioning: A survey},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on mining and analysis of uncertain graphs.
<em>KIS</em>, <em>64</em>(7), 1653–1689. (<a
href="https://doi.org/10.1007/s10115-022-01681-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An uncertain graph (also known as probabilistic graph) is a generic model to represent many real-world networks from social to biological. In recent times, analysis and mining of uncertain graphs have drawn significant attention from the researchers of the data management community. Several noble problems have been introduced, and efficient methodologies have been developed to solve those problems. Hence, there is a need to summarize the existing results on this topic in a self-organized way. In this paper, we present a comprehensive survey on uncertain graph mining focusing on mainly three aspects: (i) different problems studied, (ii) computational challenges for solving those problems, and (iii) proposed methodologies. Finally, we list out important future research directions.},
  archive      = {J_KIS},
  author       = {Banerjee, Suman},
  doi          = {10.1007/s10115-022-01681-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1653-1689},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey on mining and analysis of uncertain graphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised model for aspect categorization and implicit
aspect extraction. <em>KIS</em>, <em>64</em>(6), 1625–1651. (<a
href="https://doi.org/10.1007/s10115-022-01678-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People’s ability to quickly convey their thoughts, or opinions, on various services or items has improved as Web 2.0 has evolved. This is to look at the public perceptions expressed in the reviews. Aspect-based sentiment analysis (ABSA) deemed to receive a set of texts (e.g., product reviews or online reviews) and identify the opinion-target (aspect) within each review. Contemporary aspect-based sentiment analysis systems, like the aspect categorization, rely predominantly on lexicon-based, or manually labelled seeds that is being incorporated into the topic models. And using either handcrafted rules or pre-labelled clues for performing implicit aspect detection. These constraints are restricted to a particular domain or language which is domain-dependent. In this work, we first propose a novel unsupervised probabilistic model Topic-seeds Latent Dirichlet Allocation (TSLDA) that leverages semantic regularities for the articulation of explicit aspect-categories. Then, based on the articulated categories, a distributed vector is used for the identification of implicit aspects. The experimental results show that our approach outperforms baseline methods for different domain-data with minimal configurations. Specifically, utilizing the RI measure, our proposed TSLDA outperformed multiple clustering and topic models by an average of 0.83% in diverse domain-data, and roughly 0.89% using the Precision metric for implicit aspect detection.},
  archive      = {J_KIS},
  author       = {AL-Janabi, Omar Mustafa and Ahamed Hassain Malim, Nurul Hashimah and Cheah, Yu-N},
  doi          = {10.1007/s10115-022-01678-5},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1625-1651},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unsupervised model for aspect categorization and implicit aspect extraction},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evidence-based credit evaluation ensemble framework for
online retail SMEs. <em>KIS</em>, <em>64</em>(6), 1603–1623. (<a
href="https://doi.org/10.1007/s10115-022-01682-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of standardized financial statements makes it difficult to determine the credit ratings of small and medium-sized enterprises (SMEs). Focusing on this problem, we construct an ensemble framework based on evidence theory. First, we change the sale amount to cash flow lift through a difference table. Then, we analyse consumer comments using the high-frequency lexical sentiment degree. Finally, we combine the two results with an orthogonal sum according to the principle of evidence theory. Based on this framework, we take an online candy company, “Da Bai Tu” in Tmall, as a case to illustrate the application of this framework. Based on experiments with 50 candy SMEs, the degree scores of the framework and Tmall stores are consistent in a one-way ANOVA. The framework effectively combines objective sales records and subjective comments; thus, it can solve the difficulty in credit evaluation for SMEs.},
  archive      = {J_KIS},
  author       = {Han, Lu and Rajasekar, Arcot and Li, Shuting},
  doi          = {10.1007/s10115-022-01682-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1603-1623},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An evidence-based credit evaluation ensemble framework for online retail SMEs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous graph mining algorithm search with best
performance trade-off. <em>KIS</em>, <em>64</em>(6), 1571–1602. (<a
href="https://doi.org/10.1007/s10115-022-01683-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasiveness of graphs today has raised the demand for algorithms to answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers? Myriads of new graph algorithms are proposed every year to answer such questions—each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for practitioners to compare different algorithms and pick the most suitable one for their application. These challenges create a gap in which state-of-the-art techniques developed in academia fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM for message-passing-based graph algorithms. UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to find the best speed-accuracy trade-off in algorithms under a computation budget. On various real-world datasets, AutoGM generates novel graph algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.},
  archive      = {J_KIS},
  author       = {Yoon, Minji and Gervet, Théophile and Hooi, Bryan and Faloutsos, Christos},
  doi          = {10.1007/s10115-022-01683-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1571-1602},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Autonomous graph mining algorithm search with best performance trade-off},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Are the statistical tests the best way to deal with the
biomarker selection problem? <em>KIS</em>, <em>64</em>(6), 1549–1570.
(<a href="https://doi.org/10.1007/s10115-022-01677-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical tests are a powerful set of tools when applied correctly, but unfortunately the extended misuse of them has caused great concern. Among many other applications, they are used in the detection of biomarkers so as to use the resulting p-values as a reference with which the candidate biomarkers are ranked. Although statistical tests can be used to rank, they have not been designed for that use. Moreover, there is no need to compute any p-value to build a ranking of candidate biomarkers. Those two facts raise the question of whether or not alternative methods which are not based on the computation of statistical tests that match or improve their performances can be proposed. In this paper, we propose two alternative methods to statistical tests. In addition, we propose an evaluation framework to assess both statistical tests and alternative methods in terms of both the performance and the reproducibility. The results indicate that there are alternative methods that can match or surpass methods based on statistical tests in terms of the reproducibility when processing real data, while maintaining a similar performance when dealing with synthetic data. The main conclusion is that there is room for the proposal of such alternative methods.},
  archive      = {J_KIS},
  author       = {Urkullu, Ari and Pérez, Aritz and Calvo, Borja},
  doi          = {10.1007/s10115-022-01677-6},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1549-1570},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Are the statistical tests the best way to deal with the biomarker selection problem?},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relational metric learning with high-order neighborhood
interactions for social recommendation. <em>KIS</em>, <em>64</em>(6),
1525–1547. (<a
href="https://doi.org/10.1007/s10115-022-01680-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social information has been widely incorporated into traditional recommendation systems to alleviate the data sparsity and cold-start issues. However, existing social recommend methods typically have two common limitations: (a) they learn a unified representation for each user involved in both item and social domains, which is insufficient for fine-grained user modeling. (b) They ignore the high-order neighborhood information encoded in both user–item interactions and social relations. To overcome these two limitations, this paper proposes a novel social recommend method, SoHRML, based on social relations under the metric learning framework. Specifically, user–item interactions and social relations are modeled as two types of relation vectors, with which each user could be translated to both multiple item-aware and social-aware representations. In addition, to capture the rich information encoded in local neighborhoods, we model the relation vectors by high-order neighborhood interactions (HNI). In each domain, we design a dual layer-wise neighborhood aggregation (LNA) structure that contains dual graph attention networks (GATs) to aggregate the neighborhoods of users or items. Both high-order information encoded in user–item interactions and social relations can be captured by stacking the layer-wise structure. Extensive experimental results on three practical datasets demonstrate the superiority of the proposed model, especially under the cold-start scenarios. The performance gains over the best baseline are 0.51% to 3.31% on two ranking-based metrics.},
  archive      = {J_KIS},
  author       = {Liu, Zhen and Wang, Xiaodong and Ma, Ying and Yang, Xinxin},
  doi          = {10.1007/s10115-022-01680-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1525-1547},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Relational metric learning with high-order neighborhood interactions for social recommendation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph-based solution for writer identification from
handwritten text. <em>KIS</em>, <em>64</em>(6), 1501–1523. (<a
href="https://doi.org/10.1007/s10115-022-01676-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writer identification is an active research problem due to its applications in forensic and historic documents analysis. It is challenging to identify a writer from her handwritten characters&#39; shapes produced via practiced writing style. Different writing shapes, styles, orientations, various sizes of characters, complex structures, inconsistency, and cursive nature of the text make it a tougher undertaking. To solve this problem, we need to explore a structural representation and spatial information of the handwritten characters. For this, a novel graph-based approach is proposed here to spatially map the handwritten text, adapt its structure, size, and explore the relationship that exist between them. First, image processing steps such as binarization, baseline correction, separation of the writing region, and thinning of the strokes to a width of a single pixel are executed. This work presents a novel algorithm for detecting key points (KPs) in a handwritten skeleton image and extracting their two-dimensional pixel coordinates values. The handwriting samples are then transformed into a graph-based representation with KPs representing nodes and the line segments connecting adjacent KPs as the edges. Features are extracted from the graph-based representations of the handwritten text. For classification, ensemble learning approaches are employed. Four benchmark datasets and one custom collected dataset are utilized for experimentations. The proposed solution achieves identification accuracies of 98.26%, 98.84%, 99.67%, 98.51%, and 97.73%, on CERUG-EN, CVL, Firemaker, IAM, and custom datasets, respectively.},
  archive      = {J_KIS},
  author       = {Rahman, Atta Ur and Halim, Zahid},
  doi          = {10.1007/s10115-022-01676-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1501-1523},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A graph-based solution for writer identification from handwritten text},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applications of deep learning for phishing detection: A
systematic literature review. <em>KIS</em>, <em>64</em>(6), 1457–1500.
(<a href="https://doi.org/10.1007/s10115-022-01672-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phishing attacks aim to steal confidential information using sophisticated methods, techniques, and tools such as phishing through content injection, social engineering, online social networks, and mobile applications. To avoid and mitigate the risks of these attacks, several phishing detection approaches were developed, among which deep learning algorithms provided promising results. However, the results and the corresponding lessons learned are fragmented over many different studies and there is a lack of a systematic overview of the use of deep learning algorithms in phishing detection. Hence, we performed a systematic literature review (SLR) to identify, assess, and synthesize the results on deep learning approaches for phishing detection as reported by the selected scientific publications. We address nine research questions and provide an overview of how deep learning algorithms have been used for phishing detection from several aspects. In total, 43 journal articles were selected from electronic databases to derive the answers for the defined research questions. Our SLR study shows that except for one study, all the provided models applied supervised deep learning algorithms. The widely used data sources were URL-related data, third party information on the website, website content-related data, and email. The most used deep learning algorithms were deep neural networks (DNN), convolutional neural networks, and recurrent neural networks/long short-term memory networks. DNN and hybrid deep learning algorithms provided the best performance among other deep learning-based algorithms. 72% of the studies did not apply any feature selection algorithm to build the prediction model. PhishTank was the most used dataset among other datasets. While Keras and Tensorflow were the most preferred deep learning frameworks, 46% of the articles did not mention any framework. This study also highlights several challenges for phishing detection to pave the way for further research.},
  archive      = {J_KIS},
  author       = {Catal, Cagatay and Giray, Görkem and Tekinerdogan, Bedir and Kumar, Sandeep and Shukla, Suyash},
  doi          = {10.1007/s10115-022-01672-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1457-1500},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Applications of deep learning for phishing detection: A systematic literature review},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data pricing in machine learning pipelines. <em>KIS</em>,
<em>64</em>(6), 1417–1455. (<a
href="https://doi.org/10.1007/s10115-022-01679-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data are critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.},
  archive      = {J_KIS},
  author       = {Cong, Zicun and Luo, Xuan and Pei, Jian and Zhu, Feida and Zhang, Yong},
  doi          = {10.1007/s10115-022-01679-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1417-1455},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Data pricing in machine learning pipelines},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How do i update my model? On the resilience of predictive
process monitoring models to change. <em>KIS</em>, <em>64</em>(5),
1385–1416. (<a
href="https://doi.org/10.1007/s10115-022-01666-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing well-investigated Predictive Process Monitoring techniques typically construct a predictive model based on past process executions and then use this model to predict the future of new ongoing cases, without the possibility of updating it with new cases when they complete their execution. This can make Predictive Process Monitoring too rigid to deal with the variability of processes working in real environments that continuously evolve and/or exhibit new variant behaviours over time. As a solution to this problem, we evaluate the use of three different strategies that allow the periodic rediscovery or incremental construction of the predictive model so as to exploit new available data. The evaluation focuses on the performance of the new learned predictive models, in terms of accuracy and time, against the original one, and uses a number of real and synthetic datasets with and without explicit Concept Drift. The results provide an evidence of the potential of incremental learning algorithms for predicting process monitoring in real environments.},
  archive      = {J_KIS},
  author       = {Rizzi, Williams and Di Francescomarino, Chiara and Ghidini, Chiara and Maggi, Fabrizio Maria},
  doi          = {10.1007/s10115-022-01666-9},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1385-1416},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {How do i update my model? on the resilience of predictive process monitoring models to change},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and predicting students’ engagement behaviors using
mixture markov models. <em>KIS</em>, <em>64</em>(5), 1349–1384. (<a
href="https://doi.org/10.1007/s10115-022-01674-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Students’ engagements reflect their level of involvement in an ongoing learning process which can be estimated through their interactions with a computer-based learning or assessment system. A pre-requirement for stimulating student engagement lies in the capability to have an approximate representation model for comprehending students’ varied (dis)engagement behaviors. In this paper, we utilized model-based clustering for this purpose which generates $$K$$ mixture Markov models to group students’ traces containing their (dis)engagement behavioral patterns. To prevent the Expectation–Maximization (EM) algorithm from getting stuck in a local maxima, we also introduced a K-means-based initialization method named as K-EM. We performed an experimental work on two real datasets using the three variants of the EM algorithm: the original EM, emEM, K-EM; and, non-mixture baseline models for both datasets. The proposed K-EM has shown very promising results and achieved significant performance difference in comparison with the other approaches particularly using the Dataset1. Hence, we suggest to perform further experiments using large dataset(s) to validate our method. Additionally, visualization of the resultant clusters through first-order Markov chains reveals very useful insights about (dis)engagement behaviors depicted by the students. We conclude the paper with a discussion on the usefulness of our approach, limitations and potential extensions of this work.},
  archive      = {J_KIS},
  author       = {Maqsood, Rabia and Ceravolo, Paolo and Romero, Cristóbal and Ventura, Sebastián},
  doi          = {10.1007/s10115-022-01674-9},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1349-1384},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Modeling and predicting students’ engagement behaviors using mixture markov models},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge distillation meets recommendation: Collaborative
distillation for top-n recommendation. <em>KIS</em>, <em>64</em>(5),
1323–1348. (<a
href="https://doi.org/10.1007/s10115-022-01667-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) is a successful method for transferring knowledge from one model (i.e., teacher model) to another model (i.e., student model). Despite the success of KD in classification tasks, applying KD to recommender models is challenging because of the sparsity of positive feedback, ambiguity of missing feedback, and ranking problem for top-N recommendation. In this paper, we propose a new KD model for collaborative filtering, namely collaborative distillation (CD). Specifically, (1) we reformulate a loss function to deal with the ambiguity of missing feedback. (2) We exploit probabilistic rank-aware sampling for top-N recommendation. (3) To train the proposed model effectively, we develop two training strategies for the student model, called teacher- and student-guided training methods, adaptively selecting the most beneficial feedback from the teacher model. Furthermore, we extend our model using self-distillation, called born-again CD (BACD). That is, the teacher and student models with the same model capacity are trained by using the proposed distillation method. The experimental results demonstrate that CD outperforms the state-of-the-art method by 2.7–33.2% and 2.7–29.9% in hit rate (HR) and normalized discounted cumulative gain (NDCG), respectively. Moreover, BACD improves the teacher model by 3.5–12.0% and 4.9–13.3% in HR and NDCG, respectively.},
  archive      = {J_KIS},
  author       = {Lee, Jae-woong and Choi, Minjin and Sael, Lee and Shim, Hyunjung and Lee, Jongwuk},
  doi          = {10.1007/s10115-022-01667-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1323-1348},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge distillation meets recommendation: Collaborative distillation for top-N recommendation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Span-based relational graph transformer network for
aspect–opinion pair extraction. <em>KIS</em>, <em>64</em>(5), 1305–1322.
(<a href="https://doi.org/10.1007/s10115-022-01675-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect extraction and opinion extraction are two fundamental subtasks in aspect-based sentiment analysis. Many methods extract aspect terms or opinion terms but ignore the relationships between them. However, such relationships are crucial for downstream tasks, such as sentiment classification and commodity recommendation. Recently, methods have been proposed to extract both terms jointly; however, they fail to extract them as pairs. In this paper, we explore the aspect–opinion pair extraction task that aims to extract aspect and opinion terms in pairs. To carry out this task, we propose a span-based relational graph transformer network that consists of a span generator, a span classifier, and a relation detector. The span generator enumerates all possible spans to generate the candidates for aspect or opinion terms and filters non-aspects or non-opinions terms, while the relation classifier extracts aspect–opinion pairs. We propose a relational graph convolutional network to capture the dependent relationships between aspect and opinion terms. Extensive experiments show that the proposed model achieves the state-of-the-art performance using four benchmark datasets.},
  archive      = {J_KIS},
  author       = {Li, You and Wang, Chaoqiang and Lin, Yuming and Lin, Yongdong and Chang, Liang},
  doi          = {10.1007/s10115-022-01675-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1305-1322},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Span-based relational graph transformer network for aspect–opinion pair extraction},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimedia ontology population through semantic analysis and
hierarchical deep features extraction techniques. <em>KIS</em>,
<em>64</em>(5), 1283–1303. (<a
href="https://doi.org/10.1007/s10115-022-01669-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase of available data in different complex contexts needs automatic tasks to manage and process contents. Semantic Web technologies represent the silver bullet in the digital Internet ecosystem to allow human and machine cooperation in achieving these goals. Specific technologies as ontologies are standard conceptual representations of this view. It aims to transform data into an interoperability format providing a common vocabulary for a given domain and defining, with different levels of formality, the meaning of informative objects and their possible relationships. In this work, we focus our attention on Ontology Population in the multimedia realm. An automatic and multi-modality framework for images ontology population is proposed and implemented. It allows the enrichment of a multimedia ontology with new informative content. Our multi-modality approach combines textual and visual information through natural language processing techniques, and convolutional neural network used the features extraction task. It is based on a hierarchical methodology using images descriptors and semantic ontology levels. The results evaluation shows the effectiveness of our proposed approach.},
  archive      = {J_KIS},
  author       = {Muscetti, Michela and Rinaldi, Antonio M. and Russo, Cristiano and Tommasino, Cristian},
  doi          = {10.1007/s10115-022-01669-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1283-1303},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multimedia ontology population through semantic analysis and hierarchical deep features extraction techniques},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MEBC: Social network immunization via motif-based
edge-betweenness centrality. <em>KIS</em>, <em>64</em>(5), 1263–1281.
(<a href="https://doi.org/10.1007/s10115-022-01671-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunization of social networks has attracted increasing attention over the last decade. Various algorithms have been proposed based on the topological structure of networks, such as the degree and betweenness of nodes. However, most of these studies have only observed the basic topological structure at the level of individual nodes, ignoring higher-order structures captured by network motifs, which may lead to insufficient performance. Besides, immunization based on the connectivity pattern of nodes such as the degree in a social network may cause integrity problems and also interfere in other users’ regular activities because the absence of the hub nodes can greatly impair the connectivity of the network. Thus, we introduce the edge-betweenness as a metric of social network immunization that is much more effective than other traditional measures and reflects the significant role that edges play in reducing the damage and cost of the immunizing process. In this paper, a new network immunization algorithm is proposed by combining higher-order structures and edge-betweenness to select an edge set to be immunized. We conduct extensive experiments on real-world networks to show that the new algorithm can significantly improve the effectiveness of the immunization and reduce the impact of the structure of the network.},
  archive      = {J_KIS},
  author       = {Gao, Kuang and Yuan, Guocai and Yang, Yang and Fan, Ying and Hu, Wenbin},
  doi          = {10.1007/s10115-022-01671-y},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1263-1281},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MEBC: Social network immunization via motif-based edge-betweenness centrality},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context mining and graph queries on giant biomedical
knowledge graphs. <em>KIS</em>, <em>64</em>(5), 1239–1262. (<a
href="https://doi.org/10.1007/s10115-022-01668-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information is widely considered for NLP and knowledge discovery in life sciences since it highly influences the exact meaning of natural language. The scientific challenge is not only to extract such context data, but also to store this data for further query and discovery approaches. Classical approaches use RDF triple stores, which have serious limitations. Here, we propose a multiple step knowledge graph approach using labeled property graphs based on polyglot persistence systems to utilize context data for context mining, graph queries, knowledge discovery and extraction. We introduce the graph-theoretic foundation for a general context concept within semantic networks and show a proof of concept based on biomedical literature and text mining. Our test system contains a knowledge graph derived from the entirety of PubMed and SCAIView data and is enriched with text mining data and domain-specific language data using Biological Expression Language. Here, context is a more general concept than annotations. This dense graph has more than 71M nodes and 850M relationships. We discuss the impact of this novel approach with 27 real-world use cases represented by graph queries. Storing and querying a giant knowledge graph as a labeled property graph is still a technological challenge. Here, we demonstrate how our data model is able to support the understanding and interpretation of biomedical data. We present several real-world use cases that utilize our massive, generated knowledge graph derived from PubMed data and enriched with additional contextual data. Finally, we show a working example in context of biologically relevant information using SCAIView.},
  archive      = {J_KIS},
  author       = {Dörpinghaus, Jens and Stefan, Andreas and Schultz, Bruce and Jacobs, Marc},
  doi          = {10.1007/s10115-022-01668-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1239-1262},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Context mining and graph queries on giant biomedical knowledge graphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-driven multi-camera pedestrian detection.
<em>KIS</em>, <em>64</em>(5), 1211–1237. (<a
href="https://doi.org/10.1007/s10115-022-01673-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current worldwide situation, pedestrian detection has reemerged as a pivotal tool for intelligent video-based systems aiming to solve tasks such as pedestrian tracking, social distancing monitoring or pedestrian mass counting. Pedestrian detection methods, even the top performing ones, are highly sensitive to occlusions among pedestrians, which dramatically degrades their performance in crowded scenarios. The generalization of multi-camera setups permits to better confront occlusions by combining information from different viewpoints. In this paper, we present a multi-camera approach to globally combine pedestrian detections leveraging automatically extracted scene context. Contrarily to the majority of the methods of the state-of-the-art, the proposed approach is scene-agnostic, not requiring a tailored adaptation to the target scenario–e.g., via fine-tuning. This noteworthy attribute does not require ad hoc training with labeled data, expediting the deployment of the proposed method in real-world situations. Context information, obtained via semantic segmentation, is used (1) to automatically generate a common area of interest for the scene and all the cameras, avoiding the usual need of manually defining it, and (2) to obtain detections for each camera by solving a global optimization problem that maximizes coherence of detections both in each 2D image and in the 3D scene. This process yields tightly fitted bounding boxes that circumvent occlusions or miss detections. The experimental results on five publicly available datasets show that the proposed approach outperforms state-of-the-art multi-camera pedestrian detectors, even some specifically trained on the target scenario, signifying the versatility and robustness of the proposed method without requiring ad hoc annotations nor human-guided configuration.},
  archive      = {J_KIS},
  author       = {López-Cifuentes, Alejandro and Escudero-Viñolo, Marcos and Bescós, Jesús and Carballeira, Pablo},
  doi          = {10.1007/s10115-022-01673-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1211-1237},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semantic-driven multi-camera pedestrian detection},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Truth validation with evidence. <em>KIS</em>,
<em>64</em>(5), 1187–1209. (<a
href="https://doi.org/10.1007/s10115-022-01663-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern era, abundant information is easily accessible from various sources, however, only a few of these sources are reliable as they mostly contain unverified contents. We develop a system to validate the truthfulness of a given statement together with underlying evidence. The proposed system provides supporting evidence when the statement is tagged as false. Our work relies on an inference method on a knowledge graph (KG) to identify the truthfulness of statements. In order to extract the evidence of falseness, the proposed algorithm takes into account combined knowledge from KG and ontologies. The system shows very good results as it provides valid and concise evidence. The quality of KG plays a role in the performance of the inference method which explicitly affects the performance of our evidence-extracting algorithm.},
  archive      = {J_KIS},
  author       = {Wongchaisuwat, Papis and Klabjan, Diego},
  doi          = {10.1007/s10115-022-01663-y},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1187-1209},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Truth validation with evidence},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on extraction of causal relations from natural
language text. <em>KIS</em>, <em>64</em>(5), 1161–1186. (<a
href="https://doi.org/10.1007/s10115-022-01665-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
  archive      = {J_KIS},
  author       = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
  doi          = {10.1007/s10115-022-01665-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1161-1186},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey on extraction of causal relations from natural language text},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy modified cuckoo search for biomedical image
segmentation. <em>KIS</em>, <em>64</em>(4), 1121–1160. (<a
href="https://doi.org/10.1007/s10115-022-01659-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new method is proposed for biomedical image segmentation. The proposed method for biomedical image segmentation will be known as fuzzy modified cuckoo search (FMCS). This method falls under the category of unsupervised classification (i.e., clustering). In this work, the concept of a well-known metaheuristic method called cuckoo search is extended, modified, and combined with the modified type 2 fuzzy C-means algorithm, and the name is given accordingly. FMCS method uses a modified cuckoo search to find the optimum cluster centers based on fuzzy membership. The proposed FMCS technique fuses the idea of type 2 fuzzy sets with the MCS strategy, and it is applied in biomedical images segmentation. The proposed approach assists with deciding the clusters without having any affectability on the choice of the underlying centers. The quantity of the control variable for the MCS technique is very sensible contrasted with numerous other metaheuristics approaches. The MCS strategy can come to the global optima even subsequent to stalling out in a neighborhood optimum. The proposed method is applied to different biomedical images and compared with several standard optimization methods like genetic algorithm, particle swarm optimization, cuckoo search, etc. The proposed method does not suffer from the choice of initial cluster centers because it exploits the random behavior of the cuckoo search to initialize the cluster centers. Moreover, FMCS outperforms some of the standard methods in terms of the rate of convergence and other segmentation parameters. The proposed approach blends the type 2 fuzzy system in the modified cuckoo search procedure for efficient biomedical image segmentation. The superiority of the proposed method is verified by both quantitative and qualitative measures.},
  archive      = {J_KIS},
  author       = {Chakraborty, Shouvik and Mali, Kalyani},
  doi          = {10.1007/s10115-022-01659-8},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1121-1160},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fuzzy modified cuckoo search for biomedical image segmentation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chinese adversarial examples generation approach with
multi-strategy based on semantic. <em>KIS</em>, <em>64</em>(4),
1101–1119. (<a
href="https://doi.org/10.1007/s10115-022-01652-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that after adding small perturbations that are imperceptible to humans, deep neural networks (DNNs) with good performance and popular application are likely to produce incorrect results. These processed samples are called adversarial examples. High-quality adversarial examples help to increase the accuracy of estimating the robustness of the network model, thereby reducing the security risks behind the unreal high accuracy of the model. And there are few existing researches on Chinese texts in this field, therefore, this paper proposes a Chinese adversarial examples generation approach with multi-strategy based on semantic called GreedyAttack. Based on the analysis of the characteristics of the Chinese version, the ranking of the influence of each word in the text is obtained according to the calculation formula of the word importance with the weighted part-of-speech. Next, five strategies including synonymous words, similar words of form, similar words of sound, pinyin rewriting, and phrase disassembly are combined to replace the original words, and then, the black box attack on the DNNs models is completed. The method is evaluated by attacking the BERT and ERNIE models on three data sets. The results indicate that the adversarial examples generated by the method can effectively reduce the accuracy of the model.},
  archive      = {J_KIS},
  author       = {Ou, Hongxu and Yu, Long and Tian, Shengwei and Chen, Xin},
  doi          = {10.1007/s10115-022-01652-1},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1101-1119},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chinese adversarial examples generation approach with multi-strategy based on semantic},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preventing active re-identification attacks on social graphs
via sybil subgraph obfuscation. <em>KIS</em>, <em>64</em>(4), 1077–1100.
(<a href="https://doi.org/10.1007/s10115-022-01662-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active re-identification attacks constitute a serious threat to privacy-preserving social graph publication, because of the ability of active adversaries to leverage fake accounts, a.k.a. sybil nodes, to enforce structural patterns that can be used to re-identify their victims on anonymised graphs. Several formal privacy properties have been enunciated with the purpose of characterising the resistance of a graph against active attacks. However, anonymisation methods devised on the basis of these properties have so far been able to address only restricted special cases, where the adversaries are assumed to leverage a very small number of sybil nodes. In this paper, we present a new probabilistic interpretation of active re-identification attacks on social graphs. Unlike the aforementioned privacy properties, which model the protection from active adversaries as the task of making victim nodes indistinguishable in terms of their fingerprints with respect to all potential attackers, our new formulation introduces a more complete view, where the attack is countered by jointly preventing the attacker from retrieving the set of sybil nodes, and from using these sybil nodes for re-identifying the victims. Under the new formulation, we show that k-symmetry, a privacy property introduced in the context of passive attacks, provides a sufficient condition for the protection against active re-identification attacks leveraging an arbitrary number of sybil nodes. Moreover, we show that the algorithm K-Match, originally devised for efficiently enforcing the related notion of k-automorphism, also guarantees k-symmetry. Empirical results on real-life and synthetic graphs demonstrate that our formulation allows, for the first time, to publish anonymised social graphs (with formal privacy guarantees) that effectively resist the strongest active re-identification attack reported in the literature, even when it leverages a large number of sybil nodes.},
  archive      = {J_KIS},
  author       = {Mauw, Sjouke and Ramírez-Cruz, Yunior and Trujillo-Rasua, Rolando},
  doi          = {10.1007/s10115-022-01662-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1077-1100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Preventing active re-identification attacks on social graphs via sybil subgraph obfuscation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical comparison of supervised learning techniques for
missing value imputation. <em>KIS</em>, <em>64</em>(4), 1047–1075. (<a
href="https://doi.org/10.1007/s10115-022-01661-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data mining algorithms cannot handle incomplete datasets where some data samples are missing attribute values. To solve this problem, missing value imputation is usually conducted and commonly based on reasoning from observed data or complete data to provide estimated replacements for missing values. In general, missing imputation methods can be classified into statistical and machine learning methods. The statistical methods are usually based on the mean for continuous attributes or mode for discrete attributes, whereas the machine learning methods are based on supervised learning techniques. However, which machine learning method performs optimally for missing value imputation is unknown. This paper compares five well-known supervised learning techniques, namely k-nearest neighbor, the multilayer perceptron neural network (MLP), the classification and regression tree (CART), naïve Bayes, and the support vector machine, to examine their imputation results for categorical, numerical, and mixed data types. The experimental results demonstrate that CART outperforms the other methods for categorical datasets, whereas the MLP is optimal for numerical and mixed datasets in terms of classification accuracy. However, when computational cost is a factor, CART is superior to the MLP because CART can provide reasonably accurate imputation results and requires the least amount of time to perform missing value imputation. Moreover, CART generates the lowest root-mean-squared error of all methods.},
  archive      = {J_KIS},
  author       = {Tsai, Chih-Fong and Hu, Ya-Han},
  doi          = {10.1007/s10115-022-01661-0},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1047-1075},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Empirical comparison of supervised learning techniques for missing value imputation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omen: Discovering sequential patterns with reliable
prediction delays. <em>KIS</em>, <em>64</em>(4), 1013–1045. (<a
href="https://doi.org/10.1007/s10115-022-01660-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose we are given a discrete-valued time series $$X $$ of observed events and an equally long binary sequence $$Y $$ that indicates whether something of interest happened at that particular point in time. We consider the problem of mining serial episodes, sequential patterns allowing for gaps, from $$X $$ that reliably predict those interesting events. With reliable we mean patterns that not only predict that an interesting event is likely to follow, but in particular that we can also accurately tell how how long until that event will happen. In other words, we are specifically interested in patterns with a highly skewed distribution of delays between pattern occurrences and predicted events. As it is unlikely that a single pattern can explain a complex real-world progress, we are after the smallest, least redundant set of such patterns that together explain the interesting events well. We formally define this problem in terms of the Minimum Description Length principle, by which we identify the best patterns as those that describe the occurrences of interesting events $$Y $$ most succinctly given the data over $$X $$ . As neither discovering the optimal explanation of $$Y $$ given a set of patterns, nor the discovery of optimal pattern set are problems that allow for straightforward optimization, we break the problem in two and propose effective heuristics for both. Through extensive empirical evaluation, we show that both our main method, Omen, and its fast approximation fOmen, work well in practice and both quantitatively and qualitatively beat the state of the art.},
  archive      = {J_KIS},
  author       = {Cüppers, Joscha and Kalofolias, Janis and Vreeken, Jilles},
  doi          = {10.1007/s10115-022-01660-1},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1013-1045},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Omen: Discovering sequential patterns with reliable prediction delays},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuCrowd: Neural sampling network for representation
learning with crowdsourced labels. <em>KIS</em>, <em>64</em>(4),
995–1012. (<a href="https://doi.org/10.1007/s10115-021-01644-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning approaches require a massive amount of discriminative training data, which is unavailable in many scenarios, such as healthcare, smart city, and education. In practice, people refer to crowdsourcing to get annotated labels. However, due to issues like data privacy, budget limitation, shortage of domain-specific annotators, the number of crowdsourced labels is still very limited. Moreover, because of annotators’ diverse expertise, crowdsourced labels are often inconsistent. Thus, directly applying existing supervised representation learning (SRL) algorithms may easily get the overfitting problem and yield suboptimal solutions. In this paper, we propose NeuCrowd, a unified framework for SRL from crowdsourced labels. The proposed framework (1) creates a sufficient number of high-quality n-tuplet training samples by utilizing safety-aware sampling and robust anchor generation; and (2) automatically learns a neural sampling network that adaptively learns to select effective samples for SRL networks. The proposed framework is evaluated on both one synthetic and three real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage reproducible results, we make our code publicly available at https://github.com/tal-ai/NeuCrowd_KAIS2021 .},
  archive      = {J_KIS},
  author       = {Hao, Yang and Ding, Wenbiao and Liu, Zitao},
  doi          = {10.1007/s10115-021-01644-7},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {995-1012},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {NeuCrowd: Neural sampling network for representation learning with crowdsourced labels},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient itinerary recommendation via personalized POI
selection and pruning. <em>KIS</em>, <em>64</em>(4), 963–993. (<a
href="https://doi.org/10.1007/s10115-021-01648-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized itinerary recommendation has garnered wide research interests for their ubiquitous applications. Recommending personalized itineraries is complex because of the large number of points of interest (POI) to consider in order to construct an itinerary based on visitors’ interest and preference, time budget and uncertain queuing time. Previous studies typically aim to plan itineraries that maximize POI popularity, visitors’ interest and minimize queuing time. However, existing solutions may not reflect visitor preferences because when creating itineraries, they prefer to recommend POIs with short prior visiting periods. These recommendations can conflict with real-life scenarios as visitors typically spend less time at POIs that they do not enjoy, thus leading to the inclusion of unsuitable POIs. Moreover, constructing itineraries based on selected POIs is a challenging and time-consuming process. Existing approaches involve searching through a large number of non-optimal, duplicate itineraries that are time-consuming to review and generate. To address these issues, we propose an adaptive Monte Carlo tree search (MCTS)-based reinforcement learning algorithm EffiTourRec using an effective POI selection strategy by giving preference to POIs with long visiting times and short queuing times along with high POI popularity and visitor interest. In addition, to reduce non-optimal and duplicated itineraries generation, we propose an efficient MCTS search pruning technique to explore a smaller, more promising portion of solution space. Experiment results in real theme park datasets show clear advantages of our proposed method over baselines, where our method outperforms the current state-of-the-art by 20.89 to 52.32% in precision, 8.36 to 21.35% in F1-score and 40.00 to 67.64% in execution time.},
  archive      = {J_KIS},
  author       = {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and Zhang, Xiuzhen},
  doi          = {10.1007/s10115-021-01648-3},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {963-993},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient itinerary recommendation via personalized POI selection and pruning},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature-blind fairness in collaborative filtering
recommender systems. <em>KIS</em>, <em>64</em>(4), 943–962. (<a
href="https://doi.org/10.1007/s10115-022-01656-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems were originally proposed for suggesting potentially relevant items to users, with the unique objective of providing accurate suggestions. These recommenders started being adopted in several domains, and were identified as generating biased results that could harm the data items being recommended. The exposure in generated rankings, for instance in a job candidate selection situation, is supposed to be fairly distributed among candidates, regardless of their sensitive attributes (gender, race, nationality, age) for promoting equal opportunities. It can happen, however, that no such sensitive information is available in the data applied for training the recommender, and in this case, there is still space for biases that can lead to unfair treatment, named Feature-Blind unfairness. In this work, we adopt Variational Autoencoders (VAE), considered as the state-of-the-art technique for Collaborative Filtering (CF) recommendations, and we present a framework for addressing fairness when having only access to information about user-item interactions. More specifically, we are interested in Position and Popularity Bias. VAE loss function combines two terms associated with accuracy and quality of representation; we introduce a new term for encouraging fairness, and demonstrate the effect of promoting fair results despite of a tolerable decrease in recommendation quality. In our best scenario, position bias is reduced by 42% despite a reduction of 26% in recall in the top 100 recommendation results, compared to the same situation without any fairness constraints.},
  archive      = {J_KIS},
  author       = {Borges, Rodrigo and Stefanidis, Kostas},
  doi          = {10.1007/s10115-022-01656-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {943-962},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature-blind fairness in collaborative filtering recommender systems},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast spatial autocorrelation. <em>KIS</em>, <em>64</em>(4),
919–941. (<a href="https://doi.org/10.1007/s10115-021-01640-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical or geographic location proves to be an important feature in many data science models, because many diverse natural and social phenomenon have a spatial component. Spatial autocorrelation measures the extent to which locally adjacent observations of the same phenomenon are correlated. Although statistics like Moran’s I and Geary’s C are widely used to measure spatial autocorrelation, they are slow: All popular methods run in $$\Omega (n^2)$$ time, rendering them unusable for large datasets, or long time-courses with moderate numbers of points. We propose a new $$S_A$$ statistic based on the notion that the variance observed when merging pairs of nearby clusters should increase slowly for spatially autocorrelated variables. We give a linear-time algorithm to calculate $$S_A$$ for a variable with an input agglomeration order (available at https://github.com/aamgalan/spatial_autocorrelation ). For a typical dataset of $$n \approx 63,000$$ points, our $$S_A$$ autocorrelation measure can be computed in 1 second, versus 2 hours or more for Moran’s I and Geary’s C. Through simulation studies, we demonstrate that $$S_A$$ identifies spatial correlations in variables generated with spatially-dependent model half an order of magnitude earlier than either Moran’s I or Geary’s C. Finally, we prove several theoretical properties of $$S_A$$ : namely that it behaves as a true correlation statistic and is invariant under addition or multiplication by a constant.},
  archive      = {J_KIS},
  author       = {Amgalan, Anar and Mujica-Parodi, LR and Skiena, Steven S.},
  doi          = {10.1007/s10115-021-01640-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {919-941},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fast spatial autocorrelation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From distributed machine learning to federated learning: A
survey. <em>KIS</em>, <em>64</em>(4), 885–917. (<a
href="https://doi.org/10.1007/s10115-022-01664-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of laws or regulations, the distributed data and computing resources cannot be aggregated or directly shared among different regions or organizations for machine learning tasks. Federated learning emerges as an efficient approach to exploit distributed data and computing resources, so as to collaboratively train machine learning models. At the same time, federated learning obeys the laws and regulations and ensures data security and data privacy. In this paper, we provide a comprehensive survey of existing works for federated learning. First, we propose a functional architecture of federated learning systems and a taxonomy of related techniques. Second, we explain the federated learning systems from four aspects: diverse types of parallelism, aggregation algorithms, data communication, and the security of federated learning systems. Third, we present four widely used federated systems based on the functional architecture. Finally, we summarize the limitations and propose future research directions.},
  archive      = {J_KIS},
  author       = {Liu, Ji and Huang, Jizhou and Zhou, Yang and Li, Xuhong and Ji, Shilei and Xiong, Haoyi and Dou, Dejing},
  doi          = {10.1007/s10115-022-01664-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {885-917},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {From distributed machine learning to federated learning: A survey},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of machine learning for fault
selection in power transmission lines. <em>KIS</em>, <em>64</em>(3),
859–883. (<a href="https://doi.org/10.1007/s10115-022-01657-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning methods have been increasingly used in power engineering to perform various tasks. In this paper, a fault selection procedure in double-circuit transmission lines employing different learning methods is accordingly proposed. In the proposed procedure, the discrete Fourier transform (DFT) is used to pre-process raw data from the transmission line before it is fed into the learning algorithm, which will detect and classify any fault based on a training period. The performance of different machine learning algorithms is then numerically compared through simulations. The comparison indicates that an artificial neural network (ANN) achieves remarkable accuracy of 98.47%. As a drawback, the ANN method cannot provide explainable results and is also not robust against noisy measurements. Subsequently, it is demonstrated that explainable results can be obtained with high accuracy by using rule-based learners such as the recently developed quantitative association rule mining algorithm (QARMA). The QARMA algorithm outperforms other explainable schemes, while attaining an accuracy of 98%. Besides, it was shown that QARMA leads to a very high accuracy of 97% for highly noisy data. The proposed method was also validated using data from an actual transmission line fault. In summary, the proposed two-step procedure using the DFT combined with either deep learning or rule-based algorithms can accurately and successfully perform fault selection tasks but indicating remarkable advantages of the QARMA due to its explainability and robustness against noise. Those aspects are extremely important if machine learning and other data-driven methods are to be employed in critical engineering applications.},
  archive      = {J_KIS},
  author       = {Gutierrez-Rojas, Daniel and Christou, Ioannis T. and Dantas, Daniel and Narayanan, Arun and Nardelli, Pedro H. J. and Yang, Yongheng},
  doi          = {10.1007/s10115-022-01657-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {859-883},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Performance evaluation of machine learning for fault selection in power transmission lines},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NAG: Neural feature aggregation framework for credit card
fraud detection. <em>KIS</em>, <em>64</em>(3), 831–858. (<a
href="https://doi.org/10.1007/s10115-022-01653-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art feature-engineering method for fraud classification of electronic payments uses manually engineered feature aggregates, i.e., descriptive statistics of the transaction history. However, this approach has limitations, primarily that of being dependent on expensive human expert knowledge. There have been attempts to replace manual aggregation through automatic feature extraction approaches. They, however, do not consider the specific structure of the manual aggregates. In this paper, we define the novel Neural Aggregate Generator (NAG), a neural network-based feature extraction module that learns feature aggregates end-to-end on the fraud classification task. In contrast to other automatic feature extraction approaches, the network architecture of the NAG closely mimics the structure of feature aggregates. Furthermore, the NAG extends learnable aggregates over traditional ones through soft feature value matching and relative weighting of the importance of different feature constraints. We provide a proof to show the modeling capabilities of the NAG. We compare the performance of the NAG to the state-of-the-art approaches on a real-world dataset with millions of transactions. More precisely, we show that features generated with the NAG lead to improved results over manual aggregates for fraud classification, thus demonstrating its viability to replace them. Moreover, we compare the NAG to other end-to-end approaches such as the LSTM or a generic CNN. Here we also observe improved results. We perform a robust evaluation of the NAG through a parameter budget study, an analysis of the impact of different sequence lengths and also the predictions across days. Unlike the LSTM or the CNN, our approach also provides further interpretability through the inspection of its parameters.},
  archive      = {J_KIS},
  author       = {Ghosh Dastidar, Kanishka and Jurgovsky, Johannes and Siblini, Wissam and Granitzer, Michael},
  doi          = {10.1007/s10115-022-01653-0},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {831-858},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {NAG: Neural feature aggregation framework for credit card fraud detection},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attributed community search considering community focusing
and latent relationship. <em>KIS</em>, <em>64</em>(3), 799–829. (<a
href="https://doi.org/10.1007/s10115-022-01654-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed community search is to find a subgraph with some specific attributes online in terms of given vertices. It can help us retrieve information on a subgraph rather than the whole graph, thus enable down-stream graph search applications. However, it is difficult for users to specify exact query vertices if they are unfamiliar with the required graph. Most existing community search methods depend on the query vertices strictly and cause the searched community to shift from the truth community. Meanwhile, due to the incompleteness of original graph data, there exist many latent relationships between vertices, which may influence the search results. But most existing methods ignore these latent relationships and usually lead to a result with low F1 scores. Therefore, this research proposes an improved attributed community search method considering community focusing and latent relationships. We first build a structure attribute network embedding model to learn representations for vertices. Based on this model, the latent relationships are discovered and added to the original graph. Then a community shifting correction algorithm is presented to solve community focusing problem and achieve a more desired community. The experimental work on real-world networks confirms that our method can achieve better performance than existing methods.},
  archive      = {J_KIS},
  author       = {Xie, Xiaoqin and Zhang, Jiaming and Wang, Wei and Yang, Wu},
  doi          = {10.1007/s10115-022-01654-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {799-829},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Attributed community search considering community focusing and latent relationship},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective scheduling algorithm for load balancing in fog
environment using CNN and MPSO. <em>KIS</em>, <em>64</em>(3), 773–797.
(<a href="https://doi.org/10.1007/s10115-021-01649-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing (FC) designates a decentralized computing structure placed among the devices that produce data and cloud. Such flexible structure empowers users to place resources to increase performance. However, limited resources and low delay services obstruct the application of new virtualization technologies in the task scheduling and resource management of fog computing. Scheduling and load balancing (LB) in the cloud computing have been widely studied. However, countless efforts in LB have been proposed in the fog architectures. This presents some enticing challenges to solve the problem of how tasks are routed between different physical devices between fog nodes and cloud. Within fog, due to its mass and heterogeneity of devices, the scheduling is very difficult. There are still few studies that have been conducted. LB is a very interesting and important study area in FC as it aims to achieve high resource utilization. There are various challenges in LB such as security and fault tolerance. The main objective of this paper is to introduce an effective dynamic load balancing technique (EDLB) using convolutional neural network and modified particle swarm optimization, which is composed of three main modules, namely: (i) fog resource monitor (FRM), (ii) CNN-based classifier (CBC), and (iii) optimized dynamic scheduler (ODS). The main purpose of EDLB is to achieve LB in FC environment via dynamic real-time scheduling algorithm. This paper studies the FC architecture for Healthcare system applications. The FRM is responsible for monitoring each server resource and save the server&#39;s data into table called fog resources table. The CNN-based classifier (CBC) is responsible for classifying each fog server to suitable or not suitable. The optimized dynamic scheduler (ODS) is responsible for assigning the incoming process to the most appropriate server. Comparing EDLB with other previous LB algorithms, it reduces the response time and achieves high resource utilization. Hence, it is an efficient way to ensure the continuous service. Accordingly, EDLB is simple and efficient in real-time systems in fog computing such as in the case of healthcare system. Although several methods in LB for FC have been introduced, they have many limitations. EDLB overcomes these limitations and achieves high performance in various scenarios. It achieved better makespan, average resource utilization and load balancing level as compared to previously mentioned LB algorithms.},
  archive      = {J_KIS},
  author       = {Talaat, Fatma M. and Ali, Hesham A. and Saraya, Mohamed S. and Saleh, Ahmed I.},
  doi          = {10.1007/s10115-021-01649-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {773-797},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Effective scheduling algorithm for load balancing in fog environment using CNN and MPSO},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hidden challenge of link prediction: Which pairs to check?
<em>KIS</em>, <em>64</em>(3), 743–771. (<a
href="https://doi.org/10.1007/s10115-021-01632-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional setup of link prediction in networks assumes that a test set of node pairs, which is usually balanced, is available over which to predict the presence of links. However, in practice, there is no test set: the ground truth is not known, so the number of possible pairs to predict over is quadratic in the number of nodes in the graph. Moreover, because graphs are sparse, most of these possible pairs will not be links. Thus, link prediction methods, which often rely on proximity-preserving embeddings or heuristic notions of node similarity, face a vast search space, with many pairs that are in close proximity, but that should not be linked. To mitigate this issue, we introduce LinkWaldo, a framework for choosing from this quadratic, massively skewed search space of node pairs, a concise set of candidate pairs that, in addition to being in close proximity, also structurally resemble the observed edges. This allows it to ignore some high-proximity but low-resemblance pairs, and also identify high-resemblance, lower-proximity pairs. Our framework is built on a model that theoretically combines stochastic block models (SBMs) with node proximity models. The block structure of the SBM maps out where in the search space new links are expected to fall, and the proximity identifies the most plausible links within these blocks, using locality sensitive hashing to avoid expensive exhaustive search. LinkWaldo can use any node representation learning or heuristic definition of proximity and can generate candidate pairs for any link prediction method, allowing the representation power of current and future methods to be realized for link prediction in practice. We evaluate LinkWaldo on 13 networks across multiple domains and show that on average it returns candidate sets containing 7–33% more missing and future links than both embedding-based and heuristic baselines’ sets. Our code is available at https://github.com/GemsLab/LinkWaldo .},
  archive      = {J_KIS},
  author       = {Belth, Caleb and Büyükçakır, Alican and Koutra, Danai},
  doi          = {10.1007/s10115-021-01632-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {743-771},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hidden challenge of link prediction: Which pairs to check?},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Postimpact similarity: A similarity measure for effective
grouping of unlabelled text using spectral clustering. <em>KIS</em>,
<em>64</em>(3), 723–742. (<a
href="https://doi.org/10.1007/s10115-022-01658-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of text clustering is to partition a set of text documents into different meaningful groups such that the documents in a particular cluster are more similar to each other than the documents of other clusters according to a similarity or dissimilarity measure. Therefore, the role of similarity measure is crucial for producing good-quality clusters. The content similarity between two documents is generally used to form individual clusters, and it is measured by considering shared terms between the documents. However, the same may not be effective for a reasonably large and high-dimensional corpus. Therefore, a similarity measure is proposed here to improve the performance of text clustering using spectral method. The proposed similarity measure between two documents assigns a score based on their content similarity and their individual similarity with the shared neighbours over the corpus. The effectiveness of the proposed document similarity measure has been tested for clustering of different standard corpora using spectral clustering method. The empirical results using some well-known text collections have shown that the proposed method performs better than the state-of-the-art text clustering techniques in terms of normalized mutual information, f-measure and v-measure.},
  archive      = {J_KIS},
  author       = {Roy, Arnab Kumar and Basu, Tanmay},
  doi          = {10.1007/s10115-022-01658-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {723-742},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Postimpact similarity: A similarity measure for effective grouping of unlabelled text using spectral clustering},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved confusion matrix for fusing multiple k-SVD
classifiers. <em>KIS</em>, <em>64</em>(3), 703–722. (<a
href="https://doi.org/10.1007/s10115-022-01655-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of K-SVD classifiers has been proved to be an effective tool for improving the performance in recognition applications. The rationale of this method follows from the observation that the diverse K-SVD classifiers are weighted by the recognition rates in confusion matrix (CM). Unfortunately, in the case of small samples, the recognition rate is not suitable to quantify the performance of K-SVD classifier, thus reducing the performance obtainable with any combination strategy. In this paper, we propose an improved CM that tries to address this problem, by calculating the joint probability distribution of the difference of K-SVD reconstruction errors, in order to capture the probability of classifying a sample to different patterns. Based on the improved CM and Dempster-Shafer evidence, the proposed method combines the K-SVD classifiers obtained from different feature vectors of different sensed signals. The analysis results of experiments performed on the axle box bearing and rolling ball bearing demonstrated the efficacy and advantages of proposed method.},
  archive      = {J_KIS},
  author       = {Liu, Xiaofeng and Liu, Wan and Huang, Hongsheng and Bo, Lin},
  doi          = {10.1007/s10115-022-01655-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {703-722},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An improved confusion matrix for fusing multiple K-SVD classifiers},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved item-based collaborative filtering using a
modified bhattacharyya coefficient and user–user similarity as weight.
<em>KIS</em>, <em>64</em>(3), 665–701. (<a
href="https://doi.org/10.1007/s10115-021-01651-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Item-based filtering technique is a collaborative filtering algorithm for recommendations. Correlation-based similarity measures such as cosine similarity, Pearson correlation, and its variants have inherent limitations on sparse datasets because items may not have enough ratings for predictions. In addition, traditional similarity measures mainly focus on the orientations of the rating vectors, not magnitude, and as a result two rating vectors with different magnitudes but oriented in the same direction, can be exactly similar. Another aspect is that on a set of items, similar users’ may have different rating pattern. In addition, to calculate the similarity between items, ratings of all co-rated users are considered; however, a judicious approach is to consider the similarity between users as a weight to find the similar neighbors of a target item. To mitigate these issues, a modified Bhattacharyya coefficient is proposed in this paper. The proposed similarity measure is used to calculate user–user similarity, which in turn is used as a weight in item-based collaborative filtering. The experimental analysis on the collected MovieLens datasets shows a significant improvement of item-based collaborative filtering, when user–user similarity calculated by the proposed modified similarity measure is used as a weight.},
  archive      = {J_KIS},
  author       = {Singh, Pradeep Kumar and Sinha, Shreyashee and Choudhury, Prasenjit},
  doi          = {10.1007/s10115-021-01651-8},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {665-701},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An improved item-based collaborative filtering using a modified bhattacharyya coefficient and user–user similarity as weight},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble of classifier chains and decision templates for
multi-label classification. <em>KIS</em>, <em>64</em>(3), 643–663. (<a
href="https://doi.org/10.1007/s10115-021-01647-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is the task of inferring the set of unseen instances using the knowledge obtained through the analysis of a set of training examples with known label sets. In this paper, a multi-label classifier fusion ensemble approach named decision templates for ensemble of classifier chains is presented, which is derived from the decision templates method. The proposed method estimates two decision templates per class, one representing the presence of the class and the other representing its absence, based on the same examples used for training the set of classifiers. For each unseen instance, a new decision profile is created and the similarity between the decision templates and the decision profile determines the resulting label set. The method is incorporated into a traditional multi-label classifier algorithm: the ensemble of classifier chains. Empirical evidence indicates that the use of the proposed decision templates adaptation can improve the performance over the traditionally used combining schemes, especially for domains with a large number of instances available, improving the performance of an already high-performing multi-label learning method.},
  archive      = {J_KIS},
  author       = {Freitas Rocha, Victor and Varejão, Flávio Miguel and Segatto, Marcelo Eduardo Vieira},
  doi          = {10.1007/s10115-021-01647-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {643-663},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ensemble of classifier chains and decision templates for multi-label classification},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of cluster-based image search result
organization: Background, techniques, and ongoing challenges.
<em>KIS</em>, <em>64</em>(3), 589–642. (<a
href="https://doi.org/10.1007/s10115-021-01650-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital photographs and visual data have become increasingly available, especially on the Web considered as the largest image database to date. However, the value of multimedia content depends on how easy it is to search and manage. Thus, the need to efficiently index, store, and retrieve images is becoming evermore important, particularly on the Web where existing image search and retrieval techniques do not seem to keep pace. Most existing solutions return a large quantity of search results ranked by their relevance to the user query. This can be tedious and time-consuming for the user, since the returned results usually contain multiple topics mixed together, and the user cannot be expected to have the time to scroll through the huge result list. A possible solution is to better organize the output information (prior or after query refinement), providing a means to facilitate the assimilation of the search results. In this context, image search result organization (ISRO) has been recently investigated as an effective and efficient solution to improve image retrieval quality on the Web. Most methods in this context exploit image clustering as a methodology capable of topic extraction and rendering semantically more meaningful results to the user. This survey paper provides a concise and comprehensive review of the methods related to cluster-based ISRO on the Web. It is made of four logical parts: First, we provide a glimpse on image information retrieval. Second, we briefly cover the background on ISRO. Third, we describe and categorize various steps involved in cluster-based ISRO, ranging over image representation, similarity computation, image clustering or grouping, and cluster-based search result visualization. Fourth, we briefly summarize and discuss ongoing research challenges and future directions, including high-dimensional feature indexing, joint word image modeling and implicit semantics, describing images based on aesthetics, automatic similarity metric learning, combining ensemble clustering methods, performing adaptive clustering, allowing dynamic trade-off between clustering quality and efficiency, diversifying image search results, integrating user feedback, and adapting results to mobile devices.},
  archive      = {J_KIS},
  author       = {Tekli, Joe},
  doi          = {10.1007/s10115-021-01650-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {589-642},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An overview of cluster-based image search result organization: Background, techniques, and ongoing challenges},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fire now, fire later: Alarm-based systems for prescriptive
process monitoring. <em>KIS</em>, <em>64</em>(2), 559–587. (<a
href="https://doi.org/10.1007/s10115-021-01633-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive process monitoring is a family of techniques to analyze events produced during the execution of a business process in order to predict the future state or the final outcome of running process instances. Existing techniques in this field are able to predict, at each step of a process instance, the likelihood that it will lead to an undesired outcome. These techniques, however, focus on generating predictions and do not prescribe when and how process workers should intervene to decrease the cost of undesired outcomes. This paper proposes a framework for prescriptive process monitoring, which extends predictive monitoring with the ability to generate alarms that trigger interventions to prevent an undesired outcome or mitigate its effect. The framework incorporates a parameterized cost model to assess the cost–benefit trade-off of generating alarms. We show how to optimize the generation of alarms given an event log of past process executions and a set of cost model parameters. The proposed approaches are empirically evaluated using a range of real-life event logs. The experimental results show that the net cost of undesired outcomes can be minimized by changing the threshold for generating alarms, as the process instance progresses. Moreover, introducing delays for triggering alarms, instead of triggering them as soon as the probability of an undesired outcome exceeds a threshold, leads to lower net costs.},
  archive      = {J_KIS},
  author       = {Fahrenkrog-Petersen, Stephan A. and Tax, Niek and Teinemaa, Irene and Dumas, Marlon and Leoni, Massimiliano de and Maggi, Fabrizio Maria and Weidlich, Matthias},
  doi          = {10.1007/s10115-021-01633-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {559-587},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fire now, fire later: Alarm-based systems for prescriptive process monitoring},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Community detection combining topology and attribute
information. <em>KIS</em>, <em>64</em>(2), 537–558. (<a
href="https://doi.org/10.1007/s10115-021-01646-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structures detection is critical in the analysis of features and functions of complex networks. Traditional methods are mostly concerned with the topology information of networks when conducting community detection, and can only describe the community structures from one aspect. For a more comprehensive analysis of the network, there is often attribute information available and it is a good complement to topology information. In this paper, we propose two parameter-free models based on nonnegative matrix factorization (NMF for short), Topology and Attribute NMF (TANMF for short) and Topology and Attribute Symmetrical NMF (TASNMF for short), combining topology information and attribute information for community structures detection. In addition, the multiplicative update rules are designed and the convergence is proved. Systematic experiments on both the synthetic and the real networks demonstrate the effectiveness and efficiency of our methods.},
  archive      = {J_KIS},
  author       = {Lu, Dan-Dan and Qi, Ji and Yan, Jie and Zhang, Zhong-Yuan},
  doi          = {10.1007/s10115-021-01646-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {537-558},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Community detection combining topology and attribute information},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing top-k temporal closeness in temporal networks.
<em>KIS</em>, <em>64</em>(2), 507–535. (<a
href="https://doi.org/10.1007/s10115-021-01639-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The closeness centrality of a vertex in a classical static graph is the reciprocal of the sum of the distances to all other vertices. However, networks are often dynamic and change over time. Temporal distances take these dynamics into account. In this work, we consider the harmonic temporal closeness with respect to the shortest duration distance. We introduce an efficient algorithm for computing the exact top-k temporal closeness values and the corresponding vertices. The algorithm can be generalized to the task of computing all closeness values. Furthermore, we derive heuristic modifications that perform well on real-world data sets and drastically reduce the running times. For the case that edge traversal takes an equal amount of time for all edges, we lift two approximation algorithms to the temporal domain. The algorithms approximate the transitive closure of a temporal graph (which is an essential ingredient for the top-k algorithm) and the temporal closeness for all vertices, respectively, with high probability. We experimentally evaluate all our new approaches on real-world data sets and show that they lead to drastically reduced running times while keeping high quality in many cases. Moreover, we demonstrate that the top-k temporal and static closeness vertex sets differ quite largely in the considered temporal networks.},
  archive      = {J_KIS},
  author       = {Oettershagen, Lutz and Mutzel, Petra},
  doi          = {10.1007/s10115-021-01639-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {507-535},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Computing top-k temporal closeness in temporal networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The phantom steering effect in q&amp;a websites.
<em>KIS</em>, <em>64</em>(2), 475–506. (<a
href="https://doi.org/10.1007/s10115-021-01637-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual rewards, such as badges, are commonly used in online platforms as incentives for promoting contributions from a userbase. It is widely accepted that such rewards “steer” people’s behaviour towards increasing their rate of contributions before obtaining the reward. This paper provides a new probabilistic model of user behaviour in the presence of threshold rewards, such a badges. We find, surprisingly, that while steering does affect a minority of the population, the majority of users do not change their behaviour around the achievement of these virtual rewards. In particular, we find that only approximately 5–30% of Stack Overflow users who achieve the rewards appear to respond to the incentives. This result is based on the analysis of thousands of users’ activity patterns before and after they achieve the reward. Our conclusion is that the phenomenon of steering is less common than has previously been claimed. We identify a statistical phenomenon, termed “Phantom Steering”, that can account for the interaction data of the users who do not respond to the reward. The presence of phantom steering may have contributed to some previous conclusions about the ubiquity of steering. We conduct a qualitative survey of the users on Stack Overflow which supports our results, suggesting that the motivating factors behind user behaviour are complex, and that some of the online incentives used in Stack Overflow may not be solely responsible for changes in users’ contribution rates.},
  archive      = {J_KIS},
  author       = {Hoernle, Nicholas and Kehne, Gregory and Procaccia, Ariel D. and Gal, Kobi},
  doi          = {10.1007/s10115-021-01637-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {475-506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The phantom steering effect in Q&amp;A websites},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid quantum approach to leveraging data from HTML
tables. <em>KIS</em>, <em>64</em>(2), 441–474. (<a
href="https://doi.org/10.1007/s10115-021-01636-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Web provides many data that are encoded using HTML tables. This facilitates rendering them, but obfuscates their structure and makes it difficult for automated business processes to leverage them. This has motivated many authors to work on proposals to extract them as automatically as possible. In this article, we present a new unsupervised proposal that uses a hybrid approach in which a standard computer is used to perform pre- and post-processing tasks and a quantum computer is used to perform the core task: guessing whether the cells have labels or values. The problem is addressed using a clustering approach that is known to be NP using standard computers, but our proposal can solve it in polynomial time, which implies a significant performance improvement. It is novel in that it relies on an entropy-preservation metaphor that has proven to work very well on two large collections of real-world tables from the Wikipedia and the Dresden Web Table Corpus. Our experiments prove that our proposal can beat the state-of-the-art proposal in terms of both effectiveness and efficiency; the key difference is that our proposal is totally unsupervised, whereas the state-of-the-art proposal is supervised.},
  archive      = {J_KIS},
  author       = {Jiménez, Patricia and Roldán, Juan C. and Corchuelo, Rafael},
  doi          = {10.1007/s10115-021-01636-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {441-474},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid quantum approach to leveraging data from HTML tables},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient lightweight coordination model to multi-agent
planning. <em>KIS</em>, <em>64</em>(2), 415–439. (<a
href="https://doi.org/10.1007/s10115-021-01638-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main issue in multi-agent planning (MAP) is the agents’ coordination process that is a computationally hard problem. Thus, many works focus on the planning strategy considering the computational process, agents’ distribution roles, information privacy, and the resources coupling level. But domain-independent models that explore the balance between coordination process and privacy leading to efficient execution are missing. In this manuscript, we present a Lightweight Coordination Multi-agent Planning (LCMAP), a domain-independent model that balances the coordination process and privacy through three independent phases: (i) verification—each agent verifies its capabilities of reaching the goals; (ii) transformation—the coordinator selects agents through their capabilities and distributes the goals, transforming the original problem into single-agent problems; and (iii) validation—each plan is validated to check whether it can be parallel. LCMAP was compared to the state-of-the-art models to evaluate time efficiency and plan length during the problem-solving process using loosely and tightly coupled domains with specific evaluation metrics inherited from planning competitions. Furthermore, we conducted experiments to evaluate the execution efficiency regarding different configurations concerning planning time and plan length of the models, when LCMAP execution proves to be efficient.},
  archive      = {J_KIS},
  author       = {Moreira, Leonardo Henrique and Ralha, Célia Ghedini},
  doi          = {10.1007/s10115-021-01638-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {415-439},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient lightweight coordination model to multi-agent planning},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visionary: A framework for analysis and visualization of
provenance data. <em>KIS</em>, <em>64</em>(2), 381–413. (<a
href="https://doi.org/10.1007/s10115-021-01645-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provenance is recognized as a central challenge to establish the reliability and provide security in computational systems. In scientific workflows, provenance is considered essential to support experiments’ reproducibility, interpretation of results, and problem diagnosis. We consider that these requirements can also be used in new application domains, such as software processes and IoT. However, for a better understanding and use of provenance data, efficient and user-friendly mechanisms are needed. Ontology, complex networks, and software visualization can help in this process by generating new data insights and strategic information for decision-making. This paper presents the Visionary framework, designed to assist in the understanding and use of provenance data through ontologies, complex network analysis, and software visualization techniques. The framework captures the provenance data and generates new information using ontologies and structural analysis of the provenance graph. The visualization presents and highlights inferences and results obtained with the data analysis. Visionary is an application domain-free framework adapted to any system that uses the PROV provenance model. Evaluations were carried out, and some evidence was found that the framework assists in the understanding and analysis of provenance data when decision-making is needed.},
  archive      = {J_KIS},
  author       = {de Oliveira, Weiner and Braga, Regina and David, José Maria N. and Stroele, Victor and Campos, Fernanda and Castro, Gabriellla},
  doi          = {10.1007/s10115-021-01645-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {381-413},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Visionary: A framework for analysis and visualization of provenance data},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale underwater fish recognition via deep adversarial
learning. <em>KIS</em>, <em>64</em>(2), 353–379. (<a
href="https://doi.org/10.1007/s10115-021-01643-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish species recognition from images captured in underwater environments plays an essential role in many natural science studies, such as fish stock assessment, marine ecosystem analysis, and environmental research. However, the noisy nature of underwater images makes it difficult to train high-performance fish recognition models. This work presents a novel deep adversarial learning framework called AdvFish to train accurate deep neural networks fish recognition models from noisy large-scale underwater images. Unlike existing methods that rely on feature engineering or implicit machine learning techniques to mitigate the noise, AdvFish is a min–max bilevel adversarial optimization framework that trains the model on adversarially perturbed images via a proposed adaptive perturbation method. We show, on multiple benchmark datasets, that AdvFish holds a clear advantage over existing methods/models, especially on a noisy large-scale dataset. AdvFish is a generic learning framework that can help train better recognition models from extremely noisy images.},
  archive      = {J_KIS},
  author       = {Zhang, Zhixue and Du, Xiujuan and Jin, Long and Wang, Shuqiao and Wang, Lijuan and Liu, Xiuxiu},
  doi          = {10.1007/s10115-021-01643-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {353-379},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Large-scale underwater fish recognition via deep adversarial learning},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Named entity disambiguation in short texts over knowledge
graphs. <em>KIS</em>, <em>64</em>(2), 325–351. (<a
href="https://doi.org/10.1007/s10115-021-01642-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-growing usage of knowledge graphs (KGs) positions named entity disambiguation (NED) at the heart of designing accurate KG-driven systems such as query answering systems (QAS). According to the current research, most studies dealing with NED on KGs involve long texts, which is not the case of short text fragments, identified by their limited contexts. The accuracy of QASs strongly depends on the management of such short text. This limitation motivates this paper, which studies the NED problem on KGs, involving only short texts. First, we propose a NED approach including the following steps: (i) context expansion using WordNet to measure its similarity to the resource context. (ii) Exploiting coherence between entities in queries that contain more than one entity, such as “Is Michelle Obama the wife of Barack Obama?”. (iii) Taking into account the relations between words to calculate their similarity with the properties of a resource. (iv) the use of syntactic features. The NED solution approach is compared to state-of-the-art approaches using five datasets. The experimental results show that our approach outperforms these systems by 27% in the F-measure. A system called Welink, implementing our proposal, is available on GitHub, and it is also accessible via a REST API.},
  archive      = {J_KIS},
  author       = {Bouarroudj, Wissem and Boufaida, Zizette and Bellatreche, Ladjel},
  doi          = {10.1007/s10115-021-01642-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {325-351},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Named entity disambiguation in short texts over knowledge graphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Determining maximum cliques for community detection in
weighted sparse networks. <em>KIS</em>, <em>64</em>(2), 289–324. (<a
href="https://doi.org/10.1007/s10115-021-01631-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we have delved into detecting dense regions of weighted sparse networks through identification of cliques and communities. A novel clique finding method is introduced to generate all maximum cliques of a sparse network, with focus on analysis of real-life networks and a community detection algorithm is devised on maximal cliques to determine possible overlapping communities of the weighted sparse network. A good number of methods of clique detection already exist, some of which are truly efficient, but many of them lack direct applicability to real-life network analysis, as they deal with simple networks, hide intermediary details and allow cliques to be formed without information on strength of interactions in group behavior. The proposed method attaches a clique-intensity value with every clique and using two thresholds on intensity of interactions, at the individual level and group level, provides handle to filter stray or insignificant interactions at various stages of clique formation. Using differently sized weighted maximal cliques as building blocks, a new overlapping community detection method is proposed using a new measure, a weighted version of Jaccard index, called weighted Jaccard index. Experiments are done on real-life networks; the maximum clique structures reveal sensitivity to threshold values, while the resulting community structures demonstrate efficacy of the community detection method.},
  archive      = {J_KIS},
  author       = {Goswami, Swati and Das, Asit Kumar},
  doi          = {10.1007/s10115-021-01631-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {289-324},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Determining maximum cliques for community detection in weighted sparse networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modified marine predators algorithm for feature selection:
Case study metabolomics. <em>KIS</em>, <em>64</em>(1), 261–287. (<a
href="https://doi.org/10.1007/s10115-021-01641-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is a necessary process applied to reduce the high dimensionality of the dataset. It is utilized to obtain the most relevant information and reduce the computational efforts of the classification process. Recently, metaheuristics methods have been widely employed for various optimization problems, including FS. In the current study, we present an FS method based on a new modified version of the marine predators algorithm (MPA). In the developed MPASCA model, the sine–cosine algorithm (SCA) is utilized to improve the search ability, which works as a local search of the MPA. To evaluate the performance of the MPASCA algorithm, extensive experiments were carried out using 18 UCI datasets. More so, the metabolomics dataset is used to test the proposed method as a real-world application. Furthermore, we implemented extensive comparisons to several state-of-art methods to verify the efficiency of the MPASCA. The evaluation outcomes showed that the MPASCA has significant performance, and it outperforms the compared methods in terms of classification measures.},
  archive      = {J_KIS},
  author       = {Abd Elaziz, Mohamed and Ewees, Ahmed A. and Yousri, Dalia and Abualigah, Laith and Al-qaness, Mohammed A. A.},
  doi          = {10.1007/s10115-021-01641-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {261-287},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Modified marine predators algorithm for feature selection: Case study metabolomics},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable self-supervised graph representation learning via
enhancing and contrasting subgraphs. <em>KIS</em>, <em>64</em>(1),
235–260. (<a href="https://doi.org/10.1007/s10115-021-01635-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning has attracted lots of attention recently. Existing graph neural networks fed with the complete graph data are not scalable due to limited computation and memory costs. Thus, it remains a great challenge to capture rich information in large-scale graph data. Besides, these methods mainly focus on supervised learning and highly depend on node label information, which is expensive to obtain in the real world. As to unsupervised network embedding approaches, they overemphasize node proximity instead, whose learned representations can hardly be used in downstream application tasks directly. In recent years, emerging self-supervised learning provides a potential solution to address the aforementioned problems. However, existing self-supervised works also operate on the complete graph data and are biased to fit either global or very local (1-hop neighborhood) graph structures in defining the mutual information-based loss terms. In this paper, a novel self-supervised representation learning method via Sub-graph Contrast, namely Subg-Con, is proposed by utilizing the strong correlation between central nodes and their sampled subgraphs to capture regional structure information. Instead of learning on the complete input graph data, with a novel data augmentation strategy, Subg-Con learns node representations through a contrastive loss defined based on subgraphs sampled from the original graph instead. Besides, we further enhance the subgraph representation learning via mutual information maximum to preserve more topology and feature information. Compared with existing graph representation learning approaches, Subg-Con has prominent performance advantages in weaker supervision requirements, model learning scalability, and parallelization. Extensive experiments verify both the effectiveness and the efficiency of our work. We compared it with both classic and state-of-the-art graph representation learning approaches. Various downstream tasks are done on multiple real-world large-scale benchmark datasets from different domains.},
  archive      = {J_KIS},
  author       = {Jiao, Yizhu and Xiong, Yun and Zhang, Jiawei and Zhang, Yao and Zhang, Tianqi and Zhu, Yangyong},
  doi          = {10.1007/s10115-021-01635-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {235-260},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Scalable self-supervised graph representation learning via enhancing and contrasting subgraphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable force-directed graph representation learning and
visualization. <em>KIS</em>, <em>64</em>(1), 207–233. (<a
href="https://doi.org/10.1007/s10115-021-01634-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph embedding algorithm embeds a graph into a low-dimensional space such that the embedding preserves the inherent properties of the graph. While graph embedding is fundamentally related to graph visualization, prior work did not exploit this connection explicitly. We develop Force2Vec that uses force-directed graph layout models in a graph embedding setting with an aim to excel in both machine learning (ML) and visualization tasks. We make Force2Vec highly parallel by mapping its core computations to linear algebra and utilizing multiple levels of parallelism available in modern processors. The resultant algorithm is an order of magnitude faster than existing methods (43 $$\times $$ faster than DeepWalk, on average) and can generate embeddings from graphs with billions of edges in a few hours. In comparison to existing methods, Force2Vec is better in graph visualization and performs comparably or better in ML tasks such as link prediction, node classification, and clustering. Source code is available at https://github.com/HipGraph/Force2Vec .This paper is an extension of a conference paper by Rahman et al. (in: 20th IEEE international conference on data mining, IEEE ICDM, 2020b) published in IEEE ICDM 2020.},
  archive      = {J_KIS},
  author       = {Rahman, Md. Khaledur and Sujon, Majedul Haque and Azad, Ariful},
  doi          = {10.1007/s10115-021-01634-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {207-233},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Scalable force-directed graph representation learning and visualization},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-attribute precedence-based task scheduling
for edge computing in geo-distributed cloud environment. <em>KIS</em>,
<em>64</em>(1), 175–205. (<a
href="https://doi.org/10.1007/s10115-021-01627-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to realize globalization of cloud computing, joint use of different services of different cloud providers has become an inevitable trend. The geo-distributed cloud consists of several different clouds, providing a general environment for cloud computing. In data placement, many recently proposed data placement algorithms unilaterally use a single performance index to evaluate the performance of the algorithm. In task scheduling, when tasks are allocated with excess cloud resources, resources are wasted. When little cloud resources are allocated to the complex task, cause the overall progress of the system to stagnate, the overall progress of the system is stalled. For solving the above problems, the data placement method and the task scheduling method are proposed. In the proposed data placement scheme, multiple performance indicators are considered. The detection of the straggling nodes and the reasonable allocation of cloud resources are taken into account when the task is scheduled. For proving the superiority of the proposed methods, extensive experiments are conducted. In terms of the data placement, when the number of files is set as 800, the safety level of the proposed data placement algorithm is 7.0, which is 27.3% higher than that of the IDP algorithm, 45.8% higher than that of the GA-DPSO algorithm and 16.7% higher than that of the H2DP algorithm. As for the task scheduling, the percentage improvement in the time overhead of the proposed task scheduling method is the lowest, which implies that the time overhead of the proposed task scheduling algorithm is closest to the optimal time and is the shortest.},
  archive      = {J_KIS},
  author       = {Li, Chunlin and Zhang, Chaokun and Ma, Bingbin and Luo, Youlong},
  doi          = {10.1007/s10115-021-01627-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {175-205},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient multi-attribute precedence-based task scheduling for edge computing in geo-distributed cloud environment},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random pairwise shapelets forest: An effective classifier
for time series. <em>KIS</em>, <em>64</em>(1), 143–174. (<a
href="https://doi.org/10.1007/s10115-021-01630-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapelet is a discriminative subsequence of time series. An advanced shapelet-based method is to embed shapelet into the accurate and fast random forest. However, there are several limitations. First, random shapelet forest requires a large training cost for split threshold searching. Second, a single shapelet provides limited information for only one branch of the decision tree, resulting in insufficient accuracy. Third, the randomized ensemble decreases comprehensibility. For that, this paper presents Random Pairwise Shapelets Forest (RPSF). RPSF combines a pair of shapelets from different classes to construct random forest. It omits threshold searching to be more efficient, includes more information about each node of the forest to be more effective. Moreover, a discriminability measure, Decomposed Mean Decrease Impurity, is proposed to identify the influential region for each class. Extensive experiments show that RPSF is competitive compared with other methods, while it improves the training speed of shapelet-based forest.},
  archive      = {J_KIS},
  author       = {Yuan, Jidong and Shi, Mohan and Wang, Zhihai and Liu, Haiyang and Li, Jinyang},
  doi          = {10.1007/s10115-021-01630-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {143-174},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Random pairwise shapelets forest: An effective classifier for time series},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapting k-means for graph clustering. <em>KIS</em>,
<em>64</em>(1), 115–142. (<a
href="https://doi.org/10.1007/s10115-021-01623-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two new algorithms for clustering graphs and networks. The first, called K‑algorithm, is derived directly from the k-means algorithm. It applies similar iterative local optimization but without the need to calculate the means. It inherits the properties of k-means clustering in terms of both good local optimization capability and the tendency to get stuck at a local optimum. The second algorithm, called the M-algorithm, gradually improves on the results of the K-algorithm to find new and potentially better local optima. It repeatedly merges and splits random clusters and tunes the results with the K-algorithm. Both algorithms are general in the sense that they can be used with different cost functions. We consider the conductance cost function and also introduce two new cost functions, called inverse internal weight and mean internal weight. According to our experiments, the M-algorithm outperforms eight other state-of-the-art methods. We also perform a case study by analyzing clustering results of a disease co-occurrence network, which demonstrate the usefulness of the algorithms in an important real-life application.},
  archive      = {J_KIS},
  author       = {Sieranoja, Sami and Fränti, Pasi},
  doi          = {10.1007/s10115-021-01623-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {115-142},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adapting k-means for graph clustering},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E2EET: From pipeline to end-to-end entity typing via
transformer-based embeddings. <em>KIS</em>, <em>64</em>(1), 95–113. (<a
href="https://doi.org/10.1007/s10115-021-01626-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity typing (ET) is the process of identifying the semantic types of every entity within a corpus. ET involves labelling each entity mention with one or more class labels. As a multi-class, multi-label task, it is considerably more challenging than named entity recognition. This means existing entity typing models require pre-identified mentions and cannot operate directly on plain text. Pipeline-based approaches are therefore used to join a mention extraction model and an entity typing model to process raw text. Another key limiting factor is that these mention-level ET models are trained on fixed context windows, which makes the entity typing results sensitive to window size selection. In light of these drawbacks, we propose an end-to-end entity typing model (E2EET) using a Bi-GRU to remove the dependency on window size. To demonstrate the effectiveness of our E2EET model, we created a stronger baseline mention-level model by incorporating the latest contextualised transformer-based embeddings (BERT). Extensive ablative studies demonstrate the competitiveness and simplicity of our end-to-end model for entity typing.},
  archive      = {J_KIS},
  author       = {Stewart, Michael and Liu, Wei},
  doi          = {10.1007/s10115-021-01626-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {95-113},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {E2EET: From pipeline to end-to-end entity typing via transformer-based embeddings},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An integrated framework for diagnosing process faults with
incomplete features. <em>KIS</em>, <em>64</em>(1), 75–93. (<a
href="https://doi.org/10.1007/s10115-021-01625-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling missing values and large-dimensional features are crucial requirements for data-driven fault diagnosis systems. However, most intelligent data-driven diagnostic systems are not able to handle missing data. The presence of high-dimensional feature sets can also further complicate the process of fault diagnosis. This paper aims to devise a missing data imputation unit along with a dimensionality reduction unit in the pre-processing module of the diagnostic system. This paper proposes a novel pooling strategy for missing data imputation (PSMI). This strategy can simplify complex patterns of missingness and incrementally update the pool. The pre-processing module receives incomplete observations, PSMI estimates missing values, and, then, the dimensionality reduction unit transforms completed observations onto a lower-dimensional feature space. These transformed observations are then fed as inputs to the fault classification module for decision making and diagnosis. This diagnostic scheme makes use of various state-of-the-art missing data imputation, dimensionality reduction and classification algorithms. This enables a comprehensive comparison and allows to find the best techniques for the sake of diagnosing faults in the Tennessee Eastman process. The obtained results show the effectiveness of the proposed pooling strategy and indicate that principal component analysis imputation and heteroscedastic discriminant analysis approaches outperform other imputation and dimensionality reduction techniques in this diagnostic application.},
  archive      = {J_KIS},
  author       = {Razavi-Far, Roozbeh and Saif, Mehrdad and Palade, Vasile and Chakrabarti, Shiladitya},
  doi          = {10.1007/s10115-021-01625-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {75-93},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An integrated framework for diagnosing process faults with incomplete features},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative filtering recommender systems taxonomy.
<em>KIS</em>, <em>64</em>(1), 35–74. (<a
href="https://doi.org/10.1007/s10115-021-01628-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of internet access, recommender systems try to alleviate the difficulty that consumers face while trying to find items (e.g., services, products, or information) that better match their needs. To do so, a recommender system selects and proposes (possibly unknown) items that may be of interest to some candidate consumer, by predicting her/his preference for this item. Given the diversity of needs between consumers and the enormous variety of items to be recommended, a large set of approaches have been proposed by the research community. This paper provides a review of the approaches proposed in the entire research area of collaborative filtering recommend systems. To facilitate understanding, we provide a categorization of each approach based on the tools and techniques employed, which results to the main contribution of this paper, a collaborative filtering recommender systems taxonomy. This way, the reader acquires a quick and complete understanding of this research area. Finally, we provide a comparison of collaborative filtering recommender systems according to their ability to efficiently handle well-known drawbacks.},
  archive      = {J_KIS},
  author       = {Papadakis, Harris and Papagrigoriou, Antonis and Panagiotakis, Costas and Kosmas, Eleftherios and Fragopoulou, Paraskevi},
  doi          = {10.1007/s10115-021-01628-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {35-74},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Collaborative filtering recommender systems taxonomy},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on matrix completion for recommender systems.
<em>KIS</em>, <em>64</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10115-021-01629-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems that predict the preference of users have attracted more and more attention in decades. One of the most popular methods in this field is collaborative filtering, which employs explicit or implicit feedback to model the user–item connections. Most methods of collaborative filtering are based on matrix completion techniques which recover the missing values of user–item interaction matrices. The low-rank assumption is a critical premise for matrix completion in recommender systems, which speculates that most information in interaction matrices is redundant. Based on this assumption, a large number of methods have been developed, including matrix factorization models, rank optimization models, and frameworks based on neural networks. In this paper, we first provide a brief description of recommender systems based on matrix completion. Next, several classical and state-of-the-art algorithms related to matrix completion for collaborative filtering are introduced, most of which are based on the assumption of low-rank property. Moreover, the performance of these algorithms is evaluated and discussed by conducting substantial experiments on different real-world datasets. Finally, we provide open research issues for future exploration of matrix completion on recommender systems.},
  archive      = {J_KIS},
  author       = {Chen, Zhaoliang and Wang, Shiping},
  doi          = {10.1007/s10115-021-01629-6},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review on matrix completion for recommender systems},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
