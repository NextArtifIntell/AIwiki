<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir---44">IJMIR - 44</h2>
<ul>
<li><details>
<summary>
(2022). Similar interior coordination image retrieval with
multi-view features. <em>IJMIR</em>, <em>11</em>(4), 731–740. (<a
href="https://doi.org/10.1007/s13735-022-00247-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel similar image retrieval method for interior coordination. Interior coordination is very familiar; however, it is still an abstract and difficult concept. Even if we are involved in coordination every day, it does not mean we can become professional coordinators. By realizing the retrieval that can provide similar interior coordination images from a query room image, inspiring users’ ideas for interior coordination becomes feasible. In the proposed method, we extract image features specialized for interior coordination and realize similar interior coordination image retrieval. We employ multi-view features: object-based, color-based, and semantic-based features, in the feature extraction phase. The extracted features are used to calculate similarity between the query image and the database images for the retrieval. We conducted experiments using a sophisticated real-world interior coordination image dataset. Furthermore, we qualitatively and quantitatively evaluated the effectiveness of the proposed method.},
  archive      = {J_IJMIR},
  author       = {Togo, Ren and Honma, Yuki and Abe, Maiku and Ogawa, Takahiro and Haseyama, Miki},
  doi          = {10.1007/s13735-022-00247-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {731-740},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Similar interior coordination image retrieval with multi-view features},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal quasi-AutoRegression: Forecasting the visual
popularity of new fashion products. <em>IJMIR</em>, <em>11</em>(4),
717–729. (<a href="https://doi.org/10.1007/s13735-022-00262-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the preferences of consumers is of utmost importance for the fashion industry as appropriately leveraging this information can be beneficial in terms of profit. Trend detection in fashion is a challenging task due to the fast pace of change in the fashion industry. Moreover, forecasting the visual popularity of new garment designs is even more demanding due to lack of historical data. To this end, we propose MuQAR, a Multimodal Quasi-AutoRegressive deep learning architecture that combines two modules: (1) a multimodal multilayer perceptron processing categorical, visual and textual features of the product and (2) a Quasi-AutoRegressive neural network modelling the “target” time series of the product’s attributes along with the “exogenous” time series of all other attributes. We utilize computer vision, image classification and image captioning, for automatically extracting visual features and textual descriptions from the images of new products. Product design in fashion is initially expressed visually and these features represent the products’ unique characteristics without interfering with the creative process of its designers by requiring additional inputs (e.g. manually written texts). We employ the product’s target attributes time series as a proxy of temporal popularity patterns, mitigating the lack of historical data, while exogenous time series help capture trends among interrelated attributes. We perform an extensive ablation analysis on two large-scale image fashion datasets, Mallzee-P and SHIFT15m to assess the adequacy of MuQAR and also use the Amazon Reviews: Home and Kitchen dataset to assess generalization to other domains. A comparative study on the VISUELLE dataset shows that MuQAR is capable of competing and surpassing the domain’s current state of the art by 4.65% and 4.8% in terms of WAPE and MAE, respectively.},
  archive      = {J_IJMIR},
  author       = {Papadopoulos, Stefanos-Iordanis and Koutlis, Christos and Papadopoulos, Symeon and Kompatsiaris, Ioannis},
  doi          = {10.1007/s13735-022-00262-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {717-729},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multimodal quasi-AutoRegression: Forecasting the visual popularity of new fashion products},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tri-RAT: Optimizing the attention scores for image
captioning. <em>IJMIR</em>, <em>11</em>(4), 705–715. (<a
href="https://doi.org/10.1007/s13735-022-00260-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms and grid features are widely used in current visual language tasks like image captioning. The attention scores are the key factor to the success of the attention mechanism. However, the connection between attention scores in different layers is not strong enough since Transformer is a hierarchical structure. Additionally, geometric information is inevitably lost when grid features are flattened to be fed into a transformer model. Therefore, bias scores about geometric position information should be added to the attention scores. Considering that there are three different kinds of attention modules in the transformer architecture, we build three independent paths (residual attention paths, RAPs) to propagate the attention scores from the previous layer as a prior for attention computation. This operation is like a residual connection between attention scores, which can enhance the connection and make each attention layer obtain a global comprehension. Then, we replace the traditional attention module with a novel residual attention with relative position module in the encoder to incorporate relative position scores with attention scores. Residual attention may increase the internal covariate shifts. To optimize the data distribution, we introduce residual attention with layer normalization on query vectors module in the decoder. Finally, we build our Residual Attention Transformer with three RAPs (Tri-RAT) for the image captioning task. The proposed model achieves competitive performance on the MSCOCO benchmark with all the state-of-the-art models. We gain 135.8 $$\%$$ CIDEr on MS COCO “Karpathy” offline test split and 135.3 $$\%$$ CIDEr on the online testing server.},
  archive      = {J_IJMIR},
  author       = {Yang, You and An, Yongzhi and Hu, Juntao and Pan, Longyue},
  doi          = {10.1007/s13735-022-00260-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {705-715},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Tri-RAT: Optimizing the attention scores for image captioning},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gender classification from face images using central
difference convolutional networks. <em>IJMIR</em>, <em>11</em>(4),
695–703. (<a href="https://doi.org/10.1007/s13735-022-00259-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays gender classification which plays a vital role in face recognition systems is one of the main matters in computer vision. It is difficult to classify the gender from facial images when dealing with unconstrained images in a cross-dataset protocol. In this work, we propose two convolutional neural networks where one of the networks used the central difference convolution layer and another network used the vanilla convolution layer. The system was trained with the Casia WebFace dataset and tested on two cross-datasets, labeled faces in the wild (LFW) and FEI dataset. It is worth mentioning that the experimental results show the power and effectiveness of the proposed method. This method obtains a classification rate of 97.79% for the LFW dataset and 99.10% for the FEI dataset.},
  archive      = {J_IJMIR},
  author       = {Sheikh Fathollahi, Mohammadreza and Heidari, Rezvan},
  doi          = {10.1007/s13735-022-00259-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {695-703},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Gender classification from face images using central difference convolutional networks},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MHA-WoML: Multi-head attention and wasserstein-OT for
few-shot learning. <em>IJMIR</em>, <em>11</em>(4), 681–694. (<a
href="https://doi.org/10.1007/s13735-022-00254-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to classify novel classes with extreme few labeled samples. Existing metric-learning-based approaches tend to employ the off-the-shelf CNN models for feature extraction, and conventional clustering algorithms for feature matching. These methods neglect the importance of image regions and might trap in over-fitting problems during feature clustering. In this work, we propose a novel MHA-WoML framework for few-shot learning, which adaptively focuses on semantically dominant regions, and well relieves the over-fitting problem. Specifically, we first design a hierarchical multi-head attention (MHA) module, which consists of three functional heads (i.e., rare head, syntactic head and positional head) with masks, to extract comprehensive image features, and screen out invalid features. The MHA behaves better than current transformers in few-shot recognition. Then, we incorporate the optimal transport theory into Wasserstein distance and propose a Wasserstein-OT metric learning (WoML) module for category clustering. The WoML module focuses more on calculating the appropriately approximate barycenter to avoid the over accurate sub-stage fitting which may threaten the global fitting, thus alleviating the problem of over-fitting in the training process. Experimental results show that our approach achieves remarkably better performance compared to current state-of-the-art methods by scoring about 3% higher accuracy, across four benchmark datasets including MiniImageNet, TieredImageNet, CIFAR-FS and CUB200.},
  archive      = {J_IJMIR},
  author       = {Yang, Junyan and Jiang, Jie and Guo, Yanming},
  doi          = {10.1007/s13735-022-00254-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {681-694},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MHA-WoML: Multi-head attention and wasserstein-OT for few-shot learning},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual and semantic ensemble for scene text recognition with
gated dual mutual attention. <em>IJMIR</em>, <em>11</em>(4), 669–680.
(<a href="https://doi.org/10.1007/s13735-022-00253-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition is a challenging task in computer vision due to the significant differences in text appearance, such as image distortion and rotation. However, linguistic prior helps individuals reason text from images even if some characters are missing or blurry. This paper investigates the fusion of visual cues and linguistic dependencies to boost recognition performance. We introduce a relational attention module to leverage visual patterns and word representations. We embed linguistic dependencies from a language model into the optimization framework to ensure that the predicted sequence captures the contextual dependencies within a word. We propose a dual mutual attention transformer that promotes cross-modality interactions such that the inter- and intra-correlations between visual and linguistic can be fully explored. The introduced gate function enables the model to learn to determine the contribution of each modality and further boost the model performance. Extensive experiments demonstrate that our method enhances the recognition performance of low-quality images and achieves state-of-the-art performance on datasets of texts from regular and irregular scenes.},
  archive      = {J_IJMIR},
  author       = {Liu, Zhiguang and Wang, Liangwei and Qiao, Jian},
  doi          = {10.1007/s13735-022-00253-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {669-680},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Visual and semantic ensemble for scene text recognition with gated dual mutual attention},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel method for video shot boundary detection using
CNN-LSTM approach. <em>IJMIR</em>, <em>11</em>(4), 653–667. (<a
href="https://doi.org/10.1007/s13735-022-00251-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid growth of digital videos and the massive increase in video content, there is an urgent need to develop efficient automatic video content analysis mechanisms for different tasks, namely summarization, retrieval, and classification. In all these applications, one needs to identify shot boundary detection. This paper proposes a novel dual-stage approach for cut transition detection that can withstand certain illumination and motion effects. Firstly, we present a deep neural network model using the pre-trained model combined with long short-term memory LSTM network and the euclidean distance metric. Two parallel pre-trained models sharing the same weights extract the spatial features. Then, these features are fed to the LSTM and the euclidean distance metric to classify the frames into specific categories (similar or not similar). To train the model, we generated a new database containing 5000 frame pairs with two labels (similar, dissimilar) for training and 1000 frame pairs for testing from online videos. Secondly, we adopt the segment selection process to predict the shot boundaries. This preprocessing method can help improve the accuracy and speed of the VSBD algorithm. Then, cut transition detection based on the similarity model is conducted to identify the shot boundaries in the candidate segments. Experimental results on standard databases TRECVid 2001, 2007, and RAI show that the proposed approach achieves better detection rates over the state-of-the-art SBD methods in terms of the F1 score criterion.},
  archive      = {J_IJMIR},
  author       = {Benoughidene, Abdelhalim and Titouna, Faiza},
  doi          = {10.1007/s13735-022-00251-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {653-667},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A novel method for video shot boundary detection using CNN-LSTM approach},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative adversarial networks for 2D-based CNN
pose-invariant face recognition. <em>IJMIR</em>, <em>11</em>(4),
639–651. (<a href="https://doi.org/10.1007/s13735-022-00249-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer vision community considers the pose-invariant face recognition (PIFR) as one of the most challenging applications. Many works were devoted to enhancing face recognition performance when facing profile samples. They mainly focused on 2D- and 3D-based frontalization techniques trying to synthesize frontal views from profile ones. In the same context, we propose in this paper a new 2D PIFR technique based on Generative Adversarial Network image translation. The used GAN is Pix2Pix paired architecture covering many generator and discriminator models that will be comprehensively evaluated on a new benchmark proposed in this paper referred to as Combined-PIFR database, which is composed of four datasets that provide profiles images and their corresponding frontal ones. The paired architecture we are using is based on computing the L1 distance between the generated image and the ground truth one (pairs). Therefore, both generator and discriminator architectures are paired ones. The Combined-PIFR database is partitioned respecting person-independent constraints to evaluate our proposed framework’s frontalization and classification sub-systems fairly. Thanks to the GAN-based frontalization, the recorded results demonstrate an important improvement of 33.57% compared to the baseline.},
  archive      = {J_IJMIR},
  author       = {Kas, M. and El-merabet, Y. and Ruichek, Y. and Messoussi, R.},
  doi          = {10.1007/s13735-022-00249-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {639-651},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Generative adversarial networks for 2D-based CNN pose-invariant face recognition},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-aware visual scene representation. <em>IJMIR</em>,
<em>11</em>(4), 619–638. (<a
href="https://doi.org/10.1007/s13735-022-00246-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification is a mature and active computer vision task, due to the inherent ambiguity. The scene classification task aims to classify the visual scene images in predefined categories based on the ambient content, objects and the layout of the images. Inspired by human visual scene understanding, the visual scenes can be divided into two categories: (1) Object-based scenes that consist of the scene-specific objects and can be understood with those objects. (2) Layout-based scenes that are understandable based on the layout and the ambient content of the scene images. Scene-specific objects semantically help to understand object-based scenes, whereas the layout and the ambient content are effective in understanding layout-based scenes by representing the visual appearance of the scene images. Hence, one of the main challenges in scene classification is to create a discriminative representation that can provide a high-level perception of visual scenes. Accordingly, we have presented a discriminative hybrid representation of visual scenes, in which semantic features extracted from scene-specific objects are fused with visual features extracted from a deep CNN. The proposed scene representation method is used for the scene classification task and is applied to three benchmark scene datasets including: MIT67, SUN397, and UIUC Sports. Moreover, a new scene dataset, called &quot;Scene40,&quot; has been introduced, and also, the results of our proposed method have been presented on it. Experimental results show that our proposed method has achieved remarkable performance in the scene classification task and has outperformed the state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Parseh, Mohammad Javad and Rahmanimanesh, Mohammad and Keshavarzi, Parviz and Azimifar, Zohreh},
  doi          = {10.1007/s13735-022-00246-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {619-638},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Semantic-aware visual scene representation},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FCT: Fusing CNN and transformer for scene classification.
<em>IJMIR</em>, <em>11</em>(4), 611–618. (<a
href="https://doi.org/10.1007/s13735-022-00252-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification based on convolutional neural networks (CNNs) has achieved great success in recent years. In CNNs, the convolution operation performs well in extracting local features, but its ability to capture global feature representations is limited. In vision transformer (ViT), the self-attention mechanism can capture long-term feature dependencies, but it breaks down the details of local features. In this work, we make full use of the advantages of the CNN and ViT and propose a Transformer-based framework that combines CNN to improve the discriminative ability of features for scene classification. Specifically, we take the deep convolutional feature as the input and establish the scene Transformer module to extract the global feature in the scene image. An end-to-end scene classification framework called the FCT is built by fusing the CNN and scene Transformer module. Experimental results show that our FCT achieves a new state-of-the-art performance on two standard benchmarks MIT Indoor 67 and SUN 397, with the accuracy of 90.75% and 77.50%, respectively.},
  archive      = {J_IJMIR},
  author       = {Xie, Yuxiang and Yan, Jie and Kang, Lai and Guo, Yanming and Zhang, Jiahui and Luan, Xidao},
  doi          = {10.1007/s13735-022-00252-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {611-618},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {FCT: Fusing CNN and transformer for scene classification},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FDAM: Full-dimension attention module for deep convolutional
neural networks. <em>IJMIR</em>, <em>11</em>(4), 599–610. (<a
href="https://doi.org/10.1007/s13735-022-00248-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism is an important component of cross-modal research. It can improve the performance of convolutional neural networks by distinguishing the informative parts of the feature map from the useless ones. Various kinds of attention are proposed by recent studies. Different attentions use distinct division method to weight each part of the feature map. In this paper, we propose a full-dimension attention module, which is a lightweight, fully interactive 3-D attention mechanism. FDAM generates 3-D attention maps for both spatial and channel dimensions in parallel and then multiplies them to the feature map. It is difficult to obtain discriminative attention map cell under channel interaction at a low computational cost. Therefore, we adapt a generalized Elo rating mechanism to generate cell-level attention maps. We store historical information with a slight amount of non-training parameters to spread the computation over each training iteration. The proposed module can be seamlessly integrated into the end-to-end training of the CNN framework. Experiments demonstrate that it outperforms many existing attention mechanisms on different network structures and datasets for computer vision tasks, such as image classification and object detection.},
  archive      = {J_IJMIR},
  author       = {Cai, Silin and Wang, Changping and Ding, Jiajun and Yu, Jun and Fan, Jianping},
  doi          = {10.1007/s13735-022-00248-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {599-610},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {FDAM: Full-dimension attention module for deep convolutional neural networks},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TCKGE: Transformers with contrastive learning for knowledge
graph embedding. <em>IJMIR</em>, <em>11</em>(4), 589–597. (<a
href="https://doi.org/10.1007/s13735-022-00256-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning of knowledge graphs has emerged as a powerful technique for various downstream tasks. In recent years, numerous research efforts have been made for knowledge graphs embedding. However, previous approaches usually have difficulty dealing with complex multi-relational knowledge graphs due to their shallow network architecture. In this paper, we propose a novel framework named Transformers with Contrastive learning for Knowledge Graph Embedding (TCKGE), which aims to learn complex semantics in multi-relational knowledge graphs with deep architectures. To effectively capture the rich semantics of knowledge graphs, our framework leverages the powerful Transformers to build a deep hierarchical architecture to dynamically learn the embeddings of entities and relations. To obtain more robust knowledge embeddings with our deep architecture, we design a contrastive learning scheme to facilitate optimization by exploring the effectiveness of several different data augmentation strategies. The experimental results on two benchmark datasets show the superior of TCKGE over state-of-the-art models.},
  archive      = {J_IJMIR},
  author       = {Zhang, Xiaowei and Fang, Quan and Hu, Jun and Qian, Shengsheng and Xu, Changsheng},
  doi          = {10.1007/s13735-022-00256-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {589-597},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {TCKGE: Transformers with contrastive learning for knowledge graph embedding},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video deblurring and flow-guided feature aggregation for
obstacle detection in agricultural videos. <em>IJMIR</em>,
<em>11</em>(4), 577–588. (<a
href="https://doi.org/10.1007/s13735-022-00263-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous agricultural vehicles are increasingly common on farms, where they can replace humans in tasks such as irrigation, harvesting, and weeding, reducing labor costs. Real-time obstacle avoidance is a prerequisite for their work. At present, vehicles equipped with vision sensors cannot perform end-to-end video object detection, and their accuracy is also affected by motion blur. We propose a novel agricultural obstacle detection method based on RNN and flow-guided feature aggregation, combining video deblurring and object detection tasks for joint optimization. In addition, to make full use of the region proposals, a region shared strategy is proposed to improve the efficiency of video deblurring. The proposed method can solve the common motion blur problem in agricultural video and is expected to be suitable for all kinds of obstacle detection tasks in agricultural scenes. We experimented with this method on the FieldSAFE and GOPRO datasets. Our method provides better detection performance and is computationally less costly than other methods according to experimental results.},
  archive      = {J_IJMIR},
  author       = {Cheng, Keyang and Zhu, Xuesen and Zhan, Yongzhao and Pei, Yunshen},
  doi          = {10.1007/s13735-022-00263-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {577-588},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Video deblurring and flow-guided feature aggregation for obstacle detection in agricultural videos},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-aware coreference relation network for visual dialog.
<em>IJMIR</em>, <em>11</em>(4), 567–576. (<a
href="https://doi.org/10.1007/s13735-022-00257-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging cross-media task, visual dialog assesses whether an AI agent can converse in human language based on its understanding of visual content. So the critical issue is to pay attention not only to the problem of coreference in vision, but also to the problem of coreference in and between vision and language. In this paper, we propose the multi-aware coreference relation network (MACR-Net) to solve it from both textual and visual perspectives and to do fusion in complementary awareness. Specifically, its textual coreference relation module identifies textual coreference relations based on multi-aware textual representation from textual view. Furthermore, the visual coreference relation module adaptively adjusts visual coreference relations based on contextual-aware relations representation from visual view. Finally, the multi-modals fusion module fuses multi-aware relations to get an aligned representation. Extensive experiments on the VisDial v1.0 benchmarks show that MACR-Net achieves state-of-the-art performance.},
  archive      = {J_IJMIR},
  author       = {Zhang, Zefan and Jiang, Tianling and Liu, Chunping and Ji, Yi},
  doi          = {10.1007/s13735-022-00257-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {567-576},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-aware coreference relation network for visual dialog},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Your heart rate betrays you: Multimodal learning with
spatio-temporal fusion networks for micro-expression recognition.
<em>IJMIR</em>, <em>11</em>(4), 553–566. (<a
href="https://doi.org/10.1007/s13735-022-00250-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions can convey feelings that people are trying to hide. At present, some studies on micro-expression, most of which only use the temporal or spatial information in the image to recognize micro-expressions, neglect the intrinsic features of the image. To solve this problem, we detect the subject’s heart rate in the long micro-expression videos; we extract the image’s spatio-temporal feature through a spatio-temporal network and then extract the heart rate feature using a heart rate network. A multimodal learning method that combines the heart rate and spatio-temporal features is used to recognize micro-expressions. The experimental results on CASMEII, SAMM, and SMIC show that the proposed methods’ unweighted F1-score and unweighted average recall are 0.8867 and 0.8962, respectively. The spatio-temporal fusion network combined with heart rate information provides an essential reference for multimodal approaches to micro-expression recognition.},
  archive      = {J_IJMIR},
  author       = {Zhang, Ren and He, Ning and Liu, Shengjie and Wu, Ying and Yan, Kang and He, Yuzhe and Lu, Ke},
  doi          = {10.1007/s13735-022-00250-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {553-566},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Your heart rate betrays you: Multimodal learning with spatio-temporal fusion networks for micro-expression recognition},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Who is gambling? Finding cryptocurrency gamblers using
multi-modal retrieval methods. <em>IJMIR</em>, <em>11</em>(4), 539–551.
(<a href="https://doi.org/10.1007/s13735-022-00264-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of cryptocurrencies and the remarkable development of blockchain technology, decentralized applications emerged as a revolutionary force for the Internet. Meanwhile, decentralized applications have also attracted intense attention from the online gambling community, with more and more decentralized gambling platforms created through the help of smart contracts. Compared with conventional gambling platforms, decentralized gambling has transparent rules and a low participation threshold, attracting a substantial number of gamblers. In order to discover gambling behaviors and identify the contracts and addresses involved in gambling, we propose a tool termed ETHGamDet. The tool is able to automatically detect the smart contracts and addresses involved in gambling by scrutinizing the smart contract code and address transaction records. Interestingly, we present a novel LightGBM model with memory components, which possesses the ability to learn from its own misclassifications. As a side contribution, we construct and release a large-scale gambling dataset at https://github.com/AwesomeHuang/Bitcoin-Gambling-Dataset to facilitate future research in this field. Empirically, ETHGamDet achieves a F1-score of 0.72 and 0.89 in address classification and contract classification respectively, and offers novel and interesting insights.},
  archive      = {J_IJMIR},
  author       = {Huang, Zhengjie and Liu, Zhenguang and Chen, Jianhai and He, Qinming and Wu, Shuang and Zhu, Lei and Wang, Meng},
  doi          = {10.1007/s13735-022-00264-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {539-551},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Who is gambling? finding cryptocurrency gamblers using multi-modal retrieval methods},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prototype local–global alignment network for image–text
retrieval. <em>IJMIR</em>, <em>11</em>(4), 525–538. (<a
href="https://doi.org/10.1007/s13735-022-00258-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image–text retrieval is a challenging task due to the requirement of thorough multimodal understanding and precise inter-modality relationship discovery. However, most previous approaches resort to doing global image–text alignment and neglect fine-grained correspondence. Although some works explore local region–word alignment, they usually suffer from a heavy computing burden. In this paper, we propose a prototype local–global alignment (PLGA) network for image–text retrieval by jointly performing the fine-grained local alignment and high-level global alignment. Specifically, our PLGA contains two key components: a prototype-based local alignment module and a multi-scale global alignment module. The former enables efficient fine-grained local matching by combining region–prototype alignment and word–prototype alignment, and the latter helps perceive hierarchical global semantics by exploring multi-scale global correlations between the image and text. Overall, the local and global alignment modules can boost their performances for each other via the unified model. Quantitative and qualitative experimental results on Flickr30K and MS-COCO benchmarks demonstrate that our proposed approach performs favorably against state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Meng, Lingtao and Zhang, Feifei and Zhang, Xi and Xu, Changsheng},
  doi          = {10.1007/s13735-022-00258-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {525-538},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Prototype local–global alignment network for image–text retrieval},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on cross-modal retrieval and analysis.
<em>IJMIR</em>, <em>11</em>(4), 523–524. (<a
href="https://doi.org/10.1007/s13735-022-00265-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMIR},
  author       = {Wu, Jianlong and Hong, Richang and Tian, Qi},
  doi          = {10.1007/s13735-022-00265-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {523-524},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Special issue on cross-modal retrieval and analysis},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human pose estimation using deep learning: Review,
methodologies, progress and future research directions. <em>IJMIR</em>,
<em>11</em>(4), 489–521. (<a
href="https://doi.org/10.1007/s13735-022-00261-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) has developed over the past decade into a vibrant field for research with a variety of real-world applications like 3D reconstruction, virtual testing and re-identification of the person. Information about human poses is also a critical component in many downstream tasks, such as activity recognition and movement tracking. This review focuses on the key aspects of deep learning in the development of both 2D &amp; 3D HPE. It provides detailed information on the variety of databases, performance metrics and human body models incorporated for implementing HPE methodologies. This paper discusses variety of applications of HPE across domains like activity recognition, animation and gaming, virtual reality, video tracking, etc. The paper presents an analytical study of all the major works that use deep learning methods for various downstream tasks in each domain for both 2D &amp; 3D HPE. Finally, it discusses issues and limitations in the current topic of HPE and recommend potential future research directions in order to make meaningful progress in this area.},
  archive      = {J_IJMIR},
  author       = {Kumar, Pranjal and Chauhan, Siddhartha and Awasthi, Lalit Kumar},
  doi          = {10.1007/s13735-022-00261-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {489-521},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Human pose estimation using deep learning: Review, methodologies, progress and future research directions},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive self-supervised learning: Review, progress,
challenges and future research directions. <em>IJMIR</em>,
<em>11</em>(4), 461–488. (<a
href="https://doi.org/10.1007/s13735-022-00245-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, deep supervised learning has had tremendous success. However, its flaws, such as its dependency on manual and costly annotations on large datasets and being exposed to attacks, have prompted researchers to look for alternative models. Incorporating contrastive learning (CL) for self-supervised learning (SSL) has turned out as an effective alternative. In this paper, a comprehensive review of CL methodology in terms of its approaches, encoding techniques and loss functions is provided. It discusses the applications of CL in various domains like Natural Language Processing (NLP), Computer Vision, speech and text recognition and prediction. The paper presents an overview and background about SSL for understanding the introductory ideas and concepts. A comparative study for all the works that use CL methods for various downstream tasks in each domain is performed. Finally, it discusses the limitations of current methods, as well as the need for additional techniques and future directions in order to make meaningful progress in this area.},
  archive      = {J_IJMIR},
  author       = {Kumar, Pranjal and Rawat, Piyush and Chauhan, Siddhartha},
  doi          = {10.1007/s13735-022-00245-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {461-488},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Contrastive self-supervised learning: Review, progress, challenges and future research directions},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified approach of detecting misleading images via
tracing its instances on web and analyzing its past context for the
verification of multimedia content. <em>IJMIR</em>, <em>11</em>(3),
445–459. (<a href="https://doi.org/10.1007/s13735-022-00235-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The verification of multimedia content over social media is one of the challenging and crucial issues in the current scenario and gaining prominence in an age where user-generated content and online social web-platforms are the leading sources in shaping and propagating news stories. As these sources allow users to share their opinions without restriction, opportunistic users often post misleading/unreliable content on social media such as Twitter, Facebook, etc. At present, to lure users toward the news story, the text is often attached with some multimedia content (images/videos/audios). Verifying these contents to maintain the credibility and reliability of social media information is of paramount importance. Motivated by this, we proposed a generalized system that supports the automatic classification of images into credible or misleading. In this paper, we investigated machine learning-based as well as deep learning-based approaches utilized to verify misleading multimedia content, where the available image traces are used to identify the credibility of the content. The experiment is performed on the real-world dataset (Media-eval-2015 dataset) collected from Twitter. It also demonstrates the efficiency of our proposed approach and features using both Machine and Deep Learning Model (Bi-directional LSTM). The experiment result reveals that the Microsoft BING image search engine is quite effective in retrieving titles and performs better than our study&#39;s Google image search engine. It also shows that gathering clues from attached multimedia content (image) is more effective than detecting only posted content-based features.},
  archive      = {J_IJMIR},
  author       = {Varshney, Deepika and Vishwakarma, Dinesh Kumar},
  doi          = {10.1007/s13735-022-00235-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {445-459},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A unified approach of detecting misleading images via tracing its instances on web and analyzing its past context for the verification of multimedia content},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How can users’ comments posted on social media videos be a
source of effective tags? <em>IJMIR</em>, <em>11</em>(3), 431–443. (<a
href="https://doi.org/10.1007/s13735-022-00238-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a new approach for the extraction of tags from users’ comments made about videos. In fact, videos on the social media, like Facebook and YouTube, are usually accompanied by comments where users may give opinions about things evoked in the video. The main challenge is how to extract relevant tags from them. To the best of the authors’ knowledge, this is the first research work to present an approach to extract tags from comments posted about videos on the social media. We do not pretend that comments can be a perfect solution for tagging videos since we rather tried to investigate the reliability of comments to tag videos and we studied how they can serve as a source of tags. The proposed approach is based on filtering the comments to retain only the words that could be possible tags. We relied on the self-organizing map clustering considering that tags of a given video are semantically and contextually close. We tested our approach on the Google YouTube 8M dataset, and the achieved results show that we can rely on comments to extract tags. They could be also used to enrich and refine the existing uploaders’ tags as a second area of application. This can mitigate the bias effect of the uploader’s tags which are generally subjective.},
  archive      = {J_IJMIR},
  author       = {Ellouze, Mehdi},
  doi          = {10.1007/s13735-022-00238-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {431-443},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {How can users’ comments posted on social media videos be a source of effective tags?},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InceptionDepth-wiseYOLOv2: Improved implementation of YOLO
framework for pedestrian detection. <em>IJMIR</em>, <em>11</em>(3),
409–430. (<a href="https://doi.org/10.1007/s13735-022-00239-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is one of the most challenging research areas in computer vision, as it involves classifying the image and localizing the pedestrian. Its applications, especially in automated surveillance and robotics, are exceedingly sought-after. Compared to traditional hand-crafted methods, convolutional neural networks (CNNs) have superior detection results. The single-stage detection networks, particularly the You Only Look Once (YOLO) network, have attained a satisfactory performance in object detection without compromising the computation speed and are among the state-of-the-art CNN-based methods. The YOLO framework can be leveraged to use in pedestrian detection as well. In this work, we propose an improved YOLOv2, called InceptionDepth-wiseYOLOv2. The proposed model uses a modified DarkNet53 engineered for a robust feature formation. Three inception depth-wise convolution modules are integrated at varying levels in DarkNet53, leading to a comprehensive feature of an object in the image. The proposed method is compared with state-of-the-art detection methods, i.e., FasterRCNN, YOLOv2 with various base networks, YOLOv3, and Single Shot Multibox Detector. Detection Error Trade-off Curve, Precision–Recall Curve, Log Average Miss Rate, and Average Precision performance metrics are used to compare the methods. The analysis for the count of pedestrians detected concerning their height is also carried out. The experimental study used three benchmark pedestrian datasets: the INRIA Pedestrian, PASCAL VOC 2012, and Caltech Pedestrian.},
  archive      = {J_IJMIR},
  author       = {Panigrahi, Sweta and Raju, U. S. N.},
  doi          = {10.1007/s13735-022-00239-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {409-430},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {InceptionDepth-wiseYOLOv2: Improved implementation of YOLO framework for pedestrian detection},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGBD deep multi-scale network for background subtraction.
<em>IJMIR</em>, <em>11</em>(3), 395–407. (<a
href="https://doi.org/10.1007/s13735-022-00232-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel deep learning model called deep multi-scale network (DMSN) for background subtraction. This convolutional neural network is built to use RGB color channels and Depth maps as inputs with which it can fuse semantic and spatial information. In comparison with previous deep learning background subtraction techniques that lack information due to its use of only RGB channels, our RGBD version is able to overcome most of the drawbacks, especially in some particular kinds of challenges. Further, this paper introduces a new protocol for the SBM-RGBD dataset, concerning scene-independent evaluation, dedicated to Deep Learning methods to set up a competitive platform that includes more challenging situations. The proposed method proved its efficiency in solving the background subtraction in complex situations at different levels. The experimental results verify that the proposed work outperforms the state of the art on SBM-RGBD and GSM datasets.},
  archive      = {J_IJMIR},
  author       = {Houhou, Ihssane and Zitouni, Athmane and Ruichek, Yassine and Bekhouche, Salah Eddine and Kas, Mohamed and Taleb-Ahmed, Abdelmalik},
  doi          = {10.1007/s13735-022-00232-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {395-407},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {RGBD deep multi-scale network for background subtraction},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Music emotion recognition based on segment-level two-stage
learning. <em>IJMIR</em>, <em>11</em>(3), 383–394. (<a
href="https://doi.org/10.1007/s13735-022-00230-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most Music Emotion Recognition (MER) tasks, researchers tend to use supervised learning models based on music features and corresponding annotation. However, few researchers have considered applying unsupervised learning approaches to labeled data except for feature representation. In this paper, we propose a segment-based two-stage model combining unsupervised learning and supervised learning. In the first stage, we split each music excerpt into contiguous segments and then utilize an autoencoder to generate segment-level feature representation. In the second stage, we feed these time-series music segments to a bidirectional long short-term memory deep learning model to achieve the final music emotion classification. Compared with the whole music excerpts, segments as model inputs could be the proper granularity for model training and augment the scale of training samples to reduce the risk of overfitting during deep learning. Apart from that, we also apply frequency and time masking to segment-level inputs in the unsupervised learning part to enhance training performance. We evaluate our model on two datasets. The results show that our model outperforms state-of-the-art models, some of which even use multimodal architectures. And the performance comparison also evidences the effectiveness of audio segmentation and the autoencoder with masking in an unsupervised way.},
  archive      = {J_IJMIR},
  author       = {He, Na and Ferguson, Sam},
  doi          = {10.1007/s13735-022-00230-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {383-394},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Music emotion recognition based on segment-level two-stage learning},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-enhanced discriminative embedding learning for
cross-modal retrieval. <em>IJMIR</em>, <em>11</em>(3), 369–382. (<a
href="https://doi.org/10.1007/s13735-022-00237-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval requires the retrieval from image to text and vice versa. Most existing methods leverage attention mechanism to explore advanced encoding network and utilize the ranking losses to reduce modal gap. Although these methods have achieved remarkable performance, they still suffer from some drawbacks that hinder the model from learning discriminative semantic embeddings. For example, the attention mechanism may assign larger weights to irrelevant parts than relevant parts, which prevents the model from learning discriminative attention distribution. In addition, traditional ranking losses could disregard relatively discriminative information due to the lack of appropriate hardest negative sample mining and information weighting schemes. In this paper, in order to alleviate these issues, a novel semantic-enhanced discriminative embedding learning method is proposed to enhance the discriminative ability of the model, which mainly consists of three modules. The attention-guided erasing module enables the attention model pay more attention to the relevant parts and reduce the interferences of irrelevant parts by erasing non-attention parts. The large-scale negative sampling module leverages momentum-updated memory banks to expand the number of negative samples, which helps increase the probability of hardest negative being sampled. Moreover, the weighted InfoNCE loss module designs a weighted scheme to assign a larger weight to a harder pair. We evaluate the proposed modules by integrating them into three existing cross-modal retrieval models. Extensive experiments demonstrate that integrating each proposed module to the existing models can steadily improve the performance of all models.},
  archive      = {J_IJMIR},
  author       = {Pan, Hao and Huang, Jun},
  doi          = {10.1007/s13735-022-00237-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {369-382},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Semantic-enhanced discriminative embedding learning for cross-modal retrieval},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative adversarial networks and its applications in the
biomedical image segmentation: A comprehensive survey. <em>IJMIR</em>,
<em>11</em>(3), 333–368. (<a
href="https://doi.org/10.1007/s13735-022-00240-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements with deep generative models have proven significant potential in the task of image synthesis, detection, segmentation, and classification. Segmenting the medical images is considered a primary challenge in the biomedical imaging field. There have been various GANs-based models proposed in the literature to resolve medical segmentation challenges. Our research outcome has identified 151 papers; after the twofold screening, 138 papers are selected for the final survey. A comprehensive survey is conducted on GANs network application to medical image segmentation, primarily focused on various GANs-based models, performance metrics, loss function, datasets, augmentation methods, paper implementation, and source codes. Secondly, this paper provides a detailed overview of GANs network application in different human diseases segmentation. We conclude our research with critical discussion, limitations of GANs, and suggestions for future directions. We hope this survey is beneficial and increases awareness of GANs network implementations for biomedical image segmentation tasks.},
  archive      = {J_IJMIR},
  author       = {Iqbal, Ahmed and Sharif, Muhammad and Yasmin, Mussarat and Raza, Mudassar and Aftab, Shabib},
  doi          = {10.1007/s13735-022-00240-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {333-368},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Generative adversarial networks and its applications in the biomedical image segmentation: A comprehensive survey},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Organ segmentation from computed tomography images using the
3D convolutional neural network: A systematic review. <em>IJMIR</em>,
<em>11</em>(3), 315–331. (<a
href="https://doi.org/10.1007/s13735-022-00242-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography images are scans that combine a series of X-rays with computer processing techniques to display organs in the body. Recently, 3D CNN models have become effective in tasks relating to recognition, delineation, and classification. Therefore we propose a review to summarize different 3D CNN algorithms for segmenting organs in computed tomography images. This work systematically applies a two-stage procedure for review. A thorough screening of abstracts and titles to ascertain their relevance was done. Research papers published in the academic repositories were selected, analyzed, and reviewed. Insight relating to 3D organ segmentation is provided, with content such as database usage, disadvantages, and advantages. A comparison of two accuracies was carried out with a graph depicting database categories. Important insights, limitations, observations, and future directions were elucidated. After careful investigation, we observe that the encoder-decoder network is predominant for segmentation. The encoder-decoder framework provides a seamless procedure to segment CT images. A prediction of future trends with insightful recommendations for researchers is proposed. Finally, findings suggest that CNN algorithms produce good accuracies despite their limitations.},
  archive      = {J_IJMIR},
  author       = {Ilesanmi, Ademola E. and Ilesanmi, Taiwo and Idowu, Oluwagbenga P. and Torigian, Drew A. and Udupa, Jayaram K.},
  doi          = {10.1007/s13735-022-00242-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {315-331},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Organ segmentation from computed tomography images using the 3D convolutional neural network: A systematic review},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text detection, recognition, and script identification in
natural scene images: A review. <em>IJMIR</em>, <em>11</em>(3), 291–314.
(<a href="https://doi.org/10.1007/s13735-022-00243-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text in natural scene images plays a vital role in scene understanding. It contains a rich and abundant amount of valuable semantic information useful in many applications such as analysis of products’ labels, autonomous driving, and blind navigation. Consequently, detection, recognition, and identification of scripts of texts present in scene images have recently received massive attention. This paper intends to walk through the advances on the mentioned topics, mainly focusing on the approaches proposed in the last 8–10 years. As per our knowledge, this paper is the first to provide a review on the scene text script identification. We also provide a clear and precise classification between conventional-, deep learning-, and hybrid-based methods, including their advantages and disadvantages. State-of-the-art evaluation metrics, benchmark datasets’ characteristics, and performances of the existing methods are also analyzed and discussed. Lastly, we present an insight into potential research directions to complete the review. We hope this review will provide a brief insight for the researchers into scene text understanding.},
  archive      = {J_IJMIR},
  author       = {Naosekpam, Veronica and Sahu, Nilkanta},
  doi          = {10.1007/s13735-022-00243-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {291-314},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Text detection, recognition, and script identification in natural scene images: A review},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A literature review and perspectives in deepfakes:
Generation, detection, and applications. <em>IJMIR</em>, <em>11</em>(3),
219–289. (<a href="https://doi.org/10.1007/s13735-022-00241-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, with the advancement of deep learning methods, especially Generative Adversarial Networks (GANs) and Variational Auto-encoders (VAEs), fabricated content has become more realistic and believable to the naked eye. Deepfake is one such emerging technology that allows the creation of highly realistic, believable synthetic content. On the one hand, Deepfake has paved the way for highly advanced applications in various fields like advertising, creative arts, and film productions. On the other hand, it poses a threat to various Multimedia Information Retrieval Systems (MIPR) such as face recognition and speech recognition systems and has more significant societal implications in spreading misleading information. This paper aims to assist an individual in understanding the deepfake technology (along with its application), current state-of-the-art methods and gives an idea about the future pathway of this technology. In this paper, we have presented a comprehensive literature survey on the application of deepfakes, followed by discussions on state-of-the-art methods for deepfake generation and detection for three media: Image, Video, and Audio. Next, we have extensively discussed the architectural components and dataset used for various methods of deepfakes. Furthermore, we discuss the various limitations and open challenges of deepfakes to identify the research gaps in this field. Finally, discuss the conclusion and future directions to explore the potential of this technology in the coming years.},
  archive      = {J_IJMIR},
  author       = {Dagar, Deepak and Vishwakarma, Dinesh Kumar},
  doi          = {10.1007/s13735-022-00241-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {219-289},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A literature review and perspectives in deepfakes: Generation, detection, and applications},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain image retrieval: Methods and applications.
<em>IJMIR</em>, <em>11</em>(3), 199–218. (<a
href="https://doi.org/10.1007/s13735-022-00244-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain images have been witnessed in an increasing number of applications. This new trend triggers demands for cross-domain image retrieval (CDIR), which finds images in one visual domain according to a query image from another visual domain. Although image retrieval has been studied extensively, exploration of the CDIR remains at its initial stage. This study systematically surveys the methods and applications of the CDIR. Since images from different visual domains exhibit different features, learning discriminative feature representations while preserving domain-invariant features of images from different visual domains is the main challenge of the CDIR. According to the feature transformation stage of images from different visual domains, existing CDIR methods are categorized and analyzed. One is based on feature space migration and the other is based on image domain migration. Then, applications of CDIR in clothing, infrared, remote sensing, sketch, and other scenarios are summarized. Finally, the existing CDIR schemes are concluded, and new directions for future research are proposed.},
  archive      = {J_IJMIR},
  author       = {Zhou, Xiaoping and Han, Xiangyu and Li, Haoran and Wang, Jia and Liang, Xun},
  doi          = {10.1007/s13735-022-00244-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {199-218},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Cross-domain image retrieval: Methods and applications},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few2Decide: Towards a robust model via using few neuron
connections to decide. <em>IJMIR</em>, <em>11</em>(2), 189–198. (<a
href="https://doi.org/10.1007/s13735-021-00223-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches have shown that image classification networks are vulnerable to adversarial examples, which seriously limits their application in safely critical scenarios. Existing defense methods usually employ adversarial training or adjust the network structure to resist adversarial attack. Although these defense methods can improve the model robustness to some extent, they often significantly decrease the accuracy on the clean data and bring additional computational cost. In this work, we analyze the impact of adversarial example on neuron connections and propose a Few2Decide method to train a robust model by dropping part of non-robust connections in the fully connected layer. Our model can get high perturbed data accuracy without increasing trainable parameters, meanwhile, get high clean data accuracy. Experimental results prove that our method can provide a robust model and achieve state-of-the-art performance on the CIFAR-10 dataset. Specifically, our Few2Decide method achieves 73.01% adversarial accuracy on the CIFAR-10 dataset under the challenging untargeted attack in white-box settings with an attack strength 8/255, using ResNet-20[4 $$\times $$ ] architecture.},
  archive      = {J_IJMIR},
  author       = {Li, Jian and Guo, Yanming and Lao, Songyang and Zhao, Xiang and Bai, Liang and Wang, Haoran},
  doi          = {10.1007/s13735-021-00223-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {189-198},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Few2Decide: Towards a robust model via using few neuron connections to decide},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PDS-net: A novel point and depth-wise separable convolution
for real-time object detection. <em>IJMIR</em>, <em>11</em>(2), 171–188.
(<a href="https://doi.org/10.1007/s13735-022-00229-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous recent object detectors and classifiers have shown acceptable performance in recent years by using convolutional neural networks and other efficient architectures. However, most of them continue to encounter difficulties like overfitting, increased computational costs, and low efficiency and performance in real-time scenarios. This paper proposes a new lightweight model for detecting and classifying objects in images. This model presents a backbone for extracting in-depth features and a spatial feature pyramid network (SFPN) for accurately detecting and categorizing objects. The proposed backbone uses point-wise separable (PWS) and depth-wise separable convolutions, which are more efficient than standard convolution. The PWS convolution utilizes a residual shortcut link to reduce computation time. We also propose a SFPN that comprises concatenation, transformer encoder–decoder, and feature fusion modules, which enables the simultaneous processing of multi-scale features, the extraction of low-level characteristics, and the creation of a pyramid of features to increase the effectiveness of the proposed model. The proposed model outperforms all of the existing backbones for object detection and classification in three publicly accessible datasets: PASCAL VOC 2007, PASCAL VOC 2012, and MS-COCO. Our extensive experiments show that the proposed model outperforms state-of-the-art detectors, with mAP improvements of 2.4% and 2.5% on VOC 2007, 3.0% and 2.6% on VOC 2012, and 2.5% and 3.6% on MS-COCO in the small and large sizes of the images, respectively. In the MS-COCO dataset, our model achieves FPS of 39.4 and 33.1 in a single GPU for the small ( $$320 \times 320$$ ) and large ( $$512 \times 512$$ ) sizes of the images, respectively, which shows that our method can run in real-time.},
  archive      = {J_IJMIR},
  author       = {Junayed, Masum Shah and Islam, Md Baharul and Imani, Hassan and Aydin, Tarkan},
  doi          = {10.1007/s13735-022-00229-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {171-188},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {PDS-net: A novel point and depth-wise separable convolution for real-time object detection},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Siamese coding network and pair similarity prediction for
near-duplicate image detection. <em>IJMIR</em>, <em>11</em>(2), 159–170.
(<a href="https://doi.org/10.1007/s13735-022-00233-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-duplicate detection in a dataset involves finding the elements that are closest to a new query element according to a given similarity function and proximity threshold. The brute force approach is very computationally intensive as it evaluates the similarity between the queried item and all items in the dataset. The potential application domain is an image sharing website that checks for plagiarism or piracy every time a new image is uploaded. Among the various approaches, near-duplicate detection was effectively addressed by SimPair LSH (Fisichella et al., in Decker, Lhotská, Link, Spies, Wagner (eds) Database and expert systems applications, Springer, 2014). As the name suggests, SimPair LSH uses locality sensitive hashing (LSH) and computes and stores in advance a small set of near-duplicate pairs present in the dataset and uses them to reduce the candidate set returned for a given query using the Triangle inequality. We develop an algorithm that predicts how the candidate set will be reduced. We also develop a new efficient method for near-duplicate image detection using a deep Siamese coding neural network that is able to extract effective features from images useful for building LSH indices. Extensive experiments on two benchmark datasets confirm the effectiveness of our deep Siamese coding network and prediction algorithm.},
  archive      = {J_IJMIR},
  author       = {Fisichella, Marco},
  doi          = {10.1007/s13735-022-00233-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {159-170},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Siamese coding network and pair similarity prediction for near-duplicate image detection},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A local representation-enhanced recurrent convolutional
network for image captioning. <em>IJMIR</em>, <em>11</em>(2), 149–157.
(<a href="https://doi.org/10.1007/s13735-022-00231-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is a challenging task that aims to generate a natural description for an image. The word prediction is dependent on local linguistic contexts and fine-grained visual information and is also guided by previous linguistic tokens. However, current captioning works do not fully utilize local visual and linguistic information, generating coarse or incorrect descriptions. Also, captioning decoders have less recently focused on convolutional neural network (CNN), which has the advantage in extracting features. To solve these problems, we propose a local representation-enhanced recurrent convolutional network (Lore-RCN). Specifically, we propose a visual convolutional network to obtain enhanced local linguistic context, which incorporates selected local visual information and models short-term neighboring. Furthermore, we propose a linguistic convolutional network to obtain enhanced linguistic representation, which models long- and short-term correlations explicitly to leverage guiding information from previous linguistic tokens. Experiments conducted on COCO and Flickr30k datasets have verified the superiority of our proposed recurrent CNN-based model.},
  archive      = {J_IJMIR},
  author       = {Wang, Xiaoyi and Huang, Jun},
  doi          = {10.1007/s13735-022-00231-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {149-157},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A local representation-enhanced recurrent convolutional network for image captioning},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-sensor human activity recognition using CNN and GRU.
<em>IJMIR</em>, <em>11</em>(2), 135–147. (<a
href="https://doi.org/10.1007/s13735-022-00234-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era of rapid technological innovation, human activity recognition (HAR) has emerged as a principal research area in the field of multimedia information retrieval. The capacity to monitor people remotely is a main determinant of HAR’s central role. Multiple gyroscope and accelerometer sensors can be used to aggregate data which can be used to recognise human activities—one of the key research objectives of this study. Optimal results are attained through the use of deep learning models to carry out HAR in the collected data. We propose the use of a hierarchical multi-resolution convolutional neural networks in combination with gated recurrent uni. We conducted an experiment on the mHealth and UCI data sets, the results of which demonstrate the efficiency of the proposed model, as it achieved acceptable accuracies: 99.35% in the mHealth data set and 94.50% in the UCI data set.},
  archive      = {J_IJMIR},
  author       = {Nafea, Ohoud and Abdul, Wadood and Muhammad, Ghulam},
  doi          = {10.1007/s13735-022-00234-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {135-147},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-sensor human activity recognition using CNN and GRU},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DC-GNN: Drop channel graph neural network for object
classification and part segmentation in the point cloud. <em>IJMIR</em>,
<em>11</em>(2), 123–133. (<a
href="https://doi.org/10.1007/s13735-022-00236-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent years, the problem of 3D shape analysis in the point cloud is considered as one of the challenging research topics in the field of computer vision. The major issues here are effective representation of the 3D information, meaningful feature extraction and subsequent task of classification. In this research paper, a deep learning-based network called Drop Channel Graph Neural Network (DC-GNN) is proposed for object classification and part segmentation. The DC-GNN model employs the idea of k-NN-based drop channel with hierarchical feature selection approach at each layer for dynamic graph construction, and further, with the help of Multi-Layer Perceptron Networks accomplishes the task of object classification. The same DC-GNN model is extended to carry out part segmentation in the point cloud data using the ShapeNet-Part benchmark dataset. The proposed network reports the state-of-the-art classification accuracy of 93.64% with ModelNet-40 dataset (Source-Code- https://github.com/merazlab/DC-GNN ).},
  archive      = {J_IJMIR},
  author       = {Meraz, Md and Ansari, Md Afzal and Javed, Mohammed and Chakraborty, Pavan},
  doi          = {10.1007/s13735-022-00236-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {123-133},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DC-GNN: Drop channel graph neural network for object classification and part segmentation in the point cloud},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Caption TLSTMs: Combining transformer with LSTMs for image
captioning. <em>IJMIR</em>, <em>11</em>(2), 111–121. (<a
href="https://doi.org/10.1007/s13735-022-00228-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image to captions has attracted widespread attention over the years. Recurrent neural networks (RNN) and their corresponding variants have been the mainstream when it comes to dealing with image captioning task for a long time. However, transformer-based models have shown powerful and promising performance on visual tasks contrary to classic neural networks. In order to extract richer and more robust multimodal intersection feature representation, we improve the original abstract scene graph to caption model and propose the Caption TLSTMs which is made up of two LSTMs with Transformer blocks in the middle of them in this paper. Compared with the model before improvement, the architecture of our Caption TLSTMs enables the entire network to make the most of the long-term dependencies and feature representation ability of the LSTM, while encoding the multimodal textual, visual and graphic information with the transformer blocks as well. Finally, experiments on VisualGenome and MSCOCO datasets have shown good performance in improving the general image caption generation quality, demonstrating the effectiveness of the Caption TLSTMs model.},
  archive      = {J_IJMIR},
  author       = {Yan, Jie and Xie, Yuxiang and Luan, Xidao and Guo, Yanming and Gong, Quanzhi and Feng, Suru},
  doi          = {10.1007/s13735-022-00228-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {111-121},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Caption TLSTMs: Combining transformer with LSTMs for image captioning},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection using edge computing in video surveillance
system: review. <em>IJMIR</em>, <em>11</em>(2), 85–110. (<a
href="https://doi.org/10.1007/s13735-022-00227-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current concept of smart cities influences urban planners and researchers to provide modern, secured and sustainable infrastructure and gives a decent quality of life to its residents. To fulfill this need, video surveillance cameras have been deployed to enhance the safety and well-being of the citizens. Despite technical developments in modern science, abnormal event detection in surveillance video systems is challenging and requires exhaustive human efforts. In this paper, we focus on evolution of anomaly detection followed by survey of various methodologies developed to detect anomalies in intelligent video surveillance. Further, we revisit the surveys on anomaly detection in the last decade. We then present a systematic categorization of methodologies for anomaly detection. As the notion of anomaly depends on context, we identify different objects-of-interest and publicly available datasets in anomaly detection. Since anomaly detection is a time-critical application of computer vision, we explore the anomaly detection using edge devices and approaches explicitly designed for them. The confluence of edge computing and anomaly detection for real-time and intelligent surveillance applications is also explored. Further, we discuss the challenges and opportunities involved in anomaly detection using the edge devices.},
  archive      = {J_IJMIR},
  author       = {Patrikar, Devashree R. and Parate, Mayur Rajaram},
  doi          = {10.1007/s13735-022-00227-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {85-110},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Anomaly detection using edge computing in video surveillance system: Review},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal image and audio music transcription.
<em>IJMIR</em>, <em>11</em>(1), 77–84. (<a
href="https://doi.org/10.1007/s13735-021-00221-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Music Recognition (OMR) and Automatic Music Transcription (AMT) stand for the research fields that aim at obtaining a structured digital representation from sheet music images and acoustic recordings, respectively. While these fields have traditionally evolved independently, the fact that both tasks may share the same output representation poses the question of whether they could be combined in a synergistic manner to exploit the individual transcription advantages depicted by each modality. To evaluate this hypothesis, this paper presents a multimodal framework that combines the predictions from two neural end-to-end OMR and AMT systems by considering a local alignment approach. We assess several experimental scenarios with monophonic music pieces to evaluate our approach under different conditions of the individual transcription systems. In general, the multimodal framework clearly outperforms the single recognition modalities, attaining a relative improvement close to $$40\%$$ in the best case. Our initial premise is, therefore, validated, thus opening avenues for further research in multimodal OMR-AMT transcription.},
  archive      = {J_IJMIR},
  author       = {de la Fuente, Carlos and Valero-Mas, Jose J. and Castellanos, Francisco J. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s13735-021-00221-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {77-84},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multimodal image and audio music transcription},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing the performance of 3D auto-correlation gradient
features in depth action classification. <em>IJMIR</em>, <em>11</em>(1),
61–76. (<a href="https://doi.org/10.1007/s13735-021-00226-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D auto-correlation gradient features have demonstrated only limited success on depth action data, whereas the 2D auto-correlation gradient features have been successful in the domain. In this paper, we propose to calculate three depth motion map sequences from each depth action video by accumulating only the motion information of the action. We then obtain the three vectors of 3D auto-correlation gradient features by applying the space-time auto-correlation of gradients (STACOG) descriptor on the depth motion map sequences. The three vectors are then concatenated and passed to an unsupervised classifier to recognize the action. The experimental evaluation on four public datasets (MSR-Action3D, DHA, UTD-MHAD, and MSR-Gesture3D dataset) demonstrates the superiority of our proposed method over state-of-the-art methods.},
  archive      = {J_IJMIR},
  author       = {Bulbul, Mohammad Farhad and Islam, Saiful and Azme, Zannatul and Pareek, Preksha and Kabir, Md. Humaun and Ali, Hazrat},
  doi          = {10.1007/s13735-021-00226-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {61-76},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Enhancing the performance of 3D auto-correlation gradient features in depth action classification},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast and robust affine-invariant method for shape
registration under partial occlusion. <em>IJMIR</em>, <em>11</em>(1),
39–59. (<a href="https://doi.org/10.1007/s13735-021-00224-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acquisition of the planar images of the same object may be considerably different due to viewpoint dependencies, which influences the shape extraction, hence possibly making the curves partially visible and often accompanied by perspective distortions. In this paper, we propose a new contour alignment system relating to the special affine transformations that contain rotations and stretches, useful for describing planar contours which move in three-dimensional space and which are far enough away from the camera. The registration system that we suggest here includes a first optimization step relating to the dataset concerned. It consists in optimizing the number of correspondence points N between the curves to be registered. This is achieved by minimizing the conditioning of the correspondence matrix which is obtained by matching the re-sampling points by the equi-affine length of the two curves. This correspondence matrix is calculated for all the pairs of curves of the dataset by varying N. After extracting the optimal value of N, the estimation of the special affine transformation between a given couple of curves is realized by the pseudo-inverse of the correspondence matrix in the $$N_{0}$$ resolution. This approach allows both providing the best accuracy and stabilizing the results of registration. We evaluate and compare our algorithm with other existing methods under different shape variations including noise, missing parts, and articulated deformations. The experiments are conducted on several known datasets.},
  archive      = {J_IJMIR},
  author       = {Elghoul, Sinda and Ghorbel, Faouzi},
  doi          = {10.1007/s13735-021-00224-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {39-59},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A fast and robust affine-invariant method for shape registration under partial occlusion},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on deep learning in medical image analysis.
<em>IJMIR</em>, <em>11</em>(1), 19–38. (<a
href="https://doi.org/10.1007/s13735-021-00218-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ongoing improvements in AI, particularly concerning deep learning techniques, are assisting to identify, classify, and quantify patterns in clinical images. Deep learning is the quickest developing field in artificial intelligence and is effectively utilized lately in numerous areas, including medication. A brief outline is given on studies carried out on the region of application: neuro, brain, retinal, pneumonic, computerized pathology, bosom, heart, breast, bone, stomach, and musculoskeletal. For information exploration, knowledge deployment, and knowledge-based prediction, deep learning networks can be successfully applied to big data. In the field of medical image processing methods and analysis, fundamental information and state-of-the-art approaches with deep learning are presented in this paper. The primary goals of this paper are to present research on medical image processing as well as to define and implement the key guidelines that are identified and addressed.},
  archive      = {J_IJMIR},
  author       = {Suganyadevi, S. and Seethalakshmi, V. and Balasamy, K.},
  doi          = {10.1007/s13735-021-00218-1},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {19-38},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A review on deep learning in medical image analysis},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive video retrieval evaluation at a distance:
Comparing sixteen interactive video search systems in a remote setting
at the 10th video browser showdown. <em>IJMIR</em>, <em>11</em>(1),
1–18. (<a href="https://doi.org/10.1007/s13735-021-00225-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Video Browser Showdown addresses difficult video search challenges through an annual interactive evaluation campaign attracting research teams focusing on interactive video retrieval. The campaign aims to provide insights into the performance of participating interactive video retrieval systems, tested by selected search tasks on large video collections. For the first time in its ten year history, the Video Browser Showdown 2021 was organized in a fully remote setting and hosted a record number of sixteen scoring systems. In this paper, we describe the competition setting, tasks and results and give an overview of state-of-the-art methods used by the competing systems. By looking at query result logs provided by ten systems, we analyze differences in retrieval model performances and browsing times before a correct submission. Through advances in data gathering methodology and tools, we provide a comprehensive analysis of ad-hoc video search tasks, discuss results, task design and methodological challenges. We highlight that almost all top performing systems utilize some sort of joint embedding for text-image retrieval and enable specification of temporal context in queries for known-item search. Whereas a combination of these techniques drive the currently top performing systems, we identify several future challenges for interactive video search engines and the Video Browser Showdown competition itself.},
  archive      = {J_IJMIR},
  author       = {Heller, Silvan and Gsteiger, Viktor and Bailer, Werner and Gurrin, Cathal and Jónsson, Björn Þór and Lokoč, Jakub and Leibetseder, Andreas and Mejzlík, František and Peška, Ladislav and Rossetto, Luca and Schall, Konstantin and Schoeffmann, Klaus and Schuldt, Heiko and Spiess, Florian and Tran, Ly-Duyen and Vadicamo, Lucia and Veselý, Patrik and Vrochidis, Stefanos and Wu, Jiaxin},
  doi          = {10.1007/s13735-021-00225-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Interactive video retrieval evaluation at a distance: Comparing sixteen interactive video search systems in a remote setting at the 10th video browser showdown},
  volume       = {11},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
