<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin---750">APIN - 750</h2>
<ul>
<li><details>
<summary>
(2022). An efficient and scalable approach for mining subgraphs in a
single large graph. <em>APIN</em>, <em>52</em>(15), 17881–17895. (<a
href="https://doi.org/10.1007/s10489-022-03164-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many recent applications, a graph is used to simulate many complex systems, such as social networks, traffic models or bioinformatics, and the underlying graphs for these systems are very large. Algorithms for mining all frequent subgraphs from a single large graph have attracted much attention and been studied in more detail lately. Mining frequent subgraphs is important, and defined as finding all subgraphs whose occurrences in a dataset are greater than or equal to a given frequency threshold. Among frequent subgraph algorithms, GraMi is considered as the state-of-the-art approach. However, GraMi has a huge search space, and therefore still needs a lot of time and memory to process a large graph. In this paper, we propose two effective strategies to balance and reduce the search space of GraMi, which can decrease the number of candidate subgraphs generated, with early pruning of a large portion of the domain for each candidate. Our experiments were performed on four real datasets and the results show that the performance of our balancing GraMi is better than those of the original algorithm GraMi and the optimized version SoGraMi.},
  archive      = {J_APIN},
  author       = {Nguyen, Lam B. Q. and Nguyen, Loan T. T. and Vo, Bay and Zelinka, Ivan and Lin, Jerry Chun-Wei and Yun, Unil and Nguyen, Hung Son},
  doi          = {10.1007/s10489-022-03164-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17881-17895},
  shortjournal = {Appl. Intell.},
  title        = {An efficient and scalable approach for mining subgraphs in a single large graph},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video summarization with u-shaped transformer.
<em>APIN</em>, <em>52</em>(15), 17864–17880. (<a
href="https://doi.org/10.1007/s10489-022-03451-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, supervised video summarization has made tremendous progress with treating it as a sequence-to-sequence learning task. However, traditional recurrent neural networks (RNNs) have limitations in sequence modeling of long sequences, and the use of a transformer for sequence modeling requires a large number of parameters. We propose an efficient U-shaped transformer for video summarization tasks in this paper to address this issue, which we call “Uformer”. Precisely, Uformer consists of three key components: embedding, Uformer block, and prediction head. First of all, the image features sequence is represented by the pre-trained deep convolutional network, then represented by a liner embedding. The image feature sequence differences are also represented by another liner embedding and concatenate together to form a two-stream embedding feature in the embedding component. Secondly, we stack multiple transformer layers into a U-shaped block to integrate the representations learned from the previous layers. Multi-scale Uformer can not only learn longer sequence information but also reduce the number of parameters and calculations. Finally, prediction head regression the localization of the keyframes and learning the corresponding classification scores. Uformer combine with non-maximum suppression (NMS) for post-processing to get the final video summarization. We improved the F-score from 50.2% to 53.9% by 3.7% on the SumMe dataset and improved F-score from 62.1% to 63.0% by 0.9% on the TVSum dataset. Our proposed model with 0.85M parameters which are only 32.32% of DR-DSN’s parameters.},
  archive      = {J_APIN},
  author       = {Chen, Yaosen and Guo, Bing and Shen, Yan and Zhou, Renshuang and Lu, Weichen and Wang, Wei and Wen, Xuming and Suo, Xinhua},
  doi          = {10.1007/s10489-022-03451-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17864-17880},
  shortjournal = {Appl. Intell.},
  title        = {Video summarization with u-shaped transformer},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A complete framework for aspect-level and sentence-level
sentiment analysis. <em>APIN</em>, <em>52</em>(15), 17845–17863. (<a
href="https://doi.org/10.1007/s10489-022-03279-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Analysis (ABSA) and Sentence-Based Sentiment Analysis (SBSA) stand for two highly coupled study fields. Basically, the features required at the sentence level influence and depend on the aspect level and vice versa. Nevertheless, a few approaches have considered the correlation between these two tasks. This research work is interested in both aspect and sentence levels. It starts with the ABSA which is in turn divided into two strongly coupled tasks, namely the aspect extraction and the aspect sentiment classification. Indeed, integrating highly coupled tasks into an integrated model can lead to more significant performance improvement rather than in the case of separate models, which is also confirmed through the proposed ABSA model. The latter represents a unified-trained model based on deep learning techniques for extracting the aspects along with their sentiment polarities. Later on, the emphasis would be put on SBSA, which is a complex study, especially with the existence of opinions that include several aspects with opposing polarities. From this perspective, a combination of deep learning and fuzzy logic techniques was elaborated to address this issue. The hybrid model achieved satisfactory performance compared to the Bert model.},
  archive      = {J_APIN},
  author       = {Chiha, Rim and Ayed, Mounir Ben and Pereira, Célia da Costa},
  doi          = {10.1007/s10489-022-03279-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17845-17863},
  shortjournal = {Appl. Intell.},
  title        = {A complete framework for aspect-level and sentence-level sentiment analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic topic modeling for short text based on word
embedding networks. <em>APIN</em>, <em>52</em>(15), 17829–17844. (<a
href="https://doi.org/10.1007/s10489-022-03388-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncovering topics in short texts can be an arduous task. The inadequacy of general-purpose topic models for handling short documents may be explained by the difficulty in dealing with scarce context information. A variety of strategies have been proposed to address this problem, such as using application-specific information, generating larger pseudo-documents, or modeling a single topic per document. This paper introduces a novel strategy to solve this problem named Vec2Graph Topic Model (VGTM). It creates a graph-based representation for the analyzed corpus using word embeddings, named Vec2Graph, and infers topics from overlapping communities patterns on this graph. Vec2Graph leverages the semantics of word embeddings to create a dense similarity graph of words, mitigating the lack of context in short text documents. Experiments evaluating topic coherence quality in four benchmarks and two real-world datasets show that VGTM achieves the best overall results (obtaining statistically better results in 10 out of 18 experiments) in comparison with standard and state-of-the-art short text topic models. We also analyze the relationship between one of our evaluated metrics – NPMI – and structural patterns in the Vec2Graph representation. We found that networks that present a strong community structure tend to present higher NPMI values, suggesting the possibility of direct measurement and potential control of topic coherence through these network features.},
  archive      = {J_APIN},
  author       = {Pita, Marcelo and Nunes, Matheus and Pappa, Gisele L.},
  doi          = {10.1007/s10489-022-03388-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17829-17844},
  shortjournal = {Appl. Intell.},
  title        = {Probabilistic topic modeling for short text based on word embedding networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A nonlocal HEVC in-loop filter using CNN-based compression
noise estimation. <em>APIN</em>, <em>52</em>(15), 17810–17828. (<a
href="https://doi.org/10.1007/s10489-022-03259-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-efficiency video coding (HEVC) effectively reduces the amount of video data while unavoidably introducing compression noise. The in-loop filter can enhance the reconstructed frames at the encoder to prevent compression noise from transmitting to the subsequent frames. The existing incorporated in-loop filters for HEVC do not fully combine the advantages of CNN and video coding priors. Moreover, obtaining the accuracy noise level is essential for adaptively in-loop filtering. To further enhance the compressed video quality in the encoder, we propose a nonlocal in-loop filter for HEVC using a CNN-based compression noise estimation network(CNEN). In the noise estimation part, we adopt a classification network to estimate the noise of compressed HEVC videos according to video content characteristics. In the denoising part, we propose a spatial-temporal nonlocal low-rank(STNLLR) prior by simultaneously exploiting the nonlocal self-similarity of video in spatial and temporal domains. We also propose an adaptive narrow quantization constraint (ANQC) prior by limiting the reconstructed pixel values adaptively according to the quantization parameters(QPs). The experimental results show that CNEN outperforms the existing compression noise estimation methods. Furthermore, our in-loop filter can improve the quality of the HEVC reconstructed frames under AI, LDP, and RA configurations, achieving an average reduction of 4.17%, 10.46%, and 6.10% in the Bjøntegaard Delta Bit Rate (BDBR), respectively.},
  archive      = {J_APIN},
  author       = {Sun, Weiheng and He, Xiaohai and Chen, Honggang and Xiong, Shuhua and Xu, Yifei},
  doi          = {10.1007/s10489-022-03259-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17810-17828},
  shortjournal = {Appl. Intell.},
  title        = {A nonlocal HEVC in-loop filter using CNN-based compression noise estimation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight global-locally connected distillation network
for single image super-resolution. <em>APIN</em>, <em>52</em>(15),
17797–17809. (<a
href="https://doi.org/10.1007/s10489-022-03454-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As convolutional neural networks (CNNs) have been commonly applied to ill-posed single image super-resolution (SISR) task, most previous CNN-based methods made significant progress in terms of both high signal-to-noise ratios (PSNR) and structural similarity (SSIM). However, with the layers in those networks going deeper and deeper, they require more and more computing power, fail to consider distilling the feature maps. In this paper, we propose a lightweight global-locally connected distillation network, GLCDNet. Specifically, we propose a wide activation shrink-expand convolutional block whose filter channels will first shrink then expand to aggregate more information. This information will concatenate with feature maps of the previous blocks to further explore shallow information. Thus, the block will exploit statistics within most feature channels while refining useful information of features. Furthermore, together with the global-local connection method, our network is robust to benchmark datasets with high processing speed. Comparative results demonstrate that our GLCDNet achieves superior performance while keeping the parameters and speed balanced.},
  archive      = {J_APIN},
  author       = {Zeng, Cong and Li, Guangyao and Chen, Qiaochuan and Xiao, Qingguo},
  doi          = {10.1007/s10489-022-03454-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17797-17809},
  shortjournal = {Appl. Intell.},
  title        = {Lightweight global-locally connected distillation network for single image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel ground truth inference algorithm based on instance
similarity for crowdsourcing learning. <em>APIN</em>, <em>52</em>(15),
17784–17796. (<a
href="https://doi.org/10.1007/s10489-022-03433-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing system, each instance will be usually labeled multiple times by different workers. After obtaining the multiple noise labels of data, ground truth inference algorithms are used to infer unknown true labels of instances. However, most existing ground truth inference algorithms only utilize the information in the multiple noise labels themselves while ignoring the instance similarity. This paper proposes a novel ground truth inference algorithm based on instance similarity to further improve the performance of ground truth inference. Because similar instances are more likely to be clustered in the same cluster and similar instances are more likely to have similar labels, both fuzzy c-means clustering (FCM) and k-nearest neighbors algorithm (kNN) are used to explore the instance similarity in this paper. Specifically, FCM is firstly used to adjust label distributions of instances. Then the labels of instances are inferred according to their label distributions and kNN algorithm. Based on the instance similarity, the instances with reliable label distributions will influence the instances with unreliable label distributions. The experimental results on benchmark and real-world data sets validate that using the instance similarity can effectively enhance the performance of ground truth inference.},
  archive      = {J_APIN},
  author       = {Ma, Ben and Li, Chaoqun and Jiang, Liangxiao},
  doi          = {10.1007/s10489-022-03433-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17784-17796},
  shortjournal = {Appl. Intell.},
  title        = {A novel ground truth inference algorithm based on instance similarity for crowdsourcing learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlation filter tracking algorithm based on
spatial-temporal regularization and context awareness. <em>APIN</em>,
<em>52</em>(15), 17772–17783. (<a
href="https://doi.org/10.1007/s10489-022-03458-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tracker based on correlation filters can achieve effective positioning at a relatively fast speed, resulting from its operation in the frequency domain. As a result, it is commonly employed in the field of object tracking. However, this characteristic introduces boundary effect and affects the tracking performance in some scenes. In this work, a correlation filter tracking algorithm with spatial-temporal regularization and context awareness (STCACF) is proposed: (1) the spatial-temporal information and context awareness is added to the training process to mitigate the boundary effect and enhance the overall tracking performance; (2) the tracker model adopts the iterative method of alternating direction method of multipliers (ADMM), so that each subproblem can be solved in a closed-loop solution, which can realize real-time tracking; (3) the spatial regularization is employed to reduce the influence of filter degradation. Experiments on the OTB-2013, the OTB-2015 and the TC-128 benchmark datasets demonstrate that the suggested STCACF is capable of significantly improving the tracking performance compared with state-of-the-art trackers. The STCACF tracker runs at a frame rate of approximately 22 frames per second (FPS) on a single central processing unit (CPU).},
  archive      = {J_APIN},
  author       = {Wu, Xuedong and Xu, Jie and Zhu, Zhiyu and Wang, Yaonan and Zhang, Qiang and Tang, Siming and Liang, Mengquan and Cao, Baiheng},
  doi          = {10.1007/s10489-022-03458-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17772-17783},
  shortjournal = {Appl. Intell.},
  title        = {Correlation filter tracking algorithm based on spatial-temporal regularization and context awareness},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic channel pruning via clustering and swarm
intelligence optimization for CNN. <em>APIN</em>, <em>52</em>(15),
17751–17771. (<a
href="https://doi.org/10.1007/s10489-022-03508-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As convolutional neural networks (CNNs) have become increasingly deeper in recent years, the requirements for the quantity of data and hardware resources have gradually increased. CNN also reveals salient redundancy in several tasks. The existing magnitude-based pruning methods are efficient, but the performance of the compressed network is unpredictable. While the accuracy loss after pruning based on the structure sensitivity is relatively slight, the process is time-consuming, and the algorithm complexity is notable. To fully combine the advantages of the two types of methods, we propose a novel automatic channel pruning method (ACP). Specifically, we first perform layerwise channel clustering via the similarity of the feature maps to perform preliminary pruning on the network. Then, a population initialization method is introduced to transform the pruned structure into a candidate population. Finally, we conduct iterative searching and optimization based on particle swarm optimization (PSO) to find the optimal compressed structure. The compact network is then retrained to mitigate the accuracy loss from pruning. Our method is evaluated against several state-of-the-art CNNs on three different classification datasets CIFAR-10/100 and ILSVRC-2012. On the ILSVRC-2012, when removing 64.36% parameters and 63.34% floating-point operations (FLOPs) of ResNet-50, the Top-1 and Top-5 accuracy drops are less than 0.9%. Moreover, we demonstrate that without harming the overall performance, it is possible to compress the SSD by more than 50% on the target detection dataset PASCAL VOC. This further verifies that the proposed method can also be applied to other CNNs and application scenarios.},
  archive      = {J_APIN},
  author       = {Chang, Jingfei and Lu, Yang and Xue, Ping and Xu, Yiqun and Wei, Zhen},
  doi          = {10.1007/s10489-022-03508-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17751-17771},
  shortjournal = {Appl. Intell.},
  title        = {Automatic channel pruning via clustering and swarm intelligence optimization for CNN},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel intrinsic measure of data separability.
<em>APIN</em>, <em>52</em>(15), 17734–17750. (<a
href="https://doi.org/10.1007/s10489-022-03395-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, the performance of a classifier depends on both the classifier model and the separability/complexity of datasets. To quantitatively measure the separability of datasets, in this study, we propose an intrinsic measure – the Distance-based Separability Index (DSI), which is independent of the classifier model. We then formally show that the DSI can indicate whether the distributions of datasets are identical for any dimensionality. DSI can measure separability of datasets because we consider the situation in which different classes of data are mixed in the same distribution to be the most difficult for classifiers to separate. And, DSI is verified to be an effective separability measure by comparing it to state-of-the-art separability/complexity measures using synthetic datasets and real datasets (CIFAR-10/100). Having demonstrated the DSI’s ability to compare distributions of samples, our other studies show that it can be used in other separability-based applications, such as measuring the performance of generative adversarial networks (GANs) and evaluating the results of clustering methods.},
  archive      = {J_APIN},
  author       = {Guan, Shuyue and Loew, Murray},
  doi          = {10.1007/s10489-022-03395-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17734-17750},
  shortjournal = {Appl. Intell.},
  title        = {A novel intrinsic measure of data separability},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SARNet: Spatial attention residual network for pedestrian
and vehicle detection in large scenes. <em>APIN</em>, <em>52</em>(15),
17718–17733. (<a
href="https://doi.org/10.1007/s10489-022-03217-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of high-resolution camera technology, the shooting scene coverage has reached the square kilometer level, thousands of people can be observed at the same time, and the faces of people from a hundred meters away are clearly recognizable. The images captured by high-resolution cameras are very different from those captured by conventional cameras. In the face of many detection targets in high-resolution images, large differences in target scales due to spatial position, as well as difficulties in extracting features and poor detection results caused by target overlap and concealment phenomena, this paper proposes a multi-target detection method SARNet that combined with spatial attention optimization feature extraction. Use spatial attention to optimize the backbone network, expand the local receptive field, thereby enhance the representation ability, and enhance the feature extraction ability of small targets; the different scale features of the dilated feature pyramid network are subjected to the deformable region of interest pooling operation, which effectively improves the different scales detection accuracy. The experimental results show that the method proposed in this paper can get 51.9% mAP on the PANDA dataset, which is superior to the existing detection algorithms. At the same time, experimental verification of pedestrians and vehicles on the COCO2017 dataset fully proves the feasibility of the method in this paper.},
  archive      = {J_APIN},
  author       = {Wei, Hongyang and Zhang, Qianqian and Han, Jingjing and Fan, Yingying and Qian, Yurong},
  doi          = {10.1007/s10489-022-03217-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17718-17733},
  shortjournal = {Appl. Intell.},
  title        = {SARNet: Spatial attention residual network for pedestrian and vehicle detection in large scenes},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Log message anomaly detection with fuzzy c-means and MLP.
<em>APIN</em>, <em>52</em>(15), 17708–17717. (<a
href="https://doi.org/10.1007/s10489-022-03300-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log messages are one the most valuable sources of information in the cloud and other software systems. These logs can be used for audits and ensuring system security. Many millions of log messages are produced each day which makes anomaly detection challenging. Automating the detection of anomalies can save time and money as well as improve detection performance. In this paper, an anomaly detection method is proposed using radius-based fuzzy C-means with more clusters than the number of data classes and a multilayer perceptron (MLP) network. The cluster centers and a radius are used to select reliable positive and negative log messages. Moreover, class probabilities are used with an expert to correct the network output for suspect logs. The proposed model is evaluated with three well-known data sets, namely BGL, Openstack and Thunderbird. The results obtained show that this model provides better results than existing methods.},
  archive      = {J_APIN},
  author       = {Farzad, Amir and Gulliver, T. Aaron},
  doi          = {10.1007/s10489-022-03300-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17708-17717},
  shortjournal = {Appl. Intell.},
  title        = {Log message anomaly detection with fuzzy C-means and MLP},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast hypergraph regularized nonnegative tensor ring
decomposition based on low-rank approximation. <em>APIN</em>,
<em>52</em>(15), 17684–17707. (<a
href="https://doi.org/10.1007/s10489-022-03346-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor ring (TR) decomposition is a highly effective tool for obtaining the low-rank character of multi-way data. Recently, nonnegative tensor ring (NTR) decomposition combined with manifold learning has emerged as a promising approach for exploiting the multi-dimensional structure and extracting features from tensor data. However, an existing method such as graph regularized tensor ring (GNTR) decomposition only models the pair-wise similarities of objects. The graph cannot precisely encode similarity relationships for tensor data with a complex manifold structure. In this paper, to sufficiently utilize the high-dimensional and complex similarities among objects, we add a novel hypergraph regulation into the NTR framework to further enhance feature extraction. Based on this, we propose a hypergraph regularized nonnegative tensor ring decomposition (HGNTR) model. To reduce computational complexity and suppress noise, we apply the low-rank approximation trick to accelerate HGNTR (called LraHGNTR). Our experiment results demonstrate that the proposed HGNTR and LraHGNTR algorithms outperform other state-of-the-art algorithms; additionally, LraHGNTR significantly reduces running time without sacrificing accuracy.},
  archive      = {J_APIN},
  author       = {Zhao, Xinhai and Yu, Yuyuan and Zhou, Guoxu and Zhao, Qibin and Sun, Weijun},
  doi          = {10.1007/s10489-022-03346-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17684-17707},
  shortjournal = {Appl. Intell.},
  title        = {Fast hypergraph regularized nonnegative tensor ring decomposition based on low-rank approximation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Red deer algorithm based social trust based congestion
control in ad hoc social networks. <em>APIN</em>, <em>52</em>(15),
17668–17683. (<a
href="https://doi.org/10.1007/s10489-022-03265-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ad hoc social network (ASNET) explores social connectivity among users of mobile devices which is becoming a main essential forms of internet today. In this ASNET, the security and congestion control is considered as the main issues which degrades the performance of the system. Conventional congestion control and security enhancement methods of ASNETs are do not behave properly in congestion environments and attack conditions. To address this issue, a priority with congestion control based hybrid algorithm with security technique is proposed and designed. The proposed scheduling algorithm exploits the social popularity of sensor nodes to prioritize complete incoming flows which completely reduce the congestion problems in the system. Trap door protocol and Zero knowledge proof protocols are combined which are used to improve the security of the ASNETs networks. Two main objective functions are considered to improve the performance of the network such as congestion control and security enhancement. The congestion control is achieved by optimal scheduling scheme which is attained by applying proposed Red Deer Algorithm (RDA). The proposed method is executed by MATLAB simulator and performances are compared with existing methods such as Atom Search Optimization (ASO), Emperor Penguin Optimization (EPO), Firefly Algorithm (FA), and Particle Swarm Optimization (PSO) algorithm respectively. The performance metrics are delay, drop, throughput, energy consumption, network lifetime, over-head and delivery ratio are determined and compared with the proposed method.},
  archive      = {J_APIN},
  author       = {Pushpalatha, S. and Hemalatha, T.},
  doi          = {10.1007/s10489-022-03265-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17668-17683},
  shortjournal = {Appl. Intell.},
  title        = {Red deer algorithm based social trust based congestion control in ad hoc social networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long text feature extraction network with data augmentation.
<em>APIN</em>, <em>52</em>(15), 17652–17667. (<a
href="https://doi.org/10.1007/s10489-022-03185-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of COVID-19 has had a serious impact on either work or the lives of people. With the decrease in physical social contacts and the rise of anxiety on the pandemic, social media has become the primary approach for people to access information related to COVID-19. Social media is rife with rumors and fake news, causing great damage to the Society. Facing shortages, imbalance, and nosiness, the current Chinese data set related to the epidemic has not helped the detection of fake news. Besides, the accuracy of classification was also affected by the easy loss of edge characteristics in long text data. In this paper, long text feature extraction network with data augmentation (LTFE) was proposed, which improves the learning performance of the classifier by optimizing the data feature structure. In the stage of encoding, Twice-Masked Language Modeling for Fine-tuning (TMLM-F) and Data Alignment that Preserves Edge Characteristics (DA-PEC) was proposed to extract the classification features of the Chinese Dataset. Between the TMLM-F and DA-PEC processes, we use Attention to capture the dependencies between words and generate corresponding vector representations. The experimental results illustrate that this method is effective for the detection of Chinese fake news pertinent to the pandemic.},
  archive      = {J_APIN},
  author       = {Tang, Changhao and Ma, Kun and Cui, Benkuan and Ji, Ke and Abraham, Ajith},
  doi          = {10.1007/s10489-022-03185-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17652-17667},
  shortjournal = {Appl. Intell.},
  title        = {Long text feature extraction network with data augmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated multi-granularity reduction based on
neighborhood rough sets. <em>APIN</em>, <em>52</em>(15), 17636–17651.
(<a href="https://doi.org/10.1007/s10489-022-03371-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of multi-granularity has been introduced into various mathematical models in granular computing. For example, neighborhood rough sets can derive a good multi-granularity structure by gradually changing the size of neighborhood radius. Attribute reduction is an important topic in neighborhood rough sets. However, in the case of multi-granularity, the challenge of high computational complexity and difficulty in synthesizing multi-granularity information when performing reduction algorithms always exists. To address such limitations, an accelerated algorithm for multi-granularity reduction is designed. Firstly, we construct a multi-granularity reduction structure with multiple different neighborhood radii to reduce the elapsed time of computing reducts. In this way, the consumed time of calculating the distance is similar to the one of single granularity reduction, and the elapsed time of computing multi-granularity reducts can be reduced. Secondly, multiple granularity information is integrated in each attribute evaluation. Finally, we evaluated the proposed method from multiple perspectives on 12 UCI datasets. Compared with other multi-granularity reduction algorithms, the proposed method not only generates reducts with relatively high quality, but also improves the time efficiency of multi-granularity reduction algorithm.},
  archive      = {J_APIN},
  author       = {Li, Yizhu and Cai, Mingjie and Zhou, Jie and Li, Qingguo},
  doi          = {10.1007/s10489-022-03371-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17636-17651},
  shortjournal = {Appl. Intell.},
  title        = {Accelerated multi-granularity reduction based on neighborhood rough sets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning techniques for software vulnerability
prediction: A comparative study. <em>APIN</em>, <em>52</em>(15),
17614–17635. (<a
href="https://doi.org/10.1007/s10489-022-03350-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software vulnerabilities represent a major cause of security problems. Various vulnerability discovery models (VDMs) attempt to model the rate at which the vulnerabilities are discovered in a software. Although several VDMs have been proposed, not all of them are universally applicable. Also most of them seldom give accurate predictive results for every type of vulnerability dataset. The use of machine learning (ML) techniques has generally found success in a wide range of predictive tasks. Thus, in this paper, we conducted an empirical study on applying some well-known machine learning (ML) techniques as well as statistical techniques to predict the software vulnerabilities on a variety of datasets. The following ML techniques have been evaluated: cascade-forward back propagation neural network, feed-forward back propagation neural network, adaptive-neuro fuzzy inference system, multi-layer perceptron, support vector machine, bagging, M5Rrule, M5P and reduced error pruning tree. The following statistical techniques have been evaluated: Alhazmi-Malaiya model, linear regression and logistic regression model. The applicability of the techniques is examined using two separate approaches: goodness-of-fit to see how well the model tracks the data, and prediction capability using different criteria. It is observed that ML techniques show remarkable improvement in predicting the software vulnerabilities than the statistical vulnerability prediction models.},
  archive      = {J_APIN},
  author       = {Jabeen, Gul and Rahim, Sabit and Afzal, Wasif and Khan, Dawar and Khan, Aftab Ahmed and Hussain, Zahid and Bibi, Tehmina},
  doi          = {10.1007/s10489-022-03350-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17614-17635},
  shortjournal = {Appl. Intell.},
  title        = {Machine learning techniques for software vulnerability prediction: A comparative study},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving indoor visual navigation generalization with scene
priors and markov relational reasoning. <em>APIN</em>, <em>52</em>(15),
17600–17613. (<a
href="https://doi.org/10.1007/s10489-022-03317-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of visual navigation is the poor generalization to find the given target object in unexplored environment without the help of auxiliary sensors. We propose solving the visual navigation problem by incorporating object spatial scene priors and visible object relational reasoning. To get more accurate ground truth environment priors, we construct specific scene graph priors for indoor navigation, which provides rich object spatial relationships for helping finding the target objects by object relation detection. Furthermore, to imitate human’s reasonability in searching objects, we encode our scene graph priors with Markov model for relational reasoning and fuse them into reinforcement learning policy network, which improves model generalization in novel scenes. Moreover, we perform experiments on the AI2THOR virtual environment and outperform the current most state-of-the-art both in SPL (Success weighted by Path Length) and success rate on average.},
  archive      = {J_APIN},
  author       = {Zhou, Kang and Guo, Chi and Zhang, Huyin},
  doi          = {10.1007/s10489-022-03317-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17600-17613},
  shortjournal = {Appl. Intell.},
  title        = {Improving indoor visual navigation generalization with scene priors and markov relational reasoning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EDOA: An elastic deformation optimization algorithm.
<em>APIN</em>, <em>52</em>(15), 17580–17599. (<a
href="https://doi.org/10.1007/s10489-022-03471-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a large number of meta-heuristic algorithms have been proposed to efficiently solve various complex optimization problems in reality. Most of these algorithms are based on the intelligent behavior of swarms in the natural world. In this article, we take Hooke&#39;s law of elasticity and Newton&#39;s second law of motion as the information interaction tools and innovatively propose a new meta-heuristic algorithm that is based on the laws of physics, called the elastic deformation optimization algorithm (EDOA). A new parameter adaptive adjustment mechanism is designed in the EDOA to better explore and exploit the search space. At the same time, we compare the proposed EDOA with six well-known search algorithms and conduct simulation experiments on 23 classical benchmark functions and IEEE CEC 2020 benchmark functions respectively. We have further analyzed the experimental results, used two nonparametric statistical test methods, and drawn iterative curves of the algorithms to prove the powerful comprehensive performance of the proposed EDOA.},
  archive      = {J_APIN},
  author       = {Pan, Qingtao and Tang, Jun and Lao, Songyang},
  doi          = {10.1007/s10489-022-03471-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17580-17599},
  shortjournal = {Appl. Intell.},
  title        = {EDOA: An elastic deformation optimization algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal-score-based filter pruning for deep convolutional
neural networks. <em>APIN</em>, <em>52</em>(15), 17557–17579. (<a
href="https://doi.org/10.1007/s10489-022-03229-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) have achieved excellent performance in the processing of high-resolution images. Most of these networks contain many deep layers in quest of greater segmentation performance. However, over-sized CNN models result in overwhelming memory usage and large inference costs. Earlier studies have revealed that over-sized deep neural models tend to deal with abundant redundant filters that are very similar and provide tiny or no contribution in accelerating the inference of the model. Therefore, we have proposed a novel optimal-score-based filter pruning (OSFP) approach to prune redundant filters according to their relative similarity in feature space. OSFP not only speeds up learning in the network but also eradicates redundant filters leading to improvement in the segmentation performance. We empirically demonstrate on widely used segmentation network models (TernausNet, classical U-Net and VGG16 U-Net) and benchmark datasets (Inria Aerial Image Labeling Dataset and Aerial Imagery for Roof Segmentation (AIRS)) that computation costs (in terms of Float Point Operations (FLOPs) and parameters) are reduced significantly, while maintaining or even improving accuracy.},
  archive      = {J_APIN},
  author       = {Sawant, Shrutika S. and Bauer, J. and Erick, F. X. and Ingaleshwar, Subodh and Holzer, N. and Ramming, A. and Lang, E. W. and Götz, Th.},
  doi          = {10.1007/s10489-022-03229-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17557-17579},
  shortjournal = {Appl. Intell.},
  title        = {An optimal-score-based filter pruning for deep convolutional neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A density estimation approach for detecting and explaining
exceptional values in categorical data. <em>APIN</em>, <em>52</em>(15),
17534–17556. (<a
href="https://doi.org/10.1007/s10489-022-03271-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we deal with the problem of detecting and explaining anomalous values in categorical datasets. We take the perspective of perceiving an attribute value as anomalous if its frequency is exceptional within the overall distribution of frequencies. As a first main contribution, we provide the notion of frequency occurrence. This measure can be thought of as a form of Kernel Density Estimation applied to the domain of frequency values. As a second contribution, we define an outlierness measure for categorical values that leverages the cumulated frequency distribution of the frequency occurrence distribution. This measure is able to identify two kinds of anomalies, called lower outliers and upper outliers, corresponding to exceptionally low or high frequent values. Moreover, we provide interpretable explanations for anomalous data values. We point out that providing interpretable explanations for the knowledge mined is a desirable feature of any knowledge discovery technique, though most of the traditional outlier detection methods do not provide explanations. Considering that when dealing with explanations the user could be overwhelmed by a huge amount of redundant information, as a third main contribution, we define a mechanism that allows us to single out outstanding explanations. The proposed technique is knowledge-centric, since we focus on explanation-property pairs and anomalous objects are a by-product of the mined knowledge. This clearly differentiates the proposed approach from traditional outlier detection approaches which instead are object-centric. The experiments highlight that the method is scalable and also able to identify anomalies of a different nature from those detected by traditional techniques.},
  archive      = {J_APIN},
  author       = {Angiulli, Fabrizio and Fassetti, Fabio and Palopoli, Luigi and Serrao, Cristina},
  doi          = {10.1007/s10489-022-03271-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17534-17556},
  shortjournal = {Appl. Intell.},
  title        = {A density estimation approach for detecting and explaining exceptional values in categorical data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective offspring generation strategy for
many-objective optimization driven by knee points under variable
classification. <em>APIN</em>, <em>52</em>(15), 17508–17533. (<a
href="https://doi.org/10.1007/s10489-022-03307-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many-objective optimization problems, the difficulty of optimization increases as the number of targets increases. When the number of objectives increases, the individuals become extremely sparse in the objective space and the performance of environmental selection strategies weaken. To solve this problem, we proposed an effective offspring generation strategy driven by knee points under variable classification, termed VKOS. In VKOS, there is no mating selection, and the excellent genes of the knee points are used to guide the generation of outstanding individuals with different attributes directionally. Specifically, first, the convergence variables and diversity variables of the problem are obtained by variable classification; then identify the knee points of the current population, and finally propose a parallel mutation method to mutate the current population to obtain offspring. In order to verify the versatility and effectiveness of the strategy, RPEA-VKOS, NSGAIII-VKOS, RPDNSGAII-VKOS, MaOEA/IBP-VKOS compared with the original algorithm on 16 widely used benchmark problems. In addition, taking RPEA as an example, The VKOS proposed in this paper compared with several single mutation operators DE, PLM, NUM and representative solution generation strategies DEMR, VCEM, MM on these benchmark problems. The extensive experiments on well-known benchmark problems ranging from 3 to 15 objectives show that VKOS improves the performance of MaOEAs and has the superior performance over three single mutation operators and typical offspring generation strategies when solving most of these test MaOPs.},
  archive      = {J_APIN},
  author       = {Wei, Li-sen and Li, Er-chao},
  doi          = {10.1007/s10489-022-03307-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17508-17533},
  shortjournal = {Appl. Intell.},
  title        = {An effective offspring generation strategy for many-objective optimization driven by knee points under variable classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Multi-metric learning by a pair of twin-metric learning
framework. <em>APIN</em>, <em>52</em>(15), 17490–17507. (<a
href="https://doi.org/10.1007/s10489-022-03330-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-metric learning is important for improving classification performance since learning a single metric is usually insufficient for complex data. The existing multi-metric learning methods are based on the triplet constraints, and thus are with high computing complexity. In this work, we propose an efficient multi-metric learning framework by a pair of two-metric learning schemes (called TMML) to jointly train two local metrics and a global metric, where the distances between samples are automatically adjusted to maximize classification margin. Instead of the triplet constraints, the proposed TMML is based on the pair constraints to reduce the computational burden. Moreover, a global regularization is introduced to improve generalization and control overfitting. The proposed TMML improves the limitation of a single metric, where a pair of local metrics are interrelated to conduct adaptation for the local characteristics, while global metrics are to depict the common properties from all the data. Furthermore, we develop an alternating direction iterative algorithm to optimize the proposed TMML. The convergence of the algorithm is analyzed theoretically. Numerical experiments are carried out on different scale datasets. Under different evaluation criteria, experiments show that the proposed TMML is superior to the single metric learning methods, and achieves better performance than other state-of-the-art multi-metric learning methods in most cases.},
  archive      = {J_APIN},
  author       = {Zhang, Min and Yang, Liming and Yuan, Chao and Ren, Qiangqiang},
  doi          = {10.1007/s10489-022-03330-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17490-17507},
  shortjournal = {Appl. Intell.},
  title        = {Multi-metric learning by a pair of twin-metric learning framework},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Word-level human interpretable scoring mechanism for novel
text detection using tsetlin machines. <em>APIN</em>, <em>52</em>(15),
17465–17489. (<a
href="https://doi.org/10.1007/s10489-022-03281-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research in novelty detection focuses mainly on document-level classification, employing deep neural networks (DNN). However, the black-box nature of DNNs makes it difficult to extract an exact explanation of why a document is considered novel. In addition, dealing with novelty at the word level is crucial to provide a more fine-grained analysis than what is available at the document level. In this work, we propose a Tsetlin Machine (TM)-based architecture for scoring individual words according to their contribution to novelty. Our approach encodes a description of the novel documents using the linguistic patterns captured by TM clauses. We then adapt this description to measure how much a word contributes to making documents novel. Our experimental results demonstrate how our approach breaks down novelty into interpretable phrases, successfully measuring novelty.},
  archive      = {J_APIN},
  author       = {Bhattarai, Bimal and Granmo, Ole-Christoffer and Jiao, Lei},
  doi          = {10.1007/s10489-022-03281-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17465-17489},
  shortjournal = {Appl. Intell.},
  title        = {Word-level human interpretable scoring mechanism for novel text detection using tsetlin machines},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gated residual feature attention network for real-time
dehazing. <em>APIN</em>, <em>52</em>(15), 17449–17464. (<a
href="https://doi.org/10.1007/s10489-022-03157-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured under complicated weather conditions, such as haze, often suffer from a noticeable degradation and hamper its practical application. Traditional dehazing methods use various hand-crafted priors to get a clear image; in such cases, the performance is limited owing to unconstrained environment. In order to restore the haze-free image directly, we propose an end-to-end Gated Residual Feature Attention Network (GRFA-Net) that leverages the haze representations through feature restacking and propagation. We design a Feature Attention Residual Block (FARB) as the core of feature extraction, which employs the residual block to extract hierarchical features, and followed by a novel Feature Attention Module (FAM) that adaptively captures the inter-dependencies from channel- and spatial-wise perspectives. Furthermore, we utilize a group structure (GS) to enlarge the receptive field and merge different multi-level features via the gate fusion module (GFM), respectively. Extensive experiments demonstrate that our GRFA-Net can obtain results that are comparable or even better than previous state-of-the-art methods in terms of quantitative and qualitative evaluation metrics. Furthermore, we reduce the computational complexity considerably and obtain a real-time FPS. The code is available: https://github.com/leandepk/GRFA-Net.},
  archive      = {J_APIN},
  author       = {Yi, Weichao and Dong, Liquan and Liu, Ming and Zhao, Yuejin and Hui, Mei and Kong, Lingqin},
  doi          = {10.1007/s10489-022-03157-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17449-17464},
  shortjournal = {Appl. Intell.},
  title        = {Gated residual feature attention network for real-time dehazing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improvement and application of hybrid real-coded genetic
algorithm. <em>APIN</em>, <em>52</em>(15), 17410–17448. (<a
href="https://doi.org/10.1007/s10489-021-03048-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving constrained optimization problems (COPs) with high-dimension and multi-extreme problems, genetic algorithm (GA) has the issue of trapping into local optimum. Therefore, this paper proposes a hybrid real-coded genetic algorithm (HIRCGA). First, a sorting group selection (SGS) is given, which is a simple operation and easy to implement. Second, a combinational crossover (CX) operator is developed. It consists of a heuristic normal distribution and direction crossover based on the optimal individual (HNDDX-BOI) and a sine cosine crossover (SCX), which enhances the exploration ability of the algorithm. Third, an operation of eliminating the similarity of different variables in the same dimension (ES) is added, which significantly avoids premature convergence and maintains the population diversity. Fourth, a combinational mutation (CM) operator is proposed, where the global and local search abilities of HIRCGA are fully considered. Fifth, the chaotic search (CS) based on Tent map is introduced to enhance the search power of HIRCGA. Moreover, 28 benchmark test functions in CEC 2017 and two complex real-world optimization problems are selected to demonstrate the effectiveness and superiority of HIRCGA. The computational results and statistical analysis indicate that HIRCGA can improve the solution accuracy compared with other algorithms. The effectiveness of HIRCGA is verified in theory and practice.},
  archive      = {J_APIN},
  author       = {Song, Haohao and Wang, Jiquan and Song, Li and Zhang, Hongyu and Bei, Jinling and Ni, Jie and Ye, Bei},
  doi          = {10.1007/s10489-021-03048-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17410-17448},
  shortjournal = {Appl. Intell.},
  title        = {Improvement and application of hybrid real-coded genetic algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multiobjective prediction model with incremental learning
ability by developing a multi-source filter neural network for the
electrolytic aluminium process. <em>APIN</em>, <em>52</em>(15),
17387–17409. (<a
href="https://doi.org/10.1007/s10489-022-03314-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving current efficiency and reducing energy consumption are two important technical goals of the electrolytic aluminum process (EAP). However, because the process involves complex noise characteristics (i.e., unknown types, redundant distributions and variable forms), it is very difficult to accurately develop a multiobjective prediction model. To overcome this problem, in this paper, a novel framework of multiobjective incremental learning based on a multi-source filter neural network (MSFNN) is presented. The proposed framework first presents a “multi-source filter” (MSF) technique that utilizes the mean and variance in the unscented Kalman filter (UKF) to guide the importance function of the particle filter (PF) based on a density kernel estimation method. Then, the MSF is embedded in the mutated neural network to adjust weights in real time. Third, weights are calculated and normalized by a modified importance function, which is the basis for further optimizing a secondary sampling based on sampling importance resampling (SIR). Finally, the incremental learning model with two objectives (i.e., process power consumption and current efficiency) based on the MSFNN in the EAP is established. The presented framework has been verified by the real-world EAP and some closely related methods. All test results indicate that the MSFNN’s relative prediction errors of the above two objectives are controlled within 0.51% and 0.38%, respectively and prove that MSFNN has significant competitive advantages over other recent filtering network models. Successfully establishment of the proposed framework provides a model foundation for multiobjective optimization problems in the EAP.},
  archive      = {J_APIN},
  author       = {Yao, Lizhong and Ding, Wei and He, Tiantian and Liu, Shouxin and Nie, Ling},
  doi          = {10.1007/s10489-022-03314-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17387-17409},
  shortjournal = {Appl. Intell.},
  title        = {A multiobjective prediction model with incremental learning ability by developing a multi-source filter neural network for the electrolytic aluminium process},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A time-dependent attention convolutional LSTM method for
traffic flow prediction. <em>APIN</em>, <em>52</em>(15), 17371–17386.
(<a href="https://doi.org/10.1007/s10489-022-03324-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With traffic network becoming increasingly complicated, traffic flow prediction has important practical significance for the management of traffic roads and public safety. For example, an accurate taxi demand prediction can help to improve efficiency of vehicle scheduling and reduce traffic congestion. The main issue of flow prediction is how to extract the information of complex spatio-temporal dependencies and interactions between arrival and departure. To solve these problems, we develop a deep learning method based on time-dependent attention convolutional LSTM (TDAConvLSTM) in which a time-dependent attention mechanism is designed to learn similarities of historical traffic flows among different time intervals and a fusion mechanism is introduced to aggregate the feature information produced by convolutional LSTM and attention module. And then, the result of the feature aggregation is fed to a multi-layer deconvolutional network to gain the results of flow prediction. Experimental studies on two real-life datasets indicate that TDAConvLSTM achieves better results than the compared models. The source code of our proposed method is available at the URL1.},
  archive      = {J_APIN},
  author       = {Huang, Xiaohui and Tang, Jie and Yang, Xiaofei and Xiong, Liyan},
  doi          = {10.1007/s10489-022-03324-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17371-17386},
  shortjournal = {Appl. Intell.},
  title        = {A time-dependent attention convolutional LSTM method for traffic flow prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint intent detection and slot filling using weighted
finite state transducer and BERT. <em>APIN</em>, <em>52</em>(15),
17356–17370. (<a
href="https://doi.org/10.1007/s10489-022-03295-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intent detection and slot filling are the two most essential tasks of natural language understanding (NLU). Deep neural models have produced impressive results on these tasks. However, the predictive accuracy of these models heavily depends upon a massive amount of supervised data. In many applications collecting high-quality labeled data is a very expensive and time taking process. This paper proposes WFST-BERT model which augments the fine-tuning of BERT-like architecture with weighted finite-state transducer (WFST) to reduce the need for massive supervised data. The WFST-BERT employs regular expressions (REs) rules to encode domain knowledge and pre-trained BERT model to generate contextual representations of user sentences. In particular, the model converts REs into the trainable weighted finite-state transducer, which can generate decent predictions when limited or no training examples are available. Moreover, BERT contextual representation is combined with WFST and trained simultaneously on supervised data using a gradient descent algorithm. The experimental results on the ATIS dataset show that the F1-Score of the WFST-BERT improved by around 1.8% and 1.3% for intent detection and 0.9%, 0.7% for slot filling tasks as compared to its counterparts RE-NN and JointBERT models in limited data settings. Further, in full data settings, the proposed model generates better recall and F1-score than state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Abro, Waheed Ahmed and Qi, Guilin and Aamir, Muhammad and Ali, Zafar},
  doi          = {10.1007/s10489-022-03295-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17356-17370},
  shortjournal = {Appl. Intell.},
  title        = {Joint intent detection and slot filling using weighted finite state transducer and BERT},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention based trajectory prediction method under the air
combat environment. <em>APIN</em>, <em>52</em>(15), 17341–17355. (<a
href="https://doi.org/10.1007/s10489-022-03292-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In close-range air combat, highly reliable trajectory prediction results can help pilots to win victory to a great extent. However, traditional trajectory prediction methods can only predict the precise location that the target aircraft may reach, which cannot meet the requirements of high-precision, real-time trajectory prediction for highly maneuvering targets. To this end, this paper proposes an attention-based convolution long sort-term memory (AttConvLSTM) network to calculate the arrival probability of each space in the reachable area of the target aircraft. More specifically, by segmenting the reachable area, the trajectory prediction problem is transformed into a classification problem for solution. Second, the AttConvLSTM network is proposed as an efficient feature extraction method, and combined with the multi-layer perceptron (MLP) to solve this classification problem. Third, a novel loss function is designed to accelerate the convergence of the proposed model. Finally, the flight trajectories generated by experienced pilots are used to evaluate the proposed method. The results indicate that the mean absolute error of the proposed method is no more than 45.73m, which is of higher accuracy compared to other state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Zhang, An and Zhang, Baichuan and Bi, Wenhao and Mao, Zeming},
  doi          = {10.1007/s10489-022-03292-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17341-17355},
  shortjournal = {Appl. Intell.},
  title        = {Attention based trajectory prediction method under the air combat environment},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain feature enhancement for unsupervised domain
adaptation. <em>APIN</em>, <em>52</em>(15), 17326–17340. (<a
href="https://doi.org/10.1007/s10489-022-03306-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Till the present, the domain adaptation has been widely researched by transferring the knowledge from a labeled source domain to an unlabeled target domain. Adversarial adaptation methods have achieved great success, learning domain-invariant representations with category semantic information. Although domain-invariant representation is obtained, domain-specific variation is suppressed, which may distort the original feature distribution. In this paper, we propose a novel method called Cross-domain Feature Enhancement Domain Adaptation (CFEDA) which fills in the domain discrepancy to address the challenge of original domain feature information damage. Specifically, by leveraging the cross-domain and intra-domain prototype representations that are extracted through clustering, the features of both source and target domains can be enhanced. As a result, similar source domain and similar target domain features can be produced in the feature space to fill in the domain discrepancy. Since the target domain feature is unlabeled and can not be directly adopted for training, we exploit a feature consistency loss on it. Moreover, extensive experiments are conducted to demonstrate that CFEDA achieves significant performance improvements.},
  archive      = {J_APIN},
  author       = {Sifan, Long and Shengsheng, Wang and Xin, Zhao and Zihao, Fu and Bilin, Wang},
  doi          = {10.1007/s10489-022-03306-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17326-17340},
  shortjournal = {Appl. Intell.},
  title        = {Cross-domain feature enhancement for unsupervised domain adaptation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The best hop diffusion method for dynamic relationships
under the independent cascade model. <em>APIN</em>, <em>52</em>(15),
17315–17325. (<a
href="https://doi.org/10.1007/s10489-022-03460-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the influence maximization problem is to identify a seed set that maximizes the number of users who will be impacted. This issue is critical to the spread of information in social networks. The greedy and heuristic algorithms are two prominent algorithms for this problem. The greedy algorithms are usually effective but they have high computational complexity. The heuristic algorithms are tendly efficient while the accuracy is low. The major goal of this study is to strike a compromise between efficacy and efficiency in selecting k influential nodes to maximize influence propagation while keeping the expense of doing so to a minimal. This paper proposes an improved heuristic algorithm, called the Best Hop for Independent Cascade Model (BHICM). This approach presents a dynamic relationship strategy between propagation probability and hop count that avoids processing the seed node’s neighbors. Afterwards, the proposed algorithm defines the diffusion score as the criterion for selecting seed nodes, which ensures its accuracy and efficiency. According to experimental results on four genuine social networks, the proposed approach exceeds all comparison algorithms in terms of accuracy while maintaining an acceptable running time.},
  archive      = {J_APIN},
  author       = {Qiu, Liqing and Liu, Yuying and Duan, Xiuliang},
  doi          = {10.1007/s10489-022-03460-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17315-17325},
  shortjournal = {Appl. Intell.},
  title        = {The best hop diffusion method for dynamic relationships under the independent cascade model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attacking bitcoin anonymity: Generative adversarial networks
for improving bitcoin entity classification. <em>APIN</em>,
<em>52</em>(15), 17289–17314. (<a
href="https://doi.org/10.1007/s10489-022-03378-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of Bitcoin entities is an important task to help Law Enforcement Agencies reduce anonymity in the Bitcoin blockchain network and to detect classes more tied to illegal activities. However, this task is strongly conditioned by a severe class imbalance in Bitcoin datasets. Existing approaches for addressing the class imbalance problem can be improved considering generative adversarial networks (GANs) that can boost data diversity. However, GANs are mainly applied in computer vision and natural language processing tasks, but not in Bitcoin entity behaviour classification where they may be useful for learning and generating synthetic behaviours. Therefore, in this work, we present a novel approach to address the class imbalance in Bitcoin entity classification by applying GANs. In particular, three GAN architectures were implemented and compared in order to find the most suitable architecture for generating Bitcoin entity behaviours. More specifically, GANs were used to address the Bitcoin imbalance problem by generating synthetic data of the less represented classes before training the final entity classifier. The results were used to evaluate the capabilities of the different GAN architectures in terms of training time, performance, repeatability, and computational costs. Finally, the results achieved by the proposed GAN-based resampling were compared with those obtained using five well-known data-level preprocessing techniques. Models trained with data resampled with our GAN-based approach achieved the highest accuracy improvements and were among the best in terms of precision, recall and f1-score. Together with Random Oversampling (ROS), GANs proved to be strong contenders in addressing Bitcoin class imbalance and consequently in reducing Bitcoin entity anonymity (overall and per-class classification performance). To the best of our knowledge, this is the first work to explore the advantages and limitations of GANs in generating specific Bitcoin data and “attacking” Bitcoin anonymity. The proposed methods ultimately demonstrate that in Bitcoin applications, GANs are indeed able to learn the data distribution and generate new samples starting from a very limited class representation, which leads to better detection of classes related to illegal activities.},
  archive      = {J_APIN},
  author       = {Zola, Francesco and Segurola-Gil, Lander and Bruse, Jan L. and Galar, Mikel and Orduna-Urrutia, Raul},
  doi          = {10.1007/s10489-022-03378-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17289-17314},
  shortjournal = {Appl. Intell.},
  title        = {Attacking bitcoin anonymity: Generative adversarial networks for improving bitcoin entity classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed measure-based feature selection using the fisher score
and neighborhood rough sets. <em>APIN</em>, <em>52</em>(15),
17264–17288. (<a
href="https://doi.org/10.1007/s10489-021-03142-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing feature selection methods easily neglect the distribution of data, and require most of the neighborhood radius in neighborhood rough sets (NRS) to be selected artificially. These limitations result in the misclassification of samples. To address these drawbacks, this paper presents a mixed measure-based feature selection method using the Fisher score and an NRS model. First, the variation coefficient of the features in different decision classes is defined to depict the dispersion degree of different features, based on which, the neighborhood class is described to develop a novel NRS model. The concepts of dependency degree, neighborhood knowledge granularity, and average neighborhood entropy are defined, and then a mixed measure combining the information and algebra views is proposed to measure the uncertainty in neighborhood decision systems. Second, the average correlation degree of the feature subset is computed to assess the redundancy of the reduced feature subset. By combining the classification accuracy of the selected features, the reduction rate of the classification result, and the average correlation degree of the reduced feature set, we can construct an adaptive neighborhood radius function to avoid the artificial selection of the optimal neighborhood radius. Then, an optimal feature subset can be obtained according to the internal and external significance of the features. Third, the variation coefficient of the samples in different decision classes in each feature is defined to compute the dispersion degree of the samples, and the average of all samples in each feature is added to the between-class scatter to eliminate the effect of the different measurement dimensions of the features; then, the Fisher score model is improved to eliminate the noise of the high-dimensional data. Finally, a heuristic feature selection algorithm with the Fisher score based on the new NRS model is designed to select an optimal feature subset. Experimental results applied to five low-dimensional UCI datasets and nine high-dimensional gene expression datasets showed that the developed algorithm is effective and can select an optimal reduced subset with high classification accuracy when compared with some of the latest algorithms.},
  archive      = {J_APIN},
  author       = {Sun, Lin and Zhang, Jiuxiao and Ding, Weiping and Xu, Jiucheng},
  doi          = {10.1007/s10489-021-03142-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17264-17288},
  shortjournal = {Appl. Intell.},
  title        = {Mixed measure-based feature selection using the fisher score and neighborhood rough sets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiobjective energy efficient street lighting framework: A
data analysis approach. <em>APIN</em>, <em>52</em>(15), 17237–17263. (<a
href="https://doi.org/10.1007/s10489-022-03398-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data analysis approach for designing an energy efficient street lighting framework is proposed to maximize both energy efficiency and uniformity of the system. A multiobjective optimization problem on obtaining energy efficiency is formulated in a comprehensive manner. Three multiobjective evolutionary optimization algorithms such as nondominated sorting genetic algorithm II, strength Pareto evolutionary algorithm 2 and multiobjective differential evolutionary algorithm are used to analyse the approximated Pareto solutions of our proposed model. The performance of considered algorithms are presented and compared with regard to different metrics. The results from the best algorithm, in terms of convergence and diversity, among the algorithms are then validated using DIALux to ensure the recommendation for the standardization in different aspects. The proposed work contributes a comprehensive data analysis on genetic algorithm solutions towards obtaining a multiobjective energy efficient street lighting which is beyond the scope of the existing works. The results obtained by the proposed method are also compared with existing DIALux results. The improvement of energy efficiency obtained by the proposed methodology over existing works is shown in terms of various aspects.},
  archive      = {J_APIN},
  author       = {Sikdar, Pragna Labani and Kar, Samarjit and Thakurta, Parag Kumar Guha},
  doi          = {10.1007/s10489-022-03398-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17237-17263},
  shortjournal = {Appl. Intell.},
  title        = {Multiobjective energy efficient street lighting framework: A data analysis approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved salp swarm algorithm for solving single-objective
continuous optimization problems. <em>APIN</em>, <em>52</em>(15),
17217–17236. (<a
href="https://doi.org/10.1007/s10489-022-03269-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Salp Swarm Algorithm (SSA) is an effective single-objective optimization algorithm that was inspired by the navigating and foraging behaviors of salps in their natural habitats. Although SSA was successfully tailored and applied to solve various types of optimization problems, it often suffers from premature convergence and typically does not perform well with high-dimensional optimization problems. This paper introduces an Improved SSA (ISSA) algorithm to enhance the performance of SSA in solving single-objective continuous optimization problems. ISSA has four characteristics. First, it employs Gaussian Perturbation to improve the diversity of initial population. Second, it uses highly disruptive polynomial mutation (HDPM) to update the leader salp in the salp chain. Third, it uses the Laplace crossover operator to improve its exploration ability. Fourth, it uses a new opposition learning method called Mixed Opposition-based Learning (MOBL) to improve its convergence rate and exploration ability. A set of 14 standard benchmark functions was used to evaluate the performance of ISSA and compare it to three variations of SSA (SSA, Hybrid SSA with Particle Swarm Optimization HSSAPSO Singh et al. (2020) and Enhanced SSA (ESSA) Zhang et al. (2020)). The overall experimental and statistical results indicate that ISSA is a better optimization algorithm than the other SSA variations. Further, the single-objective IEEE CEC 2014 (IEEE Congress on Evolutionary Computation 2014) functions were used to evaluate and compare the performance of ISSA to 18 well-known and state-of-the-art optimization algorithms (Exploratory Cuckoo Search (ECS) Abed-alguni (2021)), Grey Wolf Optimizer (GWO) Mirjalili and Mirjalili (Advances in Engineering Software, 69, 46–61, 2014), Distributed Grey Wolf Optimize (DGWO) Abed-alguni and Barhoush (2018), Cuckoo Search (CS) Yang and Deb (2009), Distributed adaptive differential evolution with linear population size reduction evolution (L-SHADE) Tanabe and Fukunaga (2014), Memory-based Hybrid Dragonfly Algorithm (MHDA) KS and Murugan (Expert Syst Appl, 83, 63–78, 2017), Fireworks Algorithm with Differential Mutation (FWA-DM) Yu et al. (2014), Differential Evolution-based Salp Swarm Algorithm (DESSA) Dhabal et al. (Soft Comput, 25(3), 1941–1961, 2021), LSHADE with Fitness and Diversity Ranking-Based Mutation Operator (FD-LSHADE) Cheng et al. (Swarm and Evolutionary Computation, 61, 100816, 2021), Distance based SHADE (Db-SHADE) Viktorin et al. (Swarm and Evolutionary Computation, 50, 100462, 2019) and Zeng et al. (Knowl-Based Syst, 226, 107150, 2021), Mean–Variance Mapping Optimization (MVMO) Iacca et al. (Expert Syst Appl, 165, 113902, 2021), Time-varying strategy-based Differential Evolution (TVDE) Sun et al. (Soft Comput, 24(4), 2727–2747, 2020), Butterfly Optimization Algorithm with adaptive gbest-guided search strategy and Pinhole-Imaging-based Learning (PIL-BOA)Long et al. (Appl Soft Comput, 103, 107146, 2021), Memory Guided Sine Cosine Algorithm (MG-SCA) Gupta et al. (Eng Appl Artif Intell, 93, 103718, 2020), Lévy flight Jaya Algorithm (LJA) Iacca et al. (2021), Sine Cosine Algorithm (SCA) Dhabala et al. (2021), Covariance Matrix Adaptation Evolution Strategy (CMA-ES) Hansen et al. (Evolutionary Computation, 11(1), 1–18, 2003) and Coyote Optimization Algorithm (COA) Pierezan and Coelho (2018)). The results indicate that ISSA performs better than the tested optimization algorithms.},
  archive      = {J_APIN},
  author       = {Abed-alguni, Bilal H. and Paul, David and Hammad, Rafat},
  doi          = {10.1007/s10489-022-03269-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17217-17236},
  shortjournal = {Appl. Intell.},
  title        = {Improved salp swarm algorithm for solving single-objective continuous optimization problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A migratory behavior and emotional preference clustering
algorithm based on learning vector quantization and gaussian mixture
model. <em>APIN</em>, <em>52</em>(15), 17185–17216. (<a
href="https://doi.org/10.1007/s10489-022-03325-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering based on swarm intelligence optimization plays a central role in engineering and mathematics. The ordinary emotional preference migration model has failed to achieve a good cluster effect and easily falls into the optimal local solution. Therefore, in order to obtain a higher quality of clustering and a more precise number of clusters, the paper focuses on the integral increment of the particular learning vector quantization and the Gaussian mixture method. By combining the label information and the distribution information of the dataset to facilitate clustering, we have proposed a new algorithm named GLEPMC. In this way, the supervised and unsupervised information of the data can be fully utilized for clustering. Through numerous experiments, the performance of the proposed GLEPMC algorithm is better than the ordinary emotional preference migration model and the other six algorithms. Theoretical analyses also prove the convergence of our proposed GLEPMC algorithm.},
  archive      = {J_APIN},
  author       = {Dai, Mingzhi and Feng, Xiang and Yu, Huiqun and Guo, Weibin},
  doi          = {10.1007/s10489-022-03325-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17185-17216},
  shortjournal = {Appl. Intell.},
  title        = {A migratory behavior and emotional preference clustering algorithm based on learning vector quantization and gaussian mixture model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive multi-modal fusion hashing via hadamard matrix.
<em>APIN</em>, <em>52</em>(15), 17170–17184. (<a
href="https://doi.org/10.1007/s10489-022-03367-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing plays an important role in information retrieval, due to its low storage and high speed of processing. As an effective multi-modal representation learning method, multi-modal hashing has received particular attention. Most of the existing multi-modal hashing methods adopt the fixed weighting factors to fuse multiple modalities for any query data, which cannot capture the variation among different queries. Besides, there are too much hyper-parameters in their models while it is time-consuming and labor-intensive to determine the proper parameters. The limitations may significantly hinder their promotion in practical applications. In this paper, we propose a simple, yet effective method that is inspired by the Hadamard matrix. On the one hand, our proposed method that involves a very few hyper-parameters is flexible. On the other hand, the complementary information between multi-modal data and the semantic discrimination information are preserved well in the hash codes. Extensive experimental results on four benchmark datasets show that the proposed framework is effective and achieves superior performance compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yu, Jun and Zhang, Donglin and Shu, Zhenqiu and Chen, Feng},
  doi          = {10.1007/s10489-022-03367-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17170-17184},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive multi-modal fusion hashing via hadamard matrix},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CF-DAML: Distributed automated machine learning based on
collaborative filtering. <em>APIN</em>, <em>52</em>(15), 17145–17169.
(<a href="https://doi.org/10.1007/s10489-021-03049-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search for a good machine learning (ML) model takes a long time and requires the considerations of many alternatives, including data preprocessing, algorithm selection, and hyperparameter tuning methods. Thus, tedious searches face a combinatorial explosion problem. In this work, we build a new automated machine learning (AutoML) system called CF-DAML, a distributed automated system based on collaborative filtering (CF), to address these challenges by recommending and training suitable models for supervised learning tasks. CF-DAML first computes some informative meta-features for a new dataset, then uses a weighted $$l_1$$ -norm (W1-norm) to accurately calculate the k nearest neighbors (kNN) of the new dataset, and finally recommends the top N models with good performances on each of its neighbors to the new dataset. We also design a distributed system (DSTM) for training the models to reduce the time complexity substantially. In addition, we develop a multilayer selective stacked ensemble system (MSSE), whose base models are selected from among suitable candidate models based on their runtimes, classification accuracies, and diversities, to enhance the stability of CF-DAML. To our knowledge, this is the first work to combine memory-based CF and the selective stacked ensemble to solve the AutoML problem. Extensive experiments are conducted on many UCI datasets and the comparative results demonstrate that our approach outperforms the current state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liu, Pengjie and Pan, Fucheng and Zhou, Xiaofeng and Li, Shuai and Jin, Liang},
  doi          = {10.1007/s10489-021-03049-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17145-17169},
  shortjournal = {Appl. Intell.},
  title        = {CF-DAML: Distributed automated machine learning based on collaborative filtering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TP: Tensor product layer to compress the neural network in
deep learning. <em>APIN</em>, <em>52</em>(15), 17133–17144. (<a
href="https://doi.org/10.1007/s10489-022-03260-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decomposition is widely used to reduce the amount of model parameters and release the pressure on resource-constrained devices. However, exiting studies on tensor decomposition need tensor rank selection, which introduce a large number of additional hyper-parameters and extensive combinatorial searches on tensor ranks. In this paper, we present a new tensor product (TP) based linear layer that can replace the original convolution layer, fully-connected (FC) layer, and vector FC layer in capsule networks without tensor rank selection. Specifically, tensor-matrix product and tensor-vector product are used to the original matrix multiplication. Tensor-matrix product and tensor-outer product are used to replace element product operation. Tensor-outer product is for convolution. These tensor product operations are made up of numerous factors and have fewer parameters. The experimental results show that, when compared to the tensor decomposition algorithm, our algorithm can compress the parameter amount several times while maintaining accuracy without fine-tuning and tensor rank selection.},
  archive      = {J_APIN},
  author       = {Qiang, Wang and Ji, Yuwang},
  doi          = {10.1007/s10489-022-03260-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17133-17144},
  shortjournal = {Appl. Intell.},
  title        = {TP: Tensor product layer to compress the neural network in deep learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint channel-spatial attention network for super-resolution
image quality assessment. <em>APIN</em>, <em>52</em>(15), 17118–17132.
(<a href="https://doi.org/10.1007/s10489-022-03338-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) is an effective technique to enhance the quality of LR images. However, one of the most fundamental problems for SR is to evaluate the quality of resultant images for comparing and optimizing the performance of SR algorithms. In this paper, we propose a novel deep network model referred to as a joint channel-spatial attention network (JCSAN) for no-reference SR image quality assessment (NR-SRIQA). The JCSAN consists of a two-stream branch which learns the middle level features and the primary level features to jointly quantify the degradation of SR images. In the first middle level feature learning subnetwork, we embed a two-stage convolutional block attention module (CBAM) to capture discriminative perceptual feature maps through the channel and spatial attention in sequence. While the other shallow convolutional subnetwork is adopted to learn dense and primary level textural feature maps. In order to yield more accurate quality estimate to SR images, we integrate a unit aggregation gate (AG) module to dynamically distribute the channel-weights to the two feature maps from different branches. Extensive experimental results on two benchmark datasets verify the superiority of the proposed JCSAN-based quality metric in comparing with other state-of-the-art competitors.},
  archive      = {J_APIN},
  author       = {Zhang, Tingyue and Zhang, Kaibing and Xiao, Chuan and Xiong, Zenggang and Lu, Jian},
  doi          = {10.1007/s10489-022-03338-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17118-17132},
  shortjournal = {Appl. Intell.},
  title        = {Joint channel-spatial attention network for super-resolution image quality assessment},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing convolutional neural networks with constrained
evolutionary piecemeal training. <em>APIN</em>, <em>52</em>(15),
17103–17117. (<a
href="https://doi.org/10.1007/s10489-021-02679-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated architecture search methodology for neural networks is known as Neural Architecture Search (NAS). In recent times, Convolutional Neural Networks (CNNs) designed through NAS methodologies have achieved very high performance in several fields, for instance image classification and natural language processing. Our work is in the same domain of NAS, where we traverse the search space of neural network architectures with the help of an evolutionary algorithm which has been augmented with a novel approach of piecemeal-training. In contrast to the previously published NAS techniques, wherein the training with given data is considered an isolated task to estimate the performance of neural networks, our work demonstrates that a neural network architecture and the related weights can be jointly learned by combining concepts of the traditional training process and evolutionary architecture search in a single algorithm. The consolidation has been realised by breaking down the conventional training technique into smaller slices and collating them together with an integrated evolutionary architecture search algorithm. The constraints on architecture search space are placed by limiting its various parameters within a specified range of values, consequently regulating the neural network’s size and memory requirements. We validate this concept on two vastly different datasets, namely, the CIFAR-10 dataset in the domain of image classification, and PAMAP2 dataset in the Human Activity Recognition (HAR) domain. Starting from randomly initialized and untrained CNNs, the algorithm discovers models with competent architectures, which after complete training, reach an accuracy of of 92.5% for CIFAR-10 and 94.36% PAMAP2. We further extend the algorithm to include an additional conflicting search objective: the number of parameters of the neural network. Our multi-objective algorithm produces a Pareto optimal set of neural networks, by optimizing the search for both the accuracy and the parameter count, thus emphasizing the versatility of our approach.},
  archive      = {J_APIN},
  author       = {Sapra, Dolly and Pimentel, Andy D.},
  doi          = {10.1007/s10489-021-02679-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17103-17117},
  shortjournal = {Appl. Intell.},
  title        = {Designing convolutional neural networks with constrained evolutionary piecemeal training},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards addressing unauthorized sharing of subscriptions.
<em>APIN</em>, <em>52</em>(15), 17090–17102. (<a
href="https://doi.org/10.1007/s10489-021-02812-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subscription-based business is booming in recent years, especially in the entertainment sector such as video and music streaming. Usually one subscription account can be shared among family members for the convenience of subscribers. However, account sharing also creates challenges for service provider, as many account owners share their subscriptions outside of the household. The widely spread practice of unauthorized sharing causes huge revenue loss for service providers. However, service providers are very cautious to pursue violators because identifying unauthorized shared accounts is a challenging task. First, the sheer volume of unstructured and noisy data makes it prohibitive to manually process the data. Moreover, it is legitimate for family members to share an account from any location and use many devices as they want. It is tricky to differentiate between unauthorized and legitimate sharing. In this paper, we propose an efficient solution to address the account sharing problem. Based on usage log data, our solution builds user profiles by accumulating and representing geolocation and device usage information. Then we estimate the risk of unauthorized sharing by analyzing the usage pattern of each account. The proposed solution can identify a large number of shared accounts and help service providers to recoup a significant amount of lost revenue.},
  archive      = {J_APIN},
  author       = {Zhang, Wei and Challis, Chris},
  doi          = {10.1007/s10489-021-02812-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17090-17102},
  shortjournal = {Appl. Intell.},
  title        = {Towards addressing unauthorized sharing of subscriptions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One-class ensemble classifier for data imbalance problems.
<em>APIN</em>, <em>52</em>(15), 17073–17089. (<a
href="https://doi.org/10.1007/s10489-021-02671-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification is an important issue in machine learning. Despite various studies, solving the data imbalance problem is still difficult. Since the oversampling method uses fake minority data, such a method is untrusted and causing security instability. The main objective of this paper is to improve accuracy for data imbalance classification without generating fake minority data. For this purpose, a reliable strategy is proposed using an ensemble of one-class classifiers. Such a classifier does not suffer data imbalance problems since the model learns from a single class. In particular, training data is split into minority and majority sets. Then, one-class classifiers are trained separately and applied to compute minority and majority scores for testing data. Finally, classification is made based on the combination of both scores. The proposed method is experimented with using imbalanced-learn datasets. Moreover, the result is compared with sampling methods via Decision Tree and K Nearest Neighbors classifiers. One-class ensemble classifier outperforms sampling methods in 20 datasets.},
  archive      = {J_APIN},
  author       = {Hayashi, Toshitaka and Fujita, Hamido},
  doi          = {10.1007/s10489-021-02671-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17073-17089},
  shortjournal = {Appl. Intell.},
  title        = {One-class ensemble classifier for data imbalance problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach to the design of acyclic chemical compounds
using skeleton trees and integer linear programming. <em>APIN</em>,
<em>52</em>(15), 17058–17072. (<a
href="https://doi.org/10.1007/s10489-021-03088-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent systems are applied in a wide range of areas, and computer-aided drug design is a highly important one. One major approach to drug design is the inverse QSAR/QSPR (quantitative structure-activity and structure-property relationship), for which a method that uses both artificial neural networks (ANN) and mixed integer linear programming (MILP) has been proposed recently. This method consists of two phases: a forward prediction phase, and an inverse, inference phase. In the prediction phase, a feature function f over chemical compounds is defined, whereby a chemical compound G is represented as a vector f(G) of descriptors. Following, for a given chemical property $$\pi$$ , using a dataset of chemical compounds with known values for property $$\pi$$ , a regressive prediction function $$\psi$$ is computed by an ANN. It is desired that $$\psi (f(G))$$ takes a value that is close to the true value of property $$\pi$$ for the compound G for many of the compounds in the dataset. In the inference phase, one starts with a target value $$y^*$$ of the chemical property $$\pi$$ , and then a chemical structure $$G^*$$ such that $$\psi (f(G^*))$$ is within a certain tolerance level of $$y^*$$ is constructed from the solution to a specially formulated MILP. This method has been used for the case of inferring acyclic chemical compounds. With this paper, we propose a new concept on acyclic chemical graphs, called a skeleton tree, and based on it develop a new MILP formulation for inferring acyclic chemical compounds. Our computational experiments indicate that our newly proposed method significantly outperforms the existing method when the diameter of graphs is up to 8. In a particular example where we inferred acyclic chemical compounds with 38 non-hydrogen atoms from the set {C, O, S} times faster.},
  archive      = {J_APIN},
  author       = {Zhang, Fan and Zhu, Jianshen and Chiewvanichakorn, Rachaya and Shurbevski, Aleksandar and Nagamochi, Hiroshi and Akutsu, Tatsuya},
  doi          = {10.1007/s10489-021-03088-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17058-17072},
  shortjournal = {Appl. Intell.},
  title        = {A new approach to the design of acyclic chemical compounds using skeleton trees and integer linear programming},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel grid-based density peak clustering of big
trajectory data. <em>APIN</em>, <em>52</em>(15), 17042–17057. (<a
href="https://doi.org/10.1007/s10489-021-02757-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of data intensive applications such as navigation systems for mobile devices and unmanned vehicles, analyzing trajectory data has become a key research area. One of the main tasks is trajectory clustering, which consists of automatically grouping similar trajectories into clusters. To perform this task, Density Peak Clustering (DPC) is widely used due to its speed and small number of artificial parameters. However, a major problem is that its performance does not scale well for large datasets. To address this issue, this paper proposes an efficient parallel trajectory clustering algorithm, named Tra-PDPC (Trajectory-Parallel DPC). It is applied in three steps, namely trajectory division and partition, trajectory similarity calculation, and clustering. Those steps are all designed to run in a distributed fashion using the Spark programming model. For the first step, a scheme is proposed to divide sub-trajectories based on local grid area density. Then, a combined similarity measurement method based on Euclidean space and grid space is defined for sub-trajectories similarity calculation. Finally, a version of DPC is applied, which dramatically improves clustering speed. Experiments on multiple large realistic trajectory datasets have demonstrated that the proposed Tra-PDPC algorithm can considerably decrease runtime while providing a high accuracy.},
  archive      = {J_APIN},
  author       = {Niu, Xinzheng and Zheng, Yunhong and Fournier-Viger, Philippe and Wang, Bing},
  doi          = {10.1007/s10489-021-02757-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17042-17057},
  shortjournal = {Appl. Intell.},
  title        = {Parallel grid-based density peak clustering of big trajectory data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heuristically mining the top-k high-utility itemsets with
cross-entropy optimization. <em>APIN</em>, <em>52</em>(15), 17026–17041.
(<a href="https://doi.org/10.1007/s10489-021-02576-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high-utility itemsets (HUIs) is one of the most important research topics in data mining because HUIs consider non-binary frequency values of items in transactions and different profit values for each item. However, setting appropriate minimum utility thresholds by trial and error is a tedious process for users. Thus, mining the top-k HUIs without setting a utility threshold is becoming an alternative to determine all the HUIs. In this paper, we propose two algorithms, called the top-k high-utility itemset mining based on cross-entropy method (TKU-CE) and TKU-CE+, for mining the top-k HUIs heuristically. The TKU-CE algorithm is based on cross-entropy, and implements top-k HUI mining using combinatorial optimization. The main idea of TKU-CE is to generate the top-k HUIs by gradually updating the probabilities of itemsets with high-utility values. TKU-CE+ optimizes TKU-CE in three respects. First, unpromising items are filtered by critical utility value, to reduce the computational burden in the initial stage. Second, a sample refinement strategy is used in each iteration, to reduce the computational burden in the iterative stage. Finally, smoothing mutation is proposed, to randomly generate some new itemsets in addition to those from previous iterations. Consequently, diversity of samples is improved, so that more actual top-k HUIs can be discovered with fewer iterations. Compared with state-of-the-art algorithms, TKU-CE and TKU-CE+ are easy to implement and avoid the computational costs that would be incurred by additional data structures and threshold-raising strategies. Extensive experimental results show that both algorithms are efficient, memory-saving, scalable, and can discover the most actual top-k HUIs.},
  archive      = {J_APIN},
  author       = {Song, Wei and Zheng, Chuanlong and Huang, Chaomin and Liu, Lu},
  doi          = {10.1007/s10489-021-02576-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17026-17041},
  shortjournal = {Appl. Intell.},
  title        = {Heuristically mining the top-k high-utility itemsets with cross-entropy optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic maintenance model for high average-utility pattern
mining with deletion operation. <em>APIN</em>, <em>52</em>(15),
17012–17025. (<a
href="https://doi.org/10.1007/s10489-021-02539-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high average-utility itemset mining (HAUIM) was established to provide a fair measure instead of genetic high-utility itemset mining (HUIM) for revealing the satisfied and interesting patterns. In practical applications, the database is dynamically changed when insertion/deletion operations are performed on databases. Several works were designed to handle the insertion process but fewer studies focused on processing the deletion process for knowledge maintenance. In this paper, we then develop a PRE-HAUI-DEL algorithm that utilizes the pre-large concept on HAUIM for handling transaction deletion in the dynamic databases. The pre-large concept is served as the buffer on HAUIM that reduces the number of database scans while the database is updated particularly in transaction deletion. Two upper-bound values are also established here to reduce the unpromising candidates early which can speed up the computational cost. From the experimental results, the designed PRE-HAUI-DEL algorithm is well performed compared to the Apriori-like model in terms of runtime, memory, and scalability in dynamic databases.},
  archive      = {J_APIN},
  author       = {Wu, Jimmy Ming-Tai and Teng, Qian and Tayeb, Shahab and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s10489-021-02539-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {17012-17025},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic maintenance model for high average-utility pattern mining with deletion operation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based reasoning using answer set programming.
<em>APIN</em>, <em>52</em>(15), 16993–17011. (<a
href="https://doi.org/10.1007/s10489-022-03272-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosis, i.e., the detection and identification of faults, provides the basis for bringing systems back to normal operation in case of a fault. Diagnosis is a very important task of our daily live, assuring safe and reliable behavior of systems. The automation of diagnosis has been a successful research topic for several decades. However, there are limitations due to complexity issues and lack of expressiveness of the underlying reasoning mechanisms. More recently logic reasoning like answer set programming has gained a lot of attention and practical use. In this paper, we tackle the question whether answer set programming can be used for automating diagnosis, focusing on industrial applications. We discuss a formalization of the diagnosis problem based on answer set programming, introduce a general framework for modeling systems, and present experimental results of an answer set programming based diagnosis algorithm. Past limitations like not being able to deal with numerical operations for modeling can be solved to some extent. The experimental results indicate that answer set programming is efficient enough for being used in diagnosis applications, providing that the underlying system is of moderate size. For digital circuits having less than 500 components, diagnosis time has been less than one second even for computing triple fault diagnoses.},
  archive      = {J_APIN},
  author       = {Wotawa, Franz and Kaufmann, David},
  doi          = {10.1007/s10489-022-03272-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {16993-17011},
  shortjournal = {Appl. Intell.},
  title        = {Model-based reasoning using answer set programming},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial of special issue on emerging topics in applied
intelligence. <em>APIN</em>, <em>52</em>(15), 16991–16992. (<a
href="https://doi.org/10.1007/s10489-022-03449-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Fujita, Hamido and Fournier-Viger, Philippe and Ali, Moonis},
  doi          = {10.1007/s10489-022-03449-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {15},
  pages        = {16991-16992},
  shortjournal = {Appl. Intell.},
  title        = {Editorial of special issue on emerging topics in applied intelligence},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multi-model fusion network based real object tactile
understanding from haptic data. <em>APIN</em>, <em>52</em>(14),
16605–16620. (<a
href="https://doi.org/10.1007/s10489-022-03181-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tactile information of an object is one of the crucial features which define the impression of that object. This paper presents a novel multi-model fusion network for real object’s tactile understanding from haptic data. Furthermore, a low-cost 3D printed artificial finger-based tactile sensing system is designed for capturing haptic information in the form of acceleration profile, angular velocity, and normal force. Our proposed multi-model fusion network includes three different networks. First, we introduce a novel ensemble 2D convolutional neural network, namely SpectroNet, which captures the spatial features from the spectrogram of acceleration profile. Second, we design a 1-D convolutional neural network (CNN) with residual connection for extracting detailed spatial information from each segment of collected data. Third, we design bi-directional gated recurrent unit networks (BiGRU) to capture temporal dynamics. Moreover, the attention mechanism is utilized in all three proposed networks to assign weights to the features according to their contributions, which enhance the performance further. Finally, extensive experimental analysis is conducted on our dataset (i.e., 60 real objects, which cover both planner and non-planner surfaces) as well as the TUM surface material database. Empirical evaluations demonstrate that the proposed method significantly outperformed state-of-the-art methods in terms of accuracy, precision, recall and F1-score. Furthermore, we also found that the proposed multi-model fusion network substantially improves the performance compared to the single network.},
  archive      = {J_APIN},
  author       = {Joolee, Joolekha Bibi and Uddin, Md Azher and Jeon, Seokhee},
  doi          = {10.1007/s10489-022-03181-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16605-16620},
  shortjournal = {Appl. Intell.},
  title        = {Deep multi-model fusion network based real object tactile understanding from haptic data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Escape velocity centrality: Escape influence-based key nodes
identification in complex networks. <em>APIN</em>, <em>52</em>(14),
16586–16604. (<a
href="https://doi.org/10.1007/s10489-022-03262-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating and measuring key nodes in highly populated networks is essential to control the spreading effects of diseases or rumors. Although several incentive approaches have been proposed in complex networks to identify key nodes, these approaches still have many challenges. Most of the existing approaches consider just a single aspect of a node in a network. To cope with these challenges, based on the escape velocity formula, we propose Escape Velocity Centrality (EVC) approach to combine the concerned features of the network (i.e., local and global features) to measure key nodes in complex networks with spreading dynamics. Furthermore, we design an extended version of EVC (i.e., EVC+) to enhance the overall performance. To evaluate the effectiveness of EVC &amp; EVC+, we implemented the proposed model via real-world as well as artificial networks. The empirical results based on susceptible–infected–recovered (SIR) and Kendall&#39;s correlation evaluation modelshave demonstrated that EVC &amp; EVC+ outperformed the state-of-the-art centralities with remarkable margins of improvements with respect to all of the facets of evaluation.},
  archive      = {J_APIN},
  author       = {Ullah, Aman and Wang, Bin and Sheng, JinFang and Khan, Nasrullah},
  doi          = {10.1007/s10489-022-03262-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16586-16604},
  shortjournal = {Appl. Intell.},
  title        = {Escape velocity centrality: Escape influence-based key nodes identification in complex networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IAA-VSR: An iterative alignment algorithm for video
super-resolution. <em>APIN</em>, <em>52</em>(14), 16572–16585. (<a
href="https://doi.org/10.1007/s10489-022-03364-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Super-Resolution (VSR) aims at producing a high-resolution video from its corresponding low-resolution input frames. In VSR, the key to generating high-quality output is to exploit the spatial similarity of temporal frames. Most VSR methods achieve this by super-resolving a single reference frame with the aid of multiple frames in a temporal window. For this goal, some alignment methods have been proposed to compensate for the motion between adjacent frames. However, these methods lack more upper-level and unified guidance to progressively align neighboring frames, which often leads to poor results when encountering large motions. In this paper, we propose a unified Iterative Alignment Algorithm (IAA) for more accurate frame alignment in VSR. In IAA, each adjacent frame only needs to be aligned to its nearest neighbor, which greatly eases the alignment problem for all kinds of motions. To show the effectiveness of our method, we apply IAA to red the Enhanced Deformable Video super-Resolution (EDVR) network and obtain a new network called IAA-VSR. Extensive experiments show that our IAA-VSR consistently improves the performance of EDVR on benchmark datasets.},
  archive      = {J_APIN},
  author       = {Liu, Jie and Tang, Jie and Wu, Gangshan},
  doi          = {10.1007/s10489-022-03364-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16572-16585},
  shortjournal = {Appl. Intell.},
  title        = {IAA-VSR: An iterative alignment algorithm for video super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Intelligent identification for vertical track irregularity
based on multi-level evidential reasoning rule model. <em>APIN</em>,
<em>52</em>(14), 16555–16571. (<a
href="https://doi.org/10.1007/s10489-021-03114-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical track irregularity is one of the most significant indicators to evaluate track health. Accurate identification of vertical track irregularity is beneficial to achieve precise maintenance of the track and thus avoid accidents. However, the continuous variation of the track irregularity and the imbalance of the abnormal/normal data samples make it difficult to guarantee the accuracy of identification models. Therefore, by considering the interaction between train and track, a multi-level evidential reasoning (M-ER) rule model is proposed to build the nonlinear causal relationship of vibration signals and vertical track irregularity. In the modeling process of M-ER, the referential evidence matrix (REM) and fusion parameters (i.e., reliability factors and importance weights) are determined and optimized. In the model, the reliability factor of evidence is determined through trend analysis, while the importance weights of evidence and REM are optimized by sequential quadratic programming (SQP). In the inference process of M-ER, sample expansion strategy and two-level evidence fusion mechanism are designed. Specifically, in the first level, samples on each vibration signal are fused with their nearest neighboring historical samples obtained by K-Nearest Neighbor(K-NN) method. In the second-level, the results generated in the first-level are integrated by ER rule. We evaluate the M-ER rule model with an actual data set from China railway. The experimental results show that the model can identify the vertical track irregularity more accurately compared with the single-level ER rule model and other typical machine learning based models.},
  archive      = {J_APIN},
  author       = {Zhang, Zhenjie and Xu, Xiaobin and Zhang, Xuelin and Xu, Xiaojian and Ye, Zifa and Wang, Guodong and Dustdar, Schahram},
  doi          = {10.1007/s10489-021-03114-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16555-16571},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent identification for vertical track irregularity based on multi-level evidential reasoning rule model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data obsolescence detection in the light of newly acquired
valid observations. <em>APIN</em>, <em>52</em>(14), 16532–16554. (<a
href="https://doi.org/10.1007/s10489-022-03212-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information describing the conditions of a system or a person is constantly evolving and may become obsolete and contradict other information. A database, therefore, must be consistently updated upon the acquisition of new valid observations that contradict obsolete ones contained in the database. In this paper, we propose a novel causation-based system for dealing with the information obsolescence problem when a causal Bayesian network is our representation model. Our approach is based on studying causal dependencies between the network variables to detect, in real-time, contradictions between the observations on a single subject and then identify the obsolete ones. We propose a new approximate concept, 𝜖-Contradiction, which represents the confidence level of having a contradiction between some observations relating to a specific subject. Once identified, obsolete observations are given in an original way, in the form of an explanation AND-OR Tree. Our approach can be applied in various domains where the main issue is to detect and explain personalized situations such that the reasons and circumstances underlying unexpected outcomes. Examples include among others: detecting behaviour change by analyzing user profiles, and identifying the causes of some anomalies such as bank frauds by analyzing customer interactions. In this paper, we demonstrate the effectiveness of our approach in a real-life medical application: the elderly fall-prevention and showcase how the resulted explanation AND-OR trees can be used to give reliable recommendations to physicians and assist decision-makers. Our approach runs in a polynomial time and gives systematically and substantially good results.},
  archive      = {J_APIN},
  author       = {Chaieb, Salma and Hnich, Brahim and Mrad, Ali Ben},
  doi          = {10.1007/s10489-022-03212-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16532-16554},
  shortjournal = {Appl. Intell.},
  title        = {Data obsolescence detection in the light of newly acquired valid observations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multifactorial teaching-learning-based optimization with the
diversity and triangle cooperation mechanism. <em>APIN</em>,
<em>52</em>(14), 16512–16531. (<a
href="https://doi.org/10.1007/s10489-021-03059-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifactorial optimization (MFO) is a newly developed optimization framework that can be embedded with an evolutionary algorithm to solve multiple optimization tasks simultaneously. To further explore the generality of the MFO framework, this paper first attempts to use teaching-learning-based optimization as a base optimizer for multiple optimization tasks (MFTLBO). However, the quality of the solution obtained by MFTLBO is not satisfactory because of premature convergence. Therefore, an MFTLBO variant named multifactorial teaching-learning-based optimization with the diversity and triangle cooperation mechanism (DTMFTLBO) is proposed to reduce premature convergence. The diversity indicator is used to dynamically adjust the exploration and exploitation. The cooperative teaching strategy based on weight is designed to acquire evolution direction information, which can guide the population toward more promising solution regions. The triangle cooperation strategy is employed to promote the knowledge transfer between different tasks in the learner phase, which can overcome premature convergence. To verify the efficiency of the proposed DTMFTLBO, numerical studies have been conducted with the 9 commonly used single objective benchmark problems of MTO. Experimental results show that the average excellent rate of the DTMFTLBO for 18 tasks is 85.80%, which confirms the competitiveness of our proposed algorithm compared with eight state-of-the-art multitask optimization algorithms.},
  archive      = {J_APIN},
  author       = {Li, Wei and Fan, Yaochi and Wang, Lei and Jiang, Qiaoyong and Xu, Qingzheng},
  doi          = {10.1007/s10489-021-03059-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16512-16531},
  shortjournal = {Appl. Intell.},
  title        = {Multifactorial teaching-learning-based optimization with the diversity and triangle cooperation mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active intellectual property protection for deep neural
networks through stealthy backdoor and users’ identities authentication.
<em>APIN</em>, <em>52</em>(14), 16497–16511. (<a
href="https://doi.org/10.1007/s10489-022-03339-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the intellectual properties (IP) protection of deep neural networks (DNN) has attracted serious concerns. A number of DNN copyright protection methods have been proposed. However, most of the existing DNN watermarking methods can only verify the ownership of the model after the piracy occurs, which cannot actively prevent the occurrence of the piracy and do not support users’ identities management, thus can not satisfy the requirements of commercial DNN copyright management. In addition, the query modification attack which was proposed recently can invalidate most of the existing backdoor-based DNN watermarking methods. In this paper, we propose an active intellectual properties protection technique for DNN models via stealthy backdoor and users’ identities authentication. For the first time, we use a set of clean images (as the watermark key samples) to embed an additional class into the DNN for ownership verification, and use the image steganography to embed users’ identity information into these watermark key images. Each user will be assigned with a unique identity image for identity authentication and authorization control. Since the backdoor instances are clean images outside the dataset, the backdoor trigger is visually imperceptible and concealed. In addition, we embed the watermark by exploiting an additional class outside the main tasks, which establishes a strong connection for watermark key samples and the corresponding label. As a result, the proposed method is concealed, robust, and can resist common attacks and query modification attack. Experimental results demonstrate that, the proposed method can obtain 100% watermark accuracy and 100% fingerprint authentication success rate on Fashion-MNIST and CIFAR-10 datasets. In addition, the proposed method is demonstrated to be robust against the model fine-tuning attack, model pruning attack, and query modification attack. Compared with three existing DNN watermarking methods, the proposed method has better performance on watermark accuracy and robustness against the query modification attack.},
  archive      = {J_APIN},
  author       = {Xue, Mingfu and Sun, Shichang and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
  doi          = {10.1007/s10489-022-03339-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16497-16511},
  shortjournal = {Appl. Intell.},
  title        = {Active intellectual property protection for deep neural networks through stealthy backdoor and users’ identities authentication},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian model construction based on data-experts oriented
approaches for assessing the phosphate effluents effects. <em>APIN</em>,
<em>52</em>(14), 16475–16496. (<a
href="https://doi.org/10.1007/s10489-021-03105-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the near future, it is estimated that our world would attend a war over waters. Whereas, the effluents of the laundry of phosphate damages more and more the watershed in mining areas. The analysis of these impacts is in fact a crucial task for the preservation of the area’s water resources. In this paper, we introduced a hybrid approach-based Bayesian network model construction for assessing the phosphate laundry effluents effects. We proposed a novel discrete Bayesian Network model which is built using a hybrid approach based on Expert and data-oriented methods for structure learning. Our aim is to propose a novel data-oriented method for resolving the Bayesian network (BN) structure learning that will be improved basing on the experts’ knowledge for efficient modelling of the cause-effect relationships. And then the parameters learning procedure is performed basing on the Expectation Maximization algorithm (EM). The evaluation of the proposed data-oriented method based on two well-known benchmarking BNs demonstrates the superiority of the proposed method in terms of BN structure evaluation metrics (i.e. structure difference, correct edges, added edges, reversed edges and deleted edges). The proposed BN model allows the assessment of the groundwater quality taking into consideration several chemical factors and influencers on the absorption of discharged metals. Depending on a real collected water samples from the Gafsa phosphatic areas (southwestern Tunisia), we built the BN model which permits the analysis of different basic physico-chemical variables and its dependencies. Moreover, the generated results illustrate that our technique has higher performance compared with the other Bayesian techniques.},
  archive      = {J_APIN},
  author       = {Benmohamed, Emna and Ltifi, Hela and Ayed, Mounir Ben},
  doi          = {10.1007/s10489-021-03105-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16475-16496},
  shortjournal = {Appl. Intell.},
  title        = {Bayesian model construction based on data-experts oriented approaches for assessing the phosphate effluents effects},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining sequential patterns with flexible constraints from
MOOC data. <em>APIN</em>, <em>52</em>(14), 16458–16474. (<a
href="https://doi.org/10.1007/s10489-021-03122-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning is playing an increasingly important role in education. Massive open online course (MOOC) platforms are among the most important tools in online learning, and record historical learning data from an extremely large number of learners. To enhance the learning experience, a promising approach is to apply sequential pattern mining (SPM) to discover useful knowledge in these data. In this paper, mining sequential patterns (SPs) with flexible constraints in MOOC enrollment data is proposed, which follows that research approach. Three constraints are proposed: the length constraint, discreteness constraint, and validity constraint. They are used to describe the effect of the length of enrollment sequences, variance of enrollment dates, and enrollment moments, respectively. To improve the mining efficiency, the three constraints are pushed into the support, which is the most typical parameter in SPM, to form a new parameter called support with flexible constraints (SFC). SFC is proved to satisfy the downward closure property, and two algorithms are proposed to discover SPs with flexible constraints. They traverse the search space in a breadth-first and depth-first manner. The experimental results demonstrate that the proposed algorithms effectively reduce the number of patterns, with comparable performance to classical SPM algorithms.},
  archive      = {J_APIN},
  author       = {Song, Wei and Ye, Wei and Fournier-Viger, Philippe},
  doi          = {10.1007/s10489-021-03122-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16458-16474},
  shortjournal = {Appl. Intell.},
  title        = {Mining sequential patterns with flexible constraints from MOOC data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement method via multi-feature prior
fusion. <em>APIN</em>, <em>52</em>(14), 16435–16457. (<a
href="https://doi.org/10.1007/s10489-022-03275-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information in a single underwater image is insufficient due to the complexity of the underwater environment, which makes it challenging to meet the expectations of marine research. In this paper, we proposed a visual quality enhancement method for underwater images based on multi-feature prior fusion (MFPF), achieved by extracting and fusing multiple feature priors of underwater images. Complementary multi-features enhance the visual quality of underwater images. We designed a color correction method based on self-adaptive standard deviation, which realizes the color offset correction based on the dominant color of the underwater image. A gamma correction power function and spatial linear adjustment were also applied to achieve a set of artificial exposure map sequences obtained from a single degraded image and enhance the dark area’s brightness and structural details. This design makes full use of the advantages of white balance, guided filtering, and multi-exposure sequence technology. And it uses a multi-scale fusion of various prior features to enhance underwater images. The experimental results show that by applying the multi-feature prior fusion scheme, this design comprehensively solves various degenerated problems, removes over-enhancement, and improves dark details.},
  archive      = {J_APIN},
  author       = {Zhou, Jingchun and Zhang, Dehuan and Zhang, Weishi},
  doi          = {10.1007/s10489-022-03275-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16435-16457},
  shortjournal = {Appl. Intell.},
  title        = {Underwater image enhancement method via multi-feature prior fusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ground-based 4d trajectory prediction using bi-directional
LSTM networks. <em>APIN</em>, <em>52</em>(14), 16417–16434. (<a
href="https://doi.org/10.1007/s10489-022-03309-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate aircraft trajectory prediction in the ground based system remains one of the most challenging prediction problems in air traffic management. In comparison to airborne systems, ground-based systems have less knowledge regarding aircraft performance characteristics and weather, making this task more difficult. In order to improve the trajectory prediction accuracy of the existing state-of-the-art methods, a Bi-directional Long-Short-Term-Memory (Bi-LSTM) neural network model is proposed in this paper. The multi-layered structure of Bi-LSTM is capable of understanding both forward and backward dependencies in the sequential trajectory data. In contrast to previous methods, we employed variable length flight trajectories without interpolation as input data. The sliding window approach effectively preserves adjacent trajectory point relationships. The experimental data set consists of historical ADS-B (Automatic Dependent Surveillance-Broadcast) trajectory data collected from Flight Radar 24. For comparison, the well-known uni-directional LSTM, CNN-LSTM, and Back Propagation Neural Network are used and evaluated on the same data set. The experimental findings indicate that our model outperforms the aforementioned state-of-the-art models. The Root Mean Square Error (RMSE) value of our model is improved by 28.44%, 14.84%, 45.45%, and 19.3% when compared to the second best model for time, latitude, longitude, and altitude respectively. It establishes a solid platform for decision-making in trajectory prediction research.},
  archive      = {J_APIN},
  author       = {Sahadevan, Deepudev and M, Harikrishnan P and Ponnusamy, Palanisamy and Gopi, Varun P and Nelli, Manjunath K},
  doi          = {10.1007/s10489-022-03309-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16417-16434},
  shortjournal = {Appl. Intell.},
  title        = {Ground-based 4d trajectory prediction using bi-directional LSTM networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Walking motion real-time detection method based on walking
stick, IoT, COPOD and improved LightGBM. <em>APIN</em>, <em>52</em>(14),
16398–16416. (<a
href="https://doi.org/10.1007/s10489-022-03264-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time walking behavior monitoring is essential in ensuring safety and improving people’s physical conditions with mobility difficulties. In this paper, a real-time walking motion detection system based on the intelligent walking stick, mobile phone and multi-label imbalance classification method combining focal loss and LightGBM (MFGBoost) is proposed. The Internet of Things (IoT) technology is utilized for communicating between the walking stick and mobile phone. The new MFGBoost is embedded into the Raspberry Pi to classify human motions. MFGBoost is scalable, and other boosting models, such as XGBoost, could also be used as its base classifier. An improved derivation method of the multi-classification focal loss function is proposed in this paper, which is the key to the combination of multi-classification focal loss and Boosting algorithms. We propose a novel denoise method based on window matrix and COPOD algorithm (W-OD). The window matrix is designed to extract data features and smooth noise, and COPOD could output the noise level of the model. A weighted loss function is designed to adjust the model’s attention to different samples based on the W-OD algorithm. We evaluate the latest classification model from multiple perspectives on multiple benchmark datasets and demonstrate that MFGBoost and W-OD-MFGBoost could improve classification performance and decision-making efficiency. Experiments conducted on human motion datasets show that W-OD-MFGBoost could achieve more than 97 percent classification accuracy.},
  archive      = {J_APIN},
  author       = {Wang, Junyi and Jiang, Xuezheng and Meng, Qinggang and Saada, Mohamad and Cai, Haibin},
  doi          = {10.1007/s10489-022-03264-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16398-16416},
  shortjournal = {Appl. Intell.},
  title        = {Walking motion real-time detection method based on walking stick, IoT, COPOD and improved LightGBM},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-scale correlation analysis for robust multi-label
classification. <em>APIN</em>, <em>52</em>(14), 16382–16397. (<a
href="https://doi.org/10.1007/s10489-022-03299-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise in label space is a major challenge in the multi-label classification problem, as the noise (including false-negative noise and false-positive noise) can affect the distribution of the label space, which causes serious interference to the performance of the learned model. Existing methods have initially solved the situation where false-negative noise and false-positive noise appear separately, but classification when the two kinds of noise appear at the same time is still a challenging problem that has not yet been solved. This paper proposed a novel method named Dual-Scale Correlation Analysis for Robust Multi-label Classification (DCAMC) to deal with the above challenge, which can effectively deal with the simultaneous occurrence of these two kinds of noise. Our proposed method is based on the dual scale correlation analysis of samples and can mainly be divided into two parts, anti-noise module and classification module. In the anti-noise module, we define novel ‘leader-labels’ and ‘rare-labels’ based on the manifold assumption under fine-grained and coarse-grained data division respectively. The novel anti-noise module can solve the problem of false-negative noise and false-positive noise simultaneously without interfering with each other; In the classification module, we use the training datasets after the anti-noise process to train the multi-label classifiers. Coarse-grained data division for classification training guarantees the generalization performance of the model while fine-grained data division ensures effective label correlations mining. The two effective modules based on dual-scale data division improve the overall classification performance. Our method has been tested on the existing datasets, and the experiments demonstrate that our method has an improvement over existing methods.},
  archive      = {J_APIN},
  author       = {Wang, Kaixiang and Yang, Ming and Yang, Wanqi and Wang, Lei},
  doi          = {10.1007/s10489-022-03299-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16382-16397},
  shortjournal = {Appl. Intell.},
  title        = {Dual-scale correlation analysis for robust multi-label classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate prediction of ice surface and bottom boundary based
on multi-scale feature fusion network. <em>APIN</em>, <em>52</em>(14),
16370–16381. (<a
href="https://doi.org/10.1007/s10489-022-03362-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the locations of ice surface and bottom boundary in the radar imagery enables the calculation of ice sheet thickness, which is one of important inputs for ice-sheet modelling and global climate research. Therefore, accurate predictions of the boundaries can contribute to improve the accuracy of global climate analysis and sea level prediction. However, an accurate boundary detection in radar sounder data collected from the polar ice sheet has still been a challenge because the boundaries of the ice layer are usually very weak and noisy, and subglacial topography is highly variable. In recent years, the deep learning methods have surpassed the performances of traditional technology and helped to overcome a series of problems, including image boundary segmentation and target detection. This paper proposes a multi-scale feature fusion network (MFFN) for boundary detection of ice sheet radar echograms, where the ground truth supervises the output of the network at different stages, rather than the output of the last layer of the network. Also, a multi-scale convolution module (MCM) is introduced to learn the rich multi-scale representation of each network stage from shallow to deep, which uses convolution with different dilation rates to obtain multi-scale features. Furthermore, an improved loss function makes the proposed MFFN more effective to solve the sample imbalance problem of boundary detection, and further improves the accuracy of boundary detection. The proposed method is verified experimentally using the radar echograms from 2009 provided by the Center of Remote Sensing of Ice Sheets (CReSIS) that are used as training and test data. In the experiments, the proposed MFFN not only achieves state-of-the-art boundary detection accuracy on the test set but also improves the visual effect by generating fine boundaries.},
  archive      = {J_APIN},
  author       = {Cai, Yiheng and Wan, Fuxing and Hu, Shaobin and Lang, Shinan},
  doi          = {10.1007/s10489-022-03362-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16370-16381},
  shortjournal = {Appl. Intell.},
  title        = {Accurate prediction of ice surface and bottom boundary based on multi-scale feature fusion network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sentiment-based masked language modeling for improving
sentence-level valence–arousal prediction. <em>APIN</em>,
<em>52</em>(14), 16353–16369. (<a
href="https://doi.org/10.1007/s10489-022-03384-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment indicator prediction is a crucial task in sentiment analysis or emotion recognition. Through the accurate quantification of sentiments expressed in a text, people’s sentiments can be better understood. Many studies have used mask language modeling to predict sentiment indicators. In this approach, every word has the same masking probability irrespective of its degree of influence on sentiments because a random masking mechanism is adopted. Therefore, the learned features do not strengthen the sentiment vocabulary in a sentence. Consequently, this paper proposes a sentiment-based masked language modeling method to predict sentence-level valence–arousal scores in Chinese. The proposed method is called dimensional valence–arousal based on bidirectional encoder representations from transformers (DVA-BERT), which combines the BERT model with specific sentiment word masking. The proposed method involves two learning tasks: valence–arousal intensity estimation, which is the major task, and random masked sentiment word prediction, which is the auxiliary task modify from mask language modeling, used to enhance the model performance. The experimental results indicate that the proposed DVA-BERT model can identify effective sentiment features by masking sentiment words and can outperform the original BERT model and other word masking methods.},
  archive      = {J_APIN},
  author       = {Wu, Jheng-Long and Chung, Wei-Yi},
  doi          = {10.1007/s10489-022-03384-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16353-16369},
  shortjournal = {Appl. Intell.},
  title        = {Sentiment-based masked language modeling for improving sentence-level valence–arousal prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of haze and dehazing on deep learning-based vision
models. <em>APIN</em>, <em>52</em>(14), 16334–16352. (<a
href="https://doi.org/10.1007/s10489-022-03245-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep-learning-based vision models are trained and tested on clear images, avoiding noisy, or hazy, images. However, these models may encounter degraded images. So, it is important to recover and enhance them using a dehazing process. Dehazing usually serves as a preprocessing step for low-, medium-, and high-level vision tasks. Therefore, this article empirically studies the impact of haze and dehazing on high-level vision tasks and considers the degree to which dehazing algorithms can improve a vision model’s performance. For this purpose, we created two synthetic hazy datasets and trained several detection and classification models on both clear and hazy images. We found that haze and fog can easily affect a vision model’s performance and observed that using dehazing directly as a preprocessing step for high-level vision tasks did not substantially improve vision model’s performance but also renders performance unreliable and unpredictable. Therefore, when developing deep vision models, the research community should maintain aspects of bad weather conditions, such as haze, mist, fog, and rain, to avoid the failure of their proposed outdoor vision models.},
  archive      = {J_APIN},
  author       = {Hassan, Haseeb and Mishra, Pranshu and Ahmad, Muhammad and Bashir, Ali Kashif and Huang, Bingding and Luo, Bin},
  doi          = {10.1007/s10489-022-03245-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16334-16352},
  shortjournal = {Appl. Intell.},
  title        = {Effects of haze and dehazing on deep learning-based vision models},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social network alignment: A bi-layer graph attention neural
networks based method. <em>APIN</em>, <em>52</em>(14), 16310–16333. (<a
href="https://doi.org/10.1007/s10489-022-03216-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of social network alignment is to identify the user nodes which are active in multiple social networks simultaneously, thus the information from multiple social networks can be integrated to conduct some downstream tasks. Existing network alignment methods mostly establish anchor link prediction models using user profile information or the structural similarities in social networks, which may not effectively model user nodes, thus affecting the effectiveness of social network alignment. This paper focuses on social network alignment by modeling user nodes based on the structural information of irregular graphs and multi-dimensional user features. Specially, Graph Neural Networks (GNNs) models and bi-layer graph attention mechanism are designed to learn the embedding vectors of user nodes in social networks. First, multi-dimensional user features are comprehensively modeled. Then, a user-layer attention mechanism and a feature-layer attention mechanism based on GNN are respectively designed to learn the embedding vectors of user nodes, and the embedding vectors of user nodes in social networks are learned by designing a gated neural network to automatically learn the weight parameters of the user-layer embedding vectors and the feature-layer embedding vectors. Finally, based on the embedding vectors, a bi-directional alignment strategy is proposed to predict the anchor links between the source and target social networks, thus to ensure which meet the constraints of one-to-one alignment relationship. Simulation experiments based on multiple real social network datasets prove that our proposed method achieves better results in metrics of Accuracy and F1 than the existing mainstream social network alignment methods.},
  archive      = {J_APIN},
  author       = {Lu, Meilian and Dai, Yinlong and Zhang, Zhiqiang},
  doi          = {10.1007/s10489-022-03216-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16310-16333},
  shortjournal = {Appl. Intell.},
  title        = {Social network alignment: A bi-layer graph attention neural networks based method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer subspace learning based on double relaxed
regression for image classification. <em>APIN</em>, <em>52</em>(14),
16294–16309. (<a
href="https://doi.org/10.1007/s10489-022-03213-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method based on relaxed regression and transfer subspace learning for cross-resolution image classification is presented. Firstly, a transfer subspace learning based on the double relaxed regression (TSL_DRR) method is adopted to learn a discriminative model and simultaneously avoid over-fitting in a regression-based classification task. Secondly, the matching efficiency between low-resolution face and high-resolution face is not ideal, so a so-called transfer subspace learning (TSL) technique is introduced to the proposed method to ensure that the domain data can be better matched by projecting different resolution face images onto the common subspace. Lastly, the global data structure and local data structure can be reliably retained by applying the low-rank and sparse constraint matrices, which also reduces the noise to an extent. Extensive experiments on various real image data sets indicate that the proposed method is effective in 4.2 accuracy.},
  archive      = {J_APIN},
  author       = {Lu, Yue and Liu, Zhonghua and Huo, Hua and Yang, Chunlei and Zhang, Kaibing},
  doi          = {10.1007/s10489-022-03213-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16294-16309},
  shortjournal = {Appl. Intell.},
  title        = {Transfer subspace learning based on double relaxed regression for image classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent fault diagnosis of rolling bearing based on
novel CNN model considering data imbalance. <em>APIN</em>,
<em>52</em>(14), 16281–16293. (<a
href="https://doi.org/10.1007/s10489-022-03196-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligent fault diagnosis method based on deep learning has become a powerful tool for analyzing mechanical big data. However, a large proportion of collected data belong to the healthy condition, which will provoke data imbalance. In this article, a novel CNN-based intelligent machinery fault diagnosis method is proposed to deal with the data imbalance. This method consists of two modules, including feature extraction and fault identification. The former is optimized by the weighted-center-label loss function to extract discriminative features. The latter uses the distance between extracted features and pattern center vectors for fault identification. Two datasets are constructed to verify the effectiveness and superiority of the proposed method under data imbalance. The experimental results from two datasets show that the proposed method can effectively deal with data imbalance by extracting separable and discriminative features automatically.},
  archive      = {J_APIN},
  author       = {Xing, Ziyang and Zhao, Rongzhen and Wu, Yaochun and He, Tianjing},
  doi          = {10.1007/s10489-022-03196-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16281-16293},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent fault diagnosis of rolling bearing based on novel CNN model considering data imbalance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A repetitive feature selection method based on improved
ReliefF for missing data. <em>APIN</em>, <em>52</em>(14), 16265–16280.
(<a href="https://doi.org/10.1007/s10489-022-03327-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ReliefF is a representative and efficient algorithm amongst many feature selection methods, however, in the face of missing data, ReliefF and its variants might be invalid. To address this problem, a novel feature selection method, namely repetitive feature selection based on improved ReliefF, is proposed to obtain the optimal feature subset and make an accurate imputation delicately for missing data. The main idea is three-fold: 1) the data distribution determined by the distance of class center is introduced into the feature weights to construct a proper objective function, which greatly helps select significant and highly relevant features while removing redundant/noise ones; 2) the improved ReliefF is applied both before and after imputation to make full use of known data, and a non-negativity matrix factorization (NMF) model is established to make a sound imputation for missing data; and 3) during the NMF model learning, the mini-batch gradient descent (MBGD) technique is employed to accelerate the convergence and avoid trapping in local optima. Experiments on seven public data sets are utilized to show the effectiveness of the proposed feature selection method.},
  archive      = {J_APIN},
  author       = {Fan, Haiyan and Xue, Luyu and Song, Yan and Li, Ming},
  doi          = {10.1007/s10489-022-03327-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16265-16280},
  shortjournal = {Appl. Intell.},
  title        = {A repetitive feature selection method based on improved ReliefF for missing data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group channel pruning and spatial attention distilling for
object detection. <em>APIN</em>, <em>52</em>(14), 16246–16264. (<a
href="https://doi.org/10.1007/s10489-022-03293-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of pruning on network accuracy, we propose a novel pruning method called group channel pruning. In particular, we divide the network into multiple groups according to the scales of the feature layer and the similarity of module structure in the network, and then we use different pruning thresholds to prune the channels in each group. Finally, to recover the accuracy of the pruned network, we use an improved knowledge distillation method for the pruned network. Especially, we extract spatial attention information from the feature maps of specific scales in each group as knowledge for distillation. In the experiments, we use YOLOv4 as the object detection network and PASCAL VOC as the training dataset. Our method reduces the parameters of the model by 64.7% and the calculation by 34.9%. When the input image size is 416×416, compared with the original network model with 256MB size and 87.1 accuracies, our compressed model achieves 86.6 accuracies with 90MB size. To demonstrate the generality of our method, we replace the backbone to Darknet53 and Mobilenet and also achieve satisfactory compression results.},
  archive      = {J_APIN},
  author       = {Chu, Yun and Li, Pu and Bai, Yong and Hu, Zhuhua and Chen, Yongqing and Lu, Jiafeng},
  doi          = {10.1007/s10489-022-03293-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16246-16264},
  shortjournal = {Appl. Intell.},
  title        = {Group channel pruning and spatial attention distilling for object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CMGAN: A generative adversarial network embedded with causal
matrix. <em>APIN</em>, <em>52</em>(14), 16233–16245. (<a
href="https://doi.org/10.1007/s10489-021-03094-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the relationship between different features in multi-label learning and generating controllable features of multi-label images have always been hot topics. Unlike most other methods, in order to make the processing of the network more intuitive and convincing, we designed a generative adversarial network with a nested structural causal mechanism. The network has two functions: generating feature labels and generating multi-label images. We restore the adjacency matrix between individual features in the labels by applying an additional network layer with acyclic constraints, so as to obtain the corresponding causal directed acyclic graph. In this way, the network can autonomously learn the causal relationships between features and generate feature labels from them, enabling feature separation at the causal level. A particular innovation is that we can also add intervention mechanism to the label generation process to answer the “what if” question in the causality theory at the feature level. Finally, the second part of the network maps the labels to images, completing the entire process of multi-label image generation. We performed a series of experiments to demonstrate the effectiveness of the embedded causal mechanism in qualitative and quantitative terms.},
  archive      = {J_APIN},
  author       = {Zhang, Wenbin and Liao, Jun and Zhang, Yi and Liu, Li},
  doi          = {10.1007/s10489-021-03094-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16233-16245},
  shortjournal = {Appl. Intell.},
  title        = {CMGAN: A generative adversarial network embedded with causal matrix},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Connecting latent relationships over heterogeneous
attributed network for recommendation. <em>APIN</em>, <em>52</em>(14),
16214–16232. (<a
href="https://doi.org/10.1007/s10489-022-03340-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural network models for graph-structured data have been demonstrated to be influential in recommendation systems. Graph Neural Network (GNN), which can generate high-quality embeddings by capturing graph-structured information, is convenient for the recommendation. However, most existing GNN models mainly focus on the homogeneous graph. They cannot characterize heterogeneous and complex data in the recommendation system. Meanwhile, it is challenging to develop effective methods to mine the heterogeneity and latent correlations in the graph. In this paper, we adopt Heterogeneous Attributed Network (HAN), which involves different node types as well as rich node attributes, to model data in the recommendation system. Furthermore, we propose a novel graph neural network-based model to deal with HAN for Recommendation, called HANRec. In particular, we design a component connecting potential neighbors to explore the influence among neighbors and provide two different strategies with the attention mechanism to aggregate neighbors’ information. The experimental results on two real-world datasets prove that HANRec outperforms other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Duan, Ziheng and Wang, Yueyang and Ye, Weihao and Fan, Qilin and Li, Xiuhua},
  doi          = {10.1007/s10489-022-03340-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16214-16232},
  shortjournal = {Appl. Intell.},
  title        = {Connecting latent relationships over heterogeneous attributed network for recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Channel pruning guided by global channel relation.
<em>APIN</em>, <em>52</em>(14), 16202–16213. (<a
href="https://doi.org/10.1007/s10489-022-03198-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning approaches have achieved some success in compressing deep convolutional neural networks (CNNs). However, existing methods ignore the interdependence among channels within the same layer. In this paper, we propose a novel channel pruning approach called Channel Pruning guided by Global Channel Relation (CPGCR), which additionally takes the global channel relation into account in the channel pruning process. Considering that Squeeze-and-Excitation (SE) blocks have the ability to encode the channel relation, our method is mainly used to compress the CNNs with SE blocks. We also observe that SE blocks will enforce channel-level sparsity in the network, which is useful for the implementation of channel pruning algorithms. Extensive experiments with a variety of neural networks on five datasets clearly demonstrate the effectiveness of our proposed CPGCR method. The results show that on ImageNet, our method gives a 56% reductions in float-point-operation (FLOPs) for ResNet-50. On CIFAR-10, the CNNs compressed by CPGCR achieve comparable accuracy to that of the original models, but with significant reductions in FLOPs (61.7% for ResNet-56, 78.0% for VGG-16).},
  archive      = {J_APIN},
  author       = {Cheng, Yingjie and Wang, Xiaoqi and Xie, Xiaolan and Li, Wentao and Peng, Shaoliang},
  doi          = {10.1007/s10489-022-03198-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16202-16213},
  shortjournal = {Appl. Intell.},
  title        = {Channel pruning guided by global channel relation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IBPNet: A multi-resolution and multi-modal image fusion
network via iterative back-projection. <em>APIN</em>, <em>52</em>(14),
16185–16201. (<a
href="https://doi.org/10.1007/s10489-022-03375-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multi-modal image fusion methods are based on the prerequisite that the source images have the same resolution. However, due to the limitations of the environment and hardware facilities, the resolution of multi-modal images is always distinct. For example, spatial resolution of infrared images is usually lower than that of the corresponding visible images. Therefore, our motivation is to solve the problem of blurred details or a certain degree of information loss that are prone to appear in the fusion images. Under this motivation, a novel deep learning-based multi-resolution multi-modal image fusion network via iterative back-projection (IBPNet) is constructed to get high quality fused images. The key contribution of our IBPNet is to design up-projection and down-projection blocks to realize the feature mapping conversion between high and low-resolution images. The feedback errors generated in the alternation process are self-corrected in the reconstruction process. In addition, an effective combined loss function is designed, which can adapt to different multi-resolution and multi-modal image fusion tasks. Experimental results show that our method is superior to other state-of-the-art fusion methods in terms of both visual perception and objective evaluation.},
  archive      = {J_APIN},
  author       = {Liu, Chang and Yang, Bin and Zhang, Xiaozhi and Pang, Lihui},
  doi          = {10.1007/s10489-022-03375-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16185-16201},
  shortjournal = {Appl. Intell.},
  title        = {IBPNet: A multi-resolution and multi-modal image fusion network via iterative back-projection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interval-valued intuitionistic fuzzy jenson-shannon
divergence and its application in multi-attribute decision making.
<em>APIN</em>, <em>52</em>(14), 16168–16184. (<a
href="https://doi.org/10.1007/s10489-022-03347-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-Valued Intuitionistic Fuzzy Set (IVIFS) is an effective tool to model uncertainty, and has received much attention. When applying IVIFS to solve real problems such as Multi-Attribute Decision Making (MADM) problem, how to measure the distance between two Interval-Valued Intuitionistic Fuzzy Values (IVIFVs) is an essential problem. In this paper, a novel distance of IVIFS called Interval-Valued Intuitionistic Fuzzy Jenson-Shannon (IVIFJS) divergence is proposed, which can measure the difference or dissimilarity between IVIFSs. First, we propose a new Evaluation Score Function of the IVIFV, the score function considering the weight of membership and non-membership, which is more flexible than other existing score functions of IVIFV. Then, we find that the Evaluation Score Function is enclosed in a fixed interval, denoted as the largest possible range. Additionally, we find that the Evaluation Score Function can be approximately regarded to have Gaussian distribution over its largest range. Based on this, we propose a novel divergence measure operator for IVIFS named Interval-valued Intuitionistic Fuzzy Jenson-Shannon (IVIFJS) divergence by extending from discrete Jenson-Shannon (JS) divergence. Some useful mathematical properties of JS divergence, including boundness, symmetric and triangular inequality, are maintained in the proposed IVIFJS divergence. Next, we design a novel MADM method based on the proposed divergence operator. Further, some numerical examples are evaluated to illustrate the applicability and plausibility of the proposed method by comparing with other existing MADM methods. Then, the robustness and stability of the proposed MADM method are verified through sensitivity analysis on numerical examples. Finally, the proposed MADM method is applied in the applications of medical diagnosis and network system selection to verify the practicability of the proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Zhe and Xiao, Fuyuan and Ding, Weiping},
  doi          = {10.1007/s10489-022-03347-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16168-16184},
  shortjournal = {Appl. Intell.},
  title        = {Interval-valued intuitionistic fuzzy jenson-shannon divergence and its application in multi-attribute decision making},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic relevance and temporal activity-aware influence
maximization in social network. <em>APIN</em>, <em>52</em>(14),
16149–16167. (<a
href="https://doi.org/10.1007/s10489-022-03430-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization has attracted significant attention with the aim to find k individuals that can eventually maximize the influence individuals. The diverse applications include viral marketing, career prediction, and much more. In the classic influence maximization problem, most literature considers the dynamic of relationships between users on social networks, which results in high time complexity. Instead of relationships, from the perspective of users, we capture the dynamic through the temporal activity prediction of each user. In this paper, we introduce a Topic relevance and Temporal activity for the Influence Maximization (TTIM) problem, which is proved to be NP-hard in theory. We propose the Reverse Influence Set based framework for TTIM (RIS-TTIM), a quick, accurate, and unsupervised algorithm to solve the TTIM problem. This novel algorithm can obtain an approximation rate of (1-1/e - ε) under the general information diffusion model. Specifically, the RIS-TTIM algorithm is divided into two phases: (i) sampling topic relevance Reverse Reachable (RR) sets based on temporal activity degree, and (ii) selecting final influential nodes that can maximize the largest topic relevance nodes. Moreover, a novel method is proposed to predict the temporal activity degree of each user in the future. Experimental results on five real-world datasets demonstrate the effectiveness and efficiency of the proposed algorithm when compared with the baseline approaches.},
  archive      = {J_APIN},
  author       = {Jia, Wei and Ma, Ruizhe and Niu, Weinan and Yan, Li and Ma, Zongmin},
  doi          = {10.1007/s10489-022-03430-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16149-16167},
  shortjournal = {Appl. Intell.},
  title        = {Topic relevance and temporal activity-aware influence maximization in social network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emoji use in china: Popularity patterns and changes due to
COVID-19. <em>APIN</em>, <em>52</em>(14), 16138–16148. (<a
href="https://doi.org/10.1007/s10489-022-03195-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emojis are small pictograms that are frequently embedded within micro-texts to more directly express emotional meanings. To understand the changes in the emoji usage of internet users during the COVID-19 outbreak, we analysed a large dataset collected from Weibo, the most popular Twitter-like social media platform in China, from December 1, 2019, to March 20, 2020. The data contained 38,183,194 microblog posts published by 2,239,472 unique users in Wuhan. We calculated the basic statistics of users’ usage of emojis, topics, and sentiments and analysed the temporal patterns of emoji occurrence. After examining the emoji co-occurrence structure, we finally explored other factors that may affect individual emoji usage. We found that the COVID-19 outbreak greatly changed the pattern of emoji usage; i.e., both the proportion of posts containing emojis and the ratio of users using emojis declined substantially, while the number of posts remained the same. The daily proportion of Happy emojis significantly declined to approximately 32%, but the proportions of Sad- and Encouraging-related emojis rose to 24% and 34%, respectively. Despite a significant decrease in the number of nodes and edges in the emoji co-occurrence network, the average degree of the network increased from 34 to 39.8, indicating that the diversity of emoji usage increased. Most interestingly, we found that male users were more inclined towards using regular textual language with fewer emojis after the pandemic, suggesting that during public crises, male groups appeared to control their emotional display. In summary, the COVID-19 pandemic remarkably impacted individual sentiments, and the normal pattern of emoji usage tends to change significantly following a public emergency.},
  archive      = {J_APIN},
  author       = {Liu, Chuchu and Tan, Xu and Zhou, Tao and Zhang, Wei and Liu, Jianguo and Lu, Xin},
  doi          = {10.1007/s10489-022-03195-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16138-16148},
  shortjournal = {Appl. Intell.},
  title        = {Emoji use in china: Popularity patterns and changes due to COVID-19},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximately decoupled component supervision for salient
object detection. <em>APIN</em>, <em>52</em>(14), 16117–16137. (<a
href="https://doi.org/10.1007/s10489-021-03046-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) aims to find the most attractive object(s) in a scene. In recent years, SOD methods based on deep learning have become the mainstream. Existing methods mostly aggregate the multi-level features extracted by convolutional neural network (CNN) to model the object, or refine the boundary details of the object through multi-scale feature fusion. In this paper, the paradigm of approximately decoupled component supervision is proposed for SOD. Our insight is that the attractive performance of SOD requires explicit modeling of the body and edge of the object with different supervisions. Specifically, we first capture image features through foreground attention (FA) mechanism, cross-block semantic correlation aggregation (CBSC), and resolution-based feature integration (RFI) to make object parts more consistent and complete. Then the detailed edge is obtained by subtracting the body part from the complete mask. By explicitly sampling the body and edge pixels of the salient object, we further optimize the resulting body features and the residual edge features under the supervision of approximate decoupling. Benefiting from the abundant edge information and accurate location information, the framework with various backbone proposed by us can achieve better internal consistency and accurate boundaries of the object. Experimental results on six widely used benchmark datasets demonstrate the superiority and competitiveness of our approach in terms of four popular evaluation metrics. Moreover, the proposed method is an end-to-end saliency detection network without any pre-processing or post-processing.},
  archive      = {J_APIN},
  author       = {Liang, Yanhua and Qin, Guihe and Sun, Minghui and Yan, Jie and Zhang, Zhonghan},
  doi          = {10.1007/s10489-021-03046-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16117-16137},
  shortjournal = {Appl. Intell.},
  title        = {Approximately decoupled component supervision for salient object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved dynamic chebyshev graph convolution network for
traffic flow prediction with spatial-temporal attention. <em>APIN</em>,
<em>52</em>(14), 16104–16116. (<a
href="https://doi.org/10.1007/s10489-021-03022-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction plays a significant role in urban traffic management, including traffic congestion control and public travel route planning. Recently, several approaches have been put forward to learn the patterns from historical traffic data. However, there exist some limitations resulting from the use of the static learning method to explore the dynamical characteristics of the road network. Besides, the dynamic global temporal and spatial properties are not considered in these models. These drawbacks lead to a low prediction performance and make applying to a more extensive road network challenging. To address these issues, from the inspiration of Chebyshev polynomial, we proposed an improved dynamic Chebyshev graph convolution neural network model called iDCGCN. In the proposed approach, a novel updating method for the Laplacian matrix, which approximately constructs features from different period data, is proposed based on the attention mechanism. In addition, a novel feature construction method is proposed to integrate long-short temporal and local-global spatial features for complex traffic flow representation. Experimental results have shown that iDCGCN outperforms the state-of-the-art GCN-based methods on four real-world highway traffic datasets.},
  archive      = {J_APIN},
  author       = {Liao, Lyuchao and Hu, Zhiyuan and Zheng, Yuxin and Bi, Shuoben and Zou, Fumin and Qiu, Huai and Zhang, Maolin},
  doi          = {10.1007/s10489-021-03022-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16104-16116},
  shortjournal = {Appl. Intell.},
  title        = {An improved dynamic chebyshev graph convolution network for traffic flow prediction with spatial-temporal attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of the left ventricle endocardial border on
two-dimensional ultrasound images using deep layer aggregation for
residual dense networks. <em>APIN</em>, <em>52</em>(14), 16089–16103.
(<a href="https://doi.org/10.1007/s10489-022-03392-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound images are one of the most widely used medical images in clinical medicine. However, ultrasound images generally have the characteristics of strong noise, weak edges, and complex organizational structures, so the segmentation of ultrasound images is very difficult. Aiming at the problems of low efficiency and poor recognition accuracy in the existing ultrasound image segmentation algorithm, a deep layer aggregation for a residual dense network (DLA-RDNet) is proposed to segment the left ventricle ultrasound images. First, to locate the left ventricular region, perform morphological operations on the left ventricular ultrasound image to obtain the target image. Then, a deep aggregation residual for dense networks is proposed for left ventricular ultrasound image segmentation, that is, a residual dense network (RDNet) is designed to extract image features, and the feature information is closely integrated through the deep aggregation (DLA) method. Finally, the network is pruned through the deep supervision (DS) method, which simplifies the network structure and improves the network operation speed. We segment the left ventricular ultrasound images by the proposed algorithm. The experimental results show that the average precision rate(AP) is 95.68%, the average intersection over union (IoU) is 97.13%, the Dice coefficient is 97.15%, the average perpendicular distance(APD) is 0.31 mm, and the good contours rate (GC) is 99.32%. Compared with the other six segmentation algorithms, the proposed algorithm achieves a more effective segmentation of the left ventricle ultrasound images. Therefore, the proposed algorithm can accurately segment the left ventricle ultrasound images, which meets the needs of the segmentation of the left ventricle ultrasound images in clinical medicine.},
  archive      = {J_APIN},
  author       = {Wu, Xuanyan and Li, Xiuling and Mou, Gang and Wang, Dechun and He, Yan and Li, Zhengdong},
  doi          = {10.1007/s10489-022-03392-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16089-16103},
  shortjournal = {Appl. Intell.},
  title        = {Identification of the left ventricle endocardial border on two-dimensional ultrasound images using deep layer aggregation for residual dense networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embedding temporal networks inductively via mining
neighborhood and community influences. <em>APIN</em>, <em>52</em>(14),
16069–16088. (<a
href="https://doi.org/10.1007/s10489-021-03102-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding aims to generate an embedding for each node in a network, which facilitates downstream machine learning tasks such as node classification and link prediction. Current work mainly focuses on transductive network embedding, i.e. generating fixed node embeddings, which is not suitable for real-world applications. This paper proposes a novel continual temporal network embedding method called ConMNCI by mining neighborhood and community influences inductively. We propose an aggregator function that integrates neighborhood influence with community influence to generate node embeddings at any time, and introduce the idea from continual learning to enhance inductive learning. We conduct extensive experiments on several real-world datasets and compare ConMNCI with several state-of-the-art baseline methods on various tasks, including node classification and network visualization. The experimental results show that ConMNCI significantly outperforms the state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {Liu, Meng and Quan, Zi-Wei and Wu, Jia-Ming and Liu, Yong and Han, Meng},
  doi          = {10.1007/s10489-021-03102-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16069-16088},
  shortjournal = {Appl. Intell.},
  title        = {Embedding temporal networks inductively via mining neighborhood and community influences},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive terminal sliding mode control of stone-carving
robotic manipulators based on radial basis function neural network.
<em>APIN</em>, <em>52</em>(14), 16051–16068. (<a
href="https://doi.org/10.1007/s10489-022-03445-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stone-carving robotic manipulators (SCRM) find a broad range of applications due to their high efficiency, wide range of processing and strong flexibility. In stone carving process, high response speed and high robustness are required to realize the high precision tracking control of SCRM. This paper proposes an adaptive terminal sliding mode control strategy based on radial basis function neural network (RBFNN-based TSM). First, we deduced the dynamic model of SCRM system with the Lagrange method. Then the dynamic model with uncertainties was further considered, the radial basis function neural network (RBFNN) was employed to approximate the manipulator dynamic model. Finally, in order to improve the response speed and tracking accuracy of SCRM, the RBFNN-based TSM control strategy was designed for SCRM, and the high-gain observer was used to estimate the joint velocity information online. The Lyapunov’s theory proved the stability of the algorithm, and the experimental results show that the model-free and chattering-free control was achieved.},
  archive      = {J_APIN},
  author       = {Yin, Fang-Chen and Ji, Qing-Zhi and Wen, Cong-Wei},
  doi          = {10.1007/s10489-022-03445-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16051-16068},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive terminal sliding mode control of stone-carving robotic manipulators based on radial basis function neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on local transport mode detection on the edge of
the network. <em>APIN</em>, <em>52</em>(14), 16021–16050. (<a
href="https://doi.org/10.1007/s10489-022-03214-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a survey of smartphone-based Transport Mode Detection (TMD). We categorize TMD solutions into local and remote; the first ones are addressed in this article. A local approach performs the following steps in the smartphone (and not in some faraway cloud servers): 1) data collection or sensing, 2) preprocessing, 3) feature extraction, and 4) classification (with a previous training phase). A local TMD approach outperforms a remote approach due to less delay, improved privacy, no need for Internet connection, better or equal accuracy and smaller data size. Therefore, we present local TMD solutions taking into account the above mentioned four steps and analyze them according to the most relevant requirements: accuracy, delay, resources consumption and generalization. To achieve the highest accuracy (100%), studies used a different combination of sensors, features and Machine Learning (ML) algorithms. The results suggest that accelerometer and GPS (Global Position System) are the most useful sensors for data collection. Discriminative ML algorithms, such as random forest, outperform the other algorithms for classification. Some solutions improved the delay of the proposed system by using a small window size and a local approach. A few studies could improve battery usage of their system by utilizing low battery-consuming sensors (e.g., accelerometer) and low sampling rate (e.g., 10Hz). CPU usage is primarily dependent on data collection, while memory usage is related to the features and complexity of the ML algorithm. Finally, the generalization requirement is met in studies that consider user, location and position independency into account.},
  archive      = {J_APIN},
  author       = {Kamalian, Mahdieh and Ferreira, Paulo and Jul, Eric},
  doi          = {10.1007/s10489-022-03214-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16021-16050},
  shortjournal = {Appl. Intell.},
  title        = {A survey on local transport mode detection on the edge of the network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent semantic-enhanced discrete hashing for cross-modal
retrieval. <em>APIN</em>, <em>52</em>(14), 16004–16020. (<a
href="https://doi.org/10.1007/s10489-021-03143-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have been proposed for the cross-modal retrieval tasks due to their flexibility and effectiveness. The main idea of cross-modal hashing is to embed heterogeneous multimedia data into common Hamming space. How to effectively exploit the modal semantic information and reduce optimization loss have been a challenging problem for existing cross-modal hashing methods. To address these issues, we propose a supervised cross-modal hashing method, called Latent Semantic-Enhanced discrete Hashing (LSEH). LSEH first leverages matrix factorization to obtain individual latent semantic representations of different modalities, and then applies correlation analysis and kernel discriminant analysis when projecting the latent semantic representations into the common Hamming space. Finally, the binary codes are directly generated with discrete optimization strategy. Experimental results on four benchmark datasets demonstrate that LSEH outperforms state-of-the-art cross-modal hashing methods in terms of retrieval accuracy, especially when dealing with image to text retrieval task, using shorter hash codes to associate images and texts.},
  archive      = {J_APIN},
  author       = {Liu, Yun and Ji, Shujuan and Fu, Qiang and Zhao, Jianli and Zhao, Zhongying and Gong, Maoguo},
  doi          = {10.1007/s10489-021-03143-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {16004-16020},
  shortjournal = {Appl. Intell.},
  title        = {Latent semantic-enhanced discrete hashing for cross-modal retrieval},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DESAC: Differential evolution sample consensus algorithm for
image registration. <em>APIN</em>, <em>52</em>(14), 15980–16003. (<a
href="https://doi.org/10.1007/s10489-022-03266-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration is the process of matching and superimposing multiple images from heterogeneous sources, and it is widely used in areas such as image processing and computer vision. In point feature-based image registration, the RANdom SAmple Consensus (RANSAC) algorithm is commonly used to eliminate mis-matches and solve transform models. Traditional RANSAC tends to sample in sample space. Its discrete estimation of model parameters can lead to an inability to solve for the largest consensus set at large input sizes and low inlier ratio. RANSAC with sampling in parameter space is a novel and effective method, but it is less studied and still has many shortcomings. In this paper, a Differential Evolution SAmple Consensus (DESAC) algorithm based on sampling in parameter space is proposed, which represents the model as individuals and optimizes the model parameters by Differential Evolution (DE). First, for an adequate search of the parameter space, DESAC uses priori information for the first initialization and the resident initialization performed in the iteration. Second, to achieve a balance between exploration and exploitation, a mutation operator combining neighborhood optimal individuals and random individuals is proposed. Third, a novel selection operator is proposed in order to obtain individuals with high number of inliers and low inlier error. Finally, to reduce the verification overhead, a simplified pre-test step is applied before the full verification. Comparing with advanced RANSAC variants in real image dataset, the proposed method can robustly return consensus sets with very high number of inliers. Its average inlier error is lower and the transformation model is more accurate.},
  archive      = {J_APIN},
  author       = {Sun, Yu and Wu, FuXiang},
  doi          = {10.1007/s10489-022-03266-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15980-16003},
  shortjournal = {Appl. Intell.},
  title        = {DESAC: Differential evolution sample consensus algorithm for image registration},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent collaborative control parameter prediction for
intelligent precision loading. <em>APIN</em>, <em>52</em>(14),
15961–15979. (<a
href="https://doi.org/10.1007/s10489-022-03297-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the low adjustment accuracy of manual prediction, conventional programmable logic controller systems can easily lead to inaccurate and unpredictable load problems. The existing multi-agent systems based on various deep learning models has weak ability for advanced multi-parameter prediction while mainly focusing on the underlying communication consensus. To solve this problem, we propose a hybrid model based on a temporal convolutional network with the feature crossover method and light gradient boosting decision trees (called TCN-LightGBDT). First, we select the initial dataset according to the loading parameters&#39; tolerance range and supply supplementing method for the deviated data. Second, we use the temporal convolutional network to extract the hidden data features in virtual loading areas. Further, a two-dimensional feature matrix is reconstructed through the feature crossover method. Third, we combine these features with basic historical features as the input of the light gradient boosting decision trees to predict the adjustment values of different combinations. Finaly, we compare the proposed model with other related deep learning models, and the experimental results show that our model can accurately predict parameter values.},
  archive      = {J_APIN},
  author       = {Chen, Zihua and Wang, Chuanli and Li, Jingzhao and Zhang, Shunxiang and Ouyang, Qichun},
  doi          = {10.1007/s10489-022-03297-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15961-15979},
  shortjournal = {Appl. Intell.},
  title        = {Multi-agent collaborative control parameter prediction for intelligent precision loading},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-DZSL: A meta-dictionary learning based approach to
zero-shot recognition. <em>APIN</em>, <em>52</em>(14), 15938–15960. (<a
href="https://doi.org/10.1007/s10489-022-03257-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning is an essential paradigm for learning novel concepts, i.e., those whose instances were unavailable during training. Dictionary learning approaches have shown recent success in zero-shot applications. However, learning a single dictionary for a complete dataset is time-consuming and prone to underfitting. In this work, we propose Meta − DZSL, a novel meta-dictionary learning-based framework for learning dictionaries in an episodic manner. The source dataset is divided among different episodes in the proposed framework, and target domain data are common for each episode. The parameters learned in one episode become the initializations for the parameters to be learned in the next episode so that at the end of the last episode, optimal parameters are obtained. In our experiments, the data in the two domains come from the same modality. In addition, the base learner in the conventional setting is updated to perform recognition in the generalized setting. We experimented with and without noise on AWA2, SUN, CUB, and all superclasses of the LAD dataset. The results show that the proposed Meta − DZSL is robust to noise, fast, and achieves state-of-the-art results for the LAD − Fruits dataset. For all other datasets, the performance of the proposed approach remains robust and comparable to state-of-the-art results, and the training is computationally less expensive.},
  archive      = {J_APIN},
  author       = {Singh, Upendra Pratap and Singh, Krishna Pratap and Thakur, Manoj},
  doi          = {10.1007/s10489-022-03257-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15938-15960},
  shortjournal = {Appl. Intell.},
  title        = {Meta-DZSL: A meta-dictionary learning based approach to zero-shot recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AP-BERT: Enhanced pre-trained model through average pooling.
<em>APIN</em>, <em>52</em>(14), 15929–15937. (<a
href="https://doi.org/10.1007/s10489-022-03190-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BERT, a pre-trained language model on the large-scale corpus, has made breakthrough progress in NLP tasks. However, the experimental data shows that the BERT model’s application effect in Chinese tasks is not ideal. The reason is that we believe that only character-level embedding can be obtained through BERT. However, a single Chinese character often cannot express their comprehensive meaning. To improve the model’s ability to understand phrase-level semantic information, this paper proposes an enhanced BERT based on the average pooling(AP-BERT). Our model uses an average pooling layer to act on token embedding and reconstructs the model’s input embedding, which can effectively improve BERT’s application effect in Chinese natural language processing. Experimental data show that our proposed method has been enhanced in the four tasks of Chinese text classification, named entity recognition, reading comprehension, and summary generation. This method can not only improve the application effect of the BERT model in Chinese tasks but also can be well applied to other pre-trained language models.},
  archive      = {J_APIN},
  author       = {Zhao, Shuai and Zhang, Tianyu and Hu, Man and Chang, Wen and You, Fucheng},
  doi          = {10.1007/s10489-022-03190-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15929-15937},
  shortjournal = {Appl. Intell.},
  title        = {AP-BERT: Enhanced pre-trained model through average pooling},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample extraction and expansion method with feature
reconstruction and deformation information. <em>APIN</em>,
<em>52</em>(14), 15916–15928. (<a
href="https://doi.org/10.1007/s10489-021-03131-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks often need a large number of data to complete effective training. In the low-data regime, networks perform poor in training effect and generalization ability. Recently,most few-shot learning methods based on the data expansion generate data by utilizing generative adversarial idea,or fulfill data augmentation by directly adopting difference information between similar datasets. The data after expansion will still be lack of important data features without considering whether the features of few-shot are complete before expansion. Nor does it examine whether the adoption of difference information is reasonable, which will generate wrong samples. Therefore, this paper puts forward an adversarial data augmentation model based on feature reconstruction and deformation information. Firstly, it proposes sample extraction method based on feature reconstruction, which is used to improve the feature loss of few-shot, and it adopts feature reconstruction to extract typical few-shot for sample expansion. Moreover, it puts forward the sample expansion method based on deformation information, and it adopts deformation information of different clusters under the same class to fulfill the data expansion. The above mentioned methods are applied to the character datasets and some popular few-shot datasets. The typical few-shot after reconstruction and the dataset after expansion have good effects. Furthermore, the experiment results demonstrate the state-of-the-art performance and effectiveness of the proposed methods.},
  archive      = {J_APIN},
  author       = {Zhang, Zhengchao and Wang, Hongbin and Wang, Nianbin},
  doi          = {10.1007/s10489-021-03131-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15916-15928},
  shortjournal = {Appl. Intell.},
  title        = {Sample extraction and expansion method with feature reconstruction and deformation information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clean and robust affinity matrix learning for multi-view
clustering. <em>APIN</em>, <em>52</em>(14), 15899–15915. (<a
href="https://doi.org/10.1007/s10489-021-03146-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the popularity of multi-view clustering (MVC) has increased, and many MVC methods have been developed. However, the affinity matrix that is learned by the MVC method is only block diagonal if noise and outliers are not included in the data.; however, data always contain noise and outliers. As a result, the affinity matrix is unreliable for subspace clustering because it is neither clean nor robust enough, which affects clustering performance. To compensate for these shortcomings, in this paper, we propose a novel clean and robust affinity matrix (CRAA) learning method for MVC. Specifically, firstly, the global structure of data is obtained by constructing the representation space shared by all views. Next, by borrowing the idea of robust principal component analysis (RPCA), the affinity matrix is divided into two parts, i.e., a cleaner and more robust affinity matrix and a noisy matrix. Then, the two-step procedure is integrated into a unified optimization framework and a cleaner and robust affinity matrix is learned. Finally, based on the augmented Lagrangian multiplier (ALM) method, an efficient optimization procedure for obtaining the CRAA is also developed. In fact, the main idea for obtaining a cleaner and more robust affinity matrix can also be generalized to other MVC methods. The experimental results on eight benchmark datasets show that the clustering performance of the CRAA is better than that of some of the state-of-the-art clustering methods in terms of NMI, ACC, F-score, Recall and ARI metrics.},
  archive      = {J_APIN},
  author       = {Zhao, Jin-Biao and Lu, Gui-Fu},
  doi          = {10.1007/s10489-021-03146-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15899-15915},
  shortjournal = {Appl. Intell.},
  title        = {Clean and robust affinity matrix learning for multi-view clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NCDCN: Multi-focus image fusion via nest connection and
dilated convolution network. <em>APIN</em>, <em>52</em>(14),
15883–15898. (<a
href="https://doi.org/10.1007/s10489-022-03194-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a focus probability learning network called NCDCN is proposed for the multi-focus image fusion (MFIF) task. First, a dense network and a nest connection architecture are combined to construct an encoder alled MSDN to extract the multi-scale focus features with the same size at each level. Then, a dilated convolution-based inception network (DCIN) is designed as the decoder, which has a stronger feature aggregation ability with a small computation cost. Besides, an effective hybrid loss is introduced to effectively train our network. The fidelity loss with ℓ2 norm makes the focus probability approximate its ground-truth; the structural similarity loss makes the focus probability have better similarity in the edge between the focus and defocus regions; the intersection over union loss weakens the sensibility of the fidelity loss to the size of the focus region. Experimental results and analysis show the effectiveness of NCDCN and its superiority over other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Guan, Zheng and Wang, Xue and Nie, Rencan and Yu, Shishuang and Wang, Chengchao},
  doi          = {10.1007/s10489-022-03194-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15883-15898},
  shortjournal = {Appl. Intell.},
  title        = {NCDCN: Multi-focus image fusion via nest connection and dilated convolution network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An influence maximization algorithm based on low-dimensional
representation learning. <em>APIN</em>, <em>52</em>(14), 15865–15882.
(<a href="https://doi.org/10.1007/s10489-022-03178-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) problem is to choose k seed nodes in the social network to maximize the number of nodes that can be affected. As an essential part of the social network analysis, the influence maximization problem has been extensively investigated. Most existing algorithms usually set the activation probability to a fixed value or reciprocal of the in-degree. However, this operation is not accurate because the activation probability is more complicated than those mentioned above in the real social networks. To solve this problem, this paper proposes a new algorithm called Low-Dimensional Representation Learning for IM (LDRLIM) to address the IM problem. The LDRLIM utilizes Discount-degree Descending (DED) search strategy to generate the candidate nodes set ℂ and IC Walk to obtain the influence context of the candidate nodes. The LDRLIM algorithm learns the low-dimensional embedding vectors of influencers and susceptible nodes according to a Multi-task Neural Network Low-dimensional Representation learning model (MNNLR). Afterwards, the Similarity Influence (SI) of the node is obtained according to the representation vectors of the nodes. The LDRLIM algorithm employs SI to consider the potential influence relationship between nodes by embedding vector. Furthermore, the Cost-Effective Lazy Forward (CELF) strategy is used to accelerate the process of selecting the influential nodes, which avoids a large amount of model simulation time to improve efficiency. Therefore, the proposed LDRLIM algorithm is suitable for large-scale social networks. The experimental results on seven real-world datasets indicate that the LDRLIM significantly outperforms other comparison algorithms.},
  archive      = {J_APIN},
  author       = {Liu, Yuening and Qiu, Liqing and Sun, Chengai},
  doi          = {10.1007/s10489-022-03178-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15865-15882},
  shortjournal = {Appl. Intell.},
  title        = {An influence maximization algorithm based on low-dimensional representation learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A large-scale group decision making method to select the
ideal mobile health application for the hospital. <em>APIN</em>,
<em>52</em>(14), 15844–15864. (<a
href="https://doi.org/10.1007/s10489-022-03273-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health, which is not limited by time and space, can effectively alleviate the imbalance of medical resources. Currently, more and more hospitals begin to pay attention to online medical care and actively expand their mobile channels. Among of which, the cooperation with the third-party platform is an effective way to expand the online services of most hospitals. With the increasing number of mobile health applications (mHealth apps), it is difficult to select the ideal application. Most of the existing studies on mHealth app selection are conducted from the perspective of users who have health needs, which is insufficient. The views of multiple stakeholders should be taken into account. mHealth app selection can be regarded as a large-scale group decision making (LSGDM) problem. In this paper, a hybrid LSGDM method is proposed to select the mHealth app with the highest user satisfaction. First, the weights of criteria are obtained based on quality function deployment and 2-additive measure. Furthermore, a consensus model that considers cooperative and non-cooperative behaviors of decision makers is applied to select the ideal mHealth app. Finally, an illustrative example is implemented to exhibit the utility and validity of the proposed model.},
  archive      = {J_APIN},
  author       = {Zhang, Xumin and Meng, Fanyong},
  doi          = {10.1007/s10489-022-03273-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15844-15864},
  shortjournal = {Appl. Intell.},
  title        = {A large-scale group decision making method to select the ideal mobile health application for the hospital},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ensemble discrete water wave optimization algorithm for
the blocking flow-shop scheduling problem with makespan criterion.
<em>APIN</em>, <em>52</em>(14), 15824–15843. (<a
href="https://doi.org/10.1007/s10489-022-03236-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production scheduling plays a pivotal role in smart factories due to the development of intelligent manufacturing. As a typical scheduling problem, the blocking flow-shop scheduling problem (BFSP) has attracted enormous attention from researchers. In this paper, an ensemble discrete water wave optimization algorithm (EDWWO) is proposed with the criterion to minimize the makespan. In the proposed algorithm, a constructive heuristic is presented to suit the needs of initial solutions quality. The constructive heuristic is based on a new dispatching rule combined with the well-known NEH heuristic. The algorithmic characteristics are explored and effective technologies, such as data-driven mechanism in the propagation phase, a block-shifting operator based on the framework of the variable neighborhood search in the breaking phase, and perturbation strategy, are employed to improve the performance of the algorithm. The effectiveness of operators and parameters in EDWWO are analyzed and calibrated based on the design of experiments. To evaluate the algorithmic performance, the well-known benchmark problem is adopted for comparison with five other state-of-the-art algorithms. Meanwhile, the statistical validity of the results is investigated by introducing the Friedman-test and Wilcoxon-test. The statistical results demonstrate the effectiveness of EDWWO for solving the BFSP.},
  archive      = {J_APIN},
  author       = {Zhao, Fuqing and Shao, Dongqu and Xu, Tianpeng and Zhu, Ningning and Jonrinaldi},
  doi          = {10.1007/s10489-022-03236-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15824-15843},
  shortjournal = {Appl. Intell.},
  title        = {An ensemble discrete water wave optimization algorithm for the blocking flow-shop scheduling problem with makespan criterion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preference-driven multi-objective GP search for regression
models with new dominance principle and performance indicators.
<em>APIN</em>, <em>52</em>(14), 15809–15823. (<a
href="https://doi.org/10.1007/s10489-022-03228-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression is a multi-objective optimization task, which aims to determine accurate and simple relationship expressions between variables. Multi-objective genetic programming (MOGP) methods are popularly-used for regression, which search for trade-off solutions of all objectives, producing a set of solutions (Pareto front). Yet, users normally are not interested in the whole Pareto front and select certain solutions based on preference for specific tasks. There are existing preference-based multi-objective methods. However, existing techniques may not be compliant to Pareto dominance, leading convergence to be deteriorated, or require extra parameters. To handle these issues, a preference-driven dominance (pd-dominance) principle is designed, which is Pareto-compliant and parameterless. Then it is introduced into two base MOGP methods (NSGP (non-dominated sorting genetic programming) and SPGP (strength Pareto genetic programming)) to form two new preference-driven MOGP methods, i.e. pdNSGP and pdSPGP. In addition, three existing performance indicators for multi-objective optimization are improved to adapt to regression tasks. Results show that pdNSGP and pdSPGP outperform MOGP methods with popular preference techniques. For example on the function Dic3, pdSPGP reaches $$3E-2$$ based on a distance measure (the lower the better); while the reference MOGP methods achieve $$1.93E-1$$ . Moreover, compared with seven reference regression methods, the proposed methods ranks the first three places for most test cases.},
  archive      = {J_APIN},
  author       = {Liang, Jiayu and Zheng, Ludi and Wu, Han and Xue, Yu},
  doi          = {10.1007/s10489-022-03228-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15809-15823},
  shortjournal = {Appl. Intell.},
  title        = {Preference-driven multi-objective GP search for regression models with new dominance principle and performance indicators},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selection of investment portfolios with social
responsibility: A multiobjective model and a tabu search method.
<em>APIN</em>, <em>52</em>(14), 15785–15808. (<a
href="https://doi.org/10.1007/s10489-022-03169-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a model for the selection of investment portfolios is proposed with three objectives. In addition to the traditional objectives of maximizing profitability and minimizing risk, maximization of social responsibility is also considered. Moreover, with the purpose of controlling transaction costs, a limit is placed on the number of assets for selection. To the best of our knowledge, this specific model has not been considered in the literature to date. This model is difficult (NP-Hard), and therefore, only very small instances may be solved in an exact way. This paper proposes a method based on tabu search and multiobjective adaptive memory programming (MOAMP) strategies. With this method it is possible to obtain sets of nondominated solutions in short computational times. To check the performance of our method it is compared with adaptations of the nondominated sorting genetic algorithm (NSGA-II), strength Pareto evolutionary algorithm (SPEA-II) and multiobjective particle swarm optimization (MOPSO). The results of different computational experiments show that our tabu search-MOAMP method performed best. The quality of the sets of solutions that were obtained and the speed of execution mean that our tabu search-MOAMP can be used as a tool for financial assessment and analysis (including online services). This tool, as we can see in this work with some examples, can take into account the social concerns of many clients and their overall risk profile (very conservative, conservative, moderate, or fearless). This approach is also in line with current legal regulations that oblige financial advisors to take the client profile into account to provide greater protection and propose good financial advice.},
  archive      = {J_APIN},
  author       = {Pacheco, Joaquín and Cepa, Lara and Puche, Julio and Casado, Silvia},
  doi          = {10.1007/s10489-022-03169-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15785-15808},
  shortjournal = {Appl. Intell.},
  title        = {Selection of investment portfolios with social responsibility: A multiobjective model and a tabu search method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Item sequential recommendation based on graph embedding
model. <em>APIN</em>, <em>52</em>(14), 15764–15784. (<a
href="https://doi.org/10.1007/s10489-022-03452-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of low recommendation accuracy, caused by the low accuracy of item feature representation and data sparsity in item sequential recommendation, this paper proposes an item sequential recommendation method based on a graph embedding model (SAEGES-SSE-PT). Firstly, based on user–item historical interaction data, a directed weighted graph of items is constructed. Secondly, the graph embedding model integrating side information and the self-attention mechanism (SAEGES, proposed in this paper) is used to learn the feature representation vector of each item in the graph. Finally, after integrating the image and text information of items into side information, the item feature representation vector learned by SAEGES is used in the most advanced sequential recommendation method via personalized transformer (SSE-PT). SAEGES integrates the self-attention mechanism into the enhanced graph embedding with side information (EGES) to capture the dependency relationship between different pieces of side information, and integrates image and text information of items into side information, to further alleviate data sparsity and thereby improve the feature representation accuracy of items. By inputting the high-accuracy item feature vectors learned by SAEGES to SSE-PT, the recommendation accuracy is further improved. Because a large number of parameters to be trained about item embedding are omitted by this process, the execution time is also reduced. Experimental results on the Jdata, HetRec2011, and MIND-small datasets show that SAEGES is superior to DEEPWALK, Node2vec, and EGES, in respect of AUC. SAEGES-SSE-PT is also superior to the self-attention-based sequential model (SASRec) and SSE-PT in respect of Normalized Discounted Cumulative Gain (NDCG), recall, and execution time. The sparsity experiment results on the Jdata dataset show that SAEGES-SSE-PT is better than SASRec and SSE-PT even in the case of sparse data.},
  archive      = {J_APIN},
  author       = {Zhang, Chenkun and Wang, Cheng},
  doi          = {10.1007/s10489-022-03452-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15764-15784},
  shortjournal = {Appl. Intell.},
  title        = {Item sequential recommendation based on graph embedding model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Post-processing method with aspect term error correction for
enhancing aspect term extraction. <em>APIN</em>, <em>52</em>(14),
15751–15763. (<a
href="https://doi.org/10.1007/s10489-022-03380-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Term Extraction (ATE), which aims to extract aspect terms from review sentences, is an important subtask of sentiment analysis. Existing studies have proposed many sequence taggers, which have achieved impressive progress. However, previous work ignores the errors in aspect terms extracted by sequence taggers. This paper points out that there are aspect number error and aspect boundary error in the extracted aspect terms, which have a severe negative impact on the performance of the model on ATE task. We design post-processing method with aspect term error correction, which contain aspect number determining module and aspect boundary modifying module, to address the errors. To solve the inconsistency between the number of extracted aspect terms and the number of ground-truth aspect terms, we use the aspect number determining module to control the number of extracted aspect terms to match the number of ground-truth aspect terms. For the problem that the boundary of the extracted aspect term is not completely matched with the boundary of the ground-truth aspect term, we utilize the aspect boundary modifying module to correct the boundary of the extracted aspect term to make it the same as the boundary of the ground-truth aspect term. Experiments on four SemEval datasets show that the post-processing method can address the errors in extracted aspect terms, and our model outperforms other SOTA models. Our post-processing method can be coupled with various sequence taggers to boost their performance and have well transferability.},
  archive      = {J_APIN},
  author       = {Wang, Ruyan and Liu, Chengxin and Zhao, Rongjian and Yang, Zhigang and Zhang, Puning and Wu, Dapeng},
  doi          = {10.1007/s10489-022-03380-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15751-15763},
  shortjournal = {Appl. Intell.},
  title        = {Post-processing method with aspect term error correction for enhancing aspect term extraction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spatiotemporal attention-based neural network to evaluate
the route risk for unmanned aerial vehicles. <em>APIN</em>,
<em>52</em>(14), 15735–15750. (<a
href="https://doi.org/10.1007/s10489-021-03029-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Route risk evaluation is crucial for planning safe routes when unmanned aerial vehicles (UAVs) perform missions in hostile environments. The purpose of route risk evaluation is to fuse the intents, capabilities, and opportunities of the enemy to predict the damage to UAV in the future. Opportunities depend on the current as well as historical situation of the battlefield and are difficult to estimate effectively for the existing risk evaluation models. We propose a novel spatiotemporal attention-based evaluation network (STAEN) to automatically evaluate the route risk. In particular, the spatiotemporal attention values provided by the STAEN can reflect the opportunities of the enemy to threaten the UAV, which helps to understand the spatiotemporal evolutions of the situations on different routes. In addition, the network can automatically focus on the key route segments and defense subsystems in different evolution stages, to evaluate the route risk more accurately. The validity and interpretability of the evaluation network are verified by simulation experiments.},
  archive      = {J_APIN},
  author       = {Guo, Jun and Xia, Wei and Hu, Xiaoxuan and Ma, Huawei},
  doi          = {10.1007/s10489-021-03029-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {14},
  pages        = {15735-15750},
  shortjournal = {Appl. Intell.},
  title        = {A spatiotemporal attention-based neural network to evaluate the route risk for unmanned aerial vehicles},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering periodic cluster patterns in event sequence
databases. <em>APIN</em>, <em>52</em>(13), 15387–15404. (<a
href="https://doi.org/10.1007/s10489-022-03186-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since periodic events are very common everywhere, periodic pattern mining is increasingly more important in today’s data mining domain. However, there is currently no uniform definition of periodic patterns, and all of these definitions are incapable of discovering seasonally prevalent events. In this paper, we first define the periodic event based on the coefficient of variation of the event’s periods in event sequence. Then, in order to discover seasonally prevalent events, we propose a new concept of periodic cluster patterns and design an efficient algorithm named the PCPM(Periodic Cluster Pattern Miner) to mine periodic cluster patterns in event sequence datasets. To illustrate the application of periodic cluster patterns, we propose a new method employed periodic cluster pattern prediction for next basket recommendation, and the method is named PCPP(Periodic Cluster Pattern Predictor). Experiments show that the PCPM is effective for periodic cluster pattern mining and that PCPP has performances close to those of the baseline methods on four real-world transaction datasets. Furthermore, we believe that periodic cluster patterns, as a new concept, will have a wider application in other domains, such as time series prediction, meteorological forecasting, etc.},
  archive      = {J_APIN},
  author       = {Chen, Guisheng and Li, Zhanshan},
  doi          = {10.1007/s10489-022-03186-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15387-15404},
  shortjournal = {Appl. Intell.},
  title        = {Discovering periodic cluster patterns in event sequence databases},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An overview of probabilistic preference decision-making
based on bibliometric analysis. <em>APIN</em>, <em>52</em>(13),
15368–15386. (<a
href="https://doi.org/10.1007/s10489-022-03189-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic preference decision-making (PPDM) is a branch of uncertain decision-making, which identifies decision-makers’ preferences with probability to deal with uncertain decision-making problems more comprehensively. In order to understand the status and development process of publications related to PPDM, this paper conducts a bibliometric study. Based on the Web of Science (WoS) core collection, a total of 487 publications are obtained. Firstly, this paper makes a general analysis of the basic characteristics of PPDM, including the type of articles, the number of annual publications and the research directions. Secondly, according to the information of the country/region/institution/author/journal regarding the publications, the publication structure is analyzed, the most productive items are explored, and the partnership of each item is detected with the VOSviewer. The paper then examines the citation structure of PPDM publications, uses the VOSviewer to visualize the citation network and discusses the most influential items. Finally, VOSviewer and Bibliometrix are used to analyze the keywords and study the work focus and current hot topics of PPDM. The keywords are divided into three stages in chronological order to analyze the development trend and theme evolution of the research topics. Based on a series of analyses, we further discuss the characteristics of PPDM publications, give some reasonable suggestions and draw some main conclusions. This research thoroughly analyzes and summarizes the essential features, research status and development trend of PPDM publications, which is helpful for the interested researchers to carry out further scientific research.},
  archive      = {J_APIN},
  author       = {Xu, Zeshui and Lei, Tiantian and Qin, Yong},
  doi          = {10.1007/s10489-022-03189-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15368-15386},
  shortjournal = {Appl. Intell.},
  title        = {An overview of probabilistic preference decision-making based on bibliometric analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal tuning of FOPID controller for higher order process
using hybrid approach. <em>APIN</em>, <em>52</em>(13), 15345–15367. (<a
href="https://doi.org/10.1007/s10489-022-03167-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an optimal tuning of fractional order proportional integral derivative (FOPID) controller for higher order process using hybrid approach. The proposed hybrid approach is the joint execution of Dynamic Differential Annealed Optimization (DDAO) and Feedback Artificial tree (FAT) algorithm, hence it is named D2AOFAT approach. The FOPID controller parameters like kp, ki, kd, λ andμ. The FOPID controller parameters errors are minimized and predict the optimal parameters by the FAT algorithm. Based on FOPID controller parameters using FAT algorithm, the DDAO optimizes the FOPID controller parameters. The FOPID controller advantage is adjusted to accomplish that needed responses that are resolute with FAT theory and RDF parameters are predictable using DDAO technique. The purpose of the proposed control system is selected in light of the achieved parameters of time delay system (TDS). The proposed technique is carried out in MATLAB / Simulink, its performance is compared to the existing techniques, like Ziegler-Nichols fit, Curve Fit, Wang method, and IWLQR method.},
  archive      = {J_APIN},
  author       = {George, Thomas and Ganesan, V.},
  doi          = {10.1007/s10489-022-03167-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15345-15367},
  shortjournal = {Appl. Intell.},
  title        = {Optimal tuning of FOPID controller for higher order process using hybrid approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chaotic slime mould algorithm for economic load dispatch
problems. <em>APIN</em>, <em>52</em>(13), 15325–15344. (<a
href="https://doi.org/10.1007/s10489-022-03179-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The economic load dispatch (eld) problem strives to optimize the division of total power demand among the power generators under specified constraints. It is solved by scheduling the generating units of a power plant that meet the load demand with minimum generation cost while satisfying various equality and inequality constraints. Achieving global optimal points is considered difficult due to the involvement of a non-linear objective function and large search domain. The slime mould algorithm (SMA) was recently proposed to solve complex problems. Its convergence rate and capability of capturing optimal global solutions are pretty satisfactory. In this paper, a chaotic number-based slime mould algorithm (CSMA) is suggested for ELD problems the first time. Five test cases with different power demands have been considered to compare the performance of the proposed approach against SMA, salp swarm algorithm (SSA), moth flame optimizer (MFO), grey wolf optimizer (GWO), biogeography based optimizer (BBO), grasshopper optimization algorithm (GOA), multi-verse optimizer (MVO) on 6, 13, 15, 40, and 140 generators ELD problems. The experimental results show that the proposed algorithm reduces the total generation cost significantly. CSMA outperformed SMA in all test cases that justify the effectiveness of chaotic sequences used in the proposed work. Further, three statistical tests have been conducted to justify the competitiveness of the suggested approach.},
  archive      = {J_APIN},
  author       = {Singh, Tribhuvan},
  doi          = {10.1007/s10489-022-03179-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15325-15344},
  shortjournal = {Appl. Intell.},
  title        = {Chaotic slime mould algorithm for economic load dispatch problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MGA-YOLOv4: A multi-scale pedestrian detection method based
on mask-guided attention. <em>APIN</em>, <em>52</em>(13), 15308–15324.
(<a href="https://doi.org/10.1007/s10489-021-03061-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of numerous deep convolutions in YOLOv4, which generates many redundant background features so that it cannot focus on pedestrians at a specific scale, we propose a method named MGA-YOLOv4 (Mask-Guided Attention YOLOv4) that can dynamically select the most crucial features from a cluttered background. First, we design a semantic segmentation encode-decode network to generate a fine-grained pixel-level mask that is used to serve as a weakly supervised signal in each detection branch. Second, we build a mask-guided attention module by producing attention weights of the channel dimension and space dimension and then encode them into the mask to highlight pedestrians of a specific scale and avoid background interference. To prove the effectiveness of MGA, we demonstrate the network attention map and design ablation experiments. The results show that the miss rate of the proposed method combined with the channel concatenate space decreased by 1.82% compared with the original YOLOv4. Comparison experiment results on five challenging pedestrian detection datasets show that our method achieves very competitive performance with the state-of-the-art methods and reaches a favourable trade-off between speed and accuracy.},
  archive      = {J_APIN},
  author       = {Wang, Tingting and Wan, Liang and Tang, Lu and Liu, Mingsheng},
  doi          = {10.1007/s10489-021-03061-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15308-15324},
  shortjournal = {Appl. Intell.},
  title        = {MGA-YOLOv4: A multi-scale pedestrian detection method based on mask-guided attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). ENG-face: Cross-domain heterogeneous face synthesis with
enhanced asymmetric CycleGAN. <em>APIN</em>, <em>52</em>(13),
15295–15307. (<a
href="https://doi.org/10.1007/s10489-022-03302-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous face synthesis has attracted more and more attentions recently. The lack of paired heterogeneous face images triggers high variability of the same identity, which is challenging for heterogeneous face synthesis. Unpaired asymmetric image-to-image translation methods provide a solution for translating heterogeneous face images. However, the inherently uneven informative setting in heterogeneous face images raises two issues: how to synthesize high-fidelity heterogeneous face images, and how to preserve heterogeneous identity consistency. We therefore propose an enhanced asymmetric CycleGAN (ENG-Face) framework. Specifically, asymmetric generators and auxiliary encoder are elaborately designed to solve mapping ambiguity. Identity preservation with edge detection module is proposed to filter out non-facial area errors so that the identity consistency loss can accurately penalize the error area of face. To validate the effectiveness of ENG-Face, we conduct experiments on three datasets: CASIA NIR-VIS 2.0, Oulu-CASIA NIR-VIS and CUHK student. The experimental results, which are evaluated qualitatively and quantitatively, show that ENG-Face is effective for heterogeneous face synthesis in unpaired domains.},
  archive      = {J_APIN},
  author       = {Zhang, Yinghui and Yu, Lejun and Sun, Bo and He, Jun},
  doi          = {10.1007/s10489-022-03302-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15295-15307},
  shortjournal = {Appl. Intell.},
  title        = {ENG-face: Cross-domain heterogeneous face synthesis with enhanced asymmetric CycleGAN},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAOD: Refined oriented detector with augmented feature in
remote sensing images object detection. <em>APIN</em>, <em>52</em>(13),
15278–15294. (<a
href="https://doi.org/10.1007/s10489-022-03393-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a challenging task in remote sensing. Aerial images are distinguished by complex backgrounds, arbitrary orientations, and dense distributions. Considering those difficulties, this paper proposes a two-stage refined oriented detector with augmented features named RAOD. First, a novel Augmented Feature Pyramid Network (A-FPN) is built to enhance fusion both in spatial and channel dimensions. Specifically, it mainly consists of three modules: Scale Transfer Module (STM), Feature Aggregate Module (FAM) and Feature Refinement Module (FRM). STM reduces information loss when fusing features in the top-down pathway. FAM aggregates features from different scales. FRM aims to refine the integrated features using a lightweight attention module. Then, we adopt a two-step processing, which consists of a coarse stage and a refinement stage. In the coarse stage, deformable RoI pooling is adopted to improve the network’s ability of modeling spatial transformations and then horizontal proposals are transformed into oriented ones. In the refinement stage, Rotated RoI align (RRoI align) is used to extract rotation-invariant features from rotated RoIs and further optimize the localization. To enhance stability and robustness during training, smooth Ln is chosen as regression loss as it has better ability in terms of robustness and stability than smooth L1 loss. Extensive experiments on several rotation detection datasets demonstrate the effectiveness of our method. Results show that our method is able to achieve 79.78%, 74.7% and 94.82% on DOTA-v1.0, DOTA-v1.5 and HRSC2016, respectively.},
  archive      = {J_APIN},
  author       = {Shi, Qin and Zhu, Yu and Fang, Chuantao and Wang, Nan and Lin, Jiajun},
  doi          = {10.1007/s10489-022-03393-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15278-15294},
  shortjournal = {Appl. Intell.},
  title        = {RAOD: Refined oriented detector with augmented feature in remote sensing images object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning based bank card detection and recognition
method in complex scenes. <em>APIN</em>, <em>52</em>(13), 15259–15277.
(<a href="https://doi.org/10.1007/s10489-021-03119-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of mobile Internet, mobile payment has become a part of daily life, and bank card recognition in natural scenes has become a hot topic. Although printed character recognition has achieved remarkable success in recent years, bank card recognition is not limited to traditional printed character recognition. There are two types of bank cards: unembossed bank cards, such as most debit cards which usually use printed characters, and embossed bank cards, such as most credit cards which mainly use raised characters. Recognition of raised characters is very challenging due to its own characteristics, and there is a lack of fast and good methods to handle it. To better recognize raised characters, we propose an effective method based on deep learning to detect and recognize bank cards in complex natural scenes. The method can accurately recognize the card number characters on embossed and unembossed bank cards. First, to break the limitation that YOLOv3 algorithm is usually used for object detection, we propose a novel approach that enables YOLOv3 to be used not only for bank card detection and classification, but also for character recognition. The CANNYLINES algorithm is used for rectification and the Scharr operator is introduced to locate the card number region. The proposed method can satisfy bank card detection, classification and character recognition in complex natural scenes, such as complex backgrounds, distorted card surfaces, uneven illumination, and characters with the same or similar color to the background. To further improve the recognition accuracy, a printed character recognition model based on ResNet-32 is proposed for the unembossed bank cards. According to the color and morphological characteristics of embossed bank cards, raised character recognition model combining traditional morphological methods and LeNet-5 convolutional neural network is proposed for the embossed bank cards. The experimental results on the collected bank card dataset and bank card number dataset show that our proposed method can effectively detect and identify different types of bank cards. The accuracy of the detection and classification of bank cards reaches 100%. The accuracy of the raised characters recognition on the embossed bank card is 99.31%, and the accuracy of the printed characters recognition on the unembossed bank card reaches 100%.},
  archive      = {J_APIN},
  author       = {Lin, Hanyang and Zhan, Yongzhao and Liu, Shiqin and Ke, Xiao and Chen, Yuzhong},
  doi          = {10.1007/s10489-021-03119-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15259-15277},
  shortjournal = {Appl. Intell.},
  title        = {A deep learning based bank card detection and recognition method in complex scenes},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-contrast lesion segmentation in advanced MRI experiments
by time-domain ricker-type wavelets and fuzzy 2-means. <em>APIN</em>,
<em>52</em>(13), 15237–15258. (<a
href="https://doi.org/10.1007/s10489-022-03184-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated suspicious region segmentation has become a crucial need for the experts dealing with numerous images containing contrast-based lesions in MRI. Not all solutions, however, are based on mathematical infrastructure or providing adequate flexibility. On the other hand, segmentation of low-contrast lesions is very challenging for researchers; therefore, advanced magnetic resonance imaging (MRI) experiments are not commonly used in researches. Given the need of repeatability and adaptability, we present an automated framework for intelligent segmentation of brain lesions by wavelet imaging and fuzzy 2-means. Besides the general use of the wavelets in image processing, which is edge detection; we employed the second-order Ricker-type wavelets as the core of our novel imaging framework for low-contrast lesion segmentation. We firstly introduced the mathematical basis of several Ricker wavelet functions, which are in symmetrical form satisfying finite-energy and admissibility conditions of mother wavelets. Afterwards, we investigated three types of Ricker wavelets to apply on our clinical dataset containing susceptibility-weighted (SW) and minimum intensity projection SW (mIP-SW) images with barely-visible lesions. Finally, we adjusted the system parameters of the wavelets for optimization and post-segmentation by fuzzy 2-means. According to the preliminary results of the clinical experiments we conducted, our framework provided 93.53% average dice score (DSC) for SWI by Ricker-3 and 92.56% for mIP-SWI by Ricker-2 wavelet, as the main performance criteria of segmentation. Despite the lack of SWI or mIP-SWI experiments in the public datasets, we tested our framework with BraTS 2012 training sets containing real images with visible lesions and achieved an average of 88.13% DSC with 11.66% standard deviation by re-optimized framework for whole lesion segmentation, which is one of the highest among other relevant researches. In detail, 87.52% DSC for LG datasets with 11.32% standard deviation; while 88.34% DSC for HG datasets with 11.77% standard deviation are calculated.},
  archive      = {J_APIN},
  author       = {Alpar, Orcan and Dolezal, Rafael and Ryska, Pavel and Krejcar, Ondrej},
  doi          = {10.1007/s10489-022-03184-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15237-15258},
  shortjournal = {Appl. Intell.},
  title        = {Low-contrast lesion segmentation in advanced MRI experiments by time-domain ricker-type wavelets and fuzzy 2-means},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavioural synthesis of SGD using the CCC framework: A
simple XOR-solving MLP. <em>APIN</em>, <em>52</em>(13), 15226–15236. (<a
href="https://doi.org/10.1007/s10489-022-03376-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioural synthesis enables the automation of the design process by generating task-specific hardware configured for either FPGA and SoC platforms or custom silicon devices such as ASICs. Relevant commercial tools’ flows can bring significant benefits for software developers with no hardware design expertise. Our Custom Coprocessor Compilations (CCC) high level synthesis tool is leveraged in this work to synthesize a FPGA design for stochastic gradient descent (SGD), a cornerstone optimization approach into today’s modern deep neural networks. A simple 3-input-XOR-solving, multilayer perceptron (MLP) is implemented and transformed into a Register Transfer Level (RTL) VHDL hardware microarchitecture using the CCC hardware synthesizer. The produced VHDL is subsequently verified for correct functionality in GNU Ada. Results validate our motivation for accelerated performance, targeted to low-powered, autonomous devices.},
  archive      = {J_APIN},
  author       = {Amanatidis, Dimitrios and Dossis, Michael},
  doi          = {10.1007/s10489-022-03376-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15226-15236},
  shortjournal = {Appl. Intell.},
  title        = {Behavioural synthesis of SGD using the CCC framework: A simple XOR-solving MLP},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronously tracking entities and relations in a
syntax-aware parallel architecture for aspect-opinion pair extraction.
<em>APIN</em>, <em>52</em>(13), 15210–15225. (<a
href="https://doi.org/10.1007/s10489-022-03286-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Opinion Pair Extraction (AOPE) task aims to capture each aspect with its corresponding opinions in user reviews. Entity recognition and relation detection are two fundamental subtasks of AOPE. Although recent works take interaction into account, the two subtasks are still relatively independent during calculation. Furthermore, since AOPE task has not been formally proposed for a long time, syntactic information does not attract much attention in the current deep learning models for AOPE. In this paper, we propose a model for Synchronously Tracking Entities and Relations (STER) to deal with AOPE. Specifically, we design a network consisting of a bank of gated RNNs, where we can track all entities of a review sentence in parallel. STER utilizes three features, i.e., context, syntax and relation, to learn the representation of each tracked entity and calculate the correlated degree between all entities synchronously at each time step. The entity representation and the correlated degree are highly dependent during calculation. Finally, they will be used for entity recognition and relation detection, respectively. Therefore, in STER, the two subtasks of AOPE can achieve sufficient interaction, which enhances their mutual heuristic effect heavily. To verify the effectiveness and adaptiveness of our model, we conduct experiments on two annotation versions of SemEval datasets. The results demonstrate that STER not only achieves advanced performances but adapts to different annotation strategies well.},
  archive      = {J_APIN},
  author       = {Zhang, Yue and Peng, Tao and Han, Ridong and Han, Jiayu and Yue, Lin and Liu, Lu},
  doi          = {10.1007/s10489-022-03286-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15210-15225},
  shortjournal = {Appl. Intell.},
  title        = {Synchronously tracking entities and relations in a syntax-aware parallel architecture for aspect-opinion pair extraction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limited text speech synthesis with electroglottograph based
on bi-LSTM and modified tacotron-2. <em>APIN</em>, <em>52</em>(13),
15193–15209. (<a
href="https://doi.org/10.1007/s10489-021-03075-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a framework of applying only the EGG signal for speech synthesis in the limited categories of contents scenario. EGG is a sort of physiological signal which can reflect the trends of the vocal cord movement. Note that EGG’s different acquisition method contrasted with speech signals, we exploit its application in speech synthesis under the following two scenarios. (1) To synthesize speeches under high noise circumstances, where clean speech signals are unavailable. (2) To enable dumb people who retain vocal cord vibration to speak again. Our study consists of two stages, EGG to text and text to speech. The first is a text content recognition model based on Bi-LSTM, which converts each EGG signal sample into the corresponding text with a limited class of contents. This model achieves 91.12% accuracy on the validation set in a 20-class content recognition experiment. Then the second step synthesizes speeches with the corresponding text and the EGG signal. Based on modified Tacotron-2, our model gains the Mel cepstral distortion (MCD) of 5.877 and the mean opinion score (MOS) of 3.87, which is comparable with the state-of-the-art performance and achieves an improvement by 0.42 and a relatively smaller model size than the origin Tacotron-2. Considering to introduce the characteristics of speakers contained in EGG to the final synthesized speech, we put forward a fine-grained fundamental frequency modification method, which adjusts the fundamental frequency according to EGG signals and achieves a lower MCD of 5.781 and a higher MOS of 3.94 than that without modification.},
  archive      = {J_APIN},
  author       = {Chen, Lijiang and Ren, Jie and Chen, Pengfei and Mao, Xia and Zhao, Qi},
  doi          = {10.1007/s10489-021-03075-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15193-15209},
  shortjournal = {Appl. Intell.},
  title        = {Limited text speech synthesis with electroglottograph based on bi-LSTM and modified tacotron-2},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A heterogeneous multi-attribute case retrieval method based
on neutrosophic sets and TODIM for emergency situations. <em>APIN</em>,
<em>52</em>(13), 15177–15192. (<a
href="https://doi.org/10.1007/s10489-022-03240-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-attribute case retrieval is a crucial step in generating emergency alternatives during the course of emergency decision making (EDM) by referring to historical cases. This paper develops a heterogeneous multi-attribute case retrieval method for EDM that considers five attribute formats: crisp numbers, interval numbers, intuitionistic fuzzy numbers, single-valued neutrosophic numbers (SvNNs), and interval-valued neutrosophic numbers (IvNNs). First, we propose a similarity measurement of IvNNs and calculate the attribute similarities for the five attribute formats. The attribute weights are established using an optimal model. Next, the case similarities are calculated and the set of the similar historical cases is constructed. Furthermore, the evaluated information based on heterogeneous multi-attribute from similar historical cases is provided, and the calculation method for the evaluation of utility based on TODIM (an acronym for interactive and multi-criteria decision-making in Portugese) is proposed. The most suitable historical case is determined based on the case similarity and the evaluated utility. From this, the emergency alternative is generated. Finally, we demonstrate the efficacy of the proposed method with a case study and conduct comparisons against the performance of existing methods to assess the validity and superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Zhang, Kai and Zheng, Jing and Wang, Ying-Ming},
  doi          = {10.1007/s10489-022-03240-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15177-15192},
  shortjournal = {Appl. Intell.},
  title        = {A heterogeneous multi-attribute case retrieval method based on neutrosophic sets and TODIM for emergency situations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fusion of ANNs as decoder of retinal spike trains for scene
reconstruction. <em>APIN</em>, <em>52</em>(13), 15164–15176. (<a
href="https://doi.org/10.1007/s10489-022-03402-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retina is one of the most developed sensing organs in the human body. However, the knowledge on the coding and decoding of the retinal neurons are still rather limited. Compared with coding (i.e., transforming visual scenes to retinal spike trains), the decoding (i.e., reconstructing visual scenes from spike trains, especially those of complex stimuli) is more complex and receives less attention. In this paper, we focus on the accurate reconstruction of visual scenes from their spike trains by designing a retinal spike train decoder based on the combination of the Fully Connected Network (FCN), Capsule Network (CapsNet) and Convolutional Neural Network (CNN), and a loss function incorporating the structural similarity index measure (SSIM) and L1 loss. CapsNet is used to extract the features from the spike trains, that are fused with the original spike trains and used as the inputs to FCN and CNN to facilitate the scene reconstruction. The feasibility and superiority of our model are evaluated on five datasets (i.e., MNIST, Fashion-MNIST, Cifar-10, Celeba-HQ and COCO). The model is evaluated quantitatively with four image evaluation indices, i.e., SSIM, MSE, PSNR and Intra-SSIM. The results show that the model provides a new means for decoding visual scene stimuli from retinal spike trains, and promotes the development of brain-machine interfaces.},
  archive      = {J_APIN},
  author       = {Li, Wei and Joseph Raj, Alex Noel and Tjahjadi, Tardi and Zhuang, Zhemin},
  doi          = {10.1007/s10489-022-03402-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15164-15176},
  shortjournal = {Appl. Intell.},
  title        = {Fusion of ANNs as decoder of retinal spike trains for scene reconstruction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bat4CEP: A bat algorithm for mining of complex event
processing rules. <em>APIN</em>, <em>52</em>(13), 15143–15163. (<a
href="https://doi.org/10.1007/s10489-022-03256-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex Event Processing (CEP) is a modern software technology for the dynamic analysis of continuous data streams. CEP is able of searching extremely large data streams in real time for the presence of event patterns. So far, specifying event patterns of CEP rules is still a manual task based on the expertise of domain experts. This paper presents a novel bat-inspired swarm algorithm for automatically mining CEP rule patterns that express the relevant causal and temporal relations hidden in data streams. The basic suitability and performance of the approach is proven by extensive evaluation with both synthetically generated data and real data from the traffic domain.},
  archive      = {J_APIN},
  author       = {Bruns, Ralf and Dunkel, Jürgen},
  doi          = {10.1007/s10489-022-03256-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15143-15163},
  shortjournal = {Appl. Intell.},
  title        = {Bat4CEP: A bat algorithm for mining of complex event processing rules},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bi-layer decomposition algorithm for many-objective
optimization problems. <em>APIN</em>, <em>52</em>(13), 15122–15142. (<a
href="https://doi.org/10.1007/s10489-021-03135-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition-based evolutionary algorithms have shown great potential in solving many-objective optimization problems (MaOPs). However, manual parameters (e.g., neighbor size and scalarizing function) and the specified weight vector can easily degrade the performance of the algorithm, especially for MaOPs with irregular Pareto fronts. This paper presents a new decomposition-based evolutionary algorithm that adopts a bi-layer decision strategy to balance convergence and diversity of solutions for MaOPs, and it is free from neighborhood update and scalarizing methods. In the first layer decision, the well-converged solutions in each subregion form a primary population, where an adaptive fitness assignment considering the Pareto front shape is used to accelerate convergence. In the second layer decision, solutions in the primary population are ranked based on the diversity metric. The low-ranking solutions are added to the final population size until the population size is met. Further, to approximate the true PF as soon as possible, the intensity of convergence in the first layer decision and the activation frequency of the re-balance strategy are regulated. Moreover, we design a re-balance selection strategy to alleviate the dilemma of the specified weight vectors. The re-balance selection uses a clustering approach to adjust weight vectors to promote the uniform distribution of solutions. Finally, algorithms are verified on 150 test instances and one practical design problem. The experimental results show that the proposed algorithm performs better than five state-of-the-art peer algorithms on at least 64% test instances concerning hypervolume and has superiority over competitors on at least 72% test instances concerning the inverted generational distance plus.},
  archive      = {J_APIN},
  author       = {Zhao, Chunliang and Zhou, Yuren and Hao, Yuanyuan and Zhang, Guangyu},
  doi          = {10.1007/s10489-021-03135-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15122-15142},
  shortjournal = {Appl. Intell.},
  title        = {A bi-layer decomposition algorithm for many-objective optimization problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hinge attention network: A joint model for diabetic
retinopathy severity grading. <em>APIN</em>, <em>52</em>(13),
15105–15121. (<a
href="https://doi.org/10.1007/s10489-021-03043-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Retinopathy is one of the prominent reasons for permanent blindness in working age, long term diabetic patients. With the prevalence in raise of diabetics, majority of the people are endangered to permanent vision loss. The advancements in medical imaging techniques enabled the research community to focus on developing automated and computerized systems for diagnosing retinopathy in early stages. But, it is a very complex challenge due to the presence of high intra-class variations and imbalanced data distribution for higher grades of severity. In recent years, various deep learning based models have been designed for automating the process of retinopathy severity classification. In this research work, we present a fascinating deep learning model with multiple attention stages called Hinge Attention Network (HA-Net). Proposed model consists of a pre-trained VGG16 base to extract initial spatial representation from retinal scan images, spatial attention autoencoder to learn lesion specific latent representations in spatial dimensions and a channel attention based hinge neural network to grab category based discriminative features in channel dimension and classify the severity grade of retinopathy. In addition to spatial and channel attention mechanism, we use Convolutional LSTM layer to prioritize highly important spatial maps before passing to hinge neural network. All these components of HA-Net, enabled it to make generalised and accurate predictions on unseen data. The effectiveness and acceptability of proposed model is proved by validating it using two benchmark datasets, Kaggle APTOS 2019 and ISBI IDRiD. Extensive experimental studies on these datasets reveal that, proposed HA-Net outstrip several existing models by achieving an accuracy of 85.54% on Kaggle APTOS, and an accuracy of 66.41% on IDRiD datasets.},
  archive      = {J_APIN},
  author       = {Shaik, Nagur Shareef and Cherukuri, Teja Krishna},
  doi          = {10.1007/s10489-021-03043-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15105-15121},
  shortjournal = {Appl. Intell.},
  title        = {Hinge attention network: A joint model for diabetic retinopathy severity grading},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph topology enhancement for text classification.
<em>APIN</em>, <em>52</em>(13), 15091–15104. (<a
href="https://doi.org/10.1007/s10489-021-03113-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) can deal with complex network structures and model complex syntax structures in natural languages, which makes GNN outstanding in text classification tasks. However, most graph neural network approaches don’t seem to take full advantage of the topological gains from document graphs. In this paper, we propose a topologically enhanced text classification method to make full use of the structural features of corpus graph and sentence graph. Specifically, we construct two different graphs based on contextual information, called sentence graphs and corpus graphs, respectively. We extract the topological features of words from the corpus graph and inject them into the graph neural network model based on sentence graph classification. To better integrate the topological features, we propose an asynchronous weighted propagation scheme, which selectively fuses the topological features with the original features of the word nodes, and integrate document features to predict final results. A large number of experiments on eight datasets demonstrate the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Song, Rui and Giunchiglia, Fausto and Zhao, Ke and Tian, Mingjie and Xu, Hao},
  doi          = {10.1007/s10489-021-03113-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15091-15104},
  shortjournal = {Appl. Intell.},
  title        = {Graph topology enhancement for text classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlation-based feature partition regression method for
unsupervised anomaly detection. <em>APIN</em>, <em>52</em>(13),
15074–15090. (<a
href="https://doi.org/10.1007/s10489-022-03247-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection problem has been extensively studied in a variety of application domains, where the data tags are difficult to obtain. Most unsupervised algorithms rely on some notions such as distance and density to detect anomalies. However, the performance of such algorithms is easier to decrease as the dimension of the datasets increases. Some studies which use features as pseudo-labels for prediction detect anomalies according to the deviation value of the prediction model. Even so, the improvement of model performance is still restricted to ignoring the correlation between feature attributes. In this paper, we propose a correlation-based feature partition regression prediction method called CFPR, which can alleviate the adverse effects of dataset dimensions and irrelevant attributes on model performance to a certain extent. According to the correlation between the features, the high-dimensional datasets will be divided into multiple feature subspaces. In each subspace, the feature with the highest correlation coefficient will be conducted as a pseudo-label. After that, we use the remaining features as the prediction attributes to train a supervised regression prediction model. We can calculate the anomaly score of each sample in the subspace according to the difference between the regression prediction value and the true value of the pseudo-label. Furthermore, we define a weighting strategy based on the level of correlation in the subspace integration stage to obtain the final anomaly score ranking table. Extensive experiments on twenty-eight UCI public datasets show that the CFPR performs better than several state-of-art anomaly algorithms at the AUC metric.},
  archive      = {J_APIN},
  author       = {Liu, Zhiyu and Gao, Xin and Jia, Xin and Xue, Bing and Fu, Shiyuan and Li, Kangsheng and Huang, Xu and Huang, Zijian},
  doi          = {10.1007/s10489-022-03247-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15074-15090},
  shortjournal = {Appl. Intell.},
  title        = {Correlation-based feature partition regression method for unsupervised anomaly detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimized evacuation model with compatibility constraints
in the context of disability: An ancient-inspired giza pyramids
construction metaheuristic approach. <em>APIN</em>, <em>52</em>(13),
15040–15073. (<a
href="https://doi.org/10.1007/s10489-021-03079-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the number and severity of natural disasters have increased. These disasters always carry the risks of mortality and injury. But its mortality risks are much higher for people with disabilities than for normal people. One of the most important issues during natural disasters is to pay attention to the provision of accommodation and the possibility of evacuating people with disabilities. In this paper, a Mixed-Integer Linear Programming (MILP) model is proposed for shelter locations, evacuation routing, compatibility constraints in the context of disability, and time windows to picking up people with disabilities from different locations and transporting them to shelters. This study considers the time required for the movement of people with disabilities, heterogeneous vehicles, different capacities, several depots, and several shelters. The intended objective functions are the total distance traveled by vehicles, the maximum distance of a tour, and a hybrid objective function created by the weighted combination of both mentioned objective functions. A new Giza Pyramids Construction (GPC) algorithm is used to solve the model. Using the approach of the GPC algorithm, which is one of the soft computing methods, greatly reduces the computation complexity. To examine the performance of the proposed algorithm, thirty instances at different scales are generated with and without time window constraints. For validation, the results obtained from the proposed algorithm are compared with the results obtained from four algorithms including SA, IWO, DE, and EPC. The results show that the GPC algorithm performed better than other algorithms in solving the model. According to the results, the proposed GPC algorithm is 5%, 44%, 10%, and 4% better than SA, IWO, DE, and EPC, respectively. Furthermore, considering the time window significantly increases the total network evacuation time. Based on our experiments the proposed model and solution approach can be helpful in solving the real problems related to the evacuation of people with disabilities during natural disasters.},
  archive      = {J_APIN},
  author       = {Ebrahimnejad, Sadoullah and Harifi, Sasan},
  doi          = {10.1007/s10489-021-03079-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15040-15073},
  shortjournal = {Appl. Intell.},
  title        = {An optimized evacuation model with compatibility constraints in the context of disability: An ancient-inspired giza pyramids construction metaheuristic approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STGMN: A gated multi-graph convolutional network framework
for traffic flow prediction. <em>APIN</em>, <em>52</em>(13),
15026–15039. (<a
href="https://doi.org/10.1007/s10489-022-03224-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial for the development of intelligent transportation. It can not only effectively avoid traffic congestion and other traffic problems, but also provide a data basis for other complex tasks. The rapid development of social technology and the increasingly complex traffic environment lead to the emergence of massive traffic data. Traffic flow prediction as a spatial-temporal prediction problem has been widely concerned, but the traditional forecasting methods often ignore the spatial-temporal dependence, difficult to meet the prediction requirements. Therefore, this paper proposes a novel spatial-temporal model based on an attention one-dimension convolutional neural network (1D-CNN) and a gated interpretable framework, which models historical traffic data from the perspectives of time and space respectively. The core of the model proposed in this paper is to construct spatial-temporal blocks. First, a 1D-CNN based on channel attention mechanism and “inception” structure is proposed to extract temporal correlation. Then, considering the complexity of the actual traffic network, an interpretable multi-graph gated graph convolution framework is proposed to extract the spatial correlation. Finally, extensive experiments are carried out on real data sets, which prove the effectiveness of the proposed model, and it is very competitive compared with some state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Ni, Qingjian and Zhang, Meng},
  doi          = {10.1007/s10489-022-03224-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15026-15039},
  shortjournal = {Appl. Intell.},
  title        = {STGMN: A gated multi-graph convolutional network framework for traffic flow prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sentiment aware tensor model for multi-criteria
recommendation. <em>APIN</em>, <em>52</em>(13), 15006–15025. (<a
href="https://doi.org/10.1007/s10489-022-03267-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of sentiment analysis techniques, several studies have been on Multi-Criteria Recommender Systems (MCRS) leveraging sentiment information. However, partial preferences quite and naturally happen in MCRS and negatively affect the predictive performances of sentiment analysis and multi-criteria recommendation. In this paper, we propose a Sentiment Aware Tensor Model-based MCRS named SATM. It maps between i) a set of multiple classes from explicit user feedbacks and ii) sentiments extracted from free texts in user reviews. In particular, we found the four patterns of the partial preferences and applied a rule-based function to detect them and fill their incomplete ratings intuitively. Lastly, we introduce a mapping function of the misinterpretable patterns into sentiment scores in order to generate virtual user preferences that construct the SATM. Experiments on three datasets (i.e., hotel and restaurant reviews) collected from TripAdvisor show that the SATM is superior to various baseline techniques, including state-of-the-art approaches. Additionally, the experimental evaluation of the SATM’s variants reveals that the rule-based and mapping functions can handle the partial preferences and improve the MCRS’ performance, regardless of target domains.},
  archive      = {J_APIN},
  author       = {Hong, Minsung and Jung, Jason J.},
  doi          = {10.1007/s10489-022-03267-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {15006-15025},
  shortjournal = {Appl. Intell.},
  title        = {Sentiment aware tensor model for multi-criteria recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal and secure resource searching algorithm for
unstructured mobile peer-to-peer network using particle swarm
optimization. <em>APIN</em>, <em>52</em>(13), 14988–15005. (<a
href="https://doi.org/10.1007/s10489-022-03291-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outrageous demand for file sharing among peers has become a significant development of the Peer-to-Peer (P2P) communication system during the past few years. The essence of recent P2P file-sharing systems has been driven mainly by their architectures’ scalability and the simplicity of their search capabilities. To increase data transmission and reduce the network overhead, we need an optimal resource searching algorithm. For the heterogeneous and complex potential of peers, it is challenging to pick an ideal peer for an algorithm. Implementing an effective lookup algorithm is, therefore, an essential challenge for the unstructured P2P mobile network. This paper has suggested an Optimal and Secure Resource Searching Algorithm (OSRSA) for the highly secure and most trusted P2P system. We have used Particle Swarm Optimization (PSO) to pick a peer in this optimal resource searching algorithm. This algorithm reduces the query delay and increases the success rate of searching files in the P2P network system. This algorithm also decreases network overhead and increases search efficiency in the P2P network system. This suggested algorithm’s efficiency is determined and equated with pre-existing approaches such as Flooding, Partial Indexed Search (PIS), and P2P Resource Organization by Social Acquaintances (PROSA). Our findings are that our proposed algorithm OSRSA is better than Flooding in terms of network overhead. Query delay of OSRSA is less than PIS and PROSA. The success rate of OSRSA is relatively better than PIS and PROSA.},
  archive      = {J_APIN},
  author       = {Kumar, Dharmendra and Pandey, Mayank},
  doi          = {10.1007/s10489-022-03291-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14988-15005},
  shortjournal = {Appl. Intell.},
  title        = {An optimal and secure resource searching algorithm for unstructured mobile peer-to-peer network using particle swarm optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A modified ELECTRE II method with double attitude
parameters based on linguistic z-number and its application for
third-party reverse logistics provider selection. <em>APIN</em>,
<em>52</em>(13), 14964–14987. (<a
href="https://doi.org/10.1007/s10489-022-03315-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more attention is given to environmental protection, sustainable development has become a hot topic for the international community and the companies are motivated to focus on reverse logistics (RL) in their operations for resource conservation and sustainability. However, many enterprises are limited by their economic strength and resources so that outsourcing RL to third-party reverse logistics provider (3PRLP) is their best choice. Therefore, how to effectively assess and select the 3PRLP has been a crucial issue. Given that the preference and optimism of decision makers (DMs) should be exhibited, in this paper, an improved ELECTRE II method with double attitude parameters under linguistic Z-number (LZN) environment is proposed to deal with such decision problems. Firstly, the LZN is in use to describe experts’ assessments as well as the reliability of evaluations qualitatively. Then an improved ELECTRE II model based on ideal point method is presented to rank alternatives accurately, in which the main concept of ideal point method is pressed into service in determining the preference levels of alternatives and identifying the outranking relations. Meanwhile, two attitude parameters are introduced, which can express DMs’ preference for risk as well as profit and their optimism for decision making, thereby making the decision process more practical and the ranking more reasonable. Then a numerical example on the selection of 3PRLP is shown to minutely explain procedures of the presented method and a sensitivity analysis on the double attitude parameters is conducted to illustrate effects of the two indexes and verify the robustness of the proposed method. Finally, some comparisons are carried out to prove its effectiveness and meliority.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Wang, Wenxin and Wang, Di and Liu, Peide},
  doi          = {10.1007/s10489-022-03315-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14964-14987},
  shortjournal = {Appl. Intell.},
  title        = {A modified ELECTRE II method with double attitude parameters based on linguistic Z-number and its application for third-party reverse logistics provider selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view k-proximal plane clustering. <em>APIN</em>,
<em>52</em>(13), 14949–14963. (<a
href="https://doi.org/10.1007/s10489-022-03176-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is an active direction in machine learning and pattern recognition which aims at exploring the consensus and complementary information among multiple views. In the last few years, a number of methods based on multi-view learning have been widely investigated and achieved promising performance. Generally, classical multi-view clustering methods such as multi-view kernel k-means clustering are point-based methods. The performance of point-based methods will be fairly good when the data points are distributed around the center point. The plane-based clustering methods can handle data points that are clustered along a straight line and have never been investigated in multi-view learning. In this paper, we propose a novel multi-view k-proximal plane clustering method, which initializes cluster labels by multi-view spectralclustering and updates whole multi-view cluster hyperplanes and labels alternately until some stopping conditions are satisfied. Extensive experimental results on several benchmark datasets show that the proposed model outperforms other state-of-the-art multi-view algorithms.},
  archive      = {J_APIN},
  author       = {Sun, Feixiang and Xie, Xijiong and Qian, Jiangbo and Xin, Yu and Li, Yuqi and Wang, Chong and Chao, Guoqing},
  doi          = {10.1007/s10489-022-03176-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14949-14963},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view k-proximal plane clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frobenius norm-regularized robust graph learning for
multi-view subspace clustering. <em>APIN</em>, <em>52</em>(13),
14935–14948. (<a
href="https://doi.org/10.1007/s10489-022-03816-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning methods have been widely used for multi-view clustering. However, such methods have the following challenges: (1) they usually perform simple fusion of fixed similarity graph matrices, ignoring its essential structure. (2) they are sensitive to noise and outliers because they usually learn the similarity matrix from the raw features. To solve these problems, we propose a novel multi-view subspace clustering method named Frobenius norm-regularized robust graph learning (RGL), which inherits desirable advantages (noise robustness and local information preservation) from the subspace clustering and manifold learning. Specifically, RGL uses Frobenius norm constraint and adjacency similarity learning to simultaneously explore the global information and local similarity of views. Furthermore, the l2,1 norm is imposed on the error matrix to remove the disturbance of noise and outliers. An effectively iterative algorithm is designed to solve the RGL model by the alternation direction method of multipliers. Extensive experiments on nine benchmark databases show the clear advantage of the proposed method over fifteen state-of-the-art clustering methods.},
  archive      = {J_APIN},
  author       = {Wang, Shuqin and Chen, Yongyong and Yi, Shuang and Chao, Guoqing},
  doi          = {10.1007/s10489-022-03816-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14935-14948},
  shortjournal = {Appl. Intell.},
  title        = {Frobenius norm-regularized robust graph learning for multi-view subspace clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-view clustering networks. <em>APIN</em>,
<em>52</em>(13), 14918–14934. (<a
href="https://doi.org/10.1007/s10489-021-03129-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, deep learning has made remarkable progress on multi-view clustering (MvC), with existing literature adopting a broad target to guide the network learning process, such as minimizing the reconstruction loss. However, despite this strategy being effective, it lacks efficiency. Hence, in this paper, we proposed a novel framework, entitled Efficient Multi-view Clustering Networks (EMC-Nets), which guarantees the network’s learning efficiency and produces a common discriminative representation from multiple sources. Specifically, we developed an alternating process, involving an approximation and an instruction process, which effectively stimulate the process of multi-view feature fusion to force network to learn a discriminative common representation. The approximation process employs a standard clustering algorithm, i.e., k-means, to generate pseudo labels corresponding to the current common representation, and then it leverages the pseudo labels to force the network to approximate a reasonable cluster distribution. Considering the instruction process, it aims to provide a correct learning direction for the approximation process and prevent the network from obtaining trivial solutions. Experiment results on four real-world datasets demonstrate that the proposed method outperforms state-of-the-art methods. Our source code will be available soon at https://github.com/Guanzhou-Ke/EMC-Nets .},
  archive      = {J_APIN},
  author       = {Ke, Guanzhou and Hong, Zhiyong and Yu, Wenhua and Zhang, Xin and Liu, Zeyi},
  doi          = {10.1007/s10489-021-03129-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14918-14934},
  shortjournal = {Appl. Intell.},
  title        = {Efficient multi-view clustering networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Online unsupervised cross-view discrete hashing for
large-scale retrieval. <em>APIN</em>, <em>52</em>(13), 14905–14917. (<a
href="https://doi.org/10.1007/s10489-021-03014-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view hashing has shown great potential for large-scale retrieval due to its superiority in terms of computation and storage. In real-world applications, data emerges in a streaming manner, e.g., new images and tags are uploaded to social media by users every day. Existing cross-view hashing methods have to retrain model on new multi-view data, which is time-consuming and not applicable in the real-world applications. To fill this gap, this paper proposes a new online cross-view hashing method, dubbed online unsupervised cross-view discrete hashing (OUCDH) that considers similarity preservation and quantization loss. OUCDH generates hash code as latent embedding shared by multiple views via matrix factorization. OUCDH can well preserve similarity among newly arriving data and old data with the help of anchor graph. An iterative efficient algorithm is developed for online optimization. OUCDH further updates hash code of old data to match that of newly arriving data in each iteration. Extensive experiments on three benchmarks demonstrate that the proposed OUCDH yields superior performance than existing state-of-the-art online cross-view hashing methods.},
  archive      = {J_APIN},
  author       = {Li, Xuan and Wu, Wei and Yuan, Yun-Hao and Pan, Shirui and Shen, Xiaobo},
  doi          = {10.1007/s10489-021-03014-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14905-14917},
  shortjournal = {Appl. Intell.},
  title        = {Online unsupervised cross-view discrete hashing for large-scale retrieval},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep mutual information multi-view representation for visual
recognition. <em>APIN</em>, <em>52</em>(13), 14888–14904. (<a
href="https://doi.org/10.1007/s10489-022-03462-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view representation is a crucial but challenging issue in visual recognition task. To address this issue, a deep mutual information multi-view representation method is proposed in this paper. Firstly, multi-view inputs are fed to the encoder module of the variational auto encoder architecture to extract multi-view latent layer features. Secondly, the correlation between local features and latent layer features of each view is calculated by maximizing the mutual information. Meanwhile, to obtain a robust multi-view representation, the multi-view canonical correlation analysis and the mutual information maximization methods are used to calculate the canonical correlation of different view mean vectors and the information correlation of different view distributions, respectively. Finally, the supervised loss is used to improve the discriminability of the middle feature layers. The proposed method can obtain a more robust hidden layer representations and operate multi-view scenes with more than two views. Experimental results demonstrate that the proposed method achieves better recognition accuracy than other compared methods among five publicly available datasets.},
  archive      = {J_APIN},
  author       = {Xu, Xianfa and Chen, Zhe and Yin, Fuliang},
  doi          = {10.1007/s10489-022-03462-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14888-14904},
  shortjournal = {Appl. Intell.},
  title        = {Deep mutual information multi-view representation for visual recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust deep multi-view subspace clustering networks with a
correntropy-induced metric. <em>APIN</em>, <em>52</em>(13), 14871–14887.
(<a href="https://doi.org/10.1007/s10489-022-03209-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since multi-view subspace clustering combines the advantages of deep learning to capture the nonlinear nature of data, deep multi-view subspace clustering methods have demonstrated superior ability to shallow multi-view subspace clustering methods. Most existing methods assume that sample reconstruction errors incurred by noise conform to the prior distribution of the corresponding norm, allowing for simplification of the problem and focus on designing specific regularization on self-representation matrices to exploit consistent and diverse information among different views. However, the noise distributions in different views are always very complex, and in practice the noise distributions do not necessarily conform to this hypothesis. Furthermore, the commonly used diversity regularization based on value-awareness to enhance diversity among different view representations is not sufficiently accurate. To alleviate the above deficiencies, we propose novel robust deep multi-view subspace clustering networks with a correntropy-induced metric (RDMSCNet). (1) A correntropy-induced metric (CIM) is utilized to flexibly handle various complex noise distributions in a data-driven manner to improve the robustness of the model. (2) A position-aware diversity regularization based on the exclusivity definition is employed to enforce the diversity of the different view representations for modelling the consistency and diversity simultaneously. Extensive experiments show that RDMSCNet can deliver enhanced performance over state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Si, Xiaomeng and Yin, Qiyue and Zhao, Xiaojie and Yao, Li},
  doi          = {10.1007/s10489-022-03209-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14871-14887},
  shortjournal = {Appl. Intell.},
  title        = {Robust deep multi-view subspace clustering networks with a correntropy-induced metric},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Semi-supervised multi-view binary learning for large-scale
image clustering. <em>APIN</em>, <em>52</em>(13), 14853–14870. (<a
href="https://doi.org/10.1007/s10489-022-03205-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image clustering has attracted sustained attention in machine learning. The traditional methods based on real value representation often suffer from the data storage and calculation. To deal with these problems, the methods based on the binary representation and the multi-view learning are introduced recently. However, how to improve the clustering performance is still a challenge. Considering that one can obtain in prior parts of labels in many cases, we further develop the label information in the multi-view binary learning. This information is beneficial to the design of the involved similarity matrix, which plays an important part in the clustering problem. As a result, a new method is proposed, i.e., Semi-supervised Multi-view Binary Learning(SMBL). It is tested by using four benchmark data sets and compared with several commonly used large-scale and semi-supervised clustering approaches. The extensive experimental results show that the proposed method achieves superior performance.},
  archive      = {J_APIN},
  author       = {Liu, Mingyang and Yang, Zuyuan and Han, Wei and Chen, Junhang and Sun, Weijun},
  doi          = {10.1007/s10489-022-03205-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14853-14870},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised multi-view binary learning for large-scale image clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speech synthesis with face embeddings. <em>APIN</em>,
<em>52</em>(13), 14839–14852. (<a
href="https://doi.org/10.1007/s10489-022-03227-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings are capable of imagining a person’s voice according to his or her appearance because different people have different voice characteristics. Although researchers have made great progress in single-view speech synthesis, there are few studies on multi-view speech synthesis, especially the speech synthesis using face images. On the basis of implicit relationship between the speaker’s face image and his or her voice, we propose a multi-view speech synthesis method called SSFE (Speech Synthesis with Face Embeddings). The proposed SSFE consists of three parts: a voice encoder, a face encoder and an improved multi-speaker text-to-speech (TTS) engine. On the one hand, the proposed voice encoder generates the voice embeddings from the speaker’s speech and the proposed face encoder extracts the voice features from the speaker’s face as f-voice embeddings. On the other hand, the multi-speaker TTS engine would synthesize the speech with voice embeddings and f-voice embeddings. We have conducted extensive experiments to evaluate the proposed SSFE on the synthesized speech quality and face-voice matching degree, in which the Mean Opinion Score of the SSFE is more than 3.7 and the matching degree is about 1.7. The experimental results prove that the proposed SSFE method outperforms state-of-the-art methods on the synthesized speech in terms of speech quality and face-voice matching degree.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Ji, Sihui and Wang, Jianjia and Guo, Yike},
  doi          = {10.1007/s10489-022-03227-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14839-14852},
  shortjournal = {Appl. Intell.},
  title        = {Speech synthesis with face embeddings},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multi-view clustering based on weighted sparse
and low rank representation. <em>APIN</em>, <em>52</em>(13),
14822–14838. (<a
href="https://doi.org/10.1007/s10489-022-03246-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering utilizes the consistency and complementarity between views to group entities well. However, in real life, the lack of instances in some views often occurs, which not only reduces the available information, but also increases the difficulty of joint learning with non-aligned multi-view data. Many incomplete multi-view clustering algorithms are developed to tackle these concerns, but they usually have the following problems: 1) They mainly focus on how to construct the shared feature space for incomplete views while ignoring the essential relationship between data instances. 2) Most of them simply assume that two datapoints which are close belong to the same category, but that is not the case. 3) The hazards of overlapping, confusing features in incomplete multi-view clustering are not considered. To solve these issues, this paper proposes a new Incomplete Multi-view Graph Learning method based on Weighted Sparse and Low rank Representation (IMGLWSLR). It leverages subspace learning with double constraints to capture global and local data relationships, a weighting mechanism to reduce the negative impact of missing data and a kernel-based method to fuse incomplete multiple views. Different from previous approaches, we concentrate on inhibiting the confusion of redundant features in subspace learning, which may affect the clustering seriously with missing views. Experimental results demonstrate the superiority of IMGLWSLR over nine benchmark datasets, compared with seven state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Zhao, Liang and Zhang, Jie and Yang, Tao and Chen, Zhikui},
  doi          = {10.1007/s10489-022-03246-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14822-14838},
  shortjournal = {Appl. Intell.},
  title        = {Incomplete multi-view clustering based on weighted sparse and low rank representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multi-view clustering with multiple imputation
and ensemble clustering. <em>APIN</em>, <em>52</em>(13), 14811–14821.
(<a href="https://doi.org/10.1007/s10489-021-02978-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is an important and challenging task in machine learning and data mining. In the past decade, this topic attracted much attention and there have been many progress achieved in this field. However, in reality, due to different factors such as machine error, sensor failure, multi-view data are mostly incomplete, thus how to deal with this problem becomes a challenge. Some existing works mainly deal with view missing case, which means in certain view of datasets, the whole features of some samples would be lost. In fact, missing value can occur in any position, that is, any value missing case. In that case, there would be some values missed in any view with sheerly random way. We proposed a two-stage algorithm involved multiple imputation and ensemble clustering to deal with multi-view clustering in any value missing case. Multiple imputation is adopted to deal with missing values problem and weighted ensemble clustering is applied to implement multi-view clustering. The experimental comparison on several data sets verified the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Chao, Guoqing and Wang, Songtao and Yang, Shiming and Li, Chunshan and Chu, Dianhui},
  doi          = {10.1007/s10489-021-02978-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14811-14821},
  shortjournal = {Appl. Intell.},
  title        = {Incomplete multi-view clustering with multiple imputation and ensemble clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Cross-view vehicle re-identification based on graph
matching. <em>APIN</em>, <em>52</em>(13), 14799–14810. (<a
href="https://doi.org/10.1007/s10489-022-03349-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view identification is one of the challenges in the task of vehicle re-identification. Because of the different shapes, structures, and surface area, changes in viewing angle have a greater impact on vehicle re-ID than on person re-ID. Previous work mainly focused on generating other viewing angle features through a single view feature of the vehicle to achieve cross-view identification. This paper proposes a systematic framework to realize the alignment and discrimination of key features by learning high-order relationships and topological information. First, the feature extraction module (FEM) is applied to extract local and global features of the vehicle. Then,the graph convolution module (GCM) integrates the topological structure information between the key points of the vehicle into the local features to compensate for the unexposed key points caused by the blind zone. Finally, the graph matching module(GMM) robustly aligns the key features between the two graphs and calculates their similarity. Experimental results show that our proposed method VGM has competitive results with the existing state-of-the-art methods on benchmark datasets VeRi-776 and VARI.},
  archive      = {J_APIN},
  author       = {Zhang, Chao and Yang, Chule and Wu, Dayan and Dong, Hongbin and Deng, Baosong},
  doi          = {10.1007/s10489-022-03349-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14799-14810},
  shortjournal = {Appl. Intell.},
  title        = {Cross-view vehicle re-identification based on graph matching},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Multi-view attention-convolution pooling network for 3D
point cloud classification. <em>APIN</em>, <em>52</em>(13), 14787–14798.
(<a href="https://doi.org/10.1007/s10489-021-02840-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying 3D point clouds is an important and challenging task in computer vision. Currently, classification methods using multiple views lose characteristic or detail information during the representation or processing of views. For this reason, we propose a multi-view attention-convolution pooling network framework for 3D point cloud classification tasks. This framework uses Res2Net to extract the features from multiple 2D views. Our attention-convolution pooling method finds more useful information in the input data related to the current output, effectively solving the problem of feature information loss caused by feature representation and the detail information loss during dimensionality reduction. Finally, we obtain the probability distribution of the model to be classified using a full connection layer and the softmax function. The experimental results show that our framework achieves higher classification accuracy and better performance than other contemporary methods using the ModelNet40 dataset.},
  archive      = {J_APIN},
  author       = {Wang, Wenju and Wang, Tao and Cai, Yu},
  doi          = {10.1007/s10489-021-02840-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14787-14798},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view attention-convolution pooling network for 3D point cloud classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-mode traffic flow prediction method with clustering
based attention convolution LSTM. <em>APIN</em>, <em>52</em>(13),
14773–14786. (<a
href="https://doi.org/10.1007/s10489-021-02770-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing traffic congestion is a major obstacle to the development of cities. The prediction of traffic flow is very important to city planning and dredging. A good model of flow is able to accurately predict future flow by learning historical flow data. Traffic flow is usually affected by macro and micro factors. At the macro level, the whole city can be divided into different subregions according to the similarity in the traffic flow patterns. At the micro-level, there is a temporal and spatial correlation between the traffic flow of different road sections at di fferent times. In this paper, we propose a multi-mode traffic flow prediction method with Clustering based Attention Convolution LSTM (CACLSTM) to model spatial-temporal data of traffic flow. The framework includes three modules: a convolution LSTM encoding-decoding layer which is used to predict the traffic flow of the next time slice by encoding the historical traffic information, a clustering based attention layer which is able to extract different temporal features by clustering based attention, and an additional factors layer which can integrate weather, wind speed, holidays and other factors to improve the prediction accuracy. The experimental results on Beijing taxis data show that the CACLSTM method performs more effective than the six well-known compared methods.},
  archive      = {J_APIN},
  author       = {Huang, Xiaohui and Ye, Yuming and Wang, Cheng and Yang, Xiaofei and Xiong, Liyan},
  doi          = {10.1007/s10489-021-02770-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14773-14786},
  shortjournal = {Appl. Intell.},
  title        = {A multi-mode traffic flow prediction method with clustering based attention convolution LSTM},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-clustering based classification of multi-view data.
<em>APIN</em>, <em>52</em>(13), 14756–14772. (<a
href="https://doi.org/10.1007/s10489-021-03087-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning is an attractive area of research where data is represented using multiple views, each containing some useful information. Many multi-view learning algorithms use a unified objective function that needs to be optimized simultaneously to achieve a common goal, using a similarity or distance measure, such as the Euclidean or Cosine metric. Having a unified objective function, however, does not allow independent learning from the different views which can later be integrated to improve the overall learning experience. In this paper, we explore the principles of multi-view learning, i.e., complementary and consensus principle, to propose a new multi-view classification algorithm that simultaneously allows for both independent and unified learning. The complementary principle says that each source of data contains some information that is missing in other views while the consensus principle aims to maximize the agreement across the multiple views. Our proposed technique treats the data both individually as well as a part of a multi-view data. Within a view, we exploit a supervised co-clustering-based measure that independently learns similarity between instances of that view. Across views, these similarlity values are then shared using transfer-learning. The learning process involves both co-clustering and multi-view learning within a supervised learning framework. The test data is predicted independently on each view and a concensus based prediction across the views is used for the final label. Individually, each of supervised learning, co-clustering, and multi-view learning has been used in the literature, each with its own set of advantages. To the best of our knowledge, the current study is the premier attempt to combine these concepts for the classification task. Results show that the proposed approach significantly outperforms other recent and state-of-the-art algorithms on several sparse and high-dimensional datasets using the accuracy and normalized mutual information criteria.},
  archive      = {J_APIN},
  author       = {Hussain, Syed Fawad and Khan, Mohsin and Siddiqi, Imran},
  doi          = {10.1007/s10489-021-03087-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14756-14772},
  shortjournal = {Appl. Intell.},
  title        = {Co-clustering based classification of multi-view data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IBMvSVM: An instance-based multi-view SVM algorithm for
classification. <em>APIN</em>, <em>52</em>(13), 14739–14755. (<a
href="https://doi.org/10.1007/s10489-021-03101-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging research direction of machine learning, the multi-view learning (MVL) pays attention to the tasks that learn from datasets with several distinct views to achieve better generalization performance. Recently, various Support Vector Machine (SVM)-based algorithms with solid theoretical foundation have been proposed for MVL. However, there is a constraining assumption for these algorithms, i.e., in the learning process, different views are important equally for an instance in a data set, and the same view is important equally for all instances in a data set. In fact, an instance generally has different adaptability to different views, namely, the degree to which the information from different views accurately describes the instance varies. And naturally, different instances in a data set also have different adaptability to the same view. In this paper, the concept of view vector of each instance is proposed first, which quantitatively describes the adaptability of a specific instance to different views. It also reflects the characteristics of different instances that some instances are more suitable to be represented by a view, while others tend to be better represented by another view. Then, a new instance-based multi-view SVM algorithm, named IBMvSVM, is proposed by building the view vector of each instance into the multi-view SVM learning. IBMvSVM focuses on characteristics of each instance itself in different views rather than treating them equally. Experiments performed on 48 multi-view datasets reveal the superiority of IBMvSVM algorithm on generalization against several recently state-of-the-art MVL algorithms.},
  archive      = {J_APIN},
  author       = {Yu, Shuang and Li, Xiongfei and Sun, Siru and Wang, Hancheng and Zhang, Xiaoli and Chen, Shiping},
  doi          = {10.1007/s10489-021-03101-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14739-14755},
  shortjournal = {Appl. Intell.},
  title        = {IBMvSVM: An instance-based multi-view SVM algorithm for classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Locality sensitive hashing with bit selection.
<em>APIN</em>, <em>52</em>(13), 14724–14738. (<a
href="https://doi.org/10.1007/s10489-022-03546-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality sensitive hashing (LSH), one of the most popular hashing techniques, has attracted considerable attention for nearest neighbor search in the field of image retrieval. It can achieve promising performance only if the number of the generated hash bits is large enough. However, more hash bits assembled to the binary codes contain massive redundant information and require more time cost and storage spaces. To alleviate this limitation, we propose a novel bit selection framework to pick important bits out of the hash bits generated by hashing techniques. Within the bit selection framework, we further exploit eleven evaluation criteria to measure the importance and similarity of each bit generated by LSH, so that the bits with high importance and less similarity are selected to assemble new binary codes. To demonstrate the effectiveness of the proposed framework of bit selection, we evaluated the proposed framework with the evaluation criteria on five commonly used data sets. Experimental results show the proposed bit selection framework works effectively in different cases, and the performance of LSH has not been degraded significantly after redundant hash bits reduced by the evaluation criteria.},
  archive      = {J_APIN},
  author       = {Zhou, Wenhua and Liu, Huawen and Lou, Jungang and Chen, Xin},
  doi          = {10.1007/s10489-022-03546-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14724-14738},
  shortjournal = {Appl. Intell.},
  title        = {Locality sensitive hashing with bit selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M-FFN: Multi-scale feature fusion network for image
captioning. <em>APIN</em>, <em>52</em>(13), 14711–14723. (<a
href="https://doi.org/10.1007/s10489-022-03463-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel multi-scale feature fusion network (M-FFN) for image captioning task to incorporate discriminative features and scene contextual information of an image. We construct multi-scale feature fusion network by leveraging spatial transformation and multi-scale feature pyramid networks via feature fusion block to enrich spatial and global semantic information. In particular, we take advantage of multi-scale feature pyramid network to incorporate global contextual information by employing atrous convolutions on top layers of convolutional neural network (CNN). And, the spatial transformation network is exploited on early layers of CNN to remove intra-class variability caused by spatial transformations. Further, the feature fusion block integrates both global contextual information and spatial features to encode the visual information of an input image. Moreover, spatial-semantic attention module is incorporated to learn attentive contextual features to guide the captioning module. The efficacy of the proposed model is evaluated on the COCO dataset.},
  archive      = {J_APIN},
  author       = {Prudviraj, Jeripothula and Vishnu, Chalavadi and Mohan, Chalavadi Krishna},
  doi          = {10.1007/s10489-022-03463-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14711-14723},
  shortjournal = {Appl. Intell.},
  title        = {M-FFN: Multi-scale feature fusion network for image captioning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-enhanced dual discriminator generative adversarial
network for fast MRI with parallel imaging using multi-view information.
<em>APIN</em>, <em>52</em>(13), 14693–14710. (<a
href="https://doi.org/10.1007/s10489-021-03092-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical medicine, magnetic resonance imaging (MRI) is one of the most important tools for diagnosis, triage, prognosis, and treatment planning. However, MRI suffers from an inherent slow data acquisition process because data is collected sequentially in k-space. In recent years, most MRI reconstruction methods proposed in the literature focus on holistic image reconstruction rather than enhancing the edge information. This work steps aside this general trend by elaborating on the enhancement of edge information. Specifically, we introduce a novel parallel imaging coupled dual discriminator generative adversarial network (PIDD-GAN) for fast multi-channel MRI reconstruction by incorporating multi-view information. The dual discriminator design aims to improve the edge information in MRI reconstruction. One discriminator is used for holistic image reconstruction, whereas the other one is responsible for enhancing edge information. An improved U-Net with local and global residual learning is proposed for the generator. Frequency channel attention blocks (FCA Blocks) are embedded in the generator for incorporating attention mechanisms. Content loss is introduced to train the generator for better reconstruction quality. We performed comprehensive experiments on Calgary-Campinas public brain MR dataset and compared our method with state-of-the-art MRI reconstruction methods. Ablation studies of residual learning were conducted on the MICCAI13 dataset to validate the proposed modules. Results show that our PIDD-GAN provides high-quality reconstructed MR images, with well-preserved edge information. The time of single-image reconstruction is below 5ms, which meets the demand of faster processing.},
  archive      = {J_APIN},
  author       = {Huang, Jiahao and Ding, Weiping and Lv, Jun and Yang, Jingwen and Dong, Hao and Del Ser, Javier and Xia, Jun and Ren, Tiaojuan and Wong, Stephen T. and Yang, Guang},
  doi          = {10.1007/s10489-021-03092-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14693-14710},
  shortjournal = {Appl. Intell.},
  title        = {Edge-enhanced dual discriminator generative adversarial network for fast MRI with parallel imaging using multi-view information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trace ratio criterion for multi-view discriminant analysis.
<em>APIN</em>, <em>52</em>(13), 14679–14692. (<a
href="https://doi.org/10.1007/s10489-022-03464-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from heterogeneous views, termed multi-view learning (MvL), is a significant yet challenging problem in computer vision. Many existing MvL methods apply the two-view principle to multi-view scenarios, but this pairwise approach is neither efficient nor optimal and leads to poor performance. Then some multi-view discriminant analysis methods are proposed by simultaneously considering the intra-view and inter-view correlation across multiple views, and the final form of these methods is a trace ratio (TR) problem. Traditionally, the solution to the TR problem is normally approximated by generalized eigenvalue decomposition, failing to obtain the global optimum solution. Although some iterative algorithms are proposed to directly solve the TR problem, these methods are not rigorous in the absence of theoretical discussion. To overcome these weaknesses, we propose a novel supervised MvL model called Trace Ratio Criterion for Multi-view Discriminant Analysis (TRCMvDA) by jointly learning multiple view-specific linear transforms. In particular, we propose an iterative algorithm based on the Newton-Raphson method to directly solve the TR problem and successfully avoid deviation from the original objectives. In addition, we prove the convergence and effectiveness of our algorithm theoretically and empirically. Experiments conducted on five universal datasets reveal that our TRCMvDA achieves better performance than state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Shi, Mei and Li, Zhihui and Zhao, Xiaowei and Xu, Pengfei and Liu, Baoying and Guo, Jun},
  doi          = {10.1007/s10489-022-03464-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14679-14692},
  shortjournal = {Appl. Intell.},
  title        = {Trace ratio criterion for multi-view discriminant analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Face aging with pixel-level alignment GAN. <em>APIN</em>,
<em>52</em>(13), 14665–14678. (<a
href="https://doi.org/10.1007/s10489-022-03541-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face aging is of great significance in cross-time identity verification problem. However, there is still a huge gap between the synthesized face image and the real face in terms of quality and consistency due to identity ambiguity and image distortion caused by existing face aging methods. To meet this challenge, we propose a face aging framework named as Pixel-level Alignment GAN, PAGAN, to synthesize faces of different age groups. Face images are featured by age, identity, and fine-grained pixel-value to ensure the quality, which is a typical multi-task learning problem. The proposed face aging framework with PAGAN is a combination of age estimation, identity preservation, and image de-noising. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods not only in the accuracy of age classification but also in the image quality. With the proposed PAGAN, the face recognition accuracy with synthesized images has increased 0.21% and the image quality rating has increased around 5%, which proves the effectiveness and validity of proposed method.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Zhang, Yafei and Li, Qing and Qi, Yangyang and Wang, Jianjia and Guo, Yike},
  doi          = {10.1007/s10489-022-03541-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14665-14678},
  shortjournal = {Appl. Intell.},
  title        = {Face aging with pixel-level alignment GAN},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonconvex low-rank and sparse tensor representation for
multi-view subspace clustering. <em>APIN</em>, <em>52</em>(13),
14651–14664. (<a
href="https://doi.org/10.1007/s10489-022-03406-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has attracted significant attention due to the popularity of multi-view datasets. The effectiveness of the existing multi-view clustering methods highly depends on the quality of the affinity matrix. To derive a high quality affinity matrix, tensor optimization has been explored for multi-view subspace clustering. However, only the global low-rank correlation information among views has been explored, and the local geometric structure has been ignored. In addition, for low-rank tensor approximation learning, the commonly used tensor nuclear norm cannot retain the main information of all views. In this paper, we propose a nonconvex low-rank and sparse tensor representation (NLRSTR) method, which retains the similarity information of the view dimension from global and local perspectives. Specifically, the proposed NLRSTR method imposes nonconvex function and sparse constraint on the self-representation tensor to characterize the high relationship among views. Based on the alternating direction method of multipliers, an effective algorithm is proposed to solve our NLRSTR model. The experimental results on eight datasets show the superiority of the proposed NLRSTR method compared with seventeen state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Shuqin and Chen, Yongyong and Cen, Yigang and Zhang, Linna and Wang, Hengyou and Voronin, Viacheslav},
  doi          = {10.1007/s10489-022-03406-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14651-14664},
  shortjournal = {Appl. Intell.},
  title        = {Nonconvex low-rank and sparse tensor representation for multi-view subspace clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). A multi-view multi-omics model for cancer drug response
prediction. <em>APIN</em>, <em>52</em>(13), 14639–14650. (<a
href="https://doi.org/10.1007/s10489-022-03294-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer drug response prediction is the fundamental task in precision medicine, which provides opportunities for cancer therapy. Several methods have been proposed to screen drugs, via building computational models on multi-omics data. However, the view value missing problem caused by unknown cancers or tumors has not been addressed. For this reason, a multi-view multi-omics (MvMo) model is proposed to predict cancer drug response values. The proposed MvMo model first represents the input heterogeneous data in different kinds of embeddings and features, such as token embeddings and latent features. Then several views are generated to observe interconnections among those representations. Finally, the predictions are generated based on the outputs of these views. Experimental results on the collected real data show the efficiency of the proposed method in terms of speed and accuracy.},
  archive      = {J_APIN},
  author       = {Wang, Zhijin and Wang, Ziyang and Huang, Yaohui and Lu, Longquan and Fu, Yonggang},
  doi          = {10.1007/s10489-022-03294-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14639-14650},
  shortjournal = {Appl. Intell.},
  title        = {A multi-view multi-omics model for cancer drug response prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sign language recognition and translation network based on
multi-view data. <em>APIN</em>, <em>52</em>(13), 14624–14638. (<a
href="https://doi.org/10.1007/s10489-022-03407-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition and translation can address the communication problem between hearing-impaired and general population, and can break the sign language boundariesy between different countries and different languages. Traditional sign language recognition and translation algorithms use Convolutional Neural Networks (CNNs) to extract spatial features and Recurrent Neural Networks (RNNs) to extract temporal features. However, these methods cannot model the complex spatiotemporal features of sign language. Moreover, RNN and its variant algorithms find it difficult to learn long-term dependencies. This paper proposes a novel and effective network based on Transformer and Graph Convolutional Network (GCN), which can be divided into three parts: a multi-view spatiotemporal embedding network (MSTEN), a continuous sign language recognition network (CSLRN), and a sign language translation network (SLTN). MSTEN can extract the spatiotemporal features of RGB data and skeleton data. CSLRN can recognize sign language glosses and obtain intermediate features from multi-view input sign data. SLTN can translate intermediate features into spoken sentences. The entire network was designed as end-to-end. Our method was tested on three public sign language datasets (SLR-100, RWTH, and CSL-daily) and the results demonstrated that our method achieved excellent performance on these datasets.},
  archive      = {J_APIN},
  author       = {Li, Ronghui and Meng, Lu},
  doi          = {10.1007/s10489-022-03407-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14624-14638},
  shortjournal = {Appl. Intell.},
  title        = {Sign language recognition and translation network based on multi-view data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multi-view clustering with incomplete
graph-regularized orthogonal non-negative matrix factorization.
<em>APIN</em>, <em>52</em>(13), 14607–14623. (<a
href="https://doi.org/10.1007/s10489-022-03551-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMC) has achieved widespread attention due to its advantage in fusing the multi-view information when the view samples are unobserved partly. Recently, it is shown that the clustering performance in the subspace can be improved by preserving the clustering structure of each view, but the problem of the inconsistent clustering structure caused by the incomplete graphs are seldom considered, restricting the clustering performance. Motivated by the clustering interpretation of the orthogonal non-negative matrix factorization, it is employed to unify the clustering structure of the data, and a new model called Incomplete Graph-regularized Orthogonal Non-negative Matrix Factorization (IGONMF) is proposed in this paper. In IGONMF, the reproduced representation is developed, based on which, a set of incomplete graphs are utilized to fully take advantage of the geometric structure of the data. And the orthogonality is further employed to alleviate the problem of the inconsistent clustering structure. Also, we design an effective iterative updating algorithm to solve the proposed model, along with its analysis on the convergence and the computational cost. Finally, experimental results on several real-world datasets indicate that our method is superior to the related state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liang, Naiyao and Yang, Zuyuan and Li, Zhenni and Han, Wei},
  doi          = {10.1007/s10489-022-03551-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14607-14623},
  shortjournal = {Appl. Intell.},
  title        = {Incomplete multi-view clustering with incomplete graph-regularized orthogonal non-negative matrix factorization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-view time series model for share turnover
prediction. <em>APIN</em>, <em>52</em>(13), 14595–14606. (<a
href="https://doi.org/10.1007/s10489-021-02979-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Share turnover is a key indicator for investing in the stock market, which represents how easy or difficult it is to trade a stock. Several techniques have been proposed to predict share turnover values. However, they are often inaccurate because they utilize single-view models that have an incomplete picture of the temporal dynamics. To address this issue, a multi-view time series model (MvT) is proposed to capture temporal dynamics using three views on two data groups. The temporal dynamics of target turnover data and exogenous turnover data are captured by a view generation component. The component generates three views in three different aspects. The predictions are then made by a view combination component and a full connected layer. Extensive experiments on two real stock datasets show the effectiveness and efficiency of the proposed MvT model, when compared with ten algorithms on four groups of stock data in terms of three metrics.},
  archive      = {J_APIN},
  author       = {Wang, Zhijin and Su, Qiankun and Chao, Guoqing and Cai, Bing and Huang, Yaohui and Fu, Yonggang},
  doi          = {10.1007/s10489-021-02979-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14595-14606},
  shortjournal = {Appl. Intell.},
  title        = {A multi-view time series model for share turnover prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Special issue on multi-view learning.
<em>APIN</em>, <em>52</em>(13), 14591–14594. (<a
href="https://doi.org/10.1007/s10489-022-03650-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Chao, Guoqing and Zhu, Xingquan and Ding, Weiping and Bi, Jinbo and Sun, Shiliang},
  doi          = {10.1007/s10489-022-03650-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {13},
  pages        = {14591-14594},
  shortjournal = {Appl. Intell.},
  title        = {Editorial: Special issue on multi-view learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A faster stochastic alternating direction method for large
scale convex composite problems. <em>APIN</em>, <em>52</em>(12),
14233–14245. (<a
href="https://doi.org/10.1007/s10489-022-03319-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the fact that certain randomization schemes incorporated into the stochastic (proximal) gradient methods allow for a large reduction in computational time, we incorporate such a scheme into stochastic alternating direction method of multipliers (ADMM), yielding a faster stochastic alternating direction method (FSADM) for solving a class of large scale convex composite problems. In the numerical experiments, we observe a reduction of this method in computational time compared to previous methods. More importantly, we unify the stochastic ADMM for solving general convex and strongly convex composite problems (i.e., the iterative scheme does not change when the the problem goes from strongly convex to general convex). In addition, we establish the convergence rates of FSADM for these two cases.},
  archive      = {J_APIN},
  author       = {Hu, Jia and Guo, Tiande and Zhao, Tong},
  doi          = {10.1007/s10489-022-03319-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14233-14245},
  shortjournal = {Appl. Intell.},
  title        = {A faster stochastic alternating direction method for large scale convex composite problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action space noise optimization as exploration in
deterministic policy gradient for locomotion tasks. <em>APIN</em>,
<em>52</em>(12), 14218–14232. (<a
href="https://doi.org/10.1007/s10489-021-02995-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) algorithms with deterministic actors (policy) commonly apply noise to the action space for exploration. These exploration methods are either undirected or require extra knowledge of the environment. In the aim of addressing these fundamental limitations, this paper introduces a parameterized stochastic action-noise policy (as a probability distribution) that correlates with the objectivity of the RL algorithm. This policy is optimized based on state-action values of predicted future states. Consequently, the optimization does not rely on the explicit definition of the reward function which improves the adaptability of this exploration strategy for different environments and algorithms. Moreover, this paper presents a predictive model of system dynamics (transitional probability) with the capacity to capture the uncertainty of the environments with optimal design and fewer parameters. It significantly reduces the model complexity while maintaining the same level of accuracy as current methods. This research evaluates and analyzes the proposed method and models while demonstrating significant increase in performance and reliability across various locomotion and control tasks in comparison with current methods.},
  archive      = {J_APIN},
  author       = {Nobakht, Hesan and Liu, Yong},
  doi          = {10.1007/s10489-021-02995-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14218-14232},
  shortjournal = {Appl. Intell.},
  title        = {Action space noise optimization as exploration in deterministic policy gradient for locomotion tasks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaboration based multi-modal multi-label learning.
<em>APIN</em>, <em>52</em>(12), 14204–14217. (<a
href="https://doi.org/10.1007/s10489-021-03130-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex objects can be represented as multiple modal features and associated with multiple labels. The major challenge of complex object classification is how to jointly utilize heterogeneous modals in a mutually beneficial way. Besides, how to effectively utilize label correlations is also a challenging issue. Previous methods model the label correlations by requiring that any two label-specific classifiers behave similarly on the same modal if the associated labels are similar. To address the above challenges, we propose a novel modal-oriented deep learning framework named Collaboration based Multi-modal Multi-label Learning (CoM3L). With the help of memory structure in LSTM, CoM3L handles modalities sequentially, which predicts next modal to be extracted and learns label correlations simultaneously. On the one hand, CoM3L can extract the most useful modal sequence, which extracts different modal sequences for different instances. On the other hand, for each label, CoM3L combines the collaboration between its own prediction and the prediction of other labels. Extensive experiments on 5 multi-modal multi-label datasets validate the effectiveness of the proposed CoM3L approach.},
  archive      = {J_APIN},
  author       = {Zhang, Yi and Zhu, Yinlong and Zhang, Zhecheng and Wang, Chongjung},
  doi          = {10.1007/s10489-021-03130-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14204-14217},
  shortjournal = {Appl. Intell.},
  title        = {Collaboration based multi-modal multi-label learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-level interactive multimodal-mixup encoder for
multi-modal neural machine translation. <em>APIN</em>, <em>52</em>(12),
14194–14203. (<a
href="https://doi.org/10.1007/s10489-022-03331-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal neural machine translation (MNMT), which mainly focuses on the use of image information to guide text translation. Recent MNMT approaches have been shown that incorporating visual features into textual translation framework is helpful to improve machine translation. However, visual features always contain textual unrelated information, but the noisy visual feature fusion problem is rarely considered for traditional MNMT methods. How to extract the useful visual features to enhance textual machine translation is the key point need to be considered for MNMT. In this paper, we propose a novel Dual-level Interactive Multimodal-Mixup Encoder (DLMulMix) based on multimodal-mixup for MNMT, which can extract the useful visual features to enhance textual-level machine translation. We first employ the Textual-visual Gating to extract text related visual features, which we believe that regional features are crucial for MNMT. Then visual grid features are employed in order to establish the image context of the effective regional features. Moreover, an effective visual-textual multimodal-mixup is adopted to align textual features and visual features into multi-modal common space to improve textual-level machine translation. We evaluate our proposed method on the Multi30K dataset. The experimental results show that the proposed approach outperforms the previous efforts for both EN-DE and EN-FR tasks regarding BLEU and METEOR scores.},
  archive      = {J_APIN},
  author       = {Ye, Junjie and Guo, Junjun},
  doi          = {10.1007/s10489-022-03331-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14194-14203},
  shortjournal = {Appl. Intell.},
  title        = {Dual-level interactive multimodal-mixup encoder for multi-modal neural machine translation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2D3D-MVPNet: Learning cross-domain feature descriptors for
2D-3D matching based on multi-view projections of point clouds.
<em>APIN</em>, <em>52</em>(12), 14178–14193. (<a
href="https://doi.org/10.1007/s10489-022-03372-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust local cross-domain feature descriptors of 2D images and 3D point clouds play an important role in 2D and 3D vision applications, e.g. augmented Reality (AR) and robot navigation. Essentially, the robust local cross-domain feature descriptors have the potential to establish a spatial relationship between 2D space and 3D space. However, it is challenging for manual-based or traditional deep learning-based methods to represent the invariant cross-domain feature descriptors between 2D images and 3D point clouds. Specifically, the mainstream point cloud deep learning network is used to extract the global structure information of the scene. Due to the dimensional difference, there is a large gap between the two-dimensional picture and the three-dimensional structure feature in feature accommodation. In this paper, based on the 2D image patch and 3D point cloud volume dataset, a novel network, 2D3D-MVPNet, is proposed to jointly learn robust local cross-domain feature descriptors between 2D images and 3D point clouds. The 2D3D-MVPNet contains a point cloud branch and an image branch, which are optimized with triplet loss and a second-order similarity regularization. Specifically, for the point cloud branch, first, a novel point cloud feature descriptor extractor, named the image-based point cloud encoder, is introduced to learn a local 3D feature descriptor consistent with the local 2D feature descriptor, so that the local 3D feature descriptors contain both geometry and colour texture information. Second, to overcome the challenge of random order of projected image inputs, a symmetric function is introduced to deal with the feature combination of point cloud projections. Experiments show that the local cross-domain feature descriptors of 2D images and 3D point clouds learned by 2D3D-MVPNet achieve extraordinary 2D to 3D retrieval performance. In addition, several 3D point cloud registration results demonstrate the effectiveness of the image-based point cloud encoder.},
  archive      = {J_APIN},
  author       = {Lai, Baiqi and Liu, Weiquan and Wang, Cheng and Fan, Xiaoliang and Lin, Yangbin and Bian, Xuesheng and Wu, Shangbin and Cheng, Ming and Li, Jonathan},
  doi          = {10.1007/s10489-022-03372-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14178-14193},
  shortjournal = {Appl. Intell.},
  title        = {2D3D-MVPNet: Learning cross-domain feature descriptors for 2D-3D matching based on multi-view projections of point clouds},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallelized extreme learning machine for online data
classification. <em>APIN</em>, <em>52</em>(12), 14164–14177. (<a
href="https://doi.org/10.1007/s10489-022-03308-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges raised by the massive data are being managed by the community through the advancements of infrastructure and algorithms, and now the processing of fast data is becoming a new hurdle to the researchers. Extreme Learning Machine (ELM) is a single-layer learning model with reliable performances and it is computationally simpler than the new generation deep architectures. ELM process the data in batches and the model has to be rerun while updates happening in the datasets. In the theoretical background of ELM, the past knowledge cannot be reused for improving the performance in online learning where the data set will be updated with mini-batches. In this paper, we have introduced a knowledge base to deal with the remembrance of knowledge in ELM. The architecture of the proposed model is designed to process mini-batches of any size to speed up the processing of the data on its arrival. A group of data sets with different properties such as sparse and feature dimensions is used in the experiments to evaluate our method. The performance of the algorithm is compared with a set of benchmarked classifiers and stream classifiers in the scikit-learn public platform. It is observed that our method could perform better in most of the experiments. It clear in the results that the Parallel ELM model outperformed the other methods in the training time across all the datasets. The consistent performance of our method shows the significance of parallel algorithms of ELM that can remember past knowledge.},
  archive      = {J_APIN},
  author       = {M, Vidhya and S, Aji},
  doi          = {10.1007/s10489-022-03308-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14164-14177},
  shortjournal = {Appl. Intell.},
  title        = {Parallelized extreme learning machine for online data classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic dual quaternion knowledge graph embedding.
<em>APIN</em>, <em>52</em>(12), 14153–14163. (<a
href="https://doi.org/10.1007/s10489-021-03069-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph aims to describe the concepts, the entities and the complex relations of them in the real world. Recently, a series of quaternion rotation models that usually considering the relation vector as the rotation between head and tail entities, have been extensively studied. The results showed these models had the advantages of simplicity and efficiency. However, they are quite weak in capturing the representation and the feature interaction between entities and relations, resulting in insufficient expressiveness of the underlying models, because these models only focus on the linear relations between entities and relations. In order to solve this problem, this paper proposes a novel knowledge graph embedding model called DualDE, which dynamically maps the dual quaternions to the knowledge graph. Specifically, DualDE uses a dynamic mapping mechanism to construct the entity transition vector and the relation transition vector, and continuously adjusts the embedding position of the entity vector in the dual quaternion space according to the dual quaternion multiplication rules. In addition, DualDE can dynamically construct a variety of complex relation types, such as one-to-many, many-to-one and many-to-many. The experimental results based on three standard data sets show that the DualDE model is superior to the existing knowledge graph embedding models on many metrics.},
  archive      = {J_APIN},
  author       = {Chen, Heng and Li, Guanyu and Jiang, Wei and Sun, Yunhao},
  doi          = {10.1007/s10489-021-03069-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14153-14163},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic dual quaternion knowledge graph embedding},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster-based spatiotemporal dual self-adaptive network for
short-term subway passenger flow forecasting. <em>APIN</em>,
<em>52</em>(12), 14137–14152. (<a
href="https://doi.org/10.1007/s10489-022-03305-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal modelling of short-term forecasts of metro passenger flows continue to face tremendous challenges. First, there is a need to consider the functional domain made up of several similar stations; Secondly, complex spatiotemporal models depend on a large number of learnable parameters. This paper proposed a spatiotemporal dual self-adaptive network based on the cluster (CG-TaLK) to accurately predict the inflow and outflow of subway passengers. Specifically, through the division of clustering, the members of each group learn a shared embedding, and use the inner product of embedding to mine the flow pattern between urban functional areas, so as to provide more accurate spatial information for prediction. In addition, in order to limit the number of parameters, we migrate a temporal adaptive convolution (TaLK) to capture the time correlation of each station according to the characteristics of passenger flow. The self-adaptive mechanism in space and time can enhance the fitting ability of the model. By comparing six representative algorithms on Hangzhou Metro dataset, the results show that the proposed method is effective and takes up the least parameters. Meanwhile, experiments show that the algorithm can find the main communication between function areas.},
  archive      = {J_APIN},
  author       = {Wei, Qianjin and Qiu, Yongheng and Wen, Yimin},
  doi          = {10.1007/s10489-022-03305-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14137-14152},
  shortjournal = {Appl. Intell.},
  title        = {Cluster-based spatiotemporal dual self-adaptive network for short-term subway passenger flow forecasting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social-path embedding-based transformer for graduation
development prediction. <em>APIN</em>, <em>52</em>(12), 14119–14136. (<a
href="https://doi.org/10.1007/s10489-022-03268-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the education of students attracts more and more attention, the task of graduation development prediction has gradually become a hot topic in academia and industry. The task of graduation development prediction aims to predict the employment category of students in advance via academic achievement data, which can help administrators understand students’ learning status and set up a reasonable learning plan. However, existing research ignores the potential impact of social relationships on students’ graduation development choices. To fully explore social relationships among students, we propose a Social-path Embedding-based Transformer Neural Network (SPE-TNN) for the task of graduation development prediction in this paper. Specifically, SPE-TNN is divided into the Social-path selection layer, the Social-path embedding layer, the Transformer layer, and the Multi-layer projection layer. Firstly, the Social-path selection layer is designed to find social relationships that impact graduation development and embed them into the student’s performance features through the Social-path embedding layer. Secondly, the Transformer layer is adopted to balance the weights of the students’ features. Finally, the Multi-layer projection layer is used to achieve the student graduation development prediction. Experimental results on the real-world datasets show that SPE-TNN outperforms the existing popular approaches.},
  archive      = {J_APIN},
  author       = {Yang, Guangze and Ouyang, Yong and Ye, Zhiwei and Gao, Rong and Zeng, Yawen},
  doi          = {10.1007/s10489-022-03268-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14119-14136},
  shortjournal = {Appl. Intell.},
  title        = {Social-path embedding-based transformer for graduation development prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAV swarm path planning with reinforcement learning for
field prospecting. <em>APIN</em>, <em>52</em>(12), 14101–14118. (<a
href="https://doi.org/10.1007/s10489-022-03254-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been steady growth in the adoption of Unmanned Aerial Vehicle (UAV) swarms by operators due to their time and cost benefits. However, this kind of system faces an important problem, which is the calculation of many optimal paths for each UAV. Solving this problem would allow control of many UAVs without human intervention while saving battery between recharges and performing several tasks simultaneously. The main aim is to develop a Reinforcement Learning based system capable of calculating the optimal flight path for a UAV swarm. This method stands out for its ability to learn through trial and error, allowing the model to adjust itself. The aim of these paths is to achieve full coverage of an overflight area for tasks such as field prospection, regardless of map size and the number of UAVs in the swarm. It is not necessary to establish targets or to have any previous knowledge other than the given map. Experiments have been conducted to determine whether it is optimal to establish a single control for all UAVs in the swarm or a control for each UAV. The results show that it is better to use one control for all UAVs because of the shorter flight time. In addition, the flight time is greatly affected by the size of the map. The results give starting points for future research, such as finding the optimal map size for each situation.},
  archive      = {J_APIN},
  author       = {Puente-Castro, Alejandro and Rivero, Daniel and Pazos, Alejandro and Fernandez-Blanco, Enrique},
  doi          = {10.1007/s10489-022-03254-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14101-14118},
  shortjournal = {Appl. Intell.},
  title        = {UAV swarm path planning with reinforcement learning for field prospecting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MSIMCNN: Multi-scale inception module convolutional neural
network for multi-focus image fusion. <em>APIN</em>, <em>52</em>(12),
14085–14100. (<a
href="https://doi.org/10.1007/s10489-022-03160-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of image fusion is to obtain a clear image by combining useful information coming from multiple images. However, the fused image usually has the problem of artifacts and unclear boundary. To address these problems, a deep convolutional neural network based framework for multi-focus image fusion is proposed in this paper, called multi-scale inception module convolutional neural network (MSIMCNN). MSIMCNN converts the entire image into a binary mask to estimate the focus characteristics, and obtains the clear boundary between focus and defocus. First of all, a pair of focus images and the corresponding feature images detected by the Laplace operator are inputted into the network. The Laplace operator can detect the edge and gradient of focus in the image, which can help us accurately reconstruct the focused area in the focus map and distinguish the focus and defocus boundaries. Then, in the feature extraction stage, different scales of convolution kernels are designed to extract the rich and complementary features at different scales of the source images. At the same time, the inception module is added to increase the width of the network and reduce the parameters, which can extract more focus features required for image reconstruction and reduce the complexity. Finally, the focus map of the source image pair can be obtained in the feature reconstruction stage. In this stage, an efficient method is proposed to make the focus mask, which is used for the calculation of the loss function and the generation of the training set. The experimental results on different data sets confirm the superiority and effectiveness of MSIMCNN compared with other methods.},
  archive      = {J_APIN},
  author       = {Gao, Wenchang and Yu, Lei and Tan, Yao and Yang, Pengna},
  doi          = {10.1007/s10489-022-03160-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14085-14100},
  shortjournal = {Appl. Intell.},
  title        = {MSIMCNN: Multi-scale inception module convolutional neural network for multi-focus image fusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Decomposed-distance weighted optimal transport for
unsupervised domain adaptation. <em>APIN</em>, <em>52</em>(12),
14070–14084. (<a
href="https://doi.org/10.1007/s10489-021-03112-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain with a different but related distribution. Optimal Transport (OT) based Wasserstein distance has recently been used to measure and reduce the domain discrepancy in virtue of its robustness in distance measurement. However, the inaccurate estimation of the transport cost between samples is harmful to the fine-grained domain alignment. This paper proposes Decomposed-Distance Weighted Optimal Transport (DDW-OT) method for better adaptation. Technically, according to the clustering-based prototype generation (CPG), DDW-OT constructs a decomposed-distance reweighing matrix to revise the original inaccurate transport distance on sample-level, which conjoins the category uncertainty of the target samples and the correlation degree of category between domains. Besides, the dual-OT solver takes neural netw11 orks to parameterize the dual variables and alleviate the computation cost. DDW-OT also allocated an explicit class-conditional alignment strategy to enhance transfer performance. Extensive experiments on benchmarks demonstrate the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Bilin and Wang, Shengsheng and Zhang, Zhe and Zhao, Xin and Fu, Zihao},
  doi          = {10.1007/s10489-021-03112-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14070-14084},
  shortjournal = {Appl. Intell.},
  title        = {Decomposed-distance weighted optimal transport for unsupervised domain adaptation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bottom-up improved multistage temporal convolutional network
for action segmentation. <em>APIN</em>, <em>52</em>(12), 14053–14069.
(<a href="https://doi.org/10.1007/s10489-022-03382-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action segmentation involves locating and classifying human action segments in an untrimmed video, which is very important for understanding human activities. Segmenting actions in the video is a very challenging task due to the problem of ambiguous frames. Previous studies on this topic usually required additional inputs or constructed highly complicated network structures to achieve good performance. However, these additional inputs are not easy to obtain, and complicated network structures increase the costs of computation and storage. Hence, to mitigate these problems, we propose a bottom-up improved multistage temporal convolutional network (BUIMS-TCN) for action segmentation. Specifically, we first propose a smoothed dilated 1D convolution to learn the inherent local temporal dependencies. Second, we design an adaptive temporal fusion module (ATFM), which is a simple yet effective multiscale temporal-context information fusion module, to obtain better semantic feature representations. Finally, we introduce a new loss function to solve the imbalance between easy and hard samples. To the best of our knowledge, this is the first time that the above improvements have been incorporated into the action segmentation task. Extensive experiments verify that our model significantly outperforms the state-of-the-art baselines on three challenging benchmark datasets: Georgia Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset.},
  archive      = {J_APIN},
  author       = {Chen, Wenhe and Chai, Yuan and Qi, Miao and Sun, Hui and Pu, Qi and Kong, Jun and Zheng, Caixia},
  doi          = {10.1007/s10489-022-03382-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14053-14069},
  shortjournal = {Appl. Intell.},
  title        = {Bottom-up improved multistage temporal convolutional network for action segmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active object tracking using context estimation: Handling
occlusions and detecting missing targets. <em>APIN</em>,
<em>52</em>(12), 14041–14052. (<a
href="https://doi.org/10.1007/s10489-021-03116-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When performing visual servoing or object tracking tasks, active sensor planning is essential to keep targets in sight or to relocate them when missing. In particular, when dealing with a known target missing from the sensor’s field of view, we propose using prior knowledge related to contextual information to estimate its possible location. To this end, this study proposes a Dynamic Bayesian Network that uses contextual information to effectively search for targets. Monte Carlo particle filtering is employed to approximate the posterior probability of the target’s state, from which uncertainty is defined. We define the robot’s utility function via information theoretic formalism as seeking the optimal action which reduces uncertainty of a task, prompting robot agents to investigate the location where the target most likely might exist. Using a context state model, we design the agent’s high-level decision framework using a Partially-Observable Markov Decision Process. Based on the estimated belief state of the context via sequential observations, the robot’s navigation actions are determined to conduct exploratory and detection tasks. By using this multi-modal context model, our agent can effectively handle basic dynamic events, such as obstruction of targets or their absence from the field of view. We implement and demonstrate these capabilities on a mobile robot in real-time.},
  archive      = {J_APIN},
  author       = {Kim, Minkyu and Sentis, Luis},
  doi          = {10.1007/s10489-021-03116-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14041-14052},
  shortjournal = {Appl. Intell.},
  title        = {Active object tracking using context estimation: Handling occlusions and detecting missing targets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BILEAT: A highly generalized and robust approach for unified
aspect-based sentiment analysis. <em>APIN</em>, <em>52</em>(12),
14025–14040. (<a
href="https://doi.org/10.1007/s10489-022-03311-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) includes two subtasks, namely, aspect term extraction and aspect-level sentiment classification. Most existing works address these subtasks independently. Recently, many researchers have attempted to solve both the subtasks of ABSA with a unified framework. However, previous works have not focused on the generalization and robustness of such unified frameworks. This paper proposes a novel BERT-Based Interactive Learning with Ensemble Adversarial Training (BILEAT) to solve complete ABSA by using a unified tagging scheme. We build white-box adversarially post-trained domain knowledge BERT (WBDK-BERT) using a domain-specific dataset. During post-training, we regularize the training objective by adding perturbations in the embedding space to maximize the adversarial loss, enhancing the generalization and robustness of WBDK-BERT. BILEAT uses WBDK-BERT to generate contextualized embeddings and produce collaborative signals through interactive learning. Further, to build a highly reliable model, we generate adversarial examples using a black-box technique. These adversarial examples are grammatically fluent, semantically coherent with original input, and can mislead the neural network. Our proposed model is trained using original inputs and such adversarial examples in a combined way. Experimental results demonstrate that WBDK-BERT and black-box adversarial examples complement each other, and combining these two helps BILEAT become highly generalized and robust compared to existing methods. To the best of our knowledge, this is the first study that generates quality adversarial examples and evaluates the robustness of models for unified ABSA1.},
  archive      = {J_APIN},
  author       = {Kumar, Avinash and Balan, Raghunathan and Gupta, Pranjal and Neti, Lalita Bhanu Murthy and Malapati, Aruna},
  doi          = {10.1007/s10489-022-03311-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14025-14040},
  shortjournal = {Appl. Intell.},
  title        = {BILEAT: A highly generalized and robust approach for unified aspect-based sentiment analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach for APT malware detection based on deep graph
network for endpoint systems. <em>APIN</em>, <em>52</em>(12),
14005–14024. (<a
href="https://doi.org/10.1007/s10489-021-03138-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The form of spreading malware through end-users and thereby escalating and stealing data in organizations is one of the attack techniques widely used by Advanced Persistent Threat (APT) attackers today. Therefore, the task of timely detecting and warning about APT malware on the workstation is an important and necessary issue because if this task is successful, it will prevent the whole APT attack campaign on the system. To accomplish this purpose, this study proposes a method of detecting APT malware on the workstation based on analyzing the behavior profile of malware using the deep learning graph network. Accordingly, the proposed method includes two main tasks: (i) building behavior profiles of malware: for this task, behavior profiles will be built based on the process of gathering and evaluating Event IDs from the kernel of the workstation. The result of this process of building behavior profiles is the set of processes and labels of each process performed by executable files. The label value is normal, malicious, suspicious, or unknown; (ii) detecting malware based on analyzing behavior profiles using graph network: for this task, based on behavior profiles built from the task (i), we are evaluate and analyze these behavior profiles by the Graph Isomorphism Network (GIN) deep learning graph network method. The results of this behavior profile classification will be used as a basis to conclude which behavior profiles were generated by the APT malware and which behavior profiles are normal. The method of detecting APT malware on workstation based on analyzing behavior profiles using the graph network is a novel method. According to our survey, up to now, this method has not been proposed and applied in any research. The experimental results in Section 4.3 of the paper have shown the remarkable efficiency of our proposed method. With such results, this proposal has not only scientific but also practical significance. The method of using graph networks to analyze and evaluate behavior profiles helps improve the efficiency of the process of analyzing and detecting APT malware on the workstation.},
  archive      = {J_APIN},
  author       = {Do Xuan, Cho and Huong, DT},
  doi          = {10.1007/s10489-021-03138-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {14005-14024},
  shortjournal = {Appl. Intell.},
  title        = {A new approach for APT malware detection based on deep graph network for endpoint systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint learning affinity matrix and representation matrix for
robust low-rank multi-kernel clustering. <em>APIN</em>, <em>52</em>(12),
13987–14004. (<a
href="https://doi.org/10.1007/s10489-021-02974-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-kernel subspace clustering has attracted widespread attention, because it can process nonlinear data effectively. It usually solves the representation coefficient between data by the subspace clustering optimization model, and then the constructed affinity matrix is input into the spectral clustering method to get the final clustering result. Obviously, the quality of the affinity matrix (graph) has a significant impact on the final clustering result. Unfortunately, there are two deficiencies in the previous multi-kernel subspace clustering methods as follows: 1) this typical two-phase method restricts the learning of the affinity matrix; 2) it does not fully extract the data global structure mapped to the kernel space. In order to solve these two problems simultaneously, a novel low-rank multi-kernel subspace clustering method incorporating a joint learning scheme, namely JALSC, is proposed. The innovation of this method is reflected in the following two aspects: 1) the adaptive local structure is used to learn the representation of the data and the affinity graph in the integrated objective function at the same time. The optimal affinity graph obtained by the one-step learning scheme helps to improve the clustering performance; 2) our method uses a non-convex low-rank approximation function to constrain the consensus kernel to preserve the global structure of the data after mapping to the feature space. A mass of experiments on several commonly used datasets show that JALSC obtains the best clustering performance and has better robustness compared with several advanced multi-kernel clustering methods.},
  archive      = {J_APIN},
  author       = {Luo, Liang and Liang, Qin and Zhang, Xiaoqian and Xue, Xuqian and Liu, Zhigui},
  doi          = {10.1007/s10489-021-02974-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13987-14004},
  shortjournal = {Appl. Intell.},
  title        = {Joint learning affinity matrix and representation matrix for robust low-rank multi-kernel clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level selective potentiality maximization for
interpreting multi-layered neural networks. <em>APIN</em>,
<em>52</em>(12), 13961–13986. (<a
href="https://doi.org/10.1007/s10489-021-02705-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper aims to extract the inference mechanism of neural networks, supposed to be hidden in complicated surface phenomena, by maximizing information in terms of multiple selective constraints. For easy interpretation of the meaning of information content, information is represented in terms of selectivity of components, or selective potentiality. The selective potentiality represents an ability of neurons to respond selectively to inputs, and this selectivity should exclusively increase when going through different neurons. In addition, because the selectivity can be realized by increasing the strength of connection weights, we try to reduce this strength as much as possible, namely, cost minimization. The selectivity and cost are hierarchically applied as multiple constraints, disentangling complicated components to make the functions of neurons and connection weights as clear as possible, leading us to find the inner inference mechanism. The method was applied to the simple qualitative bankruptcy and more complicated bank marketing data sets, where the number of hidden layers increased to 15 to examine how multi-layered networks could be used to disentangle complicated components. Experimental results showed that the selective potentiality could disentangle connection weights and eventually produce linear and individual features for easy interpretation.},
  archive      = {J_APIN},
  author       = {Kamimura, Ryotaro},
  doi          = {10.1007/s10489-021-02705-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13961-13986},
  shortjournal = {Appl. Intell.},
  title        = {Multi-level selective potentiality maximization for interpreting multi-layered neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-day ransomware attack detection using deep contractive
autoencoder and voting based ensemble classifier. <em>APIN</em>,
<em>52</em>(12), 13941–13960. (<a
href="https://doi.org/10.1007/s10489-022-03244-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ransomware attacks are hazardous cyber-attacks that use cryptographic methods to hold victims’ data until the ransom is paid. Zero-day ransomware attacks try to exploit new vulnerabilities and are considered a severe threat to existing security solutions and internet resources. In the case of zero-day attacks, training data is not available before the attack takes place. Therefore, we exploit Zero-shot Learning (ZSL) capabilities that can effectively deal with unseen classes compared to the traditional machine learning techniques. ZSL is a two-stage process comprising of: Attribute Learning (AL) and Inference Stage (IS). In this regard, this work presents a new Deep Contractive Autoencoder based Attribute Learning (DCAE-ZSL) technique as well as an IS method based on Heterogeneous Voting Ensemble (DCAE-ZSL-HVE). In the proposed DCAE-ZSL approach, Contractive Autoencoder (CAE) is employed to extract core features of known and unknown ransomware. The regularization term of CAE helps in penalizing the classifier&#39;s sensitivity against the small dissimilarities in the latent space. On the other hand, in case of the IS, four combination rules Global Majority (GM), Local Majority (LM), Cumulative Vote-against based Global Majority (CVAGM), Cumulative Vote-for based Global Majority (CVFGM) are utilized to find the final prediction. It is empirically shown that in comparison to conventional machine learning techniques, models trained on contractive embedding show reasonable performance against zero-day attacks. Furthermore, it is shown that the exploitation of these core features through the proposed voting based ensemble (DCAE-ZSL-HVE) has demonstrated significant improvement in detecting zero-day attacks (recall = 0.95) and reducing False Negative (FN = 6).},
  archive      = {J_APIN},
  author       = {Zahoora, Umme and Rajarajan, Muttukrishnan and Pan, Zahoqing and Khan, Asifullah},
  doi          = {10.1007/s10489-022-03244-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13941-13960},
  shortjournal = {Appl. Intell.},
  title        = {Zero-day ransomware attack detection using deep contractive autoencoder and voting based ensemble classifier},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intention model based multi-round dialogue strategies for
conversational AI bots. <em>APIN</em>, <em>52</em>(12), 13916–13940. (<a
href="https://doi.org/10.1007/s10489-022-03288-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational AI (CoAI) bot, such as customer service bots, navigation bots, chat bots and etc., is a new form of software application. How to accurately identify user requirements and provide appropriate services is one of the biggest challenges that hinder its widespread use. This requires CoAI to get rid of the template and standard knowledge structure, enhance the reasoning ability of service and the knowledge involved. Furthermore, It have the ability to guide users to express complete requirements through multiple rounds of dialogue. To achieve this, firstly, this study purposes a new intention model, and Knowledge Graph of Requirements (KGR) to expand the requirement knowledge scope of conversational AI bots. Secondly, this study purposes a knowledge processing method based on the theory of granular computing. This method could help CoAI bot to determine the next round of inquiry content automatically before the dialogue begins. At last, considering the commonality of the requirement pattern (RP) repository and the personalization of the KGR, this study proposes three optimization methods to improve the dialogue strategy based on two intention models for scenarios with different characteristics. The experimental result shows that, compared with the existing methods, the proposed method can reduce redundancy in dialogues, and integrate the commonality and individuality of user requirements successfully. Meanwhile, this method effectively improves the efficiency and performance of requirement elicitation, and takes the user experience into account as well.},
  archive      = {J_APIN},
  author       = {Tian, Junrui and Tu, Zhiying and Li, Nan and Su, Tonghua and Xu, Xiaofei and Wang, Zhongjie},
  doi          = {10.1007/s10489-022-03288-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13916-13940},
  shortjournal = {Appl. Intell.},
  title        = {Intention model based multi-round dialogue strategies for conversational AI bots},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A theoretical analysis based on causal inference and
single-instance learning. <em>APIN</em>, <em>52</em>(12), 13902–13915.
(<a href="https://doi.org/10.1007/s10489-022-03193-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although using single-instance learning methods to solve multi-instance problems has achieved excellent performance in many tasks, the reasons for this success still lack a rigorous theoretical explanation. In particular, the potential relation between the number of causal factors (also called causal instances) in a bag and the model performance is not transparent. The goal of our study is to use the causal relationship between instances and bags to enhance the interpretability of multi-instance learning. First, we provide a lower bound on the number of instances required to determine causal factors in a real multi-instance learning task. Then, we provide a lower bound on the single-instance learning loss function when testing instances and training instances follow the same distribution and extend this conclusion to the situation where the distribution changes. Thus, theoretically, we demonstrate that the number of causal factors in the bag is an important parameter that affects the performance of the model when using single-instance learning methods to solve multi-instance learning problems. Finally, combining with a specific classification task, we experimentally validate our theoretical analysis.},
  archive      = {J_APIN},
  author       = {Wang, Chao and Lu, Xuantao and Wang, Wei},
  doi          = {10.1007/s10489-022-03193-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13902-13915},
  shortjournal = {Appl. Intell.},
  title        = {A theoretical analysis based on causal inference and single-instance learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring context with reliable collaborators: A novel
similarity estimation method for recommender systems. <em>APIN</em>,
<em>52</em>(12), 13883–13901. (<a
href="https://doi.org/10.1007/s10489-022-03162-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additional context information is vital for context-aware recommender systems. The whole paradigm of context-aware recommender systems is built upon the availability of contextual features. Apart from the significance of context, we highlight a key issue for existing context-aware recommendation paradigm that if the user environment did not provide contextual features such as time, location, or companion due to privacy constraints or if the data collection system is unable to record contextual attributes due to legal or technical concerns then the existing context-aware recommendation paradigm has no uniform mechanism to deal with this situation. In this research, we address these challenges and propose a novel item-context similarity (ICS) model capable of adaptively generating reliable collaborators for a subject user on a subject item. Additionally, ICS is fused into a weighting model called contextually reliable collaborators (CRC) that considers the current item context, the nonlinear relationship between candidate collaborators and the asymmetry between rating preferences of users to finally generate rating prediction. Experiments show that neighbors computed through ICS are more reliable than the classical similarity estimation methods and the ICS-based CRC model has outperformed state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Ali, Waqar and Kumar, Jay and Shao, Jie},
  doi          = {10.1007/s10489-022-03162-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13883-13901},
  shortjournal = {Appl. Intell.},
  title        = {Inferring context with reliable collaborators: A novel similarity estimation method for recommender systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-distribution binary neural networks. <em>APIN</em>,
<em>52</em>(12), 13870–13882. (<a
href="https://doi.org/10.1007/s10489-022-03348-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study network binarization (i.e., binary neural networks, BNNs), which is one of the most promising techniques in network compression for convolutional neural networks (CNNs). Although prior work has introduced many binarization methods that improve the accuracy of BNNs by minimizing the quantization error, there remains a non-negligible performance gap between the binarized model and the full-precision model. Given that feature representation is critical for deep neural networks and that in BNNs, the features only differ in signs, we argue that the impact on the accuracy of BNNs may be strongly related to the sign distribution of the network parameters in addition to the quantization error. To this end, Self-Distribution Binary Neural Network (SD-BNN) is proposed. First, we utilize Activation Self Distribution (ASD) to adaptively adjust the sign distribution of activations, thereby improving the sign differences of the outputs of the convolution. Second, we adjust the sign distribution of weights through Weight Self Distribution (WSD) and then fine-tune the sign distribution of the outputs of the convolution. Extensive experiments on the CIFAR-10 and ImageNet datasets with various network structures show that the proposed SD-BNN consistently outperforms state-of-the-art (SOTA) BNNs (e.g., 92.5% on CIFAR-10 and 66.5% on ImageNet with ResNet-18) with lower computational cost. Our code is available at https://github.com/pingxue-hfut/SD-BNN .},
  archive      = {J_APIN},
  author       = {Xue, Ping and Lu, Yang and Chang, Jingfei and Wei, Xing and Wei, Zhen},
  doi          = {10.1007/s10489-022-03348-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13870-13882},
  shortjournal = {Appl. Intell.},
  title        = {Self-distribution binary neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dense spatially-weighted attentive residual-haze network for
image dehazing. <em>APIN</em>, <em>52</em>(12), 13855–13869. (<a
href="https://doi.org/10.1007/s10489-022-03168-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze severely affects computer vision algorithms by degrading the quality of captured images and results in image data loss. With several available approaches for dehazing, single image dehazing is most preferred and challenging. We proposed a Dense Spatially-weighted Attentive Residual-haze Network (DSA Net), a novel end-to-end Encoder-decoder architecture to learn the residual haze layer between the hazy and haze-free image. We use encoder-decoder blocks with multiple skip connections to improve feature propagation. Feature Learning block uses a novel Residual Inception fused with Attention (RIA) block to learn the complex non-linearity from features extracted from the encoder part. Learning residual image is more straightforward than the whole haze-free image, and it improves the ability of the network to estimate the haze thickness accurately. DSA Net learns this less complex residual-map from the hazy input image and subtracts it from the input to obtain the dehazed images. Detail ablation study shows the effectiveness of different modules used in our architecture. Experiment results on different haze conditions demonstrate that our method shows significant improvement over other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Singh, Mohit and Laxmi, Vijay and Faruki, Parvez},
  doi          = {10.1007/s10489-022-03168-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13855-13869},
  shortjournal = {Appl. Intell.},
  title        = {Dense spatially-weighted attentive residual-haze network for image dehazing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Twitter-aided decision making: A review of recent
developments. <em>APIN</em>, <em>52</em>(12), 13839–13854. (<a
href="https://doi.org/10.1007/s10489-022-03241-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter is one of the largest online platforms where people exchange information. In the first few years since its emergence, researchers have been exploring ways to use Twitter data in various decision making scenarios, and have shown promising results. In this review, we examine 28 newer papers published in last five years (since 2016) that continued to advance Twitter-aided decision making. The application scenarios we cover include product sales prediction, stock selection, crime prevention, epidemic tracking, and traffic monitoring. We first discuss the findings presented in these papers, that is how much decision making performance has been improved with the help of Twitter data. Then we offer a methodological analysis that considers four aspects of methods used in these papers, including problem formulation, solution, Twitter feature, and information transformation. This methodological analysis aims to enable researchers and decision makers to see the applicability of Twitter-aided methods in different application domains or platforms.},
  archive      = {J_APIN},
  author       = {Zhang, Yihong and Shirakawa, Masumi and Wang, Yuanyuan and Li, Zhi and Hara, Takahiro},
  doi          = {10.1007/s10489-022-03241-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13839-13854},
  shortjournal = {Appl. Intell.},
  title        = {Twitter-aided decision making: A review of recent developments},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monitoring social-distance in wide areas during pandemics: A
density map and segmentation approach. <em>APIN</em>, <em>52</em>(12),
13824–13838. (<a
href="https://doi.org/10.1007/s10489-022-03172-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the relaxation of the containment measurements around the globe, monitoring the social distancing in crowded public spaces is of great importance to prevent a new massive wave of COVID-19 infections. Recent works in that matter have limited themselves by assessing social distancing in corridors up to small crowds by detecting each person individually, considering the full body in the image. In this work, we propose a new framework for monitoring the social-distance using end-to-end Deep Learning, to detect crowds violating social-distancing in wide areas, where important occlusions may be present. Our framework consists in the creation of new ground truth social distance labels, based on the ground truth density maps, and the proposal of two different solutions, a density-map-based and a segmentation-based, to detect crowds violating social-distancing constraints. We assess the results of both approaches by using the generated ground truth from the PET2009 and CityStreet datasets. We show that our framework performs well at providing the zones where people are not following the social-distance, even when heavily occluded or far away from the camera, compared to current detection and tracking approaches.},
  archive      = {J_APIN},
  author       = {Gonzalez-Trejo, Javier Antonio and Mercado-Ravell, Diego A. and Jaramillo-Avila, Uziel},
  doi          = {10.1007/s10489-022-03172-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13824-13838},
  shortjournal = {Appl. Intell.},
  title        = {Monitoring social-distance in wide areas during pandemics: A density map and segmentation approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional DenseNet self-attention neural network for
automatic detection of student’s engagement. <em>APIN</em>,
<em>52</em>(12), 13803–13823. (<a
href="https://doi.org/10.1007/s10489-022-03200-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, due to the widespread outbreak of the deadly coronavirus, popularly known as COVID-19, the traditional classroom education has been shifted to computer-based learning. Students of various cognitive and psychological abilities participate in the learning process. However, most students are hesitant to provide regular and honest feedback on the comprehensiveness of the course, making it difficult for the instructor to ensure that all students are grasping the information at the same rate. The students’ understanding of the course and their emotional engagement, as indicated via facial expressions, are intertwined. This paper attempts to present a three-dimensional DenseNet self-attention neural network (DenseAttNet) used to identify and evaluate student participation in modern and traditional educational programs. With the Dataset for Affective States in E-Environments (DAiSEE), the proposed DenseAttNet model outperformed all other existing methods, achieving baseline accuracy of 63.59% for engagement classification and 54.27% for boredom classification, respectively. Besides, DenseAttNet trained on all four multi-labels, namely boredom, engagement, confusion, and frustration has registered an accuracy of 81.17%, 94.85%, 90.96%, and 95.85%, respectively. In addition, we performed a regression experiment on DAiSEE and obtained the lowest Mean Square Error (MSE) value of 0.0347. Finally, the proposed approach achieves a competitive MSE of 0.0877 when validated on the Emotion Recognition in the Wild Engagement Prediction (EmotiW-EP) dataset.},
  archive      = {J_APIN},
  author       = {Mehta, Naval Kishore and Prasad, Shyam Sunder and Saurav, Sumeet and Saini, Ravi and Singh, Sanjay},
  doi          = {10.1007/s10489-022-03200-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13803-13823},
  shortjournal = {Appl. Intell.},
  title        = {Three-dimensional DenseNet self-attention neural network for automatic detection of student’s engagement},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale group decision-making (LSGDM) for performance
measurement of healthcare construction projects: Ordinal priority
approach. <em>APIN</em>, <em>52</em>(12), 13781–13802. (<a
href="https://doi.org/10.1007/s10489-022-04094-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People with various skill sets and backgrounds are usually found working on projects and thus, group decision-making (GDM) is one of the most important functions within any project. However, when projects concern healthcare or other critical services for proletariat or general public (especially during COVID19), the importance of GDM can hardly be overstated. Measuring the performance of healthcare construction projects is a critical activity and should be gauged based on the input from a large number of stakeholders. Such problems are usually recognized as large-scale group decision-making (LSGDM). In the current study, we aim to propose a decision support system for measuring the performance of healthcare construction projects against a large number of experts using ordinal data. The study identifies several key indicators from literature and recorded the observations of a large number of experts about these indicators. After that, the acceptable range of complexity is specified, the Silhouette plot is provided to find the optimal number of clusters, and the ordinal K-means method is employed to cluster the experts’ opinions. Later, the confidence level is measured using a novel Weighted Kendall’s W for the optimal number of the clusters, and the threshold is checked. Finally, the conventional problem is solved using the Group Weighted Ordinal Priority Approach (GWOPA) model in multiple attributes decision making (MADM), and the performance of the projects is determined. The validity of the proposed approach is confirmed through a comparative analysis. Also, a real-world case is solved, and the performance of some healthcare construction projects in China is gauged with a comprehensive sensitivity analysis.},
  archive      = {J_APIN},
  author       = {Mahmoudi, Amin and Abbasi, Mehdi and Yuan, Jingfeng and Li, Lingzhi},
  doi          = {10.1007/s10489-022-04094-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13781-13802},
  shortjournal = {Appl. Intell.},
  title        = {Large-scale group decision-making (LSGDM) for performance measurement of healthcare construction projects: Ordinal priority approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). An ensemble learning model for preference-geographical
aware point-of interest recommendation. <em>APIN</em>, <em>52</em>(12),
13763–13780. (<a
href="https://doi.org/10.1007/s10489-022-04035-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of location-based social networks (LBSNs), which contain a lot of information, creates the possibility of a point-of-interest (POI) recommendation. Meanwhile, LBSNs also make the POI recommendation an important service and have attracted widespread attention from industries and academia. Most traditional POI recommendation methods focus on finding similar users of a target user and generate suggestions by exploring check-in histories of these similar users. However, such suggestions may be biased and lack variousness. To address this problem, we design a novel ensemble learning framework for POI recommendation, named Preference-Geographical Point-of-interest Recommendation Ensemble (PG-PRE). For a target user, we first construct multiple similar user group and use a roulette selection-based sampling method to improve the variousness of such groups. Each group will give a POI recommendation suggestion. Then a Gaussian mixture-based approach is proposed to calculate the voting weight of each group. Finally, a recommendation list of the target user is achieved by comprehensively considering suggestions of each group according to the corresponding voting weight. As compared to the state-of-the-art POI recommendation methods, the experimental results demonstrate that our method exhibits much better performance.},
  archive      = {J_APIN},
  author       = {Liu, Shuang and Yang, Leilei and Zheng, Wenguang and Xiao, Yingyuan and Liu, Li},
  doi          = {10.1007/s10489-022-04035-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13763-13780},
  shortjournal = {Appl. Intell.},
  title        = {An ensemble learning model for preference-geographical aware point-of interest recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new humanitarian relief logistic network for
multi-objective optimization under stochastic programming.
<em>APIN</em>, <em>52</em>(12), 13729–13762. (<a
href="https://doi.org/10.1007/s10489-022-03776-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of affected people and thousands of victims are consequences of earthquakes, every year. Therefore, it is necessary to prepare a proper preparedness and response planning. The objectives of this paper are i) minimizing the expected value of the total costs of relief supply chain, ii) minimizing the maximum number of unsatisfied demands for relief staff and iii) minimizing the total probability of unsuccessful evacuation in routes. In this paper, a scenario based stochastic multi-objective location-allocation-routing model is proposed for a real humanitarian relief logistics problem which focused on both pre- and post-disaster situations in presence of uncertainty. To cope with demand uncertainty, a simulation approach is used. The proposed model integrates these two phases simultaneously. Then, both strategic and operational decisions (pre-disaster and post-disaster), fairness in the evacuation, and relief item distribution including commodities and relief workers, victim evacuation including injured people, corpses and homeless people are also considered simultaneously in this paper. The presented model is solved utilizing the Epsilon-constraint method for small- and medium-scale problems and using three metaheuristic algorithms for the large-scale problem (case study). Empirical results illustrate that the model can be used to locate the shelters and relief distribution centers, determine appropriate routes and allocate resources in uncertain and real-life disaster situations.},
  archive      = {J_APIN},
  author       = {Ghasemi, Peiman and Goodarzian, Fariba and Abraham, Ajith},
  doi          = {10.1007/s10489-022-03776-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13729-13762},
  shortjournal = {Appl. Intell.},
  title        = {A new humanitarian relief logistic network for multi-objective optimization under stochastic programming},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interval-valued dual hesitant fuzzy linguistic group
recommendation method by considering the double relevance.
<em>APIN</em>, <em>52</em>(12), 13714–13728. (<a
href="https://doi.org/10.1007/s10489-022-03461-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional group recommendation method cannot reflect the interactive relationships among group users or items attribute. We propose a new method,and the new method can describe the cognitive preferences of group users more completely. It can deal with the group recommendation problem that both the group users and the items attribute are related. We use the fuzzy measure as underlying technical support, combined with Choquet integral and VIKOR method, applying the expressive advantage of interval-valued dual hesitant fuzzy linguistic information, and then putting forward a novel group recommendation method. The application of the procedure to a group recommendation, the recommendation set close to the group user’s cognitive preference is obtained, which directly verified its effectiveness and indirectly verified the scientific rationality of Choquet integral and VIKOR method.},
  archive      = {J_APIN},
  author       = {Jiang, Wenchao and Yuan, Xumei and Zang, Yuqi},
  doi          = {10.1007/s10489-022-03461-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13714-13728},
  shortjournal = {Appl. Intell.},
  title        = {Interval-valued dual hesitant fuzzy linguistic group recommendation method by considering the double relevance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of group decision making methods in healthcare
industry 4.0: Bibliometrics, applications, and directions.
<em>APIN</em>, <em>52</em>(12), 13689–13713. (<a
href="https://doi.org/10.1007/s10489-021-02909-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare Industry 4.0 refers to intelligent operation processes in the medical industry. With the development of information technology, large-scale group decision making (GDM), which allows a larger number of decision makers (DMs) from different places or sectors to participate in decision making, has been rapidly developed and applied in Healthcare Industry 4.0 to help to make decisions efficiently and smartly. To make full use of GDM methods to promote the developments of the medical industry, it is necessary to review the existing relevant achievements. Therefore, this paper conducts an overview to generate a comprehensive understanding of GDM in Healthcare Industry 4.0 and to identify future development directions. Bibliometric analyses are conducted in order to learn the development trends from published papers. The implementations of GDM methods in Healthcare Industry 4.0 are reviewed in accordance with the paradigm of the general GDM process, which includes information representation, dimension reduction, consensus reaching, and result elicitation. We also provide current research challenges and future directions regarding medical GDM. It is hoped that our study will be helpful for researchers in the field of GDM in Healthcare Industry 4.0.},
  archive      = {J_APIN},
  author       = {Lu, Keyu and Liao, Huchang},
  doi          = {10.1007/s10489-021-02909-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13689-13713},
  shortjournal = {Appl. Intell.},
  title        = {A survey of group decision making methods in healthcare industry 4.0: Bibliometrics, applications, and directions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal deep learning-based LSTM for stock price
prediction using twitter sentiment analysis. <em>APIN</em>,
<em>52</em>(12), 13675–13688. (<a
href="https://doi.org/10.1007/s10489-022-03175-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock Price Prediction is one of the hot research topics in financial engineering, influenced by economic, social, and political factors. In the present stock market, the positive and negative opinions are the important indicators for the forthcoming stock prices. At the same time, the growth of the internet and social network enables the clients to express their opinions and shares their views on future stock processes. Therefore, sentiment analysis of the social media data of stock prices helps to predict future stock prices effectively. With this motivation, this research presents a new novel Teaching and Learning Based Optimization (TLBO) model with Long Short-Term Memory (LSTM) based sentiment analysis for stock price prediction using Twitter data. The tweets are generally short, having unusual grammatical structures, and hence the data pre-processing is essential to remove the unwanted data and transform it into a meaningful format. Besides, the LSTM model is applied to classify tweets into positive and negative sentiments related to stock prices. They help investigate how the tweets correlate with the nature of the stock market prices. To improve the predictive outcome of the LSTM model, the Adam optimizer is used to determine the learning rate. Furthermore, the TLBO model is applied to tune the output unit of the LSTM model optimally. Experiments are carried out on the Twitter data to ensure the better stock price predictive performance of the TLBO-LSTM model. The experimental findings of the TLBO-LSTM model show promising results over the state of art methods in terms of diverse aspects. The TLBO-LSTM model produced a superior outcome, with a maximum precision of 95.33%, a recall of 85.28%, and an F-score of 90%. By achieving a greater accuracy of 94.73%, the TLBO-LSTM model surpassed the other techniques.},
  archive      = {J_APIN},
  author       = {Swathi, T. and Kasiviswanath, N. and Rao, A. Ananda},
  doi          = {10.1007/s10489-022-03175-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13675-13688},
  shortjournal = {Appl. Intell.},
  title        = {An optimal deep learning-based LSTM for stock price prediction using twitter sentiment analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online education satisfaction assessment based on cloud
model and fuzzy TOPSIS. <em>APIN</em>, <em>52</em>(12), 13659–13674. (<a
href="https://doi.org/10.1007/s10489-022-03289-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19, colleges organized online education on a massive scale. To make better use of online education in the post-epidemic era, this paper conducts an online education satisfaction survey with four types of colleges and 129,325 students propose a fuzzy TOPSIS (technique for order preference by similarity to ideal solution) method based on the cloud model to rank the satisfaction of different colleges. Firstly, based on the characteristics of online education during the COVID-19, we build an evaluation indicator system from four dimensions: technology, instructor, learner and environment including, 10 indicators and 94 sub-indicators. Secondly, the cloud model is used to quantitatively describe the natural language and uncertainty in a large amount of assessment information. The cloud model generator is used for sub-indicators and achieves an effective and flexible conversion between linguistic information and quantitative values. The cloud model of indicators are presented by integrating the corresponding sub-indicators. The weights of indicators are determined by the entropy method based on the cloud model and possibility degree matrix, which eliminates the judgment of decision-makers and has great power for handling practical problems with unknown weight information. Finally, a fuzzy TOPSIS method based on the cloud model is proposed to rank the satisfaction of online education of different colleges. The proposed method is compared with other existing methods to shown its merits. The experimental result is consistent with the proportion of students who accept online education in the post-epidemic era. According to the second questionnaire, as the qualitative evaluation of the cloud model of indicators increases, the qualitative evaluation of satisfaction of different types of colleges will also increase. It indicates that the method proposed in this paper is practical.},
  archive      = {J_APIN},
  author       = {Xu, Xiuqin and Xie, Jialiang and Wang, Honghui and Lin, Mingwei},
  doi          = {10.1007/s10489-022-03289-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13659-13674},
  shortjournal = {Appl. Intell.},
  title        = {Online education satisfaction assessment based on cloud model and fuzzy TOPSIS},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new fuzzy network data envelopment analysis model for
measuring efficiency and effectiveness: Assessing the sustainability of
railways. <em>APIN</em>, <em>52</em>(12), 13634–13658. (<a
href="https://doi.org/10.1007/s10489-022-03336-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainability assessment of organizations in terms of technical efficiency (TE) and service effectiveness (SE) for non-storage goods has always been an interesting topic for managers and scholars. This paper develops a novel non-radial network data envelopment analysis (NDEA) model to assess the performance of railways in terms of overall efficiency (OE), TE, SE, and technical effectiveness (TEF) with sustainability considerations. Then, the fuzzy version of our proposed NDEA model is introduced. Our fuzzy NDEA model is based on the slacks-based measure (SBM) model. The developed model can deal with both qualitative and quantitative criteria and has important features, including unit invariant, monotone, projection, benchmarking, and factor efficiency index. A case study shows the applicability of the proposed model. The results reveal that the highest and the lowest average sustainability scores belong to TE and TEF, respectively. After identifying the weaknesses of railways, we discuss how to improve their sustainability.},
  archive      = {J_APIN},
  author       = {Tavassoli, Mohammad and Saen, Reza Farzipoor},
  doi          = {10.1007/s10489-022-03336-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13634-13658},
  shortjournal = {Appl. Intell.},
  title        = {A new fuzzy network data envelopment analysis model for measuring efficiency and effectiveness: Assessing the sustainability of railways},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cluster-based stratified hybrid decision support model
under uncertainty: Sustainable healthcare landfill location selection.
<em>APIN</em>, <em>52</em>(12), 13614–13633. (<a
href="https://doi.org/10.1007/s10489-022-03335-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, healthcare waste management has become one of the significant environmental, health, and social problems. Due to population and urbanization growth and an increase in healthcare waste disposals according to the growing number of diseases and pandemics like COVID-19, disposal of healthcare waste has become a critical issue. Authorities in big cities require reliable decision support systems to empower them to make strategic decisions to provide safe disposal methods with a prospective vision. Since inappropriate healthcare waste management systems would definitely bring up dangerous environmental, social, health, and economic issues for every city. Therefore, this paper attempts to address the landfill location selection problem for healthcare waste using a novel decision support system. Novel decision support model integrates K-means algorithms with Stratified Best-Worst Method (SBWM) and a novel hybrid MARCOS-CoCoSo under grey interval numbers. The proposed decision support system considers waste generate rate in medical centers, future unforeseen but potential events, and uncertainty in experts’ opinion to optimally locate required landfills for safe and economical disposal of dangerous healthcare waste. To investigate the feasibility and applicability of the proposed methodology, a real case study is performed for Mazandaran province in Iran. Our proposed methodology could efficiently deal with 79 medical centers within 4 clusters addressing 9 criteria to prioritize candidate locations. Moreover, the sensitivity analysis of weight coefficients is carried out to evaluate the results. Finally, the efficiency of the methodology is compared with several well-known methods and its high efficiency is demonstrated. Results recommend adherence to local rules and regulations, and future expansion potential as the top two criteria with importance values of 0.173 and 0.164, respectively. Later, best location alternatives are determined for each cluster of medical centers.},
  archive      = {J_APIN},
  author       = {Tirkolaee, Erfan Babaee and Torkayesh, Ali Ebadi},
  doi          = {10.1007/s10489-022-03335-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13614-13633},
  shortjournal = {Appl. Intell.},
  title        = {A cluster-based stratified hybrid decision support model under uncertainty: Sustainable healthcare landfill location selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection techniques in the context of big data:
Taxonomy and analysis. <em>APIN</em>, <em>52</em>(12), 13568–13613. (<a
href="https://doi.org/10.1007/s10489-021-03118-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Information Technology (IT) have engendered the rapid production of big data, as enormous volumes of data with high dimensional features grow exponentially in different fields. Therefore, dealing with high-dimensional data creates new challenges in terms of data processing efficiency and effectiveness. To address such challenges, Feature Selection (FS) is among the most utilized dimensionality reduction methods, which is helpful in reducing the high dimensionality of large-scale data by picking up a small subset of related and significant features and eliminating unrelated and redundant features in order to construct effective prediction models. This article provides a comprehensive review of the latest FS approaches in the context of big data along with a structured taxonomy, which categorizes the existing methods based on their nature, search strategy, evaluation process, and feature structure. Moreover, it presents a qualitative analysis of FS methods based on their objective, structure, search strategy, schema, learning task, strengths, and weaknesses. Further, a quantitative analysis is also performed to illustrate the number of publications related to FS based on the timeline, main category, and other sub-categories. An experimental study is also conducted comparing ten methods from different categories using twelve benchmark datasets from the University of California, Irvine (UCI) Machine Learning Repository and Arizona State University (ASU) Feature Selection Repository to evaluate their performance in terms of (accuracy, precision, recall, F-measures, and the number of selected features). Finally, we highlight the research issues and open challenges related to FS to assist researchers in identifying future research directions.},
  archive      = {J_APIN},
  author       = {Abdulwahab, Hudhaifa Mohammed and Ajitha, S. and Saif, Mufeed Ahmed Naji},
  doi          = {10.1007/s10489-021-03118-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13568-13613},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection techniques in the context of big data: Taxonomy and analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel integrated large-scale group MCDM model under fuzzy
environment for selection of reach stacker in a container terminal.
<em>APIN</em>, <em>52</em>(12), 13543–13567. (<a
href="https://doi.org/10.1007/s10489-021-02914-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of transshipment and handling machinery in container terminals is a complex and responsible task due to a number of daily operations required. Accordingly, there is a need to manage a circular economy that integrates economic parameters and attitudes toward the environment. Depending on the size of the container terminal itself, a necessary set of means for performing transshipment and handling operations is designed. In this paper, based on the previously identified needs of the IRT Belgrade container terminal, the evaluation and selection of a reach stacker within large-scale group decision making under fuzzy environment was performed. The main goal of the paper is to create an adequate fuzzy group multi-criteria decision making (MCDM) model based on the integration of Fuzzy FUCOM (Full Consistency Method), Fuzzy MARCOS (Measurement of alternatives and ranking according to COmpromise solution) and Fuzzy Bonferroni Mean (BM) operator. It was formed a total of 15 criteria divided into three basic groups: economic, technological and technical, which were evaluated on the basis of 18 experts. To determine the weight values ​​of the criteria, the Fuzzy FUCOM method was applied through a total of 72 models averaged using the Fuzzy BM operator. Evaluation and selection of a reach stacker (RS) was performed using the Fuzzy MARCOS method and the Fuzzy BM operator. The obtained results have shown that the most important group of criteria in group decision making and processing of a larger set of data is the technological group. The best option is the seventh variant, and thus the requirement to select RS for the container terminal is met. The verification of the obtained results was performed through the following phases: the influence of the reverse rank fuzzy matrix, simulation of the weight values ​​of the criteria through 50 formed scenarios and comparison with two other MCDM methods in a fuzzy form.},
  archive      = {J_APIN},
  author       = {Vesković, Slavko and Stević, Željko and Nunić, Zdravko and Milinković, Sanjin and Mladenović, Dušan},
  doi          = {10.1007/s10489-021-02914-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13543-13567},
  shortjournal = {Appl. Intell.},
  title        = {A novel integrated large-scale group MCDM model under fuzzy environment for selection of reach stacker in a container terminal},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A blast furnace coke ratio prediction model based on fuzzy
cluster and grid search optimized support vector regression.
<em>APIN</em>, <em>52</em>(12), 13533–13542. (<a
href="https://doi.org/10.1007/s10489-022-03234-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of blast furnace coke ratio, existing methods can only predict coke ratio of daily. At the same time, the data under abnormal furnace conditions are excluded, and the model’s robustness needs to be improved. In order to improve the prediction accuracy and time precision of the blast furnace mathematical simulation model, a blast furnace coke ratio prediction model based on fuzzy C-means (FCM) clustering and grid search optimization support vector regression (SVR) is proposed to achieve accurate prediction of coke ratio. First, preprocess the blast furnace sensor data and steel plant production data. Then, the FCM algorithm is used to cluster the data under different furnace conditions. Finally, the SVR model optimized by grid search is used to predict the coke ratio under different blast furnace conditions. The average absolute error of the improved model is 1.7721 kg/t, the hit rate within 0.5% error is 81.19%, the coefficient of determination R2 is 0.9474, and the prediction performance is better than ridge regression and decision tree regression. Experiments show that the model can predict the coke ratio of molten iron in each batch when the blast furnace conditions are going forward and fluctuating, and it has high time accuracy and stability. It objectively describes the changing trend of blast furnace conditions, and provides new research ideas for the practical application of blast furnace mathematical models.},
  archive      = {J_APIN},
  author       = {Li, Shuai and Chang, Jincai and Chu, Mansheng and Li, Jie and Yang, Aimin},
  doi          = {10.1007/s10489-022-03234-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13533-13542},
  shortjournal = {Appl. Intell.},
  title        = {A blast furnace coke ratio prediction model based on fuzzy cluster and grid search optimized support vector regression},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Location algorithm of transfer stations based on density
peak and outlier detection. <em>APIN</em>, <em>52</em>(12), 13520–13532.
(<a href="https://doi.org/10.1007/s10489-022-03206-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the express industry, few people pay attention to the rationality of the layout of the transfer station at the end of rural express outlets. Aiming at the difficulty in selecting the location of express terminal transfer stations in rural areas, this paper presents a new transfer station location algorithm. This method is based on the reverse nearest neighbor algorithm and local density characteristics to initially screen the location of the transfer station in the area, and then determine its location and coverage area based on the density from the appropriate distance. Finally, by calculating the outlier index of each station in the distribution area, the boundary of the distribution area is redefined. Experiments show that the express transfer stations selected by this method can meet the service requirements of 98.8% of the area and 99.5% of the population in Yutian County, and the division accuracy of the distribution range has been improved to a certain extent compared with the traditional algorithm. In particular, for the selection of small-range peak points, the accuracy of the traditional density peak algorithm has been significantly improved.},
  archive      = {J_APIN},
  author       = {Shao-hong, Yan and Jia-yang, Niu and Tai-long, Chen and Qiu-tong, Liu and Cen, Yang and Jia-qing, Cheng and Zhi-zhen, Fu and Jie, Li},
  doi          = {10.1007/s10489-022-03206-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13520-13532},
  shortjournal = {Appl. Intell.},
  title        = {Location algorithm of transfer stations based on density peak and outlier detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud vendor selection for the healthcare industry using a
big data-driven decision model with probabilistic linguistic
information. <em>APIN</em>, <em>52</em>(12), 13497–13519. (<a
href="https://doi.org/10.1007/s10489-021-02913-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of cloud services in the data-intensive industry is indispensable. Cision recently reported that the cloud market would grow to 55 billion USD, with an active contribution of the cloud to healthcare around 2025. Inspired by the report, cloud vendors expand their market and the quality of services to seek growth globally. The rapid growth of the cloud sector in the healthcare industry imposes a challenge: making a rational choice of a cloud vendor (CV) out of a diverse set of vendors. Typically, the healthcare industry 4.0 sees the issue as a large-scale group decision-making problem. Previous studies on a CV selection face certain challenges, such as (i) a lack of the ability to handle multiple users’ views, as well as experts’/users’ complex linguistic views; (ii) the confidence level associated with a view is not considered; (iii) the transformation of multiple users’ views into holistic data is lacking; and (iv) the systematic prioritization of CVs with minimum human intervention is a crucial task. Motivated by these challenges and circumventing them, a new big data-driven decision model is put forward in this paper. Initially, the data in the form of complex expressions are collected from multiple cloud users and are further transformed into a holistic decision matrix by adopting probabilistic linguistic information (PLI). PLI represents complex linguistic expressions along with the associated confidence levels. Later, a holistic decision matrix is formed with the missing values imputed by proposing an imputation algorithm. Furthermore, the criteria weights are determined by using a newly proposed mathematical model and partial information. Finally, the evaluation based on the distance from average solution (EDAS) approach is extended to PLI for the rational ranking of CVs. A real-time example of a CV selection for a healthcare center in India is exemplified so as to demonstrate the usefulness of the model, and the comparison reveals the merits and limitations of the model.},
  archive      = {J_APIN},
  author       = {Krishankumar, R. and Sivagami, R. and Saha, Abhijit and Rani, Pratibha and Arun, Karthik and Ravichandran, K. S.},
  doi          = {10.1007/s10489-021-02913-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13497-13519},
  shortjournal = {Appl. Intell.},
  title        = {Cloud vendor selection for the healthcare industry using a big data-driven decision model with probabilistic linguistic information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A closed-loop supply chain configuration considering
environmental impacts: A self-adaptive NSGA-II algorithm. <em>APIN</em>,
<em>52</em>(12), 13478–13496. (<a
href="https://doi.org/10.1007/s10489-021-02944-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Configuration of a supply chain network is a critical issue that contributes to choose the best combination for a set of facilities in order to attain an effective and efficient supply chain management (SCM). Designing a closed-loop distribution network of products is an important field in supply chain network design, which offers a potential factor for reducing costs and improving service quality. In this research, the question concerns a closed-loop supply chain (CLSC) network design considering suppliers, assembly centers, retailers, customers, collection centers, refurbishing centers, disassembly centers and disposal centers. It aims to design a distribution network based on customers’ needs in order to simultaneously minimize the total cost and total CO2 emission. To tackle the complexity of the problem, a self-adaptive non-dominated sorting genetic algorithm II (NSGA-II) algorithm is designed, which is then evaluated against the ε-constraint method. Furthermore, the performance of the algorithm is then enhanced using the Taguchi design method to tune its parameters. The results indicate that the solution time of the self-adaptive NSGA-II approach performs better than the epsilon constraint method. In terms of the self-adaptive NSGA-II algorithm, the average number of Pareto solutions (NPS) for small and medium-sized problems is 6.2 and 11, respectively. The average mean ideal distance (MID) for small and medium-sized problems is 2.54 and 5.01, respectively. Finally, the average maximum spread (MS) for small and medium-sized problems is 3100.19 and 3692.446, respectively. The findings demonstrate that the proposed self-adaptive NSGA-II is capable of generating efficient Pareto solutions. Moreover, according to the results obtained from sensitivity analysis, it is revealed that with increasing the capacity of distribution centers, the amount of shortage of products decreases. Moreover, as the demand increases, the number of established retailers rises. The number of retailers is increasing to some extent to establish 7 retailers.},
  archive      = {J_APIN},
  author       = {Babaeinesami, Abdollah and Tohidi, Hamid and Ghasemi, Peiman and Goodarzian, Fariba and Tirkolaee, Erfan Babaee},
  doi          = {10.1007/s10489-021-02944-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13478-13496},
  shortjournal = {Appl. Intell.},
  title        = {A closed-loop supply chain configuration considering environmental impacts: A self-adaptive NSGA-II algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A DES-based group decision model for group decision making
with large-scale alternatives. <em>APIN</em>, <em>52</em>(12),
13456–13477. (<a
href="https://doi.org/10.1007/s10489-021-02950-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve group decision making problems with large-scale alternatives, this paper proposes a dynamic ensemble selection (DES) based group decision model by using historical decision data. The historical decision data of a group of experts are collected from the same multi-criteria decision framework and are mixed to train a set of base classifiers (BCs) to learn group preferences. For each new alternative, the predictions derived from BCs are used to determine its similar historical alternatives from historical data, and the BC with the highest accuracy in predicting the similar historical alternatives is identified as the best individual BC for the new alternative. By iteratively comparing the accuracy of an ensemble of randomly selected BCs and the best individual BC in predicting the similar historical alternatives of the new alternative, a novel DES method is developed to select a competent subset of BCs for the new alternative. The developed DES method effectively avoids the error-independence assumption to a certain extent. Based on the similar historical alternatives determined by the ensemble of selected BCs, a group decision optimization model is developed to learn criterion weights from their assessments on criteria and ensemble predictions derived from the selected BCs. With the learned criterion weights, the understandable group decision result is generated for the new alternative. Case study validates the superiority of the proposed model in diagnosing thyroid nodules using group capabilities. Empirical comparisons on thirty real datasets examine the competence of the proposed DES method against five representative DES methods.},
  archive      = {J_APIN},
  author       = {Xu, Che and Liu, Weiyong and Chen, Yushu},
  doi          = {10.1007/s10489-021-02950-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13456-13477},
  shortjournal = {Appl. Intell.},
  title        = {A DES-based group decision model for group decision making with large-scale alternatives},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust fuzzy multi-objective location-routing problem for
hazardous waste under uncertain conditions. <em>APIN</em>,
<em>52</em>(12), 13435–13455. (<a
href="https://doi.org/10.1007/s10489-022-03334-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrialization and population growth have been accompanied by many problems such as waste management worldwide. Waste management and reduction have a vital role in national management. The presents study represents a multi-objective location-routing problem for hazardous wastes. The model was solved using Non dominated Sorting Genetic Algorithm-II, Multi-Objective Particle Swarm Optimization, Multi-Objective Invasive Weed Optimization, Pareto Envelope-based Selection Algorithm, Multi-Objective Evolutionary Algorithm Based on Decomposition and Multi-Objective Grey Wolf Optimizer algorithms. The findings revealed that the Multi-Objective Invasive Weed Optimization algorithm was the best and the most efficient among the algorithms used in this study. Obtaining income from the incineration of the wastes and reducing the risk of COVID-19 infection are the first innovation of the present study, which considered in the presented model. The second innovation is that uncertainty was considered for some of the crucial parameters of the model while the robust fuzzy optimization model was applied. Besides, the model was solved using several meta-heuristic algorithms such as Multi-Objective Invasive Weed Optimization, Multi-Objective Evolutionary Algorithm Based on Decomposition and Multi-Objective Grey Wolf Optimizer, which were rarely used in literature. Eventually, the most efficient algorithm was identified by comparing the considered algorithms.},
  archive      = {J_APIN},
  author       = {Raeisi, Diba and Jafarzadeh Ghoushchi, Saeid},
  doi          = {10.1007/s10489-022-03334-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13435-13455},
  shortjournal = {Appl. Intell.},
  title        = {A robust fuzzy multi-objective location-routing problem for hazardous waste under uncertain conditions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Query-oriented topical influential users detection for top-k
trending topics in twitter. <em>APIN</em>, <em>52</em>(12), 13415–13434.
(<a href="https://doi.org/10.1007/s10489-022-03582-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Social Networks (OSNs) have become inevitable for any new methodology both for viral promoting applications and instructing the creation of inciting information and data. As a result, finding influential users in OSNs is one of the most studied research problems. Existing research works paid less attention to the temporal factors associated with the activities performed by the social users. Our motivation is to find influential users who show their most powerful interests towards a given query on various subjects (topics) at different time intervals by featuring more on users’ most recent activities as well as their associations with different users. To address this problem, we propose a temporal activity-biased weight model that gives higher weight to users’ recent activities and develops an algorithm to list the most effective influential users. In addition, our proposed model also considers the impacts of topical similarities both from direct and indirect neighbors of the users. Experimental results on two real datasets demonstrate that our proposed framework yields better outcomes than the baseline method.},
  archive      = {J_APIN},
  author       = {Gomasta, Sarmistha Sarna and Dhali, Aditi and Anwar, Md Musfique and Sarker, Iqbal H.},
  doi          = {10.1007/s10489-022-03582-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13415-13434},
  shortjournal = {Appl. Intell.},
  title        = {Query-oriented topical influential users detection for top-k trending topics in twitter},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental deep forest for multi-label data streams
learning. <em>APIN</em>, <em>52</em>(12), 13398–13414. (<a
href="https://doi.org/10.1007/s10489-022-03414-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning has attracted many attentions. However, the continuous data generated in the fields of sensors, network access, etc., that is data streams, the scenario brings challenges such as real-time, limited memory, once pass. Several learning algorithms have been proposed for offline multi-label classification, but few researches develop it for dynamic multi-label incremental learning models based on cascading schemes. Deep forest can perform representation learning layer by layer, and does not rely on backpropagation, using this cascading scheme, this paper proposes a multi-label data stream deep forest (VDSDF) learning algorithm based on cascaded Very Fast Decision Tree (VFDT) forest, which can receive examples successively, perform incremental learning, and adapt to concept drift. Experimental results show that the proposed VDSDF algorithm, as an incremental classification algorithm, is more competitive than batch classification algorithms on multiple indicators. Moreover, in dynamic flow scenarios, the adaptability of VDSDF to concept drift is better than that of the contrast algorithm.},
  archive      = {J_APIN},
  author       = {Liang, Shunpan and Pan, Weiwei and You, Dianlong and Liu, Ze and Yin, Ling},
  doi          = {10.1007/s10489-022-03414-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13398-13414},
  shortjournal = {Appl. Intell.},
  title        = {Incremental deep forest for multi-label data streams learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-way selection random forest algorithm based on
decision boundary entropy. <em>APIN</em>, <em>52</em>(12), 13384–13397.
(<a href="https://doi.org/10.1007/s10489-021-03033-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of high probability of negative impact about redundant attributes in random forest algorithms, a Three-way Selection Random Forest algorithm based on decision boundary entropy (TSRF) is proposed without losing randomness and reducing the influence of redundant attributes on decision-making results. According to the characteristics of the attribute, the concept of decision boundary entropy is defined. Then a measuring method of attribute importance based on decision boundary entropy is proposed and set as an evaluation standard. Three-way decision is constructed and the attribute is divided into three candidate domains, namely positive domain, negative domain and boundary domain. In order to ensure the randomness of attributes, three-way attribute random selection rules based on attribute randomness are established and a certain number of attributes are randomly selected from the three candidate domains. Combine the samples selected by the bootstrap sampling method with attribute sets selected by three-way decision to produce training sample sets so that we can train the decision trees and generate forest. Six datasets are selected for the experiment. Two parameters of attribute randomness and three-way decision thresholds are analyzed to verify the theoretical conclusions respectively. The results show that the TSRF algorithm can meet the different requirements of different data sets by adjusting the parameters. The classification effect on the binary data is basically the same as the comparison algorithm, but TSRF has a significant improvement effect on the multi-class data compared with other algorithms. The proposed TSRF algorithm widens the idea for the measurement method of significance of attribute, innovates the random forest three-way selection integration method, and provides a better model framework for solving multi-classification problems.},
  archive      = {J_APIN},
  author       = {Zhang, Chunying and Ren, Jing and Liu, Fengchun and Li, Xiaoqi and Liu, Shouyue},
  doi          = {10.1007/s10489-021-03033-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13384-13397},
  shortjournal = {Appl. Intell.},
  title        = {Three-way selection random forest algorithm based on decision boundary entropy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Catalysis of neural activation functions: Adaptive
feed-forward training for big data applications. <em>APIN</em>,
<em>52</em>(12), 13364–13383. (<a
href="https://doi.org/10.1007/s10489-021-03082-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning in the field of Big Data has become essential for the analysis and perception of trends. Activation functions play a crucial role in the outcome of these deep learning frameworks. The existing activation functions are hugely focused on data translation from one neural layer to another. Although they have been proven useful and have given consistent results, they are static and mostly non-parametric. In this paper, we propose a new function for modified training of neural networks that is more flexible and adaptable to the data. The proposed catalysis function works over Rectified Linear Unit (ReLU), sigmoid, tanh and all other activation functions to provide adaptive feed-forward training. The function uses vector components of the activation function to provide variational flow of input. The performance of this algorithm is tested on Modified National Institute of Standards and Technology (MNIST) and Canadian Institute for Advanced Research (CIFAR10) datasets against the conventional activation functions. Visual Geometry Group (VGG) blocks and Residual Neural Network (ResNet) architectures are used for experimentation. The proposed function has shown significant improvements in comparison to the traditional functions with a 75 ± 2.5% acuuracy across activation functions. The adaptive nature of training has drastically decreased the probability of under-fitting. The parameterization has helped increase the data learning capacity of models. On performing sensitivity analysis, the catalysis activation show slight or no changes on varying initialization parameters.},
  archive      = {J_APIN},
  author       = {Sarkar, Sagnik and Agrawal, Shaashwat and Baker, Thar and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy},
  doi          = {10.1007/s10489-021-03082-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13364-13383},
  shortjournal = {Appl. Intell.},
  title        = {Catalysis of neural activation functions: Adaptive feed-forward training for big data applications},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework to evaluate the barriers for adopting the
internet of medical things using the extended generalized TODIM method
under the hesitant fuzzy environment. <em>APIN</em>, <em>52</em>(12),
13345–13363. (<a
href="https://doi.org/10.1007/s10489-021-03078-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The e-health Internet of Medical Things (IoMT) has succeeded in providing valuable wellness services that aid users in achieving lifestyles of higher quality. On the other hand, the adoption of these healthcare applications associates with some challenges and barriers that need to be further addressed by the academic community to be well managed. The current paper aims to build an inclusive framework for assessing the most important barriers when implementing the IoMT in the health system. To this end, a survey was conducted, literature was reviewed comprehensively, and experts were interviewed to identify the adoption barriers of IoMT. In total, 20 barriers were identified using the literature review and classified based on five main categories with the help of the experts. Specifically, this study developed an extended hesitant fuzzy generalized TODIM method to find the optimum solution to general MCGDM problems. Some novel operational laws and distance measures based on the least common multiple principle are employed in this course. The proposed framework comprises both qualitative and quantitative criteria including benefit, cost, or target. According to the obtained results, the most important barriers to the IoMT adoption are regulatory affairs, vendor lock-in, liability, trust management system, installation, etc. Additionally, the proposed method was found capable of efficiently and effectively analyzing the IoMT adoption barriers in the health care context.},
  archive      = {J_APIN},
  author       = {Alattas, Khalid and Wu, Qun},
  doi          = {10.1007/s10489-021-03078-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13345-13363},
  shortjournal = {Appl. Intell.},
  title        = {A framework to evaluate the barriers for adopting the internet of medical things using the extended generalized TODIM method under the hesitant fuzzy environment},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Big data-driven large-scale group decision-making under
uncertainty (BiGDM-u). <em>APIN</em>, <em>52</em>(12), 13341–13344. (<a
href="https://doi.org/10.1007/s10489-022-04113-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Mardani, Abbas and Zavadskas, Edmundas Kazimieras and Fujita, Hamido and Köppen, Mario},
  doi          = {10.1007/s10489-022-04113-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {12},
  pages        = {13341-13344},
  shortjournal = {Appl. Intell.},
  title        = {Big data-driven large-scale group decision-making under uncertainty (BiGDM-u)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Rethinking the framework constructed by counterfactual
functional model. <em>APIN</em>, <em>52</em>(11), 12957–12974. (<a
href="https://doi.org/10.1007/s10489-022-03161-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The causal inference represented by counterfactual inference technology breathes new life into the current field of artificial intelligence. Although the fusion of causal inference and artificial intelligence has an excellent performance in many various applications, some theoretical justifications have not been well resolved. In this paper, we focus on two fundamental issues in causal inference: probabilistic evaluation of counterfactual queries and the assumptions used to evaluate causal effects. Both of these issues are closely related to counterfactual inference tasks. Among them, counterfactual queries focus on the outcome of the inference task, and the assumptions provide the preconditions for performing the inference task. Counterfactual queries are to consider the question of what kind of causality would arise if we artificially apply the conditions contrary to the facts. In general, to obtain a unique solution, the evaluation of counterfactual queries requires the assistance of a functional model. We analyze the limitations of the original functional model when evaluating a specific query and find that the model arrives at ambiguous conclusions when the unique probability solution is 0. In the task of estimating causal effects, the experiments are conducted under some strong assumptions, such as treatment-unit additivity. However, such assumptions are often insatiable in real-world tasks, and there is also a lack of scientific representation of the assumptions themselves. We propose a mild version of the treatment-unit additivity assumption coined as M-TUA based on the damped vibration equation in physics to alleviate this problem. M-TUA reduces the strength of the constraints in the original assumptions with reasonable formal expression.},
  archive      = {J_APIN},
  author       = {Wang, Chao and Liu, Linfang and Sun, Shichao and Wang, Wei},
  doi          = {10.1007/s10489-022-03161-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12957-12974},
  shortjournal = {Appl. Intell.},
  title        = {Rethinking the framework constructed by counterfactual functional model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated assessment of BI-RADS categories for ultrasound
images using multi-scale neural networks with an order-constrained loss
function. <em>APIN</em>, <em>52</em>(11), 12943–12956. (<a
href="https://doi.org/10.1007/s10489-021-03140-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging is one of the most frequently used diagnostic tools for detecting and analyzing abnormalities of the breast. Recently proposed methods for the automated analysis of breast ultrasound images have shown great success, especially in the classification of breast abnormalities into malignant or benign lesions. In this study, we explore the use of a deep convolutional neural network with a multi-scale module for the automated assessment of BI-RADS category on breast ultrasound images. We propose a multi-input region of interest extraction method to extract the breast region from ultrasound images, with this method avoiding the deformation of region of interest images and providing high classification performance. Moreover, we also propose an order-constrained loss function that fully considers the continuity between BI-RADS categories and shows higher performance than traditional loss functions. A large annotated dataset containing 8246 breast ultrasound images was collected to train and evaluate the proposed methods. Ablation experiments were performed to validate the effectiveness of the proposed methods. Experimental results indicate that our method can be used to mimic experienced radiologists in the assessment of the BI-RADS category of breast ultrasound images and that the automated interpretations could be acceptable in routine clinical breast ultrasound examination reports.},
  archive      = {J_APIN},
  author       = {Pi, Yong and Li, Qian and Qi, Xiaofeng and Deng, Dan and Yi, Zhang},
  doi          = {10.1007/s10489-021-03140-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12943-12956},
  shortjournal = {Appl. Intell.},
  title        = {Automated assessment of BI-RADS categories for ultrasound images using multi-scale neural networks with an order-constrained loss function},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel method for optimizing performance in resource
constrained distributed data streams. <em>APIN</em>, <em>52</em>(11),
12924–12942. (<a
href="https://doi.org/10.1007/s10489-021-03019-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Big Data Era has presented many opportunities for using data mining techniques to discover knowledge patterns across large and diverse collections of data where the volume of data is growing at an exponential rate. Recent approaches to Distributed Data Mining (DDM) have focused on addressing the heterogeneous nature of data sources. However, such approaches do not prioritize the reduction of data communication costs which could be prohibitive in large scale sensor networks where bandwidth is a limited resource. In fact, higher communication and computational costs are the two most prominent problems that have been encountered in heterogeneous distributed environments. Moreover, an effort to decrease the communications load in the distributed environment has an adverse influence on the classification accuracy. Therefore, the research challenge lies in maintaining a balance between transmission cost, computational cost, and accuracy. This paper proposes an algorithm Performance Optimizer in Distributed Stream Mining (PODSM) based on Bayesian Inference to reduce the communication volume and resource time in a heterogeneous distributed data mining environment while retaining prediction accuracy. The approach used in this work exploits the past data for calculating statistics and these statistics are then utilized for the new data. In other words, it imparts the ability to learn from experiences. As a result, our experimental evaluation reveals that a significant reduction in the communication load and an improvement in classification response time can be achieved across a diverse range of dataset types. Reduction of 34.66% was obtained with regard to communication overhead for one of the datasets with huge savings of nearly 27% in resource time. Importantly, instead of showing a negative effect on accuracy, this dataset observes an increment of 0.44% in accuracy.},
  archive      = {J_APIN},
  author       = {Bhalla, Rashi and Pears, Russel and Naeem, M. Asif and Mirza, Farhaan},
  doi          = {10.1007/s10489-021-03019-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12924-12942},
  shortjournal = {Appl. Intell.},
  title        = {Novel method for optimizing performance in resource constrained distributed data streams},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-adaptive DE algorithm without niching parameters for
multi-modal optimization problems. <em>APIN</em>, <em>52</em>(11),
12888–12923. (<a
href="https://doi.org/10.1007/s10489-021-03003-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve multi-modal optimization problems, the niching technique is widely used because it could find and preserve multiple stable sub-populations. However, the performances of most existing evolutionary algorithms with niching techniques heavily depend on niching parameters, such as niche radius, sub-population size and crowding factor. To our best knowledge, a self-adaptive differential evolution (DE) variant without niching parameters using ring topology has not been developed. In this paper, we proposed a Self-adaptive Niching Differential Evolution (SaNDE) algorithm. The ring topology plays a crucial role in slowing the information flow, resulting in scattered niches with restricted and overlapped communications. We introduced local memory (personal best) into the DE algorithm to present a new mutation operator “current-to-pnbest” when a ring population topology was used. Moreover, the two control parameters in DE were self-adapted by using a simple but effective strategy that is based on successful parametric values in history. To improve the capability of jumping out of local optima, an adaptive re-start mechanism by using opposition-based learning was proposed to address the issue of stagnation. The performances of the proposed method were investigated through standard benchmark functions and the problem of optimizing parameters for a feedforward neural network. Comparisons with other state-of-the-art multi-modal optimization algorithms demonstrated the competitiveness of the proposed methodology.},
  archive      = {J_APIN},
  author       = {Jiang, Ruizheng and Zhang, Jundong and Tang, Yuanyuan and Feng, Jinhong and Wang, Chuan},
  doi          = {10.1007/s10489-021-03003-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12888-12923},
  shortjournal = {Appl. Intell.},
  title        = {Self-adaptive DE algorithm without niching parameters for multi-modal optimization problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Investigating the multi-objective optimization of quality
and efficiency using deep reinforcement learning. <em>APIN</em>,
<em>52</em>(11), 12873–12887. (<a
href="https://doi.org/10.1007/s10489-022-03326-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a pipeline, which is based on deep reinforcement learning, aims to solve the multi-objective problem (MOP) on efficiency and quality in manufacturing. The rapid development in the area of artificial intelligent has caused a series of reactions that stirred the traditional manufacturing, pushing for the better machining quality and higher productivity. Despite all this, there has been very little research applying reinforcement learning to solve practical problems in milling process. The proposed pipeline is a two-step algorithm and makes full use of double deep Q network (DDQN) to settle the MOP of milling parameters. Firstly, surface roughness (Ra) and material removal rate (MRR) are selected as quality and efficiency indicators, respectively. In specific, the reliable prediction model of Ra is constructed on a small batch raw data via DDQN improved support vector regression (DDQN-SVR) rather than sophisticated and complex physical modeling. The MRR model is constructed by an accepted empirical formula. Then, DDQN is employed again to solve the MOP of satisfying minimum Ra and maximum MRR and compared to other accepted algorithms. Eventually, the optimal combination of machining parameters determined by entropy method was validated by experiment.},
  archive      = {J_APIN},
  author       = {Wang, Zhenhui and Lu, Juan and Chen, Chaoyi and Ma, Junyan and Liao, Xiaoping},
  doi          = {10.1007/s10489-022-03326-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12873-12887},
  shortjournal = {Appl. Intell.},
  title        = {Investigating the multi-objective optimization of quality and efficiency using deep reinforcement learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient channel expansion and pyramid
depthwise-pointwise-depthwise neural networks. <em>APIN</em>,
<em>52</em>(11), 12860–12872. (<a
href="https://doi.org/10.1007/s10489-021-03152-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In popular lightweight convolutional neural networks (CNNs), pointwise convolution (PWC) layers for combining information occupy approximately 70% weights and computation, but depthwise convolution (DWC) layers for extracting spatial information only occupy less than 2% weights and computation. The weights and computation for extracting spatial information are not enough in lightweight CNNs. In this paper, we proposed expanding the number of channels and improving the extraction of spatial information by more efficient DWC instead of PWC. Firstly, the results of the proposed PSDNet demonstrate that DWC is more efficient than PWC for channel expansion and it can improve the accuracy of the network. Then, the efficient Depthwise-Pointwise-Depthwise (DPD) block is proposed by using DWC to expand channels. Different from the general bottleneck block, the DPD block consists of one PWC layer and two DWC layers. Four kinds of efficient lightweight DPDNets (DPDNet-G, DPDNet-A, DPDNet-C, DPDNet-D) are proposed by stacking different DPD blocks. To extract multi-scale features and achieve high accuracy, the pyramid DWC layer is used when channel expansion in DPDNet. Compared with common lightweight CNNs, DPDNets use more weights and computation in the DWC layer for extracting spatial information. Four competitive benchmark datasets (CIFAR-10, CIFAR-100, ImageNet, and PASCAL VOC) were used to verify the superiority of DPDNet. Experiments demonstrate that the proposed DPDNet has higher accuracy than MobileNet with a similar number of weights and computations. Furthermore, compared DPDNet with MobileNet, it can be found that improving the ratio of DWC to PWC can improve accuracy, which helps researchers to design better lightweight CNNs.},
  archive      = {J_APIN},
  author       = {Li, Guoqing and Zhang, Meng and Zhang, Yu and Wu, Ruixia and Weng, Dongpeng},
  doi          = {10.1007/s10489-021-03152-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12860-12872},
  shortjournal = {Appl. Intell.},
  title        = {Efficient channel expansion and pyramid depthwise-pointwise-depthwise neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). ARFP: A novel adaptive recursive feature pyramid for object
detection in aerial images. <em>APIN</em>, <em>52</em>(11), 12844–12859.
(<a href="https://doi.org/10.1007/s10489-021-03147-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aerial image is one of the most important application fields of object detection. However, the characteristics of scale variation in aerial images bring challenges to object detection. Feature pyramid network(FPN) has been proved to be a promising method for solving scale variation. However, the conventional FPN merely transfers the solid semantic information to the bottom layers through the upsampling method. Moreover, there exists no gated mechanism to distinguish effective signals. In this paper, we proposed the adaptive recursive feature pyramid(ARFP), which consists of three subparts: recursive structure, efficient global context(EGC) bottleneck module, and discriminative feature fusion(DFF) module. The DFF module makes all pyramid levels can completely leverage the firm semantic information and detailed location information through the dense connection. Besides, it adaptively learns the weights for each pyramid level to fuse and gate the signals discriminatively. The EGC module is responsible for building the pixel-wise position relevance and channel relevance, which is influential for aerial image detection. Moreover, we empirically explore the possibility of building a feature pyramid recursively. Extensive experiments on the DIOR and VisDrone2019 datasets have shown that ARFP outperforms the current state-of-art feature pyramid networks, and the average performance gain is 2.7% and 2.8%, respectively.},
  archive      = {J_APIN},
  author       = {Wang, Junjie and Yu, Jiong and He, Zhu},
  doi          = {10.1007/s10489-021-03147-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12844-12859},
  shortjournal = {Appl. Intell.},
  title        = {ARFP: A novel adaptive recursive feature pyramid for object detection in aerial images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-updating continual learning classification method based
on artificial immune system. <em>APIN</em>, <em>52</em>(11),
12817–12843. (<a
href="https://doi.org/10.1007/s10489-021-03123-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, major classification methods belong to batch learning methods, which need to obtain all data once before learning. However, in practice, it is usually difficult to handle all samples at once for samples are obtained gradually at different periods. Unfortunately, research on continual learning study is deficiency and remains to be improved. In this work, a self-updating continual learning classification method (SU-CLCM) is proposed, according to the sophisticated continual learning mechanism of biological immune system. In SU-CLCM, the self-updating cell division strategy overcomes the irrational cell strategies; SU-CLCM takes the advantage of super memory cells and totipotent stem cells with self-updating cell weight so that different types of cell edge are more distinguishable; SU-CLCM can improve the result of high-dimensional data processing. Experiments demonstrated on 20 UCI repository datasets compared with intelligent classification methods and artificial immune methods with continual learning ability to corroborate the highlights of SU-CLCM. Ultimately, a dataset of actual compressor valve fault is employed to verify the effectiveness and superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Sun, Xin and Wang, Haotian and Liu, Shulin and Li, Dong and Xiao, Haihua},
  doi          = {10.1007/s10489-021-03123-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12817-12843},
  shortjournal = {Appl. Intell.},
  title        = {Self-updating continual learning classification method based on artificial immune system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural network with RNNs based trajectory prediction
of dynamic agents for autonomous vehicle. <em>APIN</em>,
<em>52</em>(11), 12801–12816. (<a
href="https://doi.org/10.1007/s10489-021-03120-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction is an essential ability for the intelligent transportation system to navigate through complex traffic scenes. In recent times, trajectory prediction has become an important task, especially in crowded scenes, because of the great demands of emerging artificial intelligence applications like service bots and autonomous cars. As autonomous vehicles travel in interactive and highly uncertain environments shared with other dynamic road agents like other vehicles or pedestrians, predicting the trajectories of the surrounding agents is essential for an autonomous driving system (ADS) to plan safe motion, fast reaction time and comfortable maneuvers. The trajectory for each dynamic object (or road agent) is described as a sequence of states within a time interval, with each state representing the object’s spatial coordinates under the world coordinate frame. In the trajectory prediction (TP) problem, given the trajectory of each object between intervals of time, we predict their trajectories between these intervals of time. We plan to design a Multi-Scale Graph Neural Network (GNN) with temporal features architecture for this prediction problem. Experiments show that our model effectively captures comprehensive Spatio-temporal correlations through modeling GNN with temporal features for TP and consistently surpasses the existing state-of-the-art methods on three real-world datasets for trajectory. Compared to prior methods, our model’s performance is more for the sparse datasets than for the dense datasets.},
  archive      = {J_APIN},
  author       = {Singh, Divya and Srivastava, Rajeev},
  doi          = {10.1007/s10489-021-03120-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12801-12816},
  shortjournal = {Appl. Intell.},
  title        = {Graph neural network with RNNs based trajectory prediction of dynamic agents for autonomous vehicle},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Broadcast news story segmentation using sticky hierarchical
dirichlet process. <em>APIN</em>, <em>52</em>(11), 12788–12800. (<a
href="https://doi.org/10.1007/s10489-021-03098-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov model (HMM) is a popular technique for story segmentation, where hidden Markov states represent the topics. The number of hidden states has to set manually, however, this number is often unknown. This paper proposed a nonparametric approach, called SHDP-HMM, to address this problem. By defining an HDP prior distribution on transition matrices over countably infinite state spaces, SHDP-HMM can infer the number of hidden states from the data automatically. Besides, to better model the duration of topics, we utilize a parameter for self-transition bias that reduces the transition probabilities among redundant hidden states. Given a stream of text, a Gibbs sampler labels the hidden states with topic classes. The position where the topic shifts indicates a story boundary. Experiments show that the proposed SHDP-HMM approach outperforms the traditional HMM-based approaches, and the number of hidden states can be automatically inferred from data.},
  archive      = {J_APIN},
  author       = {Yu, Jia and Shao, Hongxiang},
  doi          = {10.1007/s10489-021-03098-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12788-12800},
  shortjournal = {Appl. Intell.},
  title        = {Broadcast news story segmentation using sticky hierarchical dirichlet process},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Hybrid handcrafted and learned feature framework for human
action recognition. <em>APIN</em>, <em>52</em>(11), 12771–12787. (<a
href="https://doi.org/10.1007/s10489-021-03068-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognising human actions in video is a challenging task in real-world. Dense trajectory (DT) offers accurate recording of motions over time that is rich in dynamic information. However, DT models lack the mechanism to distinguish dominant motions from secondary ones over separable frequency bands and directions. By contrast, deep learning-based methods are promising over the challenge though still suffering from limited capacity in handling complex temporal information, not mentioning huge datasets needed to guide the training. To take the advantage of semantical meaningful and “handcrafted” video features through feature engineering, this study integrates the discrete wavelet transform (DWT) technique into the DT model for gaining more descriptive human action features. Through exploring the pre-trained dual-stream CNN-RNN models, learned features can be integrated with the handcrafted ones to satisfy stringent analytical requirements within the spatial-temporal domain. This hybrid feature framework generates efficient Fisher Vectors through a novel Bag of Temporal Features scheme and is capable of encoding video events whilst speeding up action recognition for real-world applications. Evaluation of the design has shown superior recognition performance over existing benchmark systems. It has also demonstrated promising applicability and extensibility for solving challenging real-world human action recognition problems.},
  archive      = {J_APIN},
  author       = {Zhang, Chaolong and Xu, Yuanping and Xu, Zhijie and Huang, Jian and Lu, Jun},
  doi          = {10.1007/s10489-021-03068-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12771-12787},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid handcrafted and learned feature framework for human action recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel entity joint annotation relation extraction model.
<em>APIN</em>, <em>52</em>(11), 12754–12770. (<a
href="https://doi.org/10.1007/s10489-021-03002-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social network is an indispensable part of our life. Text is the most common carrier in social networks. Extracting entities and relationships from a text in social media can help to mine people’s views and attitudes. However, identifying the entity pairs that overlap between multiple relations in a sentence and the subject and object that overlap in a relation is a tricky question to be solved urgently. We propose a new relation extraction model named GraphJoint, which models the relation extraction task as a mapping from the relation to the entity. Firstly, we apply the pre-trained BERT encoder to encode the words and generate a text graph for each sentence. We use the graph neural network message-passing mechanism to extract the text features in a sentence, which are used to classify the relations in the sentences. Secondly, we reuse the extracted features and add the relation features to extract the entities. The self-attention mechanism and dilated gate convolution are used to extract entity features further. Finally, we use the joint annotation method to mark the head, tail, and overlapping parts of the subject and the object and transform the task into a sequence labeling task. Experiments compared with other advanced algorithms on two public data sets prove that our method increases the F1 value of the two data sets by 3.6% and 3.4% and achieves a perfect recognition effect in the recognition of overlapping entity pairs.},
  archive      = {J_APIN},
  author       = {Xu, Meng and Pi, Dechang and Cao, Jianjun and Yuan, Shuilian},
  doi          = {10.1007/s10489-021-03002-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12754-12770},
  shortjournal = {Appl. Intell.},
  title        = {A novel entity joint annotation relation extraction model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning characterization of cancer patients-derived
extracellular vesicles using vibrational spectroscopies: Results from a
pilot study. <em>APIN</em>, <em>52</em>(11), 12737–12753. (<a
href="https://doi.org/10.1007/s10489-022-03203-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of cancer is a challenging problem in medicine. The blood sera of cancer patients are enriched with heterogeneous secretory lipid-bound extracellular vesicles (EVs), which present a complex repertoire of information and biomarkers, representing their cell of origin, that are being currently studied in the field of liquid biopsy and cancer screening. Vibrational spectroscopies provide non- invasive approaches for the assessment of structural and biophysical properties in complex biological samples. In this pilot study, multiple Raman spectroscopy measurements were performed on the EVs extracted from the blood sera of n = 9 patients consisting of four different cancer subtypes (colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic cancer) and five healthy patients (controls). FTIR (Fourier Transform Infrared) spectroscopy measurements were performed as a complementary approach to Raman analysis, on two of the four cancer subtypes. The spectra were subjected to various machine learning classifiers with hyperparameter optimization to discriminate between healthy and cancer patients-derived EVs. The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs from those of healthy controls (N = 18 spectra) with a classification accuracy of &gt;90% when reduced to a spectral frequency range of 1800 − 1940 𝑐𝑚−1 and subjected to a 50:50 training: testing split. FTIR classification accuracy on N = 14 spectra showed an 80% classification accuracy. Our findings demonstrate that basic machine learning algorithms are powerful applied intelligence tools to distinguish the complex vibrational spectra of cancer patient EVs from those of healthy patients. These experimental methods hold promise as valid and efficient liquid biopsy for artificial intelligence-assisted early cancer screening.},
  archive      = {J_APIN},
  author       = {Uthamacumaran, Abicumaran and Elouatik, Samir and Abdouh, Mohamed and Berteau-Rainville, Melissa and Gao, Zu-hua and Arena, Goffredo},
  doi          = {10.1007/s10489-022-03203-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12737-12753},
  shortjournal = {Appl. Intell.},
  title        = {Machine learning characterization of cancer patients-derived extracellular vesicles using vibrational spectroscopies: Results from a pilot study.},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised node classification via graph learning
convolutional neural network. <em>APIN</em>, <em>52</em>(11),
12724–12736. (<a
href="https://doi.org/10.1007/s10489-022-03233-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks (GCNs) have become increasingly popular in recent times due to the emerging graph data in scenes such as social networks and recommendation systems. However, engineering graph data are often noisy and incomplete or even unavailable, making it challenging or impossible to implement the de facto GCNs method directly on them. Current efforts for tackling this issue either require an overparameterized model that is hard to scale, or simply re-weight the existing edges for different downward tasks. In this work, we tackle this problem through introducing a graph learning convolutional neural network (GLCNN), which can be employed on both Euclidean space data and non-Euclidean space data. The similarity matrix is learned by a supervised method in the graph learning layer of the GLCNN. Moreover, graph pooling and distilling operations are utilized to reduce over-fitting. Comparative experiments are done on three different datasets: citation dataset, knowledge graph dataset, and image dataset. Results demonstrate that the GLCNN can improve the accuracy of the semi-supervised node classification by mining useful relationships among nodes. The performance is more obvious especially on datasets of Euclidean space. Specifically, GLCNN outperforms the best baseline by 3.1% and 1.1% on MNIST and SVHN datasets. Moreover, the robustness is explored by adding noises on the edge of the graph. Sensitive analysis and visualizations are performed to demonstrate effects of some key parameters.},
  archive      = {J_APIN},
  author       = {Li, Kangjie and Ye, Wenjing},
  doi          = {10.1007/s10489-022-03233-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12724-12736},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised node classification via graph learning convolutional neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSA-GAN: Cyclic synthesized attention guided generative
adversarial network for face synthesis. <em>APIN</em>, <em>52</em>(11),
12704–12723. (<a
href="https://doi.org/10.1007/s10489-021-03064-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Network (GAN) is one of the recent developments in the area of deep learning to transform the images from one domain to another domain. While transforming the images, we need to make sure that the background information should not influence the learning process. The attention-based networks are developed to learn the saliency maps and to prioritize the learning based on the important image regions. We develop a new Cyclic Synthesized Attention Generative Adversarial Network (CSA-GAN) in this paper by incorporating the cycle synthesized loss with the attention network. The use of attention guidance as well as cycle synthesis objective reduces the learning space more towards the optimum solution. It also improves the rate of convergence. The proposed method is tested for Sketch to Face synthesis over CUHK and AR benchmark datasets. We also experimented for thermal to visible face synthesis over WHU-IIP dataset. The proposed CSA-GAN observed promising performance for face synthesis in comparison with state-of-the-art GAN methods.},
  archive      = {J_APIN},
  author       = {Yadav, Nand Kumar and Singh, Satish Kumar and Dubey, Shiv Ram},
  doi          = {10.1007/s10489-021-03064-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12704-12723},
  shortjournal = {Appl. Intell.},
  title        = {CSA-GAN: Cyclic synthesized attention guided generative adversarial network for face synthesis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast multi-view twin hypersphere support vector machine with
consensus and complementary principles. <em>APIN</em>, <em>52</em>(11),
12684–12703. (<a
href="https://doi.org/10.1007/s10489-021-02986-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning (MVL) is an emerging machine learning approach which concentrates on the problems of learning from data represented by multiple distinct feature sets. Various support vector machine (SVM) based MVL algorithms have been proposed and showed excellent performance in learning tasks. However, multi-view SVMs face two major challenges, one being their immense computational complexity and the other being difficult to achieve views’ consistency and views’ complementary simultaneously. In this paper, we firstly propose a fast training MVL algorithm based on twin hypersphere support vector machine (THSVM), termed as multi-view THSVM (MvTHSVM). MvTHSVM trains two independent hyperspheres on two views under a pair of co-regularization constraints, which maximizes the consensus of two distinct views. We further simplify the dual problems of MvTHSVM into a pair of smaller-scaled ones based on the idea of matrix reduction, and adopt the alternating direction multipliers method (ADMM) to boost its solving speed. Secondly, we extend MvTHSVM to a general MVL framework which realizes consensus principle and complementary principle at the same time, termed as MvTHSVM-2C. The integration of complementary principle renders MvTHSVM-2C better classification performance. Benchmark results conducted on 55 binary data sets confirm the generalization capacity and training efficiency of the proposed algorithms.},
  archive      = {J_APIN},
  author       = {Zhu, Jiayi and Wang, Huiru and Li, Hongjun and Zhang, Qing},
  doi          = {10.1007/s10489-021-02986-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12684-12703},
  shortjournal = {Appl. Intell.},
  title        = {Fast multi-view twin hypersphere support vector machine with consensus and complementary principles},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Many-objective optimization based path planning of multiple
UAVs in oilfield inspection. <em>APIN</em>, <em>52</em>(11),
12668–12683. (<a
href="https://doi.org/10.1007/s10489-021-02977-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) inspection can be considered as a path planning problem for the UAVs, which needs UAVs to traverse all task points one by one by avoiding some obstacles. In most current researches, only single objective function is used in the path planning for the UAVs, and some other objectives are seldom considered together, like: the number of UAVs, the required inspection time and so on. Therefore, in order to overcome the defect of using single objective function to plan the path of UAVs, a many-objective optimization based multiple UAVs path planning method is studied in this paper. Firstly, four objective functions are chosen which include the flight distance, the flight stability, the number of UAVs and the time offset of reaching the task point; considering the arrival time of UAVs at each task point is an interval value in the practical application, the time offset of reaching the task point is set as an interval objective function, which is handled by the matter-element extension method in this paper. Then, an improved NSGA-III algorithm is proposed to solve the established many-objective optimization problem, which uses fruit fly optimization algorithm (FOA) to replace the genetic algorithm (GA) in the NSGA-III algorithm. Finally, several evaluation indictors (Spread, generational distance (GD), inverse generational distance (IGD), and running time) are used to choose the optimal flight paths of multiple UAVs. Through simulation comparisons with other algorithms, it is concluded that the improved NSGA-III algorithm is more effective in the path planning of UAVs.},
  archive      = {J_APIN},
  author       = {Li, Kun and Yan, Xinxin and Han, Ying and Ge, Fawei and Jiang, Yu},
  doi          = {10.1007/s10489-021-02977-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12668-12683},
  shortjournal = {Appl. Intell.},
  title        = {Many-objective optimization based path planning of multiple UAVs in oilfield inspection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid salp swarm algorithm based on TLBO for reliability
redundancy allocation problems. <em>APIN</em>, <em>52</em>(11),
12630–12667. (<a
href="https://doi.org/10.1007/s10489-021-02862-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel optimization algorithm called hybrid salp swarm algorithm with teaching-learning based optimization (HSSATLBO) is proposed in this paper to solve reliability redundancy allocation problems (RRAP) with nonlinear resource constraints. Salp swarm algorithm (SSA) is one of the newest meta-heuristic algorithms which mimic the swarming behaviour of salps. It is an efficient swarm optimization technique that has been used to solve various kinds of complex optimization problems. However, SSA suffers a slow convergence rate due to its poor exploitation ability. In view of this inadequacy and resulting in a better balance between exploration and exploitation, the proposed hybrid method HSSATLBO has been developed where the searching procedures of SSA are renovated based on the TLBO algorithm. The good global search ability of SSA and fast convergence of TLBO help to maximize the system reliability through the choices of redundancy and component reliability. The performance of the proposed HSSATLBO algorithm has been demonstrated by seven well-known benchmark problems related to reliability optimization that includes series system, complex (bridge) system, series-parallel system, overspeed protection system, convex system, mixed series-parallel system, and large-scale system with dimensions 36, 38, 40, 42 and 50. After illustration, the outcomes of the proposed HSSATLBO are compared with several recently developed competitive meta-heuristic algorithms and also with three improved variants of SSA. Additionally, the HSSATLBO results are statistically investigated with the wilcoxon sign-rank test and multiple comparison test to show the significance of the results. The experimental results suggest that HSSATLBO significantly outperforms other algorithms and has become a remarkable and promising tool for solving RRAP.},
  archive      = {J_APIN},
  author       = {Kundu, Tanmay and Deepmala and Jain, Pramod K.},
  doi          = {10.1007/s10489-021-02862-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12630-12667},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid salp swarm algorithm based on TLBO for reliability redundancy allocation problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video-based vehicle re-identification via channel
decomposition saliency region network. <em>APIN</em>, <em>52</em>(11),
12609–12629. (<a
href="https://doi.org/10.1007/s10489-021-03096-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification has become an important research topic for its application prospect in real-world, such as intelligent security system and intelligent traffic management. The current vehicle re-identification algorithms mainly run on image-based datasets, while video-based datasets are very rare in the community. Therefore we collect a dataset named as Veri-Video-763, including 763 vehicle IDs and 5828 tracks. In addition, we propose a channel decomposition saliency region network, including three modules to improve the video-based vehicle re-identification. The channel decomposition saliency region extraction (CDSRE) module generate significant masks to detect multiple significant local regions by channel decomposition. The global-local stacking module encode the convolutional features of the salient regions and the global pooling feature together into re-identification feature vectors. The distributed symmetric sampling (DSS) module propose a novel video clip sampling algorithm to improve the unity and difference of the video clips. Extensive experiments demonstrate the effectiveness of our proposed methods, and thus can be considered as one strong baseline. Dataset and code are available on https://github.com/wyf27/Veri-Video-763.},
  archive      = {J_APIN},
  author       = {Wang, Yuefeng and Gong, Benhua and Wei, Ying and Ma, Ruipeng and Wang, Lin},
  doi          = {10.1007/s10489-021-03096-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12609-12629},
  shortjournal = {Appl. Intell.},
  title        = {Video-based vehicle re-identification via channel decomposition saliency region network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). An improved henry gas solubility optimization algorithm
based on lévy flight and brown motion. <em>APIN</em>, <em>52</em>(11),
12584–12608. (<a
href="https://doi.org/10.1007/s10489-021-02811-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Henry gas solubility optimization (HGSO) algorithm is a physical heuristic algorithm based on Henry’s law. It is a heuristic algorithm proposed to simulate the process of gas solubility in liquid changing with temperature. In this paper, Lévy’s flight operator and Brown motion operator are introduced respectively, which are inspired by the flight trajectory of animals and the thermal motion of particles. This increases the diversity of search strategies and enhances the ability of local search. It greatly improves the shortcoming of the original HGSO algorithm, which has a single position updating method and sometimes slow convergence speed. Lévy motion based Henry gas solubility optimization algorithm (Lévy-HGSO), Brown motion based Henry gas solubility optimization algorithm (Brown-HGSO) are proposed in this paper. It is worth mentioning that in this paper, an improved Henry gas solubility optimization algorithm (BL-HGSO) based on the Lévy and Brown motion is proposed by combining the Lévy flight operator and Brown motion operator. Different from the former two, the effective combination of different motion modes can more accurately find the optimal solution, which not only guarantees the original global search ability, but also strengthens the local search strategy, and is not easy to fall into the local optimal value. In order to verify the performance of the proposed algorithms, 40 benchmark functions were optimized by this algorithm, and two practical engineering design problems were solved. The sine and cosine algorithm (SCA), whale optimization algorithm (WOA), lightning search algorithm (LSA), water cycle algorithm(WCA)and HGSO algorithms were used in comparison experiments. The simulation results show that three improved HGSO algorithms proposed in this paper have strong ability of balancing exploration and exploitation, fast convergence speed and high precision.},
  archive      = {J_APIN},
  author       = {Li, Song and Wang, Jie-Sheng and Xie, Wei and Li, Xue-Long},
  doi          = {10.1007/s10489-021-02811-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12584-12608},
  shortjournal = {Appl. Intell.},
  title        = {An improved henry gas solubility optimization algorithm based on lévy flight and brown motion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coarse-fine point cloud registration based on local
point-pair features and the iterative closest point algorithm.
<em>APIN</em>, <em>52</em>(11), 12569–12583. (<a
href="https://doi.org/10.1007/s10489-022-03201-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud registration has a wide range of applications in object shape detection, robot navigation and 3D reconstruction. Aiming at the problems of the traditional ICP registration algorithm, such as slow convergence speed and high requirements for the initial point cloud position, this paper proposes a coarse-fine point cloud registration method based on a fast and robust local point-pair feature (LPPF) and the ICP algorithm. The LPPF feature descriptor is an improved descriptor for the local application of classic point-pair features and is a histogram descriptor formed by counting the feature information of the local point cloud neighborhood. This paper completes point cloud registration through keypoint extraction, LPPF feature description, feature matching, coarse registration and fine registration. To verify the effectiveness of this method, under the evaluation indices of L1, RMSE and MAE, we analyzed the experimental results from the three aspects of descriptors, coarse registration and coarse-fine registration. Under Gaussian noise conditions, LPPF compared to the second-ranked descriptor, the L1 scores of LPPF on the Stanford, Kinect and Princeton datasets increased by 12%, 12.4% and 9.1%, respectively. The coarse registration experiment is compared with 5 classic descriptors on 3 commonly used datasets. The LPPF feature descriptor has higher registration accuracy and less registration time. Finally, the coarse-fine registration experiment shows that our method can reduce the number of iterations of the ICP algorithm by 77% under optimal conditions.},
  archive      = {J_APIN},
  author       = {Yue, Xiaofeng and Liu, Zeyuan and Zhu, Juan and Gao, Xueliang and Yang, Baojin and Tian, Yunsheng},
  doi          = {10.1007/s10489-022-03201-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12569-12583},
  shortjournal = {Appl. Intell.},
  title        = {Coarse-fine point cloud registration based on local point-pair features and the iterative closest point algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SA-FPN: An effective feature pyramid network for crowded
human detection. <em>APIN</em>, <em>52</em>(11), 12556–12568. (<a
href="https://doi.org/10.1007/s10489-021-03121-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crowded scenario not only contains instances at various scales but also introduces a variety of occlusion patterns ranging from non-occluded situations to heavily occluded cases, making the shapes of the instances different. All of those can result in human detectors being hard to apply to them. Feature pyramid networks (FPN), as an indispensable part of generic object detectors, can significantly boost detection performance involving objects at different scales. As a result, in this paper, we equip FPN with a multi-scale feature fusion technology and attention mechanisms to improve the performance of human detection in crowded scenarios. Firstly, we designed a feature pyramid structure with a refined hierarchical-split block, referred to as Scale-FPN, which can better handle the challenging problem of scale variation across object instances. Secondly, an attention-based lateral connection (ALC) module with spatial and channel attention mechanisms was proposed to replace the lateral connection in the FPN, which enhances the representational ability of feature maps through rich spatial and semantic information and lets detectors be capable of focusing on important features of occlusion patterns. Additionally, a bottom-up path augmentation (BPA) module was adopted to exploit the features of the Scale-FPN and ALC modules. To verify the effectiveness of the proposed method, we combined Scale-FPN, ALC and BPA, namely SA-FPN, and integrated it into the architecture of a crowded human detector. Experiments on the challenging CrowdHuman benchmark sufficiently validate the effectiveness of SA-FPN. Specifically, it improves the state-of-the-art result of CrowdDet from 41.4% to 39.9% $$MR^{-2}$$ , which indicates that the detector with SA-FPN brings in fewer false positives.},
  archive      = {J_APIN},
  author       = {Zhou, Xinxin and Zhang, Long},
  doi          = {10.1007/s10489-021-03121-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12556-12568},
  shortjournal = {Appl. Intell.},
  title        = {SA-FPN: An effective feature pyramid network for crowded human detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A training-free recursive multiresolution framework for
diffeomorphic deformable image registration. <em>APIN</em>,
<em>52</em>(11), 12546–12555. (<a
href="https://doi.org/10.1007/s10489-021-03062-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffeomorphic deformable image registration is one of the crucial tasks in medical image analysis, which aims to find a unique transformation while preserving the topology and invertibility of the transformation. Deep convolutional neural networks (CNNs) have yielded well-suited approaches for image registration by learning the transformation priors from a large dataset. The improvement in the performance of these methods is related to their ability to learn information from several sample medical images that are difficult to obtain and bias the framework to the specific domain of data. In this paper, we propose a novel diffeomorphic training-free approach; this is built upon the principle of an ordinary differential equation. Our formulation yields an Euler integration type recursive scheme to estimate the changes of spatial transformations between the fixed and the moving image pyramids at different resolutions. The proposed architecture is simple in design. The moving image is warped successively at each resolution and finally aligned to the fixed image; this procedure is recursive in a way that at each resolution, a fully convolutional network (FCN) models a progressive change of deformation for the current warped image. The entire system is end-to-end and optimized for each pair of images from scratch. In comparison to learning-based methods, the proposed method neither requires a dedicated training set nor suffers from any training bias. We evaluate our method on three cardiac image datasets. The evaluation results demonstrate that the proposed method achieves state-of-the-art registration accuracy while maintaining desirable diffeomorphic properties.},
  archive      = {J_APIN},
  author       = {Sheikhjafari, Ameneh and Noga, Michelle and Punithakumar, Kumaradevan and Ray, Nilanjan},
  doi          = {10.1007/s10489-021-03062-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12546-12555},
  shortjournal = {Appl. Intell.},
  title        = {A training-free recursive multiresolution framework for diffeomorphic deformable image registration},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient photorealistic style transfer with multi-order
image statistics. <em>APIN</em>, <em>52</em>(11), 12533–12545. (<a
href="https://doi.org/10.1007/s10489-021-03154-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic style transfer concerns rendering the style of a reference image to a content image with the restraint that the stylized image should be realistic. While the existing methods have achieved promising results, they are prone to generating either structural distortions or inconsistent style due to the lack of effective style representation. In this work, to represent the inherent style information effectively, we propose a two-branch learnable transfer mechanism by considering the complementary advantages of the first-order and second-order image statistics simultaneously. Instead of directly using these two image statistics, we design a learnable transfer branch to implement the second-order image statistics learning to capture the consistent style and improve the efficiency. We further use a multi-scale representation branch to retain more structural details of the content image. In addition, a lightweight but effective adaptive-aggregation mechanism is proposed to fuse the features across different branches dynamically to balance between the consistent style and photorealism. Qualitative and quantitative experiments demonstrate that the proposed method renders the image faithfully with photorealistic results and high efficiency.},
  archive      = {J_APIN},
  author       = {Huo, Zhanqiang and Li, Xueli and Qiao, Yingxu and Zhou, Panbo and Wang, Jing},
  doi          = {10.1007/s10489-021-03154-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12533-12545},
  shortjournal = {Appl. Intell.},
  title        = {Efficient photorealistic style transfer with multi-order image statistics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDA-net: A detector for small, densely distributed, and
arbitrary-directional ships in remote sensing images. <em>APIN</em>,
<em>52</em>(11), 12516–12532. (<a
href="https://doi.org/10.1007/s10489-021-03148-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ships in remote sensing images are usually arranged in arbitrary direction, with small objects and dense distribution, and are easily interfered by background noise, which makes the existing object detection methods have a certain missed detection rate and false detection rate in this kind of complex scene. In order to solve the above problems, a neural network for ship detection is proposed. The network is composed of symmetrical deformation feature pyramid network, multi-scale attention network, multi-scale local context network and rotation branch. Firstly, in order to fully extract the features of ships with arbitrary directions and small objects in remote sensing images, an adaptive deformable convolution unit is designed and a symmetrical deformation feature pyramid network is constructed with it as core. Secondly, a multi-scale attention module is designed to guide the network to focus on ship areas of different scales and suppress background noise. Then, a multi-scale local context network is designed to capture the correlation between ships and local nearby objects, and to learn multi-scale co-occurrence features. Finally, a rotation bounding box is generated by the rotation branch, which is used to mark ships in arbitrary direction. The experimental results on two remote sensing public datasets DOTA, HRSC2016 show that the proposed method achieves state-of-the-art accuracy.},
  archive      = {J_APIN},
  author       = {Cui, Zhe and Sun, Hong-Mei and Yin, Ruo-Nan and Jia, Rui-Sheng},
  doi          = {10.1007/s10489-021-03148-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12516-12532},
  shortjournal = {Appl. Intell.},
  title        = {SDA-net: A detector for small, densely distributed, and arbitrary-directional ships in remote sensing images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). An efficient discrete differential evolution algorithm
based on community structure for influence maximization. <em>APIN</em>,
<em>52</em>(11), 12497–12515. (<a
href="https://doi.org/10.1007/s10489-021-03021-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the main contents of influence analysis, influence maximization is selecting a group of influential nodes with specified size in a given network to form a seed node set, and the influence spread cascaded by the selected seed node set can be maximized under a given propagation model. The research of influence maximization is helpful to understand social network and viral marketing. How to develop an effective algorithm to solve this problem in large-scale networks is still a challenge. In this paper, a discrete differential evolution algorithm based on community structure (CDDE) is proposed. At first, the fast Louvain algorithm is used to detect the community structure. On this basis, significant communities are defined and candidate nodes are extracted from each significant community. And then, an improved discrete differential evolution algorithm is proposed to obtain influential nodes. Furthermore, a population initialization strategy based on candidate nodes is designed, and the candidate nodes are also used to accelerate the discrete evolution process of the population. Experimental results on six real-world social networks show that the proposed CDDE is competitive with the comparison algorithms in terms of effectiveness and efficiency, and achieves comparable influence spread to CELF.},
  archive      = {J_APIN},
  author       = {Li, Huan and Zhang, Ruisheng and Liu, Xin},
  doi          = {10.1007/s10489-021-03021-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12497-12515},
  shortjournal = {Appl. Intell.},
  title        = {An efficient discrete differential evolution algorithm based on community structure for influence maximization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal and multi-objective optimization algorithm based
on two-stage search framework. <em>APIN</em>, <em>52</em>(11),
12470–12496. (<a
href="https://doi.org/10.1007/s10489-021-02969-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem that multiple Pareto solution sets correspond to the same Pareto front is called multimodal multi-objective optimization problem. Solving all Pareto solution sets in this kind of problem can provide decision makers with more convenient and accurate choices. However, the traditional multi-objective optimization algorithm often ignores the distribution of solutions in the decision space when solving such problems, resulting in poor diversity of Pareto solution sets.To solve this problem, a two-stage search algorithm framework is proposed. This framework divides the optimization process into two parts: global search and local search to balance the search ability of the algorithm. When searching globally, locate as many approximate locations with the optimal solution as possible, providing a good population distribution for subsequent local searches. In local search, DBSCAN clustering method with adaptive neighborhood radius is used to divide the population into several subpopulations, so as to enhance the local search ability with the algorithm. At the same time, an individual selection mechanism based on the farthest-candidate approach with two spaces is proposed to keep the diversity of the population in the objective space and decision space. The algorithm is compared with five state-of-the-art algorithms on 22 multimodal and multi-objective optimization test functions. The experimental results indicate that the proposed algorithm can search more Pareto solution sets while maintaining the diversity of solutions in the objective space.},
  archive      = {J_APIN},
  author       = {Zhang, Jia-Xing and Chu, Xiao-Kai and Yang, Feng and Qu, Jun-Feng and Wang, Shen-Wen},
  doi          = {10.1007/s10489-021-02969-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12470-12496},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal and multi-objective optimization algorithm based on two-stage search framework},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparative study of chinese named entity recognition with
different segment representations. <em>APIN</em>, <em>52</em>(11),
12457–12469. (<a
href="https://doi.org/10.1007/s10489-022-03274-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is a fundamental but crucial task in the field of natural language processing and has been widely studied. Nevertheless, little attention has been given to the segment representation (SR) schemes used to map multi-token entities into categories in Chinese NER. To address this issue, in this paper, we explore and compare the impact of using different SR schemes on Chinese NER. Our experiments are conducted on four benchmark Chinese NER datasets extended with labels to include seven well-known SR schemes: IO, IOB2, IOE2, IOBES, BI, IE, and BIES. Moreover, all seven SR schemes are investigated via two sets of classifiers: machine learning-based and neural network-based classifiers. The experimental results demonstrate that the proper selection of the best SR scheme is a complicated problem that depends on various factors, such as corpus size, corpus distribution, and the chosen classifier. We also provide a comparative analysis of the time consumption of each classifier in different SR schemes and discuss the impacts of using different SR schemes on NER in Chinese and other languages.},
  archive      = {J_APIN},
  author       = {Pan, Jun and Zhang, Chaohua and Wang, Haijun and Wu, Zongda},
  doi          = {10.1007/s10489-022-03274-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12457-12469},
  shortjournal = {Appl. Intell.},
  title        = {A comparative study of chinese named entity recognition with different segment representations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-criteria decision-making model for hotel selection
by online reviews: Considering the traveller types and the
interdependencies among criteria. <em>APIN</em>, <em>52</em>(11),
12436–12456. (<a
href="https://doi.org/10.1007/s10489-021-03151-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many efforts have been dedicated to the research on online reviews-based hotel selection. However, the existing methods often fail to take into account the types of potential travellers and, moreover, they rarely consider the impact of the interdependencies among hotel criteria when modelling the hotel selection process. To counter these defects, this paper proposes a multi-criteria decision-making model for hotel selection that considers the types of potential travellers and the interdependencies among criteria. To achieve this, the proposed model first converts online reviews into picture fuzzy numbers, and introduces 2-order additive fuzzy measures to capture the interdependencies among criteria by proposing a novel aggregation function based on Choquet average integral. Furthermore, it uses the similarity between each traveller type and the potential traveller type to calculate the weights of different traveller types so as to account the impact of traveller type. To validate the effectiveness of the proposed model, a case study is conducted on the reviews collected from TripAdvisor. The study shows that the proposed decision support model can effectively help potential travellers of different traveller types to make their desirable hotel choices.},
  archive      = {J_APIN},
  author       = {Tao, Ling-Ling and You, Tian-Hui},
  doi          = {10.1007/s10489-021-03151-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12436-12456},
  shortjournal = {Appl. Intell.},
  title        = {A multi-criteria decision-making model for hotel selection by online reviews: Considering the traveller types and the interdependencies among criteria},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric algebra graph neural network for cross-domain
few-shot classification. <em>APIN</em>, <em>52</em>(11), 12422–12435.
(<a href="https://doi.org/10.1007/s10489-021-03124-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) show powerful processing ability on graph structure data for nodes and graph classification. However, existing GNN models may cause information loss with the increasing number of the network layer. To improve the graph-structured data features representation quality, we introduce geometric algebra into graph neural networks. In this paper, we construct a high-dimensional geometric algebra (GA) space in the Non-Euclidean domain to better learn vector embedding for graph nodes. We focus our study on few-shot learning and propose a geometric algebra graph neural network (GA-GNN) as the metric network for cross-domain few-shot classification tasks. In the geometric algebra space, the feature nodes are mapped into hyper-complex vector, which helps reduce the distortion of feature information with the increased hidden layers. The experimental results demonstrate that the approach we proposed achieves the state-of-the-art few-shot cross-domain classification accuracy in five public datasets.},
  archive      = {J_APIN},
  author       = {Liu, Qifan and Cao, Wenming},
  doi          = {10.1007/s10489-021-03124-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12422-12435},
  shortjournal = {Appl. Intell.},
  title        = {Geometric algebra graph neural network for cross-domain few-shot classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Web attack detection based on traps. <em>APIN</em>,
<em>52</em>(11), 12397–12421. (<a
href="https://doi.org/10.1007/s10489-021-03077-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every website on the Internet is somewhat vulnerable to security attacks. These attacks are constantly changing, and it is challenging to detect the latest, not known attacks. Our goal is automation of attack detection by incremental learning of the latest types of attacks. We have placed web traps around the Internet in a way that regular users cannot find and interact with them, while they are visible to standard hacker tools and methods. Consequently, we obtain continuous information about new types of attacks, contrary to most datasets from the literature created in artificial settings. In this paper, for the purpose of effective web attack detection without many false positives, we propose an efficient way to create a dataset by combining malicious requests from the traps and benign requests from a regular website. Since our goal is automation, we tested a significant number of shallow and deep machine learning models to separate regular from malicious HTTP requests, using only simple features, such as n-grams of characters. Additionally to our dataset, we have evaluated all the models on the large publicly available FWAF dataset. We also conducted model testing on zero-day attacks, in which training and validation requests were collected in separate time intervals. One of the biggest problems in machine learning is catastrophic forgetting. When training on new data, the model forgets the knowledge learned from previous examples. To mitigate that problem, we have implemented three incremental learning approaches for web attack detection and obtained good results during testing.},
  archive      = {J_APIN},
  author       = {Stevanović, Nikola and Todorović, Branimir and Todorović, Vladan},
  doi          = {10.1007/s10489-021-03077-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12397-12421},
  shortjournal = {Appl. Intell.},
  title        = {Web attack detection based on traps},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partial label learning with noisy side information.
<em>APIN</em>, <em>52</em>(11), 12382–12396. (<a
href="https://doi.org/10.1007/s10489-021-03137-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Partial Label Learning (PLL), each training instance is assigned with several candidate labels, among which only one label is the ground-truth. Existing PLL methods mainly focus on identifying the unique ground-truth label, while the contribution of other candidate labels as well as the latent noisy side information are regrettably ignored. To tackle the above issues, we propose a novel PLL approach named PL-NSI, which simultaneously takes the feature noise and the contributions of other candidate labels into consideration. Specifically, given PLL data with noisy side information, we first leverage the latent label distribution to emphasize the different contributions of other candidate labels. Then, we utilize the low-rank representation to recover the ideal feature space from the corrupted observations. In addition, to improve the robustness of the final model, we adopt an extra regularization term to exploit the consistency between visual information and semantic information. Finally, we conduct enormous experiments on both artificial and real-world data sets, and the experimental results verify that our method can achieve competitive performance against state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Shaokai and Xia, Mingxuan and Wang, Zilong and Lyu, Gengyu and Feng, Songhe},
  doi          = {10.1007/s10489-021-03137-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12382-12396},
  shortjournal = {Appl. Intell.},
  title        = {Partial label learning with noisy side information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new mechanism of rule acquisition based on covering rough
sets. <em>APIN</em>, <em>52</em>(11), 12369–12381. (<a
href="https://doi.org/10.1007/s10489-021-03067-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rule acquisition, known as knowledge acquisition, is an important and topical issue in granular computing theory. Granules are not only composed of objects but also have feature values. However, In the granule associativity rules, the traditional rule extraction methods fail to consider the influence of granules on the decision, thus the method is not well adapted in reality. On the other hand, the existing methods lack a rule-based measure for information systems. In this paper, the action parameters are first introduced to establish a more realistic granule associativity rule in covering information system. Further, we present the rule-based data potential to address the measurement problem. In addition, rule-based scale selection in multi-scale covering rough sets is explored, followed by a scale combination integrating generalization capability, data potential, and lower approximation. Finally, algorithm is designed and experiments are conducted.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaoping and Li, Jinjin and Li, Weikang},
  doi          = {10.1007/s10489-021-03067-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12369-12381},
  shortjournal = {Appl. Intell.},
  title        = {A new mechanism of rule acquisition based on covering rough sets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Word representation using refined contexts. <em>APIN</em>,
<em>52</em>(11), 12347–12368. (<a
href="https://doi.org/10.1007/s10489-021-02898-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, inspired from the idea that the contextual distances and positions may have a substantial impact on distinguishing the relationships between words, a novel word association method with two weighting schemes for refining contexts, named as Weighted Point-wise Mutual Information with Contextual Distances and Positions (PMIDP), is proposed to eliminate the noisy and redundant information hidden in an imbalanced corpus. One weighting scheme, called PMIdist, revises the Point-wise Mutual Information (PMI) method by scaling the co-occurrence counts according to the distance between a word and the context. The second weighting scheme is a ratio that can measure the contextual position variation within the window of a given word. Then, the refined word association in PMIDP is defined as the multiplication of the two proposed weighting schemes, which essentially aims to flexibly adjust the word association when solving target-oriented similarity tasks. The proposed word association method has been applied on two widely known models, i.e., the positive PMI matrix with truncated Singular Vector Decomposition (PPMI-SVD) model and the Global Vectors (GloVe) model. Experimental results demonstrate that the PMIDP method can significantly improve the performances of the two models on both semantic and relational similarity tasks and show advantages when compared with other state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Zhang, Ming and Palade, Vasile and Wang, Yan and Ji, Zhicheng},
  doi          = {10.1007/s10489-021-02898-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12347-12368},
  shortjournal = {Appl. Intell.},
  title        = {Word representation using refined contexts},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Similarity-based second chance autoencoders for textual
data. <em>APIN</em>, <em>52</em>(11), 12330–12346. (<a
href="https://doi.org/10.1007/s10489-021-03100-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying conventional autoencoders for textual data often results in learning trivial and redundant representations due to high text dimensionality, sparsity, and following power-law word distribution. To address these challenges, we propose two novel autoencoders, SCAT (Second Chance Autoencoder for Text) and SSCAT (Similarity-based SCAT). Our autoencoders utilize competitive learning among the k winner neurons in the bottleneck layer, which become specialized in recognizing specific patterns, leading to learning more semantically meaningful representations of textual data. In addition, the SSCAT model presents a novel competition based on a similarity measurement to eliminate redundant features. Our experiments prove that SCAT and SSCAT achieve high performance on several tasks, including classification, topic modeling, and document visualization, compared to LDA, k-Sparse, KATE, ProdLDA, NVCTM and ZeroShotTM.The experiments were conducted using the 20 Newsgroups, Wiki10+, and Reuters datasets.},
  archive      = {J_APIN},
  author       = {Goudarzvand, Saria and Gharibi, Gharib and Lee, Yugyung},
  doi          = {10.1007/s10489-021-03100-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12330-12346},
  shortjournal = {Appl. Intell.},
  title        = {Similarity-based second chance autoencoders for textual data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wrapper feature selection with partially labeled data.
<em>APIN</em>, <em>52</em>(11), 12316–12329. (<a
href="https://doi.org/10.1007/s10489-021-03076-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new feature selection approach with partially labeled training examples in the multi-class classification setting. It is based on a new modification of the genetic algorithm that creates and evaluates candidate feature subsets during an evolutionary process, taking into account feature weights and recursively eliminating irrelevant features. To increase the variety of data, unlabeled observations are employed in the feature selection process, namely by pseudo-labeling them using a self-learning algorithm with a recently proposed transductive policy. Empirical results on different data sets show the effectiveness of our method compared to several state-of-the-art semi-supervised feature selection approaches.},
  archive      = {J_APIN},
  author       = {Feofanov, Vasilii and Devijver, Emilie and Amini, Massih-Reza},
  doi          = {10.1007/s10489-021-03076-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12316-12329},
  shortjournal = {Appl. Intell.},
  title        = {Wrapper feature selection with partially labeled data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive fuzzy preference relations and its applications in
decision-making. <em>APIN</em>, <em>52</em>(11), 12301–12315. (<a
href="https://doi.org/10.1007/s10489-021-03056-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive fuzzy set reveals the reason why the sum of membership and non-membership degrees of an element to a set is larger than 1, and it defines the joint degree to represent the joint part of membership and non-membership degrees. Given that the cognitive fuzzy set can reflect cognitive overlaps of experts, this paper introduces the cognitive fuzzy preference relation (CFPR) to represent preference intensities of alternatives through pairwise comparisons. To facilitate the analyses with CFPRs, the operations of CFPRs are studied based on the t-norm and t-conorm. Afterwards, the entropy and cross-entropy of CFPRs are introduced and then used to determine the weights of criteria. The score function of a CFPR is proposed and an entropy-weight-based ranking method with CFPRs is introduced to rank alternatives. A case study on selecting agricultural food supply chains is given to demonstrate the applicability of the proposed method. Sensitivity analyses and comparative analyses demonstrate that the proposed method is reliable.},
  archive      = {J_APIN},
  author       = {Jiang, Lisheng and Liao, Huchang},
  doi          = {10.1007/s10489-021-03056-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12301-12315},
  shortjournal = {Appl. Intell.},
  title        = {Cognitive fuzzy preference relations and its applications in decision-making},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-object tracking based on network flow model and ORB
feature. <em>APIN</em>, <em>52</em>(11), 12282–12300. (<a
href="https://doi.org/10.1007/s10489-021-03042-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-object tracking algorithm based on network flow model and ORB feature for solving occlusion problem. In our work, we extract the ORB features from the detection response and match them to achieve data association at the intra-tracklet stage. Then, tracklets are associated by network flow model to realize data association at the inter-tracklet stage. Each trajectory fragment is regarded as a node in the network flow model. Owing to the different occlusion situations, different network flow cost functions are proposed, by integrating the motion information obtained by the Edge Multi-channel Gradient Model, the appearance information of the tracklet and the time information. Experimental results demonstrate that compared with other state-of-art algorithms, our method improves tracking performance in complex environments.},
  archive      = {J_APIN},
  author       = {Chen, Jieyu and Xi, Zhenghao and Lu, Junxin and Ji, Jingjing},
  doi          = {10.1007/s10489-021-03042-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12282-12300},
  shortjournal = {Appl. Intell.},
  title        = {Multi-object tracking based on network flow model and ORB feature},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STGs: Construct spatial and temporal graphs for citywide
crowd flow prediction. <em>APIN</em>, <em>52</em>(11), 12272–12281. (<a
href="https://doi.org/10.1007/s10489-021-02939-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flow prediction is one of the most remarkable issues in a wide range of areas, from traffic control to public safety, and aims to forecast the inflow and outflow of crowds in each region of a city. Most existing studies adopt CNN and its variants to discover the spatial patterns of grid maps (each grid represents a region) while ignoring the correlation between distant regions that may share similar temporal patterns. In this paper, we propose a gnn-based prediction method, called STGs, for crowd flow prediction, which jointly constructs spatial and temporal graphs from grid maps and then implements graph neural networks to directly capture the relationship between regions. Additionally, we introduce a gated fusion mechanism to combine spatial and temporal embedding from the corresponding graph, which further improves the performance of our STGs. We conduct numerical experiments to compare STGs with other baseline models using two real-world datasets, BikeNYC and TLC. Experimental results demonstrate the superiority of our STGs model; specifically, our model reduces the mean absolute error (MAE) of crowd flow prediction by approximately 7-8% compared to state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {Xing, Jintao and Kong, Xiangyuan and Xing, Weiwei and Wei, Xiang and Zhang, Jian and Lu, Wei},
  doi          = {10.1007/s10489-021-02939-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12272-12281},
  shortjournal = {Appl. Intell.},
  title        = {STGs: Construct spatial and temporal graphs for citywide crowd flow prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-attention guided multi-scale network for single image
super-resolution. <em>APIN</em>, <em>52</em>(11), 12258–12271. (<a
href="https://doi.org/10.1007/s10489-022-03248-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have been employed in single image super-resolution (SISR) tasks and achieved considerable performance. However, most of the existing approaches simply deepen and widen the network structure to increase the receptive field of the convolution kernel and adopt the equal processing methods in the channel and the spatial dimension, which brings about the rise of computational complexity and the loss of important information. To address these issues, we propose dual-attention guided multi-scale network (DAMSN) for SISR, which intends to obtain better performance with relatively fewer parameters. Exactly, we first introduce a non-local residual block (NLRB) to capture the abundant information of both low-level features and high-level features. Furthermore, we design a multi-scale dual attention block (MSDAB) to diminish the redundant information and extract features from different scales powerfully. Inside the MSDAB, dual attention module (DAM) focuses on high-frequency information extraction by combining the channel and spatial mechanism. Followed by DAM, multi-scale residual module (MSRM) is utilized to learn the informative feature via multi-sized convolution kernels adaptively. Extensive experiments demonstrate that our proposed DAMSN is superior to other state-of-the-art approaches in terms of both quantitative metrics and subjective visual quality.},
  archive      = {J_APIN},
  author       = {Wen, Juan and Zha, Lei},
  doi          = {10.1007/s10489-022-03248-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12258-12271},
  shortjournal = {Appl. Intell.},
  title        = {Dual-attention guided multi-scale network for single image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OBM-CNN: A new double-stream convolutional neural network
for shield pattern segmentation in ancient oracle bones. <em>APIN</em>,
<em>52</em>(11), 12241–12257. (<a
href="https://doi.org/10.1007/s10489-021-03111-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rejoining of oracle bone (OB) rubbings is a fundamental topic in oracle research. However, severely damaged OB fragments usually lack important material information for rejoining, which has limited the progress of OB rejoining research. Identifying material information from broken OBs is difficult because interclass differences are so obscure. In this case, we can only make a judgment by relying on the “shield pattern” presented in OB rubbings. However, it is time-consuming and laborious to identify materials from “shield pattern” identification directly, and the classification accuracy is typically very low. Thus, we proposed a novel two-stream convolutional neural network (OBM-CNN), which consists of segmentation and detection subnetworks, to handle this challenge. First, the segmentation subnetwork is based on UNet++ and improved with residual block bilinear interpolation. Then, in the detection subnetwork, the backbone feature extraction network of Faster RCNN is replaced with the encoder feature extraction network, and the detection accuracy is significantly improved through cross-training. In addition, a novel dataset named OB-Material was constructed, and we provided labels for the segmentation and detection of “shield patterns”, which compensated for the lack of oracle material datasets. The experimental results show that the “shield pattern” segmentation of our proposed method reached an F1-score value of 95.23%. For “shield pattern” detection, when IoU= 0.5, the F-score value was 9.72% higher than that of the optimal contrast model. In material classification, the optimal accuracy rate reached an excellent result of 91.8%. In conclusion, this paper presents value data for promoting the combination of oracle bone inscriptions and AI technology and the application of AI-aided ancient Chinese characters.},
  archive      = {J_APIN},
  author       = {Gao, Weize and Chen, Shanxiong and Zhang, Chongsheng and Mo, Bofeng and Liu, Xuxing},
  doi          = {10.1007/s10489-021-03111-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12241-12257},
  shortjournal = {Appl. Intell.},
  title        = {OBM-CNN: A new double-stream convolutional neural network for shield pattern segmentation in ancient oracle bones},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage anomaly detection algorithm via dynamic community
evolution in temporal graph. <em>APIN</em>, <em>52</em>(11),
12222–12240. (<a
href="https://doi.org/10.1007/s10489-021-03109-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies from a massive amount of user behavioral data is often liken to finding a needle in a haystack. While tremendous efforts have been devoted to anomaly detection from temporal graphs, existing studies rarely consider community evolution and evolutionary paths simultaneously, and analyze those characteristics for the purpose of anomaly detection. Therefore, we propose a two-stage anomaly detection (TSAD) framework to detect anomalies. In this study, we suggest detecting the community evolution events from a sequence of snapshot graphs by constructing an evolution bipartite graph and designing community similarity scores. We then propose a novel anomaly detection method combining community evolution-based anomaly detection and evolutionary path-based anomaly detection. An anomalous score is designed to detect anomalous community evolution events by extracting the characteristics of evolution communities in the community evolution-based anomaly detection method. Moreover, to reduce the false alarm rate, we propose evolutionary path-based anomaly detection to further detect the abnormality of the identified normal evolutionary paths by extracting the characteristics of the identified anomalous evolutionary paths based on community evolution-based anomaly detection. We conduct extensive experiments on real-world datasets and demonstrate that TSAD consistently outperforms competitive baseline methods in anomaly detection.},
  archive      = {J_APIN},
  author       = {Jiang, Yan and Liu, Guannan},
  doi          = {10.1007/s10489-021-03109-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12222-12240},
  shortjournal = {Appl. Intell.},
  title        = {Two-stage anomaly detection algorithm via dynamic community evolution in temporal graph},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inverted residual based lightweight network for object
detection in sweeping robots. <em>APIN</em>, <em>52</em>(11),
12206–12221. (<a
href="https://doi.org/10.1007/s10489-021-03104-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a core capability required by sweeping robots to conduct their cleanup jobs. Currently, deep learning-based object detection has achieved high accuracy; however, owing to the large number of backbone network layers, detection speed depends on the computing hardware, which hinders its application to real-time sweeping jobs. To address this problem, this paper proposes an accurate, fast, and lightweight You Only Look Once (YOLO) network for sweeping robots (SR). First, an improved online enhancement method is proposed to enrich the original dataset. Second, an inverted residual block based lightweight network is constructed for object recognition. Finally, an optimized spatial pyramid pooling method is added behind the backbone network to further improve network performance. Comparative experiments with many state-of-the-art methods are conducted, and the results reveal that the new model accuracy is 13.79% better than that of YOLOv4_tiny using a similar model size. Furthermore, its model size is only 1/9 that of YOLOv4 and achieves similar accuracy, demonstrating the advantages of the proposed model in terms of size, accuracy, and speed.},
  archive      = {J_APIN},
  author       = {Lv, Yong and Liu, Jie and Chi, Wenzheng and Chen, Guodong and Sun, Lining},
  doi          = {10.1007/s10489-021-03104-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12206-12221},
  shortjournal = {Appl. Intell.},
  title        = {An inverted residual based lightweight network for object detection in sweeping robots},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HRANet: Hierarchical region-aware network for crowd
counting. <em>APIN</em>, <em>52</em>(11), 12191–12205. (<a
href="https://doi.org/10.1007/s10489-021-03030-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to tackle the most intractable problems of scale variation and complex backgrounds in crowd counting, we present an innovative framework called Hierarchical Region-Aware Network (HRANet) for crowd counting in this paper, which can better focus on crowd regions to accurately predict crowd density. In our implementation, first, we design a Region-Aware Module (RAM) to capture the internal differences within different regions of the feature map, thus adaptively extracting contextual features within different regions. Furthermore, we propose a Region Recalibration Module (RRM) which adopts a novel region-aware attention mechanism (RAAM) to further recalibrate the feature weights of different regions. By the integration of the above two modules, the influence of background regions can be effectively suppressed. Besides, considering the local correlations within different regions of the crowd density map, a Region Awareness Loss (RAL) is designed to reduce false identification while producing the locally consistent density map. Extensive experiments on five challenging datasets demonstrate that the proposed method significantly outperforms existing methods in terms of counting accuracy and quality of the generated density map. In addition, a series of specific experiments in crowd gathering scenes indicate that our method can be practically applied to crowd localization.},
  archive      = {J_APIN},
  author       = {Xie, Jinyang and Gu, Lingyu and Li, Zhonghui and Lyu, Lei},
  doi          = {10.1007/s10489-021-03030-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12191-12205},
  shortjournal = {Appl. Intell.},
  title        = {HRANet: Hierarchical region-aware network for crowd counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning bi-grained cross-correlation siamese networks for
visual tracking. <em>APIN</em>, <em>52</em>(11), 12175–12190. (<a
href="https://doi.org/10.1007/s10489-021-03015-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese network based trackers measure the similarity between a target template and a search region by computing their cross-correlation. Specifically, Siamese trackers regard the target template as a spatial filter to convolve the search region, putting emphasis on the coarse-grained semantic abstraction of the target in the spatial domain. Along with the demonstrated success of Siamese trackers, little attention has been paid to fine-grained spatial details in cross-correlation computation, which is crucial to precise target localization. In this paper, we propose to learn point-wise cross-correlation Siamese networks for visual tracking. By sketching the contour of the target, the proposed point-wise cross-correlation module helps Siamese networks to be aware of the distinctive boundary between the target and background. In conjunction with traditional depth-wise cross-correlation, the proposed Siamese network takes both advantages of coarse-grained semantic abstraction and fine-grained details to precisely locate the target. Extensive experiments demonstrate the effectiveness and efficiency of the proposed tracker, which achieves new state-of-the-art results on five visual tracking benchmarks including VOT2016, VOT2018, VOT2019, OTB100, and LaSOT with the speed of 38 FPS. As an extra benefit, our tracker can output the segmentation mask for the target. We demonstrate the favorable performance of our tracker on the video object segmentation datasets in comparison with the state-of-the-art.},
  archive      = {J_APIN},
  author       = {Zhao, Defang and Ma, Chao and Zhu, Dandan and Shuai, Jia and Lu, Jianwei},
  doi          = {10.1007/s10489-021-03015-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12175-12190},
  shortjournal = {Appl. Intell.},
  title        = {Learning bi-grained cross-correlation siamese networks for visual tracking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetDPO: (Delta, gamma)-approximate pattern matching with gap
constraints under one-off condition. <em>APIN</em>, <em>52</em>(11),
12155–12174. (<a
href="https://doi.org/10.1007/s10489-021-03000-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate pattern matching not only is more general than exact pattern matching, but also allows some data noise. Most of them adopt the Hamming distance to measure similarity, which indicates the number of different characters in two sequences, but it cannot reflect the approximation between two characters. This paper addresses the approximate pattern matching with a local distance no larger than δ and a global distance no larger than γ, which is named Delta and gamma Pattern matching with gap constraints under One-off condition (DPO). First, we show that the problem is an NP-Hard problem. Therefore, we construct a heuristic algorithm named approximate Nettree for DPO (NetDPO), which transforms the problem into an approximate Nettree based on δ distance which is a specially designed data structure. Then, NetDPO calculates the number of paths that reach the roots within γ distance. To find the maximal occurrences, we employ the rightmost parent strategy and the optimal parent strategy to select the better occurrence which can minimize the influence after removing the occurrence. Iterate this process until there are no occurrences. Finally, we analyze the time and space complexities of NetDPO. Extensive experimental results verify the superiority of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Li, Yan and Yu, Lei and Liu, Jing and Guo, Lei and Wu, Youxi and Wu, Xindong},
  doi          = {10.1007/s10489-021-03000-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12155-12174},
  shortjournal = {Appl. Intell.},
  title        = {NetDPO: (delta, gamma)-approximate pattern matching with gap constraints under one-off condition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Competitive search algorithm: A new method for stochastic
optimization. <em>APIN</em>, <em>52</em>(11), 12131–12154. (<a
href="https://doi.org/10.1007/s10489-021-03133-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach of swarm intelligence(SI) optimization, namely Competitive Search Algorithm(CSA), is proposed in this paper based on some social activities in human life, such as all-around sports competitions and talent variety shows. Firstly, the mathematical model and the algorithm framework are introduced and the working principle is explained in detail. Then the computational complexity and the parameter sensitivity in the proposed algorithm are analyzed. Moreover, it is compared and tested with the eleven algorithms commonly used such as the algorithms of Archimedes optimization, the particle swarm in 15 test functions and CEC’14 test functions. The results show that the proposed algorithm has obvious advantages in the search accuracy, the convergence speed and the stability. Finally, the algorithm CSA is applied to the maximum power point tracking(MPPT) in the photovoltaic system and the reactive power optimization of active distribution network. Therefore, the effectiveness of the proposed algorithm is verified.},
  archive      = {J_APIN},
  author       = {Xu, Yanchun and Liu, Haiquan and Xie, Shasha and Xi, Lei and Lu, Mi},
  doi          = {10.1007/s10489-021-03133-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12131-12154},
  shortjournal = {Appl. Intell.},
  title        = {Competitive search algorithm: A new method for stochastic optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal stacking model for skeleton-based action
recognition. <em>APIN</em>, <em>52</em>(11), 12116–12130. (<a
href="https://doi.org/10.1007/s10489-021-02994-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the prevalence of affordable depth sensors, skeleton-based action recognition has attracted much attention as a significant computer vision task. The state-of-the-art recognition precision usually comes from the complicated deep learning networks which need a large quantity of training data. On the counterparts, none-deep learning methods are easy to be trained and understood, however, have restricted expressive ability to extract the spatial-temporal features of skeleton data simultaneously. Therefore, it is a challenging problem to use shallow learning architecture to effectively identify complex actions in skeleton data. In this paper, we first combine Temporal Hierarchy Pyramid (THP) and Symmetric Positive Definite (SPD) features to simultaneously capture the temporal relationship of inter-frame and the spatial relationship of intra-frame. Then, to achieve the same learning ability as the deep learning network for a non-linear system, we propose a novel stacking ensemble-based method to effectively identify complex actions in skeleton data. We carry out extensive verification of our method on widely used 3D action recognition datasets. The experiment results indicate that we achieve state-of-the-art performance on all compared datasets.},
  archive      = {J_APIN},
  author       = {Zhong, Yufeng and Yan, Qiuyan},
  doi          = {10.1007/s10489-021-02994-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12116-12130},
  shortjournal = {Appl. Intell.},
  title        = {Spatio-temporal stacking model for skeleton-based action recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical mode decomposition and convolutional neural
network-based approach for diagnosing psychotic disorders from eeg
signals. <em>APIN</em>, <em>52</em>(11), 12103–12115. (<a
href="https://doi.org/10.1007/s10489-022-03252-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychotic disorders are mental disorders that negatively affect human life. Diagnosis of psychotic patients is usually done in consultation with the patient, and this is a time-consuming process. In this study, a Computer Aided Diagnosis (CAD) system that will support expert opinion with automatic diagnosis of schizophrenia (SZ) disease, which is the leading psychotic disorder, is presented. In this study, Hilbert Huang Transform (HHT) method was used to analyze the non-stationary and non-periodic structure of EEG (Electroencephalograph) signals in the best way. The data set we used in our study includes 19-channel EEG signals from 28 (14 SZ and 14 healthy controls) participants, and the second data set includes 16-channel EEG signals from 84 (45 SZ and 39 healthy controls) participants. First of all, HS (Hilbert Spectrum) images of the first four Intrinsic Mode Functions (IMF) components obtained by applying Empirical Mode Decomposition (EMD) to EEG signals were created. These images were then classified with the VGG16 pre-trained Convolutional Neural Network (CNN) network. With our proposed method, the highest classification performance was obtained as 98.2% for Dataset I and 96.02% for Dataset II, respectively, by training the HS images obtained from the IMF 1 component with the VGG16 pre-trained CNN network. In the next step, classification performances were tested with VGG16, XCeption, DenseNet121, ResNet152 and Inception V3 pre-trained CNN networks. The high classification success achieved by the proposed method in our study demonstrates the accuracy of the model in distinguishing between SZ and healthy control.},
  archive      = {J_APIN},
  author       = {Zülfikar, Aslan and Mehmet, Akin},
  doi          = {10.1007/s10489-022-03252-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12103-12115},
  shortjournal = {Appl. Intell.},
  title        = {Empirical mode decomposition and convolutional neural network-based approach for diagnosing psychotic disorders from eeg signals},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SGCNet: Scale-aware and global contextual network for crowd
counting. <em>APIN</em>, <em>52</em>(11), 12091–12102. (<a
href="https://doi.org/10.1007/s10489-022-03230-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, visual attention mechanisms have been employed in CNN-based crowd counting methods to overcome the interference of background noise and have achieved good performance. However, the existing methods usually focus on designing complex attention structures and extracting pixel-level contextual information, while ignoring global contextual information extraction at different scales. In this paper, to overcome scale variation and complex background noise, we propose a novel scale-aware and global contextual network (SGCNet) that employs multi-scale attention mechanisms to selectively strengthen features with different network scales. The key component of SGCNet is a multi-scale global contextual block that consists of multi-scale feature selection and global contextual information extraction, where global contextual information is adopted as guidance to weight features at different scales. Compared with the previous methods that ignore scale information injected into the attention mechanism, SGCNet achieves better counting performance via multi-scale contextual information extraction. Extensive experiments on four crowd counting datasets (ShanghaiTech, UCF_CC_50, UCF-QNRF, UCSD) demonstrate the effectiveness and superiority of the proposed method in highly congested noisy crowd scenes.},
  archive      = {J_APIN},
  author       = {Liu, Qi and Guo, Yanqun and Sang, Jun and Tan, Jinghan and Wang, Fusen and Tian, Shaoli},
  doi          = {10.1007/s10489-022-03230-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {11},
  pages        = {12091-12102},
  shortjournal = {Appl. Intell.},
  title        = {SGCNet: Scale-aware and global contextual network for crowd counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A connected network-regularized logistic regression model
for feature selection. <em>APIN</em>, <em>52</em>(10), 11672–11702. (<a
href="https://doi.org/10.1007/s10489-021-02877-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection on a network structure can not only discover interesting variables but also mine out their intricate interactions. Regularization is often employed to ensure the sparsity and smoothness of the coefficients in logistic regression. However, currently available methods fail to embed the network connectivity in regularized penalty functions. In this paper, a connected network-regularized logistic regression (CNet-RLR) model for feature selection considering the structural connectivity in a network was proposed. Mathematically, it was a convex optimization problem constrained by inequalities reflecting network connectivity. Considering the non-differentiability of Lasso penalty, we constructed an equivalent formulation of CNet-RLR by employing auxiliary variables. An interior-point algorithm was designed to efficiently achieve the solutions. Theoretically, we proved their grouping effect and oracle property and guaranteed algorithmic convergence. In both synthetic simulation data and real-world uterine corpus endometrial carcinoma (UCEC) cancer genomics data, we validated the CNet-RLR model is efficient to identify the connected-network-structured features that can serve as diagnostic biomarkers. In the comparison study, we also proved the proposed CNet-RLR model results in better classification performance and feature interpretability than the other regularized logistic regression (RLR) alternatives and another graph embedded feature selection model.},
  archive      = {J_APIN},
  author       = {Li, Lingyu and Liu, Zhi-Ping},
  doi          = {10.1007/s10489-021-02877-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11672-11702},
  shortjournal = {Appl. Intell.},
  title        = {A connected network-regularized logistic regression model for feature selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label feature selection based on manifold
regularization and imbalance ratio. <em>APIN</em>, <em>52</em>(10),
11652–11671. (<a
href="https://doi.org/10.1007/s10489-021-03141-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dimension of data in the domain of multi-label learning is usually high, which makes the calculation cost very high. As an important data dimension reduction technology, feature selection has attracted the attention of many researchers. And the imbalance of data labels is also one of the factors that perplex multi-label learning. To tackle these problems, we propose a new multi-label feature selection algorithm named IMRFS, which combines manifold learning and label imbalance. Firstly, in order to keep the manifold structure between samples, the Laplacian graph is used to construct the manifold regularization. In addition, the local manifold structure of each label is considered to find the correlation between labels. And the imbalance distribution of labels is also considered, which is embedded into the manifold structure of labels. Furthermore, in order to ensure the robustness and sparsity of the IMRFS method, the L2,1-norm is applied to loss function and sparse regularization term simultaneously. Then, we adopt an iterative strategy to optimize the objective function of IMRFS. Finally, comparison results on multiple datasets show the effectiveness of IMRFS method.},
  archive      = {J_APIN},
  author       = {Lu, Haohan and Chen, Hongmei and Li, Tianrui and Chen, Hao and Luo, Chuan},
  doi          = {10.1007/s10489-021-03141-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11652-11671},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label feature selection based on manifold regularization and imbalance ratio},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measure information quality of basic probability assignment:
An information volume method. <em>APIN</em>, <em>52</em>(10),
11638–11651. (<a
href="https://doi.org/10.1007/s10489-021-03066-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information quality is a concept that can be used to measure the information of probability distribution. Dempster-Shafer evidence theory can describe uncertain information more reasonably than probability theory. Therefore, it is a research hot spot to propose information quality applicable to evidence theory. Recently, Deng proposed the concept of information volume based on Deng entropy. It is worth noting that, compared with the Deng entropy, the information volume of the Deng entropy contains more information. Obviously, it may be more reasonable to use the information volume of Deng entropy to represent uncertain information. Therefore, this article proposes a new information quality, which is based on the information volume of Deng entropy. In addition, when the basic probability assignment (BPA) degenerates into a probability distribution, the proposed information quality is consistent with the information quality proposed by Yager and Petry. What’s more, based on the information quality of information volume, a correlation coefficient is proposed. After that, several numerical examples illustrate the effectiveness of this new method. Finally, a weight average fusion method based on information quality of information volume is proposed, and a target recognition task and a pattern recognition experiment are used to illustrate the efficiency of the method.},
  archive      = {J_APIN},
  author       = {Li, Dingbin and Deng, Yong},
  doi          = {10.1007/s10489-021-03066-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11638-11651},
  shortjournal = {Appl. Intell.},
  title        = {Measure information quality of basic probability assignment: An information volume method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradient-based elephant herding optimization for cluster
analysis. <em>APIN</em>, <em>52</em>(10), 11606–11637. (<a
href="https://doi.org/10.1007/s10489-021-03020-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering analysis is essential for obtaining valuable information from a predetermined dataset. However, traditional clustering methods suffer from falling into local optima and an overdependence on the quality of the initial solution. Given these defects, a novel clustering method called gradient-based elephant herding optimization for cluster analysis (GBEHO) is proposed. A well-defined set of heuristics is introduced to select the initial centroids instead of selecting random initial points. Specifically, the elephant optimization algorithm (EHO) is combined with the gradient-based algorithm GBO for assigning initial cluster centers across the search space. Second, to overcome the imbalance between the original EHO exploration and exploitation, the initialized population is improved by introducing Gaussian chaos mapping. In addition, two operators, i.e., random wandering and variation operators, are set to adjust the location update strategy of the agents. Nine datasets from synthetic and real-world datasets are adopted to evaluate the effectiveness of the proposed algorithm and the other metaheuristic algorithms. The results show that the proposed algorithm ranks first among the 10 algorithms. It is also extensively compared with state-of-the-art techniques, and four evaluation criteria of accuracy rate, specificity, detection rate, and F-measure are used. The obtained results clearly indicate the excellent performance of GBEHO, while the stability is also more prominent.},
  archive      = {J_APIN},
  author       = {Duan, Yuxian and Liu, Changyun and Li, Song and Guo, Xiangke and Yang, Chunlin},
  doi          = {10.1007/s10489-021-03020-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11606-11637},
  shortjournal = {Appl. Intell.},
  title        = {Gradient-based elephant herding optimization for cluster analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An industrial portrait background removal solution based on
knowledge infusion. <em>APIN</em>, <em>52</em>(10), 11592–11605. (<a
href="https://doi.org/10.1007/s10489-021-03099-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background removal of an identity (ID) picture consists in separating the foreground (face, body, hair and clothes) from the background of the image. It is a necessary groundwork for all modern identity documents that also has many benefits for improving ID security. State of the art image processing techniques encountered several segmentation issues and offer only partial solutions. It is due to the presence of erratic components like hairs, poor contrast, luminosity variation, shadow, color overlap between clothes and background. In this paper, a knowledge infused approach is proposed that hybridizes smart image processing tasks and prior knowledge. The research is based on a divide and conquer strategy aiming at simulating the sequential attention of human when performing a manual segmentation. Knowledge is infused by considering the spatial relation between anatomic elements of the ID image (face feature, forehead, body and hair) as well as their “signal properties”. The process consists in first determining a convex hull around the person’s body including all the foreground while keeping very close to the contour between the background and the foreground. Then, a body map generated from biometric analysis associated to an automatic grab cut process is applied to reach a finer segmentation. Finally, a heuristic-based post-processing step consisting in correcting potential hair and fine boundary issues leads to the final segmentation. Experimental results show that the newly proposed architecture achieves better performances than tested current state-of-the-art methodologies including active contours, generalist popular deep learning techniques, and also two other ones considered as the smartest for portrait segmentation. This new technology has been adopted by an international company as its industrial ID foreground solution.},
  archive      = {J_APIN},
  author       = {Riad, Rabia and Ros, Frédéric and hajji, Mohamed El and Harba, Rachid},
  doi          = {10.1007/s10489-021-03099-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11592-11605},
  shortjournal = {Appl. Intell.},
  title        = {An industrial portrait background removal solution based on knowledge infusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved decomposition method for large-scale global
optimization: Bidirectional-detection differential grouping.
<em>APIN</em>, <em>52</em>(10), 11569–11591. (<a
href="https://doi.org/10.1007/s10489-021-03023-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential grouping (DG) is an efficient decomposition method that is used to solve large-scale global optimization (LSGO) problems. To further reduce the computational cost, a bidirectional-detection differential grouping (BDDG) method is proposed in this paper. By exploiting the bidirectional detection structure (BDS), BDDG is able to spend less computation than other DG-based methods. An adaptive perturbation strategy (APS) is proposed to improve the problem with the BDS decomposition accuracy. Analytical methods are used to demonstrate that the complexity of BDDG is lower than that of other state-of-the-art DG-based methods. Experiments showed that BDDG substantially reduced the computational cost for problem decomposition and that the computational cost used by BDDG grew slowly, as the problem dimension grew compared to other DG-based methods. When BDDG was embedded in the cooperative coevolution (CC) framework, it improved the performance of the CC for solving LSGO problems.},
  archive      = {J_APIN},
  author       = {Sun, Yu and Yue, Hongda},
  doi          = {10.1007/s10489-021-03023-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11569-11591},
  shortjournal = {Appl. Intell.},
  title        = {An improved decomposition method for large-scale global optimization: Bidirectional-detection differential grouping},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage clustering algorithm based on evolution and
propagation patterns. <em>APIN</em>, <em>52</em>(10), 11555–11568. (<a
href="https://doi.org/10.1007/s10489-021-03016-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of current popular clustering algorithms needing to set the number of clusters and hyperparameters according to prior knowledge, we use the average nearest neighbour distance, a statistic that represents the characteristics of sample aggregation in the data space, and propose a two-stage clustering algorithm based on evolution and propagation patterns (EPC). In the evolution stage, the EPC algorithm obtains the initial clustering results and the number of clusters by evolving a small number of samples from random sampling in the data space in an incremental way. According to the nearest neighbour principle, the EPC propagates the cluster labels of the initial clustering results to the unlabelled samples in the propagation stage. Furthermore, the EPC algorithm uses a correction mechanism. It adopts Monte Carlo multiple simulation methods in the evolution stage to improve the stability of clustering results obtained by random sampling. Experiments on datasets and applications on image segmentation datasets show that the EPC algorithm is superior to the current popular clustering algorithm in performance. Finally, we conducted a systematic and comprehensive analysis of the EPC algorithm through ablation experiments, showing that the EPC algorithm has good robustness.},
  archive      = {J_APIN},
  author       = {Li, Peng and Xie, Haibin},
  doi          = {10.1007/s10489-021-03016-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11555-11568},
  shortjournal = {Appl. Intell.},
  title        = {Two-stage clustering algorithm based on evolution and propagation patterns},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aspect-level sentiment classification based on location and
hybrid multi attention mechanism. <em>APIN</em>, <em>52</em>(10),
11539–11554. (<a
href="https://doi.org/10.1007/s10489-021-02966-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main task of aspect-level sentiment classification is to judge the sentiment polarity of a sentence under a given aspect word. Existing methods such as recurrent neural networks or attention mechanisms cannot make full use of location information. Moreover, when the sentence length is longer and the grammatical structure is more complicated, the above methods may cause the loss of important information and it is difficult to dig deeper semantic emotional features. Therefore, we propose a hybrid network model, and this model designs a shallow and deep two-layer network, and constructs a positional attention mechanism and an interactive multi-head attention mechanism in the corresponding network to capture multi-level emotional characteristics. The experimental results show that, in most case, the model performs better than the relevant baseline model on Restaurant and Laptop of the SemEval 2014 and ACL14 Twitter, and can effectively identify different aspects of emotional polarity.},
  archive      = {J_APIN},
  author       = {Wu, Yuchen and Li, Weijiang},
  doi          = {10.1007/s10489-021-02966-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11539-11554},
  shortjournal = {Appl. Intell.},
  title        = {Aspect-level sentiment classification based on location and hybrid multi attention mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A supervised machine learning framework with combined
blocking for detecting serial crimes. <em>APIN</em>, <em>52</em>(10),
11517–11538. (<a
href="https://doi.org/10.1007/s10489-021-02942-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting serial crimes is to find criminals who have committed multiple crimes. A classification technique is often used to process serial crime detection, but the pairwise comparison of crimes is of quadratic complexity, and the number of nonserial case pairs far exceeds the number of serial case pairs. The blocking method can play a role in reducing pairwise calculation and eliminating nonserial case pairs. But the limitation of previous studies is that most of them use a single criterion to select blocks, which is difficult to guarantee an excellent blocking result. Some studies integrate multiple criteria into one comprehensive index. However, the performance is easily affected by the weighting method. In this paper, we propose a combined blocking (CB) approach. Each criminal behaviour is defined as a behaviour key (BHK) and used to form a block. CB learns several weak blocking schemes by different blocking criteria and then combines them to form the final blocking scheme. The final blocking scheme consists of several BHKs. Because rare behaviour can better identify crime series, each BHK is assigned a score according to its rarity. BHKs and their scores are used to determine whether a case pair need to be compared. After comparing with multiple blocking methods, CB can effectively guarantee the number of serial case pairs while greatly reducing unnecessary nonserial case pairs. The CB is embedded in a supervised machine learning framework. Experiments on real-world robbery cases demonstrate that it can effectively reduce pairwise comparison, alleviate the class imbalance problem and improve detection performance.},
  archive      = {J_APIN},
  author       = {Li, Yusheng and Shao, Xueyan},
  doi          = {10.1007/s10489-021-02942-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11517-11538},
  shortjournal = {Appl. Intell.},
  title        = {A supervised machine learning framework with combined blocking for detecting serial crimes},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized face-pose estimation network using
incrementally updated face shape parameters. <em>APIN</em>,
<em>52</em>(10), 11506–11516. (<a
href="https://doi.org/10.1007/s10489-021-02888-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a deep learning method is proposed for human image processing that incorporates a mechanism to update target-specific parameters. The aim is to improve system performance in situations where the target can be continuously observed. Network-based algorithms typically rely on offline training processes that use large datasets, while trained networks typically operate in a one-shot fashion. That is, each input image is processed one by one in the static network. On the other hand, many practical applications can be expected to use continuous observation rather than observation of a single image. The proposed method employs dynamic use of multiple observations to improve system performance. In this paper, the effectiveness of the proposed method adopting an iterative update process is clarified through its implementation in the task of face-pose estimation. The method consists of two separate processes: 1) sequential estimation and updating of face-shape parameters (target-specific parameters) and 2) face-pose estimation for every single image using the updated parameters. Experimental results indicate the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Sei, Makoto and Utsumi, Akira and Yamazoe, Hirotake and Lee, Joo-Ho},
  doi          = {10.1007/s10489-021-02888-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11506-11516},
  shortjournal = {Appl. Intell.},
  title        = {Personalized face-pose estimation network using incrementally updated face shape parameters},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid extreme learning machine model with harris hawks
optimisation algorithm: An optimised model for product demand
forecasting applications. <em>APIN</em>, <em>52</em>(10), 11489–11505.
(<a href="https://doi.org/10.1007/s10489-022-03251-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and real-time product demand forecasting is the need of the hour in the world of supply chain management. Predicting future product demand from historical sales data is a highly non-linear problem, subject to various external and environmental factors. In this work, we propose an optimised forecasting model - an extreme learning machine (ELM) model coupled with the Harris Hawks optimisation (HHO) algorithm to forecast product demand in an e-commerce company. ELM is preferred over traditional neural networks mainly due to its fast computational speed, which allows efficient demand forecasting in real-time. Our ELM-HHO model performed significantly better than ARIMA models that are commonly used in industries to forecast product demand. The performance of the proposed ELM-HHO model was also compared with traditional ELM, ELM auto-tuned using Bayesian Optimisation (ELM-BO), Gated Recurrent Unit (GRU) based recurrent neural network and Long Short Term Memory (LSTM) recurrent neural network models. Different performance metrics, i.e., Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) and Mean Percentage Error (MPE) were used for the comparison of the selected models. Horizon forecasting at 3 days and 7 days ahead was also performed using the proposed approach. The results revealed that the proposed approach is superior to traditional product demand forecasting models in terms of prediction accuracy and it can be applied in real-time to predict future product demand based on the previous week’s sales data. In particular, considering RMSE of forecasting, the proposed ELM-HHO model performed 62.73% better than the statistical ARIMA(7,1,0) model, 40.73% better than the neural network based GRU model, 34.05% better than the neural network based LSTM model, 27.16% better than the traditional non-optimised ELM model with 100 hidden nodes and 11.63% better than the ELM-BO model in forecasting product demand for future 3 months. The novelty of the proposed approach lies in the way the fast computational speed of ELMs has been combined with the accuracy gained by tuning hyperparameters using HHO. An increased number of hyperparameters has been optimised in our methodology compared to available models. The majority of approaches to improve the accuracy of ELM so far have only focused on tuning the weights and the biases of the hidden layer. In our hybrid model, we tune the number of hidden nodes, the number of input time lags and even the type of activation function used in the hidden layer in addition to tuning the weights and the biases. This has resulted in a significant increase in accuracy over previous methods. Our work presents an original way of performing product demand forecasting in real-time in industry with highly accurate results which are much better than pre-existing demand forecasting models.},
  archive      = {J_APIN},
  author       = {Chaudhuri, Koushiki Dasgupta and Alkan, Bugra},
  doi          = {10.1007/s10489-022-03251-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11489-11505},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid extreme learning machine model with harris hawks optimisation algorithm: An optimised model for product demand forecasting applications},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data association and loop closure in semantic dynamic SLAM
using the table retrieval method. <em>APIN</em>, <em>52</em>(10),
11472–11488. (<a
href="https://doi.org/10.1007/s10489-021-03091-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) plays an important role in the area of robotics and augmented reality to simultaneously obtain its location and construct environment maps in real-time. There are many challenges in SLAM, such as data association, loop closure, and dynamic environments. In this paper, we propose a table retrieval method for SLAM data association and loop closure using semantic information in a dynamic environment. The detected landmarks are stored in a table for retrieval, and each landmark has its own semantic and location information for data association and loop closure. The proposed method only checks the corresponding items, so it is not necessary to traverse all the landmarks in the reference frames, which is beneficial to real-time performance and cost efficiency. Experiments are performed to verify the effectiveness of our method on the public TUM and KITTI dataset. The results show that our system achieves considerable performance improvement compared with state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Song, Chengqun and Zeng, Bo and Su, Tong and Zhang, Ke and Cheng, Jun},
  doi          = {10.1007/s10489-021-03091-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11472-11488},
  shortjournal = {Appl. Intell.},
  title        = {Data association and loop closure in semantic dynamic SLAM using the table retrieval method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis and implementation of no-equilibrium chaotic system
with application in image encryption. <em>APIN</em>, <em>52</em>(10),
11448–11471. (<a
href="https://doi.org/10.1007/s10489-021-03071-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of no-equilibrium chaotic system is one of the recent hot topics. This paper constructs a new no-equilibrium chaotic system by introducing an additional variable and a constant term to a three-dimensional chaotic system. Different from the previous no-equilibrium chaotic system, the new system has period-doubling bifurcation and performs hidden chaotic attractors for a large constant term. The analog circuit and field-programmable gate array (FPGA) implementation are given to illustrate the existence of the system. By utilizing the new system, a chaotic magic cube transformation image encryption algorithm (CMCT-IEA) is proposed, which has a classical permutation-diffusion structure. A new permutation method is designed to scramble image pixels in three-dimensional space, and a diffusion method is developed to diffuse small pixel changes of the original image to all pixels in three-dimensional space. A dynamic key is also designed to improve the security of the encryption algorithm. We also analyze the security of CMCT-IEA in terms of computational complexity, statistical properties, and the ability to defend against several common attacks. Compared with several advanced algorithms, the CMCT-IEA exhibits excellent security characteristics.},
  archive      = {J_APIN},
  author       = {Lai, Qiang and Zhang, Hui and Kuate, Paul Didier Kamdem and Xu, Guanghui and Zhao, Xiao-Wen},
  doi          = {10.1007/s10489-021-03071-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11448-11471},
  shortjournal = {Appl. Intell.},
  title        = {Analysis and implementation of no-equilibrium chaotic system with application in image encryption},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCHGAT: Pedestrian trajectory prediction using group
constrained hierarchical graph attention networks. <em>APIN</em>,
<em>52</em>(10), 11434–11447. (<a
href="https://doi.org/10.1007/s10489-021-02997-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the motion of pedestrians is a challenge due to the uncertainty of the target pedestrian itself and the influence of other people in the environment. Modelling social interactions is of great significance for pedestrian trajectory prediction. However, most of the existing works only focus on the pair-wise interactions of humans but ignore the group-wise interactions. This paper proposes a group constrained hierarchical graph attention network, GCHGAT, to capture the intragroup, outgroup, and intergroup interaction separately. We first get a rough prediction via a vanilla generative adversarial network. Then, a state-refinement module is used to refine the rough prediction based on interaction information. We compare the performance of our method with related methods on the ETH and UCY datasets. The results show that our approach outperforms all benchmarks with the lowest average prediction error and successfully predicts complex social behaviours.},
  archive      = {J_APIN},
  author       = {Zhou, Lei and Zhao, Yingli and Yang, Dingye and Liu, Jingtai},
  doi          = {10.1007/s10489-021-02997-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11434-11447},
  shortjournal = {Appl. Intell.},
  title        = {GCHGAT: Pedestrian trajectory prediction using group constrained hierarchical graph attention networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Harris hawks optimization algorithm based on elite
fractional mutation for data clustering. <em>APIN</em>, <em>52</em>(10),
11407–11433. (<a
href="https://doi.org/10.1007/s10489-021-02985-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering (DPC) algorithm is an efficient clustering algorithm that can automatically find the class center and realize arbitrary shape data clustering. The design of the local density is the core of the DPC algorithm, and the value of the cutoff distance parameter involved in the calculation of the local density has a great impact on the performance of the algorithm. In this paper, based on analyzing the defects of the local density design of the DPC algorithm, we use the cosine similarity and exponential decay function to establish a new method of segmented local density calculation, and build an optimization model for the selection of cutoff distance parameters. A new density peak clustering algorithm (NDPC) is proposed. When solving the model, an improved Harris hawks optimization algorithm (FHHO)based on elite fractional derivative mutation is proposed. Simultaneously, the FHHO-NDPC algorithm combining FHHO and NDPC algorithm is put forward. The FHHO algorithm uses Grünwald-Letnikov (G-L) fractional derivative to correct the elite population which changes with the number of iterations and uses a more random exploration strategy to enhance the exploration performance of HHO algorithm. Therefore, the proposed FHHO algorithm inherits the merits of fractional derivative memory, mends the ability of exploration and exploitation by random exploration strategy, and refrains from the algorithm sinking into local optimum. Two groups of experiments are devised simultaneously to verify the significance and usefulness of the FHHO algorithm and FHHO-NDPC algorithm. Experimental results on the CEC2017 test set show that FHHO has obvious dominant positions in solving high dimensional problems in terms of convergence speed and solution precision compared with other representative intelligent algorithms. The clustering results of twelve representative data sets show that FHHO-NDPC has an excellent clustering performance, which provides a useful reference for the design of large-scale data clustering algorithms.},
  archive      = {J_APIN},
  author       = {Guo, Wenyan and Xu, Peng and Dai, Fang and Hou, Zhuolin},
  doi          = {10.1007/s10489-021-02985-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11407-11433},
  shortjournal = {Appl. Intell.},
  title        = {Harris hawks optimization algorithm based on elite fractional mutation for data clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation network based on multi-granular hypergraphs for
person re-identification. <em>APIN</em>, <em>52</em>(10), 11394–11406.
(<a href="https://doi.org/10.1007/s10489-021-02992-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, person re-identification (re-ID) has become a widespread research topic that focuses on retrieving target pedestrians from a set of images, typically taken by multiple cameras with different fields of view. Since the images of people often suffer from occlusion, misalignment and background clutter, the core challenge is to explore how to extract more discriminative and optimised features. State-of-the-art studies have found that further mining the relationships between local features can provide more sufficient semantic information for the final feature descriptors. Motivated by the idea, in this work, we propose an efficient Multi-Granular Hypergraph Relation Learning (MGHRL) module to explore the dependencies between part features. Specifically, a hypergraph is designed for a particular granularity where the relationships between each local node and other local nodes are modeled by propagating information along the hyperedges. Further, Hierarchical Complementary Identification (HCI) module is introduced to selectively activate diverse salient regions within the multi-scale feature maps to provide a more integrated global feature. By the cooperation of these two modules, the relationship between local features can be established from a global perspective. Extensive experiments on three popular benchmarks including Market1501, CUHK03 and DukeMTMC-reID demonstrate the feasibility and effectiveness of our approach.},
  archive      = {J_APIN},
  author       = {Guo, Chenchen and Zhao, Xiaoming and Zou, Qiang},
  doi          = {10.1007/s10489-021-02992-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11394-11406},
  shortjournal = {Appl. Intell.},
  title        = {Relation network based on multi-granular hypergraphs for person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controllable image caption with an encoder-decoder
optimization structure. <em>APIN</em>, <em>52</em>(10), 11382–11393. (<a
href="https://doi.org/10.1007/s10489-021-02988-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable image caption, which belongs to the intersection of Computer Vision (CV) and Natural Language Process (NLP), is an important part of applying artificial intelligence to many life scenes. We adopt an encoder-decoder structure, which considers visual models as the encoder and regards language models as the decoder. In this work, we introduce a new feature extraction model, namely FVC R-CNN, to learn both the salient features and the visual commonsense features. Furthermore, a novel MT-LSTM neural network for sentence generation is proposed, which is activated by m-tanh and is superior to the traditional Long Short-term memory Network (LSTM) by a significant margin. Finally, we put forward a multi-branch decision strategy to optimize the output. The experimental results are conducted on the widely used COCO Entities dataset, which demonstrates that the proposed method simultaneously outperforms the baseline, surpassing the state-of-the-art methods under a wide range of evaluation metrics. There are CIDEr and SPICE respectively achieves 206.3 and 47.6, yield state-of-the-art (SOTA) performance.},
  archive      = {J_APIN},
  author       = {Shao, Jie and Yang, Runxia},
  doi          = {10.1007/s10489-021-02988-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11382-11393},
  shortjournal = {Appl. Intell.},
  title        = {Controllable image caption with an encoder-decoder optimization structure},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep spatial-temporal bi-directional residual optimisation
based on tensor decomposition for traffic data imputation on urban road
network. <em>APIN</em>, <em>52</em>(10), 11363–11381. (<a
href="https://doi.org/10.1007/s10489-021-03060-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity of fully exploiting underlying spatial-temporal dependencies holds the key for missing traffic data imputation, however, previous studies have neglected the residual information from recovery models. To refine this task, we propose a spatial-temporal bi-directional residual optimisation (ST-BiRT) model on the basis of tensor decomposition to effectively improve the imputation performance. The novelty of our approach concentrates on a well-designed bi-directional residual structure, which reduces model errors dramatically. We can greatly exploit the potential of the optimisation structure by dynamically stacking massive residual units, thereby significantly enhancing the recovery capability. When faced with various combinations of missing scenario and missing rate problems, ST-BiRT model can perform with better accuracy and robustness. Here, the experiments on the Guangzhou traffic speed dataset demonstrate that the proposed ST-BiRT model outperforms the state-of-the-art baseline models. In addition, the mechanism of the bi-directional residual optimisation can address extreme cases and their evaluation metrics can reach acceptable values even when the missing rate exceeds 90%. Finally, the superiority of the ST-BiRT model in repairing loss or low-quality traffic data is confirmed by visualising the experimental results.},
  archive      = {J_APIN},
  author       = {Li, Jinlong and Xu, Lunhui and Li, Ruonan and Wu, Pan and Huang, Zilin},
  doi          = {10.1007/s10489-021-03060-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11363-11381},
  shortjournal = {Appl. Intell.},
  title        = {Deep spatial-temporal bi-directional residual optimisation based on tensor decomposition for traffic data imputation on urban road network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGB-t salient object detection via CNN feature and result
saliency map fusion. <em>APIN</em>, <em>52</em>(10), 11343–11362. (<a
href="https://doi.org/10.1007/s10489-021-02984-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal infrared sensors have unique advantages under the conditions of insufficient illumination, complex scenarios, or occluded appearances. RGB-T salient object detection methods integrate the complementary advantages of visual and thermal modalities to capture salient objects more accurately. Considering the characteristics of visual and thermal images, we combine CNN feature and result saliency map fusion methods to achieve RGB-T salient object detection. First, a two-stream encoder-decoder network is proposed to handle the different saliency cues within RGB-T images. Specifically, the global attention module introduces the complementary saliency cues within thermal images to visual images, thereby ensuring the consistency of salient object locations. Subsequently, the two-stream decoder module gradually fuses the high-level salient object location cues with low-level detail saliency cues to obtain single-modality saliency maps. Then, saliency maps are fused and refined by the proposed result saliency map fusion method to achieve the final saliency map with high precision. In this way, the salient object is segmented with the fine boundary, and the noise inside the salient object is effectively suppressed. Experimental results demonstrate the effectiveness of each component within the CNN feature and result saliency map fusion methods. The proposed method facilitates desirable complementation for RGB-T images and performs favorably against state-of-the-art methods, especially in the challenges of low illumination, cluttered background, and low contrast.},
  archive      = {J_APIN},
  author       = {Xu, Chang and Li, Qingwu and Zhou, Mingyu and Zhou, Qingkai and Zhou, Yaqin and Ma, Yunpeng},
  doi          = {10.1007/s10489-021-02984-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11343-11362},
  shortjournal = {Appl. Intell.},
  title        = {RGB-T salient object detection via CNN feature and result saliency map fusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph representation learning via simple jumping knowledge
networks. <em>APIN</em>, <em>52</em>(10), 11324–11342. (<a
href="https://doi.org/10.1007/s10489-021-02889-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent graph neural networks for graph representation learning depend on a neighborhood aggregation process. Several works focus on simplifying the neighborhood aggregation process and model structures. However, as the depth of the models increases, the simplified models will encounter oversmoothing, resulting in a decrease in model performance. Several works leverage sophisticated learnable neighborhood aggregation algorithms to learn more accurate graph representations. However, the high computational cost limits the depth of these models and the ability to tackle large graphs. In this paper, we propose simple jumping knowledge networks (SJK-Nets), which first leverage a simple no-learning method to complete the neighborhood aggregation process, and then utilize a jumping architecture to combine the different neighborhood ranges of each node to achieve a better structure-aware representation. Under such circumstances, first, we use a simple neighborhood aggregation algorithm to reduce computational complexity of the model. Then, we aggregate the features of high-order neighboring nodes to learn more informative node feature representations. Finally, by combining the above methods, the oversmoothing problem of the deep graph neural networks is alleviated. Our experimental evaluation demonstrates that SJK-Nets achieve or match state-of-the-art results in node classification tasks, text classification tasks, and community prediction tasks. Moreover, since SJK-Nets’ neighborhood aggregation is a no-learning process, SJK-Nets are successfully extended to node clustering tasks.},
  archive      = {J_APIN},
  author       = {Yang, Fei and Zhang, Huyin and Tao, Shiming and Hao, Sheng},
  doi          = {10.1007/s10489-021-02889-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11324-11342},
  shortjournal = {Appl. Intell.},
  title        = {Graph representation learning via simple jumping knowledge networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Modified group theory-based optimization algorithms for
numerical optimization. <em>APIN</em>, <em>52</em>(10), 11300–11323. (<a
href="https://doi.org/10.1007/s10489-021-02982-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group Theory-based Optimization Algorithm (GTOA) is a novel population-based global optimization algorithm, which is used to solve combinatorial optimization problems. This paper studies the applicability of GTOA in numerical optimization and proposes two versions of GTOA based on binary coding (GTOA-b) and $0\sim 9$ integer coding (GTOA-d). Firstly, the coding transformation methods for representing the feasible solutions are introduced, which make GTOA suitable for continuous optimization. On this basis, the original evolution operators are modified to reduce the deviation and speed up global convergence. The experiment using the CEC2017 test suit is carried out to validate the performance of the algorithms. The influence of parameter values and the differences between the calculated results are analyzed by nonparametric tests. Computation results showed that the convergence rate of GTOA-d is faster than that of GTOA-b, and it achieved better results on the benchmark functions with higher dimensions. The comparison against twelve state-of-the-art and recently introduced meta-heuristic algorithms showed that GTOA-d has the superiority on convergence stability, it obtained significantly better performance than seven of its competitors. Finally, all the algorithms are applied to an Amplitude Variation with Offset inversion case study. The simulation showed that the proposed GTOA-d achieved satisfactory inversion results, and it has better performance in terms of convergence rate and average accuracy. The results demonstrate that the proposed GTOA-d is an effective algorithm for numerical optimization.},
  archive      = {J_APIN},
  author       = {Li, Zewen and Zhang, Qisheng and He, Yichao},
  doi          = {10.1007/s10489-021-02982-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11300-11323},
  shortjournal = {Appl. Intell.},
  title        = {Modified group theory-based optimization algorithms for numerical optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Automatic and robust hand gesture recognition by SDD
features based model matching. <em>APIN</em>, <em>52</em>(10),
11288–11299. (<a
href="https://doi.org/10.1007/s10489-021-02933-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic and robust hand gesture recognition remains challenging after many decades of study. Human beings are able to recognize a variety of hand gestures with 100% accuracy solely based on the contour of the hand. Hence, there must be an automatic method that is able to recognize the same variety of hand gestures solely based on the contour of the hand with 100% accuracy. The key technique lies in how to extract the features of the hand’s contour effectively. In this paper, we propose to recognize the hand gestures with the contour features extracted by slope difference distribution (SDD). Firstly, the hand is segmented, its centroid is computed and its contour is extracted. Secondly, the peak features and valley features of the hand contour are computed by the SDD. Thirdly, the hand gesture is recognized by model matching based on the SDD peak features and the SDD valley features. The proposed hand gesture recognition method was tested on three public datasets and it achieved 100% recognition accuracy for all the 10 gestures in two public datasets.},
  archive      = {J_APIN},
  author       = {Wang, ZhenZhou},
  doi          = {10.1007/s10489-021-02933-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11288-11299},
  shortjournal = {Appl. Intell.},
  title        = {Automatic and robust hand gesture recognition by SDD features based model matching},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Second-order convolutional networks for iris recognition.
<em>APIN</em>, <em>52</em>(10), 11273–11287. (<a
href="https://doi.org/10.1007/s10489-021-02925-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris recognition in less constrained environments is challenging as the images taken therein contain severe noisy factors. How to represent iris texture for accurate and robust recognition in such environments is still an open issue. Towards addressing this problem, this paper proposes a novel convolutional network (ConvNet) for effective iris texture representation. The key of the proposed ConvNet is an interaction block which computes an affinity matrix among all pairwise high-level features for learning second-order relationships. The interaction block can model relationships of neighboring and long-range features, and is architecture-agnostic, suitable for different deep network architectures. To further improve the robustness of iris representation, we encode the affinity matrix based on ordinal measure. In addition, we develop a mask network corresponding to the feature learning network, which can exclude the noisy factors during iris matching. We perform thorough ablation studies to evaluate the effectiveness of the proposed networks. Experiments have shown that the proposed networks outperform state-of-the-art (SOTA) methods, achieving a false reject rate (FRR) of 5.49%, 10.41% and 5.80% at 10− 6 false accept rate (FAR) on ND-IRIS-0405, CASIA-IrisV4-Thousand and CASIA-IrisV4-Lamp respectively. And the improvements in equal error rates (EERs) are 0.41%, 0.72% and 0.40%, respectively, as compared with the SOTA methods.},
  archive      = {J_APIN},
  author       = {Jia, Lingyao and Shi, Xueyu and Sun, Qiule and Tang, Xingqiang and Li, Peihua},
  doi          = {10.1007/s10489-021-02925-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11273-11287},
  shortjournal = {Appl. Intell.},
  title        = {Second-order convolutional networks for iris recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Satisfying user preferences in optimised ridesharing
services: <em>APIN</em>, <em>52</em>(10), 11257–11272. (<a
href="https://doi.org/10.1007/s10489-021-02887-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridesharing services offer on-demand transportation solutions while improving the utilization of the available capacity of the vehicles on the road. Profit, travel time, and cost are the commonly optimized objectives in current ridesharing services. Existing multi-objective optimization-based services mainly focus on maximizing profit for service providers while minimizing the travel time for passengers. However, various personal preferences (e.g., co-riders’ gender for a passenger or preferred area of service for a driver) should be considered when offering such services. Such preferences are often conflicting with one another and with the objectives such as cost and travel time. Therefore finding an optimized solution and satisfying such preferences simultaneously is challenging. To address this challenge, this paper proposes a Multi-agent, Multi-objective, Preference-based ridesharing model (MaMoP) that offers an optimized ridesharing solution while satisfying users’ preferences simultaneously. MaMoP uses social-reasoning techniques to model preferences and their relations and employs evolutionary algorithms to find an optimized solution.},
  archive      = {J_APIN},
  author       = {de Carvalho, Vinicius Renan and Golpayegani, Fatemeh},
  doi          = {10.1007/s10489-021-02887-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11257-11272},
  shortjournal = {Appl. Intell.},
  title        = {Satisfying user preferences in optimised ridesharing services: },
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Graph pruning for model compression. <em>APIN</em>,
<em>52</em>(10), 11244–11256. (<a
href="https://doi.org/10.1007/s10489-021-02802-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous AutoML pruning works utilized individual layer features to automatically prune filters. We analyze the correlation for two layers from the different blocks which have a short-cut structure. It shows that, in one block, the deeper layer has many redundant filters which can be represented by filters in the former layer. So, it is necessary to take information from other layers into consideration in pruning. In this paper, a novel pruning method, named GraphPruning, is proposed. Any series of the network is viewed as a graph. To automatically aggregate neighboring features for each node, a graph aggregator based on graph convolution networks (GCN) is designed. In the training stage, a PruningNet that is given aggregated node features generates reasonable weights for any size of the sub-network. Subsequently, the best configuration of the Pruned Network is searched by reinforcement learning. Different from previous work, we take the node features from a well-trained graph aggregator instead of the hand-craft features, as the states in reinforcement learning. Compared with other AutoML pruning works, our method has achieved the state-of-the-art under the same conditions on ImageNet-2012.},
  archive      = {J_APIN},
  author       = {Zhang, Mingyang and Yu, Xinyi and Rong, Jingtao and Ou, Linlin},
  doi          = {10.1007/s10489-021-02802-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11244-11256},
  shortjournal = {Appl. Intell.},
  title        = {Graph pruning for model compression},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting patient arrivals at emergency department using
calendar and meteorological information. <em>APIN</em>, <em>52</em>(10),
11232–11243. (<a
href="https://doi.org/10.1007/s10489-021-03085-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overcrowding in emergency departments (EDs) is a serious problem in many countries. Accurate ED patient arrival forecasts can serve as a management baseline to better allocate ED personnel and medical resources. We combined calendar and meteorological information and used ten modern machine learning methods to forecast patient arrivals. For daily patient arrival forecasting, two feature selection methods are proposed. One uses kernel principal component analysis(KPCA) to reduce the dimensionality of all of the features, and the other is to use the maximal information coefficient(MIC) method to select the features related to the daily data first and then perform KPCA dimensionality reduction. The current study focuses on a public hospital ED in Hefei, China. We used the data November 1, 2019 to August 31, 2020 for model training; and patient arrival data September 1, 2020 to November 31, 2020 for model validation. The results show that for hourly patient arrival forecasting, each machine learning model has better forecasting results than the traditional autoRegressive integrated moving average (ARIMA) model, especially long short-term memory (LSTM) model. For daily patient arrival forecasting, the feature selection method based on MIC-KPCA has a better forecasting effect, and the simpler models are better than the ensemble models. The method we proposed could be used for better planning of ED personnel resources.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Zhang, Jie and Tao, Min and Shu, Jian and Zhu, Degang},
  doi          = {10.1007/s10489-021-03085-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11232-11243},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting patient arrivals at emergency department using calendar and meteorological information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stage deep probabilistic prediction for travel demand.
<em>APIN</em>, <em>52</em>(10), 11214–11231. (<a
href="https://doi.org/10.1007/s10489-021-03047-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate demand prediction is an essential component of any decision support system for smart vehicle dispatching. However, predicting real time demand at the micro-geographical level is a challenging task due to its underlying complexity, and prior work has focused largely on predicting one-step-ahead demand at a macro-geographical level, often using classical times series models that do not consider exogenous factors or quantify the prediction uncertainty in a probabilistic setting. In this paper, we propose an end-to-end deep learning-based framework with a novel architecture to predict multi-step-ahead real time travel demand, along with uncertainty estimation. The model is multi-stage and captures both spatial and temporal aspects. We employ the encoder-decoder framework with variations of convolution and LSTM units, and incorporate an attention mechanism into the model to quantify the interdependence between the input and output elements. We illustrate our model using two real data sets: a taxi and a ride-sharing service (Uber) for the city of New York.},
  archive      = {J_APIN},
  author       = {Alghamdi, Dhaifallah and Basulaiman, Kamal and Rajgopal, Jayant},
  doi          = {10.1007/s10489-021-03047-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11214-11231},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stage deep probabilistic prediction for travel demand},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous information network-based interest composition
with graph neural network for recommendation. <em>APIN</em>,
<em>52</em>(10), 11199–11213. (<a
href="https://doi.org/10.1007/s10489-021-03018-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information networks (HINs) are widely applied to recommendation systems due to their capability of modeling various auxiliary information with meta-paths. However, existing HIN-based recommendation models usually fuse the information from various meta-paths by simple weighted sum or concatenation, which limits performance improvement because it lacks the capability of interest compositions among meta-paths. In this article, we propose an HIN-based Interest Composition model for Recommendation (HicRec). Specifically, user and item representations are learned with a graph neural network on both the graph structure and features in each meta-path, and a parameter sharing mechanism is utilized here to ensure that the user and item representations are in the same latent space. Then, users’ interests in each item from each pair of related meta-paths are calculated by a combination of the user and item representations. The composed user interests are obtained by their single interest from both intra- and inter-meta-paths for recommendation. Extensive experiments are conducted on three real-world datasets and the results demonstrate that our proposed HicRec model outperforms the baselines.},
  archive      = {J_APIN},
  author       = {Yan, Dengcheng and Xie, Wenxin and Zhang, Yiwen},
  doi          = {10.1007/s10489-021-03018-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11199-11213},
  shortjournal = {Appl. Intell.},
  title        = {Heterogeneous information network-based interest composition with graph neural network for recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-text interaction graph neural network for image-text
sentiment analysis. <em>APIN</em>, <em>52</em>(10), 11184–11198. (<a
href="https://doi.org/10.1007/s10489-021-02936-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As various social platforms are experiencing fast development, the volume of image-text content generated by users has grown rapidly. Image-text based sentiment of social media analysis has also attracted great interest from researchers in recent years. The main challenge of image-text sentiment analysis is how to construct a model that can promote the complementarity between image and text. In most previous studies, images and text were simply merged, while the interaction between them was not fully considered. This paper proposes an image-text interaction graph neural network for image-text sentiment analysis. A text-level graph neural network is used to extract the text features, and a pre-trained convolutional neural network is employed to extract the image features. Then, an image-text interaction graph network is constructed. The node features of the graph network are initialized by the text features and the image features, while the node features in the graph are updated based on the graph attention mechanism. Finally, combined with image-text aggregation layer to realize sentiment classification. The results of the experiments prove that the presented method is more effective than existing methods. In addition, a large-scale Twitter image-text sentiment analysis dataset was built by us and used in the experiments.},
  archive      = {J_APIN},
  author       = {Liao, Wenxiong and Zeng, Bi and Liu, Jianqi and Wei, Pengfei and Fang, Jiongkun},
  doi          = {10.1007/s10489-021-02936-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11184-11198},
  shortjournal = {Appl. Intell.},
  title        = {Image-text interaction graph neural network for image-text sentiment analysis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayes-DCGRU with bayesian optimization for rolling bearing
fault diagnosis. <em>APIN</em>, <em>52</em>(10), 11172–11183. (<a
href="https://doi.org/10.1007/s10489-021-02924-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been diffusely used in bearing fault diagnosis. In view of the increasing complexity of the model and the exponential growth of the hyperparameters, the adjustment becomes increasingly difficult. In this paper, a Bayesian optimization-Deep convolution gate recurring unit (Bayes-DCGRU) based on Bayesian optimization is proposed. It adopts Bayesian optimization algorithm to automatically adjust the hyperparameters of the model, and convolutional neural network (CNN) adaptively extracts the spatial characteristics of bearing signals. Combined with the Gate recurring unit (GRU) to learn the time series characteristics of signals, and then achieve high precision bearing fault identification. This method overcomes the shortcomings of the traditional hyperparameter adjustment method based on experience and feeling. It provides a solution for the hyperparameter adjustment of bearing fault diagnosis model under multi-hyperparameter coupling. The experimental results show that the model obtained by this method converges quickly and the fault identification accuracy is higher.},
  archive      = {J_APIN},
  author       = {Jiaocheng, Ma and Jinan, Shang and Xin, Zhao and Peng, Zhong},
  doi          = {10.1007/s10489-021-02924-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11172-11183},
  shortjournal = {Appl. Intell.},
  title        = {Bayes-DCGRU with bayesian optimization for rolling bearing fault diagnosis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven physical law learning model for chaotic robot
dynamics prediction. <em>APIN</em>, <em>52</em>(10), 11160–11171. (<a
href="https://doi.org/10.1007/s10489-021-02902-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robot control system is a multivariable, nonlinear automatic control system, as well as a dynamic coupling system. To address the difficult problem of data prediction under a chaotic system, a data-driven physical law learning model (DPM) is proposed, which can learn the underlying physical rules that the data follow. First, two independent autoencoder neural networks are stacked and merged to explore potential physical rules. Then, a virtual Hamiltonian represented as the sum of kinetic energy and potential energy of chaotic data is introduced. Combined with the Hamiltonian equation, the learned Hamiltonian is transformed into a symplectic transformation, whose first-order differential w.r.t. the generalized coordinates and momentum can be regarded as a time-dependent prediction instead of a direct numerical approximation. Finally, the DPM continuously learns implicit Hamiltonian equations from chaotic data until it can fit the law of phase space motion in a chaotic environment. The experimental results show that the model has a better robot dynamics prediction ability in long-term chaotic systems than the existing SOTA methods.},
  archive      = {J_APIN},
  author       = {Qian, Kui and Tian, Lei},
  doi          = {10.1007/s10489-021-02902-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11160-11171},
  shortjournal = {Appl. Intell.},
  title        = {Data-driven physical law learning model for chaotic robot dynamics prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). LightNet+: A dual-source lightning forecasting network with
bi-direction spatiotemporal transformation. <em>APIN</em>,
<em>52</em>(10), 11147–11159. (<a
href="https://doi.org/10.1007/s10489-021-03089-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightning disaster causes a huge threat to human lives and industrial facilities. Data-driven lightning forecasting plays an effective role in alleviating such disaster losses. The forecasting process usually faces multi-source meteorological data characterized by spatiotemporal structure. However, established data-driven forecasting methods are mostly built on classic convolutional and recurrent neural blocks which processes one local neighborhood at a time, failing to capture long-range spatiotemporal dependencies within data. To address this issue, we propose a dual-source lightning forecasting network with bi-direction spatiotemporal transformation, referred to as LightNet $$+$$ . The core of LightNet $$+$$ is a novel module, namely bi-directional spatiotemporal propagator, which aims to model long-range connections among different spatiotemporal locations, going beyond the constraints of the receptive field of a local neighborhood. Moreover, a spatiotemporal encoder is introduced to extract historical trend information from recent observation data. Finally, all the obtained features are organically fused via a non-local spatiotemporal decoder, which then produces final forecasting results. We evaluate LightNet $$+$$ on a real-world lightning dataset from North China and compare it with several state-of-the-art data-driven lightning forecasting methods. Experimental results show that the proposed LightNet $$+$$ yields overall best performance.},
  archive      = {J_APIN},
  author       = {Zhou, Xinyuan and Geng, Yangli-ao and Yu, Haomin and Li, Qingyong and Xu, Liangtao and Yao, Wen and Zheng, Dong and Zhang, Yijun},
  doi          = {10.1007/s10489-021-03089-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11147-11159},
  shortjournal = {Appl. Intell.},
  title        = {LightNet+: A dual-source lightning forecasting network with bi-direction spatiotemporal transformation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute and label distribution driven multi-label active
learning. <em>APIN</em>, <em>52</em>(10), 11131–11146. (<a
href="https://doi.org/10.1007/s10489-021-03086-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning, each instance is simultaneously associated with multiple class labels. A large number of labels in an application exacerbates the problem of label scarcity. An interesting issue concerns how to query as few labels as possible while obtaining satisfactory classification accuracy. For this purpose, we propose the attribute and label distribution driven multi-label active learning (MCAL) algorithm. MCAL considers the characteristics of both attributes and labels to enable the selection of critical instances based on different measures. Representativeness is measured by the probability density function obtained by non-parametric estimation, while informativeness is measured by the bilateral softmax predicted entropy. Diversity is measured by the distance metric among instances, and richness is measured by the number of softmax predicted labels. We describe experiments performed on eight benchmark datasets and eleven real Yahoo webpage datasets. The results verify the effectiveness of MCAL and its superiority over state-of-the-art multi-label algorithms and multi-label active learning algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Min and Feng, Tingting and Shan, Zhaohui and Min, Fan},
  doi          = {10.1007/s10489-021-03086-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11131-11146},
  shortjournal = {Appl. Intell.},
  title        = {Attribute and label distribution driven multi-label active learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A feature-level full-reference image denoising quality
assessment method based on joint sparse representation. <em>APIN</em>,
<em>52</em>(10), 11115–11130. (<a
href="https://doi.org/10.1007/s10489-021-03052-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes feature-level full-reference image denoising quality metrics based on a joint sparse representation model. By decomposing a denoised image and its clean reference jointly and sparsely with a specific learning dictionary, our method measures the denoising quality from two contradictory perspectives, i.e., the detail preservation capability and noise suppression capability, which determine the denoising quality together, in an image feature space. This novel multiperspective method can not only measure the performance of denoising algorithms accurately but also provide a unique means for investigating denoising characteristics in a learning feature space. In the experiments, nine representative denoising methods and six widely used full-reference objective metrics were employed to verify the effectiveness of our method. In addition, the denoising influences exerted on dictionary atoms are investigated in depth, and several statistical conclusions are reported. Furthermore, our work also provides a new feasible assessment framework for other image recovery and generation tasks.},
  archive      = {J_APIN},
  author       = {Hu, Yanxiang and Zhang, Bo and Zhang, Ya and Jiang, Chuan and Chen, Zhijie},
  doi          = {10.1007/s10489-021-03052-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11115-11130},
  shortjournal = {Appl. Intell.},
  title        = {A feature-level full-reference image denoising quality assessment method based on joint sparse representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). DLIR: A deep learning-based initialization recommendation
algorithm for trust-aware recommendation. <em>APIN</em>,
<em>52</em>(10), 11103–11114. (<a
href="https://doi.org/10.1007/s10489-021-03039-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In collaborative filtering recommendations, a good local minimum depends largely on the initialization of the latent feature vectors of users and items. However, many existing methods initialize them by random initialization or zero ones, which results in recommendation performance degradation. In addition, they also ignore the role of users’ trust and items’ information in obtaining reliable recommendations. Aiming at addressing these challenges, this paper proposes a deep learning-based initialization recommendation method for trust-aware recommendation, named DLIR. In DLIR framework, we first leverage deep learning to learn a better latent feature vector of users and items and then take it as the initial value of the latent factor model (LFM) recommendations. Next, the users’ trust information and items’ information are employed to construct users’ social trust ensemble and the items’ intrinsic feature relationship, respectively. Finally, the constructed information is integrated into LFM for trust-aware recommendation. To verify the effectiveness of the proposed DLIR, the extensive experiments conducted on two real datasets (e.g., Epinions and Last.fm) show that the proposed approach performs much better than state-of-the-art recommendation algorithms on recommendation accuracy.},
  archive      = {J_APIN},
  author       = {Liu, Taiheng and He, Zhaoshui},
  doi          = {10.1007/s10489-021-03039-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11103-11114},
  shortjournal = {Appl. Intell.},
  title        = {DLIR: A deep learning-based initialization recommendation algorithm for trust-aware recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fuzzy rough set approach to hierarchical feature selection
based on hausdorff distance. <em>APIN</em>, <em>52</em>(10),
11089–11102. (<a
href="https://doi.org/10.1007/s10489-021-03028-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increases in feature dimensions and the emergence of hierarchical class structures, hierarchical feature selection has become an important data preprocessing step in machine learning. A variety of effective feature selection methods based on granular computing and hierarchical information have been proposed. The fuzzy rough set method is an effective granular computing method for dealing with uncertainty. However, it is time-consuming because the distance calculations are only based on single samples. In this paper, we propose a fuzzy rough set approach using the Hausdorff distance of the sample set for hierarchical feature selection. This integrates the benefits of sample granularity and class hierarchical granularity. Firstly, the general feature selection task is decomposed into coarse-grained and fine-grained tasks according to the hierarchical structure of the data’s semantic labels. This allows a large and difficult classification task to be divided into several small and controllable subtasks. Then, the Hausdorff distance-based fuzzy rough set method is used to select the best feature subset in each coarse- and fine-grained subtask. Unlike single-sample-based distance calculation, Hausdorff distance calculation uses a sample set of different classes. The new model greatly reduces the computational complexity of classification. Finally, we use the top-down support vector machine classifier to experimentally verify the effectiveness of the proposed methods on five hierarchical datasets. Compared with five existing feature selection algorithms in terms of three evaluation metrics, the proposed method provides the highest average accuracy and much lower running time. In particular, on the F194 dataset, our method takes the least time to improve the FH indicator by 2% compared with that of the second-best algorithm.},
  archive      = {J_APIN},
  author       = {Qiu, Zeyu and Zhao, Hong},
  doi          = {10.1007/s10489-021-03028-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11089-11102},
  shortjournal = {Appl. Intell.},
  title        = {A fuzzy rough set approach to hierarchical feature selection based on hausdorff distance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ConsilientSFL: Using preferential voting system to generate
combinatorial ranking metrics for spectrum-based fault localization.
<em>APIN</em>, <em>52</em>(10), 11068–11088. (<a
href="https://doi.org/10.1007/s10489-021-02954-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectrum-based fault localization (SFL) techniques have shown considerable effectiveness in localizing software faults. They leverage a ranking metric to automatically assign suspiciousness scores to certain entities in a given faulty program. However, for some programs, the current SFL ranking metrics lose effectiveness. In this paper, we introduce ConsilientSFL that is served to synthesize a new ranking metric for a given program, based on a customized combination of a set of given ranking metrics. ConsilientSFL can be significant since it demonstrates the usage of voting systems into a software engineering task. First, several mutated, faulty versions are generated for a program. Then, the mutated versions are executed with the test data. Next, the effectiveness of each existing ranking metric is computed for each mutated version. After that, for each mutated version, the computed existing metrics are ranked using a preferential voting system. Consequently, several top metrics are chosen based on their ranks across all mutated versions. Finally, the chosen ranking metrics are normalized and synthesized, yielding a new ranking metric. To evaluate ConsilientSFL, we have conducted experiments on 27 subject programs from Code4Bench and Siemens benchmarks. In the experiments, we found that ConsilientSFL outperformed every single ranking metric. In particular, for all programs on average, we have found performance measures recall, precision, f-measure, and percentage of code inspection, to be nearly 7, 9, 12, and 5 percentages larger than using single metrics, respectively. The impact of this work is twofold. First, it can mitigate the issue with the choice and usage of a proper ranking metric for the faulty program at hand. Second, it can help debuggers find more faults with less time and effort, yielding higher quality software.},
  archive      = {J_APIN},
  author       = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Bagheri, Babak},
  doi          = {10.1007/s10489-021-02954-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11068-11088},
  shortjournal = {Appl. Intell.},
  title        = {ConsilientSFL: Using preferential voting system to generate combinatorial ranking metrics for spectrum-based fault localization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying cardiomegaly in chest x-rays using dual
attention network. <em>APIN</em>, <em>52</em>(10), 11058–11067. (<a
href="https://doi.org/10.1007/s10489-021-02935-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The chest X-ray (CXR) is one of the most commonly available radiological examinations for identifying chest diseases. The application of deep learning methods in computer vision is becoming more and more mature, it provides new methods for automatic analysis of medical images and assisting doctors in high-precision intelligent diagnosis. In this paper, we propose a dual attention network to identify cardiomegaly (CXRDANet) on CXR images. CXRDANet is equipped with channel attention module (CAM) and spatial attention module (SAM), which selectively enhance features highly related to lesion area. We select CXR images of cardiomegaly and normal from ChestX-ray14 and NLM-CXR, without overlapping images, as the training set and the test set. Experimental results show that our method attains the accuracy of 0.9050, the sensitivity of 0.9445, the specificity of 0.8610, the F1 score of 0.9059, the AUC of 0.9588, which is a new state-of-the-art performance. In addition, we apply our method to the multi-label CXR image classification, and its performance has reached an excellent level.},
  archive      = {J_APIN},
  author       = {Chen, Lifang and Mao, Tengfei and Zhang, Qian},
  doi          = {10.1007/s10489-021-02935-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11058-11067},
  shortjournal = {Appl. Intell.},
  title        = {Identifying cardiomegaly in chest x-rays using dual attention network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel decomposition-based ensemble model for short-term
load forecasting using hybrid artificial neural networks. <em>APIN</em>,
<em>52</em>(10), 11043–11057. (<a
href="https://doi.org/10.1007/s10489-021-02864-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly accurate short-term load forecasting (STLF) is essential in the operation of power systems. However, the existing predictive methods cannot achieve an effective balance between prediction accuracy and computational cost. Furthermore, the prediction residual is rarely used to improve the predictive accuracy in STLF. This paper proposes a novel decomposition-based ensemble model for the STLF task. First, an optimized empirical wavelet transform (OEWT) is developed to rationally decompose the STLF load by combining the approximate entropy method with the empirical wavelet transform. Particularly, OEWT improves both prediction accuracy and computational cost in STLF. Second, a new hybrid machine learning method (named master learner) is proposed by rationally combining long short-term memory networks (LSTMs) with broad learning system (BLS) in STLF, effectively strengthening the predictive accuracy without significantly increasing the computational cost. Third, a residual learning model (named residual learner) is developed in the master learner to extract the effective predictive information from residual results, further improving the prediction accuracy in STLF. Fourth, an auxiliary learner is proposed by introducing another BLS to connect the input and output of the proposed model, enhancing the predictive robustness. The proposed decomposition-based ensemble model is compared with state-of-the-art and traditional models in STLF. Experimental results show that the model not only has high predictive accuracy and robustness but also low computational cost.},
  archive      = {J_APIN},
  author       = {Liao, Zhiyuan and Huang, Jiehui and Cheng, Yuxin and Li, Chunquan and Liu, Peter X.},
  doi          = {10.1007/s10489-021-02864-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11043-11057},
  shortjournal = {Appl. Intell.},
  title        = {A novel decomposition-based ensemble model for short-term load forecasting using hybrid artificial neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired feature enhancement network for edge detection.
<em>APIN</em>, <em>52</em>(10), 11027–11042. (<a
href="https://doi.org/10.1007/s10489-022-03202-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the basis of mid-level and high-level vision tasks, edge detection has great significance in the field of computer vision. Edge detection methods based on deep learning usually adopt the structure of the encoding-decoding network, among which the deep convolutional neural network is generally adopted in the encoding network, and the decoding network is designed by researchers. In the design of the encoding-decoding network, researchers pay more attention to the design of the decoding network and ignore the influence of the encoding network, which makes the existing edge detection methods have the problems of weak feature extraction ability and insufficient edge information extraction. To improve the existing methods, this work combines the information transmission mechanism of the retina/lateral geniculate nucleus with an edge detection network based on convolutional neural network and proposes a bionic feature enhancement network. It consists of a pre-enhanced network, an encoding network, and a decoding network. By simulating the information transfer mechanism of the retina/lateral geniculate nucleus, we designed the pre-enhanced network to enhance the ability of the encoding network to extract details and local features. Based on the hierarchical structure of the visual pathway and the integrated feature function of the inferior temporal (IT) cortex, we designed a novel feature fusion network as a decoding network. In a feature fusion network, a down-sampling enhancement module is introduced to boost the feature integration ability of the decoding network. Experimental results demonstrate that we achieve state-of-the-art performance on several available datasets.},
  archive      = {J_APIN},
  author       = {Lin, Chuan and Zhang, Zhenguang and Hu, Yihua},
  doi          = {10.1007/s10489-022-03202-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {11027-11042},
  shortjournal = {Appl. Intell.},
  title        = {Bio-inspired feature enhancement network for edge detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Harris hawk optimization algorithm based on cauchy
distribution inverse cumulative function and tangent flight operator.
<em>APIN</em>, <em>52</em>(10), 10999–11026. (<a
href="https://doi.org/10.1007/s10489-021-03080-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harris Hawk Optimization (HHO) algorithm is a new population-based and nature-inspired optimization paradigm, which has strong global search ability, but its diversified local search strategies easily make it fall into local optimum. In order to enhance its search mechanism and speed of convergence, an new improved HHO algorithm based on the inverse cumulative function operator of Cauchy distribution and tangent flight operator was proposed. The proposed two operators are used as scale factors to control the step size. The walk path of Cauchy inverse cumulative integral function shows that its trajectory step length is relative to the average, which can further enhance the search stability of the algorithm. The Tangent flight has the function of balanced exploitation and exploration, and enhances the convergence ability of the algorithm. In order to verify the performance of the proposed algorithm, the 30 benchmark functions of the 2017 Institute of Electrical and Electronic Engineers (IEEE) Conference on Evolutionary Computation (CEC2017) and two practical engineering design problems are adopted to carry out the simulation experiments. On the other hand, the covariance matrix adaptation evolutionary strategies (CMA-ES), arithmetic optimization algorithm (AOA), butterfly optimization algorithm (BOA), bat algorithm (BA), whale optimization algorithm (WOA), sine cosine algorithm (SCA), and the proposed HHO algorithms were used for comparison experiments. Simulation results show that the proposed the Cauchy-distribution and Tangent-Flight Harris Hawk Optimization (CTHHO) Algorithm has strong optimization capability.},
  archive      = {J_APIN},
  author       = {Wang, Min and Wang, Jie-Sheng and Li, Xu-Dong and Zhang, Min and Hao, Wen-Kuo},
  doi          = {10.1007/s10489-021-03080-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10999-11026},
  shortjournal = {Appl. Intell.},
  title        = {Harris hawk optimization algorithm based on cauchy distribution inverse cumulative function and tangent flight operator},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image encryption using permutation generated by modified
regula-falsi method. <em>APIN</em>, <em>52</em>(10), 10979–10998. (<a
href="https://doi.org/10.1007/s10489-021-03063-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission and sharing of multimedia data have drastically increased in the last few years due to the availability of low-cost image capturing devices, development of communication technology, and the popularity of social networks. However several security fissure of a public network such as the Internet has made the jobs of eavesdroppers easy to grab the contents without any impediment. Well-known cipher techniques like DES, AES, RSA can be used to encrypt an image. But, due to the huge volume and high correlation of image data, a lightweight image encryption method is important. Permutation-based encryption methods disrupt the correlation and can act well with huge volumes. Chaos theory is proved to generate pseudo-random sequences and is extensively used to define permutation. A wide range of image encryption proposals based on the permutation defined by the chaotic map is found in the literature. Several non-chaotic techniques are also gaining popularity for defining a permutation. An image encryption proposal based on a non-chaotic method is presented in this current communication. The permutation is defined by the modified Regula-Falsi method and image encryption is achieved by pixel value substitution and iterative addition with the cyclic shift. As the result of the proposed method, fully noisy images are obtained. Security analysis has proved immunity against different attacks. Comparison with state-of-the-art methods has established the applicability of the proposed technique in image encryption.},
  archive      = {J_APIN},
  author       = {Paul, Aakash and Kandar, Shyamalendu and Dhara, Bibhas Chandra},
  doi          = {10.1007/s10489-021-03063-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10979-10998},
  shortjournal = {Appl. Intell.},
  title        = {Image encryption using permutation generated by modified regula-falsi method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lie group continual meta learning algorithm. <em>APIN</em>,
<em>52</em>(10), 10965–10978. (<a
href="https://doi.org/10.1007/s10489-021-03036-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can use acquired experience to learn new skills quickly and without forgetting the knowledge they already have. However, the neural network cannot do continual learning like humans, because it is easy to fall into the stability-plasticity dilemma and lead to catastrophic forgetting. Since meta-learning with the already acquired knowledge as a priori can directly optimize the final goal, this paper proposes LGCMLA (Lie Group Continual Meta Learning Algorithm) based on meta-learning, this algorithm is an improvement of CMLA (Continual Meta Learning Algorithm) proposed by Jiang et al. On the one hand, LGCMLA enhances the continuity between tasks by changing the inner-loop update rule (from using random initialization parameters for each task to using the updated parameters of the previous task for the subsequent task). On the other hand, it uses orthogonal groups to limit the parameter space and adopts the natural Riemannian gradient descent to accelerate the convergence speed. It not only corrects the shortcomings of poor convergence and stability of CMLA, but also further improves the generalization performance of the model and solves the stability-plasticity dilemma more effectively. Experiments on miniImageNet, tieredImageNet and Fewshot-CIFAR100 (Canadian Institute For Advanced Research) datasets prove the effectiveness of LGCMLA. Especially compared to MAML (Model-Agnostic Meta-Learning) with standard four-layer convolution, the accuracy of 1 shot and 5 shot is improved by 16.4% and 17.99% respectively under the setting of 5-way on miniImageNet.},
  archive      = {J_APIN},
  author       = {Jiang, Mengjuan and Li, Fanzhang},
  doi          = {10.1007/s10489-021-03036-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10965-10978},
  shortjournal = {Appl. Intell.},
  title        = {Lie group continual meta learning algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning models for predictive maintenance: A survey,
comparison, challenges and prospects. <em>APIN</em>, <em>52</em>(10),
10934–10964. (<a
href="https://doi.org/10.1007/s10489-021-03004-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the growing amount of industrial data in the 4th industrial revolution, deep learning solutions have become popular for predictive maintenance (PdM) tasks, which involve monitoring assets to anticipate their requirements and optimise maintenance tasks. However, given the large variety of such tasks in the literature, choosing the most suitable architecture for each use case is difficult. This work aims to facilitate this task by reviewing various state-of-the-art deep learning (DL) architectures and analysing how well they integrate with predictive maintenance stages to meet industrial companies’ requirements from a PdM perspective. This review includes a self-organising map (SOM), one-class neural network (OC-NN) and generative techniques. This article explains how to adapt DL architectures to facilitate data variability handling, model adaptability and ensemble learning, all of which are characteristics relevant to industrial requirements. In addition, this review compares the results of state-of-the-art DL architectures on a publicly available dataset to facilitate reproducibility and replicability, enabling comparisons. Furthermore, this work covers the mitigation step with deep learning models, the final PdM stage that is essential for implementing PdM systems. Moreover, state-of-the-art deep learning architectures are categorised, analysed and compared; their industrial applications are presented; and an explanation of how to combine different architectures in a solution is presented that addresses their gaps. Finally, open challenges and possible future research paths are presented and supported in this review, and current research trends are identified.},
  archive      = {J_APIN},
  author       = {Serradilla, Oscar and Zugasti, Ekhi and Rodriguez, Jon and Zurutuza, Urko},
  doi          = {10.1007/s10489-021-03004-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10934-10964},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning models for predictive maintenance: A survey, comparison, challenges and prospects},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot domain adaptation through compensation-guided
progressive alignment and bias reduction. <em>APIN</em>,
<em>52</em>(10), 10917–10933. (<a
href="https://doi.org/10.1007/s10489-021-02987-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing domain adaptation methods require large amounts of data in the target domain to train the model and a relatively long time to adapt different domains. Recently, few-shot domain adaptation (FDA) attracts lots of research attention, which only requires a small number of labeled target data and is more consistent with real-world applications. Previous works on FDA suffer from the risk of bias towards source domain and over-adapting on the target training data, which decreases the generalization of the model on the test data. In this paper, we propose a generalized framework to handle few-shot domain adaptation, named as compensation-guided progressive alignment and bias reduction (CPABR). Specifically, CPABR introduces source and target virtual data as compensations to deal with the scarcity of target data explicitly and fill in the gap between source and target domains, which promotes knowledge transfer. With the help of these virtual data, CPABR performs progressively distribution matching to gradually align the marginal and conditional distributions, and conducts weighted variance maximization to alleviate the bias of the model to the source domain. Moreover, CPABR integrates both homogeneous FDA and heterogeneous FDA into a unified framework. Extensive experiments on widely used benchmark datasets demonstrate the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Shang, Junyuan and Niu, Chang and Huang, Junchu and Zhou, Zhiheng and Yang, Junmei and Xu, Shiting and Yang, Liu},
  doi          = {10.1007/s10489-021-02987-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10917-10933},
  shortjournal = {Appl. Intell.},
  title        = {Few-shot domain adaptation through compensation-guided progressive alignment and bias reduction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-way enhanced part-aware network for fine-grained
sketch-based image retrieval. <em>APIN</em>, <em>52</em>(10),
10901–10916. (<a
href="https://doi.org/10.1007/s10489-021-02960-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based image retrieval is of import practical significance in today’s world populated by smart touch screen devices. Fine-grained sketch-based image retrieval (FG-SBIR) is particularly challenging and uses characteristic of free-hand sketches to retrieve natural photos at the instance level. From outline and semantic perspectives, a free-hand sketch may have many natural photos corresponding to it, we call the relationship “one-to-many”, which means that the effectiveness of FG-SBIR mainly depends on the quality of fine-grained information extracted. Existing deep convolutional neural network (DCNN) models for FG-SBIR commonly use coarse or first-order attention modules to focus on specific local regions, yet cannot capture high-order or complex information and the subtle differences between sketch–photo pairs. It is widely known that the features learned from higher layers of the network are more abstract and of a higher semantic level compared to those learned from the lower layers, but lose some important fine-grained information. To address these limitations, this paper proposes a three-way enhanced part-aware network (EPAN), in which a mixed high-order attention module is added after the middle-level feature space to generate a variety of high-order attention maps and capture rich features contained in the middle convolutional layer. An enhanced part-aware module is proposed to capture useful part cues and enhance the semantic consistency of local regions. This allows for learning more discriminative cross-domain feature representations. A larger number of experiments on several popular datasets demonstrate that our model is superior to state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Wang, Xiuying and Tang, Jun and Tan, Shoubiao},
  doi          = {10.1007/s10489-021-02960-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10901-10916},
  shortjournal = {Appl. Intell.},
  title        = {Three-way enhanced part-aware network for fine-grained sketch-based image retrieval},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partial person re-identification using a pose-guided
alignment network with mask learning. <em>APIN</em>, <em>52</em>(10),
10885–10900. (<a
href="https://doi.org/10.1007/s10489-021-02928-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial person re-identification is a challenging task, in which only a partial observation of a person is available. There is severe misalignment when directly comparing a partial image with the holistic image, which leads to performance degradation with re-identification algorithms. In this paper, we propose a pose-guided alignment and mask learning network (PMN) to solve the problems of large parts missing and significant pedestrian misalignment. The proposed model includes a pose-guided spatial transformer (PST) module and a masked feature extractor. The PST module samples an affine transformed image from a holistic/partial image to align the pedestrian image with a standard pose. The masked feature extractor, which consists of a backbone network and a mask learning branch (MLB), is designed to learn the visibility of body parts to select effective features. The experimental results on two reported partial person benchmarks show that the proposed method achieves competitive performance compared to that of state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Qiu, Qilu and Zhao, Jieyu and Zheng, Ye},
  doi          = {10.1007/s10489-021-02928-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10885-10900},
  shortjournal = {Appl. Intell.},
  title        = {Partial person re-identification using a pose-guided alignment network with mask learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning graph-constrained cascade regressors for single
image super-resolution. <em>APIN</em>, <em>52</em>(10), 10867–10884. (<a
href="https://doi.org/10.1007/s10489-021-02904-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning cascade regression has been shown an effective strategy to further enhance the perceptual quality of resulted high-resolution (HR) images. However, previous cascade regression-based SR methods have two obvious weaknesses: (1)edge structures cannot be preserved well when applying texture features to represent low-resolution (LR) images, and (2)the local manifold structures spanned by the LR-HR feature spaces cannot be revealed by the learned local linear mappings. To alleviate the aforementioned problems, a novel example regression-based super-resolution (SR) approach called learning graph-constrained cascade regressors (LGCCR) is presented, which learns a group of multi-round residual regressors in a unique way. Specifically, we improve the edge preservation capability by synthesizing the whole HR image rather than local image patches, which facilitates to extract the edge features to represent LR images. Moreover, we utilize a graph-constrained regression model to build the local linear regressors, where each local linear regressor responds to an anchored atom in the learned over-complete dictionary. Both quantitative and qualitative quality evaluations on seven benchmark databases indicate the superiority of the proposed LGCCR-based SR approach in comparing with other state-of-the-art SR predecessors.},
  archive      = {J_APIN},
  author       = {Yan, Jianqiang and Zhang, Kaibing and Luo, Shuang and Xu, Jian and Lu, Jian and Xiong, Zenggang},
  doi          = {10.1007/s10489-021-02904-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10867-10884},
  shortjournal = {Appl. Intell.},
  title        = {Learning graph-constrained cascade regressors for single image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-branch attention and alignment network for person
re-identification. <em>APIN</em>, <em>52</em>(10), 10845–10866. (<a
href="https://doi.org/10.1007/s10489-021-02885-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification plays a critical role in video surveillance and has a variety of applications. However, the body misalignment caused by detectors or pose changes sometimes makes it challenging to match features extracted from different images. To address the issues above, we propose a multi-branch attention and alignment network (MAAN). This approach is based on a deep network with three main branches. One branch is used for global feature representations. Another branch implements a multi-attention process based on keypoints, filters the practical information in the image, and then horizontally partitions the image to extract local features. For the last branch, we create a method based on part feature alignment. We obtain 17 keypoints from a pretrained pose estimation model, and nine local regions from the corresponding feature map are extracted for alignment. Experimental results on various popular datasets demonstrate that our method can produce competitive results under posture changes and body misalignment.},
  archive      = {J_APIN},
  author       = {Lyu, Chunyan and Ning, Wu and Wang, Chenhui and Wang, Kejun},
  doi          = {10.1007/s10489-021-02885-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10845-10866},
  shortjournal = {Appl. Intell.},
  title        = {A multi-branch attention and alignment network for person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influence maximization based on community structure and
second-hop neighborhoods. <em>APIN</em>, <em>52</em>(10), 10829–10844.
(<a href="https://doi.org/10.1007/s10489-021-02880-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the spread of Internet and big data research and applications, influence propagation in networks becomes one of the hot topics in the field of social network analysis in recent years. Influence Maximization (IM), which selects a set of k seeds from a network to maximize the expected number of influenced nodes, has been extensively studied due to its immense application potential and enormous technical challenges. In this paper, we present a new heuristic method named IMCAN (I nfluence M aximization based on C ommunity A nd Second-hop N eighbors), which makes utilization of community structure to select the seed nodes. We employ two efficient community detection algorithms, FastQ and LPA, to extract multiple community structures first, then calculate an influence score for each node by considering the average number of its adjacent communities and its influences among its first- and second-order neighbors. After that, we select the node with the largest influence score as a seed, remove its direct neighbors from the candidate set, attenuate its second-order neighbors’ influence scores, and then choose the node with the largest influence score as the next seed. This procedure is repeated until all the k seeds are selected. The experiments on some real-world networks show that IMCAN can extract the seeds with the larger propagation abilities from networks, it outperforms the comparison algorithms significantly.},
  archive      = {J_APIN},
  author       = {Cheng, Jianjun and Yang, Ke and Yang, Zeyi and Zhang, Handong and Zhang, Wenbo and Chen, Xiaoyun},
  doi          = {10.1007/s10489-021-02880-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {10},
  pages        = {10829-10844},
  shortjournal = {Appl. Intell.},
  title        = {Influence maximization based on community structure and second-hop neighborhoods},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traffic density estimation via a multi-level feature fusion
network. <em>APIN</em>, <em>52</em>(9), 10417–10429. (<a
href="https://doi.org/10.1007/s10489-022-03188-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic density estimation plays a positive role in improving the traffic efficiency of contemporary cities. Accurate estimation of traffic flow density can provide effective information for traffic dispatching command and effectively alleviate road traffic congestion. However, due to the problems of perspective distortion, scale change, serious occlusion and background interference in traffic video images, it brings great challenges to traffic density estimation. To solve the above problems, this paper constructs a traffic density estimation network based on multi-level fusion network (MFNet). Firstly, the low-level feature map and high-level feature map after Depthwide convolution Block (DCB) are combined to fuse features of different scales, which solves the problems of perspective distortion and scale change in the image; Then, the fused feature map is sent to the channel attention mechanism module to realize the smooth transition between pixels; Finally, by restoring the location information of the vehicle space on the density map and combining with the estimated number of vehicles, the traffic density is calculated quantitatively. In addition, our network also uses multiple superimposed dilated convolutions to obtain high-quality density map. Experimental results show that the Gride Average Mean absolute Error (GAME) metric of the proposed method is reduced to 14.32 on the TRANCOS dataset. Compared with the existing traffic density estimation methods, the estimation accuracy is significantly improved, especially in the case of serious perspective distortion and vehicle height overlap.},
  archive      = {J_APIN},
  author       = {Hu, Ying-Xiang and Jia, Rui-Sheng and Li, Yong-Chao and Zhang, Qi and Sun, Hong-Mei},
  doi          = {10.1007/s10489-022-03188-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10417-10429},
  shortjournal = {Appl. Intell.},
  title        = {Traffic density estimation via a multi-level feature fusion network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIRA: Scale illumination rotation affine invariant mask
r-CNN for pedestrian detection. <em>APIN</em>, <em>52</em>(9),
10398–10416. (<a
href="https://doi.org/10.1007/s10489-021-03073-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we resolve the challenging obstacle of detecting pedestrians with the ubiquity of irregularities in scale, rotation, and the illumination of the natural scene images natively. Pedestrian instances with such obstacles exhibit significantly unique characteristics. Thus, it strongly influences the performance of pedestrian detection techniques. We propose the new robust Scale Illumination Rotation and Affine invariant Mask R-CNN (SIRA M-RCNN) framework for overcoming the predecessor’s difficulties. The first phase of the proposed system deals with illumination variation by histogram analysis. Further, we use the contourlet transformation, and the directional filter bank for the generation of the rotational invariant features. Finally, we use Affine Scale Invariant Feature Transform (ASIFT) to find points that are translation and scale-invariant. Extensive evaluation of the benchmark database will prove the effectiveness of SIRA M-RCNN. The experimental results achieve state-of-the-art performance and show a significant performance improvement in pedestrian detection.},
  archive      = {J_APIN},
  author       = {Gawande, Ujwalla and Hajari, Kamal and Golhar, Yogesh},
  doi          = {10.1007/s10489-021-03073-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10398-10416},
  shortjournal = {Appl. Intell.},
  title        = {SIRA: Scale illumination rotation affine invariant mask R-CNN for pedestrian detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Truss optimization with natural frequency constraints using
generalized normal distribution optimization. <em>APIN</em>,
<em>52</em>(9), 10384–10397. (<a
href="https://doi.org/10.1007/s10489-021-03051-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The newly proposed Generalized Normal Distribution Optimization (GNDO) algorithm is used to design the truss structures with optimal weight. All trusses optimized have frequency constraints, which make them very challenging optimization problems. A large number of locally optimal solutions and non-convexity of search space make them difficult to solve, therefore, they are suitable for testing the performance of optimization algorithm. This work investigates whether the proposed algorithm is capable of coping with such problems. To evaluate the GNDO algorithm, three benchmark truss optimization problems are considered with frequency constraints. Numerical data show GNDO’s reliability, stability, and efficiency for structural optimization problems than other meta-heuristic algorithms. We thoroughly analyse and investigate the performance of GNDO in this engineering area for the first time in the literature.},
  archive      = {J_APIN},
  author       = {Khodadadi, Nima and Mirjalili, Seyedali},
  doi          = {10.1007/s10489-021-03051-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10384-10397},
  shortjournal = {Appl. Intell.},
  title        = {Truss optimization with natural frequency constraints using generalized normal distribution optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic consistency generative adversarial network for
cross-modality domain adaptation in ultrasound thyroid nodule
classification. <em>APIN</em>, <em>52</em>(9), 10369–10383. (<a
href="https://doi.org/10.1007/s10489-021-03025-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional networks have been widely used for various medical image processing tasks. However, the performance of existing learning-based networks is still limited due to the lack of large training datasets. When a general deep model is directly deployed to a new dataset with heterogeneous features, the effect of domain shifts is usually ignored, and performance degradation problems occur. In this work, by designing the semantic consistency generative adversarial network (SCGAN), we propose a new multimodal domain adaptation method for medical image diagnosis. SCGAN performs cross-domain collaborative alignment of ultrasound images and domain knowledge. Specifically, we utilize a self-attention mechanism for adversarial learning between dual domains to overcome visual differences across modal data and preserve the domain invariance of the extracted semantic features. In particular, we embed nested metric learning in the semantic information space, thus enhancing the semantic consistency of cross-modal features. Furthermore, the adversarial learning of our network is guided by a discrepancy loss for encouraging the learning of semantic-level content and a regularization term for enhancing network generalization. We evaluate our method on a thyroid ultrasound image dataset for benign and malignant diagnosis of nodules. The experimental results of a comprehensive study show that the accuracy of the SCGAN method for the classification of thyroid nodules reaches 94.30%, and the AUC reaches 97.02%. These results are significantly better than the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhao, Jun and Zhou, Xiaosong and Shi, Guohua and Xiao, Ning and Song, Kai and Zhao, Juanjuan and Hao, Rui and Li, Keqin},
  doi          = {10.1007/s10489-021-03025-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10369-10383},
  shortjournal = {Appl. Intell.},
  title        = {Semantic consistency generative adversarial network for cross-modality domain adaptation in ultrasound thyroid nodule classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFE-MERT: Imbalanced text classification with abstract
feature extraction. <em>APIN</em>, <em>52</em>(9), 10352–10368. (<a
href="https://doi.org/10.1007/s10489-021-02983-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem occurs when the distribution among classes is not balanced. This can be a problem that causes classifier models to bias toward classes with many training samples. The class imbalance problem is inherent in text classification. The abstract feature extraction method is a versatile term weighting scheme. It serves not only as a feature extractor to form a structural form from unorganized text data but also as a dimension reduction technique and classifier. In this study, we tackle the problem of class imbalance in abstract feature extraction. The proposed method utilizes relative imbalance ratio as a factor to elevate the representation of minority classes. Besides, we also integrate relevant term factors to boost the general accuracy. Experiments conducted with three different data sets, one of which is collected for this study, show that the original abstract feature extraction method indeed suffers from the class imbalance problem and the proposed methods demonstrate significant improvements in terms of f1-micro, f1-macro, and Matthew’s correlation coefficient. The experimental results also suggest that the proposed method is a competitive classifier and term weighting scheme when compared to the well-known classifiers (KNN, SVM, and Nearest Centroid) and term weighting schemes (TF-IDF, TF-ICF, TF-ICSDF, TF-RF, TF-PROB, TF-IGM, and TF-MONO).},
  archive      = {J_APIN},
  author       = {Okkalioglu, Murat and Okkalioglu, Burcu Demirelli},
  doi          = {10.1007/s10489-021-02983-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10352-10368},
  shortjournal = {Appl. Intell.},
  title        = {AFE-MERT: Imbalanced text classification with abstract feature extraction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter identification of robot manipulators with unknown
payloads using an improved chaotic sparrow search algorithm.
<em>APIN</em>, <em>52</em>(9), 10341–10351. (<a
href="https://doi.org/10.1007/s10489-021-02972-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter identification is essential for the model-based high-accuracy control of robot manipulators. The objective of this work is to develop a swarm intelligence-based technique to estimate distinct system parameters and to promote the control accuracy of robot manipulators. For this purpose, an improved chaotic sparrow search algorithm (ICSSA) integrated with Kent chaotic mapping, Student’s t-distribution, and the Lévy flight strategy is implemented based on the basic sparrow search algorithm (SSA). Benefitting from the unique advantages in local and global optimization of the ICSSA, the inertial and friction parameters of a two-link robot manipulator with unknown payloads are estimated via simulation experiments. The estimated parameters are analyzed and compared with other popular advanced optimization methods and the classic least square method. The results demonstrate that the ICSSA provides more competitive results over other popular optimization algorithms. It might offer another promising approach of high-level accuracy for advanced control techniques in industry robot manipulators.},
  archive      = {J_APIN},
  author       = {Li, Xingjia and Gu, Jinan and Sun, Xiaohong and Li, Jing and Tang, Shixi},
  doi          = {10.1007/s10489-021-02972-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10341-10351},
  shortjournal = {Appl. Intell.},
  title        = {Parameter identification of robot manipulators with unknown payloads using an improved chaotic sparrow search algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A disease network-based recommender system framework for
predictive risk modelling of chronic diseases and their comorbidities.
<em>APIN</em>, <em>52</em>(9), 10330–10340. (<a
href="https://doi.org/10.1007/s10489-021-02963-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of chronic diseases and their comorbidities is an essential task in healthcare, aiming to predict patients’ future disease risk based on their previous medical records. The accumulation of administrative data has laid a solid foundation for applying deep learning approaches in healthcare. Existing studies focused on the patients’ characteristics such as gender, age and location to predict the risk of the different diseases. However, there are high dimensional, incomplete and noisy problems in the administrative data. In this research, using administrative health data, we implemented graph theory and content-based recommender system approaches to analyse and predict chronic diseases and their comorbidities. Firstly, we used bipartite graphs to represent the relationships between patients and diseases. Then, we projected this graph to a one-mode graph, namely ‘disease network’. After that, six recommender system models with patient features and network features were trained. The outputs of these models are the severity levels of diseases and the predicted diseases with rank. Finally, we evaluated the performance of these models against the same models without network features. The results demonstrated that the models with network features have lower prediction error and better performances for predicting chronic diseases and their latent comorbidities on large administrative data. Among these models, the graph convolution matrix completion model reveals the least amount of error and the best performance for prediction. Further, using a case study of a specific patient, we demonstrated the application of these models in predictive disease risk analysis. Thus, this study showed the potential application of the recommender system approaches to the health sector utilising administrative claim data, which could significantly contribute to healthcare services and stakeholders.},
  archive      = {J_APIN},
  author       = {Lu, Haohui and Uddin, Shahadat},
  doi          = {10.1007/s10489-021-02963-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10330-10340},
  shortjournal = {Appl. Intell.},
  title        = {A disease network-based recommender system framework for predictive risk modelling of chronic diseases and their comorbidities},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault diagnosis of diesel engine information fusion based on
adaptive dynamic weighted hybrid distance-taguchi method (ADWHD-t).
<em>APIN</em>, <em>52</em>(9), 10307–10329. (<a
href="https://doi.org/10.1007/s10489-021-02962-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the case of the fuzzy correlation between variables of faults due to the complex monitoring information of diesel engines, neither Mahalanobis distance (MD) nor Euclidean distance (ED) can effectively diagnose by features, in this paper, a novel Adaptive Dynamic Weighted Hybrid Distance-Taguchi method (ADWHD-T) is presented to diagnose diesel engine faults. The method used Adaptive Dynamic Weighted Hybrid Distance (ADWHD) to fuse data of many sensors into the single system-level performance index. The ADWHD is an adaptive and dynamic weighting of MD and Standardized Euclidean distance (SED). The adaptive weights are adjusted according to the distance scale of MD and SED. The dynamic weight coefficients are calculated by the correlation coefficient of characteristic variables to consider the correlation and independence of characteristic variables. The diagnosis results are derived according to the optimized and adjusted fault threshold of ADWHD defined by 3σ method. In view of the dimension reduction optimization of characteristic variables, combining Taguchi method (T), ADWHD-T provides one systematic method for determining the key parameters of characteristic variables to solve the cost problem of multi-sensor analysis. Aiming at the real-time diagnosis, offline-online modeling and real-time fault diagnosis program based on ADWHD-T are designed. Quoting real-time data from diesel engine benches verifies the effectiveness of the scheme. Compared with MD and MD-T methods, ADWHD-T could promote diagnosis efficiency, enhance classification accuracy and expand its application range in fault diagnosis.},
  archive      = {J_APIN},
  author       = {Liu, Gang and Zhou, Xiaolong and Xu, Xinli and Wang, Longda and Zhang, Weidong},
  doi          = {10.1007/s10489-021-02962-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10307-10329},
  shortjournal = {Appl. Intell.},
  title        = {Fault diagnosis of diesel engine information fusion based on adaptive dynamic weighted hybrid distance-taguchi method (ADWHD-t)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross attention fusion for knowledge graph optimized
recommendation. <em>APIN</em>, <em>52</em>(9), 10297–10306. (<a
href="https://doi.org/10.1007/s10489-021-02930-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph has attracted a wide range of attention in the field of recommendation, which is usually applied as auxiliary information to solve the problem of data sparsity. However, most recommendation models cannot effectively mine the associations between the items to be recommended and the entities in the Knowledge Graph. In this paper, we propose CAKR, a knowledge graph recommendation method based on the cross attention unit, which is similar to MKR, a multi-task feature learning general framework that uses knowledge graph embedding tasks to assist recommendation tasks. Specifically, we design a new method to optimize the feature interaction between the items and the corresponding entities in the Knowledge Graph and propose a feature cross-unit combined with the attention mechanism to enhance the recommendation effect. Through extensive experiments on the public datasets of movies, books, and music, we prove that CAKR is better than MKR and other knowledge graph recommendation methods so that the new feature cross-unit designed in this paper is effective in improving the accuracy of the recommendation system.},
  archive      = {J_APIN},
  author       = {Huang, Weijian and Wu, Jianhua and Song, Weihu and Wang, Zehua},
  doi          = {10.1007/s10489-021-02930-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10297-10306},
  shortjournal = {Appl. Intell.},
  title        = {Cross attention fusion for knowledge graph optimized recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep spatio-temporal neural network based on interactive
attention for traffic flow prediction. <em>APIN</em>, <em>52</em>(9),
10285–10296. (<a
href="https://doi.org/10.1007/s10489-021-02879-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow forecasting is of great significance to urban traffic control and public safety applications. The key challenge of traffic flow forecasting is how to capture the complex correlation of different time levels and learn time dependence. Some external information is closely related to traffic flow, such as accidental traffic accidents, weather, and Point of Interests (PoI) information. This paper proposes a deep learning-based model, called AttDeepSTN+, which is used to predict the inflow and outflow of each area of the entire city. Specifically, AttDeepSTN+ uses the structure of interactive attention and convolution to model the temporal closeness, trend, and periodicity of crowd flow, in the interactive attention layer, learn the importance of closeness to periodicity and trend respectively to model the long-term dependence of time, and then use feature fusion to capture complex correlations at different levels, thereby reducing model prediction accuracy. In addition, PoI information are combined with time factors to express the influence of location attributes on crowd flow, to learn prior knowledge of crowd flow. Finally, a new fusion mechanism is used to fuse the attention layer modules and PoI information and other information together into the module to capture the complex correlation between different levels of features, to predict the final crowd flow in each area, and further improve the prediction accuracy of the model. The New York City crowd flow experiment shows that the model is better than the current state-of-the-art baseline.},
  archive      = {J_APIN},
  author       = {Zeng, Hui and Peng, Zhiying and Huang, XiaoHui and Yang, Yixue and Hu, Rong},
  doi          = {10.1007/s10489-021-02879-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10285-10296},
  shortjournal = {Appl. Intell.},
  title        = {Deep spatio-temporal neural network based on interactive attention for traffic flow prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel intelligent denoising method of ecg signals based on
wavelet adaptive threshold and mathematical morphology. <em>APIN</em>,
<em>52</em>(9), 10270–10284. (<a
href="https://doi.org/10.1007/s10489-022-03182-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to high-frequency noise and low-frequency noise in ECG signals will interfere with the accurate diagnosis of cardiovascular diseases. With the intrinsic mode function (IMF), which is the main component indicators of high-frequency noise and low-frequency noise, this paper proposes an intelligent denoising method of ECG signals based on wavelet adaptive threshold and mathematical morphology. Firstly, this method performs Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) for signals containing noise, and adopts zero-crossing rate to identify IMFs containing high-frequency noise and low-frequency noise. Secondly, according to the discreteness and randomness of IMF containing high-frequency noise, a wavelet adaptive threshold mathematical model is constructed. In this model, with the signal-to-noise ratio (SNR) improvement as the threshold adjustment parameter, the wavelet threshold is modified by niche genetic algorithm, and the optimal solution is obtained after removing high-frequency noise by wavelet decomposition and reconstruction. The waveform of IMF containing low-frequency noise changes slowly and its amplitude is large and it is difficult to remove low-frequency noise. Therefore, mathematical morphology is used to remove low-frequency noise. Finally, the intelligent denoising method of ECG signals is designed by superimposing denoised IMFs. MIT-BIH experiments show that in the process of removing high-frequency noise and low-frequency noise, compared with other denoising methods, the percent root mean square difference (PRD) and SNR improvement of the method proposed in this paper are improved, and the denoising effect is significant, which can provide expert knowledge and decision-making guidance for related application fields.},
  archive      = {J_APIN},
  author       = {Gao, Li and Gan, Yi and Shi, Juncheng},
  doi          = {10.1007/s10489-022-03182-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10270-10284},
  shortjournal = {Appl. Intell.},
  title        = {A novel intelligent denoising method of ecg signals based on wavelet adaptive threshold and mathematical morphology},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). URO-GAN: An untrustworthy region optimization approach for
adipose tissue segmentation based on adversarial learning.
<em>APIN</em>, <em>52</em>(9), 10247–10269. (<a
href="https://doi.org/10.1007/s10489-021-02976-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of adipose tissue from CT images is an essential module of medical assistant diagnosis. A large scale of abdominal cross-section CT images can be used to segment subcutaneous adipose tissue (SAT) and visceral adipose tissue (VAT) with deep learning method. However, the CT images still need to be professionally and accurately annotated to improve the segmentation quality. The paper proposes a semi-supervised segmentation network based on adversarial learning. The model is called URO-GAN and consists of two paths used to segment SAT and VAT, respectively. An SAT-to-VAT transmission mechanism is set up between these two paths, where several inverse-SAT excitation blocks are set to help the SAT segmentation network guide the VAT segmentation network. An untrustworthy region optimization mechanism is proposed to improve the segmentation quality and keep the adversarial learning stable. With the confidence map output from the discriminator network, an optimizer network is used to fix the error in the masks predicted by the segmentation network. The URO-GAN achieves good results by training with 84 annotated images and 3969 unannotated images. Experimental results demonstrate the effectiveness of our approach on the segmentation of adipose tissue in medical images.},
  archive      = {J_APIN},
  author       = {Shen, Kaifei and Quan, Hongyan and Han, Jun and Wu, Min},
  doi          = {10.1007/s10489-021-02976-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10247-10269},
  shortjournal = {Appl. Intell.},
  title        = {URO-GAN: An untrustworthy region optimization approach for adipose tissue segmentation based on adversarial learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video super-resolution network using detail component
extraction and optical flow enhancement algorithm. <em>APIN</em>,
<em>52</em>(9), 10234–10246. (<a
href="https://doi.org/10.1007/s10489-021-02882-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video super-resolution (SR) task refers to the use of corresponding low-resolution (LR) frames and multiple neighboring frames to generate high-resolution (HR) frames. Existing deep learning-based approaches usually utilize LR optical flow for video SR tasks. However, the accuracy of LR optical flow is not enough to recover the fine detail part. In this paper, we propose a video SR network that uses optical flow SR and optical flow enhancement algorithms to provide accurate temporal dependency. And extract the detail component of LR adjacent frames as supplementary information for accurate feature extraction. Firstly, the network infers HR optical flow from LR optical flow, and uses the optical flow enhancement algorithm to enhance HR optical flow. Then the processed HR optical flows are used as the input of the motion compensation network. Secondly, we extract detail component to reduce the error caused by motion compensation based on optical flow. Finally, the SR results are generated through the SR network. We perform comprehensive comparative experiments on two datasets: Vid4 and DAVIS. The results show that, compared with other state-of-the-art methods, the proposed video SR method achieves the better performance.},
  archive      = {J_APIN},
  author       = {Chen, Zhensen and Yang, Wenyuan and Yang, Jingmin},
  doi          = {10.1007/s10489-021-02882-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10234-10246},
  shortjournal = {Appl. Intell.},
  title        = {Video super-resolution network using detail component extraction and optical flow enhancement algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving hypergraph convolution network collaborative
filtering with feature crossing and contrastive learning. <em>APIN</em>,
<em>52</em>(9), 10220–10233. (<a
href="https://doi.org/10.1007/s10489-021-03144-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizing user-item interaction data into a graph has brought many benefits to recommendation methods. Compared with the user-item bipartite graph structure, a hypergraph structure provides a natural way to directly model high-order correlations among users or items. Hypergraph Convolution Network (HGCN) has the capability of aggregating and propagating latent features of nodes in the hypergraph nonlinearly. Recently, recommendation models based on simplified HGCN have shown good performance. However, such models lose the powerful expression ability of feature crossing and suffer from limited labeled data. To tackle these two problems, a framework called HGCN-CC is proposed to improve HGCN with feature Crossing and Contrastive learning. Specifically, HGCN is combined with a feature cross network in a parallel manner to balance between feature crossing and over smoothing. By such a design, HGCN-CC not only utilizes simplified propagation operation in HGCN to capture high-order correlations among users or items, but also enjoys the powerful expressing ability of high-order feature interactions. Furthermore, HGCN-CC resorts to contrastive learning to help learn good representations. Under the HGCN-CC framework, two models called item-based HGCN-CC (I-HGCN-CC) and user-based HGCN-CC (U-HGCN-CC) are constructed to emphasize different aspects of data. Results of extensive experiments on four benchmark datasets demonstrate that proposed models have superiority in modelling hypergraph structure data for recommendations.},
  archive      = {J_APIN},
  author       = {Yuan, Huanhuan and Yang, Jian and Huang, Jiajin},
  doi          = {10.1007/s10489-021-03144-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10220-10233},
  shortjournal = {Appl. Intell.},
  title        = {Improving hypergraph convolution network collaborative filtering with feature crossing and contrastive learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian maximal information coefficient (BMIC) to reason
novel trends in large datasets. <em>APIN</em>, <em>52</em>(9),
10202–10219. (<a
href="https://doi.org/10.1007/s10489-021-03090-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian network (BN) is a probability inference model to describe the explicit relationship between cause and effect, which may be examined in the complex system of rice price with data uncertainty. However, discovering the optimized structure from a super-exponential number of graphs in the search space is an NP-hard problem. In this paper, Bayesian Maximal Information Coefficient (BMIC) is proposed to uncover the causal correlations from a large data set in a random system by integrating probabilistic graphical model (PGM) and maximal information coefficient (MIC) with Bayesian linear regression (BLR). First, MIC is to capture the strong dependence between predictor variables and a target variable to reduce the number of variables for the BN structural learning of PGM. Second, BLR is responsible for assigning orientation in a graph resulting from a posterior probability distribution. It conforms to what BN needs to acquire a conditional probability distribution when given the parents for each node by the Bayes’ Theorem. Third, the Bayesian information criterion (BIC) is treated as an indicator to determine the well-explained model with its data to ensure correctness. The score shows that the proposed BMIC obtains the highest score compared to the two traditional learning algorithms. Finally, the proposed BMIC is applied to discover the causal correlations from the large data set on Thai rice price by identifying the causal changes in the paddy price of Jasmine rice. The results of the experiments show that the proposed BMIC returns directional relationships with clues to identify the cause(s) and effect(s) of paddy price with a better heuristic search.},
  archive      = {J_APIN},
  author       = {Shuliang, Wang and Surapunt, Tisinee},
  doi          = {10.1007/s10489-021-03090-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10202-10219},
  shortjournal = {Appl. Intell.},
  title        = {Bayesian maximal information coefficient (BMIC) to reason novel trends in large datasets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved similarity based prognostics method for turbine
engine degradation with degradation consistency test. <em>APIN</em>,
<em>52</em>(9), 10181–10201. (<a
href="https://doi.org/10.1007/s10489-021-03034-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity-based prediction methods have gained increasing attention in data-driven remaining useful life (RUL) technologies, mainly because of their strong generalization capability in multiple industrial scenarios and simple model update processes. The traditional similarity-based prediction methods obtain the referential prediction instances in the referential degradation instance library, which are similar to the degradation trend of the in-service system. The final RUL is the weighted result of each estimated remaining useful life of the referential prediction instances. The key to these methods is the construction of degradation trajectories and the design of similarity matching rules. This study adopts the unsupervised learning of a convolutional neural network autoencoder with an attention mechanism to accurately construct the degradation trajectories of the system. In addition, a new similarity matching rule is proposed to check the degradation consistency of the predicted referential instances, which can eliminate the prediction referential instances that result in marked prediction errors. The experimental results on the turbine engine datasets showed improved prediction performance and lower sensitivity to the size of the training instances. In addition, the proposed method can be easily transplanted to the original similarity-based framework.},
  archive      = {J_APIN},
  author       = {Xue, Bin and Xu, Fangmin and Huang, Xing and Xu, Zhongbin and Zhang, Xuechang},
  doi          = {10.1007/s10489-021-03034-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10181-10201},
  shortjournal = {Appl. Intell.},
  title        = {Improved similarity based prognostics method for turbine engine degradation with degradation consistency test},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative particle swarm optimizer with depth first search
strategy for global optimization of multimodal functions. <em>APIN</em>,
<em>52</em>(9), 10161–10180. (<a
href="https://doi.org/10.1007/s10489-021-03005-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Cooperative Particle Swarm Optimizer with Depth First Search Strategy (DFS-CPSO), which has better seacrch capality than classical Particle Swarm Optimizer (PSO) in solving multimodal optimization problems. In order to improve the quality of information exchange, the Depth First Search (DFS) strategy is hybridized to Cooperative Particle Swarm Optimization(CPSO), which makes information transfer more effectively and generates better quality solution. Specifically, DFS strategy enables different components of solution vector to exchange information separately with PSO and increases the diversity of the population, so that the information of solution components could be preserved by multiple iterations in CPSO. Confirmatory experiments are performed to prove the effectiveness of employing the DFS strategy to CPSO. The comparative results demonstrate superior performance of DFS-CPSO in solving high dimensional multimodal functions than CPSO and other advanced methods.},
  archive      = {J_APIN},
  author       = {Wang, Jie and Xie, Yongfang and Xie, Shiwen and Chen, Xiaofang},
  doi          = {10.1007/s10489-021-03005-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10161-10180},
  shortjournal = {Appl. Intell.},
  title        = {Cooperative particle swarm optimizer with depth first search strategy for global optimization of multimodal functions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GeSe: Generalized static embedding. <em>APIN</em>,
<em>52</em>(9), 10148–10160. (<a
href="https://doi.org/10.1007/s10489-021-03001-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In natural language processing, most text representation methods can be generally categorized into two paradigms: static and dynamic. Both have distinctive advantages, which are reflected in the cost of training resources, the scale of input data, and the interpretability of the representation model. Dynamic representation methods, such as BERT, have achieved excellent results on many tasks based on expensive pre-training. However, this representation paradigm is black-box, and the intrinsic properties cannot be measured by standard word similarity and analogy benchmarks. Most importantly, it is not in all cases that there are adequate resources and unlimited data to use. While static methods are solid alternatives for these scenarios because they can be efficiently trained with limited resources, keeping straightforward interpretability and verifiable intrinsic properties. Although many static embedding methods have been proposed, few attempts have been made to investigate the connections between these algorithms. Thus, it is natural to ask which implementation is more efficient, and is there any way to combine the merits of these algorithms into a generalized framework? In this paper, we try to explore answers to these questions by focusing on two popular static embedding models, Continual-Bag-of-Words (CBOW) and Skip-gram (SG), with detailed analysis of their merits and drawbacks under both Negative Sampling (NS) and Hierarchy Softmax (HS) settings. Then, we propose a novel learning framework to train generalized static embeddings in a unified architecture. Our proposed method is estimator-agnostic. Thus, it can be optimized by either NS, HS, or any other equivalent estimators. Experiments show that embeddings learned from the proposed framework outperform strong baselines on standard intrinsic evaluations. We also test the proposed method on three extrinsic tasks. Empirical results show that the proposed method achieves considerable improvements across all these tasks.},
  archive      = {J_APIN},
  author       = {Gong, Ning and Yao, Nianmin},
  doi          = {10.1007/s10489-021-03001-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10148-10160},
  shortjournal = {Appl. Intell.},
  title        = {GeSe: Generalized static embedding},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interconnected backlash inverse compensation in neural
decentralized control for switched nonlinear systems. <em>APIN</em>,
<em>52</em>(9), 10135–10147. (<a
href="https://doi.org/10.1007/s10489-021-02996-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study, a tracking control method is addressed when switched interconnected systems are subjected to backlash nonlinearities. For one inequality with multiple smooth inverse models, it is not clear how to establish the boundedness of system states. In order to remove above restriction, a novel interconnected smooth inverse compensator is presented. Then, combined the proposed inverse model with common Lyapunov method, a new adaptive neural decentralized controller is presented to assure stability of system. Eventually, availability of developed control approach can be proven by the examples.},
  archive      = {J_APIN},
  author       = {Chen, Yanxian},
  doi          = {10.1007/s10489-021-02996-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10135-10147},
  shortjournal = {Appl. Intell.},
  title        = {Interconnected backlash inverse compensation in neural decentralized control for switched nonlinear systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective framework using identification and image
reconstruction algorithm for train component defect detection.
<em>APIN</em>, <em>52</em>(9), 10116–10134. (<a
href="https://doi.org/10.1007/s10489-021-02981-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under long-term high-speed movement, the precision components of trains are extremely prone to defects, which could potentially endanger the safe operation of the train. However, there are many types of precision train components prone to defects; they are small in size and difficult to locate accurately. The defects themselves are also highly uncertain and diverse, making it impossible to establish an effective defect database. Meanwhile, a detection algorithm should have a high processing speed to ensure timely maintenance. Within the above context, in this paper an effective framework for multi-type train component defect detection based on an identification and image reconstruction algorithm is proposed. The framework is composed of a component identification stage and a defect diagnosis stage. The component identification method based on a component pre-location algorithm focuses attention on key areas of the train and ensures the visual integrity of the detected components. The component defect diagnosis method is based on an image-similarity generative adversarial network, which allows unsupervised reconstruction of template images to participate in defect diagnosis, thus coping with the diversity of component types and defect conditions effectively. The evaluation results on a CR400BF electric multiple unit series image dataset show that the framework has good robustness in complex environments and better performance in defect detection of train components.},
  archive      = {J_APIN},
  author       = {Zhang, Hao-Dong and Yuan, Xue and Li, Dan-Yong and You, Jia and Liu, Bing and Zhao, Xiao-Ming and Cai, Wen-Ming and Ju, Shan},
  doi          = {10.1007/s10489-021-02981-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10116-10134},
  shortjournal = {Appl. Intell.},
  title        = {An effective framework using identification and image reconstruction algorithm for train component defect detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global-local neighborhood based network representation for
citation recommendation. <em>APIN</em>, <em>52</em>(9), 10098–10115. (<a
href="https://doi.org/10.1007/s10489-021-02964-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many researchers study citation recommendation approaches using network representation recently. It learns low-dimensional vector representation of nodes in a citation network, generates a recommendation list using similarity scores within the obtained node representations. Most of the existing approaches learn network representation by preserving structure information ofthe citation network. However, nodes in a citation network often associated with content information, recent approaches tend to learn each node’s structure and content representations separately, and apply a simple and empirical combination strategy to produce the final node representation vectors which are suboptimal. To solve the above problems, we propose a Global-Local Neighborhoods based Network Representation model, named GLNNR, to integrate network structure and non- structural information, obtaining better node representation in the network. For global neighborhoods, we first generate parallel sequences containing node identity sequences and the corresponding content sequences, then we propose an attention- based sequence to sequence component to obtain node embeddings (the learned hidden representations of encoder) under global neighborhoods. For local neighborhoods, inspired by matrix factorization, a component can be designed to fuse structural and non-structural information in the lower-order neighborhood, here we first build a co-occurrence matrix (adjacent matrix) of the network, and then use multilayer perceptron to learn node representations under the lower-order neighborhood. The final node representations are global-local neighborhood based node embeddings. Empirical experiments prove the effectiveness of the GLNNR on a real-world information citation networks, i.e. CiteSeer and DBLP. Besides, we conduct experiments on a web page citation network, i.e. Wikipedia, to prove extensibility and portability of the proposed model.},
  archive      = {J_APIN},
  author       = {Cai, Xiaoyan and Wang, Nanxin and Yang, Libin and Mei, Xin},
  doi          = {10.1007/s10489-021-02964-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10098-10115},
  shortjournal = {Appl. Intell.},
  title        = {Global-local neighborhood based network representation for citation recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series reconstruction and classification: A
comprehensive comparative study. <em>APIN</em>, <em>52</em>(9),
10082–10097. (<a
href="https://doi.org/10.1007/s10489-021-02926-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series approximation techniques can provide approximate results for the data in another new space by dimensionality reduction or feature extraction. In this study, we propose a new time series approximation strategy based on the Fuzzy C-Means (FCM) clustering and elaborate on a comprehensive analysis of relationships between reconstruction error and classification performance when dealing with various representation (approximation) mechanisms of time series. Typically, time series approximation leads to the representation of original time series in the space of lower dimensionality compared to the dimensionality of the original input space. We reveal, quantify, and visualize the relationships between the reconstruction error and classification error (classification rate) for several commonly encountered representation methods. Through carefully structured experiments completed for sixteen publicly available datasets, we demonstrate experimentally and analytically that the classification error obtained for time series in the developed representation space becomes smaller than when dealing with original time series. It has been also observed that the reconstruction error decreases when increasing the dimensionality of the representation space. In addition, when compared with the state-of-the-art algorithms reported in the literature, experimental results show the efficiency of the proposed approach.},
  archive      = {J_APIN},
  author       = {Li, Jinbo and Pedrycz, Witold and Gacek, Adam},
  doi          = {10.1007/s10489-021-02926-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10082-10097},
  shortjournal = {Appl. Intell.},
  title        = {Time series reconstruction and classification: A comprehensive comparative study},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast algorithm for complex discord searches in time
series: HOT SAX time. <em>APIN</em>, <em>52</em>(9), 10060–10081. (<a
href="https://doi.org/10.1007/s10489-021-02897-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis is quickly proceeding towards long and complex tasks. In recent years, fast approximate algorithms for discord search have been proposed in order to compensate for the increasing size of the time series. It is more interesting, however, to find quick exact solutions. In this research, we improved HOT SAX (Heuristically Ordered Time series using Symbolic Aggregate ApproXimation) by exploiting two main ideas: the warm-up process, and the similarity between sequences close in time. These improvements can reduce the size of the discord search space by orders of magnitude when compared with HOT SAX. The resulting algorithm, called HOT SAX Time (HST), has been validated with real and synthetic time series, and successfully compared with HOT SAX, RRA (Rare Rule Anomaly), SCAMP (SCAlable Matrix Profile), and DADD (Disk Aware Discord Discovery). The complexity of a discord search has been evaluated with a new indicator, the cost per sequence (cps), which allows one to compare searches on time series of different lengths. Numerical evidence suggests that two conditions are involved in determining the complexity of a discord search in a non-trivial way: the length of the discords, and the noise/signal ratio. This is the first exact discord search algorithm that has been demonstrated being more than 100 times faster than HOT SAX.},
  archive      = {J_APIN},
  author       = {Avogadro, Paolo and Dominoni, Matteo Alessandro},
  doi          = {10.1007/s10489-021-02897-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10060-10081},
  shortjournal = {Appl. Intell.},
  title        = {A fast algorithm for complex discord searches in time series: HOT SAX time},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LCRCA: Image super-resolution using lightweight concatenated
residual channel attention networks. <em>APIN</em>, <em>52</em>(9),
10045–10059. (<a
href="https://doi.org/10.1007/s10489-021-02891-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images that are more similar to the original high-resolution images can be generated by deep neural network-based super-resolution methods than the non-learning-based ones, but the huge and sometimes redundant network structure and parameters make them unbearable. To get high-quality super-resolution results in computation resource-limited scenarios, we propose a lightweight skip concatenated residual channel attention network, LCRCA for image super-resolution. Specifically, we design a light but efficient deep residual block (DRB) which can generate more precise residual information by using more convolution layers under the same computation budget. To enhance the feature maps of DRB, an improved channel attention mechanism named statistical channel attention (SCA) is proposed by introducing channel statistics. Besides, compared with the commonly used skip connections, we propose to use skip concatenation (SC) to build information flows for feature maps of different layers. Finally, DRB, SCA, and SC are efficiently used to form the proposed network LCRCA. Experiments on four test sets show that our method can gain up to 3.2 dB and 0.12 dB over the bicubic interpolation and the representative lightweight method FERN, respectively, and can recover image details more accurately than the compared algorithms. Code can be found at https://github.com/pengcm/LCRCA .},
  archive      = {J_APIN},
  author       = {Peng, Changmeng and Shu, Pei and Huang, Xiaoyang and Fu, Zhizhong and Li, Xiaofeng},
  doi          = {10.1007/s10489-021-02891-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10045-10059},
  shortjournal = {Appl. Intell.},
  title        = {LCRCA: Image super-resolution using lightweight concatenated residual channel attention networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving recommender system via knowledge graph based
exploring user preference. <em>APIN</em>, <em>52</em>(9), 10032–10044.
(<a href="https://doi.org/10.1007/s10489-021-02872-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph(KG) has proven to improve recommendation performance. However, most efforts explore inter-entity relatedness by mining multi-hop relations on KG, thus failing to efficiently exploit these relations for enhanced user preference. To address this, we propose an end-to-end framework to improve the recommender system via a knowledge graph based on fusing entity relation(KGFER), which can sufficiently capture the users’ preferences. The model samples from the 1-hop neighbors and relations in KG for the item with which the user interacts, and feeds them into TransR layer. Following this, CNN is employed to learn item features from entity-relations, and then aggregate item features with the interacting item by MLP. Finally, we apply user preference to project the refined item embeddings to the user latent space to predict the potential probability of the target item in which the user is interested in. Extensive experiments on four datasets about book, movie, music, and yelp2018 demonstrate that our approach outperforms state-of-the-art baselines. Also, further experiments show that the user preference matrix indeed makes a great contribution to our approach.},
  archive      = {J_APIN},
  author       = {Fan, Huilian and Zhong, Yuanchang and Zeng, Guangpu and Ge, Chenhao},
  doi          = {10.1007/s10489-021-02872-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10032-10044},
  shortjournal = {Appl. Intell.},
  title        = {Improving recommender system via knowledge graph based exploring user preference},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A relation aware embedding mechanism for relation
extraction. <em>APIN</em>, <em>52</em>(9), 10022–10031. (<a
href="https://doi.org/10.1007/s10489-021-02699-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting possible relational triples from natural language text is a fundamental task of information extraction, which has attracted extensive attention. The embedding mechanism has a significant impact on the performance of relation extraction models, and the embedding vectors should contain rich semantic information that has close relevance to the relation extraction task. Driven by this motivation, we propose a Relation Aware Embedding Mechanism (RA) for relation extraction. In specific, this mechanism incorporates the relation label information into sentence embedding by leveraging the attention mechanism to distinguish the importance of different relation labels to each word of a sentence. We apply the proposed method to three state-of-the-art relation extraction models: CasRel, SMHSA and ETL-Span, and implement the corresponding models named RA-CasRel, RA-SMHSA and RA-ETL-Span. To evaluate the effectiveness of our method, we conduct extensive experiments on two widely-used open datasets: NYT and WebNLG, and compare RA-CasRel, RA-SMHSA and RA-ETL-Span with 12 state-of-the-art models. The experimental results show that our method can effectively improve the performance of relation extraction. For instance, RA-CasRel reaches 91.7% and 92.4% of F1-score on NYT and WebNLG, respectively, which is the best performance among all the compared models. We have open sourced the code of our proposed method in [1] to facilitate future research in relation extraction.},
  archive      = {J_APIN},
  author       = {Li, Xiang and Li, Yuwei and Yang, Junan and Liu, Hui and Hu, Pengjiang},
  doi          = {10.1007/s10489-021-02699-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10022-10031},
  shortjournal = {Appl. Intell.},
  title        = {A relation aware embedding mechanism for relation extraction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-dominated differential context modeling for
context-aware recommendations. <em>APIN</em>, <em>52</em>(9),
10008–10021. (<a
href="https://doi.org/10.1007/s10489-021-03027-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context plays an important role in the process of decision making. A user’s preferences on the items may vary from contexts to contexts, e.g., a user may prefer to watch a different type of the movies, if he or she is going to enjoy the movie with partner rather than with children. Context-aware recommender systems, therefore, were developed to adapt the recommendations to different contextual situations, such as time, location, companion, etc. Differential context modeling is a series of recommendation models which incorporate contextual hybrid filtering into the neighborhood based collaborative filtering approaches. In this paper, we propose to enhance differential context modeling by utilizing a non-dominated user neighborhood. The notion of dominance relation was originally proposed in multi-objective optimization, and it was reused to definite non-dominated user neighborhood in collaborative filtering recently. These non-dominated user neighbors refer to the neighbors that dominate others from different perspectives of the user similarities, such as the user-user similarities based on ratings, demographic information, social relationships, and so forth. In this paper, we propose to identify the non-dominated user neighborhood by exploiting user-user similarities over multiple contextual preferences. Our experimental results can demonstrate the effectiveness of the proposed approaches in comparison with popular context-aware collaborative filtering models over five real-world contextual rating data sets.},
  archive      = {J_APIN},
  author       = {Zheng, Yong},
  doi          = {10.1007/s10489-021-03027-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {10008-10021},
  shortjournal = {Appl. Intell.},
  title        = {Non-dominated differential context modeling for context-aware recommendations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finger vein image inpainting using neighbor
binary-wasserstein generative adversarial networks (NB-WGAN).
<em>APIN</em>, <em>52</em>(9), 9996–10007. (<a
href="https://doi.org/10.1007/s10489-021-03017-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional inpainting methods obtain poor performance for finger vein images with blurred texture. In this paper, a finger vein image inpainting method using Neighbor Binary-Wasserstein Generative Adversarial Networks (NB-WGAN) is proposed. Firstly, the proposed algorithm uses texture loss, reconstruction loss, and adversarial loss to constrain the network, which protects the texture in the inpainting process. Secondly, the proposed NB-WGAN is designed with a coarse-to-precise generator network and a discriminator network composed of two Wasserstein Generative Adversarial Networks with Gradient Penalty (WGAN-GP). The cascade of a coarse generator network and a precise generator network based on Poisson fusion can obtain richer information and get natural boundary connection. The discriminator consists of a global WGAN-GP and a local WGAN-GP, which enforces consistency between the entire image and the repaired area. Thirdly, a training dataset is designed by analyzing the locations and sizes of the damaged finger vein images in practical applications (i.e., physical oil dirt, physical finger molting, etc). Experimental results show that the performance of the proposed algorithm is better than traditional inpainting methods including Curvature Driven Diffusions algorithm without texture constraints, a traditional inpainting algorithm with Gabor texture constraints, and a WGAN inpainting algorithm based on attention mechanism without texture constraints.},
  archive      = {J_APIN},
  author       = {Jiang, Hanqiong and Shen, Lei and Wang, Huaxia and Yao, Yudong and Zhao, Guodong},
  doi          = {10.1007/s10489-021-03017-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9996-10007},
  shortjournal = {Appl. Intell.},
  title        = {Finger vein image inpainting using neighbor binary-wasserstein generative adversarial networks (NB-WGAN)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting the abnormal events at well drilling with
machine learning. <em>APIN</em>, <em>52</em>(9), 9980–9995. (<a
href="https://doi.org/10.1007/s10489-021-03013-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a data-driven and physics-informed algorithm for drilling accident forecasting. The core machine-learning algorithm uses the data from the drilling telemetry representing the time-series. We have developed a Bag-of-features representation of the time series that enables the algorithm to predict the probabilities of six types of drilling accidents in real-time. The machine-learning model is trained on the 125 past drilling accidents from 100 different Russian oil and gas wells. Validation shows that the model can forecast 70% of drilling accidents with a false positive rate equals to 40%. The model addresses partial prevention of the drilling accidents at the well construction.},
  archive      = {J_APIN},
  author       = {Gurina, Ekaterina and Klyuchnikov, Nikita and Antipova, Ksenia and Koroteev, Dmitry},
  doi          = {10.1007/s10489-021-03013-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9980-9995},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting the abnormal events at well drilling with machine learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transformer-based two-source motion model for multi-object
tracking. <em>APIN</em>, <em>52</em>(9), 9967–9979. (<a
href="https://doi.org/10.1007/s10489-021-03012-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, benefit from the development of detection models, the multi-object tracking method based on tracking-by-detection has greatly improved performance. However, most methods still utilize traditional motion models for position prediction, such as the constant velocity model and Kalman filter. Only a few methods adopt deep network-based methods for prediction. Still, these methods only exploit the simplest RNN(Recurrent Neural Network) to predict the position, and the position offset caused by the camera movement is not considered. Therefore, inspired by the outstanding performance of Transformer in temporal tasks, this paper proposes a Transformer-based motion model for multi-object tracking. By taking the historical position difference of the target and the offset vector between consecutive frames as input, the model considers the motion of the target itself and the camera at the same time, which improves the prediction accuracy of the motion model used in the multi-target tracking method, thereby improving tracking performance. Through comparative experiments and tracking results on MOTchallenge benchmarks, the effectiveness of the proposed method is proved.},
  archive      = {J_APIN},
  author       = {Yang, Jieming and Ge, Hongwei and Su, Shuzhi and Liu, Guoqing},
  doi          = {10.1007/s10489-021-03012-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9967-9979},
  shortjournal = {Appl. Intell.},
  title        = {Transformer-based two-source motion model for multi-object tracking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative model based robotic grasp pose prediction with
limited dataset. <em>APIN</em>, <em>52</em>(9), 9952–9966. (<a
href="https://doi.org/10.1007/s10489-021-03011-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present investigation, we propose an architecture which we name as Generative Inception Neural Network (GI-NNet), capable of predicting antipodal robotic grasps intelligently, on seen as well as unseen objects. It is trained on Cornell Grasping Dataset (CGD) and attains a 98.87% grasp pose accuracy for detecting both regular/irregular shaped objects from RGB-Depth images while requiring only one-third of the network trainable parameters as compared to the existing approaches. However, to attain this level of performance the model requires the entire 90% of the available labelled data of CGD keeping only 10% labelled data for testing which makes it vulnerable to poor generalization. Furthermore, getting a sufficient and quality labelled dataset for robot grasping is extremely difficult. To address these issues, we subsequently propose another architecture where our proposed GI-NNet model is attached as a decoder of a Vector Quantized Variational Auto-Encoder (VQ-VAE), which works more efficiently when trained both with the available labelled and unlabelled data. The proposed model, which we name as Representation based GI-NNet (RGI-NNet) has been trained utilizing the various split of available CGD dataset to test the learning ability of our architecture starting from only 10% label data with the latent embedding of VQ-VAE to 90% label data with the latent embedding. However, being trained with only 50% label data of CGD with latent embedding, the proposed architecture produces the best results which, we believe, is a remarkable accomplishment. The logical reasoning of this together with the other relevant technological details have been elaborated in this paper. The performance level, in terms of grasp pose accuracy of RGI-NNet, varies between 92.1348% to 97.7528% which is far better than several existing models trained with only labelled dataset. For the performance verification of both the proposed models, GI-NNet and RGI-NNet, we have performed rigorous experiments on Anukul (Baxter) hardware cobot.},
  archive      = {J_APIN},
  author       = {Shukla, Priya and Pramanik, Nilotpal and Mehta, Deepesh and Nandi, G. C.},
  doi          = {10.1007/s10489-021-03011-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9952-9966},
  shortjournal = {Appl. Intell.},
  title        = {Generative model based robotic grasp pose prediction with limited dataset},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I2DKPCN: An unsupervised deep learning network.
<em>APIN</em>, <em>52</em>(9), 9938–9951. (<a
href="https://doi.org/10.1007/s10489-021-03007-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed an incremental two-dimensional kernel PCA-based convolutional network (I2DKPCN) which is a novel unsupervised deep learning network. In our architecture, I2DKPCN consists of several feature extraction stages and one output stage, and each feature extraction stage includes a convolutional layer, a feature pooling layer and a feature fusion layer. In the output stage, binary hashing and blockwise histograms are exploited for the generation of the final features. In particular, the filters of the convolutional layer are learned by using incremental two-dimensional kernel principal component analysis(I2DKPCA) rather than the gradient-based optimization. Due to the fact that the back propagation is not used to learn the parameters of the filter, the calculation speed of I2DKPCN is much faster than that of the existing deep network. We have extensively tested I2DKPCN on multiple data sets for three challenging tasks, including hand-written digit recognition, texture classification and face recognition. It can be seen from the results that I2DKPCN performs competitively or even better compared with other networks in all tests. Because the filters are learned by I2DKPCA, which combined the 2DPCA with kernel method and incremental learning, the non-linear problem was solved and the computational time was reduced.},
  archive      = {J_APIN},
  author       = {Zhao, Ruyi and Shi, Fanhuai},
  doi          = {10.1007/s10489-021-03007-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9938-9951},
  shortjournal = {Appl. Intell.},
  title        = {I2DKPCN: An unsupervised deep learning network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network representation learning based on community-aware and
adaptive random walk for overlapping community detection. <em>APIN</em>,
<em>52</em>(9), 9919–9937. (<a
href="https://doi.org/10.1007/s10489-021-02999-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Network representation learning methods based on random walk aim to learn a low-dimensional embedding vector for each node in a network by randomly traversing the network to capture the features of nodes and edges, which is beneficial to many downstream machine learning tasks such as community detection. Most of the existing random-walk-based network representation learning algorithms emphasize the neighborhood of nodes but ignore the communities they may form and apply the same random walk strategy to all nodes without distinguishing the characteristics of different nodes. In addition, it is time-consuming to determine the most suitable random walk parameters for a given network. In this paper, we propose a novel overlapping community detection algorithm based on network representation learning which integrates community information into embedding vectors to improve the cohesion degree of similar nodes in the embedding space. First, a node-centrality-based walk strategy is designed to determine the parameters of random walk automatically to avoid the time-consuming manual selection. Second, two community-aware random walk strategies for high and low degree nodes are developed to capture the characteristics of the community centers and boundaries. The experimental results on the synthesized and real-world datasets demonstrate the effectiveness and efficiency of our algorithm on overlapping community detection compared with the state-of-the-art algorithms},
  archive      = {J_APIN},
  author       = {Guo, Kun and Wang, Qinze and Lin, Jiaqi and Wu, Ling and Guo, Wenzhong and Chao, Kuo-Ming},
  doi          = {10.1007/s10489-021-02999-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9919-9937},
  shortjournal = {Appl. Intell.},
  title        = {Network representation learning based on community-aware and adaptive random walk for overlapping community detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Location-based deep factorization machine model for service
recommendation. <em>APIN</em>, <em>52</em>(9), 9899–9918. (<a
href="https://doi.org/10.1007/s10489-021-02998-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The era of everything as a service led to an explosion of services with similar functionalities on the internet. Quickly obtaining a high-quality service has become a research focus in the field of service recommendation. Studies show that quality of service (QoS) predictions are an effective way to discover services with high quality. However, sparse data and performance fluctuation challenge the accuracy and robustness of QoS prediction. To solve these two challenges, this paper proposes a location-based deep factorization machine model, namely LDFM, by employing information entropy and location projection of users and services. Particularly, our LDFM can be decomposed into three phases: i) extending a raw QoS dataset without introducing additional information, where LDFM projects the existing users (services) in the direction of their position vectors to increase the number of users (services) as well as the number of records that users invoke services; ii) mining a sufficient number of potential features behind the behaviors of users who invoke services, where LDFM employs a factorization machine to extract potential features of breadth with low dimensions (i.e., one and two dimensions) and utilizes deep learning to seek potential depth features with high dimensions; and iii) weighting extracted features within various dimensions, where LDFM employs information entropy to strengthen the positive effects of valid features while reducing the negative impacts generated by biased features. Our experimental results (including t-test analyses) show that our proposed LDFM always performs well under different user-service matrix densities and performs better than existing start-of-the-art methods in terms of the accuracy and robustness of QoS predictions.},
  archive      = {J_APIN},
  author       = {Wang, Qingren and Zhang, Min and Zhang, Yiwen and Zhong, Jinqin and Sheng, Victor S.},
  doi          = {10.1007/s10489-021-02998-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9899-9918},
  shortjournal = {Appl. Intell.},
  title        = {Location-based deep factorization machine model for service recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offline reinforcement learning with anderson acceleration
for robotic tasks. <em>APIN</em>, <em>52</em>(9), 9885–9898. (<a
href="https://doi.org/10.1007/s10489-021-02953-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) can learn effective policy from a fixed batch of data without interaction. However, the real-world requirements, such as better performance and high sample efficiency, put substantial challenges on current offline RL algorithms. In this paper, we propose a novel offline RL method, Constrained and Conservative Reinforcement Learning with Anderson Acceleration (CCRL-AA), which aims to enable the agent to effectively and efficiently learn from offline demonstration data. In our method, Constrained and Conservative Reinforcement Learning (CCRL) restricts the policy’s actions with respect to a batch of training data and learns a conservative Q-function to make the agent effectively learn from the previously collected demonstrations. The mechanism of Anderson acceleration (AA) is integrated to speed up the learning process and improve sample efficiency. Experiments were conducted on robotic simulation tasks, and the results demonstrate that our method can efficiently learn from given demonstrations and give better performance than several other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zuo, Guoyu and Huang, Shuai and Li, Jiangeng and Gong, Daoxiong},
  doi          = {10.1007/s10489-021-02953-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9885-9898},
  shortjournal = {Appl. Intell.},
  title        = {Offline reinforcement learning with anderson acceleration for robotic tasks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). NetNMSP: Nonoverlapping maximal sequential pattern mining.
<em>APIN</em>, <em>52</em>(9), 9861–9884. (<a
href="https://doi.org/10.1007/s10489-021-02912-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonoverlapping sequential pattern mining, as a kind of repetitive sequential pattern mining with gap constraints, can find more valuable patterns. Traditional algorithms focused on finding all frequent patterns and found lots of redundant short patterns. However, it not only reduces the mining efficiency, but also increases the difficulty in obtaining the demand information. To reduce the frequent patterns and retain its expression ability, this paper focuses on the Nonoverlapping Maximal Sequential Pattern (NMSP) mining which refers to finding frequent patterns whose super-patterns are infrequent. In this paper, we propose an effective mining algorithm, Nettree for NMSP mining (NetNMSP), which has three key steps: calculating the support, generating the candidate patterns, and determining NMSPs. To efficiently calculate the support, NetNMSP employs the backtracking strategy to obtain a nonoverlapping occurrence from the leftmost leaf to its root with the leftmost parent node method in a Nettree. To reduce the candidate patterns, NetNMSP generates candidate patterns by the pattern join strategy. Furthermore, to determine NMSPs, NetNMSP adopts the screening method. Experiments on biological sequence datasets verify that not only does NetNMSP outperform the state-of-the-arts algorithms, but also NMSP mining has better compression performance than closed pattern mining. On sales datasets, we validate that our algorithm guarantees the best scalability on large scale datasets. Moreover, we mine NMSPs and frequent patterns in SARS-CoV-1, SARS-CoV-2 and MERS-CoV. The results show that the three viruses are similar in the short patterns but different in the long patterns. More importantly, NMSP mining is easier to find the differences between the virus sequences.},
  archive      = {J_APIN},
  author       = {Li, Yan and Zhang, Shuai and Guo, Lei and Liu, Jing and Wu, Youxi and Wu, Xindong},
  doi          = {10.1007/s10489-021-02912-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9861-9884},
  shortjournal = {Appl. Intell.},
  title        = {NetNMSP: Nonoverlapping maximal sequential pattern mining},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mention detection in coreference resolution: survey.
<em>APIN</em>, <em>52</em>(9), 9816–9860. (<a
href="https://doi.org/10.1007/s10489-021-02878-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coreference Resolution is an essential task for Natural Language Processing (NLP) application, which has a paramount impact on the performance of text summarization, machine translation, text classification, and recognizing textual entailment. Mention Detection (MD) is the core component of the coreference resolution task and is additionally a process of extraction of all possible mentions from the text. Mention is referred to as a textual representation of entities in the text, such as Name, Nominal, and Pronominal mentions. The mentions appear in the text using different representations but indicating the same entity. The performance of an MD module positively affects the performance of NLP tasks such as Coreference resolution, Relation Extraction, Information retrieval, Information extraction, etc. Incorrect identification of mentions in the text severely affects the efficiency of the coreference resolution task. This paper aims to provide a comprehensive overview for the state of the art of mention detection approaches, which is utilized in the coreference resolution task and explains the importance of MD in Coreference resolution. The subsisting approaches are classified based on the underlying techniques adopted by each approach in three categories: Rule-based mention detection, Statistics-based mention detection, and Deep learning-based mention detection. The performance of deep learning is improving as more data and more powerful computing resources become available. This study endeavors to provide a comparative analysis of various mention detection approaches and help the researchers to assimilate knowledge about the mention detection approaches from sundry aspects.},
  archive      = {J_APIN},
  author       = {Lata, Kusum and Singh, Pardeep and Dutta, Kamlesh},
  doi          = {10.1007/s10489-021-02878-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9816-9860},
  shortjournal = {Appl. Intell.},
  title        = {Mention detection in coreference resolution: Survey},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing firefly algorithm with adaptive multi-group
mechanism. <em>APIN</em>, <em>52</em>(9), 9795–9815. (<a
href="https://doi.org/10.1007/s10489-021-02766-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firefly algorithm (FA) is efficient in solving continuous optimal problems, because of its ability to a global search. However, the redundant attractions and incorrect directions may reduce the efficiency of FA. To improve the performance of FA, a novel multi-group mechanism is proposed based on an assumption that firefly has a visual field. The modified firefly algorithm is called the visual firefly algorithm(VFA). The framework of VFA combines the assumption with the designed strategies to balance the exploration and exploitation. Where the proposed observer strategy works for the exploration, the suggested selective random strategy plays the role of the exploiter. To verify the performance of the presented algorithm, extensive experiments are executed on CEC2013 benchmark functions. Additionally, the efficiency of the proposed multi-group mechanism is analyzed in-depth. The experimental results reveal that the proposed multi-group mechanism improves FA and provides a suitable solution for most CEC2013 problems with different dimensions. Especially, its performance remains robust, where the problems become more complex.},
  archive      = {J_APIN},
  author       = {Cao, Lianglin and Ben, Kerong and Peng, Hu and Zhang, Xian},
  doi          = {10.1007/s10489-021-02766-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9795-9815},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing firefly algorithm with adaptive multi-group mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiamET: A siamese based visual tracking network with
enhanced templates. <em>APIN</em>, <em>52</em>(9), 9782–9794. (<a
href="https://doi.org/10.1007/s10489-021-03057-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative correlation filter (DCF) played a dominant role in visual tracking tasks in early years. However, with the recent development of deep learning, the Siamese based networks begin to prevail. Unlike DCF, most Siamese network based tracking methods take the first frame as the reference, while ignoring the information from the subsequent frames. As a result, these methods may fail under unforeseeable situations (e.g. target scale/size changes, variant illuminations, occlusions etc.). Meanwhile, other deep learning based tracking methods learn discriminative filters online, where the training samples are extracted from a few fixed frames with predictable labels. However, these methods have the same limitations as Siamese-based trackers. The training samples are prone to have cumulative errors, which ultimately lead to tracking loss. In this situation, we propose SiamET, a Siamese-based network using Resnet-50 as its backbone with enhanced template module. Different from existing methods, our templates are acquired based on all historical frames. Extensive experiments have been carried out on popular datasets to verify the effectiveness of our method. It turns out that our tracker achieves superior performances than the state-of-the-art methods on 4 challenging benchmarks, including OTB100, VOT2018, VOT2019 and LaSOT. Specifically, we achieve an EAO score of 0.480 on VOT2018 with 31 FPS. Code is available at https://github.com/yu-1238/SiamET},
  archive      = {J_APIN},
  author       = {Zhou, Yuxin and Zhang, Yi},
  doi          = {10.1007/s10489-021-03057-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9782-9794},
  shortjournal = {Appl. Intell.},
  title        = {SiamET: A siamese based visual tracking network with enhanced templates},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tree-based stacking ensemble technique with feature
selection for network intrusion detection. <em>APIN</em>,
<em>52</em>(9), 9768–9781. (<a
href="https://doi.org/10.1007/s10489-021-02968-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have used machine learning algorithms to develop intrusion systems (IDS), which differentiate anomalous behaviours from the normal activities of network systems. Due to the ease of automated data collection and subsequently an increased size of collected data on network traffic and activities, the complexity of intrusion analysis is increasing exponentially. A particular issue, due to statistical and computation limitations, a single classifier may not perform well for large scale data as existent in modern IDS contexts. Ensemble methods have been explored in literature in such big data contexts. Although more complicated and requiring additional computation, literature has a note that ensemble methods can result in better accuracy than single classifiers in different large scale data classification contexts, and it is interesting to explore how ensemble approaches can perform in IDS. In this research, we introduce a tree-based stacking ensemble technique (SET) and test the effectiveness of the proposed model on two intrusion datasets (NSL-KDD and UNSW-NB15). We further enhance incorporate feature selection techniques to select the best relevant features with the proposed SET. A comprehensive performance analysis shows that our proposed model can better identify the normal and anomaly traffic in network than other existing IDS models. This implies the potentials of our proposed system for cybersecurity in Internet of Things (IoT) and large scale networks.},
  archive      = {J_APIN},
  author       = {Rashid, Mamunur and Kamruzzaman, Joarder and Imam, Tasadduq and Wibowo, Santoso and Gordon, Steven},
  doi          = {10.1007/s10489-021-02968-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9768-9781},
  shortjournal = {Appl. Intell.},
  title        = {A tree-based stacking ensemble technique with feature selection for network intrusion detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel method to rank fuzzy numbers using the developed
golden rule representative value. <em>APIN</em>, <em>52</em>(9),
9751–9767. (<a
href="https://doi.org/10.1007/s10489-021-02965-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking fuzzy numbers is an important subject of fuzzy set theory, which has been widely studied and applied to many practical problems. However, the previous fuzzy number ranking methods have some weaknesses, such as incomplete ranking objects, complicated calculations, and ignoring interpretability. To overcome these weaknesses and develop a ranking method that performs better in all aspects, the concept of the golden rule representative value is used. The golden rule representative value was first introduced by Yager to solve the order of interval values. This paper expands it and proposes a novel fuzzy number ranking method based on the developed golden rule representative value. The centroid point and area of fuzzy numbers are considered, and some new rules are formulated to capture the preference of the decision-maker. The TSK fuzzy model is used to model the rules. The constructed Rep function associates each fuzzy number with a scalar value. By comparing these scalar values, we get the ranking order of fuzzy numbers. The proposed ranking method is simple to use and can overcome the shortcomings of existing methods. Some specific numerical examples are used to illustrate the property of the proposed method, and the corresponding explanations show the interpretability of the ranking process. The comparative experiment with the existing ranking method shows the advantages of the proposed method. An application example of fuzzy risk analysis proves the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Cheng, Ruolan and Kang, Bingyi and Zhang, Jianfeng},
  doi          = {10.1007/s10489-021-02965-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9751-9767},
  shortjournal = {Appl. Intell.},
  title        = {A novel method to rank fuzzy numbers using the developed golden rule representative value},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised nonnegative matrix factorization with
positive and negative label propagations. <em>APIN</em>, <em>52</em>(9),
9739–9750. (<a
href="https://doi.org/10.1007/s10489-021-02940-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised nonnegative matrix factorization (SNMF) methods yield the enhanced representation ability over nonnegative matrix factorization (NMF) by incorporating the label information. Label propagation (LP) is a popular graph-based method used in SNMF to propagate label information from the labeled data to the unlabeled ones. However, label constraint propagation is always ignored to propagate label restrictions for the data. In this paper, a novel SNMF method, namely positive and negative label propagations based SNMF (PNLP-SNMF), is proposed to improve clustering performance by leveraging both positive and negative label information. The proposed method fulfills nonnegative matrix factorization and label constraint propagation in an unified optimization model. By the label indicator, PNLP-SNMF could guide the unlabeled data of the same predicted label to be mapped into the same class and enhance the discriminative ability of the representation in the feature space. Moreover, we further design an effective iterative updating optimization scheme to solve the objective function the the proposed PNLP-SNMF, whose convergence is theoretically proven. Extensive experimental results demonstrate the effectiveness of our proposed method in image clustering tasks by comparing with several state-of-the-art NMF-based methods.},
  archive      = {J_APIN},
  author       = {Wang, Changpeng and Zhang, Jiangshe and Wu, Tianjun and Zhang, Meng and Shi, Guang},
  doi          = {10.1007/s10489-021-02940-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9739-9750},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised nonnegative matrix factorization with positive and negative label propagations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A hierarchical conditional random field-based attention
mechanism approach for gastric histopathology image classification.
<em>APIN</em>, <em>52</em>(9), 9717–9738. (<a
href="https://doi.org/10.1007/s10489-021-02886-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Gastric Histopathology Image Classification (GHIC) tasks, which are usually weakly supervised learning missions, there is inevitably redundant information in the images. Therefore, designing networks that can focus on distinguishing features has become a popular research topic. In this paper, to accomplish the tasks of GHIC superiorly and assist pathologists in clinical diagnosis, an intelligent Hierarchical Conditional Random Field based Attention Mechanism (HCRF-AM) model is proposed. The HCRF-AM model consists of an Attention Mechanism (AM) module and an Image Classification (IC) module. In the AM module, an HCRF model is built to extract attention regions. In the IC module, a Convolutional Neural Network (CNN) model is trained with the attention regions selected, and then an algorithm called Classification Probability-based Ensemble Learning is applied to obtain the image-level results from the patch-level output of the CNN. In the experiment, a classification specificity of 96.67% is achieved on a gastric histopathology dataset with 700 images. Our HCRF-AM model demonstrates high classification performance and shows its effectiveness and future potential in the GHIC field. In addition, the AM module and transfer learning technique allow the network to generalize well to other types of image data except histopathology images, and we obtain 95.5% and 95.8% accuracies on IG02 and Oxford-IIIT Pet Datasets.},
  archive      = {J_APIN},
  author       = {Li, Yixin and Wu, Xinran and Li, Chen and Li, Xiaoyan and Chen, Haoyuan and Sun, Changhao and Rahaman, Md Mamunur and Yao, Yudong and Zhang, Yong and Jiang, Tao},
  doi          = {10.1007/s10489-021-02886-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9717-9738},
  shortjournal = {Appl. Intell.},
  title        = {A hierarchical conditional random field-based attention mechanism approach for gastric histopathology image classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing cooperation by cognition differences and
consistent representation in multi-agent reinforcement learning.
<em>APIN</em>, <em>52</em>(9), 9701–9716. (<a
href="https://doi.org/10.1007/s10489-021-02873-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent reinforcement learning is efficient to deal with tasks that require cooperation among different individuals. And communication plays an important role to enhance the cooperation of agents in scalable and unstable environments. However, there are still many challenges because some information of communication may fail to facilitate cooperation or even have a negative effect. Thus, how to explore efficient information for the cooperation of agents is a critical issue to be solved. In this paper, we propose a multi-agent reinforcement learning algorithm with cognition differences and consistent representation (CDCR). The criteria of cognition differences are formulated to explore information possessed by different agents, to help each agent have a better understanding of others. We further train a cognition encoding network to obtain the global cognition consistent representation for each agent, then the representation is used to realize the cognitive consistency of the agent for the environment. To validate the effectiveness of the CDCR, we carry out experiments in Predator-Prey and StarCraft II environments. The results in Predator-Prey demonstrate that the proposed cognition differences can achieve effective communication among agents; the results in StarCraft II demonstrate that considering both cognition differences and consistent representation can increase the test win rate of the baseline algorithm by 29% in the best case, and the ablation studies further demonstrate the positive roles played by the proposed strategies.},
  archive      = {J_APIN},
  author       = {Ge, Hongwei and Ge, Zhixin and Sun, Liang and Wang, Yuxin},
  doi          = {10.1007/s10489-021-02873-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9701-9716},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing cooperation by cognition differences and consistent representation in multi-agent reinforcement learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rescuing emergency cases of COVID-19 patients: An
intelligent real-time MSC transfusion framework based on multicriteria
decision-making methods. <em>APIN</em>, <em>52</em>(9), 9676–9700. (<a
href="https://doi.org/10.1007/s10489-021-02813-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesenchymal stem cells (MSCs) have shown promising ability to treat critical cases of coronavirus disease 2019 (COVID-19) by regenerating lung cells and reducing immune system overreaction. However, two main challenges need to be addressed first before MSCs can be efficiently transfused to the most critical cases of COVID-19. First is the selection of suitable MSC sources that can meet the standards of stem cell criteria. Second is differentiating COVID-19 patients into different emergency levels automatically and prioritising them in each emergency level. This study presents an efficient real-time MSC transfusion framework based on multicriteria decision-making(MCDM) methods. In the methodology, the testing phase represents the ability to adhere to plastic surfaces, the upregulation and downregulation of specific surface protein markers and finally the ability to differentiate into different kinds of cells. In the development phase, firstly, two scenarios of an augmented dataset based on the medical perspective are generated to produce 80 patients with different emergency levels. Secondly, an automated triage algorithm based on a formal medical guideline is proposed for real-time monitoring of COVID-19 patients with different emergency levels (i.e. mild, moderate, severe and critical) considering the improvement and deterioration procedures from one level to another. Thirdly, a unique decision matrix for each triage level (except mild) is constructed on the basis of the intersection between the evaluation criteria of each emergency level and list of COVID-19 patients. Thereafter, MCDM methods (i.e. analytic hierarchy process [AHP] and vlsekriterijumska optimizcija i kaompromisno resenje [VIKOR]) are integrated to assign subjective weights for the evaluation criteria within each triage level and then prioritise the COVID-19 patients on the basis of individual and group decision-making(GDM) contexts. Results show that: (1) in both scenarios, the proposed algorithm effectively classified the patients into four emergency levels, including mild, moderate, severe and critical, taking into consideration the improvement and deterioration cases. (2) On the basis of experts’ perspectives, clear differences in most individual prioritisations for patients with different emergency levels in both scenarios were found. (3) In both scenarios, COVID-19 patients were prioritised identically between the internal and external group VIKOR. During the evaluation, the statistical objective method indicated that the patient prioritisations underwent systematic ranking. Moreover, comparison analysis with previous work proved the efficiency of the proposed framework. Thus, the real-time MSC transfusion for COVID-19 patients can follow the order achieved in the group VIKOR results.},
  archive      = {J_APIN},
  author       = {Alsalem, M. A. and Albahri, O. S. and Zaidan, A. A. and Al-Obaidi, Jameel R. and Alnoor, Alhamzah and Alamoodi, A. H. and Albahri, A. S. and Zaidan, B. B. and Jumaah, F. M.},
  doi          = {10.1007/s10489-021-02813-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9676-9700},
  shortjournal = {Appl. Intell.},
  title        = {Rescuing emergency cases of COVID-19 patients: An intelligent real-time MSC transfusion framework based on multicriteria decision-making methods},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-CT-mask-net: Prediction of COVID-19 from CT scans
using regional features. <em>APIN</em>, <em>52</em>(9), 9664–9675. (<a
href="https://doi.org/10.1007/s10489-021-02731-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present COVID-CT-Mask-Net model that predicts COVID-19 in chest CT scans. The model works in two stages: in the first stage, Mask R-CNN is trained to localize and detect two types of lesions in images. In the second stage, these detections are fused to classify the whole input image. To develop the solution for the three-class problem (COVID-19, Common Pneumonia and Control), we used the COVIDx-CT data split derived from the dataset of chest CT scans collected by China National Center for Bioinformation. We use 3000 images (about 5% of the train split of COVIDx-CT) to train the model. Without any complicated data normalization, balancing and regularization, and training only a small fraction of the model’s parameters, we achieve a 90.80% COVID-19 sensitivity, 91.62% Common Pneumonia sensitivity and 92.10% true negative rate (Control sensitivity), an overall accuracy of 91.66% and F1-score of 91.50% on the test data split with 21192 images, bringing the ratio of test to train data to 7.06. We also establish an important result that regional predictions (bounding boxes with confidence scores) detected by Mask R-CNN can be used to classify whole images. The full source code, models and pretrained weights are available on https://github.com/AlexTS1980/COVID-CT-Mask-Net .},
  archive      = {J_APIN},
  author       = {Ter-Sarkisov, Aram},
  doi          = {10.1007/s10489-021-02731-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9664-9675},
  shortjournal = {Appl. Intell.},
  title        = {COVID-CT-mask-net: Prediction of COVID-19 from CT scans using regional features},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abstractive document summarization via multi-template
decoding. <em>APIN</em>, <em>52</em>(9), 9650–9663. (<a
href="https://doi.org/10.1007/s10489-021-02607-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most previous abstractive summarization models generate the summary in a left-to-right manner without making the most use of target-side global information. Recently, many researchers seek to alleviate this issue by retrieving target-side templates from large-scale training corpus, yet have limitations in template quality. To overcome the problem of template selection bias, one promising direction is to get better target-side global information from multiple high-quality templates. Hence, this paper extends the encoder-decoder framework by introducing a multi-template decoding mechanism, which can utilize multiple templates retrieved from the training corpus based on the semantic distance. In addition, we introduce a multi-granular attention mechanism by simultaneously taking into account the importance of words in templates and the importance of different templates. Extensive experiment results on CNN/Daily mail and English Gigaword show that our proposed model significantly outperforms several state-of-the-art abstractive and extractive baseline models.},
  archive      = {J_APIN},
  author       = {Huang, Yuxin and Yu, Zhengtao and Guo, Junjun and Xiang, Yan and Yu, Zhiqiang and Xian, Yantuan},
  doi          = {10.1007/s10489-021-02607-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9650-9663},
  shortjournal = {Appl. Intell.},
  title        = {Abstractive document summarization via multi-template decoding},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local feature extraction network with high correspondences
for 3d point cloud registration. <em>APIN</em>, <em>52</em>(9),
9638–9649. (<a
href="https://doi.org/10.1007/s10489-021-03055-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud registration is an important task in computer vision. Due to the irregularity of point clouds, it is still a challenging problem to realize the accurate registration. Recently, with the development of deep learning, scholars have proposed many learning-based methods, which can enhance the correspondence of points and not rely on the initial alignment conditions. However, most works tend to ignore the importance of local features, leading to the unreasonable matching. To solve this issue, we propose two networks to extract richer local information. In order to find a closer internal relation between the points, a Subtract Attention Network (SANet) is designed. In which, we propose a Subtract Attention Module (SAM) to aggregate the point-wise feature representations and construct the key points of feature space on this basis. We also propose a Position Encoding Network (PENet) to determine the spatial correlation with the utility of local coordinates. After combining the spatial features of different dimensions, the connections of key points in the feature space tend to be more credible. Thus, we can effectively obtain the local correspondence between each point and then improve the accuracy of registration. The results on the commonly used dataset ModelNet40 show the superiority of our method.},
  archive      = {J_APIN},
  author       = {Li, Dashuang and He, Kai and Wang, Lei and Zhang, Dazhuang},
  doi          = {10.1007/s10489-021-03055-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9638-9649},
  shortjournal = {Appl. Intell.},
  title        = {Local feature extraction network with high correspondences for 3d point cloud registration},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generation and interpretation of parsimonious predictive
models for load forecasting in smart heating networks. <em>APIN</em>,
<em>52</em>(9), 9621–9637. (<a
href="https://doi.org/10.1007/s10489-021-02949-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting future heat load in smart district heating networks is a key problem for utility companies that need such predictions for optimizing their operational activities. From the statistical learning viewpoint, this problem is also very interesting because it requires to integrate multiple time series about weather and social factors into a dynamical model, and to generate models able to explain the relationships between weather/social factors and heat load. Typical questions in this context are: “Which variables are more informative for the prediction?” and “Do variables have different influence in different contexts (e.g., time instant or situations)?” We propose a methodology for generating simple and interpretable models for heat load forecasting, then we apply this methodology to a real dataset, and, finally, provide new insight about this application domain. The methodology merges multi-equation multivariate linear regression and forward variable selection. We generate a (sparse) equation for each pair day-of-the-week/hour-of-the-day (for instance, one equation concerns predictions of Monday at 0.00, another predictions of Monday at 1.00, and so on). These equations are simple to explain because they locally approximate the prediction problem in specific times of day/week. Variable selection is a key contribution of this work. It provides a reduction of the prediction error of 2.4% and a decrease of the number of parameters of 49.8% compared to state-of-the-art models. Interestingly, different variables are selected in different equations (i.e., times of the day/week), showing that weather and social factors, and autoregressive variables with different delays, differently influence heat predictions in different times of the day/week.},
  archive      = {J_APIN},
  author       = {Castellini, Alberto and Bianchi, Federico and Farinelli, Alessandro},
  doi          = {10.1007/s10489-021-02949-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9621-9637},
  shortjournal = {Appl. Intell.},
  title        = {Generation and interpretation of parsimonious predictive models for load forecasting in smart heating networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local community detection with hints. <em>APIN</em>,
<em>52</em>(9), 9599–9620. (<a
href="https://doi.org/10.1007/s10489-021-02946-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local community detection is a widely used method for identifying groups of nodes starting from seeding nodes. The seed(s) are usually selected either randomly or based only on structural properties of the network. However, in many cases the choice of seed(s) incorporates external knowledge that attaches to these nodes an additional importance for their community. This knowledge, may be derived from an expert on the domain, or may arise from the network’s side information and it constitutes our motivation for the present work; this additional information about the importance of seed(s) can be exploited for detection of better and more relevant communities. We call such biased seed(s), hint(s). Our approach, is to reflect the importance of hints by changing appropriately the network in their vicinity. To the best of our knowledge, no such viewpoint of the seeding nodes in local community detection has been considered before. The aim of this study is to identify a single community which contains the hint(s). Our key contribution is the proposed Hint Enhancement Framework(HEF) that applies a two-step procedure to discover the community of the hint(s): 1) it changes the network by amplifying the hint(s) using re-weighting or re-wiring strategies so as to materialize the bias towards them and 2) it applies local community detection algorithms on the altered network of step 1. We experimentally evaluate HEF in synthetic and real datasets, and demonstrate the positive aspects of the framework in identifying better communities, in comparison with plain local community detection algorithms as well as a global one.},
  archive      = {J_APIN},
  author       = {Baltsou, Georgia and Tsichlas, Konstantinos and Vakali, Athena},
  doi          = {10.1007/s10489-021-02946-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {9},
  pages        = {9599-9620},
  shortjournal = {Appl. Intell.},
  title        = {Local community detection with hints},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical pseudo-label learning for one-shot person
re-identification. <em>APIN</em>, <em>52</em>(8), 9225–9238. (<a
href="https://doi.org/10.1007/s10489-021-02959-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of one-shot person re-identification (person re-ID) is to recognize the target person from abundant pedestrians with only one labeled image of each person in training samples. The major problems faced by one-shot person re-ID algorithms are information loss and label noise. In this work, we propose a novel approach targeting one-shot person re-identification, planning to solve both the problems. It overcomes the information loss problem by introducing a hierarchical pseudo-label assignment strategy to fully exploit the unlabeled data information. Further, we also enhance the independence of two network branches by multiplying them by different weights for co-training in our asymmetric mutual teaching framework, which can generate soft labels to reduce hard pseudo-label noise. Extensive experiments on three benchmark datasets suggest that our proposed method significantly outperforms the existing state-of-the-art algorithm of mAP, respectively on Market-1501 and on DukeMTMC-reID, by 31.5 and 18.1. Our method can also be applied to the few-shots setting and achieve respectable performance.},
  archive      = {J_APIN},
  author       = {Shao, Jie and Ma, Xiaoyu},
  doi          = {10.1007/s10489-021-02959-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9225-9238},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical pseudo-label learning for one-shot person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Semi-supervised deep learning recognition method for the
new classes of faults in wind turbine system. <em>APIN</em>,
<em>52</em>(8), 9212–9224. (<a
href="https://doi.org/10.1007/s10489-021-03024-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fault recognition algorithms can only identify the known classes of fault in wind turbine systems(WTs). These faults have already appearedin the WTs, thus, the fault recognition algorithm can identify the classesof them. However, if a new classfault didnot happen in the WTs before, the traditional fault recognition algorithm can only identify it as known class of fault, which results in an incorrect identification. Toaddress this problem, a new class fault recognition method based on semi-supervised deep learning(SDL-NCFR)is proposed. Firstly, multiple WTs signals are used as input and features are extracted by convolutional autoencoder network; secondly, the initialization model is built with compressed features as input to the classifier and error feature map as input to the detector; finally, the detector will put new class fault instances into the buffer. When the buffer overflows, the algorithm starts to update, thus achieving the purpose of identifying new class faults. The experimental results show that the average accuracy of the initialized model could reach more than 98%. The accuracy of updated model could still reach 89.39%, and the detection rate could reach 99.50%, the recall reached 88.76%, the precision reached 92.03%, and the F1 score reached 90.36% respectively. The experimental results show that the proposed algorithm can effectively solve the problem of identifying new class faults in WTs, and the accuracy is much higher than that of traditional detection methods.},
  archive      = {J_APIN},
  author       = {Liu, Jun and Wang, Junnian and Yu, Wenxin and Wang, Zhenheng and Zhong, Guang’an and He, Feng},
  doi          = {10.1007/s10489-021-03024-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9212-9224},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised deep learning recognition method for the new classes of faults in wind turbine system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online algorithm configuration for differential evolution
algorithm. <em>APIN</em>, <em>52</em>(8), 9193–9211. (<a
href="https://doi.org/10.1007/s10489-021-02752-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of evolutionary algorithms (EAs) is strongly affected by their configurations. Thus, algorithm configuration (AC) problem, that is, to properly set algorithm’s configuration, including the operators and parameter values for maximizing the algorithm’s performance on given problem(s) is an essential and challenging task in the design and application of EAs. In this paper, an online algorithm configuration (OAC) approach is proposed for differential evolution (DE) algorithm to adapt its configuration in a data-driven way. In our proposed OAC, the multi-armed bandit algorithm is adopted to select trial vector generation strategies for DE, and the kernel density estimation method is used to adapt the associated control parameters during the evolutionary search process. The performance of DE algorithm using the proposed OAC (OAC-DE) is evaluated on a benchmark set of 30 bound-constrained numerical optimization problems and compared with several adaptive DE variants. Besides, the influence of OAC’s hyper-parameter on its performance is analyzed. The comparison results show OAC-DE achieves better average performance than the compared algorithms, which validates the effectiveness of the proposed OAC. The sensitivity analysis indicates that the hyper-parameter of OAC has little impact on OAC-DE’s performance.},
  archive      = {J_APIN},
  author       = {Huang, Changwu and Bai, Hao and Yao, Xin},
  doi          = {10.1007/s10489-021-02752-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9193-9211},
  shortjournal = {Appl. Intell.},
  title        = {Online algorithm configuration for differential evolution algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SOCP approach to robust twin parametric margin support
vector machine. <em>APIN</em>, <em>52</em>(8), 9174–9192. (<a
href="https://doi.org/10.1007/s10489-021-02859-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin parametric-margin support vector machine (TPMSVM) is one of the classification tools based on parametric margin ν-SVM (par-ν-SVM) and twin SVM (TWSVM) which is useful for the data having heteroscedastic error structure. However, TPMSVM does not take into account the data uncertainty that might affect model accuracy. In this paper, we study TPMSVM under ellipsoidal uncertainty and its robust counterpart is presented as a second-order cone program (SOCP). Moreover, the robust formulation is extended to the nonlinear case using Gaussian kernels. To evaluate the efficiency of the proposed robust models, experiments have been conducted on nineteen University of California Irvine (UCI) datasets and the results have been compared with TWSVM, twin minimax probability machine classification (TMPMC), TPMSVM, Robust parametric TWSVM (RPTWSVM), and Twin SOCP−SVM (TSOCP−SVM) in the literature. The results show that the new model yieldsbetter classification accuracy.},
  archive      = {J_APIN},
  author       = {Sahleh, Ali and Salahi, Maziar and Eskandari, Sadegh},
  doi          = {10.1007/s10489-021-02859-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9174-9192},
  shortjournal = {Appl. Intell.},
  title        = {SOCP approach to robust twin parametric margin support vector machine},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic updating approximations of local generalized
multigranulation neighborhood rough set. <em>APIN</em>, <em>52</em>(8),
9148–9173. (<a
href="https://doi.org/10.1007/s10489-021-02861-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximation space in rough set theory is important for dealing with uncertainties. As the information contained in various information systems is constantly updated and changed with the development of information technology, how to effectively obtain the approximation space in dynamic environments is essential. The local rough set as an excellent model avoids unnecessary calculation of information granules, and can significantly improve learning efficiency. In this paper, we mainly investigate a dynamic approximation update mechanism of multigranulation data from local viewpoint. We first define a support and inclusion function to construct local generalized multigranulation neighborhood rough set model. Then, the dynamic updating process of global rough set and local rough set is analyzed when object chandes. Meanwhile, the corresponding dynamic update algorithms for dynamic objects are proposed based on local generalized multigranulation rough set model. The complexity analysis about them theoretically proves the efficiency of local dynamic algorithm compared with global algorithm and static algorithm. To illustrate the effectiveness of proposed algorithms, twelve datasets from UCI are adopted to contrast experiments.},
  archive      = {J_APIN},
  author       = {Xu, Weihua and Yuan, Kehua and Li, Wentao},
  doi          = {10.1007/s10489-021-02861-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9148-9173},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic updating approximations of local generalized multigranulation neighborhood rough set},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long term and short term forecasting of horticultural
produce based on the LSTM network model. <em>APIN</em>, <em>52</em>(8),
9117–9147. (<a
href="https://doi.org/10.1007/s10489-021-02845-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting the price of agricultural produce helps grower decide planting, harvesting, and trading time. Price forecasting of crops has garnered many researchers’ attention, hence plenty of forecasting models already exist in literature. Price forecasting of horticultural products is still a lesser explored area for researchers. Pricing strategy of vegetables do not follow the same strategies as that of crops. Due to seasonality and short lifetime, maintenance of vegetables differ from other agricultural products. As horticultural products are generated in a very short time after plantation, close forecasting of price may provide farmers more profit by choosing vegetables for plantation and guiding the appropriate harvesting time. In this paper, we have analyzed the performance of some machine learning and statistical models in this front. The error metrics like Root Mean Square Error(RMSE), Mean Absolute Error(MAE), and Mean Absolute Percentage Error (MAPE) have been used to study the performance of the models. We have proposed a Long Short-Term Memory(LSTM) based model for long-term price forecasting of vegetables like cabbage, Cauliflower, and Brinjal for some Indian markets. Friedman test and Wilcoxon signed rank test are used to analyze the employed models’ similarity and dissimilarity. The experiment results indicate that the proposed model outshines other models. A short-term price forecasting model has also been experimented with in this paper.},
  archive      = {J_APIN},
  author       = {Banerjee, Tumpa and Sinha, Shreyashee and Choudhury, Prasenjit},
  doi          = {10.1007/s10489-021-02845-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9117-9147},
  shortjournal = {Appl. Intell.},
  title        = {Long term and short term forecasting of horticultural produce based on the LSTM network model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple weak supervision for short text classification.
<em>APIN</em>, <em>52</em>(8), 9101–9116. (<a
href="https://doi.org/10.1007/s10489-021-02958-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For short text classification, insufficient labeled data, data sparsity, and imbalanced classification have become three major challenges. For this, we proposed multiple weak supervision, which can label unlabeled data automatically. Different from prior work, the proposed method can generate probabilistic labels through conditional independent model. What’s more, experiments were conducted to verify the effectiveness of multiple weak supervision. According to experimental results on public dadasets, real datasets and synthetic datasets, unlabeled imbalanced short text classification problem can be solved effectively by multiple weak supervision. Notably, without reducing precision, recall, and F1-score can be improved by adding distant supervision clustering, which can be used to meet different application needs.},
  archive      = {J_APIN},
  author       = {Chen, Li-Ming and Xiu, Bao-Xin and Ding, Zhao-Yun},
  doi          = {10.1007/s10489-021-02958-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9101-9116},
  shortjournal = {Appl. Intell.},
  title        = {Multiple weak supervision for short text classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine-learning-based method for automatizing
lattice-boltzmann simulations of respiratory flows. <em>APIN</em>,
<em>52</em>(8), 9080–9100. (<a
href="https://doi.org/10.1007/s10489-021-02808-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many simulation workflows require to prepare the data for the simulation manually. This is time consuming and leads to a massive bottleneck when a large number of numerical simulations is requested. This bottleneck can be overcome by an automated data processing pipeline. Such a novel pipeline is developed for a medical use case from rhinology, where computer tomography recordings are used as input and flow simulation data define the results. Convolutional neural networks are applied to segment the upper airways and to detect and prepare the in- and outflow regions for accurate boundary condition prescription in the simulation. The automated process is tested on three cases which have not been used to train the networks. The accuracy of the pipeline is evaluated by comparing the network-generated output surfaces to those obtained from a semi-automated procedure performed by a medical professional. Except for minor deviations at interfaces between ethmoidal sinuses, the network-generated surface is sufficiently accurate. To further analyze the accuracy of the automated pipeline, flow simulations are conducted with a thermal lattice-Boltzmann method for both cases on a high-performace computing system. The comparison of the results of the respiratory flow simulations yield averaged errors of less than 1% for the pressure loss between the in- and outlets, and for the outlet temperature. Thus, the pipeline is shown to work accurately and the geometrical deviations at the ethmoidal sinuses to be negligible.},
  archive      = {J_APIN},
  author       = {Rüttgers, Mario and Waldmann, Moritz and Schröder, Wolfgang and Lintermann, Andreas},
  doi          = {10.1007/s10489-021-02808-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9080-9100},
  shortjournal = {Appl. Intell.},
  title        = {A machine-learning-based method for automatizing lattice-boltzmann simulations of respiratory flows},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple graph fusion based on riemannian geometry for motor
imagery classification. <em>APIN</em>, <em>52</em>(8), 9067–9079. (<a
href="https://doi.org/10.1007/s10489-021-02975-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In motor imagery-based brain-computer interfaces (BCIs), the spatial covariance features of electroencephalography (EEG) signals that lie on Riemannian manifolds are used to enhance the classification performance of motor imagery BCIs. However, the problem of subject-specific bandpass frequency selection frequently arises in Riemannian manifold-based methods. In this study, we propose a multiple Riemannian graph fusion (MRGF) model to optimize the subject-specific frequency band for a Riemannian manifold. After constructing multiple Riemannian graphs corresponding to multiple bandpass frequency bands, graph embedding based on bilinear mapping and graph fusion based on mutual information were applied to simultaneously extract the spatial and spectral features of the EEG signals from Riemannian graphs. Furthermore, with a support vector machine (SVM) classifier performed on learned features, we obtained an efficient algorithm, which achieves higher classification performance on various datasets, such as BCI competition IIa and in-house BCI datasets. The proposed methods can also be used in other classification problems with sample data in the form of covariance matrices.},
  archive      = {J_APIN},
  author       = {Xie, Xiaofeng and Zou, Xiaokun and Yu, Tianyou and Tang, Rongnian and Hou, Yao and Qi, Feifei},
  doi          = {10.1007/s10489-021-02975-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9067-9079},
  shortjournal = {Appl. Intell.},
  title        = {Multiple graph fusion based on riemannian geometry for motor imagery classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards MANET-based recommender systems for open facilities.
<em>APIN</em>, <em>52</em>(8), 9045–9066. (<a
href="https://doi.org/10.1007/s10489-021-03117-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, most recommender systems are based on a centralized architecture, which can cause crucial issues in terms of trust, privacy, dependability, and costs. In this paper, we propose a decentralized and distributed MANET-based (Mobile Ad-hoc NETwork) recommender system for open facilities. The system is based on mobile devices that collect sensor data about users locations to derive implicit ratings that are used for collaborative filtering recommendations. The mechanisms of deriving ratings and propagating them in a MANET network are discussed in detail. Finally, extensive experiments demonstrate the suitability of the approach in terms of different performance metrics.},
  archive      = {J_APIN},
  author       = {Dunkel, Jürgen and Hermoso, Ramón},
  doi          = {10.1007/s10489-021-03117-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9045-9066},
  shortjournal = {Appl. Intell.},
  title        = {Towards MANET-based recommender systems for open facilities},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolutional networks: Analysis, improvements and
results. <em>APIN</em>, <em>52</em>(8), 9033–9044. (<a
href="https://doi.org/10.1007/s10489-021-02973-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph can represent a complex organization of data in which dependencies exist between multiple entities or activities. Such complex structures create challenges for machine learning algorithms, particularly when combined with the high dimensionality of data in current applications. Graph convolutional networks were introduced to adopt concepts from deep convolutional networks (i.e. the convolutional operations/layers) that have shown good results. In this context, we propose two major enhancements to two of the existing graph convolutional network frameworks: (1) topological information enrichment through clustering coefficients; and (2) structural redesign of the network through the addition of dense layers. Furthermore, we propose minor enhancements using convex combinations of activation functions and hyper-parameter optimization. We present extensive results on four state-of-art benchmark datasets. We show that our approach achieves competitive results for three of the datasets and state-of-the-art results for the fourth dataset while having lower computational costs compared to competing methods.},
  archive      = {J_APIN},
  author       = {Ullah, Ihsan and Manzo, Mario and Shah, Mitul and Madden, Michael G.},
  doi          = {10.1007/s10489-021-02973-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9033-9044},
  shortjournal = {Appl. Intell.},
  title        = {Graph convolutional networks: Analysis, improvements and results},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative strategy network for spatial attention image
captioning. <em>APIN</em>, <em>52</em>(8), 9017–9032. (<a
href="https://doi.org/10.1007/s10489-021-02943-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image captioning is an interesting task that lies at the intersection of computer vision and natural language processing. Although image captioning based on reinforcement learning has made significant progress in the past few years, the problem of inconsistent evaluation indicators for training and testing remains. Reinforcement learning optimizes a single metric, and the caption generated by the model is monotonous and non-characteristics. The model cannot reflect the diversity among images. In response to the above problems, we design a novel image captioning model based on lightweight spatial attention and a generative adversarial network. The lightweight spatial attention module discards the coarse-grained approach of maximum pooling after convolution and transforms the spatial information to preserve key information in the feature map. Then, the game mechanism between the generator and the discriminator is used to optimize the evaluation metric of the model. Finally, we design a discriminator network that cooperates with reinforcement learning to update the model parameters and objectively optimize the language metric inconsistencies between the evaluation and test indicators. We verified the effectiveness of the proposed model on the MS-COCO and Flickr 30K datasets. The experimental results show that the model proposed in this paper achieves state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Zhou, Dongming and Yang, Jing and Bao, Riqiang},
  doi          = {10.1007/s10489-021-02943-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9017-9032},
  shortjournal = {Appl. Intell.},
  title        = {Collaborative strategy network for spatial attention image captioning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FATALRead - fooling visual speech recognition models.
<em>APIN</em>, <em>52</em>(8), 9001–9016. (<a
href="https://doi.org/10.1007/s10489-021-02846-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual speech recognition is essential in understanding speech in several real-world applications such as surveillance systems and aiding differently-abled. It proliferates the research in the realm of visual speech recognition, also known as Automatic Lip Reading (ALR). In recent years, Deep Learning (DL) methods are being utilised for developing ALR systems. DL models tend to be vulnerable to adversarial attacks. Studying these attacks creates new research directions in designing robust DL systems. Existing attacks on images and videos classification models are not directly applicable to ALR systems. Since the ALR systems encompass temporal information, attacking these systems is comparatively more challenging and strenuous than attacking image classification models. Similarly, compared to other video classification tasks, the region-of-interest is smaller in the case of ALR systems. Despite these factors, our proposed method, Fooling AuTomAtic Lip Reading (FATALRead), can successfully perform adversarial attacks on state-of-the-art ALR systems. To the best of our knowledge, we are the first to successfully fool ALR systems for the word recognition task. We further demonstrate that the success of the attack is increased by incorporating logits instead of probabilities in the loss function. Our extensive experiments on a publicly available dataset, show that our attack successfully circumvents the well-known transformation based defences.},
  archive      = {J_APIN},
  author       = {Gupta, Anup Kumar and Gupta, Puneet and Rahtu, Esa},
  doi          = {10.1007/s10489-021-02846-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {9001-9016},
  shortjournal = {Appl. Intell.},
  title        = {FATALRead - fooling visual speech recognition models},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute reduction approaches under interval-valued q-rung
orthopair fuzzy soft framework. <em>APIN</em>, <em>52</em>(8),
8975–9000. (<a
href="https://doi.org/10.1007/s10489-021-02853-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interval-valued q-rung orthopair fuzzy sets and soft sets are two different uncertainty theories to cope with incomplete and uncertain information in several real-world multi-attribute decision-making (MADM) situations. This study develops a novel hybrid model called interval-valued q-rung orthopair fuzzy soft sets (IVqROFSSs, for brevity) to generalize the interval-valued q-rung orthopair fuzzy set model and to address the decision-makers preference information more effectively in complicated MADM processes. Afterward, some basic useful properties of the proposed model are explored, including subset relation, complement, union, intersection, the ‘AND’ operation, and the ‘OR’ operation. Further, four kinds of attribute reduction techniques for IVqROFSSs are presented. An algorithm for each reduction approach is developed and explained through an illustrative numerical example which verifies that developed reduction methods remove the redundant attributes by preserving the ranking order of decision objects unchanged. Later on, an application is proposed, that is, site selection for a wind power plant, to explain the developed model’s reliability and its reduction approaches. Finally, the proposed hybrid model and its attribute reduction methods are compared with some existing models, and their attribute reduction approaches, respectively.},
  archive      = {J_APIN},
  author       = {Ali, Ghous and Afzal, Muhammad and Asif, Muhammad and Shazad, Adeel},
  doi          = {10.1007/s10489-021-02853-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8975-9000},
  shortjournal = {Appl. Intell.},
  title        = {Attribute reduction approaches under interval-valued q-rung orthopair fuzzy soft framework},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep multivariate time series multistep forecasting
network. <em>APIN</em>, <em>52</em>(8), 8956–8974. (<a
href="https://doi.org/10.1007/s10489-021-02899-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to that multivariate time series, multistep forecasting technology has a guiding role in many fields, such as electricity consumption, traffic flow detection, and stock price prediction, many approaches have been proposed, seeking to realize accurate prediction based on historical data. However, multivariate time series in real-world applications often contain complex and non-linear interdependencies between time steps and series, for which traditional approaches that just model dependencies in time dimension may fail to make forecasting accurately. In addition, the traditional cyclic multistep prediction strategy leads to error accumulation and consequently results in the reduction of prediction performance. To address these limitations, we propose a novel Deep Multivariate Time Series Multistep Forecasting Network based on the Encoder-Decoder model, abbreviated as DualMNet. DualMNet uses the temporal patterns module to capture long-term patterns between time steps, and, simultaneously, employs the spatial patterns module to discover the interdependencies between time series. Furthermore, the decoder utilizes the historical data of the target sequence to predict the long time-series sequences at the final forward operation, significantly ameliorating the issue of error accumulation. Finally, the conducted t-test results, based upon the extensive experimental results on the three benchmark multivariate time series datasets, they demonstrate that the multistep predictive performance of DualMNet is significantly superior to those of the comparison models, and the ablation study shows that integrating all the components of DualMNet together results in robust forecasting performance.},
  archive      = {J_APIN},
  author       = {Yin, Chenrui and Dai, Qun},
  doi          = {10.1007/s10489-021-02899-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8956-8974},
  shortjournal = {Appl. Intell.},
  title        = {A deep multivariate time series multistep forecasting network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified classifiability analysis framework based on
meta-learner and its application in spectroscopic profiling data.
<em>APIN</em>, <em>52</em>(8), 8947–8955. (<a
href="https://doi.org/10.1007/s10489-021-02810-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectroscopic profiling data (e.g., Raman spectroscopy and mass spectroscopy), combined with machine learning, have provided a data-driven approach for discriminative tasks. In these tasks, researchers often start with simple classification models. If one model doesn’t work, they will try more sophisticated models. If all models fail, the researchers will deem the data set as “inseparable.“ This “trial-and-error” practice reveals a fundamental question: does the dataset possess the necessary statistical power for the current discriminative task? This “classifiability analysis” is an implicit and often neglected step in the data-driven pipeline. This paper aims to design a unified methodological framework for classifiability analysis. In this framework, a meta-learner model combines diversified atom metrics (e.g., Bayes error rate / irreducible error, classification accuracy, information gain / mutual information) into one unified metric (d). We have successfully used the proposed framework to analyze a spectroscopic profiling dataset to discriminate vintage liquors of different ages. A significant difference (d = 1.447. d &gt; 0.8 indicates a significant difference) between 5-year and 16-year liquors.},
  archive      = {J_APIN},
  author       = {Zhang, Yinsheng and Zhang, Zhengyong and Wang, Haiyan},
  doi          = {10.1007/s10489-021-02810-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8947-8955},
  shortjournal = {Appl. Intell.},
  title        = {A unified classifiability analysis framework based on meta-learner and its application in spectroscopic profiling data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel clustering algorithm based on the
gravity-mass-square ratio and density core with a dynamic denoising
radius. <em>APIN</em>, <em>52</em>(8), 8924–8946. (<a
href="https://doi.org/10.1007/s10489-021-02753-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis has been widely used in pattern recognition, image segmentation, document clustering, intrusion detection, market research, and so on. There have been many clustering algorithms for cluster analysis now. However, most of them are unsuitable for complex patterns with large variations in density and manifold structure. Also, they are not robust to noises. To overcome the above deficiencies, we propose a novel clustering algorithm (called DCLGMS) based on the gravity-mass-square ratio (LGMS) and density core (DCore) with a dynamic denoising radius. Our algorithm can obtain the correct clustering result in arbitrarily shaped datasets excluding noises without parameter settings. In this algorithm, DCore can maintain the shapes of clusters, and LGMS can help to extract local core points. In addition, a dynamic denoising radius is beneficial to detect noise points caused by arbitrary reasons and avoid interference at the beginning of the algorithm. The results of experiments on both synthetic datasets and real datasets show that our algorithm has excellent performance.},
  archive      = {J_APIN},
  author       = {Zhang, Yu-Fang and Wang, Yu-Qin and Li, Ge-Ge and Gao, Qin-Qin and Gao, Qiang and Xiong, Zhong-Yang and Zhang, Min},
  doi          = {10.1007/s10489-021-02753-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8924-8946},
  shortjournal = {Appl. Intell.},
  title        = {A novel clustering algorithm based on the gravity-mass-square ratio and density core with a dynamic denoising radius},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). VARL: A variational autoencoder-based reinforcement
learning framework for vehicle routing problems. <em>APIN</em>,
<em>52</em>(8), 8910–8923. (<a
href="https://doi.org/10.1007/s10489-021-02920-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem as a classic NP-hard problem could be optimized by path choices due to its practical application value. This study proposes a novel variational autoencoder framework for path optimization on graphs, involving graph neural networks and generative adversarial networks. We took the center node as the root node to divide the graph into different subgraphs and find the nodes that compose the optimal solution through variational reasoning. We next used reinforcement learning to optimize the entire variational framework end-to-end. This contribution can also apply in both modeling and training combinatorial optimization over graphs. An extensive experiment on different scales of traveling salesman and vehicle routing instances was conducted. The findings indicate that our framework is efficient and effective in learning and reasoning, and its accuracy and generalization outperform the baselines.},
  archive      = {J_APIN},
  author       = {Wang, Qi},
  doi          = {10.1007/s10489-021-02920-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8910-8923},
  shortjournal = {Appl. Intell.},
  title        = {VARL: A variational autoencoder-based reinforcement learning framework for vehicle routing problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transformer-based networks over tree structures for code
classification. <em>APIN</em>, <em>52</em>(8), 8895–8909. (<a
href="https://doi.org/10.1007/s10489-021-02894-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software engineering (SE), code classification and related tasks, such as code clone detection are still challenging problems. Due to the elusive syntax and complicated semantics in software programs, existing traditional SE approaches still have difficulty differentiating between the functionalities of code snippets at the semantic level with high accuracy. As artificial intelligence (AI) techniques have increased in recent years, exploring different machine/deep learning techniques for code classification algorithms has become important. However, most existing machine/deep learning-based approaches often consider using convolutional neural networks (CNNs) or recurrent neural networks (RNNs) to process code texts. However, the two networks inevitably suffer from gradient vanishing problems and fail to capture long-distance dependencies from code statements, resulting in poor performance in downstream tasks. In this paper, we propose the TBCC (Transformer-Based Code Classifier), a novel transformer-based neural network for programming language processing, which can avoid these two problems. Moreover, to capture the important syntactical features from programming languages, we split the deep abstract syntax trees (ASTs) into smaller subtrees that, aim to exploit syntactical information in code statements. We have applied the TBCC to two different common program comprehension tasks to verify its effectiveness: a code classification task for C programs and a code clone detection task for Java programs. The experimental results show that the TBCC achieves state-of-the-art performance, outperforming the baseline methods in terms of accuracy, recall, and F1 score. For subsequent research, the code of TBCC has been released ∗.},
  archive      = {J_APIN},
  author       = {Hua, Wei and Liu, Guangzhong},
  doi          = {10.1007/s10489-021-02894-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8895-8909},
  shortjournal = {Appl. Intell.},
  title        = {Transformer-based networks over tree structures for code classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generation and verification of learned stochastic automata
using k-NN and statistical model checking. <em>APIN</em>,
<em>52</em>(8), 8874–8894. (<a
href="https://doi.org/10.1007/s10489-021-02884-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deriving an accurate behavior model from historical data of a black box for verification and feature forecasting is seen by industry as a challenging issue especially for a large featured dataset. This paper focuses on an alternative approach where stochastic automata can be learned from time-series observations captured from a set of deployed sensors. The main advantage offered by such techniques is that they enable analysis and forecasting from a formal model instead of traditional learning methods. We perform statistical model checking to analyze the learned automata by expressing temporal properties. For this purpose, we consider a critical water infrastructure that provides a scenario based on a set of input and output values of heterogeneous sensors to regulate the dam spill gates. The method derives a consistent approximate model with traces collected over thirty years. The experiments show that the model provides not only an approximation of the desired output of a feature value but, also, forecasts the ebb and flow of the sensed data.},
  archive      = {J_APIN},
  author       = {Baouya, Abdelhakim and Chehida, Salim and Ouchani, Samir and Bensalem, Saddek and Bozga, Marius},
  doi          = {10.1007/s10489-021-02884-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8874-8894},
  shortjournal = {Appl. Intell.},
  title        = {Generation and verification of learned stochastic automata using k-NN and statistical model checking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The design of soft recoding-based strategies for improving
error-correcting output codes. <em>APIN</em>, <em>52</em>(8), 8856–8873.
(<a href="https://doi.org/10.1007/s10489-021-02870-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Error-Correcting Output Codes (ECOC) algorithms had been proposed based on the hard coding (HC) schemes: binary coding {1, 0} or ternary coding {+ 1, -1, 0}. This paper introduces two novel strategies to recode the original code matrices with the mean values and the intervals of learners’ outputs, which are named Mean Value Recoding (MVR) and Interval Recoding (IR) strategies. Both strategies are designed to reduce the distance between the outputs of base learners and the target codewords, aiming to produce more accurate results compared with the HC schemes. It is the first time that two concepts, soft recoding, and learner dependent, are injected into the ECOC framework to the best of our knowledge. To verify the effectiveness of our strategies, four data-independent ECOC algorithms and two data-dependent ECOC algorithms are deployed in the experiments based on UCI data sets. The experiments are carried out using the original HC strategies and our soft recoding strategies, and results verify that our strategies outperform the HC-based algorithms in most cases by producing balanced results among classes. In short, our strategies can improve the performance of different ECOC algorithms. Our python code and the corresponding data sets are available for non-commercial or research use at: https://github.com/MLDMXM2017/softcoding-ECOC .},
  archive      = {J_APIN},
  author       = {Liu, Kun-Hong and Ye, Xiao-Na and Guo, Hong-Zhou and Wu, Qing-Qiang and Hong, Qing-Qi},
  doi          = {10.1007/s10489-021-02870-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8856-8873},
  shortjournal = {Appl. Intell.},
  title        = {The design of soft recoding-based strategies for improving error-correcting output codes},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovery of closed high utility itemsets using a fast
nature-inspired ant colony algorithm. <em>APIN</em>, <em>52</em>(8),
8839–8855. (<a
href="https://doi.org/10.1007/s10489-021-02922-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high utility itemset (HUIM) from an extensive database is a crucial descriptive task in data mining, which considers both the quantity and unit profit factor in revealing the ultimately profitable items. However, it may discover a vast number of HUIs which can be challenging to interpret by a user and also reduce the efficiency of the mining process. A solution to this problem is to mine a Closed high utility itemset, a more compact and lossless form of HUIs. In this paper, a fast nature-inspired meta-heuristic approach CHUI-AC (Closed high utility itemset mining using ant colony algorithm) has been introduced to mine CHUIs. This is the first work on mining CHUI using a nature-inspired ant colony algorithm. CHUI-AC maps the feasible solution space to a directed graph with quadratic space complexity to guide the searching efficiently. Several experiments on real-world datasets show that the proposed algorithm outrun the state-of-the-art algorithms in terms of execution time and rate of convergence. Moreover, the scalability experiments demonstrate that CHUI-AC is linearly scalable with respect to the number of transaction and number of items.},
  archive      = {J_APIN},
  author       = {Pramanik, Subhadip and Goswami, Adrijit},
  doi          = {10.1007/s10489-021-02922-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8839-8855},
  shortjournal = {Appl. Intell.},
  title        = {Discovery of closed high utility itemsets using a fast nature-inspired ant colony algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vehicle anomalous trajectory detection algorithm based on
road network partition. <em>APIN</em>, <em>52</em>(8), 8820–8838. (<a
href="https://doi.org/10.1007/s10489-021-02867-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of carrying passengers, taxi drivers may have fraud problems. To solve the problem, we propose an abnormal trajectory detection algorithm(RNPAT) via road network partition in the paper which is divided into four stages: map matching, road network partition based on insert points, off-line training, and anomaly detection. A trajectory is converted into a series of ordered combinations of points sequence to make it follow the actual direction of the road network at the stage of map matching, and the problem of low data quality obtained by location devices is solved. In the road network partition phase, the missing point at the intersection of adjacent roads of trajectory is calculated, and then according to insert points, trajectories are divided to train the road consumption. At the stage of off-line training, the consumption of the road is modeled, and the Dijkstra algorithm is used to train the minimum consumption between each S-D pair, in which S is the starting point of vehicle operation and D is the destination of vehicle operation. In the anomaly detection phase, we calculate the consumption threshold matrix supporting anomaly detection and the consumption of each trajectory, and compare the trajectory’s consumption with corresponding threshold to judge whether the trajectory is abnormal. Finally, the effectiveness of RNPAT is verified by Shanghai Taxi data. In addition, RNPAT is compared with TADSS and TRAOD validates that RNPAT has higher efficiency and higher accuracy.},
  archive      = {J_APIN},
  author       = {Zhao, Xujun and Su, Jianhua and Cai, Jianghui and Yang, Haifeng and Xi, Tingting},
  doi          = {10.1007/s10489-021-02867-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8820-8838},
  shortjournal = {Appl. Intell.},
  title        = {Vehicle anomalous trajectory detection algorithm based on road network partition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trine: Syslog anomaly detection with three transformer
encoders in one generative adversarial network. <em>APIN</em>,
<em>52</em>(8), 8810–8819. (<a
href="https://doi.org/10.1007/s10489-021-02863-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System logs provide powerful support for maintaining system security and stability, but the determination of anomalies often relies on sequence context while hiding in the traces under the massive background normal behavior. Recently transformers have shown remarkable success in feature extraction of long sequences and text classification tasks. Thus, we combine our syslog anomaly detection work with implementing multiple application methods in an integrated model. That is, our proposed generative adversarial network based on three transformer encoders, which is called Trine. One of the encoders is used to extract feature representations of the system logs, while the other two respectively serve as a generator and a discriminator for Generative Adversarial Networks to mitigate the class imbalance of the data. We evaluated Trine on two real-world datasets, HDFS and OpenStack. It shows great competitiveness compared with current state-of-the-art models for syslog anomaly detection. The experimental results demonstrate that the best architecture of our model get an F1-score 0.906, at least 27.8% higher than previous methods.},
  archive      = {J_APIN},
  author       = {Zhao, Zhenfei and Niu, Weina and Zhang, Xiaosong and Zhang, Runzi and Yu, Zhenqi and Huang, Cheng},
  doi          = {10.1007/s10489-021-02863-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8810-8819},
  shortjournal = {Appl. Intell.},
  title        = {Trine: Syslog anomaly detection with three transformer encoders in one generative adversarial network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-dimensional bhattacharyya bound linear discriminant
analysis with its applications. <em>APIN</em>, <em>52</em>(8),
8793–8809. (<a
href="https://doi.org/10.1007/s10489-021-02843-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed L2-norm linear discriminant analysis criterion based on Bhattacharyya error bound estimation (L2BLDA) was an effective improvement over linear discriminant analysis (LDA) and was used to handle vector input samples. When faced with two-dimensional (2D) inputs, such as images, converting two-dimensional data to vectors, regardless of the inherent structure of the image, may result in some loss of useful information. In this paper, we propose a novel two-dimensional Bhattacharyya bound linear discriminant analysis (2DBLDA). 2DBLDA maximizes the matrix-based between-class distance, which is measured by the weighted pairwise distances of class means and minimizes the matrix-based within-class distance. The criterion of 2DBLDA is equivalent to optimizing the upper bound of the Bhattacharyya error. The weighting constant between the between-class and within-class terms is determined by the involved data that make the proposed 2DBLDA adaptive. The construction of 2DBLDA avoids the small sample size (SSS) problem, is robust, and can be solved through a simple standard eigenvalue decomposition problem. The experimental results on image recognition and face image reconstruction demonstrate the effectiveness of 2DBLDA.},
  archive      = {J_APIN},
  author       = {Guo, Yan-Ru and Bai, Yan-Qin and Li, Chun-Na and Bai, Lan and Shao, Yuan-Hai},
  doi          = {10.1007/s10489-021-02843-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8793-8809},
  shortjournal = {Appl. Intell.},
  title        = {Two-dimensional bhattacharyya bound linear discriminant analysis with its applications},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A q-rung orthopair fuzzy MARCOS method using novel score
function and its application to solid waste management. <em>APIN</em>,
<em>52</em>(8), 8770–8792. (<a
href="https://doi.org/10.1007/s10489-021-02921-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of the current study is to explore a novel q-rung orthopair fuzzy score function and extend the measurement of alternatives and ranking according to the compromise solution (MARCOS) method with unknown weight information to the context of q-rung orthopair fuzzy numbers (q-ROFNs). For this, first, the drawbacks of the existing score functions are highlighted via several solid examples. Then, to fill the gaps of the existing ones, a novel score function and its relevant characteristics are delineated. To determine the objective weights of criteria, q-rung orthopair fuzzy criteria importance through intercriteria correlation (CRITIC) method is modeled based on the derived weights of decision-makers (DMs), standard deviation, and correlation coefficient. Following that, the q-rung orthopair fuzzy MARCOS approach is established to cope with multi-criteria group decision-making (MCGDM) problems. Later, a case study of solid waste management is addressed to show the practicality of the presented method. Lastly, the derived results are validated through three phases: two sensitivity analyses, rank reversal phenomena, and comparative analysis.},
  archive      = {J_APIN},
  author       = {Ali, Jawad},
  doi          = {10.1007/s10489-021-02921-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8770-8792},
  shortjournal = {Appl. Intell.},
  title        = {A q-rung orthopair fuzzy MARCOS method using novel score function and its application to solid waste management},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose estimation for workpieces in complex stacking
industrial scene based on RGB images. <em>APIN</em>, <em>52</em>(8),
8757–8769. (<a
href="https://doi.org/10.1007/s10489-021-02857-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, pose estimation algorithms have been widely discussed. Although been investigated by many prior works, pose estimation for heavily stacked workpieces, like bins, is still a challenge for industrial applications. Moreover, the scene of stacked workpieces is much more arduous than the general scene for robots to carry out routine tasks, such as object detection and picking. This paper aims to address the problem of pose estimation for stacked symmetrical workpieces in the presence of partial occlusions and cluttered backgrounds. To tackle those problems, we propose a novel pose estimation method for industrial stacking scenes based on RGB images. Specifically, due to the common symmetry characteristics of industrial workpieces, we firstly present a new standardized spatial representation method, which could auto-encode the 2D-3D correspondences of symmetrical workpieces. Besides, we introduce a novel GAN-based deep neural network model to reconstruct the representation of stacked workpieces. Based on that, the pose of the target workpieces is predicted based on the reconstructed expression and an improved RANSAC-PnP algorithm. Finally, comprehensive experiments demonstrate that the proposed method outperforms state-of-the-art methods, especially in complex stacking scenes.},
  archive      = {J_APIN},
  author       = {Zhang, Yajun and Yi, Jianjun and Chen, Yuanhao and Dai, Zhiyong and Han, Fei and Cao, Shuqing},
  doi          = {10.1007/s10489-021-02857-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8757-8769},
  shortjournal = {Appl. Intell.},
  title        = {Pose estimation for workpieces in complex stacking industrial scene based on RGB images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local linear embedding based interpolation neural network in
pancreatic tumor segmentation. <em>APIN</em>, <em>52</em>(8), 8746–8756.
(<a href="https://doi.org/10.1007/s10489-021-02847-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low resolution of thick CT/MR with large spacing will lead to misdiagnosis and bring a huge difficulty to the automatic organ or lesion segmentation. Especially in automatic pancreatic tumor segmentation, due to the limitation of equipment, the tumors only exist in a few slices, and the continuity between slices of 3D CT/MR image is poor. Besides, the features of tumors, such as the size, shape, location, intensity and so on, vary dramatically according to different cases. The fuzzy boundaries of small tumors also bring great uncertainty to the segmentation task. Aiming to solve those problems, we introduce the LLE-based interpolation neural network into the pancreatic tumor segmentation task, which mainly includes the following improvements: 1) We utilize local linear embedding (LLE) to model the relationship between adjacent slices and the interpolated slice. It adapts the spatial transformation of the organ between slices. 2) Neural network, combining with the LLE module, is designed to significantly enhance the image resolution, thus can generate more continuous and clearer images for each sequence. 3) Multiscale cascade strategy is adopted in the network to reduce the influence of drastic changes in tumor size on segmentation results. Experiments are carried out in MR images with 3.5mm thickness provided by Changhai hospital, and CT images on Medical Segmentation Decathlon pancreatic tumor challenge, respectively. The results show that our proposed model has a sensitivity of 96.55% and a dice coefficient of 62.8% (± 25.8%) on the MR dataset, and a dice coefficient of 50.6% (± 30.9%) on the CT dataset.},
  archive      = {J_APIN},
  author       = {Yang, Xiaoyu and Chen, Yufei and Yue, Xiaodong and Ma, Chao and Yang, Panpan},
  doi          = {10.1007/s10489-021-02847-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8746-8756},
  shortjournal = {Appl. Intell.},
  title        = {Local linear embedding based interpolation neural network in pancreatic tumor segmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed wavelet neural networks. <em>APIN</em>,
<em>52</em>(8), 8735–8745. (<a
href="https://doi.org/10.1007/s10489-021-02892-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing the wavelet theory, the wavelet coefficients with respect to translation and scaling factors can be obtained through the iteration of neural network, which effectively solves the window immobility problem of short time Fourier transform. Notice that, the centralized wavelet neural network is weak in the representation of signal integrity as localized information is lost in the case of the signal properties are large span time-domain and high frequency. To solve this problem, the learning algorithm of distributed wavelet neural networks (DWNNs) based on domain segment is proposed in this paper, where the time frequency characteristic of objective function is set as a constraint condition, while the time width, frequency, center of time domain and band center of the wavelet are employed to determine the translation and scaling parameters. Especially, wavelet networks are distributed into several orthogonal subnet by partitioning the information of input data, and the complexity of calculating for each nodes is thus decreased. Moreover, simulations are presented to demonstrate the advantages and efficiency of the proposed DWNNs.},
  archive      = {J_APIN},
  author       = {Yang, Wangzhuo and Chen, Bo and Yu, Li},
  doi          = {10.1007/s10489-021-02892-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8735-8745},
  shortjournal = {Appl. Intell.},
  title        = {Distributed wavelet neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-hop analysis method for rich-club phenomenon of
influence maximization in social networks. <em>APIN</em>,
<em>52</em>(8), 8721–8734. (<a
href="https://doi.org/10.1007/s10489-021-02818-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When studying the Influence Maximization (IM) problem in social networks, it is found that the initial seed nodes selected by some heuristic algorithms have the characteristics of aggregation. This situation is called the rich club phenomenon of seed sets. Once the seed node is over-aggregated, it will limit the spread of influence. Therefore, analyzing the rich-club phenomenon is necessary for solving the IM problem. The paper proposes the rich club coefficient and reactivation rate to quantitatively analyze this phenomenon. The analysis mainly focuses on the relationship between the hop distance and the propagation probability. In addition, the relationship between the rich club phenomenon and the IM problem is also an aspect of concern. When dealing with the main aspects, a key problem needs to be solved, which is that the optimal range of hop is different under different propagation probabilities. To solve this problem, the Multi Hop Remove (MHR) algorithm is proposed, which is based on the independent cascade model. By the MHR algorithm, the hop range is determined under different propagation probabilities. According to our experimental results, the more serious the rich club phenomenon accumulates, the smaller the influence spread. To reduce the obstruction of this phenomenon, the multi-hop selection of seed nodes is a superior solution.},
  archive      = {J_APIN},
  author       = {Duan, Xiuliang and Qiu, Liqing and Sun, Chengai and Shi, Qiang},
  doi          = {10.1007/s10489-021-02818-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8721-8734},
  shortjournal = {Appl. Intell.},
  title        = {Multi-hop analysis method for rich-club phenomenon of influence maximization in social networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust semi non-negative low-rank graph embedding algorithm
via the l21 norm. <em>APIN</em>, <em>52</em>(8), 8708–8720. (<a
href="https://doi.org/10.1007/s10489-021-02837-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-negative matrix factorization (NMF) method is applied in many fields, including pattern recognition, visual analysis, and biomedicine. However, existing NMF methods acquire low dimensional representation directly from high dimensional data, and the low-rank part of the data contains the most useful information. Furthermore, the NMF method is vulnerable to noise interference and is not adequately robust. Therefore, a robust semi non-negative low-rank graph embedding algorithm via the L21 norm (RSNLGEL21) is proposed to address the stated problems. The algorithm considers the effective low-rank structure and geometric information of the original data and constrains the coefficient matrix to be non-negative; however, it does not impose a non-negative constraint on the base matrix. In addition, the L21 norm is introduced into the graph embedding and data reconstruction functions slightly to improve its recognition ability and robustness. Moreover, this paper also provides an iterative formula to the RSNLGEL21 algorithm and the corresponding proof of convergence and to analyze its computational complexity. Experiments on the ORL, FEI, AR, and FERET image datasets shows that the RSNLGEL21 algorithm has certain advantages in clustering performance.},
  archive      = {J_APIN},
  author       = {Liu, Guoqing and Ge, Hongwei and JinlongYang and Wang, Shuangxi},
  doi          = {10.1007/s10489-021-02837-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8708-8720},
  shortjournal = {Appl. Intell.},
  title        = {Robust semi non-negative low-rank graph embedding algorithm via the l21 norm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection via minimizing global redundancy for
imbalanced data. <em>APIN</em>, <em>52</em>(8), 8685–8707. (<a
href="https://doi.org/10.1007/s10489-021-02855-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining knowledge from imbalanced data is challenging due to the uneven distribution of classes and increasing dimensionality of data accumulated from real-life applications. Selecting informative features from imbalanced data is especially important for building an effective learning method. The global redundancy and the effect of imbalanced distribution need to be considered simultaneously. In this study, a feature selection method that considers the imbalanced distribution of classes in data is investigated by embedding the weighted constraint on the majority class into the global redundancy minimization GRM framework. Global redundancy minimization is acquired through an objective function that contains a feature redundancy matrix and feature scores. A new form of regularization to a within-class scatter matrix is first presented, which emphasizes the minority class and replaces the redundancy measurement approach. Then, after employing this new form of a within-class scatter matrix in GRM and taking the between-class distance as the GRM input score, a GRM-based discriminant feature selection algorithm (GRM-DFS) is proposed. Comparison studies on a within-class scatter matrix with different forms of regularization indicate that the proposed form of a within-class scatter matrix is effective when dealing with imbalanced data. Experiments on public imbalanced datasets are performed. The experimental results indicate that GRM-DFS is effective.},
  archive      = {J_APIN},
  author       = {Huang, Shuhao and Chen, Hongmei and Li, Tianrui and Chen, Hao and Luo, Chuan},
  doi          = {10.1007/s10489-021-02855-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8685-8707},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection via minimizing global redundancy for imbalanced data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep finesse network model with multichannel syntactic and
contextual features for target-specific sentiment classification.
<em>APIN</em>, <em>52</em>(8), 8664–8684. (<a
href="https://doi.org/10.1007/s10489-021-02692-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target-specific sentiment classification has a dependency over the target term extraction. The majority of current studies in sentiment classification tasks do not utilize the complete linguistic and sentiment knowledge. Consequently, strenuous efforts are to be made for expressing the implications of each word from the sentences, which have significant amount of contextual dependencies. Hence, it leads to the problems like loss of semantics, missing of context-dependent information and also results in poor classification of the models. In this paper, we propose a Deep Finesse Network (DFN) to address these limitations and enhance the accuracy. The DFN employs a multichannel paradigm to exploit multi-grained sentiment features by leveraging the existing linguistic and sentiment knowledge more effectively without any human involvement. In each channel, the model firstly extracts the local features from the multi-grained sentiment features and then captures the global and spatial information of the identified local features. Secondly, it directly models the contextual relationships with enriched semantic information from the global features. Subsequently, the intra-sequence relations were also modeled among the contextual features to identify the target features in order to understand and predict the sentiments of identified contextual features. Finally, the effectiveness of the DFN is also evaluated on different datasets. The results proved that DFN outperforms all the current and advanced state-of-art models in classification accuracy in most cases.},
  archive      = {J_APIN},
  author       = {Edara, Deepak Chowdary and Sistla, Venkatramaphanikumar and Kolli, Venkata Krishna Kishore},
  doi          = {10.1007/s10489-021-02692-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8664-8684},
  shortjournal = {Appl. Intell.},
  title        = {Deep finesse network model with multichannel syntactic and contextual features for target-specific sentiment classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical attention and feature projection for
click-through rate prediction. <em>APIN</em>, <em>52</em>(8), 8651–8663.
(<a href="https://doi.org/10.1007/s10489-021-02931-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Click-through rate (CTR) prediction plays an important role in many industrial applications, feature engineering directly influences CTR prediction performance because features are normally the multi-field type. However, the existing CTR prediction techniques either neglect the importance of each feature or regard the feature interactions equally for feature learning. In addition, using an inner product or a Hadamard product is too simple to effectively model the feature interactions. These limitations lead to suboptimal performances of existing models. In this paper, we propose a framework called Hierarchical Attention and Feature Projection neural network (HAFP) for CTR prediction, which enables the automatically learning of more representative and efficient feature representation in an end-to-end manner. Towards this end, we employ a feature learning layer with a hierarchical attention mechanism to jointly extract more generalized and dominant features and feature interactions. In addition, a projective bilinear function is designed in meaningful second-order interaction encoder to effectively learn more fine-grained and comprehensive second-order feature interactions. Taking advantages of the hierarchical attention mechanism and the projective bilinear function, our proposed model can not only model feature learning in a flexible fashion, but also provide an interpretable capability of the prediction results. Experimental results on two real-world datasets demonstrate that HAFP outperforms the state-of-the-art in terms of Logloss and AUC for CTR prediction baselines. Further analysis verifies the importance of the proposed hierarchical attention mechanism and the projective bilinear function for modelling the feature representation, showing the rationality and effectiveness of HAFP.},
  archive      = {J_APIN},
  author       = {Zhang, Jinjin and Zhong, Chengliang and Fan, Shouxiang and Mu, Xiaodong and Ni, Zhen},
  doi          = {10.1007/s10489-021-02931-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8651-8663},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical attention and feature projection for click-through rate prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel hierarchical clustering algorithm with merging
strategy based on shared subordinates. <em>APIN</em>, <em>52</em>(8),
8635–8650. (<a
href="https://doi.org/10.1007/s10489-021-02830-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical clustering is a common unsupervised learning technique that is used to discover potential relationships in data sets. Despite the conciseness and interpretability, hierarchical clustering algorithms still face some challenges such as inaccuracy, time-consuming, and difficulty in choosing merging strategies. To overcome these limitations, we propose a novel Hierarchical Clustering algorithm with a Merging strategy based on Shared Subordinates (HCMSS), which defines new concepts of the local core representative and the shared subordinate belonging to multiple representatives. First, the state-of-the-art natural neighbor (NaN) is introduced to compute the local neighborhood and the local density of each data point. Next, a sharing-based local core searching algorithm (SLORE) is proposed to find local core points and divide the input data set into numerous initial small clusters. Lastly, these small clusters are merged hierarchically and form the final clustering result. We creatively split the merging process into two sub-steps: first, pre-connecting small clusters according to a shared-subordinates-based indicator that measures the stickiness between clusters; second, merging the pre-connected intermediate clusters and the remaining unconnected small clusters in a classical hierarchical way. Experiments on 8 synthetic and 8 real-world data sets demonstrate that HCMSS can effectively improve the clustering accuracy and is less time-consuming than 2 state-of-the-art benchmarks.},
  archive      = {J_APIN},
  author       = {Shi, Jinxin and Zhu, Qingsheng and Li, Junnan},
  doi          = {10.1007/s10489-021-02830-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8635-8650},
  shortjournal = {Appl. Intell.},
  title        = {A novel hierarchical clustering algorithm with merging strategy based on shared subordinates},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Generalized zero-shot emotion recognition from body
gestures. <em>APIN</em>, <em>52</em>(8), 8616–8634. (<a
href="https://doi.org/10.1007/s10489-021-02927-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human-human interaction, body language is one of the most important emotional expressions. However, each emotion category contains abundant emotional body gestures, and basic emotions used in most researches are difficult to describe complex and diverse emotional states. It is costly to collect sufficient samples of all emotional expressions, and new emotions or new body gestures that are not included in the training set may appear during testing. To address the above problems, we design a novel mechanism that treats each emotion category as a collection of multiple body gesture categories to make better use of gesture information for emotion recognition. A Generalized Zero-Shot Learning (GZSL) framework is introduced to recognize both seen and unseen body gesture categories with the help of semantic information, and emotion predictions are further provided based on the relationship between gestures and emotions. This framework consists of two branches. The first branch is a Hierarchical Prototype Network (HPN) which learns the prototypes of body gestures and uses them to calculate the emotion attentive prototypes. This branch aims to obtain predictions on samples of the seen gesture categories. The second branch is a Semantic Auto-Encoder (SAE) which utilizes semantic representations to predict samples of unseen gesture categories. Thresholds are further trained to determine which branch result will be used during testing, and the emotion labels are finally obtained from these results. Comprehensive experiments are conducted on an emotion recognition dataset which contains skeleton data of multiple body gestures, and the performance of our framework is superior to both the traditional emotion classifier and state-of-the-art zero-shot learning methods.},
  archive      = {J_APIN},
  author       = {Wu, Jinting and Zhang, Yujia and Sun, Shiying and Li, Qianzhong and Zhao, Xiaoguang},
  doi          = {10.1007/s10489-021-02927-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8616-8634},
  shortjournal = {Appl. Intell.},
  title        = {Generalized zero-shot emotion recognition from body gestures},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BPSL: A new rumor source location algorithm based on the
time-stamp back propagation in social networks. <em>APIN</em>,
<em>52</em>(8), 8603–8615. (<a
href="https://doi.org/10.1007/s10489-021-02919-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a rumor source is a major issue in the analysis of social networks. In this problem, the rumor source is usually estimated from a given diffusion snapshot. How to estimate the rumor source accurately is a challenging problem. Usually, the rumor source location problem is regarded as a node ranking problem. However, most of the existing algorithms ignore the structure of the infected subgraph or the randomness of the rumor spread. Therefore, they have defects in applicability and accuracy. To solve this problem, this paper takes into account the above two aspects at the same time, and propose a new algorithm to locate the rumor source, which is called Back Propagation Source Location(BPSL). The proposed algorithm contains an estimation method which is based on the time-stamp back propagation. This method makes the proposed algorithm’s accuracy outperform previous algorithms’ accuracy. Moreover, the susceptible-infected model is used to simulate the information spread of the networks. The steps of the proposed algorithm can be stated as follows. First, a new method based on the influence maximization is proposed to determine the observer set, which can greatly reduce the number of observer nodes. Second, a new estimation method based on the time-stamp back propagation is proposed to locate the source, which makes the proposed algorithm more accuracy and doesn’t change the structure of infected subgraph at the same time. Finally, the experimental results on two artificial networks and four real-world networks show the superiority of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Qiu, Liqing and Sai, Shiqi and Wei, Moji},
  doi          = {10.1007/s10489-021-02919-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8603-8615},
  shortjournal = {Appl. Intell.},
  title        = {BPSL: A new rumor source location algorithm based on the time-stamp back propagation in social networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-linear target trajectory prediction for robust visual
tracking. <em>APIN</em>, <em>52</em>(8), 8588–8602. (<a
href="https://doi.org/10.1007/s10489-021-02829-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occlusion of the target in tracking usually causing failure that is a serious issue for long-term sequences. Though the siamese network-based trackers obtained considerable performance, it still suffers from the target missing issue when the target is obscured by the same semantic information interferent. To address this issue, a novel occlusion awareness algorithm is proposed, which can both address the occlusion issue and the same semantic information false identification issues. In addition, a novel generative adversarial training and long short term memory (LSTM) based target trajectory prediction algorithm is proposed to predict the possible direction of the target in the following frames. The proposed trajectory prediction algorithm can deal with complicated tracking situations more robustly than the traditional algorithms, e.g. Kalman filter. To further improve the occlusion awareness ability of the proposed algorithm, an occlusion supervision-based training strategy is proposed, which can improve the robustness of the occlusion awareness ability of the proposed occlusion awareness model. In addition, for accurate estimation of the target bounding box, a distance intersection over union (DIOU) loss for regression training is adopted. A comprehensive evaluation is performed on OTB2015, VOT2016, and VOT2018 to evaluate the effectiveness of the proposed algorithm. The experiment results demonstrate that the proposed algorithms perform well and can largely alleviate the tracking failure issue of the siamese network-based tracker caused by occlusion and the same semantic information target identification.},
  archive      = {J_APIN},
  author       = {Xu, Long and Diao, Zhaofu and Wei, Ying},
  doi          = {10.1007/s10489-021-02829-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8588-8602},
  shortjournal = {Appl. Intell.},
  title        = {Non-linear target trajectory prediction for robust visual tracking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel risk-sensitive mean p-power loss based hyper-graph
regularized robust extreme learning machine and its semi-supervised
extension for sample classification. <em>APIN</em>, <em>52</em>(8),
8572–8587. (<a
href="https://doi.org/10.1007/s10489-021-02852-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM) has fast learning speed and perfect performance, at the same time, ELM provides a unified learning framework with a widespread type of feature mappings which can be applied in multiclass classification applications directly. These advantages make ELM become one of the best classification algorithms, and ELM has attracted great attention in supervised learning and semi-supervised learning. However, noise and outliers of data are usually existed in the real world, which will affect the performance of ELM. To improve the robustness and classification performance of ELM, we propose the Kernel Risk-Sensitive Mean p-power Loss Based Hyper-graph Regularized Robust Extreme Learning Machine (KRP-HRELM) method. On the one side, as a nonlinear similarity measure defined in the reproducing kernel space, the kernel risk-sensitive mean p-power loss (KRP) can effectively weaken the negative effects caused by noise and outliers. Therefore, the KRP is introduced into ELM to enhance its robustness. Then, the application of hyper-graph can help the ELM to explore higher-order geometric structures among more sampling points, thereby obtaining more comprehensive data information. In addition, to obtain a more sparsity network model, the L2,1-norm is used to constrain the output weight. On the other side, improving the practical application ability of KRP-HRELM is also the focus of our research, so KRP-HRELM is extended to semi-supervised learning, which is called the semi-supervised KRP-HRELM (SS-KRP-HRELM). Notably, the results of the robustness experiment have proved that our method has extraordinary robustness. At the same time, by using four evaluation measures such as accuracy, recall, precision, and F1-measure, to evaluate the classification results, we can find that our method has obtained better classification performance than other advanced methods.},
  archive      = {J_APIN},
  author       = {Niu, Zhen-Xin and Jiao, Cui-Na and Ren, Liang-Rui and Zhu, Rong and Wang, Juan and Liu, Jin-Xing},
  doi          = {10.1007/s10489-021-02852-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8572-8587},
  shortjournal = {Appl. Intell.},
  title        = {Kernel risk-sensitive mean p-power loss based hyper-graph regularized robust extreme learning machine and its semi-supervised extension for sample classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decision and feature level fusion of deep features extracted
from public COVID-19 data-sets. <em>APIN</em>, <em>52</em>(8),
8551–8571. (<a
href="https://doi.org/10.1007/s10489-021-02945-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coronavirus disease (COVID-19), which is an infectious pulmonary disorder, has affected millions of people and has been declared as a global pandemic by the WHO. Due to highly contagious nature of COVID-19 and its high possibility of causing severe conditions in the patients, the development of rapid and accurate diagnostic tools have gained importance. The real-time reverse transcription-polymerize chain reaction (RT-PCR) is used to detect the presence of Coronavirus RNA by using the mucus and saliva mixture samples taken by the nasopharyngeal swab technique. But, RT-PCR suffers from having low-sensitivity especially in the early stage. Therefore, the usage of chest radiography has been increasing in the early diagnosis of COVID-19 due to its fast imaging speed, significantly low cost and low dosage exposure of radiation. In our study, a computer-aided diagnosis system for X-ray images based on convolutional neural networks (CNNs) and ensemble learning idea, which can be used by radiologists as a supporting tool in COVID-19 detection, has been proposed. Deep feature sets extracted by using seven CNN architectures were concatenated for feature level fusion and fed to multiple classifiers in terms of decision level fusion idea with the aim of discriminating COVID-19, pneumonia and no-finding classes. In the decision level fusion idea, a majority voting scheme was applied to the resultant decisions of classifiers. The obtained accuracy values and confusion matrix based evaluation criteria were presented for three progressively created data-sets. The aspects of the proposed method that are superior to existing COVID-19 detection studies have been discussed and the fusion performance of proposed approach was validated visually by using Class Activation Mapping technique. The experimental results show that the proposed approach has attained high COVID-19 detection performance that was proven by its comparable accuracy and superior precision/recall values with the existing studies.},
  archive      = {J_APIN},
  author       = {Ilhan, Hamza Osman and Serbes, Gorkem and Aydin, Nizamettin},
  doi          = {10.1007/s10489-021-02945-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8551-8571},
  shortjournal = {Appl. Intell.},
  title        = {Decision and feature level fusion of deep features extracted from public COVID-19 data-sets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Similarity modifiers for enhancing the recommender system
performance. <em>APIN</em>, <em>52</em>(8), 8534–8550. (<a
href="https://doi.org/10.1007/s10489-021-02900-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching users or items is the cornerstone for the success story of the recommender systems. This importance drives many efforts for enhancing the performance of the matching process in two directions. One direction investigates proposing new similarity measures, while the other one reshapes the existing ones using additional factors called modifiers to look at some statistical information of the two users into consideration. Sometimes, the modified versions are named new; however, they are just modified versions and can be used for other similarity measures as well. This paper studies current modifiers and proposes new ones. All are added to three well-known similarity measures and examined on three popular datasets. The results show that the weighing similarity modifiers enhance the performance of all the contested similarity measures. Two of the proposed modifiers show a significant improvement in prediction accuracy and error, especially for the users assisted by a few neighbors. They help the active users from the beginning, even with only one or two neighbors. Hence, this performance reflects the user preferences for items via the early election of trustworthy neighbors.},
  archive      = {J_APIN},
  author       = {Al-Shamri, Mohammad Yahya H.},
  doi          = {10.1007/s10489-021-02900-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8534-8550},
  shortjournal = {Appl. Intell.},
  title        = {Similarity modifiers for enhancing the recommender system performance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid genetic-particle swarm optimizer using precise
mutation strategy for computationally expensive problems. <em>APIN</em>,
<em>52</em>(8), 8510–8533. (<a
href="https://doi.org/10.1007/s10489-021-02828-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rational use of resources and efficient production are the focus of today’s society, which makes it urgent to develop an algorithm that can solve most problems and has intelligent characteristics. Particle swarm optimization (PSO) and genetic algorithm (GA) are evolutionary algorithms that have been applied successfully in various complex fields. However, both have their advantages and disadvantages. A hybrid GA-PSO algorithm that employs a precise mutation strategy (PMGPSO) is developed in this paper to utilize their outstanding abilities fully and minimize their shortcomings. In PMGPSO, the precise mutation strategy related to two clustering coefficients is proposed. This strategy distinguishes the degree of required mutation by particle swarm in different periods and coordinates the performance of exploration and exploitation. Furthermore, adopt a sort-based selection method to accelerate the convergence speed and cross the poor particles with the excellent particles to expand the search range. Chaotic sequences are used to generate initial particles and update inertia weight. The performance of PMGPSO is studied with 12 basic benchmark functions and 20 functions extracted from the CEC’2014 test suite. The comparison results with some representative algorithms and existing data demonstrate that PMGPSO provides a high-accuracy solution with a faster convergence speed.},
  archive      = {J_APIN},
  author       = {Duan, Xiongcheng and Zhang, Xiaobing},
  doi          = {10.1007/s10489-021-02828-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8510-8533},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid genetic-particle swarm optimizer using precise mutation strategy for computationally expensive problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evolutionary many-objective algorithm based on
decomposition and hierarchical clustering selection. <em>APIN</em>,
<em>52</em>(8), 8464–8509. (<a
href="https://doi.org/10.1007/s10489-021-02669-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many multi-objective evolutionary algorithms have been proposed to solve many-objective optimization problems with regular Pareto front. These algorithms have shown good performance in balancing convergence and diversity. However, in the high-dimensional objective space, the non-dominated solutions increases exponentially as the number of objectives increases. The metrics to evaluate algorithm performance are also computationally intensive. In particular, solving the many-objective optimization problem of the irregular Pareto front faces great challenges. Moreover, many-objective evolutionary algorithms, do not easily show their convergence and diversity through visualization, as multi-objective evolutionary algorithms do. To address these problems, a many-objective optimization algorithm based on decomposition and hierarchical clustering selection is proposed in this paper. First, a set of uniformly distributed reference vectors divides non-dominanted individuals into different sub-populations, and then candidate solutions are selected based on the aggregation function values in the sub-populations. Second, a set of adaptive reference vectors is used to rank the dominant individuals in the population and retain promising candidate solutions. Third, a hierarchical clustering selection strategy is used to enable solutions with good convergence to be selected. Finally, a diversity maintenance strategy is used to remove solutions with poor diversity. The experimental results show that the proposed algorithm EA-DAH has advantages over other comparative algorithms in many-objective optimization problems with irregular Pareto fronts.},
  archive      = {J_APIN},
  author       = {Sun, Yuehong and Xiao, Kelian and Wang, Siqiong and Lv, Qiuyue},
  doi          = {10.1007/s10489-021-02669-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8464-8509},
  shortjournal = {Appl. Intell.},
  title        = {An evolutionary many-objective algorithm based on decomposition and hierarchical clustering selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RSOD: Real-time small object detection algorithm in
UAV-based traffic monitoring. <em>APIN</em>, <em>52</em>(8), 8448–8463.
(<a href="https://doi.org/10.1007/s10489-021-02893-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing applications of Unmanned Aerial Vehicles (UAVs) in transportation systems promote the development of object detection methods to collect real-time traffic information through UAVs. However, due to the small size and high density of objects from the aerial perspective, most existing algorithms are difficult to accurately process and extract informative features from the traffic images collected by UAVs. To address the challenges, this paper proposes a new real-time small object detection (RSOD) algorithm based on YOLOv3, which improves the small object detection accuracy by (i) using feature maps of a shallower layer containing more fine-grained information for location prediction; (ii) fusing local and global features of shallow and deep feature maps in Feature Pyramid Network(FPN) to enhance the ability to extract more representative features; (iii)assigning weights to output features of FPN and fusing them adaptively; and(iv) improving the excitation layer in Squeeze-and-Excitation attention mechanism to adjust the feature responses of each channel more precisely. Experimental results show that, when the input size is 608 × 608 × 3, the precision of the proposed RSOD algorithm measured by mAP@0.5 is 43.3% and 52.7% on the Visdrone-DET2018 and UAVDT datasets, which is 3.4% and 5.1% higher than those of YOLOv3, respectively.},
  archive      = {J_APIN},
  author       = {Sun, Wei and Dai, Liang and Zhang, Xiaorui and Chang, Pengshuai and He, Xiaozheng},
  doi          = {10.1007/s10489-021-02893-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8448-8463},
  shortjournal = {Appl. Intell.},
  title        = {RSOD: Real-time small object detection algorithm in UAV-based traffic monitoring},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Make aspect-based sentiment classification go further: Step
into the long-document-level. <em>APIN</em>, <em>52</em>(8), 8428–8447.
(<a href="https://doi.org/10.1007/s10489-021-02836-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment classification (ABSC) is a fine-grained analysis task that obtains different sentiment polarities contained in a single text from the views of different aspects. Its practicability draws so much attention from researchers that the number of related works grows explosively. However, existing works mainly aim to obtain polarities from short texts (shorter than 100 words), only a few works analyze documents (shorter than 500 words), but almost no work analyzes long documents (LD, longer than 500 words). This situation makes ABSC powerless when dealing with some texts like in-depth analysis articles. In this paper, we make ABSC step into the LD level by proposing the Hierarchical Aspect-Oriented Framework for Long Document (HAOFL). HAOFL solves two challenges that rarely appear in short texts and normal documents. The first is the too-long input sequence that can cause the model to forget previously learned information or ignore the tailed unlearned information. The second is the unstable sentiment information of the target aspect contained in LD, which increases the difficulty for a model to draw a proper result. HAOFL constructs the data transformation module, dependency processing module, and sentiment aggregation module to solve these two challenges. Numerical experiments prove HAOFL can solve the aforementioned challenges and achieve superior performance in an effective and resource-saving way. With HAOFL, the performances of popular ABSC models on LD are improved at most 8.69% of accuracy and 11.37% of F1-score. In terms of resource-consuming, up to 82.10% of training time and 71.03% of GPU memory are saved.},
  archive      = {J_APIN},
  author       = {Wu, Zhenhao and Gao, Jianbo and Li, Qingshan and Guan, Zhi and Chen, Zhong},
  doi          = {10.1007/s10489-021-02836-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8428-8447},
  shortjournal = {Appl. Intell.},
  title        = {Make aspect-based sentiment classification go further: Step into the long-document-level},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforced exploitation and exploration grey wolf optimizer
for numerical and real-world optimization problems. <em>APIN</em>,
<em>52</em>(8), 8412–8427. (<a
href="https://doi.org/10.1007/s10489-021-02795-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey Wolf Optimizer (GWO) has been proposed recently. As GWO has superior performance, it has been employed to solve various numerical and engineering issues. However, it easily traps into stagnation when solving complex and multimodal problems. GWO mainly searches around the top three wolves and assigns the same weights to them, deteriorating the convergence and exploration. A reinforced exploitation and exploration GWO (REEGWO) is developed. In the proposed REEGWO algorithm, the top three wolves are given different weights on the basis of their knowledge about the location of the prey. Then, a random search based on the tournament selection is used to enhance the exploration. A well-designed mechanism is developed to balance exploration and exploitation. The experimental results have proved that REEGWO is perfect among GWO and its four recently top variants. Then, the proposed REEGWO is compared with the latest heuristic algorithms and their latest variants. The results have shown that REEGWO is competitive. Four real-world applications are also solved by six algorithms, and results have further validated the efficiency of REEGWO.},
  archive      = {J_APIN},
  author       = {Yu, Xiaobing and Xu, WangYing and Wu, Xuejing and Wang, Xueming},
  doi          = {10.1007/s10489-021-02795-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8412-8427},
  shortjournal = {Appl. Intell.},
  title        = {Reinforced exploitation and exploration grey wolf optimizer for numerical and real-world optimization problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-ant colony optimization algorithm based on hybrid
recommendation mechanism. <em>APIN</em>, <em>52</em>(8), 8386–8411. (<a
href="https://doi.org/10.1007/s10489-021-02839-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional ant colony algorithm has the problems of slow convergence speed and easy to fall into local optimum when solving traveling salesman problem. To solve these problems, a multi-ant colony optimization algorithm based on hybrid recommendation mechanism is proposed. Firstly, a heterogeneous multi-ant colony strategy is proposed to balance the convergence and diversity of the algorithm. Secondly, a content-based recommendation strategy is proposed to dynamically divide the traveling salesman problem by self-organizing mapping clustering algorithm, which improves the convergence speed of the algorithm. Thirdly, a collaborative filtering recommendation mechanism based on a multi-attribute decision making model is proposed, including three recommendation strategies: the high-quality solution guidance recommendation strategy based on the convergence factor to improve the convergence of the algorithm; the pheromone fusion recommendation strategy based on the browsing factor to enrich the diversity of the subpopulations; the public path update recommendation strategy based on the population similarity to adaptively regulate the diversity of the algorithm. Finally, when the algorithm stagnates, the association rule-based recommendation strategy is used to help the ant colony jump out of the local optimum. The performance of the improved algorithm is tested on the traveling salesman problem library, and the experimental results show that the proposed algorithm significantly improves the convergence speed and solution accuracy, especially when solving large-scale problems.},
  archive      = {J_APIN},
  author       = {Liu, Yifan and You, Xiaoming and Liu, Sheng},
  doi          = {10.1007/s10489-021-02839-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8386-8411},
  shortjournal = {Appl. Intell.},
  title        = {Multi-ant colony optimization algorithm based on hybrid recommendation mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A CNN-based methodology for cow heat analysis from
endoscopic images. <em>APIN</em>, <em>52</em>(8), 8372–8385. (<a
href="https://doi.org/10.1007/s10489-021-02910-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cattle farming, the artificial insemination technique is a biotechnology that brings to farmers a wide range of benefits namely health security, genetic gain and economic costs. The main condition for the success of artificial insemination within cattle is the heat (or estrus) detection. In this context, several cow heat detection systems have been recently proposed in the literature to assist the farmer in this task. Nevertheless, they are mainly based on the analysis of the physical behavior of the cow which may be affected by several factors related to its health and its environment. In this paper, we present a new vision system for cow heat detection which is based on the analysis of the genital tract of the cow. The main core of our system is a CNN model that has been designed and tailored for analyzing endoscopic images collected using an innovative insemination technology named Eye breed. The conducted experiments on two datasets namely our own dataset and a public dataset show the high accuracy of our CNN model (more than 97% for both datasets) outperforming 19 methods from the state of the art. Moreover, we propose an optimized version of our model for an Android deployment by exploiting several techniques namely quantization, GPU acceleration and video downsampling. The conducted tests on a smart-phone shows that our heat detection system has a response time of a few seconds.},
  archive      = {J_APIN},
  author       = {He, Ruiwen and Benhabiles, Halim and Windal, Feryal and Even, Gaël and Audebert, Christophe and Decherf, Agathe and Collard, Dominique and Taleb-Ahmed, Abdelmalik},
  doi          = {10.1007/s10489-021-02910-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8372-8385},
  shortjournal = {Appl. Intell.},
  title        = {A CNN-based methodology for cow heat analysis from endoscopic images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time delay system identification using controlled recurrent
neural network and discrete bayesian optimization. <em>APIN</em>,
<em>52</em>(8), 8351–8371. (<a
href="https://doi.org/10.1007/s10489-021-02823-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have been widely studied in system modeling due to their strong abilities in feature representation and function fitting. However, most deep learning models are end-to-end black box models, and some key information of the system (such as time delay) cannot be obtained. This paper proposes a grey box model that combines discrete bayesian optimization (DBO) and controlled recurrent neural network (CRNN), namely CRNN-DBO model, aiming at modeling and time delay identification for time delay systems. CRNN is introduced and learns to map the relationship between input data and output data by backpropagation algorithm, while the unknown time delays are modeled as hyperparameters of the mask layer in the CRNN model which are identified by DBO method. The backpropagation algorithm and the DBO method are combined to find the minimal loss value of the model, and the correct time delays as well. To ensure the convergence of the DBO method, l2 regularization term of the mask layer is added in the loss function. The effectiveness and robustness of the model are verified through simulation in the situations of short time delay, long time delay, and nonlinear time delay system. The results indicate that time delays are accurately identified, and the prediction error is smaller than other models.},
  archive      = {J_APIN},
  author       = {Ding, Shenyi and Wang, Zhijie and Zhang, Jue and Han, Fang and Gu, Xiaochun},
  doi          = {10.1007/s10489-021-02823-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8351-8371},
  shortjournal = {Appl. Intell.},
  title        = {Time delay system identification using controlled recurrent neural network and discrete bayesian optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross entropy of mass function and its application in
similarity measure. <em>APIN</em>, <em>52</em>(8), 8337–8350. (<a
href="https://doi.org/10.1007/s10489-021-02890-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster-Shafer (D-S) evidence theory needs the weaker conditions than Bayesian probability theory by assigning the probability into power sets. Hence it has the more stronger ability to express imprecise and unknown information which can be used in many fields. In D-S evidence theory, how to measure the cross entropy between different mass functions is also an open issue. Hence, the paper proposed the new cross entropy by considering the cardinality of mass function which is compatible with classical cross entropy. In addition, there are some numerical examples to explain the reasonableness of proposed cross entropy. Cross entropy can describe the difference of information. Similarity is also an effective method to measure the non-difference of information. It is an interesting question to set the relationship between cross entropy and similarity. Hence, the paper proposed similarity measure based on the entropy and cross entropy. Besides, the paper also discussed some measurement axioms of proposed similarity measure to verify its reasonableness. Finally, based on the new similarity measure, the paper proposed new classification method under D-S evidence theory. The IRIS data set is used to the classification method to verify the effectiveness of its by setting different the number of training data and comparing with other methods.},
  archive      = {J_APIN},
  author       = {Gao, Xiaozhuan and Pan, Lipeng and Deng, Yong},
  doi          = {10.1007/s10489-021-02890-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {8},
  pages        = {8337-8350},
  shortjournal = {Appl. Intell.},
  title        = {Cross entropy of mass function and its application in similarity measure},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-relational knowledge graph completion method with
local information fusion. <em>APIN</em>, <em>52</em>(7), 7985–7994. (<a
href="https://doi.org/10.1007/s10489-021-02876-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph completion(KGC) has attracted increasing attention in recent years, aiming at complementing missing relationships between entities in a Knowledge Graph(KG). While the existing KGC approaches utilizing the knowledge within KG could only complement a very limited number of missing relations, more and more approaches tend to study the completion of the multi-relationship knowledge graph. However, the existing completion methods of multi-relation knowledge graph regard knowledge graph as an undirected graph, which ignores the directionality of knowledge graph, so that the potential characteristics of multi-relation cannot be learned. Besides, most algorithms fail to explore the local information of knowledge because they ignore the different importance of entity adjacencies. In this paper, we propose to use local information fusion to join the entity and its adjacency relation, to acquiring the multi-relation representation. In addition, we try to specify distinct weights to model the direction of the relationship and apply the attention mechanism between entity nodes to obtain local information between entity nodes. Experiments conducted on three benchmark datasets and a medical domain knowledge graph dataset that we collect demonstrate the effectiveness of the proposed framework.},
  archive      = {J_APIN},
  author       = {Huang, Jin and Lu, Tian and Zhu, Jia and Yu, Weihao and Zhang, Tinghua},
  doi          = {10.1007/s10489-021-02876-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7985-7994},
  shortjournal = {Appl. Intell.},
  title        = {Multi-relational knowledge graph completion method with local information fusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast local laplacian filtering based enhanced medical image
fusion using parameter-adaptive PCNN and local features-based fuzzy
weighted matrices. <em>APIN</em>, <em>52</em>(7), 7965–7984. (<a
href="https://doi.org/10.1007/s10489-021-02834-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, the anatomical CT/MRI modalities exhibit the brain tissue anatomy with a high spatial resolution, where PET/SPECT modalities show the metabolic features with low spatial resolution. Therefore, the integration of these two classes significantly improves several clinical applications and computer-aided diagnoses. In the proposed scheme, a fast local Laplacian filter (FLLF) is first applied to the source images to enhance the edge information and suppress the noise artifacts. Second, the RGB images are converted to YUV color space to separate the Y-component. Then to capture the spatial and spectral features of the source images, the NSST is applied to decompose the input (grayscale and/or Y-component) images into one low (LFS) and several high-frequency subbands (HFS). Third, an improved salience measure and matching factor (SMF) method by the local features-based fuzzy-weighted matrix (FW-SMF) is introduced to fuse the LFS coefficient. Due to the fast convergence with fewer iterations and robust pixels selection procedure, the PA-PCNN model is adopted to fuse the HFS coefficients. Fourth, the final fused image is obtained by applying inverse NSST and YUV format. Visual and statistical analysis performed on various experiments prove that the proposed scheme not only integrates the spatial and texture features details of the source images but also enhances the visual quality and contrast of the fused image compared to the existing state-of-arts.},
  archive      = {J_APIN},
  author       = {Ullah, Hikmat and Zhao, Yaqin and Abdalla, Fakheraldin Y. O. and Wu, Longwen},
  doi          = {10.1007/s10489-021-02834-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7965-7984},
  shortjournal = {Appl. Intell.},
  title        = {Fast local laplacian filtering based enhanced medical image fusion using parameter-adaptive PCNN and local features-based fuzzy weighted matrices},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rank-driven salp swarm algorithm with orthogonal
opposition-based learning for global optimization. <em>APIN</em>,
<em>52</em>(7), 7922–7964. (<a
href="https://doi.org/10.1007/s10489-021-02776-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salp swarm algorithm (SSA) is a relatively new and straightforward swarm-based meta-heuristic optimization algorithm, which is inspired by the flocking behavior of salps when foraging and navigating in oceans. Although SSA is very competitive, it suffers from some limitations including unbalanced exploration and exploitation operation, slow convergence. Therefore, this study presents an improved version of SSA, called OOSSA, to enhance the comprehensive performance of the basic method. In preference, a new opposition-based learning strategy based on optical lens imaging principle is proposed, and combined with the orthogonal experimental design, an orthogonal lens opposition-based learning technique is designed to help the population jump out of a local optimum. Next, the scheme of adaptively adjusting the number of leaders is embraced to boost the global exploration capability and improve the convergence speed. Also, a dynamic learning strategy is applied to the canonical methodology to improve the exploitation capability. To confirm the efficacy of the proposed OOSSA, this paper uses 26 standard mathematical optimization functions with various features to test the method. Alongside, the performance of the proposed methodology is validated by Wilcoxon signed-rank and Friedman statistical tests. Additionally, three well-known engineering optimization problems and unknown parameters extraction issue of photovoltaic model are applied to check the ability of the OOSA algorithm to obtain solutions to intractable real-world problems. The experimental results reveal that the developed OOSSA is significantly superior to the standard SSA, currently popular SSA-based algorithms, and other state-of-the-artmeta-heuristic algorithms for solving numerical optimization, real-world engineering optimization, and photovoltaic model parameter extraction problems. Finally, an OOSSA-based path planning approach is developed for creating the shortest obstacle-free route for autonomous mobile robots. Our introduced method is compared with several successful swarm-based metaheuristic techniques in five maps, and the comparative results indicate that the suggested approach can generate the shortest collision-free trajectory as compared to other peers.},
  archive      = {J_APIN},
  author       = {Wang, Zongshan and Ding, Hongwei and Yang, Zhijun and Li, Bo and Guan, Zheng and Bao, Liyong},
  doi          = {10.1007/s10489-021-02776-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7922-7964},
  shortjournal = {Appl. Intell.},
  title        = {Rank-driven salp swarm algorithm with orthogonal opposition-based learning for global optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved crow search algorithm based on oppositional
forgetting learning. <em>APIN</em>, <em>52</em>(7), 7905–7921. (<a
href="https://doi.org/10.1007/s10489-021-02701-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crow search algorithm (CSA) is a novel meta-heuristic optimization algorithm based on the intelligent behavior of the crow population. Although the algorithm has the characteristics of few parameters, simple structure, and easy application, it has the shortcomings of low convergence accuracy and imbalance between exploration and exploitation capabilities. The occurrence of these issues is originated from crow learning from only one goal. In this paper, an improved crow search algorithm based on oppositional forgetting learning (OFLCSA) is proposed. In order to solve the shortcomings of CSA, the forgetting mechanism is introduced to help the algorithm jump out of the local optimum. Moreover, the opposition-based learning (OBL) strategy is combined with the forgetting mechanism to increase the probability of approaching the optimal solution. In addition, the elite crow and adaptive flight length are used to improve the convergence accuracy. To verify the performance of OFLCSA, experiments were conducted on the Congress on Evolutionary Computation (CEC) 2014 and CEC 2019 benchmark functions. OFLCSA is compared with the ten state-of-the-art meta-heuristic optimization algorithms. Moreover, OFLCSA is evaluated by four real-world engineering applications. Experimental results and analysis show that OFLCSA is a competitive meta-heuristic optimization algorithm.},
  archive      = {J_APIN},
  author       = {Xu, Wei and Zhang, Ruifeng and Chen, Lei},
  doi          = {10.1007/s10489-021-02701-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7905-7921},
  shortjournal = {Appl. Intell.},
  title        = {An improved crow search algorithm based on oppositional forgetting learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view fusion segmentation for brain glioma on CT
images. <em>APIN</em>, <em>52</em>(7), 7890–7904. (<a
href="https://doi.org/10.1007/s10489-021-02784-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) images of the brain aid the radiotherapy of glioma. The identification and contouring of the gross tumor volume (GTV) are important for radiotherapy. However, manual segmenting GTV is time-consuming, exhausting, and subjective, and automated methods are desired. To overcome these shortcomings, a novel neural network framework based on multi-view fusion is proposed to segment GTV in brain glioma automatically. The multi-view image that includes the previous image, current image, and following image is inputted in this framework to abstract extra spatial features and then aggregated to segment the GTV. Compared with the 2D segmentation framework, the proposed framework retains more spatial information due to the multi-view image. Meanwhile, compared with the 3D segmentation framework, the proposed framework considers fewer images, which means the model has fewer parameters and is easier to train while retaining much useful spatial information. Moreover, the GliomaCT dataset, a large CT dataset collected from West China Hospital, is used to train, validate, and test the proposed method. The performance of the proposed method and other state-of-the-art methods are compared on this dataset. The high dice similarity coefficient achieved in the experiments demonstrates the effectiveness of the proposed method for segmenting the GTV in brain glioma.},
  archive      = {J_APIN},
  author       = {Wang, Han and Hu, Junjie and Song, Ying and Zhang, Lei and Bai, Sen and Yi, Zhang},
  doi          = {10.1007/s10489-021-02784-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7890-7904},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view fusion segmentation for brain glioma on CT images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LMNNB: Two-in-one imbalanced classification approach by
combining metric learning and ensemble learning. <em>APIN</em>,
<em>52</em>(7), 7870–7889. (<a
href="https://doi.org/10.1007/s10489-021-02901-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real-world applications of machine learning and cybernetics, the data with imbalanced distribution of classes or skewed class proportions is very pervasive. When dealing with imbalanced data, traditional classification approaches might fail to learn a good classifier. In the phase of learning, these algorithms are greatly impacted by the skewed distribution of data. Consequently, the performance of classification drops drastically. In this study, we propose a novel two-in-one algorithm for classifying the imbalanced data by integrating metric learning and ensemble learning algorithms. Firstly, we design a new metric learning algorithm for imbalanced data, which is called Large Margin Nearest Neighbors Balance (called LMNNB). This method can minimize the distance between one sample and its similar neighbors which belong to the same class, and maximize the distance from its dissimilar neighbors which belong to different classes as well. Essentially, this beneficial effect can also be achieved even if the distribution of data is imbalanced. Through metric learning, the imbalance data can be used to learn a better classifier. Secondly, we propose an ensemble learning algorithm to further improve the performance of classification. This method combines multiple sub-classifiers and makes decisions by applying a soft voting strategy. Extensive experiments are conducted on real benchmark imbalanced datasets to demonstrate the effectiveness of LMNNB with ensemble algorithm (called LMNNB-E) in several evaluation measurements. The results show that LMNNB and LMNNB-E outperform the state-of-the-art methods in classifying imbalance data.},
  archive      = {J_APIN},
  author       = {Qiao, Shaojie and Han, Nan and Huang, Faliang and Yue, Kun and Wu, Tao and Yi, Yugen and Mao, Rui and Yuan, Chang-an},
  doi          = {10.1007/s10489-021-02901-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7870-7889},
  shortjournal = {Appl. Intell.},
  title        = {LMNNB: Two-in-one imbalanced classification approach by combining metric learning and ensemble learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The design of error-correcting output codes algorithm for
the open-set recognition. <em>APIN</em>, <em>52</em>(7), 7843–7869. (<a
href="https://doi.org/10.1007/s10489-021-02854-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Open-Set recognition is an important topic in the pattern recognition research field. Different from the close-set recognition task, in the open-set recognition problem, the test data contains unknown classes that do not appear in the training phase. Consequently, the recognition of the open-set data is much more difficult than that of the close-set problem. This study applies the Error-Correcting Output Codes (ECOC) framework to handle the open-set problem by dynamically adding new functions to deal with the unknown classes, named ECOC-OS. Our algorithm includes two steps: (1) the unknown data discovery step based on a rejection strategy; (2) the code matrix expanding step for the separation of the unknown classes from the known classes. Due to the wide and chaotic distribution of the unknown class samples, this paper refines the unknown class into multiple sub-classes, and each sub-class has its own feature distribution. After preliminary row and column expansion and class splitting for the unknown class, the clustering algorithm is used to continuously refine the characteristics of the unknown class, dividing it into several sub-classes. Then the algorithm adds multiple coding rows and multiple &quot;one-to-all&quot; basic classifiers, so as to distinguish each unknown sub-class from multiple known classes. Finally, without re-training the existing learners, the zero symbols in the code matrix are selectively re-encoded according to the basic learners’ preference. The experiments deploy 16 data sets for the test, and the results confirm that ECOC-OS algorithm effectively improves the performance compared with other open-set recognition methods.},
  archive      = {J_APIN},
  author       = {Liu, Kun-Hong and Zhan, Wang-Ping and Liang, Yi-Fan and Zhang, Ya-Nan and Guo, Hong-Zhou and Yao, Jun-Feng and Wu, Qing-Qiang and Hong, Qing-Qi},
  doi          = {10.1007/s10489-021-02854-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7843-7869},
  shortjournal = {Appl. Intell.},
  title        = {The design of error-correcting output codes algorithm for the open-set recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new correlation coefficient of mass function in evidence
theory and its application in fault diagnosis. <em>APIN</em>,
<em>52</em>(7), 7832–7842. (<a
href="https://doi.org/10.1007/s10489-021-02797-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence theory as a generalization of probability theory has an advantage of handling uncertainty. The mass function, also named as basic probability assignment (BPA), plays an important role in evidence theory. Many kinds of measures were proposed based on mass functions like entropy, distance, negation. However, how to determine the correlation coefficient between two mass functions is still an open issue. In this paper, a new correlation coefficient is proposed, which can better show the relationship between mass functions. In numerical examples, the effectiveness of proposed coefficient is illustrated by comparing with existing methods. Besides, an application of fault diagnosis is given.},
  archive      = {J_APIN},
  author       = {Qiang, Chenhui and Deng, Yong},
  doi          = {10.1007/s10489-021-02797-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7832-7842},
  shortjournal = {Appl. Intell.},
  title        = {A new correlation coefficient of mass function in evidence theory and its application in fault diagnosis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Möbius transformation in generalized evidence theory.
<em>APIN</em>, <em>52</em>(7), 7818–7831. (<a
href="https://doi.org/10.1007/s10489-021-02827-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Möbius transformation is a very important information inversion tool. Möbius transformation is sought after by many experts and scholars at home and abroad, and is a hot research topic at present. Möbius transformation can use the known information to reverse the unknown information, indicating that it has a strong ability to process information. Generalized evidence theory is an extension of classical evidence theory. When belief degree of the null subset is 0, then the generalized evidence theory will be degenerated as classical Dempster-Shafer evidence theory. However, how to apply Möbius transformation to generalized evidence theory is still an open problem. This paper proposes Möbius transformation in generalized evidence theory, which can perform function inversion of generalized evidence theory effectively. Numerical examples are used to prove the validity of Möbius transformation in generalized evidence theory. The experimental results show that the Möbius transformation in generalized evidence theory can effectively invert the generalized evidence theory and is a very effective function inversion method.},
  archive      = {J_APIN},
  author       = {Xue, Yige and Deng, Yong},
  doi          = {10.1007/s10489-021-02827-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7818-7831},
  shortjournal = {Appl. Intell.},
  title        = {Möbius transformation in generalized evidence theory},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative residual block for image generation.
<em>APIN</em>, <em>52</em>(7), 7808–7817. (<a
href="https://doi.org/10.1007/s10489-021-02858-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) consisting of the generator and discriminator is widely studied to synthesize photorealistic images. Recently, researchers usually build their generator and discriminator using multiple residual blocks (RBs) which enables both networks to ease the adversarial learning. Following this trend, most prior studies have focused on developing the normalization or attention techniques that are compatible with the RB without modifying the block architecture. This paper proposes a novel architectural unit, called generative residual block (GRB), which is effective to produce high-quality images. GRB contains an additional residual path which effectively emphasizes the informative feature while suppressing the less useful one. We provide comprehensive empirical evidence proving that the proposed method brings significant improvements in GAN performance with slight additional computational costs. Furthermore, we reveal the generalization ability of GRB by conducting extensive experiments across various datasets including CIFAR-10, CIFAR-100, LSUN, and tiny-ImageNet. Quantitative evaluations show that the proposed method significantly improves the performance of GAN and conditional GAN in terms of Frechet inception distance (FID). For instance, GRB boosts FID scores on the tiny-ImageNet dataset from 32.78 to 26.57.},
  archive      = {J_APIN},
  author       = {Park, Seung and Shin, Yong-Goo},
  doi          = {10.1007/s10489-021-02858-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7808-7817},
  shortjournal = {Appl. Intell.},
  title        = {Generative residual block for image generation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid immune genetic algorithm with tabu search for
minimizing the tool switch times in CNC milling batch-processing.
<em>APIN</em>, <em>52</em>(7), 7793–7807. (<a
href="https://doi.org/10.1007/s10489-021-02869-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to enhance the machining efficiency, batch-processing is widely used in computer numerical control (CNC) milling machining. Each job in a batch requires a set of different tools to be processed, so tool switching is needed during processing. However, the frequent tool switching not only affects the machining efficiency, but also affects the life of the machine spindle. In order to solve this problem, a hybrid immune genetic algorithm with tabu search (HIGATS) integrated is proposed to minimize the tool switch times. In HIGATS, a well-designed encoding/decoding scheme is developed to represent the solution and evaluate the fitness; a novel constructive heuristic is used for initializing population; in order to balance the intensification and diversification, tabu search is integrated into genetic algorithm, and also, a problem-specific greedy immune operator is applied to intensify the searching ability. Simulation experiments are conducted to verify the performance of HIGATS by comparing it with other five algorithms. The results and analyses demonstrate that HIGATS outperforms the other five algorithms in minimizing the tool switch times.},
  archive      = {J_APIN},
  author       = {Shi, Shuangyuan and Xiong, Hegen},
  doi          = {10.1007/s10489-021-02869-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7793-7807},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid immune genetic algorithm with tabu search for minimizing the tool switch times in CNC milling batch-processing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized bayesian adaptive resonance theory mapping model
using a rational quadratic kernel and bayesian quadratic regularization.
<em>APIN</em>, <em>52</em>(7), 7777–7792. (<a
href="https://doi.org/10.1007/s10489-021-02883-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian adaptive resonance theory (ART) and ARTMAP-based neural network classifier (known as BAM) are widely used and achieve good classification performance when solving the problem of the undefinable number of clusters and diffusion of classes found in other networks based on ART, such as fuzzy ART. However, the existing BAM classification model is not sufficiently stable to calculate the likelihood when dealing with a small number of highdimensional data. Further, it is difficult to achieve global convergence, which can affect classification performance. To solve these issues and improve the generalization ability of the BAM approach, we propose a BAM classification model that incorporates two overfitting suppression mechanisms. The first mechanism is based on a rational quadratic kernel function to reduces the sensitivity at the cluster decision boundary to novel data, which improves the adaptability of the model to small samples. The second mechanism is based on Bayesian quadratic regularization and reduces the dependence of the classifier on the likelihood estimation and the prior probability, thus preventing overfitting. Experimental results on six different datasets show that the proposed model improves the accuracy, precision, recall, and F1-score of BAM by 8.65%, 11.17%, 30.89%, and 19.25%, respectively.},
  archive      = {J_APIN},
  author       = {Yang, Shunkun and Li, Hongman and Gou, Xiaodong and Bian, Chong and Shao, Qi},
  doi          = {10.1007/s10489-021-02883-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7777-7792},
  shortjournal = {Appl. Intell.},
  title        = {Optimized bayesian adaptive resonance theory mapping model using a rational quadratic kernel and bayesian quadratic regularization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank tensor completion via combined tucker and tensor
train for color image recovery. <em>APIN</em>, <em>52</em>(7),
7761–7776. (<a
href="https://doi.org/10.1007/s10489-021-02833-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, low-rank tensor completion has been widely used in color image recovery. Tensor Train (TT), as a balanced tensor rank minimization method, has achieved good results in actual image recovery because of its ability to capture the hidden information of images. When processing the third-order tensor data of a color image, TT transforms it into a higher-order tensor through kat augmentation to exploit the local feature of the tensor effectively. However, this method actually destroys the original structure of the tensor, which leads to insufficient access to the global structure information. Therefore, the algorithm performs poorly when processing images with a significant amount of missing information. Aiming at this problem, a new tensor completion model is proposed, which combines Tucker rank and Tensor Train rank. Among them, Tucker is used to obtain the global structure information, and Tensor Train is used to capture the local hidden information. To tackle the proposed model, we develop an efficient alternating direction method based algorithm. The numerical experiments using various tensor data show the effectiveness of the model.},
  archive      = {J_APIN},
  author       = {Zhang, Tianheng and Zhao, Jianli and Sun, Qiuxia and Zhang, Bin and Chen, Jianjian and Gong, Maoguo},
  doi          = {10.1007/s10489-021-02833-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7761-7776},
  shortjournal = {Appl. Intell.},
  title        = {Low-rank tensor completion via combined tucker and tensor train for color image recovery},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance interactive association graph convolutional network
for domain adaptive person re-identification. <em>APIN</em>,
<em>52</em>(7), 7747–7760. (<a
href="https://doi.org/10.1007/s10489-021-02806-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive person re-identification (re-ID) is a challenging task due to the large domain divergency between different datasets and the complicated variations of the target domain. Style-transferring based methods mainly narrow the domain divergency with respect to a specific imaging factor, e.g., illumination. However, in consideration of the complex practical scenarios, domain adaptive re-ID demands comprehensive domain characteristic knowledge, i.e., seasons, illuminations, camera views, etc., to alleviate the domain divergency and intra-domain variations. This paper proposes a data-driven Instance Interactive Association Graph Convolutional Network (IIAGCN) to tackle the problems. Specifically, our IIAGCN method first constructs a cross-domain knowledge graph with the inter-domain nodes (target-source image pairs) and the intra-domain nodes (target-target image pairs). Then an Information Interactive Graph Convolutional (IIGC) layer is designed to extract the instance-level domain characteristic knowledge from the knowledge graph. With the learned knowledge, we can learn domain characteristic-aware and discriminative image representations for better domain adaptation. In addition, we introduce the memory bank component to store image features of the whole dataset, which enlarges the node diversity of the knowledge graph. Experiments on large-scale person re-ID datasets demonstrate the superiority of our method under the unsupervised re-ID setting.},
  archive      = {J_APIN},
  author       = {Yang, Qingyuan and Hou, Chunping and Huang, Meiyan and Wang, Zhipeng},
  doi          = {10.1007/s10489-021-02806-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7747-7760},
  shortjournal = {Appl. Intell.},
  title        = {Instance interactive association graph convolutional network for domain adaptive person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object matching between visible and infrared images using a
siamese network. <em>APIN</em>, <em>52</em>(7), 7734–7746. (<a
href="https://doi.org/10.1007/s10489-021-02841-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a method for object matching between visible and infrared images. We consider object matching between visible and infrared images as a computational patch-matching problem, and the main solution to this problem involves calculating the similarities of the relationships between the objects in the target and search images. Therefore, in this study, we propose a Siamese neural network, which takes a pair of visible and infrared images as the input. Our proposed Siamese network comprises a convolutional neural network (CNN) to ensure the effective extraction of features from visible and infrared images. The CNN comprises convolutional and pooling layers without padding. By calculating the cross-correlation of the objects in the visible image and those in the entire infrared image, we regard the parts with the highest similarity as the matched targets. During the training process, we use focal loss to solve the problem of the imbalance between the positive and negative samples in the dataset, after which we use interpolation to determine the locations of the target patches in the infrared images. We then conduct experiments on different classes of targets, and the results demonstrate that our proposed approach achieves greater accuracy and precision than other methods.},
  archive      = {J_APIN},
  author       = {Li, Wuxin and Chen, Qian and Gu, Guohua and Sui, Xiubao},
  doi          = {10.1007/s10489-021-02841-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7734-7746},
  shortjournal = {Appl. Intell.},
  title        = {Object matching between visible and infrared images using a siamese network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-granular document-level sentiment topic analysis for
online reviews. <em>APIN</em>, <em>52</em>(7), 7723–7733. (<a
href="https://doi.org/10.1007/s10489-021-02817-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is key to identify both sentiment and topic for well understanding and managing social media data such as online reviews and microblogs. This paper studies a robust and reliable solution for synchronous analysis of sentiment and topic in online reviews. Specifically, a probabilistic model is proposed for joint sentiment topic detection with multi-granular computation, named MgJST (multi-granular joint sentiment topic). The MgJST model introduces sentence level structural knowledge to detect sentiment and topic simultaneously from reviews based on latent Dirichlet allocation (LDA). The sets of experiments are conducted on seven sentiment analysis datasets. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art unsupervised approaches WSTM and STSM in terms of sentiment detection quality, and has powerful ability to extract topics from reviews.},
  archive      = {J_APIN},
  author       = {Huang, Faliang and Yuan, Changan and Bi, Yingzhou and Lu, Jianbo and Lu, Liqiong and Wang, Xing},
  doi          = {10.1007/s10489-021-02817-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7723-7733},
  shortjournal = {Appl. Intell.},
  title        = {Multi-granular document-level sentiment topic analysis for online reviews},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient method for tracking failure detection using
parallel correlation filtering and siamese network. <em>APIN</em>,
<em>52</em>(7), 7713–7722. (<a
href="https://doi.org/10.1007/s10489-021-02768-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object tracking based on Siamese network has attracted the attention by taking advantage of both speed and accuracy. However, when similar object interference, severe occlusion and other factors cause temporary tracking failure, a restricted search window makes Siamese network difficult to retrieve the object again, and results in unrecoverable tracking failures. In this paper, a general and efficient two-way verification tracking failure detection method using parallel correlation filtering and Siamese network is proposed, which can detect the tracking failure and retrieve the object again. Firstly, we construct parallel Siamese network and correlation filter tracking network, and get tracking results respectively. Secondly, we make a preliminary judgment on the reliability of the tracking results based on the overlap ratio of the tracking results. Finally, based on two-way verification, which tracker failed to track is finally determined, and the search window of Siamese network is optimized to retrieve the object again. We comparisons with state-of-the art trackers on benchmark datasets: OTB100, VOT2016, VOT2018, VOT2019 and NFS. The results show that the method we proposed can detect the tracking failure and retrieve the object again, thus improving the accuracy of object tracking.},
  archive      = {J_APIN},
  author       = {Chen, Shaolong and Qiu, Changzhen and Zhang, Zhiyong},
  doi          = {10.1007/s10489-021-02768-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7713-7722},
  shortjournal = {Appl. Intell.},
  title        = {An efficient method for tracking failure detection using parallel correlation filtering and siamese network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced semantic representation learning for implicit
discourse relation classification. <em>APIN</em>, <em>52</em>(7),
7700–7712. (<a
href="https://doi.org/10.1007/s10489-021-02785-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit discourse relation classification is one of the most challenging tasks in discourse parsing. Without connectives as linguistic clues, classifying discourse relations usually requires understanding text semantics at the word level, sentence level, and sentence span level. In this paper, we mainly proposed a graph-based model for relation classification. A semantic graph is firstly built to describe the syntactic dependencies and sentence interaction. Then, based on the learning principle of graph neural networks, a bidirectional gated recurrent unit (Bi-GRU) was introduced to work with graph attention network (GAT), which allows the expanded GAT to capture syntactic dependencies of long-distance nodes and selectively mine semantic features from multi-hop neighborhood nodes. In addition, we utilized the hierarchical self-organization ability of hyperbolic spaces to classify multi-level discourse relations, improving the accuracy of fine-grained discourse relation classification. Experimental results on Penn Discourse Treebank 2.0 (PDTB 2.0) demonstrated that our model could achieve improvements without any external knowledge.},
  archive      = {J_APIN},
  author       = {Ma, Yuhao and Zhu, Jian and Liu, Jie},
  doi          = {10.1007/s10489-021-02785-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7700-7712},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced semantic representation learning for implicit discourse relation classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAFA-net: Pedestrian detection network based on multi-scale
attention feature aggregation. <em>APIN</em>, <em>52</em>(7), 7686–7699.
(<a href="https://doi.org/10.1007/s10489-021-02796-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With pedestrian detection algorithms, balancing the trade-off between accuracy and speed remains challenging. Following the central point-based one-stage object detection paradigm, a pedestrian detection algorithm based on multi-scale attention feature aggregation (MAFA) is proposed to improve accuracy while considering real-time performance. We refer to the proposed algorithm as MAFA-Net. Through the design of deep dilate blocks, deeper features are extracted. Pedestrian attention blocks are added to mine more relevant information between features from the perspective of spatial and passage-wise dimensions, and pedestrian features are enhanced. Feature aggregation modules are used to fuse different scale features, and combine the rich high-level semantic features with the accurate location features of the low-level features. Experiments were conducted on two challenging pedestrian detection datasets, i.e., CityPersons and Caltech, using MR−2 as the evaluation index. For Caltech, MR−2 is 4.58% under reasonable conditions. For CityPersons, MR−2 is 11.47% and 10.05% under reasonable and partial occlusion conditions, which is 0.43% and 1.35% better than the suboptimal comparison detection method. The results demonstrate that a good performance is obtained, and the effectiveness and feasibility of the algorithm are verified.},
  archive      = {J_APIN},
  author       = {Xia, Hao and Wan, Honglin and Ou, Jiayu and Ma, Jun and Lv, Xinyao and Bai, Chengjie},
  doi          = {10.1007/s10489-021-02796-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7686-7699},
  shortjournal = {Appl. Intell.},
  title        = {MAFA-net: Pedestrian detection network based on multi-scale attention feature aggregation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised hash retrieval based on multiple similarity
matrices and text self-attention mechanism. <em>APIN</em>,
<em>52</em>(7), 7670–7685. (<a
href="https://doi.org/10.1007/s10489-021-02804-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to find the similarity between different modal data, while the hash retrieval method improves retrieval efficiency. This paper proposes a cross-modal hash retrieval method based on multiple similarity matrices. This paper proposes an unsupervised cross-modal hash retrieval method based on multiple similarity matrices. This paper uses a weighted combination method to construct fusion features through hash features and original features. Based on the three features, the auxiliary similarity matrix of each of the three features is established. Finally, the fusion matrix is constructed by a weighted combination of the similarity matrix of the original features and the hash features. These four different matrices include similarity matrices with varying forms of features and similarity matrices with varying construction methods, which concentrate the similarity information of other modalities. The loss function between different similarity matrices and the loss function between different modalities are calculated through these four different matrices. Considering that most models have a single method for extracting text features, this paper uses text self-attention to strengthen the effect of text feature extraction so that the final performance of this paper is effectively improved. In order to verify the impact of this article, the results are tested on the Wikipedia, MIRFlickr, and NUS-WIDE datasets, and the results prove that the effect of this article has certain advantages compared with the latest methods.},
  archive      = {J_APIN},
  author       = {Hou, Chuanwen and Li, Zhixin and Wu, Jingli},
  doi          = {10.1007/s10489-021-02804-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7670-7685},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised hash retrieval based on multiple similarity matrices and text self-attention mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correntropy-based dual graph regularized nonnegative matrix
factorization with lp smoothness for data representation. <em>APIN</em>,
<em>52</em>(7), 7653–7669. (<a
href="https://doi.org/10.1007/s10489-021-02826-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization methods have been widely used in many applications in recent years. However, the clustering performances of such methods may deteriorate dramatically in the presence of non-Gaussian noise or outliers. To overcome this problem, in this paper, we propose correntropy-based dual graph regularized NMF with LP smoothness (CDNMFS) for data representation. Specifically, we employ correntropy instead of the Euclidean norm to measure the incurred reconstruction error. Furthermore, we explore the geometric structures of both the input data and the feature space and impose an Lp norm constraint to obtain an accurate solution. In addition, we introduce an efficient optimization scheme for the proposed model and present its convergence analysis. Experimental results on several image datasets demonstrate the superiority of the proposed CDNMFS method.},
  archive      = {J_APIN},
  author       = {Shu, Zhenqiu and Weng, Zonghui and Yu, Zhengtao and You, Congzhe and Liu, Zhen and Tang, Songze and Wu, Xiaojun},
  doi          = {10.1007/s10489-021-02826-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7653-7669},
  shortjournal = {Appl. Intell.},
  title        = {Correntropy-based dual graph regularized nonnegative matrix factorization with lp smoothness for data representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GE-STDGN: A novel spatio-temporal weather prediction model
based on graph evolution. <em>APIN</em>, <em>52</em>(7), 7638–7652. (<a
href="https://doi.org/10.1007/s10489-021-02824-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many crucial tasks in weather prediction require large-scale and long-term spatio-temporal predictions. However, these tasks usually face three challenges: high feature redundancy, dependence of long-term prediction, and complexity in spatial relations of geographical location. To overcome these challenges, the Graph Evolution-based Spatio-Temporal Dense Graph Neural (GE-STDGN) network is proposed in spatio-temporal series prediction1. In this paper, a graph structure learning and optimization method based on an Evolutionary Multi-objective Optimization (EMO) algorithm, called Graph Evolution (GE), is adopted to improve the model’s ability in analyzing complex node correlations. Then, to avoid the over-fitting caused by Graph Neural Network (GNN) and reduce the constraint of graph structure, a GNN based on dense connection, which is named DenseGNN, is applied to message passing. Finally, by combining the multi-head attention mechanism GRU model with DenseGNN, GE-STDGN can handle complex spatio-temporal series prediction tasks. Experimental results on a public real-world weather dataset demonstrate that our model steadily outperforms many state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Ni, Qingjian and Wang, Yuhui and Fang, Yifei},
  doi          = {10.1007/s10489-021-02824-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7638-7652},
  shortjournal = {Appl. Intell.},
  title        = {GE-STDGN: A novel spatio-temporal weather prediction model based on graph evolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A transfer-learning approach for corrosion prediction in
pipeline infrastructures. <em>APIN</em>, <em>52</em>(7), 7622–7637. (<a
href="https://doi.org/10.1007/s10489-021-02771-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pipeline infrastructures, carrying either gas or oil, are often affected by internal corrosion, which is a dangerous phenomenon that may cause threats to both the environment (due to potential leakages) and the human beings (due to accidents that may cause explosions in presence of gas leakages). For this reason, predictive mechanisms are needed to detect and address the corrosion phenomenon. Recently, we have seen a first attempt at leveraging Machine Learning (ML) techniques in this field thanks to their high ability in modeling highly complex phenomena. In order to rely on these techniques, we need a set of data, representing factors influencing the corrosion in a given pipeline, together with their related supervised information, measuring the corrosion level along the considered infrastructure profile. Unfortunately, it is not always possible to access supervised information for a given pipeline since measuring the corrosion is a costly and time-consuming operation. In this paper, we will address the problem of devising a ML-based predictive model for internal corrosion under the assumption that supervised information is unavailable for the pipeline of interest, while it is available for some other pipelines that can be leveraged through Transfer Learning (TL) to build the predictive model itself. We will cover all the methodological steps from data set creation to the usage of TL. The whole methodology will be experimentally validated on a set of real-world pipelines.},
  archive      = {J_APIN},
  author       = {Canonaco, Giuseppe and Roveri, Manuel and Alippi, Cesare and Podenzani, Fabrizio and Bennardo, Antonio and Conti, Marco and Mancini, Nicola},
  doi          = {10.1007/s10489-021-02771-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7622-7637},
  shortjournal = {Appl. Intell.},
  title        = {A transfer-learning approach for corrosion prediction in pipeline infrastructures},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal trajectory anomaly detection based on common
sub-sequence. <em>APIN</em>, <em>52</em>(7), 7599–7621. (<a
href="https://doi.org/10.1007/s10489-021-02754-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of GPS positioning and wireless communication, more and more trajectories are collected. How to accurately and efficiently detect abnormal trajectories from a large number of trajectories has become a focused issue. The similarity measurement method adopted by the existing abnormal trajectory detection technology often ignores the situation that the abnormal sub-trajectory has enough neighbors. If a trajectory is composed of multiple such sub-trajectories, this anomaly will not be detected. At present, the trajectory outlier detection algorithm based on common slices sub-sequence(TODCSS) has improved the above problems. However, it is not accurate enough in feature extraction. Its detection scope is limited to 2D-plane and the time dimension is ignored, so it can’t detect abnormal vehicle behaviors such as multiple stops, detention, too slow speed and so on. Based on the above problems, this paper proposes a spatio-temporal trajectory anomaly detection based on common sub-sequence (STADCS). Firstly, in order to obtain accurate and reasonable similar trajectories, the length of sub-trajectory is added to the common sequence of trajectories, and non-common parts between two trajectories are added to the similarity measurement. Then the time is added to detect trajectories of time anomalies. It improves the accuracy and rationality of detection. Finally, we conducted experiments on real datasets and used F1 − measure to evaluate the accuracy of this algorithm. Compared with existing algorithms, the accuracy of STADCS is improved by about 15.15%.},
  archive      = {J_APIN},
  author       = {He, Ling and Niu, Xinzheng and Chen, Ting and Mei, Kejin and Li, Mao},
  doi          = {10.1007/s10489-021-02754-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7599-7621},
  shortjournal = {Appl. Intell.},
  title        = {Spatio-temporal trajectory anomaly detection based on common sub-sequence},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factorized multi-scale multi-resolution residual network for
single image deraining. <em>APIN</em>, <em>52</em>(7), 7582–7598. (<a
href="https://doi.org/10.1007/s10489-021-02772-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of vision systems can be affected when used in severe weather conditions such as heavy rain or snow. Rain streak removal is an ill posed problem as they can vary in shape, size, and density across the image. In this paper, a single-image deraining network named Factorized Multi-scale Multi-resolution Residual Network (FMMRNet), which follows a U-Net backbone, is proposed. As rain streaks affect non-local regions of the image, larger receptive fields are beneficial to capture these non-local dependencies. We propose the use of multi-scale grouped convolutions to integrate information from global and local scales. In the proposed FMMRNet, multi-scale convolutions are factorized so that they can have large effective kernel sizes while reducing the computational complexity. During training, intermediate multi-resolution outputs are produced for loss computation which improves gradient flow in the deeper layers of the network and promotes better learning. A channel-wise attention mechanism is included to recalibrate feature maps before fusion instead of direct concatenation at each stage of the decoder. A higher-level reconstruction loss called perceptual loss is included for effective training to improve the visual quality of the derained images. The performance of the proposed FMMRNet is quantitatively and qualitatively compared on public benchmark datasets, and it outperforms the state-of-the-art deraining methods. Furthermore, to show the practical applicability of the proposed network, we demonstrate that when FMMRNet is used as a preprocessing step for object-detection methods such as Faster-RCNN and YOLO, it improves their performance on images degraded by rain streaks by 12.6% and 75.2% respectively.},
  archive      = {J_APIN},
  author       = {Sujit, Shivakanth and Deivalakshmi S and Ko, Seok-Bum},
  doi          = {10.1007/s10489-021-02772-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7582-7598},
  shortjournal = {Appl. Intell.},
  title        = {Factorized multi-scale multi-resolution residual network for single image deraining},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual tracking for UAV using adaptive spatio-temporal
regularized correlation filters. <em>APIN</em>, <em>52</em>(7),
7566–7581. (<a
href="https://doi.org/10.1007/s10489-021-02825-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advance of visual tracking has provided unmanned aerial vehicle (UAV) with the intriguing capability for various practical applications. With promising performance and efficiency, discriminative correlation filter (DCF)-based trackers have drawn significant attention and undergone remarkable progress. However, the boundary effect and filter degradation remain two intractable problems. In this work, we propose a novel Adaptive Spatio-Temporal Regularized Correlation Filters (ASTR-CF) model to address the two problems. The ASTR-CF model simultaneously optimizes the spatial and temporal regularization weights adaptively, and it is optimized by the alternating direction method of multipliers (ADMM) effectively. Extensive experiments on 4 UAV tracking benchmarks have proven the superiority of the proposed ASTR-CF compared with more than 30 state-of-the-art trackers in terms of accuracy and speed.},
  archive      = {J_APIN},
  author       = {Xu, Libin and Gao, Mingliang and Li, Qilei and Zou, Guofeng and Pan, Jinfeng and Jiang, Jun},
  doi          = {10.1007/s10489-021-02825-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7566-7581},
  shortjournal = {Appl. Intell.},
  title        = {Visual tracking for UAV using adaptive spatio-temporal regularized correlation filters},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale image super-resolution via hierarchical filter
groups. <em>APIN</em>, <em>52</em>(7), 7550–7565. (<a
href="https://doi.org/10.1007/s10489-021-02832-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of deep convolutional neural networks (DNN) has progressed the recent research interest on super-resolution (SR). The existing DNNs benefited from residual learning and achieved improved performance. In this paper, we propose a discrete wavelet-based hierarchical filter groups (HFG) for multi-scale image SR. Discrete wavelet transform (DWT) produces four sub-bands with different frequency components. Three detail sub-bands (i.e., horizontal, vertical and diagonal) contain sparse wavelet coefficients whereas, approximation sub-band contains average spatial image information. The reconstruction accuracy is improved by exploiting the information in all the wavelet sub-bands simultaneously. Furthermore, hierarchical filter groups are adopted for creating computationally efficient DNN without compromising performance. Moreover, symmetrical skip connections are used for feature reuse in different layers of DNN to improve reconstruction accuracy. In addition, the problem of vanishing of gradients and feature redundancy are mitigated by short skip paths created using symmetrical skip connections. The proposed HFG method achieves superior performance over the other recent competitive methods on benchmark datasets.},
  archive      = {J_APIN},
  author       = {Amaranageswarao, Gadipudi and Deivalakshmi, S.},
  doi          = {10.1007/s10489-021-02832-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7550-7565},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale image super-resolution via hierarchical filter groups},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new algorithm for modeling online search behavior and
studying ranking reliability variations. <em>APIN</em>, <em>52</em>(7),
7529–7549. (<a
href="https://doi.org/10.1007/s10489-021-02856-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design an information retrieval algorithm that mimics the stochastic behavior of decision-makers (DMs) when evaluating the alternatives displayed by an online search engine. The algorithm consists of a decision tree that incorporates all the 1024 decision nodes that may arise from the information retrieval process of DMs. We calibrate the behavior of the algorithm to the one observed from online users and run several sets of 1,000,000 queries. Each query lets DMs decide which subset of the ten alternatives composing the initial page of results to click, allowing us to evaluate their behavior as ranking reliability is assumed to decrease when DMs decide not to click on an alternative. We compare the click-through rates (CTRs) obtained when modifying the degree of ranking reliability derived from the alternatives displayed on the first page of search results. We illustrate how the stability of the CTR prevails among the top-ranked alternatives within relatively reliable scenarios while it drops when imposing large initial decrements in reliability. The resulting consequences regarding the importance of relative ranking positions are analyzed, the top three alternatives exhibiting a generally contained decrease in their CTRs that contrasts with the cumulative pattern arising from the fourth position onwards.},
  archive      = {J_APIN},
  author       = {Di Caprio, Debora and Santos-Arteaga, Francisco J. and Tavana, Madjid},
  doi          = {10.1007/s10489-021-02856-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7529-7549},
  shortjournal = {Appl. Intell.},
  title        = {A new algorithm for modeling online search behavior and studying ranking reliability variations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GC-LSTM: Graph convolution embedded LSTM for dynamic network
link prediction. <em>APIN</em>, <em>52</em>(7), 7513–7528. (<a
href="https://doi.org/10.1007/s10489-021-02518-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic network link prediction is becoming a hot topic in network science, due to its wide applications in biology, sociology, economy and industry. However, it is a challenge since network structure evolves with time, making long-term prediction of adding/deleting links especially difficult. Inspired by the great success of deep learning frameworks, especially the convolution neural network (CNN) and long short-term memory (LSTM) network, we propose a novel end-to-end model with a Graph Convolution Network(GCN) embedded LSTM, named GC-LSTM, for dynamic network link prediction. Thereinto, LSTM is adopted as the main framework to learn the temporal features of all snapshots of a dynamic network. While for each snapshot, GCN is applied to capture the local structural properties of nodes as well as the relationship between them. One benefit is that our GC-LSTM can predict both added and removed links, making it more practical in reality, while most existing dynamic link prediction methods can only handle removed links. Extensive experiments demonstrated that GC-LSTM achieves outstanding performance and outperforms existing state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Chen, Jinyin and Wang, Xueke and Xu, Xuanheng},
  doi          = {10.1007/s10489-021-02518-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7513-7528},
  shortjournal = {Appl. Intell.},
  title        = {GC-LSTM: Graph convolution embedded LSTM for dynamic network link prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient data evacuation strategy using multi-objective
reinforcement learning. <em>APIN</em>, <em>52</em>(7), 7498–7512. (<a
href="https://doi.org/10.1007/s10489-021-02640-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a disaster occurs, utilizing residual network resources as much as possible and allocating a reasonable bandwidth ratio for concurrent evacuation transfers are two key factors to improve the efficiency of data evacuation. However, because the evacuation activity is often implemented in a large-scale network of high complexity, these two factors are likely to be conflicting in some cases. Therefore, it is difficult to achieve or approximate the optimal evacuation solution only by single objective optimization. To achieve better utilization of network transmission capability in data evacuation, we leverage multi-objective reinforcement learning to simultaneously maximize the total evacuation flow and allocate proportional bandwidth to concurrent evacuation transfers. We design the vector of rewards for the two objectives and update Pareto approximate set by multiple state steps to approach the optimal solution. In every step, we leverage marked evacuation routing search based on heuristic information to construct action space. To improve the quality of candidate action, we search one available evacuation path for every evacuation transfer, and adjust bandwidth allocation to reduce the deviation between bandwidth ratio and data amount ratio. For action selection, we propose a roulette-based Chebyshev scalarization function to optimize the weight selection process for multi-objectives and enforce exploration to avoid falling into the local optimum. The simulation results demonstrates that our new strategy solves the weight selection problem successfully and achieves better performance with higher transmission efficiency.},
  archive      = {J_APIN},
  author       = {Li, Xiaole},
  doi          = {10.1007/s10489-021-02640-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7498-7512},
  shortjournal = {Appl. Intell.},
  title        = {An efficient data evacuation strategy using multi-objective reinforcement learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MMKRL: A robust embedding approach for multi-modal knowledge
graph representation learning. <em>APIN</em>, <em>52</em>(7), 7480–7497.
(<a href="https://doi.org/10.1007/s10489-021-02693-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most knowledge representation learning (KRL) methods only use structured knowledge graphs (KGs); however, there is still much multi-modal (textual, visual) knowledge that has not been used. To address this challenge, we propose a novel solution called multi-modal knowledge representation learning (MMKRL) to take advantage of multi-source (structured, textual, and visual) knowledge. Instead of simply integrating multi-modal knowledge with structured knowledge in a unified space, we introduce a component alignment scheme and combine it with translation methods to accomplish multi-modal KRL. Specifically, MMKRL firstly reconstructs multi-source knowledge by summing different plausibility functions and then aligns multi-source knowledge using specific norm constraints to reduce reconstruction errors. We also select an adversarial training strategy to enhance the robustness of MMKRL, which is rarely considered in existing multi-modal KRL methods. Experimental results show that MMKRL can effectively utilize multi-modal knowledge to achieve better link prediction and triple classification than other baselines in two widely used datasets. Further, when relying on structured knowledge or limited multi-source knowledge, MMKRL still achieves competitive results in link prediction, demonstrating our model’s superiority.},
  archive      = {J_APIN},
  author       = {Lu, Xinyu and Wang, Lifang and Jiang, Zejun and He, Shichang and Liu, Shizhong},
  doi          = {10.1007/s10489-021-02693-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7480-7497},
  shortjournal = {Appl. Intell.},
  title        = {MMKRL: A robust embedding approach for multi-modal knowledge graph representation learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical learning from human preferences and curiosity.
<em>APIN</em>, <em>52</em>(7), 7459–7479. (<a
href="https://doi.org/10.1007/s10489-021-02726-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent success in scaling deep reinforcement algorithms (DRL) to complex problems has been driven by well-designed extrinsic rewards, which limits their applicability to many real-world tasks where rewards are naturally extremely sparse. One solution to this problem is to introduce human guidance to drive the agent’s learning. Although low-level demonstrations is a promising approach, it was shown that such guidance may be difficult for experts to demonstrate since some tasks require a large amount of high-quality demonstrations. In this work, we explore human guidance in the form of high-level preferences between sub-goals, leading to drastic reductions in both human effort and cost of exploration. We design a novel hierarchical reinforcement learning method that introduces non-expert human preferences at the high-level, and curiosity to drastically speed up the convergence of subpolicies to reach any sub-goals. We further propose a strategy based on curiosity to automatically discover sub-goals. We evaluate the proposed method on 2D navigation tasks, robotic control tasks, and image-based video games (Atari 2600), which have high-dimensional observations, sparse rewards, and complex state dynamics. The experimental results show that the proposed method can learn significantly faster than traditional hierarchical RL methods and drastically reduces the amount of human effort required over standard imitation learning approaches.},
  archive      = {J_APIN},
  author       = {Bougie, Nicolas and Ichise, Ryutaro},
  doi          = {10.1007/s10489-021-02726-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7459-7479},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical learning from human preferences and curiosity},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re-weighting regression and sparsity regularization for
multi-view classification. <em>APIN</em>, <em>52</em>(7), 7442–7458. (<a
href="https://doi.org/10.1007/s10489-021-02860-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data widely exists in real life, which contains rich and comprehensive information. Multi-view learning aims to make full use of the information of multiple views to improve the performance of the learner. Most traditional supervised methods learn an entire transformation matrix by concatenating multiple views into a long vector, thus they often ignore the relationship between views. To tackle this problem, in this paper, a novel re-weighting regression and sparsity regularization method for multi-view classification is proposed. The proposed method adopts view-based joint sparsity-inducing norm regularization to reduce the impact of redundant features via exploring the correlation between the features of different views and the predicted categories. Moreover, the model adopts the re-weighting strategy to weigh the importance of different views by adding a proper weight for each view. Benefited from the clustering idea, the proposed model learns several sub-matrices in different view subspaces independently and integrates them into the final decision classifier with different weights to improve the classification performance. Extensive experimental results show that the proposed model obtains better classification performance compared to several state-of-the-art classification methods.},
  archive      = {J_APIN},
  author       = {Wang, Zhi and Men, Min and Zhong, Ping},
  doi          = {10.1007/s10489-021-02860-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7442-7458},
  shortjournal = {Appl. Intell.},
  title        = {Re-weighting regression and sparsity regularization for multi-view classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wireless sensor node deployment scheme based on embedded
virtual force resampling particle swarm optimization algorithm.
<em>APIN</em>, <em>52</em>(7), 7420–7441. (<a
href="https://doi.org/10.1007/s10489-021-02745-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, wireless sensor network (WSN) has been widely used in many fields. Network coverage is the basis of providing perception services and collecting location information and has become one of the hot topics. For node deployment, this paper proposes two algorithms. One is an improved virtual force (VF) algorithm. The virtual forces of nodes include repulsive force between nodes and repulsive force at the boundary. The improved VF algorithm sets the virtual force threshold. The other is the resampling particle swarm optimization algorithm embedded with virtual force (RPSO-DV). The algorithm combines the advantages of three algorithms, including resampling particle swarm optimization (RPSO) algorithm, particle swarm optimization algorithm based on coefficient adjustment (PSO-D) and improved VF algorithm. In this paper, the two proposed algorithms and reference algorithms in the pieces of literature and are simulated and compared. Firstly, this paper compares the impact of different node numbers and deployment modes on coverage performance in the improved VF algorithm. The simulation shows that the improved VF algorithm can make the network reach a stable state quickly and achieve a high coverage rate. Secondly, this paper lists the confidence intervals for the coverage rate of multiple algorithms at the significance level of 0.05. At the same time, we analyze the specific coverage rate curves and deployment diagrams. The simulation results show that our proposed RPSO-DV algorithm improves the diversity of the population and speeds up the convergence speed. Compared with other reference algorithms, the RPSO-DV algorithm has the highest coverage rate. Finally, this paper analyzes the sensitivity of the parameters of the proposed RPSO-DV algorithm. According to the orthogonal experiment design method, we design 64 sets of experiments. The simulation results show that the algorithm has a certain tolerance and robustness to parameter values.},
  archive      = {J_APIN},
  author       = {Qi, Xiaogang and Li, Zhinan and Chen, Chen and Liu, Lifang},
  doi          = {10.1007/s10489-021-02745-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7420-7441},
  shortjournal = {Appl. Intell.},
  title        = {A wireless sensor node deployment scheme based on embedded virtual force resampling particle swarm optimization algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Occluded person re-identification based on differential
attention siamese network. <em>APIN</em>, <em>52</em>(7), 7407–7419. (<a
href="https://doi.org/10.1007/s10489-021-02820-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (ReID) aims to retrieve images of the same person with occlusion. The key is excavating discriminative features from the nonoccluded regions to achieve better matching. To this end, we propose a novel Differential Attention Siamese Network (DASN) for occluded person ReID. Specifically, the proposed DASN method includes a siamese network module and a Differential Occlusion Weighted Aggregation (DOWA) module. First, a shared two-branch framework with the original nonoccluded image and the corresponding occluded image generated by the random erasing operation as inputs is adopted for shared feature representation. Then, the DOWA module is proposed to obtain the difference features in nonoccluded regions by first subtracting the feature map of the occluded image from the original image to obtain features corresponding to the occluded regions, and subsequently subtracting these features from those corresponding to the original image. During this process, the attention mechanism is integrated to alleviate the exploration inaccuracy of the features of nonoccluded areas. In the training process, multi-dimensional loss functions are integrated as the mentor for more effective learning supervision. Finally, an occluded person ReID model with better feature representation capability is produced. Extensive experiments are conducted on five publicly released databases for ReID. The results demonstrate that the proposed method outperforms the state-of-the-art methods in the occluded person ReID task and another two ReID tasks.},
  archive      = {J_APIN},
  author       = {Wang, Liangbo and Zhou, Yu and Sun, Yanjing and Li, Song},
  doi          = {10.1007/s10489-021-02820-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7407-7419},
  shortjournal = {Appl. Intell.},
  title        = {Occluded person re-identification based on differential attention siamese network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reversible data hiding: A contemporary survey of
state-of-the-art, opportunities and challenges. <em>APIN</em>,
<em>52</em>(7), 7373–7406. (<a
href="https://doi.org/10.1007/s10489-021-02789-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this survey is to review the state-of-the art Reversible Data Hiding (RDH) methods, classify these methods into different classes, and list out new trends in this field. RDH, in general, is a challenging problem and has potential applications in the today’s digital world. Reversible data hiding methods not only securely transfer secret data but also recover the cover media faithfully. Recently, RDH methods are mainly focused on obtaining high capacity along with tuneable quality. Although, extensive investigations in the field of reversible data hiding was carried out in the recent past, a comprehensive review of existing literature for listing out research gap and future directions has not yet been reported. In this survey, we have classified the reversible data hiding methods mainly into a) Plain domain b) Encrypted domain and also examine their pro and cons. Tabular comparison of various RDH methods has been provided considering various design and analysis aspects. Moreover, we discuss important issues related to reversible data hiding and use of benchmarked datasets along with performance metrics for evaluation of RDH methods.},
  archive      = {J_APIN},
  author       = {Kumar, Sanjay and Gupta, Anjana and Walia, Gurjit Singh},
  doi          = {10.1007/s10489-021-02789-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7373-7406},
  shortjournal = {Appl. Intell.},
  title        = {Reversible data hiding: A contemporary survey of state-of-the-art, opportunities and challenges},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel segmentation of hippocampus images using global
steered quantum inspired firefly algorithm. <em>APIN</em>,
<em>52</em>(7), 7339–7372. (<a
href="https://doi.org/10.1007/s10489-021-02688-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microscopic Image segmentation has a crucial role in detecting and diagnosing numerous critical diseases like Alzheimer’s disease, Kidney disease, Cancer, many infectious diseases, etc. Precise segmentation of hippocampus microscopic images is a prerequisite for analyzing and interpreting the brain tissues. A few metaheuristic-based multilevel image segmentation methods are found in the literature for the same. In this work, an enhanced firefly algorithm-based image segmentation method has been proposed to achieve a good quality segmentation. The proposed algorithm utilizes the classical firefly algorithm’s movement operation along with the concept of quantum superposition and quantum update operation. In this algorithm, the movements of quantum fireflies have been modeled based on two strategies: firstly, the less bright fireflies move towards the comparatively brighter ones and secondly, quantum fireflies are updated according to the global optimum by the quantum update operation. This global steered Quantum Inspired Firefly Algorithm (QIFA) has been proposed and used for the multilevel hippocampus image segmentation considering correlation and structural similarity index as objective functions. In order to validate the quality of segmentation, the F-score values with respect to the segmented images have been reported. The proposed algorithm’s performance has been compared with seven other metaheuristic algorithms. The experimental results establish that the proposed algorithm is effective in producing good quality segmentation of hippocampus images.},
  archive      = {J_APIN},
  author       = {Choudhury, Alokeparna and Samanta, Sourav and Pratihar, Sanjoy and Bandyopadhyay, Oishila},
  doi          = {10.1007/s10489-021-02688-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7339-7372},
  shortjournal = {Appl. Intell.},
  title        = {Multilevel segmentation of hippocampus images using global steered quantum inspired firefly algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal deep learning framework for multi-type
hemorrhagic lesions detection and quantification in head CT images for
traumatic brain injury. <em>APIN</em>, <em>52</em>(7), 7320–7338. (<a
href="https://doi.org/10.1007/s10489-021-02782-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traumatic Brain Injury (TBI) could lead to intracranial hemorrhage (ICH), which has now been identified as a major cause of death after trauma if it is not adequately diagnosed and properly treated within the first 24 hours. CT examination is widely preferred for urgent ICH diagnosis, which enables the fast identification and detection of ICH regions. However, the use of it requires the clinical interpretation by experts to identify the subtypes of ICH. Besides, it is unable to provide the details needed to conduct quantitative assessment, such as the volume and thickness of hemorrhagic lesions, which may have prognostic importance to the decision-making on emergency treatment. In this paper, an optimal deep learning framework is proposed to assist the quantitative assessment for ICH diagnosis and the accurate detection of different subtypes of ICH through head CT scan. Firstly, the format of raw input data is converted from 3D DICOM to NIfTI. Secondly, a pre-trained multi-class semantic segmentation model is applied to each slice of CT images, so as to obtain a precise 3D mask of the whole ICH region. Thirdly, a fine-tuned classification neural network is employed to extract the key features from the raw input data and identify the subtypes of ICH. Finally, a quantitative assessment algorithm is adopted to automatically measure both thickness and volume via the 3D shape mask combined with the output probabilities of the classification network. The results of our extensive experiments demonstrate the effectiveness of the proposed framework where the average accuracy of 96.21 percent is achieved for three types of hemorrhage. The capability of our optimal classification model to distinguish between different types of lesion plays a significant role in reducing the false-positive rate in the existing work. Furthermore, the results suggest that our automatic quantitative assessment algorithm is effective in providing clinically relevant quantification in terms of volume and thickness. It is more important than the qualitative assessment conducted through visual inspection to the decision-making on emergency surgical treatment.},
  archive      = {J_APIN},
  author       = {Phaphuangwittayakul, Aniwat and Guo, Yi and Ying, Fangli and Dawod, Ahmad Yahya and Angkurawaranon, Salita and Angkurawaranon, Chaisiri},
  doi          = {10.1007/s10489-021-02782-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7320-7338},
  shortjournal = {Appl. Intell.},
  title        = {An optimal deep learning framework for multi-type hemorrhagic lesions detection and quantification in head CT images for traumatic brain injury},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved adaptive coding learning for artificial bee colony
algorithms. <em>APIN</em>, <em>52</em>(7), 7271–7319. (<a
href="https://doi.org/10.1007/s10489-021-02711-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the artificial bee colony (ABC) algorithm has become increasingly popular in the field of evolutionary computing and manystate- of-the-art ABC variants (ABCs) have been developed. It has found that ABCs are optimal for separable problems, but suffer drastic performance losses for non-separable problems. Driven by this phenomenon, improved adaptive encoding learning (IAEL) has been integrated into ABCs (IAEL+ABCs) to enhance their performance for non-separable problems. In IAEL+ABCs, the cumulative population distribution information is utilized to establish an Eigen coordinate system that can effectively increase the improvement interval of variables, and thus make the population converge quickly in the early stage of evolution. In addition, a multivariable perturbation strategy serves as a supplementary method for reducing the risk of ABCs falling into local optima in complex multimodal non-separable problems. For comparison purposes, all experiments were conducted on CEC2014 competition benchmark suite. The experimental results show that the proposed IAEL+ABCs perform better than their corresponding ABCs and previously developed AEL+ABCs.},
  archive      = {J_APIN},
  author       = {Jiang, Qiaoyong and Cui, Jianan and Ma, Yueqi and Wang, Lei and Lin, Yanyan and Li, Xiaoyu and Feng, Tongtong and Wu, Yali},
  doi          = {10.1007/s10489-021-02711-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7271-7319},
  shortjournal = {Appl. Intell.},
  title        = {Improved adaptive coding learning for artificial bee colony algorithms},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A coordinated many-objective evolutionary algorithm using
random adaptive parameters. <em>APIN</em>, <em>52</em>(7), 7248–7270.
(<a href="https://doi.org/10.1007/s10489-021-02707-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection strategy is an essential evolutionary component for many-objective evolutionary algorithms, including mating selection and environmental selection. However, there are still many challenges in evolution as the number of objectives increases, such as the conflict between diversity and convergence, and insufficient Pareto selection pressure. To address these problems, this paper proposes a coordinated many-objective evolutionary algorithm using random adaptive parameters (MaOEA-CO). Specifically, the algorithm adopts the coordinated selection mechanism as a new mating selection strategy that regulates the diversity and convergence weight of individuals through random adaptive parameters design, which can better balance the diversity and convergence of individuals at the edge of the Pareto front. Moreover, an environmental selection method based on coordination angle and Pareto distance indicators is designed. The angle indicator selects the two less diverse individuals from the whole population, and the Pareto distance indicator is used to remove the poor convergence individuals. We are ensuring population diversity while improving the selection pressure of the algorithm. The results of comparative experiments conducted on the standard test suite and Wilcoxon demonstrate the superiority of the MaOEA-CO algorithm in comparison with six state-of-the-art designs in terms of solution quality and computational efficiency. Besides, a many-objective coal model is applied to verify the performance of the MaOEA-CO algorithm further. The algorithm provides a better Pareto solution and promotes the development of coal enterprises.},
  archive      = {J_APIN},
  author       = {Wu, Di and Zhang, Jiangjiang and Geng, Shaojin and Cai, Xingjuan},
  doi          = {10.1007/s10489-021-02707-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7248-7270},
  shortjournal = {Appl. Intell.},
  title        = {A coordinated many-objective evolutionary algorithm using random adaptive parameters},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling knowledge proficiency using multi-hierarchical
capsule graph neural network. <em>APIN</em>, <em>52</em>(7), 7230–7247.
(<a href="https://doi.org/10.1007/s10489-021-02765-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Tracking (KT) predicts student performance by modeling the mastery of knowledge components in past interactions. Although the latest research on KT is excellent to model behavior changes, the increasingly complex knowledge components and semantic relationships of new exercises have made courses sparse and difficult to adequately diagnose knowledge mastery. Hence, it is not enough to satisfy only the personalized learning. This paper proposes a graph-based method with multi-hierarchical network (Caps-HAGKT) for tracking capsule knowledge, which integrates rich structural information of the knowledge space, including multiple skills, prerequisites, attribution, and conceptual meta-paths. Firstly, we use the multi-hierarchical knowledge capsule network to track a variety of high-level knowledge concepts, and then extract the latent knowledge structure between levels. In addition, we adopt a new graph network with an external memory matrix to model the relationship between concept capsules at the same level and update concept states. In previous works, it was difficult to use complex graph information without any expert pre-definition. In Caps-HAGKT, the semantic attention of knowledge capsules is used to merge different semantics in knowledge according to their importance. Extensive experiments on some real-world datasets show that the prediction performance of Caps-HAGKT is better than that of the existing KT method (AUC is increased by 3.05% on an average). The visualization of both latent graphs and case studies indicates the excellent interpretability of predictions.},
  archive      = {J_APIN},
  author       = {He, Zeyu and Li, Wang and Yan, Yonghong},
  doi          = {10.1007/s10489-021-02765-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7230-7247},
  shortjournal = {Appl. Intell.},
  title        = {Modeling knowledge proficiency using multi-hierarchical capsule graph neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating emotion for response generation in multi-turn
dialogues. <em>APIN</em>, <em>52</em>(7), 7218–7229. (<a
href="https://doi.org/10.1007/s10489-021-02819-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating semantically and emotionally context-consistent responses is key to intelligent dialogue systems. Previous works mainly refer to the context in the dialogue history to generate semantically related responses, ignoring the potential emotion in the conversation. In addition, existing methods mainly fail to consider the emotional changes of interlocutors and emotional categories simultaneously. However, emotion is crucial to reflect the interlocutor’s intent. In this paper, we propose an Emotion Capture Chat Machine (ECCM) that is able to capture the explicit and underlying emotional signal in the context to generate appropriate responses. In detail, we design a hierarchical recursive encoder-decoder framework with two enhanced self-attention encoders to capture the semantic signal and emotional signal, respectively, which are then fused in the decoder to produce the response. In general, we consider the dynamic and potential information of emotion to generate the response in multi-turn dialogues in the field of both daily conversation and psychological counseling. Our experimental results on a daily Chinese conversation dataset and a psychological counseling dataset show that ECCM outperforms the state-of-the-art baselines in terms of Perplexity, Distinct-1, Distinct-2, and manual evaluation. In addition, we find that ECCM performs well for input contexts with different lengths.},
  archive      = {J_APIN},
  author       = {Mao, Yanying and Cai, Fei and Guo, Yupu and Chen, Honghui},
  doi          = {10.1007/s10489-021-02819-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7218-7229},
  shortjournal = {Appl. Intell.},
  title        = {Incorporating emotion for response generation in multi-turn dialogues},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). D3FC: Deep feature-extractor discriminative
dictionary-learning fuzzy classifier for medical imaging. <em>APIN</em>,
<em>52</em>(7), 7201–7217. (<a
href="https://doi.org/10.1007/s10489-021-02781-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing accurate and speedy diagnosis and, in turn, treatment, automated medical image analysis plays a significant role in survival rate improvement. Inherent different kinds of uncertainties and complexities prove machine learning-based, particularly dictionary-learning-based classification approaches, very promisingly. This work concerns class-specific fuzzy discriminative dictionary learning using deep features on the continuum of our machine-learning-based medical image classifiers’ evolution path. In D3FC, a deep autoencoder generates a more relevant, representative, and compact features set. The distinctive-hidden information and inherent complexity and uncertainty of medical images are addressed using fuzzy-discriminative terms in the optimization function, simultaneously improving the inter-class-representation distance and intra-class-representation similarity. A comprehensive set of experiments on cancer tumor images from three different databases shows the outperformance of D3FC over related state-of-the-art competitions in accuracy, sensitivity, specificity, precision, convergence speed, and noise resilience. The meaningfulness of the experiments’ results is statistically verified.},
  archive      = {J_APIN},
  author       = {Ghasemi, Majid and Kelarestaghi, Manoochehr and Eshghi, Farshad and Sharifi, Arash},
  doi          = {10.1007/s10489-021-02781-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7201-7217},
  shortjournal = {Appl. Intell.},
  title        = {D3FC: Deep feature-extractor discriminative dictionary-learning fuzzy classifier for medical imaging},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory prediction and tracking using a multi-behaviour
social particle filter. <em>APIN</em>, <em>52</em>(7), 7158–7200. (<a
href="https://doi.org/10.1007/s10489-021-02286-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D motion tracking is a challenging task when both the tracked object and the observer are moving. In this paper, we present a multi-behavioural social force-based particle filter to track a group of moving humans from a moving robot using a limited field-of-view monocular camera. The application is a robotic guide and while moving, the robot often loses visibility of one or more people in the group, who must still be tracked. As an example, due to limited space, when the robot takes a sharp turn to avoid an obstacle or circumvent a corner, the visibility of the people at the rear is lost for some time. Therefore, several human social behavioural aspects have been implemented to predict the human’s motion in a group. The model accounts for attraction and repulsion between the people of the group and those with the robot, to maintain a comfortable social distance with each other at equilibrium. Additionally, when any person leaves the group then the track is deleted and after joining the track is automatically re-initialized. In the literature, the time of invisibility is a criterion to detect a person who has left the system, which however cannot be used here since the invisibility may be due to a limited field of view or the robot making a sharp turn to avoid an obstacle or circumventing a corner. Social heuristics are used to accurately detect people leaving the robotic system. The tracked trajectory is compared with ground truth and our system gives a very less error when compared with several baseline approaches. False positives are reduced, and the accuracy also increased with our proposed model as compared to other baseline methods. This method has been tested on several scenarios to ensure its validity.},
  archive      = {J_APIN},
  author       = {Malviya, Vaibhav and Kala, Rahul},
  doi          = {10.1007/s10489-021-02286-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7158-7200},
  shortjournal = {Appl. Intell.},
  title        = {Trajectory prediction and tracking using a multi-behaviour social particle filter},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Mining interesting sequences with low average cost and high
average utility. <em>APIN</em>, <em>52</em>(7), 7136–7157. (<a
href="https://doi.org/10.1007/s10489-021-02505-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering high utility sequences in a quantitative database is a popular data mining task. The goal is to enumerate all sequences of items (symbols) that have a high value for the user, as measured by a utility function. A representative application of high utility sequence mining is the identification of profitable sequences of purchases in transactions from online stores. Though useful, a drawback of that task is that the cost of items is not considered. However, cost is a key factor for decision-making in that domain and many others. To consider both the cost and utility of items for sequence mining, this paper defines a novel problem $$ \mathcal{FLCHUSM} $$ of mining frequent sequences having a high average utility and a low average cost. Though the proposed problem is a generalization of the traditional problem of frequent sequence mining, it is more challenging because the average utility and average cost functions do not satisfy the downward-closure property traditionally used to reduce the search space. To offer a solution to this issue, this paper presents a lower bound on the cost and two novel upper bounds on the utility. Besides, four width, depth pruning, reducing and tightening strategies are devised to eliminate unpromising patterns from the search space. Taking these theoretical results as a foundation, a new CUL (Cost-Utility List) data structure is conceived for storing and quickly updating the utility and cost information of patterns, and a novel algorithm named FLCHUSPM is proposed for $$ \mathcal{FLCHUSM} $$ . Results from several experiments show that FLCHUSPM is efficient in terms of memory usage and runtime, and that interesting patterns can be discovered in real data.},
  archive      = {J_APIN},
  author       = {Truong, Tin and Duong, Hai and Le, Bac and Fournier-Viger, Philippe and Yun, Unil},
  doi          = {10.1007/s10489-021-02505-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7136-7157},
  shortjournal = {Appl. Intell.},
  title        = {Mining interesting sequences with low average cost and high average utility},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault centrality: Boosting spectrum-based fault localization
via local influence calculation. <em>APIN</em>, <em>52</em>(7),
7113–7135. (<a
href="https://doi.org/10.1007/s10489-021-02822-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectrum-Based Fault Localization (SBFL) is a widely investigated heuristic approach and a lightweight but efficient technique. Recently, to discover useful latent information for accelerating fault localization, more attention has been paid to the research of the fault-relevant correlation among software entities. Considering that the interactive behaviors among software entities may imply some fault patterns, we introduce the fault influence of interactive entities and develop a novel synthetical fault localization approach based on the software network. (1) In line with the intuition that the entity correlated with more suspicious entities is more likely to be faulty, we firstly construct a directed, node-weighted, and link-weighted Software Fault Network (SFN). As a characterization solution, SFN maps methods into nodes and method-call relations into links. Then, SFN initializes the node weights by the raw suspiciousness score of a specific SBFL formula and assigns the link weights by the faulty similarity Fault Influence Coefficient (FIC). (2) A suspiciousness measure criterion named Fault Centrality (FC) is proposed based on SFN. This approach calculates the final suspiciousness score by aggregating the faulty influence of local interactive methods. We conduct the experiments on 349 faults of Defects4J and separately apply 33 existing SBFL formulas. According to the results, this approach can boost almost all the performance of the existing formulas. The outcomes of acc@1, acc@3, and acc@5 are better by 9.43%, 17.40%, and 18.92%, respectively.},
  archive      = {J_APIN},
  author       = {Zhao, Guyu and He, Hongdou and Huang, Yifang},
  doi          = {10.1007/s10489-021-02822-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7113-7135},
  shortjournal = {Appl. Intell.},
  title        = {Fault centrality: Boosting spectrum-based fault localization via local influence calculation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Faster doubly stochastic functional gradient by gradient
preconditioning for scalable kernel methods. <em>APIN</em>,
<em>52</em>(7), 7091–7112. (<a
href="https://doi.org/10.1007/s10489-021-02618-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The doubly stochastic functional gradient descent algorithm (DSG) that is memory friendly and computationally efficient can effectively scale up kernel methods. However, in solving the highly ill-conditioned large-scale nonlinear machine learning problem, the convergence speed of DSG is quite slow. This is because the condition number of the Hessian matrix of this problem is quite large, which will make stochastic gradient methods converge very slowly. Fortunately, gradient preconditioning is a well-established technique in optimization aiming to reduce the condition number. Therefore, we propose a preconditioned doubly stochastic functional gradient descent algorithm (P-DSG) by combining DSG with gradient preconditioning. P-DSG first uses the gradient preconditioning to adaptively scale the individual components of the estimated functional gradient obtained by DSG, and then utilizes the preconditioned functional gradient as the descent direction in each iteration. Theoretically, an appropriate preconditioner is always the inverse of the Hessian matrix at the optimum, which is not easy to get due to its high computation cost. Therefore, we first choose an empirical covariance matrix of random Fourier features to approximate the Hessian matrix, and then perform a low-rank approximation to the empirical covariance matrix. P-DSG has a fast convergence rate $\mathcal {O}(1/t)$ and produces a smaller constant factor in the boundary than that of DSG while remains $\mathcal {O}(t)$ memory friendly and $\mathcal {O}(td)$ computationally efficient. Finally, we test the performance of P-DSG on the kernel ridge regression, kernel support vector machines, and kernel logistic regression, respectively. The experimental results show that P-DSG speeds up convergence and achieves better performance.},
  archive      = {J_APIN},
  author       = {Zhang, Zhuan and Zhou, Shuisheng and Yang, Ting and Zhang, Junna},
  doi          = {10.1007/s10489-021-02618-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7091-7112},
  shortjournal = {Appl. Intell.},
  title        = {Faster doubly stochastic functional gradient by gradient preconditioning for scalable kernel methods},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variable neighborhood descent with ant colony optimization
to solve a bilevel problem with station location and vehicle routing.
<em>APIN</em>, <em>52</em>(7), 7070–7090. (<a
href="https://doi.org/10.1007/s10489-021-02748-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicles are becoming popular in transport systems due to subsidies provided by the governments and the reduction of environmental issues. A bilevel optimization problem rises when the interests of governments (minimizing the infrastructure costs) and transportation companies (minimizing the routing costs) are considered. Also, both electric vehicles and internal combustion vehicles can be used, increasing the complexity of the problem. This work proposes a Variable Neighborhood Descent combined with an Ant Colony Optimization with local search and a Route Selection Procedure for solving a bilevel optimization problem. Variable Neighborhood Descent is applied at the upper level in the Station Allocation Problem while Ant Colony Optimization with local search and Route Selection Procedure are applied to the lower level in the Vehicle Routing Problem. Computational experiments were performed using two different sets of instances and the results obtained indicate that the proposal achieved good results at both levels when compared with other approaches from the literature, with low construction and routing cost and always keeping the proportion of electric vehicles higher than requested.},
  archive      = {J_APIN},
  author       = {Leite, Marcos R.C.O. and Bernardino, Heder S. and Gonçalves, Luciana B.},
  doi          = {10.1007/s10489-021-02748-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7070-7090},
  shortjournal = {Appl. Intell.},
  title        = {A variable neighborhood descent with ant colony optimization to solve a bilevel problem with station location and vehicle routing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating performance of AI operators using roofline model.
<em>APIN</em>, <em>52</em>(7), 7054–7069. (<a
href="https://doi.org/10.1007/s10489-021-02794-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence algorithms have shown performance advantages in a wide range of application domains. However, the increasing demand for hardware resources raises challenges for AI accelerator design. To alleviate this issue, the academic community has presented much research. Among these, evaluating the performance of AI algorithms on accelerators is a hot topic. However, such work usually requires a miscellaneous experimental setup configuration, and may involve repetitive tests. Instead of conducting redundant experiments with prior research, in this paper, we present a comprehensive evaluation of AI operators rather than AI algorithms in an easy-to-operate manner. We first explore common AI operators in a variety of AI algorithms with an in-depth analysis. We identify six representative operator categories. Then, we analyze their performance using roofline model. To verify our analysis, we conduct simple evaluation experiments, where several AI operators are evaluated on two NVIDIA GPUs. We observe from the evaluation results that AI operators benefit from low-precision, large-size on-chip cache and high-bandwidth off-chip memory, and sparsity processing. Based on the observations, we propose three optimization opportunities for AI accelerator design, including multiple-precision support, an efficient memory system, and sparsity processing.},
  archive      = {J_APIN},
  author       = {Chen, Zhengbo and Zheng, Fang and Yu, Qi and Sun, Rujun and Guo, Feng and Chen, Zuoning},
  doi          = {10.1007/s10489-021-02794-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7054-7069},
  shortjournal = {Appl. Intell.},
  title        = {Evaluating performance of AI operators using roofline model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented domain agreement for adaptable meta-learner on
few-shot classification. <em>APIN</em>, <em>52</em>(7), 7037–7053. (<a
href="https://doi.org/10.1007/s10489-021-02744-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning approaches are recently introduced to obtain a proficient model in the problem of few-shot learning. The existing state-of-the-art methods can resolve the training difficulty to achieve the model generalization with limited instances by incorporating several tasks containing various drawn classes. However, during the fast-adaptation stage, the characteristics of instances in meta-training and meta-test sets are assumed to be similar. In contrast, most meta-learning algorithms are implemented in challenging settings where those are drawn from different populations. This critical assumption can cause the model to exhibit degraded performance. We propose an Augmented Domain Agreement for Adaptable Meta-Learner (AD2AML), which augments the domain adaptation framework in meta-learning to overcome this problem. We minimize the latent representation divergence of the inputs drawn from different distributions to enhance the model at obtaining more general features. Therefore, the trained network can be more transferable at shifted domain conditions. Furthermore, we extend our main idea by augmenting the image reconstruction network with cross-consistency loss to encourage the shared network to extract a similar input representation. We demonstrate our proposed method’s effectiveness on the benchmark datasets of few-shot classification and few-shot domain adaptation problems. Our experiment shows that our proposed idea can improve generalization performance. Moreover, the extension with image reconstruction and cross-consistency loss can stabilize domain loss minimization during training.},
  archive      = {J_APIN},
  author       = {Widhianingsih, Tintrim Dwi Ary and Kang, Dae-Ki},
  doi          = {10.1007/s10489-021-02744-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {7},
  pages        = {7037-7053},
  shortjournal = {Appl. Intell.},
  title        = {Augmented domain agreement for adaptable meta-learner on few-shot classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detailed 3D human body reconstruction from multi-view images
combining voxel super-resolution and learned implicit representation.
<em>APIN</em>, <em>52</em>(6), 6739–6759. (<a
href="https://doi.org/10.1007/s10489-021-02783-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of reconstructing detailed 3D human body models from images is interesting but challenging in computer vision due to the high freedom of human bodies. This work proposes a coarse-to-fine method to reconstruct detailed 3D human body from multi-view images combining Voxel Super-Resolution (VSR) based on learning the implicit representation. Firstly, the coarse 3D models are estimated by learning an Pixel-aligned Implicit Function based on Multi-scale Features (MF-PIFu) which are extracted by multi-stage hourglass networks from the multi-view images. Then, taking the low resolution voxel grids which are generated by the coarse 3D models as input, the VSR is implemented by learning an implicit function through a multi-stage 3D convolutional neural network. Finally, the refined detailed 3D human body models can be produced by VSR which can preserve the details and reduce the false reconstruction of the coarse 3D models. Benefiting from the implicit representation, the training process in our method is memory efficient and the detailed 3D human body produced by our method from multi-view images is the continuous decision boundary with high-resolution geometry. In addition, the coarse-to-fine method based on MF-PIFu and VSR can remove false reconstructions and preserve the appearance details in the final reconstruction, simultaneously. In the experiments, our method quantitatively and qualitatively achieves the competitive 3D human body models from images with various poses and shapes on both the real and synthetic datasets.},
  archive      = {J_APIN},
  author       = {Li, Zhongguo and Oskarsson, Magnus and Heyden, Anders},
  doi          = {10.1007/s10489-021-02783-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6739-6759},
  shortjournal = {Appl. Intell.},
  title        = {Detailed 3D human body reconstruction from multi-view images combining voxel super-resolution and learned implicit representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). BBW: A batch balance wrapper for training deep neural
networks on extremely imbalanced datasets with few minority samples.
<em>APIN</em>, <em>52</em>(6), 6723–6738. (<a
href="https://doi.org/10.1007/s10489-021-02623-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Deep Neural Networks (DNNs) have achieved excellent performance on many tasks, but it is very difficult to train good models from imbalanced datasets. Creating balanced batches either by majority data down-sampling or by minority data up-sampling can solve the problem in certain cases. However, it may lead to learning process instability and overfitting. In this paper, we propose the Batch Balance Wrapper (BBW), a novel framework which can adapt a general DNN to be well trained from extremely imbalanced datasets with few minority samples. In BBW, two extra network layers are added to the start of a DNN. The layers prevent overfitting of minority samples and improve the expressiveness of the sample distribution of minority samples. Furthermore, Batch Balance (BB), a class-based sampling algorithm, is proposed to make sure the samples in each batch are always balanced during the learning process. We test BBW on three well-known extremely imbalanced datasets with few minority samples. The maximum imbalance ratio reaches 1167:1 with only 16 positive samples. Compared with existing approaches, BBW achieves better classification performance. In addition, BBW-wrapped DNNs are 16.39 times faster, relative to unwrapped DNNs. Moreover, BBW does not require data preprocessing or additional hyper-parameter tuning, operations that may require additional processing time. The experiments prove that BBW can be applied to common applications of extremely imbalanced data with few minority samples, such as the classification of EEG signals, medical images and so on.},
  archive      = {J_APIN},
  author       = {Hu, Jingzhao and Zhang, Hao and Liu, Yang and Sutcliffe, Richard and Feng, Jun},
  doi          = {10.1007/s10489-021-02623-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6723-6738},
  shortjournal = {Appl. Intell.},
  title        = {BBW: A batch balance wrapper for training deep neural networks on extremely imbalanced datasets with few minority samples},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling adaptive empathy based on neutral assessment: A way
to enhance the prosocial behaviors of socialized agents under the
premise of self-security. <em>APIN</em>, <em>52</em>(6), 6692–6722. (<a
href="https://doi.org/10.1007/s10489-021-02712-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethical concerns over artificial intelligence (AI) have recently drawn extensive interest in both academia and industry. According to behavioral economics and neuropsychology, empathy may be an inherent mechanism to elicit prosocial behaviors. Therefore, we establish a three-layer general framework of empathetic AI (eAI) with emotion, empathy, and decision-making. By introducing different sub-models, three learning structures based on eAI are proposed, including a gradient ascent (GA)-based structure, an adaptive learning structure, and a practical learning structure. The dynamics of the first two structures are analyzed theoretically, and the practical dynamics are tested in games. We prove that in the prisoner’s dilemma (PD) environment, the GA-based eAI with neutral assessment can carry out adaptive cooperation and competition under the premise of self-security. In addition, although the modeling of empathy by extracting the emotional contagion and limited cognitive regulation is simplified and primitive, tests in the prisoner’s dilemma, the ultimatum game, and a multi-agent dilemma game, show that the eAI structure successfully elicits prosocial behaviors including altruism, cooperation and fairness. Compared with other socialized algorithms, the eAI structure has a more comprehensive coverage in terms of convergence, fairness, security, adaptability, and structural expansibility. Therefore, we believe this work can provide novel methods and insights for regulating the behaviors of socialized agents, as well as artificial subjects in psychological and economic experiments.},
  archive      = {J_APIN},
  author       = {Chen, Jize and Zhang, Dali and Qu, Zhenshen and Wang, Changhong},
  doi          = {10.1007/s10489-021-02712-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6692-6722},
  shortjournal = {Appl. Intell.},
  title        = {Modeling adaptive empathy based on neutral assessment: A way to enhance the prosocial behaviors of socialized agents under the premise of self-security},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized eigenvalue extreme learning machine for
classification. <em>APIN</em>, <em>52</em>(6), 6662–6691. (<a
href="https://doi.org/10.1007/s10489-021-02654-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM) has attracted widespread attention due to its simple, quick and good performance. In this work, in order to deal with cross data quickly and efficiently, we first propose generalized eigenvalue proximal extreme learning machine (GEPELM). It takes the form of ratio into consideration to seek for two non-parallel separating hyperplanes in ELM feature space, each of which is close to the samples of its own class and far away from the others simultaneously. Then generalized eigenvalue algorithm is adopted to solve, which incurs GEPELM to enjoy faster calculation speed than TELM. Further, improved generalized eigenvalue proximal extreme learning machine (IGEPELM) is proposed, which uses minus instead of ratio to avoid singular value phenomenon and further mitigate the computational burden by solving two standard eigenvalue problems. To further improve classification performance, generalized eigenvalue proximal extreme learning machine based on inter-class graph (GGEPELM) is proposed, which incorporates the geometric structure information of dissimilar samples into the guideline of GEPELM. In addition, the proposed classifiers are all extended to kernel ELM framework to handle non-linear data more precisely. Moreover, Sherman-Morrison-Woodbury formula is utilized to reduce time complexity of matrix inversion. Simultaneously, a quick solution strategy is incorporated into GEPELMs and GGEPELMs to mitigate the burden of solving large-scale problems. The numerical simulations are carried out on three databases including a benchmark database, an artificial database and a practical application database, which demonstrates the proposed classifiers enjoy high computational speed, good generalization performance and insensitivity to parameters.},
  archive      = {J_APIN},
  author       = {Sun, Ping and Yang, Liming},
  doi          = {10.1007/s10489-021-02654-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6662-6691},
  shortjournal = {Appl. Intell.},
  title        = {Generalized eigenvalue extreme learning machine for classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-adaptive nonoverlapping sequential pattern mining.
<em>APIN</em>, <em>52</em>(6), 6646–6661. (<a
href="https://doi.org/10.1007/s10489-021-02763-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive sequential pattern mining (SPM) with gap constraints is a data analysis task that consists of identifying patterns (subsequences) appearing many times in a discrete sequence of symbols or events. By using gap constraints, the user can filter many meaningless patterns, and focus on those that are the most interesting for his needs. However, it is difficult to set appropriate gap constraints without prior knowledge. Hence, users generally find suitable constraints by trial and error, which is time-consuming. Besides, current algorithms are inefficient as they repeatedly check whether the gap constraints are satisfied. To address these problems, this paper presents a complete algorithm called SNP-Miner that has two key phases: candidate pattern generation and support (number of occurrences or occurrence frequency) calculation. To reduce the number of candidate patterns, SNP-Miner employs a pattern join strategy. Moreover, to efficiently calculate the support, SNP-Miner uses an incomplete Nettree structure stored in an array, and scans the structure once to avoid redundant calculations and reduce the time complexity. Experimental results show that SNP-Miner not only outperforms competitive algorithms, but can also discover more valuable patterns without user-predefined gap constraints. Algorithms and data can be downloaded from https://github.com/wuc567/Pattern-Mining/tree/master/SNP-Miner .},
  archive      = {J_APIN},
  author       = {Wang, Yuehua and Wu, Youxi and Li, Yan and Yao, Fang and Fournier-Viger, Philippe and Wu, Xindong},
  doi          = {10.1007/s10489-021-02763-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6646-6661},
  shortjournal = {Appl. Intell.},
  title        = {Self-adaptive nonoverlapping sequential pattern mining},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ennoble class topper optimization algorithm based fuzzy
PI-PD controller for micro-grid. <em>APIN</em>, <em>52</em>(6),
6623–6645. (<a
href="https://doi.org/10.1007/s10489-021-02704-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-area micro-grid system (MAMG) is a complex nonlinear system. Instability or performance degradation may cause due to inadequate damping under the sudden load fluctuation in the system. Owing to this, to provide uninterrupted electric power with additional attributes, strong and intelligent control methods are curiously necessary for automatic generation control of micro-grid (MG). The utilization of ennoble class topper optimization (E-CTO) tuned fuzzy cascaded proportional-integral proportional-derivative (fuzzy PI-PD), i.e., fuzzy PI-PD controller in frequency control in two areas interconnected MG (isolation mode) with renewable penetration is a novel work. A maiden attempt of the E-CTO is intended to optimize the range of the membership function of the fuzzy controller by employing the integral time absolute error (ITAE) criterion. The effectiveness of the proposed controller is exposed by opposing the dynamic answers of MAMG with a fuzzy PI-PD and traditional controller. In the end, a sensitivity study is offered to show the power of the planned approach to wide fluctuations in the MG parameters, magnitude likewise the situation of step/ random load disturbance. The recommended MAMG and optimization techniques are simulated in the LabVIEW@2015 environment. Also, OPAL-RT based real-time analysis is presented to full-prof the novel work.},
  archive      = {J_APIN},
  author       = {Rai, Ankur and Das, Dushmanta Kumar},
  doi          = {10.1007/s10489-021-02704-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6623-6645},
  shortjournal = {Appl. Intell.},
  title        = {Ennoble class topper optimization algorithm based fuzzy PI-PD controller for micro-grid},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video representation learning by identifying spatio-temporal
transformations. <em>APIN</em>, <em>52</em>(6), 6613–6622. (<a
href="https://doi.org/10.1007/s10489-021-02790-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning becomes a prevalent paradigm in both image and video domains due to the difficulty in obtaining a large amount of annotated data. In this paper, we adopt the self-supervised learning paradigm and propose to learn 3D video representations by identifying spatio-temporal transformations. Specifically, we choose a set of transformations and apply them to unlabelled videos to change the spatio-temporal structure of these videos. By identifying these spatio-temporal transformations, the network learns knowledge about both spatial appearance and temporal relation of video frames. In this paper, we choose the spatio-temporal rotations as the transformations. We conduct extensive experiments to validate the effectiveness of the proposed method. After fine-tuning on action recognition benchmarks, our model yields a remarkable gain of 29.6% on UCF101 and 25.1% on HMDB51 compared with models trained from scratch, which belongs to the current advanced method.},
  archive      = {J_APIN},
  author       = {Geng, Sheng and Zhao, Shimin and Liu, Hu},
  doi          = {10.1007/s10489-021-02790-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6613-6622},
  shortjournal = {Appl. Intell.},
  title        = {Video representation learning by identifying spatio-temporal transformations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of protein-nucleotide binding residues via
graph regularized k-local hyperplane distance nearest neighbor model.
<em>APIN</em>, <em>52</em>(6), 6598–6612. (<a
href="https://doi.org/10.1007/s10489-021-02737-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of protein-nucleotide binding residues is crucial for the study of drug structure and protein functional annotation. The study of protein-nucleotide binding residues is a typical problem of sample imbalance. The minority class (binding residues) are far less than the majority class (non-binding residues). The traditional machine learning algorithm is not universal for this kind of research, the results will be seriously biased to majority class. To deal with the serious imbalance problem, we propose a new computational method to identify protein-nucleotide binding residues via Graph Regularized k-local Hyperplane Distance Nearest Neighbor (GHKNN). On the training set, we compare the performance of the basic classifier, the ensemble classifier and the single classifier. On the independent test sets, we compare the performance with other existing models. The experimental results prove that our proposed method has higher accuracy in the identification of protein-nucleotide binding residues and is more prominent than other existing models. The data and material are freely available at https://github.com/guofei-tju/GHKNN .},
  archive      = {J_APIN},
  author       = {Ding, Yijie and Yang, Chao and Tang, Jijun and Guo, Fei},
  doi          = {10.1007/s10489-021-02737-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6598-6612},
  shortjournal = {Appl. Intell.},
  title        = {Identification of protein-nucleotide binding residues via graph regularized k-local hyperplane distance nearest neighbor model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image captioning with adaptive incremental global context
attention. <em>APIN</em>, <em>52</em>(6), 6575–6597. (<a
href="https://doi.org/10.1007/s10489-021-02734-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The encoder-decoder framework has proliferated in current image captioning task, where the decoder generates target description word by word based on the preceding captions. However, this framework encounters two main concerns. Firstly, the decoder cannot adequately capture global dependencies between the current predicted target word and all the previously generated words. Secondly, some generated words (e.g., “on”, “the” and “of”) provide insufficient information, which may deviate from the sentence semantics during gradually generating captions. To address above concerns, in this paper we propose a novel adaptive incremental global context attention (IGCA) method to capture the global information between target words, thus enhancing target word predictions in image captioning. Specifically, all of previous historical decoder hidden states are utilized as the global feature to guide the generation of subsequent word. During the generation procedure, the proposed IGCA mechanism is able to dynamically focus on these text features that are most correlated with the currently generated word. To verify the efficiency of our IGCA model, we conducted extensive experiments on the three public benchmark datasets. The experimental results demonstrate that the proposed model brings significant improvement over the conventional attention-based encoder-decoder methods and achieves state-of-the-art performance on Flick 30k and Flick 8k datasets.},
  archive      = {J_APIN},
  author       = {Wang, Changzhi and Gu, Xiaodong},
  doi          = {10.1007/s10489-021-02734-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6575-6597},
  shortjournal = {Appl. Intell.},
  title        = {Image captioning with adaptive incremental global context attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ant colony algorithm with stackelberg game and
multi-strategy fusion. <em>APIN</em>, <em>52</em>(6), 6552–6574. (<a
href="https://doi.org/10.1007/s10489-021-02774-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the disadvantages of the ant colony algorithm, such as slow convergence speed and easy to fall into local optimum, this paper proposes an ant colony algorithm with Stackelberg game and multi-strategy fusion. Firstly, Stackelberg game is established between ant colonies, and the population with the excellent performance is taken as the leader to increase the influence of excellent ant colony. Secondly, a multi-strategy fusion system is proposed, which is composed of three strategies: One is the pheromone fusion strategy, which selects the population whose entropy is less than the threshold value and the population with the highest similarity for pheromone fusion to increase the diversity of the algorithm. The second is the elite ant learning strategy, which speeds up the convergence rate by learning the elite ants of the elite population; The third is the pheromone recombination strategy, which helps the algorithm jump out of the local optimum. The simulation experiments of multiple cases in TSPLIB show that the improved algorithm balances diversity and the convergence speed, and effectively improves the quality of the solution.},
  archive      = {J_APIN},
  author       = {Chen, Da and You, XiaoMing and Liu, Sheng},
  doi          = {10.1007/s10489-021-02774-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6552-6574},
  shortjournal = {Appl. Intell.},
  title        = {Ant colony algorithm with stackelberg game and multi-strategy fusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent multi-view self-representations for clustering via
the tensor nuclear norm. <em>APIN</em>, <em>52</em>(6), 6539–6551. (<a
href="https://doi.org/10.1007/s10489-021-02710-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to design effective multi-view subspace clustering (MVSC) algorithms has recently become a research hotspot. In this paper, we propose a new MVSC algorithm, termed latent multi-view self-representation for clustering via the tensor nuclear norm (LMVS/TNN), which can seamlessly unify multi-view clustering and dimensionality reduction into a framework. Specifically, for each view data, LMVS/TNN learns the transformed data from the original space, which can maintain the original manifold structure, and each subspace representation matrix from the transformed latent space simultaneously. Furthermore, to use the high-order correlations and complementary information from multi-view data, LMVS/TNN constructs a third-order tensor by taking the representation matrix extracted from the transformed latent space as the frontal slice of the third-order tensor and the tensor is constrained by a new low-rank tensor constraint, i.e., the tensor nuclear norm (TNN). In addition, based on the augmented Lagrangian scheme, we develop an efficient procedure to solve LMVS/TNN. To verify the performance of LMVS/TNN, we conduct experiments on public datasets and find that LMVS/TNN outperforms some representative clustering algorithms.},
  archive      = {J_APIN},
  author       = {Lu, Gui-Fu and Zhao, Jinbiao},
  doi          = {10.1007/s10489-021-02710-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6539-6551},
  shortjournal = {Appl. Intell.},
  title        = {Latent multi-view self-representations for clustering via the tensor nuclear norm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A referenceless image degradation perception method based on
the underwater imaging model. <em>APIN</em>, <em>52</em>(6), 6522–6538.
(<a href="https://doi.org/10.1007/s10489-021-02815-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of underwater image processing, more and more underwater image restoration algorithms have been proposed. To date, most underwater image degradation perception methods are ineffective in multi-color environments. In this paper, we proposed a referenceless image degradation perception method based on the underwater imaging model. Our method includes the colorfulness index, contrast index, and sharpness index. The colorfulness index is used to measure the color loss. The contrast index and sharpness index are used to measure the blurring. For the contrast index and sharpness index, we proposed a grayscale conversion method that can adaptively adjust the coefficients of red, green, and blue (RGB) values. Their generalization ability under multi-colored environments can be enhanced by the proposed adaptive grayscale conversion method. It is useful for underwater images that are normally dominated by green and blue color channels. Compared with the leading evaluation metrics available in the literature, our proposed method can better perceive the degradation of underwater images. No matter under what lighting conditions, our proposed three indexes can decrease with the degradation of images. It makes our proposed three indexes can be effective in both general underwater scenarios and hash scenarios. The correlation coefficients of our proposed indexes are the largest. More importantly, the proposed indexes can be used to process underwater images in real time and evaluate the performance of underwater image restoration algorithms. Our proposed indexes can also be applied to monitor the turbidity of water and improve the underwater image restoration algorithms. A single proposed index cannot comprehensively perceive the quality of underwater images. The proposed three indexes need to be integrated into an evaluation metric in future work.},
  archive      = {J_APIN},
  author       = {Luo, Zhihang and Tang, Zhijie and Jiang, Lizhou and Ma, Gaoqian},
  doi          = {10.1007/s10489-021-02815-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6522-6538},
  shortjournal = {Appl. Intell.},
  title        = {A referenceless image degradation perception method based on the underwater imaging model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach of bursty event detection in social networks
based on topological features. <em>APIN</em>, <em>52</em>(6), 6503–6521.
(<a href="https://doi.org/10.1007/s10489-021-02729-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User relations and information propagation on social networks can reflect events in real society. Online detection of bursty events is of great significance to studying the evolution of social networks and cyberspace security. Current research works focus on building an event recognition model based on text information and then utilizing clustering or topic model methods to extract features from the data stream and then detect events that have not existed before. The text-based method is designed for specific content, and it does not consider the features of network dynamic evolution. It is restricted by the type and quality of text in social networks, limiting its practical application scenarios. However, there exist remarkable correlations between the occurrence of events and the evolution of the network. In this paper, we consider mining the network structure changes to identify bursty events, the superiority which is that it is sensitive and widely used. We integrate snapshot network topology indexes to quantify its structural features. Then we can judge whether there is a burst event by investigating the change degree of the network structure features of the adjacent snapshots. The effectiveness and efficiency of our approach are further confirmed by experimental studies on four real social network data sets. In addition, we also discuss the salient features of the bursty events and compare the impact of the bursty events on the network structure with that of the scheduled events.},
  archive      = {J_APIN},
  author       = {Yang, Jie and Wu, Yu},
  doi          = {10.1007/s10489-021-02729-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6503-6521},
  shortjournal = {Appl. Intell.},
  title        = {An approach of bursty event detection in social networks based on topological features},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Imbalanced data classification based on improved
EIWAPSO-AdaBoost-c ensemble algorithm. <em>APIN</em>, <em>52</em>(6),
6477–6502. (<a
href="https://doi.org/10.1007/s10489-021-02708-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Boosting (AdaBoost) algorithm is a widely used ensemble learning algorithm. And it can effectively improve the classification performance of ordinary datasets when combined with many other types of learning algorithms. The AdaBoost algorithm focuses on the overall classification performance of weak classifiers and aims to minimize the overall classification error. However, it ignores the imbalance in the number of samples between different classes, so it is not suitable for imbalanced data classification directly. In order to improve the classification accuracy of the minority samples in imbalanced datasets, this paper proposes an improved AdaBoost algorithm based on weight adjustment factors (AdaBoost-C). It redefines the error rate function by assigning a higher weight to the minority sample to emphasize its importance, and assigning a lower weight to the majority sample to suppress its importance. In addition, this paper also proposes an adaptive particle swarm optimization algorithm with exponential dynamic adjustment of inertia weight (EIWAPSO) to further optimize the weight of the weak classifier. It can effectively prevent the ensemble algorithm from generating redundant and useless weak classifiers to consume system resources, and avoid falling into local optimum. The experimental results show that the Recall and AUC values of the EIWAPSO-AdaBoost-C ensemble algorithm proposed in this paper have reached the highest values on datasets with different IR, and the maximum, minimum and average errors of this algorithm have reached the minimum values in a variety of comparison algorithms. Therefore, the algorithm proposed in this paper can not only effectively improve the classification accuracy of minority samples on imbalanced datasets, but also the algorithm is more stable.},
  archive      = {J_APIN},
  author       = {Li, Xiao and Li, Kewen},
  doi          = {10.1007/s10489-021-02708-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6477-6502},
  shortjournal = {Appl. Intell.},
  title        = {Imbalanced data classification based on improved EIWAPSO-AdaBoost-C ensemble algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach of classifiers fusion based on hierarchical
modifications. <em>APIN</em>, <em>52</em>(6), 6464–6476. (<a
href="https://doi.org/10.1007/s10489-021-02777-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifiers fusion is considered as an effective way to promote the accuracy of pattern recognition. In practice, its performance is mainly limited by potentials and reliabilities of base classifiers, which are learned from different attribute spaces. In order to overcome the above problems, we present a new approach of classifiers fusion based on hierarchical modifications in the framework of belief function theory. At first, an intra-attribute modification is proposed to taking into account the potentials and reliabilities of base classifiers. Instead of discounting a classifier with a weight only, we employ a piece of evidence derived from the nearest labeled neighbor to modify the weighted output of one base classifier in its individual attribute space. Then, the modified output is combined with other modified results from their own attribute spaces and this procedure could be seen as an inter-attribute modification. Both modifications aim to make the classification result as close to the truth as possible, so we take them into account to construct a new objective function for optimizing the weights. Finally, some real data sets are used in experimental applications to demonstrate that the proposed method is superior to other related belief based fusion methods.},
  archive      = {J_APIN},
  author       = {Song, Lin and Sun, Yi-xiao},
  doi          = {10.1007/s10489-021-02777-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6464-6476},
  shortjournal = {Appl. Intell.},
  title        = {An approach of classifiers fusion based on hierarchical modifications},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analytics of high average-utility patterns in the industrial
internet of things. <em>APIN</em>, <em>52</em>(6), 6450–6463. (<a
href="https://doi.org/10.1007/s10489-021-02751-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, revealing more valuable information except for quantity value for a database is an essential research field. High utility itemset mining (HAUIM) was suggested to reveal useful patterns by average-utility measure for pattern analytics and evaluations. HAUIM provides a more fair assessment than generic high utility itemset mining and ignores the influence of the length of itemsets. There are several high-performance HAUIM algorithms proposed to gain knowledge from a disorganized database. However, most existing works do not concern the uncertainty factor, which is one of the characteristics of data gathered from IoT equipment. In this work, an efficient algorithm for HAUIM to handle the uncertainty databases in IoTs is presented. Two upper-bound values are estimated to early diminish the search space for discovering meaningful patterns that greatly solve the limitations of pattern mining in IoTs. Experimental results showed several evaluations of the proposed approach compared to the existing algorithms, and the results are acceptable to state that the designed approach efficiently reveals high average utility itemsets from an uncertain situation.},
  archive      = {J_APIN},
  author       = {Wu, Jimmy Ming-Tai and Li, Zhongcui and Srivastava, Gautam and Yun, Unil and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s10489-021-02751-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6450-6463},
  shortjournal = {Appl. Intell.},
  title        = {Analytics of high average-utility patterns in the industrial internet of things},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TL-FCM: A hierarchical prediction model based on two-level
fuzzy c-means clustering for bike-sharing system. <em>APIN</em>,
<em>52</em>(6), 6432–6449. (<a
href="https://doi.org/10.1007/s10489-021-02186-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, shared bikes have gradually emerged into public life as a new way to travel and helped solve the last-mile problem of residents’ travel. While this development has brought convenient travel to users, a series of problems exist, a prominent one is the uneven distribution of bikes at each shared bike station. Accurately predicting bike usage in a bike-sharing system can help solve this problem. In this paper, we investigate how to improve the accuracy of predicting the usage of bikes in bike-sharing system. First, considering both geographic location information of shared bike stations and the migration trend of bikes between stations, we design a two-level fuzzy c-means clustering algorithm to cluster shared bicycle stations into groups, which can better capture the connection between shared bicycle stations and improve the clustering accuracy of shared bicycle sites, then, we combine the two-level fuzzy c-means clustering algorithm with the multi-similarity reference model to predict the usage of bikes, which can significantly improve the accuracy of the forecast. To evaluate the performance of our model, we validate our model in the New York Bike-Sharing System. The results shows that our model obtained significantly better results than other models.},
  archive      = {J_APIN},
  author       = {Wang, Bin and Tan, Yanyan and Jia, Wenzhen},
  doi          = {10.1007/s10489-021-02186-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6432-6449},
  shortjournal = {Appl. Intell.},
  title        = {TL-FCM: A hierarchical prediction model based on two-level fuzzy c-means clustering for bike-sharing system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning techniques to predict different levels of
hospital care of CoVid-19. <em>APIN</em>, <em>52</em>(6), 6413–6431. (<a
href="https://doi.org/10.1007/s10489-021-02743-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we analyze the capability of several state of the art machine learning methods to predict whether patients diagnosed with CoVid-19 (CoronaVirus disease 2019) will need different levels of hospital care assistance (regular hospital admission or intensive care unit admission), during the course of their illness, using only demographic and clinical data. For this research, a data set of 10,454 patients from 14 hospitals in Galicia (Spain) was used. Each patient is characterized by 833 variables, two of which are age and gender and the other are records of diseases or conditions in their medical history. In addition, for each patient, his/her history of hospital or intensive care unit (ICU) admissions due to CoVid-19 is available. This clinical history will serve to label each patient and thus being able to assess the predictions of the model. Our aim is to identify which model delivers the best accuracies for both hospital and ICU admissions only using demographic variables and some structured clinical data, as well as identifying which of those are more relevant in both cases. The results obtained in the experimental study show that the best models are those based on oversampling as a preprocessing phase to balance the distribution of classes. Using these models and all the available features, we achieved an area under the curve (AUC) of 76.1% and 80.4% for predicting the need of hospital and ICU admissions, respectively. Furthermore, feature selection and oversampling techniques were applied and it has been experimentally verified that the relevant variables for the classification are age and gender, since only using these two features the performance of the models is not degraded for the two mentioned prediction problems.},
  archive      = {J_APIN},
  author       = {Hernández-Pereira, Elena and Fontenla-Romero, Oscar and Bolón-Canedo, Verónica and Cancela-Barizo, Brais and Guijarro-Berdiñas, Bertha and Alonso-Betanzos, Amparo},
  doi          = {10.1007/s10489-021-02743-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6413-6431},
  shortjournal = {Appl. Intell.},
  title        = {Machine learning techniques to predict different levels of hospital care of CoVid-19},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A polynomial kernel neural network classifier based on
random sampling and information gain. <em>APIN</em>, <em>52</em>(6),
6398–6412. (<a
href="https://doi.org/10.1007/s10489-021-02762-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a polynomial kernel neural network classifier (PKNNC) based on the random sampling and information gain. Random sampling is used here to generate datasets for the construction of polynomial neurons located in the neural networks, while information gain is used to evaluate the importance of the input variables (viz. dataset features) of each neuron. Both random sampling and information gain stem from the concepts of well-known random forest models. Some traditional neural networks have certain limitations, such as slow convergence speed, easily falling to local optima and difficulty describing the polynomial relation between the input and output. In this regard, a general PKNNC is proposed, and it consists of three parts: the premise, conclusion, and aggregation. The method of designing the PKNNC is summarized as follows. In the premise section, random sampling and information gain are used to obtain multiple subdatasets that are passed to the aggregation part, and the conclusion part uses three types of polynomials. In the aggregation part, the least squares method (LSM) is used to estimate the parameters of polynomials. Furthermore, the particle swarm optimization (PSO) algorithm is exploited here to optimize the PKNNC. The overall optimization of the PKNNC combines structure optimization and parameter optimization. The PKNNC takes advantage of three types of polynomial kernel functions, random sampling techniques and information gain algorithms, which have a good ability to describe the higher-order nonlinear relationships between input and output variables and have high generalization and fast convergence capabilities. To evaluate the effectiveness of the PKNNC, numerical experiments are carried out on two types of data: machine learning data and face data. A comparative study illustrates that the proposed PKNNC leads to better performance than several conventional models.},
  archive      = {J_APIN},
  author       = {Xiao, Yueyue and Huang, Wei and Oh, Sung-Kwun and Zhu, Liehuang},
  doi          = {10.1007/s10489-021-02762-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6398-6412},
  shortjournal = {Appl. Intell.},
  title        = {A polynomial kernel neural network classifier based on random sampling and information gain},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel self-learning weighted fuzzy local information
clustering algorithm integrating local and non-local spatial information
for noise image segmentation. <em>APIN</em>, <em>52</em>(6), 6376–6397.
(<a href="https://doi.org/10.1007/s10489-021-02722-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clustering algorithm (FCM) can be directly used to segment images, it takes no account of the neighborhood information of the current pixel and does not have a robust segmentation noise suppression. Fuzzy Local Information C-means Clustering (FLICM) is a widely used robust segmentation algorithm, which combines spatial information with the membership degree of adjacent pixels. In order to further improve the robustness of FLICM algorithm, non-local information is embedded into FLICM algorithm and a fuzzy C-means clustering algorithm has local and non-local information (FLICMLNLI) is obtained. When calculating distance from pixel to cluster center, FLICMLNLI algorithm considers two distances from current pixel and its neighborhood pixels to cluster center. However, the algorithm gives the same weight to two different distances, which incorrectly magnifies the importance of neighborhood information in calculating the distance, resulting in unsatisfactory image segmentation effects and loss of image details. In order to solve this problem, we raise an improved self-learning weighted fuzzy algorithm, which directly obtains different weights in distance calculation through continuous iterative self-learning, then the distance metric with the weights obtained from self-learning is embedded in the objective function of the fuzzy clustering algorithm in order to improve the segmentation performance and robustness of the algorithm. A large number of experiments on different types of images show that the algorithm can not only suppress the noise but also retain the details in the image, the effect of segmenting complex noise images is better, and it provides better image segmentation results than the existing latest fuzzy clustering algorithms.},
  archive      = {J_APIN},
  author       = {Song, Qiuyu and Wu, Chengmao and Tian, Xiaoping and Song, Yue and Guo, Xiaokang},
  doi          = {10.1007/s10489-021-02722-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6376-6397},
  shortjournal = {Appl. Intell.},
  title        = {A novel self-learning weighted fuzzy local information clustering algorithm integrating local and non-local spatial information for noise image segmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight network architecture using difference saliency
maps for facial action unit detection. <em>APIN</em>, <em>52</em>(6),
6354–6375. (<a
href="https://doi.org/10.1007/s10489-021-02755-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action unit (AU) detection has been applied in a wild range of fields, and has attracted great attention over the last decades. Most existing methods employ the predefined regions of interest with same number and range for all samples. However, we find that the flexibility of predefined regions of interest is finite, as the occurrence of different AUs may not be simultaneous and their ranges change with intensity changes. In addition, many AU detection works try to independently design feature extraction modules and classifiers for each AU, which is of high computation cost and ignores the dependency among different AUs. In view of the limited flexibility of predefined regions of interest, we propose difference saliency maps that do not depend on facial landmarks. They are the spatial pixel-wise attentions, where each element represents the importance of the corresponding pixel on the entire image. Therefore, all the regions of interest can be irregular. In addition, in order to solve the problem of high computation cost, we combine group convolution with skip connection to propose a lightweight network that is more suitable for AU detection. All AUs share features and there is only one classifier, so the computation cost and the number of parameters are greatly reduced. In particular, the difference saliency maps and the global feature maps are combined to obtain the regional enhancement features. To maximize the enhancement effect, the down-sampled difference saliency maps are added to multiple blocks of the lightweight network. The enhanced global features are directly sent to the classifier for AU detection. By changing the number of neurons in the classifier, our framework can easily adapt to different datasets. Extensive experimental results show that the proposed framework soundly outperforms the classic deep learning method when evaluated on the DISFA+ and CK+ datasets. After adding the difference saliency maps, the detection result is better than the state-of-the-art AU detection methods. Further experiments demonstrate that our network is more efficient in using parameters, computation complexity and inference time.},
  archive      = {J_APIN},
  author       = {Chen, Jing and Wang, Chenhui and Wang, Kejun and Liu, Meichen},
  doi          = {10.1007/s10489-021-02755-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6354-6375},
  shortjournal = {Appl. Intell.},
  title        = {Lightweight network architecture using difference saliency maps for facial action unit detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation based COVID-19 CT infection
segmentation network. <em>APIN</em>, <em>52</em>(6), 6340–6353. (<a
href="https://doi.org/10.1007/s10489-021-02691-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of infection areas in computed tomography (CT) images has proven to be an effective diagnostic approach for COVID-19. However, due to the limited number of pixel-level annotated medical images, accurate segmentation remains a major challenge. In this paper, we propose an unsupervised domain adaptation based segmentation network to improve the segmentation performance of the infection areas in COVID-19 CT images. In particular, we propose to utilize the synthetic data and limited unlabeled real COVID-19 CT images to jointly train the segmentation network. Furthermore, we develop a novel domain adaptation module, which is used to align the two domains and effectively improve the segmentation network’s generalization capability to the real domain. Besides, we propose an unsupervised adversarial training scheme, which encourages the segmentation network to learn the domain-invariant feature, so that the robust feature can be used for segmentation. Experimental results demonstrate that our method can achieve state-of-the-art segmentation performance on COVID-19 CT images.},
  archive      = {J_APIN},
  author       = {Chen, Han and Jiang, Yifan and Loew, Murray and Ko, Hanseok},
  doi          = {10.1007/s10489-021-02691-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6340-6353},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised domain adaptation based COVID-19 CT infection segmentation network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The identity-level angular triplet loss for cross-age face
recognition. <em>APIN</em>, <em>52</em>(6), 6330–6339. (<a
href="https://doi.org/10.1007/s10489-021-02742-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite promising progress has been achieved on face recognition problems, cross-age face recognition remains a challenging task due to its age variations. Human appearances change along with the age growing process, which increases the difficulty of recognition tasks. Existing methods mainly focus on synthesizing new facial images according to different age levels or isolating age-related features and identity related features. In this paper, we propose an identity-level angular triplet loss for cross-age face recognition. The facial images are projected to an embedding space where the angle between feature embeddings can represent similarities of images. Different from Euclidean distance metric, the angular metric used in our method guides the model to learn discriminative features under large intra-class discrepancy. Angles between intra-class embeddings are reduced while that between inter-class are enlarged. The selection of good triplets is conducted on an identity-level rather than instance-level with a moderate positive mining strategy. Experiments are conducted on cross-age databases and results prove the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Chen, Xiaoyu and Lau, Henry Y. K.},
  doi          = {10.1007/s10489-021-02742-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6330-6339},
  shortjournal = {Appl. Intell.},
  title        = {The identity-level angular triplet loss for cross-age face recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WDIBS: Wasserstein deterministic information bottleneck for
state abstraction to balance state-compression and performance.
<em>APIN</em>, <em>52</em>(6), 6316–6329. (<a
href="https://doi.org/10.1007/s10489-021-02787-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important branch of reinforcement learning, Apprenticeship learning studies how an agent learns good behavioral decisions by observing an expert policy from the environment. It has made many encouraging breakthroughs in real-world applications. State abstraction is typically used to compress the state space of the environment to eliminate redundant information, thereby improving learning efficiency. However, excessive compression results in poor decision performance. Therefore, it is important to balance the compression degree and decision performance. Deterministic Information Bottleneck for State abstraction (DIBS) attempts to solve this problem. Specifically, DIBS uses the information rate to represent the compression degree at first. Then, decision performance after compression is measured using the Kullback-Leibler (KL) divergence of distributions between the policy after state compression and the expert policy. However, if the two distributions do not have exactly overlapping support sets, then the KL divergence is usually infinity, which leads to poor decision performance under the low information rate. In this paper, we propose the Wasserstein DIBS (WDIBS) algorithm to optimize the trade-off between the compression degree and decision performance. Specifically, we use the Wasserstein distance to calculate the difference of the distributions between the policy after state compression and the expert policy. Even if the two distributions do not have precisely overlapping support sets, the Wasserstein distance can still reflect their actual difference, thereby ensuring that WDIBS has good decision performance under the low information rate. Theoretical analyses and experiments demonstrate that our method provides a better trade-off between the compression degree and decision performance than DIBS.},
  archive      = {J_APIN},
  author       = {Zhu, Xianchao and Huang, Tianyi and Zhang, Ruiyuan and Zhu, William},
  doi          = {10.1007/s10489-021-02787-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6316-6329},
  shortjournal = {Appl. Intell.},
  title        = {WDIBS: Wasserstein deterministic information bottleneck for state abstraction to balance state-compression and performance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent neighbor selection for efficient query routing
in unstructured P2P networks using q-learning. <em>APIN</em>,
<em>52</em>(6), 6306–6315. (<a
href="https://doi.org/10.1007/s10489-021-02793-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last two decades, peer-to-peer systems have proven their vital role in sharing various resources and services to diverse user communities over the internet. The unstructured P2P network is the most popular topology, and the resources are fully distributed among participating peers. Therefore, searching is a challenging issue due to the absence of control over resource locations. Intelligent decisions should be made to select a particular number of neighbors that can hold relevant resources for queries instead of selecting neighbors randomly. In this paper, an intelligent neighbor selection (INS) algorithm is introduced that uses a reinforcement learning approach, ‘Q-learning’. The main objective of this algorithm is to achieve better retrieval effectiveness with reduced searching costs by fewer connected peers, exchanged messages, and less time. To achieve this, INS relies on Q-learning, which makes a Q-table in each peer and stores the Q-values gathered from the results of previously sent queries, and uses them for the forthcoming queries. The cold start issue during the training phase is also addressed in this research, which allows INS to improve its results continuously. The simulation results show a significant improvement in searching for a resource with compression to controlled flooding and learning processes after sufficient training. Here, retrieval effectiveness, search cost in terms of connected peers, and average overhead are 1.23, 104, 167, respectively.},
  archive      = {J_APIN},
  author       = {Shoab, Mohammad and Jubayrin, Saad Al},
  doi          = {10.1007/s10489-021-02793-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6306-6315},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent neighbor selection for efficient query routing in unstructured P2P networks using Q-learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference during reading: Multi-label classification for
text with continuous semantic units. <em>APIN</em>, <em>52</em>(6),
6292–6305. (<a
href="https://doi.org/10.1007/s10489-021-02778-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the developing of electronic platform such as education, commerce and etc., text with continuous semantic units (CSU-text) emerges in large numbers. Each CSU-text is usually short but contains series of independent semantic units. Mining CSU-text is helpful to determine users’ preferences or intentions, and further improve service quality in real life. Even though there are lots of text mining techniques, they are hard to well handle CSU-text because they usually learn one representation for a whole document. In this case, the information hidden in various semantic units can not be sufficiently captured. Inspired by how a human being understands a text and acquires knowledge in cognitive science, in this paper, we treat multi-label classification for CSU-text as a sequence tagging task and propose a novel inference during reading (InfDR) model. The model is able to simultaneously partition continuous semantic units and map them to semantic labels. Extensive experiments are conducted on three real-world datasets, demonstrating that the proposed model is effective and significantly outperforms the existing baselines with one single text representation.},
  archive      = {J_APIN},
  author       = {Tian, Xuetao and Jing, Liping and Luo, Fang and Liu, Feng},
  doi          = {10.1007/s10489-021-02778-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6292-6305},
  shortjournal = {Appl. Intell.},
  title        = {Inference during reading: Multi-label classification for text with continuous semantic units},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy clustering algorithm for outlier-interval data based
on the robust exponent distance. <em>APIN</em>, <em>52</em>(6),
6276–6291. (<a
href="https://doi.org/10.1007/s10489-021-02773-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outlier elements of a data are ones that differs significantly from others. For many reasons, we have to face with outlier elements in data analysis for the different fields. Because an outlier element can cause the serious problems in statistical analyses, studying about it is interested in many researchers. This article proposes the fuzzy clustering algorithm for outlier - interval data based on the robust exponent distance to overcome the drawback of traditional clustering algorithm which to clean the outliers before performing. The outstanding advantage of this algorithm is to find the suitable number of clusters, to cluster for the interval data with outlier elements, and to determine the probability belonging to clusters for the intervals at the same time. The proposed algorithm is described step by step via numerical examples, and can be performed effectively by the Matlab procedure. In addition, it also applied in reality with the air pollution, mushroom, and image data sets. These real applications demonstrate the robustness of the proposed algorithm in comparison with the existing ones.},
  archive      = {J_APIN},
  author       = {Phamtoan, Dinh and Nguyenhuu, Khanh and Vovan, Tai},
  doi          = {10.1007/s10489-021-02773-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6276-6291},
  shortjournal = {Appl. Intell.},
  title        = {Fuzzy clustering algorithm for outlier-interval data based on the robust exponent distance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial example detection based on saliency map
features. <em>APIN</em>, <em>52</em>(6), 6262–6275. (<a
href="https://doi.org/10.1007/s10489-021-02759-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning has greatly improved image recognition capability. However, studies have shown that neural network models are vulnerable to adversarial examples that make models output wrong answers with high confidence. To understand the vulnerabilities of models, we use interpretability methods to reveal the internal decision-making behaviors of models. Interpretation results reflect that the evolutionary process of nonnormalized saliency maps between clean and adversarial examples are increasingly differentiated along model hidden layers. By taking advantage of this phenomenon, we propose an adversarial example detection method based on multilayer saliency features, which can comprehensively capture the abnormal characteristics of adversarial example interpretations. Experimental results show that the proposed method can effectively detect adversarial examples based on gradient, optimization and black-box attacks, and it is comparable with the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Shen and Gong, Yuxin},
  doi          = {10.1007/s10489-021-02759-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6262-6275},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial example detection based on saliency map features},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable neighborhood search for a planning problem with
resource constraints in a health simulation center. <em>APIN</em>,
<em>52</em>(6), 6245–6261. (<a
href="https://doi.org/10.1007/s10489-021-02730-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose the Variable Neighborhood Search (VNS) algorithm SimULS to solve a planning problem in the Health Simulation Center SimUSanté. This center offers numerous training sessions based on simulation learning for health actors, be they professionals or students. The data and constraints of the SimUSanté problem, close to the academic Curriculum-Based Courses Timetabling (CB-CTT) Problem, are presented in detail using a 0-1 linear program modelization. A dedicated greedy algorithm SimUG is used to generate a relevant initial solution in the VNS algorithm. SimULS combines different neighborhood functions stemmed from operators saturator, intra, extra and extra +. A diversification function is applied when the search becomes trapped by a local optimum. First, SimULS was compared to the open source KHE solver by relaxing the precedence constraints. Next, SimULS was tested on all the generated SimUSanté instances. Both experiments show that the strength of SimULS is to schedule all the activities, even for the largest instances, without violating any hard constraints. In addition, the solutions given by SimULS are close to the optimum with a gap less than 7.33%.},
  archive      = {J_APIN},
  author       = {Caillard, Simon and Devendeville, Laure Brisoux and Lucet, Corinne},
  doi          = {10.1007/s10489-021-02730-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6245-6261},
  shortjournal = {Appl. Intell.},
  title        = {Variable neighborhood search for a planning problem with resource constraints in a health simulation center},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-intrusive load monitoring method with inception
structured CNN. <em>APIN</em>, <em>52</em>(6), 6227–6244. (<a
href="https://doi.org/10.1007/s10489-021-02690-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-intrusive load monitoring (NILM) is an important part of smart grid, which can recognize home electrical appliances. Compared to traditional statistical manners, deep learning can extremely increase the recognition accuracy by more than 10%. However, most NILM methods based on neural networks try to deepen the network to extend the feature extraction capability, which will cause the overfitting and gradient to disappeare in NILM. This paper focuses on solving this problem by optimizing the disaggregation process, and proposes a method based on multiple overlapping sliding windows combined with the inception structure of CNN to disaggregate highly mixed loads of multiple appliances, which can stack each layer disorderly and run each process in parallel, without deepening the depth. Firstly, this work designed a multiple overlap sliding window for NILM to segment the sequence data. Then, the improved inception structure of CNN is used to extract the features, which can provide a rewarding feature extraction capability. After that, the same multiple overlap window is used to smooth the extracted feature of sequence data base on the average filtering. Moreover, this paper makes a comparative analysis of different slide step sizes, which can be concluded that the recognition accuracy is higher when the slide step is shorter. Highly mixed experimental data of multiple appliances is used to test the method. The results highlight the disaggregation performance of the proposed model in the high mixing of multiple electrical load data.},
  archive      = {J_APIN},
  author       = {Ding, Dong and Li, Junhuai and Zhang, Kuo and Wang, Huaijun and Wang, Kan and Cao, Ting},
  doi          = {10.1007/s10489-021-02690-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6227-6244},
  shortjournal = {Appl. Intell.},
  title        = {Non-intrusive load monitoring method with inception structured CNN},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global contextual guided residual attention network for
salient object detection. <em>APIN</em>, <em>52</em>(6), 6208–6226. (<a
href="https://doi.org/10.1007/s10489-021-02713-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-level semantic features and low-level detail features matter for salient object detection in fully convolutional neural networks (FCNs). Further integration of low-level and high-level features increases the ability to map salient object features. In addition, different channels in the same feature are not of equal importance to saliency detection. In this paper, we propose a residual attention learning strategy and a multistage refinement mechanism to gradually refine the coarse prediction in a scale-by-scale manner. First, a global information complementary (GIC) module is designed by integrating low-level detailed features and high-level semantic features. Second, to extract multiscale features of the same layer, a multiscale parallel convolutional (MPC) module is employed. Afterwards, we present a residual attention mechanism module (RAM) to receive the feature maps of adjacent stages, which are from the hybrid feature cascaded aggregation (HFCA) module. The HFCA aims to enhance feature maps, which reduce the loss of spatial details and the impact of varying the shape, scale and position of the object. Finally, we adopt multiscale cross-entropy loss to guide network learning salient features. Experimental results on six benchmark datasets demonstrate that the proposed method significantly outperforms 15 state-of-the-art methods under various evaluation metrics.},
  archive      = {J_APIN},
  author       = {Wang, Jun and Zhao, Zhengyun and Yang, Shangqin and Chai, Xiuli and Zhang, Wanjun and Zhang, Miaohui},
  doi          = {10.1007/s10489-021-02713-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6208-6226},
  shortjournal = {Appl. Intell.},
  title        = {Global contextual guided residual attention network for salient object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized recommendation with knowledge graph via
dual-autoencoder. <em>APIN</em>, <em>52</em>(6), 6196–6207. (<a
href="https://doi.org/10.1007/s10489-021-02647-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, personalized recommendation systems have attracted a vast amount of attention and researches from multiple disciplines. Recently, for the powerful ability of feature representation learning, deep neural networks have achieved sound performance in the recommendation. However, most of the existing deep recommendation approaches require a large number of labeled data, which is often expensive and labor-some in applications. Meanwhile, the side information of users and items that can extend the feature space effectively is usually scarce. To address these problems, we propose a Personalized Recommendation method, which extends items’ feature representations with Knowledge Graph via dual-autoencoder (short for PRKG). More specifically, we first extract items’ side information from open knowledge graph like DBpedia as items’ feature extension. Secondly, we learn the low-dimensional representations of additional features collected from DBpedia via the autoencoder module and then integrate the processed features into the original feature space. Finally, the reconstructed features is incorporated into the semi-autoencoder for personalized recommendations. Extensive experiments conducted on several real-world datasets validate the effectiveness of our proposed methods compared to several state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Yang, Yang and Zhu, Yi and Li, Yun},
  doi          = {10.1007/s10489-021-02647-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6196-6207},
  shortjournal = {Appl. Intell.},
  title        = {Personalized recommendation with knowledge graph via dual-autoencoder},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale fusion light CNN for hyperspectral face recognition
with knowledge distillation and attention mechanism. <em>APIN</em>,
<em>52</em>(6), 6181–6195. (<a
href="https://doi.org/10.1007/s10489-021-02721-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging technology, combining traditional imaging and spectroscopy technologies to simultaneously acquire spatial and spectral information, is deemed to be an intuitive medium for robust face recognition. However, the intrinsic structure of hyperspectral images is more complicated than ordinary gray-scale or RGB images, how to fully explore the discriminant and correlation features with only a limited number of hyperspectral samples for deep learning training has not been well studied. In response to these problems, this paper proposes an end-to-end multiscale fusion lightweight convolution neural network (CNN) framework for hyperspectral face recognition, termed as the features fusion with channel attention network (FFANet). Firstly, to capture richer subtle details, we introduce Second-Order Efficient Channel Attention (SECA) as the variant of Efficient Channel Attention (ECA) into the framework. The difference from ECA is that SECA can extract the second-order information of each channel to improve the network’s feature extraction ability and is more suitable for the complexity of hyperspectral data. Secondly, we further fuse multiscale information to yield a comprehensive and discriminative representation learning. Finally, the joint of Self-Supervision and Knowledge Distillation (SSKD) is exploited to train an efficient deep model, which can learn more dark knowledge from the trained teacher network. The experimental results on three benchmark hyperspectral face databases of PolyU, CMU, and UWA show that the proposed approach has achieved competitive accuracy and efficiency on the basis of significantly reducing the storage space and computation overheads. These characteristics also show its wide applicability on edge/mobile devices.},
  archive      = {J_APIN},
  author       = {Niu, Jie-Yi and Xie, Zhi-Hua and Li, Yi and Cheng, Si-Jia and Fan, Jia-Wei},
  doi          = {10.1007/s10489-021-02721-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6181-6195},
  shortjournal = {Appl. Intell.},
  title        = {Scale fusion light CNN for hyperspectral face recognition with knowledge distillation and attention mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware pyramid attention network for crowd counting.
<em>APIN</em>, <em>52</em>(6), 6164–6180. (<a
href="https://doi.org/10.1007/s10489-021-02639-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving accurate crowd counting still faces many challenges due to continuous scale variations. To this end, we present an innovative Context-Aware Pyramid Attention Network for crowd counting which is realized by extracting rich contextual features and dealing with dependencies on space and channels. To extract rich contextual features, we propose a context-aware pyramid feature extraction module. With this module, the rich contextual feature extraction is implemented by dividing the input features into four blocks with different scales. In addition, we design an attention module consisting of a space attention block and a channel attention block which are responsible for dealing with the interdependence on feature information in the spatial dimension and the channel dimension, respectively. We first perform an ablation study to evaluate the effectiveness of each component on ShanghaiTech Part_A. Then we conduct several comparative experiments with several state-of-art methods on five challenging crowd counting datasets, including the ShanghaiTech, the UCF_CC_50, the UCF-QNRF, the WorldExpo’10, and NWPU-Crowd datasets. Experimental results demonstrate that the mean absolute error and mean square error of CAPAN are significantly reduced compared to the state-of-art methods in counting accuracy, and at the same time, the quality of density maps and the false recognition rate are improved. These results validate the effectiveness of CAPAN in crowd counting.},
  archive      = {J_APIN},
  author       = {Gu, Lingyu and Pang, Chen and Zheng, Yanjun and Lyu, Chen and Lyu, Lei},
  doi          = {10.1007/s10489-021-02639-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6164-6180},
  shortjournal = {Appl. Intell.},
  title        = {Context-aware pyramid attention network for crowd counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot contrastive learning for image classification and
its application to insulator identification. <em>APIN</em>,
<em>52</em>(6), 6148–6163. (<a
href="https://doi.org/10.1007/s10489-021-02769-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel discriminative Few-shot learning architecture based on batch compact loss. Currently, Convolutional Neural Network (CNN) has achieved reasonably good performance in image recognition. Most existing CNN methods facilitate classifiers to learn discriminating patterns to identify existing categories trained with large samples. However, learning to recognize novel categories from a few examples is a challenging task. To address this, we propose the Residual Compact Network to train a deep neural network to learn hierarchical nonlinear transformations to project image pairs into the same latent feature space, under which the distance of each positive pair is reduced. To better use the commonality of class-level features for category recognition, we develop a batch compact loss to form robust feature representations relevant to a category. The proposed methods are evaluated on several datasets. Experimental evaluations show that our proposed method achieves acceptable results in Few-shot learning.},
  archive      = {J_APIN},
  author       = {Li, Liang and Jin, Weidong and Huang, Yingkun},
  doi          = {10.1007/s10489-021-02769-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6148-6163},
  shortjournal = {Appl. Intell.},
  title        = {Few-shot contrastive learning for image classification and its application to insulator identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Distractor-aware visual tracking using hierarchical
correlation filters adaptive selection. <em>APIN</em>, <em>52</em>(6),
6129–6147. (<a
href="https://doi.org/10.1007/s10489-021-02694-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the ensembled trackers composed of multi-level features from the pre-trained Convolutional Neural Network (CNN) have achieved top performance in visual tracking. However, due to the background clutters and the distractors in the search area, the tracker tends to drift towards an area that is similar to the target. In order to suppress interference of background and similar objects, we propose an effective Distractor-Aware Map (DAM), which can reduce the weights of the interference area in the multi-level features. Thus, the tracker can focus on the target to greatly eliminate the risk of drift. In addition, we build a Hierarchical Correlation Filters Model (HCFM) based on the multi-level convolutional features to track targets in parallel. To further improve the robustness of tracking, a novel Multi-Model Adaptive Selection (MAS) mechanism is presented. This mechanism can evaluate the confidence of the response map in HCFM to adaptively select the most reliable model. Finally, in order to appropriately update the model to adapt to appearance changes of the target, we propose an adaptive updating strategy for the updates of the DAM and HCFM. We perform comprehensive experiments on OTB-2013, OTB-2015 and Temple Color datasets and the experimental results show the superiority of our algorithm over other state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Zhang, Jianming and Liu, Yang and Liu, Hehua and Wang, Jin and Zhang, Yudong},
  doi          = {10.1007/s10489-021-02694-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6129-6147},
  shortjournal = {Appl. Intell.},
  title        = {Distractor-aware visual tracking using hierarchical correlation filters adaptive selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Frequent high minimum average utility sequence mining with
constraints in dynamic databases using efficient pruning strategies.
<em>APIN</em>, <em>52</em>(6), 6106–6128. (<a
href="https://doi.org/10.1007/s10489-021-02520-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility sequence mining is a popular data mining task, which aims at finding sequences having a high utility (importance) in a quantitative sequence database. Though it has several applications, state-of-the-art algorithms have one or more of the following limitations: (1) they rely on a utility function that tends to be biased toward finding long patterns, (2) some algorithms do take pattern length into account using an average-utility function but they adopt an optimistic perspective that can be risky or misleading for some applications, (3) they do not let the user specify additional constraints on patterns to be found. To address these three limitations, this paper defines a novel task of mining frequent high minimum average-utility sequences (FHAUS) with constraints in a quantitative sequence database. This task has the following benefits. First, it uses the average-utility au function based on the minimum utility, which takes the length of a pattern into account to calculate its utility. This helps finding short patterns missed by traditional algorithms and it is based on more safe pessimistic utility calculations. Second, the user can specify a set of monotonic and anti-monotonic constraints C on patterns to filter irrelevant patterns and improve the performance of the mining process. To efficiently find all FHAUSs with constraints, this paper first proposes some novel upper bounds (UBs) and weak upper bounds (WUBs) on the average-utility, which satisfy downward-closure (DC) properties or DC-like properties. Then, to effectively reduce the search space, the paper designs novel width pruning, depth pruning, reducing, and tightening strategies based on the proposed bounds. These proposed novel theoretical results are integrated into an algorithm named C-FHAUSPM (Constrained Frequent High minimum Average-Utility Sequential Pattern Mining) for efficiently discovering all FHAUSs with constraints. Results from extensive experiments on both real-life and synthetic quantitative sequence databases show that C-FHAUSPM is highly efficient in terms of runtime and memory usage.},
  archive      = {J_APIN},
  author       = {Truong, Tin and Duong, Hai and Le, Bac and Fournier-Viger, Philippe and Yun, Unil},
  doi          = {10.1007/s10489-021-02520-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6106-6128},
  shortjournal = {Appl. Intell.},
  title        = {Frequent high minimum average utility sequence mining with constraints in dynamic databases using efficient pruning strategies},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetry label correlation for multi-label learning.
<em>APIN</em>, <em>52</em>(6), 6093–6105. (<a
href="https://doi.org/10.1007/s10489-021-02725-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective method for mining latent information between labels, label correlation is widely adopted by many scholars to model multi-label learning algorithms. Most existing multi-label algorithms usually ignore that the correlation between labels may be asymmetric while asymmetry correlation commonly exists in the real-world scenario. To tackle this problem, a multi-label learning algorithm with asymmetry label correlation (ACML, Asymmetry Label Correlation for Multi-Label Learning) is proposed in this paper. First, measure the adjacency between labels to construct the label adjacency matrix. Then, cosine similarity is utilized to construct the label correlation matrix. Finally, we constrain the label correlation matrix with the label adjacency matrix. Thus, asymmetry label correlation is modeled for multi-label learning. Experiments on multiple multi-label benchmark datasets show that the ACML algorithm has certain advantages over other comparison algorithms. The results of statistical hypothesis testing further illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Bao, Jiachao and Wang, Yibin and Cheng, Yusheng},
  doi          = {10.1007/s10489-021-02725-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6093-6105},
  shortjournal = {Appl. Intell.},
  title        = {Asymmetry label correlation for multi-label learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Margin attribute reductions for multi-label classification.
<em>APIN</em>, <em>52</em>(6), 6079–6092. (<a
href="https://doi.org/10.1007/s10489-021-02740-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is a typical supervised machine learning problem and widely applied in text classification and image recognition. When there are redundant attributes in the data, the efficiency of classification will be reduced. However, the existing attribute reduction algorithms have high computational complexity. This paper aims to design an efficient attribute reduction algorithm. The k pairs of boundary samples were selected from the positive and negative classes respectively, and the distance between each pair was calculated as the evaluation of attributes. By maximizing the evaluation function, the definition of reduction and the design of the algorithm were established. The comparison experiment is carried out on eight generic multi-label data. The experimental results show that the attribute importance evaluation defined in this paper can better represent the classification performance of the attribute for multi-label classification. The boundary samples can better reflect the classification effect of attributes. The proposed model avoids the point-by-point statistics of all samples’ information and improves the computational efficiency.},
  archive      = {J_APIN},
  author       = {Fan, Xiaodong and Chen, Xiangyue and Wang, Changzhong and Wang, Yang and Zhang, Ying},
  doi          = {10.1007/s10489-021-02740-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6079-6092},
  shortjournal = {Appl. Intell.},
  title        = {Margin attribute reductions for multi-label classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A temporal-based SVM approach for the detection and
identification of pollutant gases in a gas mixture. <em>APIN</em>,
<em>52</em>(6), 6065–6078. (<a
href="https://doi.org/10.1007/s10489-021-02761-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air toxicity and pollution phenomena are on the rise across the planet. Thus, the detection and control of gas pollution are nowadays major economic and environmental challenges. There exists a wide variety of sensors that can detect gas pollution events. However, they are either gas-specific or weak in the presence of gas mixtures. This paper handles this issue by presenting method based on a Temporal-based Support Vector Machine for for the detection and identification of several toxic gases in a gas mixture. The considered gases are carbon monoxide (CO), ozone (O3) and nitrogen dioxide (NO2). Furthermore, an incremental algorithm is proposed in this paper for the selection of the best performing kernel function in terms of accuracy and simplicity of implementation. Then, a decision-making algorithm based on the rate of appearance of a class on a moving window is proposed to improve decision making in presence of uncertainties. This algorithm allows the user to master the false-alarms and no-detection dilemma, and quantify the level of confidence attributed to the decision. Experimental results, obtained with different gas mixtures, show the effectiveness of the proposed approach with 100% of accuracy in the learning and testing stages.},
  archive      = {J_APIN},
  author       = {Djeziri, Mohand A. and Djedidi, Oussama and Morati, Nicolas and Seguin, Jean-Luc and Bendahan, Marc and Contaret, Thierry},
  doi          = {10.1007/s10489-021-02761-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6065-6078},
  shortjournal = {Appl. Intell.},
  title        = {A temporal-based SVM approach for the detection and identification of pollutant gases in a gas mixture},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint model based on interactive gate mechanism for spoken
language understanding. <em>APIN</em>, <em>52</em>(6), 6057–6064. (<a
href="https://doi.org/10.1007/s10489-021-02544-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slot filling and intent detection are two important tasks in a spoken language understanding (SLU) system, it is becoming a tendency that two tasks are jointing learn in SLU. However, many existing model only conduct join model by share parameters on the surface level rather than bi-directional interaction for slot filling and intent detection tasks. In this paper, we designed a dual interaction model based on the gate mechanism. First, We utilize a Dilated Convolutional Neural Networks (DCNN) block with self-attention to better capture the semantic of utterance. Besides, for the two tasks we adopt gate mechanism to get the interaction information of intent and slot, which can control the passing rate and make fully use of semantic relevance between slot filling and intent detection. Finally, the experiments results show that our model has significantly improved in the slot filling F1, intent detection accuracy on the ATIS and SNIPS datasets and overmatch other prior methods.},
  archive      = {J_APIN},
  author       = {Sun, Chengai and Lv, Liangyu and Liu, Tailu and Li, Tangjun},
  doi          = {10.1007/s10489-021-02544-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6057-6064},
  shortjournal = {Appl. Intell.},
  title        = {A joint model based on interactive gate mechanism for spoken language understanding},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network for beam hardening artifacts removal in
image reconstruction. <em>APIN</em>, <em>52</em>(6), 6037–6056. (<a
href="https://doi.org/10.1007/s10489-021-02604-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image reconstruction with limited angles projection data is a challenging task in computed tomography (CT). The amount of radiation associated with CT induces health implications to the patient. Besides, image reconstruction with limited-angles projection data distorts the image, thus emasculating the efficiency of diagnosis. Also, the poly-chromatic nature of the X-ray adds beam-hardening artifacts in the reconstruction. The state-of-the-art approaches available in the literature have proposed the solutions for beam-hardening artifacts correction in full span computed tomography. Most of the solutions are hardware based and need extra hardware to remove the beam hardening artifacts. The present manuscript proposes artificial intelligence based software solution for the beam hardening artifacts removal. This manuscript has presented a cascaded encoder-decoder architecture named cascaded deep neural network for image reconstruction (CDNN). The CDNN architecture has convolution neural network blocks that include convolution layers, rectified linear units ReLU, and batch normalization layers. The network has skip-connections for better learning of features between input and output. The network has been designed as a forward model. The stochastic gradient descent optimization method has been used for training the network. Image reconstructed from Fourier transform-based approach has been used as a prior. A novel approach for reduction of beam-hardening artifacts in case of limited-angles computed tomography using CDNN has been presented. The proposed approach is comparable to other hardware/software solutions for aforesaid purpose and does not require any extra hardware. The proposed approach has improved the image quality as compared to U-Net and the other state-of-the-art methods. It has been found from the experiments that the CDNN suppresses the artifacts and improves the reconstruction. The performance of the proposed CDNN has been tested with real-life data having beam hardening artifacts. It has been observed that the CDNN has improved the reconstruction quality by reducing streak, ring artifacts, and beam hardening artifacts and also preserving the profound structures.},
  archive      = {J_APIN},
  author       = {Kalare, Kailash and Bajpai, Manish and Sarkar, Shubhabrata and Munshi, Prabhat},
  doi          = {10.1007/s10489-021-02604-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6037-6056},
  shortjournal = {Appl. Intell.},
  title        = {Deep neural network for beam hardening artifacts removal in image reconstruction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust unsupervised anomaly detection framework.
<em>APIN</em>, <em>52</em>(6), 6022–6036. (<a
href="https://doi.org/10.1007/s10489-021-02736-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection plays an essential role in monitoring dependable systems and networks such as computer clusters, water treatment systems, sensor networks, etc. However, anomaly detection nowadays remains a big challenge since previous researches suffer from inaccessible anomaly labels and inconsistent data types. Therefore, we propose a robust unsupervised anomaly detection framework (RUAD) to tackle the above problems. RUAD combines a deep AutoEncoder and a robust layer to extract the latent representations of data and separate normal data from abnormal data respectively, then utilizes Gaussian Mixture Model (GMM) to learn the distribution of normal data. In addition, our model can adapt to different types of data by simply modifying the structure of the deep AutoEncoder. Extensive experiments show that RUAD outperforms state-of-art anomaly detection techniques.},
  archive      = {J_APIN},
  author       = {Luo, Zhengyu and He, Kejing and Yu, Zhixing},
  doi          = {10.1007/s10489-021-02736-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6022-6036},
  shortjournal = {Appl. Intell.},
  title        = {A robust unsupervised anomaly detection framework},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A novel personalized recommendation algorithm by exploiting
individual trust and item’s similarities. <em>APIN</em>, <em>52</em>(6),
6007–6021. (<a
href="https://doi.org/10.1007/s10489-021-02655-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, incorporating user information, social network information, item information and user ratings to improve recommendation performance has attracted great attention. However, most of the existing recommendation methods suffer from the following problems: (1) They only use user information, the item’s information, or user ratings to make recommendations, thus, they demonstrate low recommendation accuracy. (2) They employ decision trees to execute user information and item information, therefore, they ignore the correlation between attributes, resulting in a decrease in recommendation performance. (3) It is difficult to cope with the problem of data sparsity. To address these problems, we propose a novel personalized recommendation algorithm by exploiting the individual trust and item’s similarities, named PRAITIS. In PRAITIS framework, the zero-mean spherical Gaussian prior is first applied to the item feature vector, user feature vector, and user-rating-data matrix to obtain their latent feature space and the user-rating-data space. Then, Bayesian inference is used to get the posterior probability of potential features. Finally, under the condition that the hyper-parameters are fixed, the framework of the algorithm is obtained by maximizing the log-posterior probability of three potential features. In order to verify the effectiveness of the proposed algorithm, a series of experiments done on two real-world datasets (e.g., Douban and Epinions) show that the proposed algorithm is superior to the state-of-the-art recommendation algorithms in terms of recommendation accuracy and quality.},
  archive      = {J_APIN},
  author       = {Liu, Taiheng and He, Zhaoshui},
  doi          = {10.1007/s10489-021-02655-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {6007-6021},
  shortjournal = {Appl. Intell.},
  title        = {A novel personalized recommendation algorithm by exploiting individual trust and item’s similarities},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved henry gas solubility optimization for
optimization tasks. <em>APIN</em>, <em>52</em>(6), 5966–6006. (<a
href="https://doi.org/10.1007/s10489-021-02670-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The henry gas solubility optimization (HGSO) is a new nature-inspired algorithm that mimics Henry Gas Solubility to solve global optimization problems. The main changes of premature convergence and poor balance between exploration and exploitation persist, which cannot yet do well in solving some complex optimization problems. To solve the above problems and get better performance, and improved henry gas solubility optimization with dynamic opposite learning, sine cosine factor, conversion probability and interval contraction strategy is proposed in this paper. Firstly, to increase population diversity, using the asymmetry of the dynamic-opposite learning search space to enable individuals to traverse the entire solution space as much as possible. Secondly, change the position update method of henry gas solubility optimization and combine the sine and cosine strategies to better balance the exploration and exploitation of the algorithm. Thirdly, the interval shrinking strategy makes the algorithm better approach the optimal solution and accelerates the algorithm convergence. Finally, the well-known CEC2017 benchmark functions and three real-world engineering design problems were employed to demonstrate the performance of our algorithm. The diversity of algorithms and the coordination of different strategies are analyzed. The experimental results and statistical analyses show that the performance of our algorithm is better than the comparison algorithms.},
  archive      = {J_APIN},
  author       = {Bi, Jie and Zhang, Yong},
  doi          = {10.1007/s10489-021-02670-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5966-6006},
  shortjournal = {Appl. Intell.},
  title        = {An improved henry gas solubility optimization for optimization tasks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved bagging ensemble surrogate-assisted evolutionary
algorithm for expensive many-objective optimization. <em>APIN</em>,
<em>52</em>(6), 5949–5965. (<a
href="https://doi.org/10.1007/s10489-021-02709-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the surrogate-assisted evolutionary algorithm is used to solve expensive many-objective optimization problems, the surrogate is used to approximate the expensive fitness functions. However, with the increase of the number of objectives, the approximate error of the surrogate will accumulate gradually, and the computational cost will also increase sharply. This paper proposes an improved bagging ensemble surrogate-assisted evolutionary algorithm (IBE-CSEA) to solve these problems. An ensemble classifier is used to classify the offspring instead of building the surrogate to approximate the fitness function of each objective. Firstly, a group of classification boundary individuals are selected one by one from the individuals evaluated by the expensive fitness function. All the individuals evaluated by the expensive fitness function are divided into two categories; Secondly, the individuals in these two categories are divided into the training set and the test set. The training set is used to train an improved bagging ensemble classifier. The test set is used to calculate the reliability of the classification; Finally, the classification results and the reliability are used to select the promising individuals for expensive fitness function evaluation. Compared with the current popular surrogate-assisted evolutionary algorithm, IBE-CSEA algorithm is more competitive.},
  archive      = {J_APIN},
  author       = {Gu, Qinghua and Zhang, Xiaoyue and Chen, Lu and Xiong, Naixue},
  doi          = {10.1007/s10489-021-02709-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5949-5965},
  shortjournal = {Appl. Intell.},
  title        = {An improved bagging ensemble surrogate-assisted evolutionary algorithm for expensive many-objective optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Support vector machine classification over encrypted data.
<em>APIN</em>, <em>52</em>(6), 5938–5948. (<a
href="https://doi.org/10.1007/s10489-021-02727-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine is one of the most extensively used machine learning algorithms. A typical application scenario of support vector machine classification is the client-server model where a client holding an image requests a server holding a support vector machine model to provide image classification services. However, applying support vector machine in the client-server model requires careful attention to maintain data privacy. In this paper, we will propose a privacy-preserving support vector machine classification scheme based on fully homomorphic encryption. Our scheme guarantees that the server cannot learn the user’s image while providing classification service and the client eventually obtains the classification result without learning about the support vector machine model. We introduce several novel techniques to optimize our proposal. Specifically, we use the single instruction multiple data techniques to accelerate the linear component and the approximate method to compute the non-linear component. Our scheme significantly improves computational and communication performance compared to the state-of-the-art work. Experimental results show that our scheme takes only seconds to classify an encrypted image while the state-of-the-art work takes several hundred seconds.},
  archive      = {J_APIN},
  author       = {Huang, Hai and Wang, Yongjian and Zong, Haoran},
  doi          = {10.1007/s10489-021-02727-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5938-5948},
  shortjournal = {Appl. Intell.},
  title        = {Support vector machine classification over encrypted data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel spatiotemporal attention enhanced discriminative
network for video salient object detection. <em>APIN</em>,
<em>52</em>(6), 5922–5937. (<a
href="https://doi.org/10.1007/s10489-021-02649-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to image salient object detection, on which many achievements have been made, video salient object detection remains a considerable challenge. Not all features are useful in salient object detection, and some even cause interferences. In this paper, we propose a novel multiscale spatiotemporal ConvLSTM model based on an attention mechanism, which introduces space-based and channel-based attention mechanisms and improves the network’s capability to extract high-level semantic information and low-level spatial structural features. First, to obtain more effective spatiotemporal information, a ConvLSTM module embedded with an attention mechanism (CSAtt-ConvLSTM) is designed at higher layers of the network to weight salient features of the extracted spatiotemporal consistency. Second, a multiscale attention (MSA) module for distinguishing features is designed, which introduces two attention mechanisms: channel-wise attention (CA) units and spatial-wise attention (SA) units. The CA and SA units are used after high-level feature mapping obtained by the CSAtt-ConvLSTM module and shallow feature mapping, respectively, and then their outputs are fused as final output feature maps. A large number of experiments on multiple datasets verified the effectiveness of our proposed model, which reached a real-time speed on a single GPU of 20 fps.},
  archive      = {J_APIN},
  author       = {Liu, Bing and Mu, Kezhou and Xu, Mingzhu and Wang, Fangyuan and Feng, Lei},
  doi          = {10.1007/s10489-021-02649-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5922-5937},
  shortjournal = {Appl. Intell.},
  title        = {A novel spatiotemporal attention enhanced discriminative network for video salient object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data imputation via conditional generative adversarial
network with fuzzy c mean membership based loss term. <em>APIN</em>,
<em>52</em>(6), 5912–5921. (<a
href="https://doi.org/10.1007/s10489-021-02661-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are some missing values in the data when the data is acquired from the sensors or other equipments. This makes it difficult for performing the analysis based on the data. There are two major types of existing methods for performing the data imputation. They are the discriminative methods and the generative methods. However, these methods are incapable for dealing the data either with a high missing rate or with an unacceptable error. This paper proposes an effective method for performing the data imputation. In particular, the conditional generative adversarial network (CGAN) is used to predict the missing data. Here, the enhanced fuzzy c mean algorithm is employed for performing the clustering so that the information on the local samples is exploited in the algorithm. The computer numerical simulations are performed on several real world datasets. Since this CGAN exploits the class of the missing values of the data, it is shown that our proposed method achieves a higher imputation accuracy compared to state of the art methods.},
  archive      = {J_APIN},
  author       = {Wu, Zisheng and Ling, Bingo Wing-Kuen},
  doi          = {10.1007/s10489-021-02661-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5912-5921},
  shortjournal = {Appl. Intell.},
  title        = {Data imputation via conditional generative adversarial network with fuzzy c mean membership based loss term},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual tracking via dynamic saliency discriminative
correlation filter. <em>APIN</em>, <em>52</em>(6), 5897–5911. (<a
href="https://doi.org/10.1007/s10489-021-02260-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discriminative correlation filter (DCF) is one of the crucial visual tracking methods, and it has outstanding performance. Nevertheless, DCF-based methods have an unavoidable boundary effect, which results in poor tracking performance in an abrupt scene, such as fast motion or deformation. To address this problem, we propose a novel dynamic saliency discriminative correlation filter for visual tracking. In our approach, a response guided saliency map is constructed to introduce saliency information into the filter. The method effectively highlights the target by further increasing the number of positive samples to alleviate the boundary effect. We also investigate an effective multifeature integration method to extract the target feature by employing the Felzenszwalb Histograms of Oriented Gradients (fHOG) from each color space. Finally, we apply a novel update approach to prevent filter model degradation, which uses a temporal regularization term to update the filter model. Extensive experiments on the standard OTB-2015 benchmark validate that our approach achieves competitive performance compared to other state-of-the-art trackers. Moreover, we conducted an ablation study to evaluate the effectiveness of the components in our tracker.},
  archive      = {J_APIN},
  author       = {Gao, Lina and Liu, Bing and Fu, Ping and Xu, Mingzhu and Li, Junbao},
  doi          = {10.1007/s10489-021-02260-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {5897-5911},
  shortjournal = {Appl. Intell.},
  title        = {Visual tracking via dynamic saliency discriminative correlation filter},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). K-centroid link: A novel hierarchical clustering linkage
method. <em>APIN</em>, <em>52</em>(5), 5537–5560. (<a
href="https://doi.org/10.1007/s10489-021-02624-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hierarchical clustering, the most important factor is the selection of the linkage method which is the decision of how the distances between clusters will be calculated. It extremely affects not only the clustering quality but also the efficiency of the algorithm. However, the traditional linkage methods do not consider the effect of the objects around cluster centers. Based on this motivation, in this article, we propose a novel linkage method, named k-centroid link, in order to provide a better solution than the traditional linkage methods. In the proposed k-centroid link method, the dissimilarity between two clusters is mainly defined as the average distance between all pairs of k data objects in each cluster, which are the k closest ones to the centroid of each cluster. In the experimental studies, the proposed method was tested on 24 different publicly available benchmark datasets. The results demonstrate that by hierarchical clustering via the k-centroid link method, it is possible to obtain better performance in terms of clustering quality compared to the conventional linkage methods such as single link, complete link, average link, mean link, centroid link, and the Ward method.},
  archive      = {J_APIN},
  author       = {Dogan, Alican and Birant, Derya},
  doi          = {10.1007/s10489-021-02624-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5537-5560},
  shortjournal = {Appl. Intell.},
  title        = {K-centroid link: A novel hierarchical clustering linkage method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mitigating sparsity using bhattacharyya coefficient and
items’ categorical attributes: Improving the performance of
collaborative filtering based recommendation systems. <em>APIN</em>,
<em>52</em>(5), 5513–5536. (<a
href="https://doi.org/10.1007/s10489-021-02462-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering has been the most popular and effective recommendation technique to predict ratings using similar users or items. But in a sparse dataset, due to fewer co-rated items, the traditional similarity measures fail to compute the similarity between a pair of users. This influences the predicted rating negatively, which results in degraded recommendation performance. Similarity calculation using Bhattacharya Coefficient can be a more judicious approach because it works well with few or no co-rated items between a pair of users. However, Bhattacharya Coefficient also fails to compute the similarity between a pair of users when co-rated items are zero and the rating vector of items are disjoint. In this paper, we propose a novel approach to address the limitation of the Bhattacharya Coefficient with improved rating prediction accuracy in collaborative filtering. Instead of using only user ratings, to have more rating prediction accuracy, we use categorical attributes of rated items in findings of k-nearest neighbors. The performance of the proposed approach is evaluated on the collected datasets of MovieLens and LDOS-CoMoDa and compared with recent approaches. The comparative results corroborate the anticipated performance of the proposed approach.},
  archive      = {J_APIN},
  author       = {Singh, Pradeep Kumar and Pramanik, Pijush Kanti Dutta and Choudhury, Prasenjit},
  doi          = {10.1007/s10489-021-02462-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5513-5536},
  shortjournal = {Appl. Intell.},
  title        = {Mitigating sparsity using bhattacharyya coefficient and items’ categorical attributes: Improving the performance of collaborative filtering based recommendation systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Masked face recognition with convolutional neural networks
and local binary patterns. <em>APIN</em>, <em>52</em>(5), 5497–5512. (<a
href="https://doi.org/10.1007/s10489-021-02728-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition is one of the most common biometric authentication methods as its feasibility while convenient use. Recently, the COVID-19 pandemic is dramatically spreading throughout the world, which seriously leads to negative impacts on people’s health and economy. Wearing masks in public settings is an effective way to prevent viruses from spreading. However, masked face recognition is a highly challenging task due to the lack of facial feature information. In this paper, we propose a method that takes advantage of the combination of deep learning and Local Binary Pattern (LBP) features to recognize the masked face by utilizing RetinaFace, a joint extra-supervised and self-supervised multi-task learning face detector that can deal with various scales of faces, as a fast yet effective encoder. In addition, we extract local binary pattern features from masked face’s eye, forehead and eyebow areas and combine them with features learnt from RetinaFace into a unified framework for recognizing masked faces. In addition, we collected a dataset named COMASK20 from 300 subjects at our institution. In the experiment, we compared our proposed system with several state of the art face recognition methods on the published Essex dataset and our self-collected dataset COMASK20. With the recognition results of 87% f1-score on the COMASK20 dataset and 98% f1-score on the Essex dataset, these demonstrated that our proposed system outperforms Dlib and InsightFace, which has shown the effectiveness and suitability of the proposed method. The COMASK20 dataset is available on https://github.com/tuminguyen/COMASK20 for research purposes.},
  archive      = {J_APIN},
  author       = {Vu, Hoai Nam and Nguyen, Mai Huong and Pham, Cuong},
  doi          = {10.1007/s10489-021-02728-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5497-5512},
  shortjournal = {Appl. Intell.},
  title        = {Masked face recognition with convolutional neural networks and local binary patterns},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient method for mining multi-level high utility
itemsets. <em>APIN</em>, <em>52</em>(5), 5475–5496. (<a
href="https://doi.org/10.1007/s10489-021-02681-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility itemset mining (HUIM) is a useful tool for analyzing customer behavior in the field of data mining. HUIM algorithms can discover the most beneficial itemsets in transaction databases, namely the high-utility itemsets (HUIs), in contrast to frequent itemset mining (FIM) algorithms that rely on detecting frequent patterns. Several algorithms have been proposed to effectively carry out this task, but most of them ignore the categorization of items. In many real-world transaction databases, this helpful information about the categories and subcategories of items, represented as a taxonomy, is useful. Therefore, traditional HUIM algorithms can only discover itemsets at the lowest level of abstraction and leave out several important patterns from higher levels. To address this limitation, this work suggests the use of items taxonomy. Besides, to further enhance the performance of the task several effective pruning techniques are also revised and utilized to tighten the search space when considering the taxonomy of items. To accurately find multi-level HUIs from transaction databases enhanced with taxonomy information, a new algorithm called MLHMiner (Multiple-Level HMiner) is proposed, which is an extended version of the HMiner algorithm. We also prove that the pruning techniques of HMiner can be applied in different abstraction levels to efficiently mine multi-level HUIs. It can be seen from the experimental evaluations on several databases (both real and synthetic) that the designed approach is capable of identifying useful patterns from different abstraction levels with high efficiency.},
  archive      = {J_APIN},
  author       = {Tung, N. T. and Nguyen, Loan T. T. and Nguyen, Trinh D. D. and Vo, Bay},
  doi          = {10.1007/s10489-021-02681-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5475-5496},
  shortjournal = {Appl. Intell.},
  title        = {An efficient method for mining multi-level high utility itemsets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection based on mutual information with
correlation coefficient. <em>APIN</em>, <em>52</em>(5), 5457–5474. (<a
href="https://doi.org/10.1007/s10489-021-02524-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important preprocessing process in machine learning. It selects the crucial features by removing irrelevant features or redundant features from the original feature set. Most of feature selection algorithms focus on maximizing relevant information and minimizing redundant information. In order to remove more redundant information in the evaluation criteria, we propose a feature selection based on mutual information with correlation coefficient (CCMI) in this paper. We introduce the correlation coefficient in the paper, and combine the correlation coefficient and mutual information to measure the relationship between different features. We use the absolute value of the correlation coefficient between two different features as the weight of the redundant item denoted by the mutual information in the evaluation standard. In order to select low redundancy features effectively, we also use the principle of minimization in the evaluation criteria. By comparing with 7 popular contrast algorithms in 12 data sets, CCMI has achieved the highest average classification accuracy for two classifiers of SVM and KNN. Experimental results show that our proposed CCMI has better feature classification capability.},
  archive      = {J_APIN},
  author       = {Zhou, Hongfang and Wang, Xiqian and Zhu, Rourou},
  doi          = {10.1007/s10489-021-02524-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5457-5474},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection based on mutual information with correlation coefficient},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coverage hole detection in WSN with force-directed algorithm
and transfer learning. <em>APIN</em>, <em>52</em>(5), 5435–5456. (<a
href="https://doi.org/10.1007/s10489-021-02714-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coverage hole detection is an important research problem in wireless sensor network research community. However, distributed approaches proposed in recent years for coverage hole detection problem have high computational complexity. In this paper, we propose a novel approach for coverage hole detection in wireless sensor networks called FD-TL (Force-directed and Transfer-learning) which is based on layout generation capability of Force-directed Algorithms and image recognition power of Convolutional Neural Network with transfer learning. In contrast to existing approaches, the proposed approach is a pure topology-based approach since FD-TL can detect both triangular and non-triangular coverage holes from a wireless sensor network based on the input network topology without relying on the physical locations of the anchor nodes. In FD-TL, a Force-directed Algorithm is used to generate a series of possible layouts from a given input topology. Next, a Convolutional Neural Network is used to recognize potential coverage holes from the generated layouts. During the training phase, a transfer learning method is used to aid the recognition process. Experimental results show that FD-TL method can achieve 90% sensitivity and 96% specificity for coverage hole detection in wireless sensor networks.},
  archive      = {J_APIN},
  author       = {Lai, Yue-Hui and Cheong, Se-Hang and Zhang, Hui and Si, Yain-Whar},
  doi          = {10.1007/s10489-021-02714-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5435-5456},
  shortjournal = {Appl. Intell.},
  title        = {Coverage hole detection in WSN with force-directed algorithm and transfer learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LabCor: Multi-label classification using a label correction
strategy. <em>APIN</em>, <em>52</em>(5), 5414–5434. (<a
href="https://doi.org/10.1007/s10489-021-02674-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is a branch of machine learning that can effectively reflect real-world problems. Among all the multi-label classification methods, stacked binary relevance (2BR) is a classic approach. Based on 2BR, a series of optimized algorithms have been derived. Although these algorithms have adapted complex optimizing processes and shown remarkable performance, their core concepts are rather similar, mainly involving restructuring the feature spaces of meta-classifiers. Existing research has rarely discussed that the use of inappropriate two-level predictions causes negative impacts on 2BR structures. In this study, we propose a 2BR-based label correction method named LabCor, which focuses on the identification and correction of unreliable two-level predictions. We first discuss the inner mechanism by which the 2BR-based algorithms obtain their two-level outputs and find a marker that can reflect the reliabilities of the samples predictions. Based on the mechanism, we then introduce a graph based output determination method that can use the training samples to generate dimensional decision patterns. The global label count distribution is also used to reflect the goal of classification problems. In the prediction phase, LabCor uses the decision patterns and label count constraints to identify and correct the misclassified labels. According to the evaluation results, the proposed method can effectively reduce the impact of troubling two-level predictions and yield superior or competitive performance versus well-established 2BR-based algorithms.},
  archive      = {J_APIN},
  author       = {Wu, Chengkai and Zhou, Tianshu and Wu, Junya and Tian, Yu and Li, Jingsong},
  doi          = {10.1007/s10489-021-02674-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5414-5434},
  shortjournal = {Appl. Intell.},
  title        = {LabCor: Multi-label classification using a label correction strategy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical direction projection networks for few-shot
learning. <em>APIN</em>, <em>52</em>(5), 5400–5413. (<a
href="https://doi.org/10.1007/s10489-020-02110-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, visual systems perform better than human beings in many classification tasks. However, the scarcity of labelled data is the most critical problem in such visual systems. Few-shot learning is adopted to tackle this problem, wherein a classifier should acquire the ability to identify some class is not in the training data when given only a few examples. In this paper, critical direction projection (CDP) networks are proposed for few-shot learning. Basically, two crucial steps are involved in CDP: The first step is to find the critical directions for each category in the embedding space, and the second step is to measure the similarity between samples and critical directions according to the projection length. It emerges that CDP networks can be effectively compatible with existing classification networks and achieve state-of-the-art performance on several benchmark datasets. Moreover, CDP achieves outstanding performance both on 2D image and 3D object classification. This study is a new attempt to achieve 3D object classification in a few-shot learning scenario. To summarize, our major research contributions are as follows: 1) a novel metric learning method, CDP, is proposed; 2) a new feature extraction module, EffNet, is introduced; and 3) a benchmark for few-shot 3D object classification is provided.},
  archive      = {J_APIN},
  author       = {Bi, Sheng and Wang, Yongxing and Li, Xiaoxiao and Dong, Min and Zhu, Jinhui},
  doi          = {10.1007/s10489-020-02110-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5400-5413},
  shortjournal = {Appl. Intell.},
  title        = {Critical direction projection networks for few-shot learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of human lower limb mechanical axis key points and
its application on patella misalignment detection. <em>APIN</em>,
<em>52</em>(5), 5385–5399. (<a
href="https://doi.org/10.1007/s10489-021-02718-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human lower limb mechanical axis is the most basic and essential diagnosis reference in clinical orthopedics. Orthopedists diagnose the varus or valgus knee according to the status of the lower limb mechanical axis. The conventional method used in this task relies on manual measurement, which is time-consuming and has operational differences. Given the above reason, in this work, we focus on designing a deep learning algorithm to address this problem and present a novel convolutional neural network architecture for mechanical axis detection. After the mechanical axis is detected, HKAA (Hip-Knee-Ankle Angle), which is a medical index, can be calculated automatically to assist in the medical diagnosis. We locate the mechanical axis by detecting both ends’ key points. Then we apply the detected key points to implement the patella misalignment detection for auxiliary radiography imaging. The mechanical axis key points detection network is based on the stacked hourglass module and adopts the deformable convolution for modeling the geometric features. Besides, we introduce an offset branch to reduce the systematic error. Then a detector trained in a semi-supervised strategy is applied for patella detection. The horizontal deviation of the patella from the knee center reflects the alignment of the patella. We use 879 collected radiographs (X-ray images) to train the key point detection model and other 98 radiographs perform as the validation set in this study. The proposed model achieves an accuracy of 83.0% for key points and reaches 61.1 mAP in patella detection. This model achieves excellent performance in human lower limb mechanical axis and patella detection.},
  archive      = {J_APIN},
  author       = {Zhang, Yueming and Zhang, Guoshan and Guan, Bin and Yao, Jinkun},
  doi          = {10.1007/s10489-021-02718-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5385-5399},
  shortjournal = {Appl. Intell.},
  title        = {Detection of human lower limb mechanical axis key points and its application on patella misalignment detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid CNN-LSTM deep learning model and ensemble technique
for automatic detection of myocardial infarction using big ECG data.
<em>APIN</em>, <em>52</em>(5), 5366–5384. (<a
href="https://doi.org/10.1007/s10489-021-02696-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic and accurate prognosis of myocardial infarction (MI) using electrocardiogram (ECG) signals is a challenging task for the diagnosis and treatment of heart diseases. MI is also referred as a “Heart Attack”, which is the most fatal cardiovascular disease. In many cases, MI does not show any symptoms, hence it is also called a “silent heart attack”. In such cases, patients do not get time to prepare themselves. Hence this disease is more dangerous and fatal with a high mortality rate. Hence, we have proposed an automated detection system of MI using electrocardiogram (ECG) signals by a convolutional neural network (CNN), hybrid CNN- long short-term memory network (LSTM), and ensemble technique to choose the optimum performing model. In this work, we have used 123,998 ECG beats obtained from the “PTB diagnostic database (PTBDB)” and “MIT-BIH arrhythmia database (MITDB) to develop the model. The experiment is performed in two stages: (i) using original and unbalanced datasets and (ii) using a balanced dataset, obtained from synthetic minority oversampling technique (SMOTE) data sampling technique. We have obtained the highest classification accuracy of 99.82 %, 99.88 %, and 99.89 % using CNN, hybrid CNN-LSTM, and ensemble techniques, respectively. Hence the proposed novel data balancing technique (SMOTE-Tomek Link) not only solves the imbalanced data problem but also increases the minority class accuracy significantly. Now our developed model is ready for the clinical application that can be installed in hospitals for the detection of MI.},
  archive      = {J_APIN},
  author       = {Rai, Hari Mohan and Chatterjee, Kalyan},
  doi          = {10.1007/s10489-021-02696-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5366-5384},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid CNN-LSTM deep learning model and ensemble technique for automatic detection of myocardial infarction using big ECG data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pyramid-attention based multi-scale feature fusion network
for multispectral pan-sharpening. <em>APIN</em>, <em>52</em>(5),
5353–5365. (<a
href="https://doi.org/10.1007/s10489-021-02732-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images with high spatial resolution and high spectral resolution have important applications in human society. In general, due to the limitations faced by the optical sensors’, we are limited to obtain only low spatial resolution multispectral images (MS) and high spatial resolution panchromatic images (PAN). To address this limitation, this study proposes a pyramid-attention based multi-scale feature fusion network (PAMF-Net) that combines the pyramid attention mechanism and feature aggregation. Initially, the MS and PAN images are input to the network, and the PAN images pass through the input pyramid branch to generate a multi-level receiving domain. Then, the result is combined with the features of the MS image as the input of the encoder, and these composite features are input to the pyramid attention mechanism module to capture multi-scale corresponding features. Next, the result of the input pyramid branch is input to the feature aggregation module to seamlessly merge with the features of the pyramid attention mechanism. Finally, in the encoding stage, multiple levels of features are multiplexed as encoding secondary lines by skipping connections to obtain high-quality HRMS images. After quantitative and qualitative experiments, the results show that our method is superior to other advanced methods.},
  archive      = {J_APIN},
  author       = {Chi, Yang and Li, Jinjiang and Fan, Hui},
  doi          = {10.1007/s10489-021-02732-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5353-5365},
  shortjournal = {Appl. Intell.},
  title        = {Pyramid-attention based multi-scale feature fusion network for multispectral pan-sharpening},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DBNLDA: Deep belief network based representation learning
for lncRNA-disease association prediction. <em>APIN</em>,
<em>52</em>(5), 5342–5352. (<a
href="https://doi.org/10.1007/s10489-021-02675-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in the field of high throughput analysis show abnormal expression of long non-coding RNAs (lncRNAs) in many complex diseases. Accurately identifying the disease association of lncRNA is essential in understanding their role in disease mechanism and subsequent therapy. The contemporary methods for predicting lncRNA-disease association use heterogeneous information learned from different biological sources such as lncRNAs, miRNAs, and diseases. However, learning topological features from diverse network structured data is one of the limiting factors of these methods. To address this challenge, we propose a method for lncRNA-disease association prediction based on Deep Belief Network (DBN), referred to as DBNLDA. In this method, three interaction networks such as lncRNA-miRNA similarity (LMS), disease-miRNA similarity (DMS), and lncRNA-disease association (LDA) network are constructed. A new framework based on the node embedding, DBN, and a neural network regression model is used to learn network and local representation of lncRNA-disease pairs. From the node embedding matrices of LMS, DMS, and LDA networks, lncRNA-disease features are learned by DBN layers. These DBN features are used to predict the association score by an ANN regression model. Compared to several state-of-the-art methods, DBNLDA obtained better AUC (0.96) and AUPR (0.967) under five-fold cross-validation. Case studies on breast, lung, and stomach cancer also affirmed the ability of DBNLDA in predicting potential lncRNAs associated with various diseases.},
  archive      = {J_APIN},
  author       = {Madhavan, Manu and Gopakumar, G.},
  doi          = {10.1007/s10489-021-02675-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5342-5352},
  shortjournal = {Appl. Intell.},
  title        = {DBNLDA: Deep belief network based representation learning for lncRNA-disease association prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-grained and multi-layered gradient boosting decision
tree for credit scoring. <em>APIN</em>, <em>52</em>(5), 5325–5341. (<a
href="https://doi.org/10.1007/s10489-021-02715-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring is an important process for banks and financial institutions to manage credit risk. Tree-based ensemble algorithms have made promising progress in credit scoring. However, tree-based ensemble algorithms lack representation learning, making them cannot well express the potential distribution of loan data. In this study, we propose a multi-grained and multi-layered gradient boosting decision tree (GBDT) for credit scoring. Multi-layered GBDT considers the advantages of the explicit learning process of tree-based model and the representation learning ability to discriminate good/bad applicants; multi-grained scanning augments original credit features while enhancing the representation learning ability of multi-layered GBDT. The experimental results on 6 credit scoring datasets show that the hierarchical structure can effectively reduce the intra-class distance and increase the inter-class distance of the credit scoring dataset. In addition, Multi-grained feature augmentation effectively increases the diversity of prediction and further improves the performance of credit scoring, providing more precise credit scoring results.},
  archive      = {J_APIN},
  author       = {Liu, Wan’an and Fan, Hong and Xia, Min},
  doi          = {10.1007/s10489-021-02715-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5325-5341},
  shortjournal = {Appl. Intell.},
  title        = {Multi-grained and multi-layered gradient boosting decision tree for credit scoring},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). A novel POI recommendation model based on joint
spatiotemporal effects and four-way interaction. <em>APIN</em>,
<em>52</em>(5), 5310–5324. (<a
href="https://doi.org/10.1007/s10489-021-02677-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point of interest (POI) recommendation is a fundamental task in location-based social networks (LBSN). The increasing proliferation of LBSNs brings about considerable amounts of user-generated check-in data. Such data can significantly contribute to understanding user behaviors, based on which personalized recommendations can be efficiently derived. Spatial and temporal effects are crucial factors in the user’s decision-making for choosing a POI to visit. Most existing methods treat them as two independent features and cannot accurately capture users’ interests. We argue that spatial and temporal effects should be analyzed simultaneously in POI recommendations. To this end, we propose a S patioT emporal heterogeneous information Network (HIN)-based PO I RE commendation model (STORE) to model various heterogeneous context features, e.g., the joint spatiotemporal effects, types of POI, and social relations. Specifically, we defined the spatiotemporal effects entity (St) in HIN to model the joint spatiotemporal effects. Instead of modeling the traditional two-way interaction &lt;user, item&gt;, we further design a four-way neural interaction model &lt;User, Meta-path, St, POI&gt;. In this way, our model can effectively mine and extract useful information from the meta-path-based context and spatiotemporal effects, thereby improving recommendation performance. We conduct extensive experiments on two real-world datasets, and the results demonstrate that the STORE model outperforms the best baseline by about 12% in NDCG@5 and 11% in Rec@5.},
  archive      = {J_APIN},
  author       = {Liu, Yongheng and Yang, Zhen and Li, Tong and Wu, Di},
  doi          = {10.1007/s10489-021-02677-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5310-5324},
  shortjournal = {Appl. Intell.},
  title        = {A novel POI recommendation model based on joint spatiotemporal effects and four-way interaction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A one-shot next best view system for active object
recognition. <em>APIN</em>, <em>52</em>(5), 5290–5309. (<a
href="https://doi.org/10.1007/s10489-021-02657-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active vision is the ability of intelligent agents to dynamically gather more information about their surroundings by physical motion of the camera. In the case of object recognition, active vision enables improved performance by incorporating classification decisions from new viewpoints when there is some degree of uncertainty in the current recognition result. A natural question in an autonomous active vision system is, nonetheless, how to determine the new viewpoint, i.e. in what pose should the camera be moved? This is the traditional question of next best view in active perception systems. Current approaches to the next best view problem either need construction of occupancy grids or require training datasets of 3D objects or multiple captures of the same object in specified poses. Occupancy grid methods are usually dependent on multiple camera movements to perform well, which make them more useful for 3D reconstruction applications than object recognition. In this paper, a next best view method for active object recognition based on object appearance and surface direction is proposed that decides on the next cameras pose without requiring any specifically structured training datasets of 3D objects. It is also designed for single-shot deductions of next viewpoint and is able to determine next best views without the need for substantial knowledge of 3D voxels in the environment around the camera. The experimental results illustrate the efficiency of the proposed method, while showing large improvements in accuracy and F1 score.},
  archive      = {J_APIN},
  author       = {Hoseini, Pourya and Paul, Shuvo Kumar and Nicolescu, Mircea and Nicolescu, Monica},
  doi          = {10.1007/s10489-021-02657-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5290-5309},
  shortjournal = {Appl. Intell.},
  title        = {A one-shot next best view system for active object recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating clusters of similar sizes by constrained balanced
clustering. <em>APIN</em>, <em>52</em>(5), 5273–5289. (<a
href="https://doi.org/10.1007/s10489-021-02682-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balanced clustering, which generates clusters of similar sizes, can be useful in a variety of applications. However, existing clustering algorithms either cannot guarantee balanced clustering results or require relatively high time complexities for balanced clustering. In this work, we propose a constrained balanced clustering method, which is referred to as τ-balanced clustering, to generate clusters with a controllable balance degree. The proposed method constrains the cluster sizes in the cluster assignment phase based on an established cluster bound size and an established bound for the number of largest clusters. Second, we optimize the basic τ-balanced clustering method by reducing some unnecessary calculations with two-level filtering. Third, we also design a parallel version for the basic τ-balanced clustering method and the optimized method on GPUs (Graphics Processing Units), to enhance the execution efficiency with high parallelism. Finally, we conduct a series of experiments on nine benchmark datasets to verify the proposed methods. The experimental results show that our methods successfully outperform the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Lin, Yuming and Tang, Haibo and Li, You and Fang, Chuangxin and Xu, Zejun and Zhou, Ya and Zhou, Aoying},
  doi          = {10.1007/s10489-021-02682-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5273-5289},
  shortjournal = {Appl. Intell.},
  title        = {Generating clusters of similar sizes by constrained balanced clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Collaborative filtering algorithm with social information
and dynamic time windows. <em>APIN</em>, <em>52</em>(5), 5261–5272. (<a
href="https://doi.org/10.1007/s10489-021-02519-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social networks, the problem of information overload is increasingly serious. The recommendation system can deal with the problem of information overload effectively and provide users with personalized recommendation services. In the process of recommendation, the traditional recommendation algorithms do not take the social relationship of users as the basis of recommendation; at the same time, they do not take for the dynamic change of user’s interest and think that it is immutable. About these problems, the paper proposes a personalized recommendation algorithm with social information and dynamic time windows. Firstly, a collaborative filtering algorithm is proposed which integrates social information and user interest in the process of searching the nearest neighbor. Secondly, the time windows are dynamically adjusted to obtain a stable increment and better reflect the short-term interests of users. Then, the concept of time function is introduced to allocate corresponding time weights for users’ interests in different periods. Finally, we conduct a series of experiments to verify the practicability and effectiveness of our algorithm. Experimental results show that the performance of the proposed algorithm is better than the traditional collaborative filtering recommendation algorithm.},
  archive      = {J_APIN},
  author       = {Li, Dun and Wang, Cui and Li, Lun and Zheng, Zhiyun},
  doi          = {10.1007/s10489-021-02519-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5261-5272},
  shortjournal = {Appl. Intell.},
  title        = {Collaborative filtering algorithm with social information and dynamic time windows},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIVCN: Multimodal interaction video captioning network based
on semantic association graph. <em>APIN</em>, <em>52</em>(5), 5241–5260.
(<a href="https://doi.org/10.1007/s10489-021-02612-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision, it is a challenging task to generate natural language captions from videos as input. To deal with this task, videos are usually regarded as feature sequences and input into Long-Short Term Memory (LSTM) to generate natural language. To get richer and more detailed video content representation, a Multimodal Interaction Video Captioning Network based on Semantic Association Graph (MIVCN) is developed towards this task. This network consists of two modules: Semantic association Graph Module (SAGM) and Multimodal Attention Constraint Module (MACM). Firstly, owing to lack of the semantic interdependence, existing methods often produce illogical sentence structures. Therefore, we propose a SAGM based on information association, which enables network to strengthen the connection between logically related languages and alienate the relations between logically unrelated languages. Secondly, features of each modality need to pay attention to different information among them, and the captured multimodal features are great informative and redundant. Based on the discovery, we propose a MACM based on LSTM, which can capture complementary visual features and filter redundant visual features. The MACM is applied to integrate multimodal features into LSTM, and make network to screen and focus on informative features. Through the association of semantic attributes and the interaction of multimodal features, the semantically contextual interdependent and visually complementary information can be captured by this network, and the informative representation in videos also can be better used for generating captioning. The proposed MIVCN realizes the best caption generation performance on MSVD: 56.8%, 36.4%, and 79.1% on BLEU@4, METEOR, and ROUGE-L evaluation metrics, respectively. Superior results are also reported on MSR-VTT about BLEU@4, METEOR, and ROUGE-L compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Ying and Huang, Guoheng and Yuming, Lin and Yuan, Haoliang and Pun, Chi-Man and Ling, Wing-Kuen and Cheng, Lianglun},
  doi          = {10.1007/s10489-021-02612-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5241-5260},
  shortjournal = {Appl. Intell.},
  title        = {MIVCN: Multimodal interaction video captioning network based on semantic association graph},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MMNet: A multi-scale deep learning network for the left
ventricular segmentation of cardiac MRI images. <em>APIN</em>,
<em>52</em>(5), 5225–5240. (<a
href="https://doi.org/10.1007/s10489-021-02720-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning network models, the automatic segmentation of medical images is becoming increasingly popular. Left ventricular cavity segmentation is an important step in the diagnosis of cardiac disease, but post-processing segmentation is a time-consuming and challenging task. That is why a fully automated segmentation method can assist specialists in increasing their efficiency. Inspired by the power of deep neural networks, a multi-scale multi-skip connection network (MMNet) model is proposed to fully automate the left ventricular segmentation of cardiac magnetic resonance imaging (MRI) images; this model is simple and efficient and has high segmentation accuracy without pre-detecting left ventricular localization. MMNet redesigns the classic encoder and decoder to take advantage of multi-scale feature information, effectively solving the problem of difficult segmentation due to blurred left ventricular edge information and the low accuracy of end-systolic segmentation of the cardiac area. In the model encoding stage, a multi-scale feature fusion module applying dilated convolution is proposed to obtain richer semantic information from different perceptual fields. The decoding stage reconstructs the full-size skip connection structure to make full use of the feature information obtained from different layers for contextual semantic information fusion. At the same time, a pre-activation module is used before each weighting layer to prevent overfitting phenomena from arising. The experimental results demonstrate that the proposed model has better segmentation performance than advanced benchmark models. Ablation experiments show that the proposed modules are effective at improving segmentation results. Therefore, MMNet is a promising approach for the left ventricular fully automated segmentation.},
  archive      = {J_APIN},
  author       = {Wang, Ziyue and Peng, Yanjun and Li, Dapeng and Guo, Yanfei and Zhang, Bin},
  doi          = {10.1007/s10489-021-02720-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5225-5240},
  shortjournal = {Appl. Intell.},
  title        = {MMNet: A multi-scale deep learning network for the left ventricular segmentation of cardiac MRI images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint extraction of entities and overlapping relations by
improved graph convolutional networks. <em>APIN</em>, <em>52</em>(5),
5212–5224. (<a
href="https://doi.org/10.1007/s10489-021-02667-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint extraction of entities and relations is to recognize entities and semantic relations simultaneously, which is significant for knowledge graph construction. Recently, many effective joint models use dependency trees to capture the structural information of sentences. However, most dependency-based methods cannot make full use of the dependency information. This is because these methods just consider the connection information of dependency trees and ignore the influence of different nodes on the connected edges. In this paper, we establish a novel model to extract entities and relations simultaneously by using improved Multi-task Graph Convolutional networks, called MGCN. Specifically, considering the importance of node information, we merge both node and edge information into Graph Convolutional networks (GCN). In addition, in order to recognize the overlapping relations, we propose an efficient strategy to map multiple relational labels of a sentence into a unique code. Finally, we evaluate our joint model on two public datasets, and the experimental results show that our model outperforms the state-of-art models.},
  archive      = {J_APIN},
  author       = {Sun, Qi and Zhang, Kun and Lv, Laishui and Li, Xun and Huang, Kun and Zhang, Ting},
  doi          = {10.1007/s10489-021-02667-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5212-5224},
  shortjournal = {Appl. Intell.},
  title        = {Joint extraction of entities and overlapping relations by improved graph convolutional networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Residual deep attention mechanism and adaptive
reconstruction network for single image super-resolution. <em>APIN</em>,
<em>52</em>(5), 5197–5211. (<a
href="https://doi.org/10.1007/s10489-021-02568-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-level image processing task single image super-resolution (SISR) has a long research history. In recent years, convolutional neural networks (CNNs) have been widely used in single image super-resolution (SISR), and significant performance has been achieved. However, most existing CNN-based SISR methods seldom take into account the feature correlations of the original low-quality images, neglecting to treat spatial and channel-wise features differently. The performance of CNN-based SISR models is often enhanced by deploying very deep networks, which inevitably hinders the representational power of the CNNs and results in many obvious shortcomings. To address these issues, in this paper, we propose a residual deep attention mechanism and adaptive reconstruction network (RAAN) with stronger feature expression and learning abilities. Specifically, to discriminate between large and small gray changes in adjacent areas in LR images, a novel spatial and channel attention processing module (SCAM) is developed that incorporates non-local operations to capture long-distance dependencies between pixels in the spatial domain, and automatically rescales hierarchical features with different weights. Furthermore, we present an enhanced residual attention group (ERAG) structure that not only incorporates some feature processing groups (FPGs), but also contains several source skip connections (SSCs). Similarly, we deploy a global residual long skip connection. With the combination of these skip connections, the low-frequency information flows more effectively to the tail of the network. Moreover, in the upsampling module, we implement four different sizes of convolution kernels (i.e., 3×3, 5×5, 7×7 and 9×9) to extract feature fusion and magnify to the required scale. Experimental results demonstrate the superiority of our RAAN over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.},
  archive      = {J_APIN},
  author       = {Wang, Hongjuan and Wei, Mingrun and Cheng, Ru and Yu, Yue and Zhang, Xingli},
  doi          = {10.1007/s10489-021-02568-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5197-5211},
  shortjournal = {Appl. Intell.},
  title        = {Residual deep attention mechanism and adaptive reconstruction network for single image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attentive frequency learning network for super-resolution.
<em>APIN</em>, <em>52</em>(5), 5185–5196. (<a
href="https://doi.org/10.1007/s10489-021-02703-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the strong capability of capturing long-range dependencies, a series of self-attention based single image super-resolution (SISR) methods have achieved promising performance. However, the existing self-attention mechanisms generally suffer great computational costs both in training and inference. In this study, we propose an innovative attentive frequency learning network (AFLN) for single image super-resolution. Our AFLN can greatly reduce computational costs of self-attention mechanism yet well capture long-range dependencies in SISR tasks. Specifically, our AFLN mainly consists of a series of extensive attentive frequency learning blocks (AFLB). In each AFLB, we firstly integrate the hierarchical features by residual dense connections and decompose the original features into low- and high-frequency domains with a half size of original features via discrete wavelet transform (DWT). Then, we adopt self-attention to explore long-range dependency relations in low- and high-frequency feature domains, respectively. In this way, we can model the self-attention in the quarter size of original input image, greatly reducing computational costs. In addition, the separating attention from low- and high-frequency domain can effectively maintain detailed information. Finally, we adopt the inverse discrete wavelet transform (IDWT) to reconstruct these attentive features. Extensive experiments on publicly available datasets demonstrate the efficiency and effectiveness of our AFLN against the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Li, Fenghai and Cai, Qiang and Li, Haisheng and Chen, Yifan and Cao, Jian and Li, Shanshan},
  doi          = {10.1007/s10489-021-02703-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5185-5196},
  shortjournal = {Appl. Intell.},
  title        = {Attentive frequency learning network for super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CIM: Clique-based heuristic for finding influential nodes in
multilayer networks. <em>APIN</em>, <em>52</em>(5), 5173–5184. (<a
href="https://doi.org/10.1007/s10489-021-02656-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying Influential nodes (Influence maximization) in complex networks is an essential factor for spreading and controlling the information spreading dynamics in social networks. The majority of the influence maximization problems are in monolayer networks. After advancements and increased social network usage, the need to perform influence maximization in multilayer networks has increased. The critical issue is to identify the influential nodes that can effectively spread the information across the networks. Detecting such influential nodes with high precision in multilayer networks is a challenging and yet unexplored task. Based on our experiments, it is observed that a potential node may have strong connections in both the interlayers and intralayers. A comparative study of various influence maximization algorithms in multilayer networks is carried out with this observation. We propose a novel algorithm, clique-based influence maximization (CIM) in a multilayer network. We also propose ignoring noted nodes in the network to increase the efficiency of influence maximization and remove the information redundancy. CIM is generating better results for influence spread in multilayer networks compared to the other algorithms. The simulation studies have shown that CIM can detect influential nodes on both real and synthetic networks under various environments.},
  archive      = {J_APIN},
  author       = {K, Venkatakrishna Rao. and Katukuri, Mahender and Jagarapu, Maheswari},
  doi          = {10.1007/s10489-021-02656-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5173-5184},
  shortjournal = {Appl. Intell.},
  title        = {CIM: Clique-based heuristic for finding influential nodes in multilayer networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning with open set based discrimination
enhancement. <em>APIN</em>, <em>52</em>(5), 5159–5172. (<a
href="https://doi.org/10.1007/s10489-021-02643-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Class Incremental Learning (MCIL) is an important task in deep learning that aims to solve real world problems of allowing models to continuously learn new concepts without forgetting old ones. However, MCIL typically suffers from catastrophic forgetting. In addition, it is difficult to implement many MCIL algorithms, which require to store data associated with the number of existing classes, on edge devices with limited memory. We believe that the data imbalance between the old and new classes and the utilization of old classes in the incremental process, which increases the memory requirements, are the main reasons for the above problems. Hence, we propose a novel approach, called ‘Open Set Incremental Learning(OSIL)’, to preserve the information of old classes, without storing any of their data, while making the classifier progressively learn the new classes and forget less information about the old classes. In OSIL, we present an open-set incremental strategy, which uses enhanced discrimination combined with open-set identification to overcome the imbalance between old classes and new classes. Meanwhile we demonstrate that attention mechanisms help to address catastrophic forgetting. The proposed method is evaluated on CIFAR-100 and ImageNet-100 under various settings. Experimental results show that OSIL effectively alleviates catastrophic forgetting and significantly outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Ding, Jiexuan},
  doi          = {10.1007/s10489-021-02643-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5159-5172},
  shortjournal = {Appl. Intell.},
  title        = {Incremental learning with open set based discrimination enhancement},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CCF-LRU: Hybrid storage cache replacement strategy based on
counting cuckoo filter hot-probe method. <em>APIN</em>, <em>52</em>(5),
5144–5158. (<a
href="https://doi.org/10.1007/s10489-021-02567-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the massive increase in the data generation has given rise to enormous challenges in storage systems. Single storage media, such as hard disk drives (HDDs) or solid state drives (SSDs), cannot meet the actual needs owing to their inherent physical characteristics. One feasible solution is to adopt a hybrid storage architecture that uses both an SSD and HDD. In this case, the management of the cache replacement strategy of the hybrid storage becomes key in improving storage performance. Based on the cuckoo filter, this study proposes a counting cuckoo filter (CCF) hot-probe method that exhibits a high space and time efficiency and supports deletion. Moreover, a CCF-least recently used (LRU) cache replacement strategy is proposed by combining CCF and the adaptive two-level LRU technique. This strategy uses CCF to identify hot data and the adaptive two-level LRU technique to manage the cache. Experimental results indicate that in comparison with traditional strategies, the cache replacement strategy combined with the hot-probe method can significantly improve cache hit ratios. Furthermore, in comparison with other cache replacement strategies that use hot-probe methods, CCF-LRU exhibits a lower time and space complexity and a higher hit ratio.},
  archive      = {J_APIN},
  author       = {Wang, Yinyin and Yang, Yuwang and Qiu, Xiulin and Ke, Yaqi and Wang, Qingguang},
  doi          = {10.1007/s10489-021-02567-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5144-5158},
  shortjournal = {Appl. Intell.},
  title        = {CCF-LRU: Hybrid storage cache replacement strategy based on counting cuckoo filter hot-probe method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active-learning-based reconstruction of circuit model.
<em>APIN</em>, <em>52</em>(5), 5125–5143. (<a
href="https://doi.org/10.1007/s10489-021-02700-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing the circuit model presents a challenge for circuits with unknown functional specifications. The circuit is thought of as a black box that, given an input, produces an output. The model of the circuit, on the other hand, is unknown. Given a set of inputs and their corresponding outputs, the goal is then to recover the circuit specification while maximizing reconstruction accuracy. This process is computationally difficult, and it becomes even more difficult to solve when only a subset of inputs and outputs are provided, as is the case in many large and complex circuits. This issue is addressed in this paper: Reconstructing the model of a circuit from a set of components and observations describing its inputs and outputs. Previous work proposed a decision tree approach, but this approach only works when the entire set of inputs and outputs is available. Nonetheless, for most systems, this requirement is unrealistic. To address this challenge, we propose an active learning approach and applying orthogonal arrays of fractional factorial design to sample labeled data for learning the reconstructed circuit. Evaluation on 9 well known circuits shows the benefits of the proposed algorithms in terms of accuracy, run time and the reconstructed circuit model.},
  archive      = {J_APIN},
  author       = {Rozenfeld, Gal and Kalech, Meir and Rokach, Lior},
  doi          = {10.1007/s10489-021-02700-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5125-5143},
  shortjournal = {Appl. Intell.},
  title        = {Active-learning-based reconstruction of circuit model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADD: A new average divergence difference-based outlier
detection method with skewed distribution of data objects.
<em>APIN</em>, <em>52</em>(5), 5100–5124. (<a
href="https://doi.org/10.1007/s10489-021-02399-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is of vital importance in data mining tasks, with numerous applications, including video surveillance and credit card fraud detection. Quite a few outlier detection algorithms have been developed and have received considerable attention, and most existing methods are classified as distance-based algorithms and density-based algorithms. However, both of these approaches have some flaws. The former has difficulty detecting local outliers, and the latter cannot handle low-density pattern problems. Moreover, outlier detection algorithms are sensitive to parameter settings. This paper proposes a simple and efficient outlier detection approach (called ADD) based on the average divergence difference of data objects; in this method there is no need to artificially define the number of neighbors of objects k to solve the above issues. In this algorithm, two new measures, called the divergence factor (DF) and the average divergence difference (LADD), are developed based on the skewed distribution characteristics of data objects and their natural neighbors, thus improving the accuracy of local outlier detection from an innovative research perspective. These factors are presented as external and internal characterization factors because the former characterizes the skew distribution characteristics and compactness relationship of data objects and the latter represents the difference in the skew distribution characteristics of data objects in a neighborhood. Then, we set an appropriate threshold to distinguish whether a data point is an outlier, which eliminates the interference of the Top-N problem. Finally, the final experimental results show that the ADD algorithm achieves an overall improvement in local outlier detection, especially in the detection of outliers in some datasets with complex distributions and in low-density areas, compared to that achieved by state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Xiong, Zhong-Yang and Gao, Qin-Qin and Gao, Qiang and Zhang, Yu-Fang and Li, Lin-Tao and Zhang, Min},
  doi          = {10.1007/s10489-021-02399-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5100-5124},
  shortjournal = {Appl. Intell.},
  title        = {ADD: A new average divergence difference-based outlier detection method with skewed distribution of data objects},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFQN: Approximate qn estimation in data streams.
<em>APIN</em>, <em>52</em>(5), 5082–5099. (<a
href="https://doi.org/10.1007/s10489-021-02614-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present afqn (Approximate Fast Qn), a novel algorithm for approximate computation of the Qn scale estimator in a streaming setting, in the sliding window model. It is well-known that computing the Qn estimator exactly may be too costly for some applications, and the problem is a fortiori exacerbated in the streaming setting, in which the time available to process incoming data stream items is short. In this paper we show how to efficiently and accurately approximate the Qn estimator. As an application, we show the use of afqn for fast detection of outliers in data streams. In particular, the outliers are detected in the sliding window model, with a simple check based on the Qn scale estimator. Extensive experimental results on synthetic and real datasets confirm the validity of our approach by showing up to three times faster updates per second. Our contributions are the following ones: (i) to the best of our knowledge, we present the first approximation algorithm for online computation of the Qn scale estimator in a streaming setting and in the sliding window model; (ii) we show how to take advantage of our UDDSketch algorithm for quantile estimation in order to quickly compute the Qn scale estimator; (iii) as an example of a possible application of the Qn scale estimator, we discuss how to detect outliers in an input data stream.},
  archive      = {J_APIN},
  author       = {Epicoco, Italo and Melle, Catiuscia and Cafaro, Massimo and Pulimeno, Marco},
  doi          = {10.1007/s10489-021-02614-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5082-5099},
  shortjournal = {Appl. Intell.},
  title        = {AFQN: Approximate qn estimation in data streams},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel filter feature selection algorithm based on relief.
<em>APIN</em>, <em>52</em>(5), 5063–5081. (<a
href="https://doi.org/10.1007/s10489-021-02659-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Relief algorithm is a feature selection algorithm that uses the nearest neighbor to weight attributes. However, Relief only considers the correlation between features, which leads to a low classification accuracy on noisy datasets whose interaction effect is weak. To overcome the weaknesses of Relief, a novel feature selection algorithm, named Multidirectional Relief (MRelief), is proposed. The MRelief algorithm includes four improvements. First, the multidirectional neighbor search method, which finds all neighbors within a distance threshold from different orientations, is included to obtain regularly distributed neighbors. Therefore, the weights provided by MRelief are more accurate than those provided by Relief. Second, a novel objective function that incorporates the instances’ force coefficients is introduced to reduce the influence of noise. Thus, the new objective function improves the classification accuracy of MRelief. Third, subset generation is introduced to the MRelief algorithm and combined with the maximum Pearson maximum distance (MPMD) to generate a promising candidate subset for feature selection. Finally, a novel multiclass margin definition is proposed and introduced to the MRelief algorithm to handle multiclass data. As demonstrated by extensive experiments on eleven UCI datasets and eleven real-world gene expression benchmarking datasets, MRelief is significantly better than other algorithms including LPLIR, ReliefF, LLH-Relief, MultiSURF, MSLIR-NN, MRMR, MPMD and STIR in our study.},
  archive      = {J_APIN},
  author       = {Cui, Xueting and Li, Ying and Fan, Jiahao and Wang, Tan},
  doi          = {10.1007/s10489-021-02659-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5063-5081},
  shortjournal = {Appl. Intell.},
  title        = {A novel filter feature selection algorithm based on relief},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Civil airline fare prediction with a multi-attribute
dual-stage attention mechanism. <em>APIN</em>, <em>52</em>(5),
5047–5062. (<a
href="https://doi.org/10.1007/s10489-021-02602-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airfare price prediction is one of the core facilities of the decision support system in civil aviation, which includes departure time, days of purchase in advance and flight airline. The traditional airfare price prediction system is limited by the nonlinear interrelationship of multiple factors and fails to deal with the impact of different time steps, resulting in low prediction accuracy. To address these challenges, this paper proposes a novel civil airline fare prediction system with a Multi-Attribute Dual-stage Attention (MADA) mechanism integrating different types of data extracted from the same dimension. In this method, the Seq2Seq model is used to add attention mechanisms to both the encoder and the decoder. The encoder attention mechanism extracts multi-attribute data from time series, which are optimized and filtered by the temporal attention mechanism in the decoder to capture the complex time dependence of the ticket price sequence. Extensive experiments with actual civil aviation data sets were performed, and the results suggested that MADA outperforms airfare prediction models based on the Auto-Regressive Integrated Moving Average (ARIMA), random forest, or deep learning models in MSE, RMSE, and MAE indicators. And from the results of a large amount of experimental data, it is proven that the prediction results of the MADA model proposed in this paper on different routes are at least 2.3% better than the other compared models.},
  archive      = {J_APIN},
  author       = {Zhao, Zhichao and You, Jinguo and Gan, Guoyu and Li, Xiaowu and Ding, Jiaman},
  doi          = {10.1007/s10489-021-02602-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5047-5062},
  shortjournal = {Appl. Intell.},
  title        = {Civil airline fare prediction with a multi-attribute dual-stage attention mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Slither: Finding local dense subgraphs measured by average
degree. <em>APIN</em>, <em>52</em>(5), 5034–5046. (<a
href="https://doi.org/10.1007/s10489-021-02684-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for dense subgraphs is the crux of a variety of graph mining applications. It is also meaningful to investigate problems of finding local dense subgraphs related to particular nodes, especially for large real-world graphs. However, none of the problems focus on maximizing the average degrees of the found subgraphs. We formulate the MDS-N problem, which aims to find such a subgraph with the maximum average degree, near or containing a given node in an undirected graph. We propose the Slither algorithm and the Slither PageRank (PR) algorithm, built on a reduction from the MDS-N problem to the minimum conductance problem and the Lovász-Simonovits Theorem of random walks. A simple hierarchically repetition frame is also proposed to advance the two algorithms. Experiments conducted on both unipartite graphs and bipartite graphs show the effectiveness, stability, and scalability of our algorithms. Additionally, we verify the MDS-N problem and the proposed algorithms on a large social network Twitter, the experimental results of which show our algorithms can successfully detect local fraud-related subgraphs based on particular fraudulent user accounts.},
  archive      = {J_APIN},
  author       = {Deng, Zixuan and Xiang, Yanping},
  doi          = {10.1007/s10489-021-02684-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5034-5046},
  shortjournal = {Appl. Intell.},
  title        = {Slither: Finding local dense subgraphs measured by average degree},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel deep pixel restoration video prediction algorithm
integrating attention mechanism. <em>APIN</em>, <em>52</em>(5),
5015–5033. (<a
href="https://doi.org/10.1007/s10489-021-02631-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of deep learning, in recent years, many excellent deep learning models have been developed to solve the problem of video frame prediction. Among them, most models directly generate predicted target frames. However, the predicted frames obtained in this way are often fuzzy and not realistic enough. In order to solve this problem, this paper first attempts to integrate the attention mechanism with Convolutional Long Short-Term Memory, and correspondingly proposes a new deep learning model, abbreviated as AttConvLSTM. One prominent and original characteristic of this newly constructed model is that, each of its layer calculates the attention weight of the obtained information, focusing on the information key part. Although the proposed AttConvLSTM model effectively improves the prediction accuracy, it still does not solve the problem that the prediction frames directly generated by classical deep learning models are often fuzzy and not realistic. Therefore, inspired by the concept of optical flow, this work further develops a novel Deep Pixel Restoration AttConvLSTM (DPRAConvLSTM) model. This model cleverly uses the input frames and the end-to-end characteristics of deep learning. We innovatively restore the pixels of the input frames to the predicted frames, thereby avoiding the defects that typical deep learning models can easily cause, when directly generating the predicted frames. The experimental results effectively confirm that the finally formed DPRAConvLSTM model can not only improve the accuracy of prediction, but also obtain clearer and more realistic prediction frames.},
  archive      = {J_APIN},
  author       = {Yuan, Muxuan and Dai, Qun},
  doi          = {10.1007/s10489-021-02631-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {5015-5033},
  shortjournal = {Appl. Intell.},
  title        = {A novel deep pixel restoration video prediction algorithm integrating attention mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling risk and return awareness for p2p lending
recommendation with graph convolutional networks. <em>APIN</em>,
<em>52</em>(5), 4999–5014. (<a
href="https://doi.org/10.1007/s10489-021-02680-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {P2P lending recommendation (P2PLR) has become an important and popular component of P2P lending platform, which aim to align the right loans with the right investors according to historical interaction. Most of the existing P2PLR models take risk and return as side information into P2PLR to capture the relationship among loans. Although some of them have been proven effective, the following two insights are often neglected. First, risk, the possibility of investors’ loss, is the most important attribute in P2P lending platform, and the implicit capture of risk may lose critical information in the P2PLR scenario. Second, the preference and sensitivity of return on loans is unknown, which is only implicitly reflected in the loans that the investor has invested. These insights motivate us to propose a novel P2PLR model riSk and return Aware Recommendation (STAR), which propagates the influence of risk and return on investor to make the learned investor representations be risk and return aware. Particularly, we specify two STAR methods, named STHCW (riSk and return aware Recommendation with High-order Connectivity with Weight) and STHC (riSk and return aware Recommendation with High-order Connectivity without weight), based on two strategies of embedding and propagation mechanisms, respectively. We conduct extensive experiments on three real-world datasets, demonstrating the effectiveness of our proposed method in recommendation via risk and return. Further analysis reveals that modeling the risk and return awareness is particularly useful for learning the risk and return aware preference of investor.},
  archive      = {J_APIN},
  author       = {Liu, Yuhang and Ma, Huifang and Jiang, Yanbin and Li, Zhixin},
  doi          = {10.1007/s10489-021-02680-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4999-5014},
  shortjournal = {Appl. Intell.},
  title        = {Modelling risk and return awareness for p2p lending recommendation with graph convolutional networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event detection from text using path-aware graph
convolutional network. <em>APIN</em>, <em>52</em>(5), 4987–4998. (<a
href="https://doi.org/10.1007/s10489-021-02695-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event detection aims to detect events from text by locating event triggers and classifying them into predefined event types. Current state-of-the-art event detection methods benefit from integration of syntactic dependency into graph convolutional network (GCN). Despite the great success of GCN-based event detection methods, there are still two problems. Firstly, most GCN-based methods are designed as stacked structure to capture high-order contextual information, which will result in over-smoothing problem; secondly, dependency type information are not fully utilized in current GCN-based methods due to severe sparsity problem of some dependency types. In this paper, we propose P ath-Aware G raph C onvolutional N etwork (PGCN) model, shedding lights on simultaneously tackling these two problems. Specifically, PGCN is designed as flat structure to avoid over-smoothing problem, while path-aware aggregation is proposed to capture all-order contextual information and integrate dependency type information into feature space at the same time. Moreover, to deal with sparsity problem of some path types, we further adopt latent factor decomposition (LFD) technique by sharing parameters among different kinds of path. Our method is verified on the benchmark ACE 2005 English dataset. Experimental results show that our method gets 1.4% improvement on F1 score over state-of-the-art method, and performs more stably than other GCN-based methods with separate random initializations.},
  archive      = {J_APIN},
  author       = {Lu, Shudong and Li, Si and Xu, Yajing and Wang, Kai and Lan, Haibo and Guo, Jun},
  doi          = {10.1007/s10489-021-02695-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4987-4998},
  shortjournal = {Appl. Intell.},
  title        = {Event detection from text using path-aware graph convolutional network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiamPCF: Siamese point regression with coarse-fine
classification network for visual tracking. <em>APIN</em>,
<em>52</em>(5), 4973–4986. (<a
href="https://doi.org/10.1007/s10489-021-02651-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current tracking methods use bounding box to describe objects, which only provides a rough outline and is unable to accurately capture the shape and posture of the target. Instead of using a bounding box directly, we use points adaptively positioned on the target to describe the target and transform these points to bounding boxes. In this way, we can use a training strategy based on bounding box while describing the target more accurately. Furthermore, a Coarse-Fine classification module is employed to improve the robustness, which is important in the case of scale variation and deformation. Combining the above modules, we propose our SiamPCF, which is an anchor-free tracking method that avoids the carefully selected hyperparameters needed to design anchors. Extensive experiments conducted on five benchmarks show that our SiamPCF can achieve state-of-the-art results. In the analysis of video attributes, our SiamPCF ranks first in scale variance, which demonstrates its effectiveness. Our SiamPCF runs more than 45 frames per second.},
  archive      = {J_APIN},
  author       = {Zeng, Yulin and Zeng, Bi and Yin, Xiuwen and Chen, Guangke},
  doi          = {10.1007/s10489-021-02651-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4973-4986},
  shortjournal = {Appl. Intell.},
  title        = {SiamPCF: Siamese point regression with coarse-fine classification network for visual tracking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence interval for micro-averaged f1 and macro-averaged
f1 scores. <em>APIN</em>, <em>52</em>(5), 4961–4972. (<a
href="https://doi.org/10.1007/s10489-021-02635-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A binary classification problem is common in medical field, and we often use sensitivity, specificity, accuracy, negative and positive predictive values as measures of performance of a binary predictor. In computer science, a classifier is usually evaluated with precision (positive predictive value) and recall (sensitivity). As a single summary measure of a classifier’s performance, F1 score, defined as the harmonic mean of precision and recall, is widely used in the context of information retrieval and information extraction evaluation since it possesses favorable characteristics, especially when the prevalence is low. Some statistical methods for inference have been developed for the F1 score in binary classification problems; however, they have not been extended to the problem of multi-class classification. There are three types of F1 scores, and statistical properties of these F1 scores have hardly ever been discussed. We propose methods based on the large sample multivariate central limit theorem for estimating F1 scores with confidence intervals.},
  archive      = {J_APIN},
  author       = {Takahashi, Kanae and Yamamoto, Kouji and Kuchiba, Aya and Koyama, Tatsuki},
  doi          = {10.1007/s10489-021-02635-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4961-4972},
  shortjournal = {Appl. Intell.},
  title        = {Confidence interval for micro-averaged f1 and macro-averaged f1 scores},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of small and medium-sized enterprises’
sustainable development with hesitant fuzzy linguistic group
decision-making method. <em>APIN</em>, <em>52</em>(5), 4940–4960. (<a
href="https://doi.org/10.1007/s10489-021-02372-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the economy, the competition among enterprises is increasingly intensifying and the internal contradictions of the enterprise are prominent, which pose many challenges to the sustainable development of small- and medium-sized enterprises (SMEs). Therefore, the scientific assessment of an enterprise’s sustainable development capability is important, as it reflects the current development status and the potential future opportunities. Hesitant fuzzy linguistic preference relations (HFLPRs), a combination of linguistic term sets and hesitant fuzzy preference relations are useful in solving group decision-making (GDM) problems. HFLTS can tackle situations in which decision makers (DMs) consider multiple potential linguistic terms at the same time than a single term for an indicator, alternative, variable, etc., to express their preferences without the use of numerical values. This paper introduces a new GDM approach under the hesitant fuzzy linguistic environment based on a multiplicative consistency adjustment algorithm and a Charnes-Cooper-Rhodes Data Envelopment Analysis (CCR-DEA) model to assess the sustainable development of SMEs. First, the concepts of HFLPR and multiplicative consistency, including the consistency index and consistency checking approach, are reviewed. Then, a new consistency-improving method for achieving an acceptable HFLPR is introduced, which ensures that each HFLPR satisfies the requirements for multiplicative consistency. After this transformation, an innovative CCR-DEA model is developed using the input–output technique to determine the final ranking of alternatives and to achieve an optimal decision-making solution. Finally, the hesitant fuzzy linguistic decision-making method is intended to be used to assess the economic growth of SMEs. The advantages and applicability of the proposed approach are determined by comparative analysis.},
  archive      = {J_APIN},
  author       = {Jin, Feifei and Zhang, Ying and Garg, Harish and Liu, Jinpei and Chen, Jia},
  doi          = {10.1007/s10489-021-02372-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4940-4960},
  shortjournal = {Appl. Intell.},
  title        = {Evaluation of small and medium-sized enterprises’ sustainable development with hesitant fuzzy linguistic group decision-making method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tuning database-friendly random projection matrices for
improved distance preservation on specific data. <em>APIN</em>,
<em>52</em>(5), 4927–4939. (<a
href="https://doi.org/10.1007/s10489-021-02626-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Projection is one of the most popular and successful dimensionality reduction algorithms for large volumes of data. However, given its stochastic nature, different initializations of the projection matrix can lead to very different levels of performance. This paper presents a guided random search algorithm to mitigate this problem. The proposed method uses a small number of training data samples to iteratively adjust a projection matrix, improving its performance on similarly distributed data. Experimental results show that projection matrices generated with the proposed method result in a better preservation of distances between data samples. Conveniently, this is achieved while preserving the database-friendliness of the projection matrix, as it remains sparse and comprised exclusively of integers after being tuned with our algorithm. Moreover, running the proposed algorithm on a consumer-grade CPU requires only a few seconds.},
  archive      = {J_APIN},
  author       = {López-Sánchez, Daniel and de Bodt, Cyril and Lee, John A. and Arrieta, Angélica González and Corchado, Juan M.},
  doi          = {10.1007/s10489-021-02626-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4927-4939},
  shortjournal = {Appl. Intell.},
  title        = {Tuning database-friendly random projection matrices for improved distance preservation on specific data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimization approach with weighted SCiForest and
weighted hausdorff distance for noise data and redundant data.
<em>APIN</em>, <em>52</em>(5), 4909–4926. (<a
href="https://doi.org/10.1007/s10489-021-02685-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligent technology, data obtained from practical applications may be subject to noise information (outlier data or redundant data). Noise data usually leads to the deterioration of the performance and robustness of classifiers. In order to address the above problem, in this paper, we propose an optimization method for Outlier samples and Redundant samples Detection (ORD). Firstly, we leverage the maximum information compression to eliminate irrelevant feature information. Secondly, an outlier optimization filter is proposed, called WSCiForest, which utilizes the fusion strategy based on the entropy-weighted and group optimization theory to calculate the distribution estimated score for each sample. Eventually, ORD adopts the improved Hausdorff distance to obtain redundant samples effectively. The experimental results show that the proposed method can effectively optimize the data space.},
  archive      = {J_APIN},
  author       = {Zheng, Yifeng and Li, Guohe and Li, Ying and Zhang, Wenjie and Pan, Xueling and Lin, Yaojin},
  doi          = {10.1007/s10489-021-02685-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4909-4926},
  shortjournal = {Appl. Intell.},
  title        = {An optimization approach with weighted SCiForest and weighted hausdorff distance for noise data and redundant data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic FDB selection method and its application: Modeling
and optimizing of directional overcurrent relays coordination.
<em>APIN</em>, <em>52</em>(5), 4873–4908. (<a
href="https://doi.org/10.1007/s10489-021-02629-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article has four main objectives. These are: to develop the dynamic fitness-distance balance (dFDB) selection method for meta-heuristic search algorithms, to develop a strong optimization algorithm using the dFDB method, to create an optimization model of the coordination of directional overcurrent relays (DOCRs) problem, and to optimize the DOCRs problem using the developed algorithm, respectively. A comprehensive experimental study was conducted to analyze the performance of the developed dFDB selection method and to evaluate the optimization results of the DOCRs problem. Experimental studies were carried out in two steps. In the first step, to test the performance of the developed dFDB method and optimization algorithm, studies were conducted on three different benchmark test suites consisting of different problem types and dimensions. The data obtained from the experimental studies were analyzed using non-parametric statistical methods and the most effective among the developed optimization algorithms was determined. In the second step, the DOCRs problem was optimized using the developed algorithm. The performance of the proposed method for the solution to the DOCRs coordination problem was evaluated on five test systems including the IEEE 3-bus, the IEEE 4-bus, the 8-bus, the 9-bus, and the IEEE 30-bus test systems. The numerical results of the developed algorithm were compared with previously proposed algorithms available in the literature. Simulation results showed the effectiveness of the proposed method in minimizing the relay operating time for the optimal coordination of DOCRs.},
  archive      = {J_APIN},
  author       = {Kahraman, Hamdi Tolga and Bakir, Huseyin and Duman, Serhat and Katı, Mehmet and ARAS, Sefa and Guvenc, Ugur},
  doi          = {10.1007/s10489-021-02629-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4873-4908},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic FDB selection method and its application: Modeling and optimizing of directional overcurrent relays coordination},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep convolutional self-paced clustering. <em>APIN</em>,
<em>52</em>(5), 4858–4872. (<a
href="https://doi.org/10.1007/s10489-021-02569-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a crucial but challenging task in data mining and machine learning. Recently, deep clustering, which derives inspiration primarily from deep learning approaches, has achieved state-of-the-art performance in various applications and attracted considerable attention. Nevertheless, most of these approaches fail to effectively learn informative cluster-oriented features for data with spatial correlation structure, e.g., images. To tackle this problem, in this paper, we develop a deep convolutional self-paced clustering (DCSPC) method. Specifically, in the pretraining stage, we propose to utilize a convolutional autoencoder to extract a high-quality data representation that contains the spatial correlation information. Then, in the finetuning stage, a clustering loss is directly imposed on the learned features to jointly perform feature refinement and cluster assignment. We retain the decoder to avoid the feature space being distorted by the clustering loss. To stabilize the training process of the whole network, we further introduce a self-paced learning mechanism and select the most confident samples in each iteration. Through comprehensive experiments on seven popular image datasets, we demonstrate that the proposed algorithm can consistently outperform state-of-the-art rivals.},
  archive      = {J_APIN},
  author       = {Chen, Rui and Tang, Yongqiang and Tian, Lei and Zhang, Caixia and Zhang, Wensheng},
  doi          = {10.1007/s10489-021-02569-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4858-4872},
  shortjournal = {Appl. Intell.},
  title        = {Deep convolutional self-paced clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HOB-net: High-order block network via deep metric learning
for person re-identification. <em>APIN</em>, <em>52</em>(5), 4844–4857.
(<a href="https://doi.org/10.1007/s10489-021-02450-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective feature representations with deep convolutional neural network (CNN) and metric learning methods to distinguish pedestrians is the key to the success of recent advances in person re-identification (re-ID) tasks. However, the features provided by the common CNN network are not strong enough to distinguish between similar subjects because these common features describe only the scattered local patterns while neglecting the correlation and combinations between them. In fact, the high-order correlations of common features can be significant to the recognition of identities. In this paper, to tackle this problem, we design a flexible high-order block (HOB) module and a scheme of deep metric learning to produce the high-order representations of deep features for the re-identification of pedestrians. Extensive experiments prove the superiority of our proposed HOB module for person re-ID issue. On three large-scale datasets, including Market-1501, DukeMTMC-ReID, and CUHK03-NP, the HOB-net method achieves the competitive results with the state-of-the-arts, particularly in the mAP.},
  archive      = {J_APIN},
  author       = {Chen, Dongyue and Wu, Pengfei and Jia, Tong and Xu, Fangbin},
  doi          = {10.1007/s10489-021-02450-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4844-4857},
  shortjournal = {Appl. Intell.},
  title        = {HOB-net: High-order block network via deep metric learning for person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIDNN- a classification approach for the EEG based motor
imagery tasks using deep neural network. <em>APIN</em>, <em>52</em>(5),
4824–4843. (<a
href="https://doi.org/10.1007/s10489-021-02622-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, Motor Imagery (MI) tasks have gained great attraction among researchers in the field of Brain-Computer Interface (BCI). The MI tasks are the core field of the human brain in which the person imagines the movement of the body parts without performing actual movement of the body. MI tasks cause the activation of lakhs of neurons in the brain to interact with each other. The activation of the neurons generates electrical signals that can be captured through electroencephalogram (EEG) devices. The MI-EEG-based signals can move the external devices such as a wheelchair, moving cursors, etc., and hence, are very helpful to design and develop personal assistants for the disabled person for interaction and communication to the outside world. In this paper, MI-EEG data for left-hand(LH) and right-hand(RH) movements are recorded using a multi-channel EEG device. Further, a Deep Neural Network (DNN) model (MIDNN) is proposed for the binary-class classification of the collected dataset. The performance of the proposed model has been tested on the BCI benchmark dataset BCI competition III V dataset for LH and RH MI tasks.The fifth order low pass Butterworth filter is used to denoise the raw signals and then decomposed into six frequency sub-bands of (0.5–4) Hz, (4–8) Hz, (8–12) Hz, (12–16) Hz, (16–24) Hz, and (24–40) Hz using Butterworth bandpass filter of same order. The sub-bands are used to extract features from MI-EEG signals of LH and RH movement using welch power spectral density (PSD). The accuracy obtained by the MIDNN model is around 70% on the local dataset and 72.51% on the BCI dataset using PSD as features from each channel for classification of LH and RH tasks. To further improve the performance of the model, the spectral features from the estimated PSD of each of the six sub-band are obtained in the form of band power. The accuracy obtained by the same MIDNN model using band power as features is 88.89% on the local dataset and 82.48% on the V dataset of BCI competition III. The proposed MIDNN model acheived a significant increase in classification accuracy by 13.7% and 26.9% on BCI and Emotiv dataset respectively.},
  archive      = {J_APIN},
  author       = {Tiwari, Smita and Goel, Shivani and Bhardwaj, Arpit},
  doi          = {10.1007/s10489-021-02622-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4824-4843},
  shortjournal = {Appl. Intell.},
  title        = {MIDNN- a classification approach for the EEG based motor imagery tasks using deep neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on the application of high-efficiency detectors
into the detection of prohibited item in x-ray images. <em>APIN</em>,
<em>52</em>(5), 4807–4823. (<a
href="https://doi.org/10.1007/s10489-021-02582-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray imaging can be used to inspect the internal structure of the objects without destruction, so visual inspection based on X-ray images is widely used in the security check such as customs, airports, railway stations, and postal express. Especially in the postal express industry, fast and accurate inspection of express parcels can effectively improve logistics efficiency. This article studies the application of computer vision technology to detect prohibited items in X-ray images. Due to the multi-pose objects in the packages under multi-views, it is difficult to find out the prohibited item from the packages under a single view. This article explores how to solve this problem with the loss function of classification and the attention mechanism of convolutional neural network, and apply them to high-efficiency detectors. On the one hand, we proposed a new loss function named truncated loss for X-ray image classification task. In the proposed loss, we truncated input vector of loss layer to reduce the difference within the intra-classes and increase the difference between the inter-classes. On the other hand, we proposed two new architectures for the high-efficiency detectors for the purpose of obtaining the visual features of prohibited item more effectively. One of the new architectures named channel context block (CC block), and it is based on global context (GC block). It contains global context information on each channel through operations of global average pooling, which is different from global context (GC) block. The other one of the architectures named GCC block, it is formed by merging channel context block (CC block) and global context (GC) block, and it is used to further improve the detection accuracy of prohibited item. The results of experiments on the currently widely used high-efficiency detectors in GDXray dataset show that our proposed truncated loss can improve the detection accuracy of prohibited item to a certain extent, and the new architectures can improve detection accuracy to a greater extent. The algorithms proposed in this article are also state-of-the-art on GDXray dataset.},
  archive      = {J_APIN},
  author       = {Wei, Yuanxi and Liu, Xiaoping and Liu, Yinan},
  doi          = {10.1007/s10489-021-02582-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4807-4823},
  shortjournal = {Appl. Intell.},
  title        = {Research on the application of high-efficiency detectors into the detection of prohibited item in X-ray images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental neighborhood entropy-based feature selection for
mixed-type data under the variation of feature set. <em>APIN</em>,
<em>52</em>(5), 4792–4806. (<a
href="https://doi.org/10.1007/s10489-021-02526-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is to find relevant features and delete redundant features, which provides a basis for classification problems. In many real-world applications, mixed-type data including missing, numerical, and categorical features are ubiquitous in medical treatment, intrusion detection, traffic analysis and so on. Feature selection from mixed-type data has attracted considerable research attention. The neighborhood rough set model has attracted much attention to select a feature subset when handling with mixed-type data. In this study, we focus on the feature selection process for mixed-type data under the variation of feature set by the utilization of neighborhood rough sets. At first, the hybrid relation is given to define the similarity between objects for the mixed-type data without resorting to the discretization process. On this basis, the neighborhood entropy is given to evaluate the uncertainty of the mixed-type data. When new features may appear while old features are deleted, the updated neighborhood entropy is computed incrementally to reflect the significance of mixed-type features, which is an important step in the dynamic feature selection process. Finally, an efficient incremental feature selection algorithm for selecting a new feature subset is developed when deleting and adding a feature set simultaneously. Experimental results over different real-life data sets have verified the feasibility and efficiency of the proposed algorithm from the perspective of the runtime.},
  archive      = {J_APIN},
  author       = {Shu, Wenhao and Qian, Wenbin and Xie, Yonghong},
  doi          = {10.1007/s10489-021-02526-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4792-4806},
  shortjournal = {Appl. Intell.},
  title        = {Incremental neighborhood entropy-based feature selection for mixed-type data under the variation of feature set},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simplified multilayer graph convolutional networks with
dropout. <em>APIN</em>, <em>52</em>(5), 4776–4791. (<a
href="https://doi.org/10.1007/s10489-021-02617-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) and their variants are excellent deep learning methods for graph-structured data. Moreover, multilayer GCNs can perform feature smoothing repeatedly, which creates considerable performance improvements. However, they may inherit unnecessary complexity and redundant computation; to make matters worse, they introduce overfitting as the number of layers increases. In this paper, we present simplified multilayer graph convolutional networks with dropout (DGCs), novel neural network architectures that successively perform nonlinearity removal and weight matrix merging between graph conventional layers, leveraging a dropout layer to achieve feature augmentation and effectively reduce overfitting. Under such circumstances, first, we extend a shallow GCN to a multilayer GCN. Then, we reduce the complexity and redundant calculations of the multilayer GCN, while improving its classification performance. Finally, we make DGCs readily applicable to inductive and transductive tasks. Extensive experiments on citation networks and social networks offer evidence that the proposed model matches or outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yang, Fei and Zhang, Huyin and Tao, Shiming},
  doi          = {10.1007/s10489-021-02617-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4776-4791},
  shortjournal = {Appl. Intell.},
  title        = {Simplified multilayer graph convolutional networks with dropout},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Research on multi-source POI data fusion
based on ontology and clustering algorithms. <em>APIN</em>,
<em>52</em>(5), 4775. (<a
href="https://doi.org/10.1007/s10489-021-02791-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Cai, Li and Zhu, Longhao and Jiang, Fang and Zhang, Yihan and He, Jing},
  doi          = {10.1007/s10489-021-02791-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4775},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Research on multi-source POI data fusion based on ontology and clustering algorithms},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Research on multi-source POI data fusion based on ontology
and clustering algorithms. <em>APIN</em>, <em>52</em>(5), 4758–4774. (<a
href="https://doi.org/10.1007/s10489-021-02561-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional point-of-interest (POI) data are collected by professional surveying and mapping organizations and are distributed in electronic maps. With the booming Internet and the development of crowdsourcing, the POI data defined in various formats are issued by some Internet companies and non-profit organizations. Due to the multiple sources and diverse formats of POI data, some problems occur in the data fusion process, such as conceptual definition differences, inconsistent classification, inefficient fusion algorithms, inaccurate fusion results, etc. To overcome the challenges of multi-source POI data fusion, this paper proposes a standardized POI data model and an ontology-based POI category system. Furthermore, a fusion framework and a fusion algorithm based on a two-stage clustering approach are proposed. The proposed method is compared with existing algorithms using datasets of different sizes, including POI surveying and mapping data from Kunming, China, Weibo check-in POI data, and real estate POI data. The experimental results demonstrate that the fusion effects of the proposed algorithm are superior to those of existing algorithms in terms of different evaluation indexes and operational efficiency.},
  archive      = {J_APIN},
  author       = {Cai, Li and Zhu, Longhao and Jiang, Fang and Zhang, Yihan and He, Jing},
  doi          = {10.1007/s10489-021-02561-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4758-4774},
  shortjournal = {Appl. Intell.},
  title        = {Research on multi-source POI data fusion based on ontology and clustering algorithms},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diverse dialogue generation by fusing mutual persona-aware
and self-transferrer. <em>APIN</em>, <em>52</em>(5), 4744–4757. (<a
href="https://doi.org/10.1007/s10489-021-02660-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, personalized dialogue generation has attracted researchers’ attention due to its wide range of applications. Although there has been much excellent research, current chatbots are known to generate uninteresting responses without personality. In a general way, persona information is adopted to mitigate these issues. Yet, how to integrate persona information reasonably based on historical dialogue in responses is still a severe challenge. Thus, we present a solution that consolidates the persona information memory process and pretrained method to generate personalized and fluency responses. Moreover, we adopt a reinforcement learning algorithm to joint each part of the dialogue model. Last but not least, we present a self-learning framework to explore the hypothetical space, and make the responses more personalized and fascinating. Extensive experiments on the large-scale dialogue public dataset ConvAI2 verify the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Xu, Fuyong and Xu, Guangtao and Wang, Yuanying and Wang, Ru and Ding, Qi and Liu, Peiyu and Zhu, Zhenfang},
  doi          = {10.1007/s10489-021-02660-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4744-4757},
  shortjournal = {Appl. Intell.},
  title        = {Diverse dialogue generation by fusing mutual persona-aware and self-transferrer},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of interpretability methods for multivariate time
series forecasting. <em>APIN</em>, <em>52</em>(5), 4727–4743. (<a
href="https://doi.org/10.1007/s10489-021-02662-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to interpret a model’s predictions is a crucial task in many machine learning applications. Specifically, local interpretability is important in determining why a model makes particular predictions. Despite the recent focus on interpretable Artificial Intelligence (AI), there have been few studies on local interpretability methods for time series forecasting, while existing approaches mainly focus on time series classification tasks. In this study, we propose two novel evaluation metrics for time series forecasting: Area Over the Perturbation Curve for Regression and Ablation Percentage Threshold. These two metrics can measure the local fidelity of local explanation methods. We extend the theoretical foundation to collect experimental results on four popular datasets. Both metrics enable a comprehensive comparison of numerous local explanation methods, and an intuitive approach to interpret model predictions. Lastly, we provide heuristical reasoning for this analysis through an extensive numerical study.},
  archive      = {J_APIN},
  author       = {Ozyegen, Ozan and Ilic, Igor and Cevik, Mucahit},
  doi          = {10.1007/s10489-021-02662-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {5},
  pages        = {4727-4743},
  shortjournal = {Appl. Intell.},
  title        = {Evaluation of interpretability methods for multivariate time series forecasting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attack-less adversarial training for a robust adversarial
defense. <em>APIN</em>, <em>52</em>(4), 4364–4381. (<a
href="https://doi.org/10.1007/s10489-021-02523-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have proved efficacious in fooling deep neural networks recently. Many researchers have studied this issue of adversarial examples by evaluating neural networks against their attack techniques and increasing the robustness of neural networks with their defense techniques. To the best of our knowledge, adversarial training is one of the most effective defense techniques against the adversarial examples. However, the method is not able to cope with new attacks because it requires attack techniques in the training phase. In this paper, we propose a novel defense technique, Attack-Less Adversarial Training (ALAT) method, which is independent from any attack techniques, thereby is useful in preventing future attacks. Specifically, ALAT regenerates every pixel of an image into different pixel value, which commonly eliminates the majority of the adversarial noises in the adversarial example. This pixel regeneration is useful in defense because the adversarial noises are the core problem that make the neural networks produce high misclassification rate. Our experiment results with several benchmark datasets show that our method not only relieves over-fitting issue during the training of neural networks with a large number of epochs, but also boosts the robustness of the neural network.},
  archive      = {J_APIN},
  author       = {Ho, Jiacang and Lee, Byung-Gook and Kang, Dae-Ki},
  doi          = {10.1007/s10489-021-02523-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4364-4381},
  shortjournal = {Appl. Intell.},
  title        = {Attack-less adversarial training for a robust adversarial defense},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient intuitionistic fuzzy MULTIMOORA approach based
on novel aggregation operators for the assessment of solid waste
management techniques. <em>APIN</em>, <em>52</em>(4), 4330–4363. (<a
href="https://doi.org/10.1007/s10489-021-02541-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper intends to propose a multi-attribute group decision making (MAGDM) methodology based on MULTIMOORA under IFS theory for application in the assessment of solid waste management techniques. The present work is divided into three folds. The first fold is that some novel operational laws for intuitionistic fuzzy numbers are introduced and a series of aggregation operators based on them are developed. The properties related to new operations are discussed in detail. The second fold is that particle swarm optimization (PSO) algorithm is applied for attribute weight determination by formulating a non-linear optimization model with the goal of maximizing the distance of each alternative from negative ideal solution and minimizing the distance from positive ideal solution. Lastly, a MAGDM method based on MULTIMOORA is put forward and is applied in ranking different solid waste management techniques by taking various social, economical, environmental and technological factors into consideration. The reliability and effectiveness of the proposed methodology is explored by comparing the obtained results with several existing studies. The sensitivity analysis is done by taking different parameter values in order to show the stability of the proposed method.},
  archive      = {J_APIN},
  author       = {Garg, Harish and Rani, Dimple},
  doi          = {10.1007/s10489-021-02541-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4330-4363},
  shortjournal = {Appl. Intell.},
  title        = {An efficient intuitionistic fuzzy MULTIMOORA approach based on novel aggregation operators for the assessment of solid waste management techniques},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse data augmentation based on encoderforest for brain
network classification. <em>APIN</em>, <em>52</em>(4), 4317–4329. (<a
href="https://doi.org/10.1007/s10489-021-02579-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain network classification has attracted increasing attention with the widespread application in the automatic diagnosis of brain diseases. However, limited by the higher cost of detecting and marking for medical imaging, the amount of brain network data is usually small, which largely restricts the performance of current brain network classification models. In this paper, we propose a new sparse data augmentation model (SDAM) based on EncoderForest to effectively enhance the brain network data and improve the classification performance. The EncoderForest based SDAM uses a generator which innovatively encodes the rules of a set of parallel decision trees to generate sparse data with only discriminative connections. The generated data expands the original data set effectively by utilizing the advantages of EncoderForest in learning data feature sparsely and constructing a feature association generation model compactly. In addition, the SDAM is flexible to combine with different classification models, such as random forest, support vector machine, deep neural network, etc. The experimental results on three common brain disease data sets show that our model is able to reasonably augment the brain network data and remarkably improve the performance of various classifiers.},
  archive      = {J_APIN},
  author       = {Ji, Junzhong and Wang, Zihan and Zhang, Xiaodan and Li, Junwei},
  doi          = {10.1007/s10489-021-02579-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4317-4329},
  shortjournal = {Appl. Intell.},
  title        = {Sparse data augmentation based on encoderforest for brain network classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive spatial-temporal graph attention networks for
traffic flow forecasting. <em>APIN</em>, <em>52</em>(4), 4300–4316. (<a
href="https://doi.org/10.1007/s10489-021-02648-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow forecasting, which requires modelling involuted spatial and temporal dependence and uncertainty regarding road networks and traffic conditions, is a challenge for intelligent transportation systems (ITS). Recent studies have mainly focused on modelling spatial-temporal dependence through a fixed weighted graph based on prior knowledge. However, collecting up-to-date and accurate road information is costly. Moreover, is a single fixed graph enough to describe the correlation between sensors? The fixed weighted graph cannot directly relate to prediction tasks, which may result in considerable biases. To tackle this issue, in this paper, we propose a novel deep learning model framework: an adaptive spatial-temporal graph attention network (ASTGAT). Our ASTGAT simultaneously learns the dynamic graph structure and spatial-temporal dependency for traffic flow forecasting. Specifically, our framework consists of two joint training parts: a Network Generator model that generates a discrete graph with the Gumbel-Softmax technique and a Spatial-Temporal model that utilizes the generated network to predict traffic speed. Our Network Generator can adaptively infer the hidden correlations from data. Moreover, we propose a graph talking-heads attention layer (GTHA) for capturing spatial dependencies and design a gate temporal convolution (GTCN) layer for handling long temporal sequences. We evaluated our ASTGAT on two public datasets: METR-LA is collected in Los Angeles and PEMS-BAY is collected in California. Experimental results indicate that our ASTGAT outperforms the state-of-the-art (SOTA) baselines. Finally, to further describe our model, we visualize the forecasting results and the generated graph.},
  archive      = {J_APIN},
  author       = {Kong, Xiangyuan and Zhang, Jian and Wei, Xiang and Xing, Weiwei and Lu, Wei},
  doi          = {10.1007/s10489-021-02648-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4300-4316},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive spatial-temporal graph attention networks for traffic flow forecasting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Order dispatching for an ultra-fast delivery service via
deep reinforcement learning. <em>APIN</em>, <em>52</em>(4), 4274–4299.
(<a href="https://doi.org/10.1007/s10489-021-02610-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a real-life application of deep reinforcement learning to address the order dispatching problem of a Turkish ultra-fast delivery company, Getir. Before applying off-the-shelf reinforcement learning methods, we define the specific problem at Getir and one of the solutions the company has implemented. We discuss the novel aspects of Getir’s problem compared to the state-of-the-art order dispatching studies and highlight the limitations of Getir’s solution. The overall aim of the company is to deliver to as many customers as possible within 10 minutes. The orders arrive throughout the day, and centralized warehouses in the regions decide whether an incoming order should be served or canceled depending on their couriers’ shifts and status. We use Deep Q-networks to learn the actions of warehouses, i.e., accepting or canceling an order, directly from state dimensions using reinforcement learning. We design the networks with two different rewards. We conduct empirical analyses using real-life data provided by Getir to generate training samples and to assess the models’ performance during a selected 30-day period with a total of 9880 orders. The results indicate that our proposed models are able to generate policies that outperform the rule-based heuristic employed in practice.},
  archive      = {J_APIN},
  author       = {Kavuk, Eray Mert and Tosun, Ayse and Cevik, Mucahit and Bozanta, Aysun and Sonuç, Sibel B. and Tutuncu, Mehmetcan and Kosucu, Bilgin and Basar, Ayse},
  doi          = {10.1007/s10489-021-02610-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4274-4299},
  shortjournal = {Appl. Intell.},
  title        = {Order dispatching for an ultra-fast delivery service via deep reinforcement learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Primitive-contrastive network: Data-efficient
self-supervised learning from robot demonstration videos. <em>APIN</em>,
<em>52</em>(4), 4258–4273. (<a
href="https://doi.org/10.1007/s10489-021-02527-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the costly collection of expert demonstrations for robots, robot imitation learning suffers from the demonstration-insufficiency problem. A promising solution to this problem is self-supervised learning that leverages pretext tasks to extract general and high-level features from a relatively small amount of data. Since imitation learning tasks are typically composed of primitives (e.g., primary skills, such as grasping and reaching), learning representations of these primitives is crucial. However, existing methods have a weak ability to represent primitive, leading to unsatisfactory generalizability to learning scenarios with few data. To address this problem, we propose a novel primitive-contrastive network (PCN) and pretext task that optimizes the distances between pseudo-primitive distributions as a learning objective. Experimental results show that the proposed PCN can learn a more discriminative embedding space of primitives than existing self-supervised learning methods. Four representative robot manipulation experiments are conducted to demonstrate the superior data efficiency of the proposed method.},
  archive      = {J_APIN},
  author       = {Sun, Pengfei and Yang, Zhile and Zhang, Tianren and Guo, Shangqi and Chen, Feng},
  doi          = {10.1007/s10489-021-02527-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4258-4273},
  shortjournal = {Appl. Intell.},
  title        = {Primitive-contrastive network: Data-efficient self-supervised learning from robot demonstration videos},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Small object detection via dual inspection mechanism for UAV
visual images. <em>APIN</em>, <em>52</em>(4), 4244–4257. (<a
href="https://doi.org/10.1007/s10489-021-02512-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are utilized instead of humans to complete aerial assignments in various fields. With the development of computer vision, object detection has become one of the core technologies in UAV application. However, object detection of small targets often has missed detection, and the detection performance is far less than that of large targets. In this paper, we propose a dual inspection mechanism, which identifies missed targets in suspicious areas to assist single-stage detection branches, and shares dual decisions to make feature-level multi-instance detection modules produce reliable results. Firstly, the detection results contain missed targets is confirmed, which are in the part that does not reach the confidence threshold. For this reason, the feature vector provided by the denoising sparse autoencoder is calculated, and this part of the result is filtered again. Secondly, we empirically reveal that single detection results are not reliable enough, and the multiple attributes of the target need to be considered. Motivated by this, the initial and secondary detection results are combined and rank by importance. Finally, we give the corresponding confidence to the top-ranked instance, making it possible to become the object again. Experimental results reflect that our mechanism improves 2.7% mAP on the VisDrone2020 dataset, 1.0% mAP on the UAVDT dataset and 1.8% mAP on the MS COCO dataset. We propose detection mechanism which achieves state-of-the-art levels on these datasets and it performs better on small object detection.},
  archive      = {J_APIN},
  author       = {Tian, Gangyi and Liu, Jianran and Zhao, Hong and Yang, Wenyuan},
  doi          = {10.1007/s10489-021-02512-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4244-4257},
  shortjournal = {Appl. Intell.},
  title        = {Small object detection via dual inspection mechanism for UAV visual images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negation of BPA: A belief interval approach and its
application in medical pattern recognition. <em>APIN</em>,
<em>52</em>(4), 4226–4243. (<a
href="https://doi.org/10.1007/s10489-021-02641-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assignment of confidence mass appropriately is still an open issue in information fusion. The most popular rule of combination, Dempster’s rule of combination, has been widely used in various fields. In this paper, a belief interval negation is proposed based on belief interval. The belief interval has a stronger ability to express basic probability assignment (BPA) uncertainty. By establishing belief interval in an exhaustive frame of discernment (FOD), the negation is obtained in the form of a new interval. Belief interval negation as an essential tool for measuring uncertainty builds the relationship among BPA, belief interval and entropy. Furthermore, the new negation is applicable to various belief entropies and entropy increment is verified in negation iterations. Two novel uncertainty measures proposed in this paper are applicable to the newly proposed belief interval negation, too. Finally, convergent mass distribution is discussed. Some numerical examples and its application in medical pattern recognition are exhibited.},
  archive      = {J_APIN},
  author       = {Mao, Haiyi and Deng, Yong},
  doi          = {10.1007/s10489-021-02641-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4226-4243},
  shortjournal = {Appl. Intell.},
  title        = {Negation of BPA: A belief interval approach and its application in medical pattern recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bloat-aware GP-based methods with bloat quantification.
<em>APIN</em>, <em>52</em>(4), 4211–4225. (<a
href="https://doi.org/10.1007/s10489-021-02245-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic programming (GP) solves optimization problems by simulating the evolution procedure in nature. It has a serious problem termed as bloat, which can cost memory, hamper effective breeding and slow down the evolution process. However, there are only a limited number of works to quantify bloat directly, and existing techniques use the solution size/complexity as an indirect indicator for bloat control. Therefore, a new bloat quantification measure is designed in this work, based on which three bloat-aware GP methods are proposed. Specifically, the bloat quantification measure is incorporated with two parsimony pressure techniques and a multi-objective technique respectively, termed as GPLTSb (GP Lexicographic Tournament Selection bloat), GPPTSb (GP Proportional Tournament Selection bloat), and MOGPb (Multi-objective GP bloat). Unlike the existing bloat control methods, the bloat-aware methods apply the bloat values directly for bloat control. The proposed methods are tested on benchmark symbolic regression tasks, and are compared with GP, existing bloat control methods and four widely-used regression methods. Results show that MOGPb is effective for bloat control with the solution size reduced obviously; while GPLTSb and GPPTSb can also reduce bloat in GP with the solution size reduced slightly. In addition, compared with GP and existing bloat control methods, the proposed methods evolve solutions with similar/better regression performance. Moreover, the evolved solutions of proposed methods can outperform most reference regression methods for the given tasks consistently.},
  archive      = {J_APIN},
  author       = {Liang, Jiayu and Xue, Yu},
  doi          = {10.1007/s10489-021-02245-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4211-4225},
  shortjournal = {Appl. Intell.},
  title        = {Bloat-aware GP-based methods with bloat quantification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The arithmetics of two dimensional belief functions.
<em>APIN</em>, <em>52</em>(4), 4192–4210. (<a
href="https://doi.org/10.1007/s10489-021-02435-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world information is imperfect and usually characterized by uncertainty and partial reliability. Given these limitations, Z-numbers is introduced as a more suitable concept for describing real-world information. In recent years, Z-numbers has received immense attention. However, the general approach for computations over Z-number is too complex. Thus a simpler mathematical model has been proposed—Two Dimensional Belief Function (TDBF). A TDBF is an ordered pair of belief functions; the first belief function is on the frame of discernment of the values that a variable can take, and the second belief function is a measure of reliability of the first belief function. However, the processing of TDBF-based information requires a new theory to be developed, together with new approaches and procedures for computation with TDBF. In this paper, we propose a general framework for computations over TDBF, comprising addition, subtraction, multiplication, division, square and square root of TDBFs. In particular, a general approach to implementing combination over two TDBFs is also given. Our proposed method is validated by a variety of numerical examples, with an application in decision making problem.},
  archive      = {J_APIN},
  author       = {Li, Yangxue and Pelusi, Danilo and Cheong, Kang Hao and Deng, Yong},
  doi          = {10.1007/s10489-021-02435-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4192-4210},
  shortjournal = {Appl. Intell.},
  title        = {The arithmetics of two dimensional belief functions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-based benchmarking and visualization for large-scale
global optimization. <em>APIN</em>, <em>52</em>(4), 4161–4191. (<a
href="https://doi.org/10.1007/s10489-021-02637-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of optimization, visualization techniques can be useful for understanding the behaviour of optimization algorithms and can even provide a means to facilitate human interaction with an optimizer. Towards this goal, an image-based visualization framework, without dimension reduction, that visualizes the solutions to large-scale global optimization problems as images is proposed. In the proposed framework, the pixels visualize decision variables while the entire image represents the overall solution quality. This framework affords a number of benefits over existing visualization techniques including enhanced scalability (in terms of the number of decision variables), facilitation of standard image processing techniques, providing nearly infinite benchmark cases, and explicit alignment with human perception. To the best of the authors’ knowledge, this is the first realization of a dimension-preserving, scalable visualization framework that embeds the inherent relationship between decision space and objective space. The proposed framework is utilized with different mapping schemes on an image-reconstruction problem that encompass continuous, discrete, constrained, dynamic, and multi-objective optimization. The proposed framework is then demonstrated on arbitrary benchmark problems with known optima. Experimental results elucidate the flexibility and demonstrate how valuable information about the search process can be gathered via the proposed visualization framework. Results of a user survey strongly support that users perceive a correlation between objective fitness values and the quality of the corresponding images generated by the proposed framework.},
  archive      = {J_APIN},
  author       = {Harrison, Kyle Robert and Bidgoli, Azam Asilian and Rahnamayan, Shahryar and Deb, Kalyanmoy},
  doi          = {10.1007/s10489-021-02637-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4161-4191},
  shortjournal = {Appl. Intell.},
  title        = {Image-based benchmarking and visualization for large-scale global optimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nation-wide human mobility prediction based on graph neural
networks. <em>APIN</em>, <em>52</em>(4), 4144–4160. (<a
href="https://doi.org/10.1007/s10489-021-02645-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the anticipation of human mobility flow has important applications in many domains ranging from urban planning to epidemiology. Because of the high predictability of human movements, numerous successful solutions to perform such forecasting have been proposed. However, most focus on predicting human displacements on an intra-urban spatial scale. This study proposes a predictor for nation-wide mobility that allows anticipating inter-urban displacements at larger spatial granularity. For this goal, a Graph Neural Network (GNN) was used to consider the latent relationships among large geographical regions. The solution has been evaluated with an open dataset including trips throughout the country of Spain and the current weather conditions. The results indicate a high accuracy in predicting the number of trips for multiple time horizons, and more important, they show that our proposal only needs a single model for processing all the mobility areas in the dataset, whereas other techniques require a different model for each area under study.},
  archive      = {J_APIN},
  author       = {Terroso-Sáenz, Fernando and Muñoz, Andrés},
  doi          = {10.1007/s10489-021-02645-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4144-4160},
  shortjournal = {Appl. Intell.},
  title        = {Nation-wide human mobility prediction based on graph neural networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Fuzzy entropy and fuzzy support-based boosting random
forests for imbalanced data. <em>APIN</em>, <em>52</em>(4), 4126–4143.
(<a href="https://doi.org/10.1007/s10489-021-02620-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets with skewed class distribution bring difficulties to learning algorithms of pattern classification. The undersampling methods mostly consider the imbalanced ratio and rarely consider the distribution of the original dataset. Also, many algorithms separate the resampling of imbalanced data from classifier training, which may lead to the loss of important information and degradation of classifier performance. To address the mentioned problems, this paper proposes a boosting random forest based on fuzzy entropy and fuzzy support (FESBoost). The proposed algorithm mainly includes two parts, static undersampling and training of ensemble classifiers. First, the attenuation function and shared k-nearest neighbor algorithm are used to construct the global class entropy based on which the area where the majority samples are located is divided into a safe area and a boundary area. Second, density peak clustering (DPCA) is used to select representative samples of the safe area, and this process represents static resampling. Finally, the classifier is trained based on the boosting framework. Since the dataset is not balanced after static undersampling, before each iteration of the algorithm, data are undersampled again based on the global class entropy and the average class support. The number of undersampled samples depends on the number of iterations and the imbalance ratio. In the FESBoost algorithm, methods of static and dynamic resampling are used. Static resampling reduces the imbalance ratio of data and overlap between classes, as well as the classifier training cost. Based on data distribution and the possibility of sample misclassification, dynamic resampling updates the majority samples. The superiority of the proposed algorithm is verified experimentally on 9 synthetic datasets and 34 KEEL datasets. The proposed algorithm is also compared with seven algorithms, and the results show that the proposed algorithm has better generalization performance than other compared algorithms.},
  archive      = {J_APIN},
  author       = {Jiang, Mingxue and Yang, Youlong and Qiu, Haiquan},
  doi          = {10.1007/s10489-021-02620-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4126-4143},
  shortjournal = {Appl. Intell.},
  title        = {Fuzzy entropy and fuzzy support-based boosting random forests for imbalanced data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAMNer: Uncertainty-aware multimodal named entity
recognition in social media posts. <em>APIN</em>, <em>52</em>(4),
4109–4125. (<a
href="https://doi.org/10.1007/s10489-021-02546-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) on social media is a challenging task, as social media posts are usually short and noisy. Recently, some work explores different ways to incorporate the visual information from the image to improve NER on social media and achieves great success. However, existing methods ignore a common scenario on social media—the image sometimes does not match the posted text. Thus, the irrelevant images may introduce noisy information in existing models. In this paper, a novel uncertainty-aware framework for multimodal NER (UAMNer) on social media is put forward, which combines visual features with text when the text information is insufficient, thus suppressing noisy information from the irrelevant images. Specifically, we propose a two-stage label refinement framework for multimodal NER in social media posts. Given a multimodal post, we first use a bayesian neural network to produce candidate labels from the text. If the candidate labels have high uncertainty, we then use a multimodal transformer to refine the label with textual and visual features. We experiment on two public datasets, namely Twitter-2015 and Twitter-2017. The proposed method achieves better performance compared with the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liu, Luping and Wang, Meiling and Zhang, Mozhi and Qing, Linbo and He, Xiaohai},
  doi          = {10.1007/s10489-021-02546-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4109-4125},
  shortjournal = {Appl. Intell.},
  title        = {UAMNer: Uncertainty-aware multimodal named entity recognition in social media posts},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cooperative approach for combining particle swarm
optimization and differential evolution algorithms to solve
single-objective optimization problems. <em>APIN</em>, <em>52</em>(4),
4089–4108. (<a
href="https://doi.org/10.1007/s10489-021-02605-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper proposes a new algorithm designed for solving optimization problems. This algorithm is a hybrid of Differential Evolution (DE) and Particle Swarm Optimization (PSO) algorithms. The proposed algorithm uses a coalition or cooperation model in the game theory to combine the DE and PSO algorithms. This is done in an attempt to keep a balance between the exploration and exploitation capabilities by preventing population stagnation and avoiding the local optimum. The DE and PSO algorithms are two players in the state space, which play cooperative games together using the Nash bargaining theory to find the best solution. To evaluate the performance of the proposed algorithm, 25 benchmark functions are used in terms of the CEC2005 structure. The proposed algorithm is then compared with the classical DE and PSO algorithms and the hybrid algorithms recently proposed. The results indicated that the proposed hybrid algorithm outperformed the classical algorithms and other hybrid models.},
  archive      = {J_APIN},
  author       = {Dadvar, Marziyeh and Navidi, Hamidreza and Javadi, Hamid Haj Seyyed and Mirzarezaee, Mitra},
  doi          = {10.1007/s10489-021-02605-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4089-4108},
  shortjournal = {Appl. Intell.},
  title        = {A cooperative approach for combining particle swarm optimization and differential evolution algorithms to solve single-objective optimization problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lie group manifold analysis: An unsupervised domain
adaptation approach for image classification. <em>APIN</em>,
<em>52</em>(4), 4074–4088. (<a
href="https://doi.org/10.1007/s10489-021-02564-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to minimize the mismatch between the source domain in which models are trained and the target domain to which those models are applied. Most existing works focus on instance reweighting, feature representation, and classifier learning independently, which are ineffective when the domain discrepancy is substantially large. In this study, we propose a new unified hybrid approach that takes advantage of Lie group theory, weighted distribution alignment, and manifold alignment, which are referred to as Lie Group Manifold Analysis (LGMA). LGMA mainly finds a one-parameter sub-group decided by the Lie algebra elements of the intrinsic mean of all samples, and this one-parameter sub-group is a geodesic on the original Lie group. Moreover, the Lie group samples are projected onto the geodesics to maximize the separability of the projected samples for realizing discrimination in the nonlinear Lie group manifold space. As far as we know, LGMA is the first attempt to perform Lie algebra transformation to project the original features in the Lie group space onto Lie algebra manifold space for domain adaptation. Comprehensive experiments validate that our approach considerably outperforms competitive methods on real-world datasets.},
  archive      = {J_APIN},
  author       = {Yang, Hongwei and He, Hui and Zhang, Weizhe and Bai, Yawen and Li, Tao},
  doi          = {10.1007/s10489-021-02564-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4074-4088},
  shortjournal = {Appl. Intell.},
  title        = {Lie group manifold analysis: An unsupervised domain adaptation approach for image classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised weighting for averaged one-dependence
estimators. <em>APIN</em>, <em>52</em>(4), 4057–4073. (<a
href="https://doi.org/10.1007/s10489-021-02650-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Averaged one-dependence estimators (AODE) is a state-of-the-art machine learning tool for classification due to its simplicity, high computational efficiency, and excellent classification accuracy. Weighting provides an effective mechanism to ensemble superparent one-dependence estimators (SPODEs) in AODE by linearly aggregating their weighted probability estimates. Supervised weighting and unsupervised weighting are proposed to learn weights from labeled or unlabeled data, whereas their interoperability has not previously been investigated. In this paper, we propose a novel weighting paradigm in the framework of semi-supervised learning, called semi-supervised weighting (SSW). Two different versions of weighted AODEs, supervised weighted AODE (SWAODE) which performs weighting at training time and unsupervised weighted AODE (UWAODE) which performs weighting at classification time, are built severally. Log likelihood function is introduced to linearly aggregate the outcomes of these two weighted AODEs. The proposed algorithm, called SSWAODE, is validated on 38 benchmark datasets from the University of California at Irvine (UCI) machine learning repository and the experimental results prove the effectiveness and robustness of SSW for weighting AODE in terms of zero-one loss, bias, variance and etc. SSWAODE well achieves the balance between the ground-truth dependencies approximation and the effectiveness of probability estimation.},
  archive      = {J_APIN},
  author       = {Wang, Limin and Zhang, Shuai and Mammadov, Musa and Li, Kuo and Zhang, Xinhao and Wu, Siyuan},
  doi          = {10.1007/s10489-021-02650-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4057-4073},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised weighting for averaged one-dependence estimators},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Consensus of a new multi-agent system with impulsive
control which can heuristically construct the communication network
topology. <em>APIN</em>, <em>52</em>(4), 4041–4056. (<a
href="https://doi.org/10.1007/s10489-021-02644-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly studies the consensus of a new multi-agent system which can heuristically construct the communication network topology. Firstly, we presented some preparatory knowledge and constructed a dynamic model for the new multi-agent system. Secondly, we studied the control protocol. Inspired by the laws of nature, we designed some strategies for constructing the communication network topology heuristically, and these strategies were used in the control protocol. At the same time, in order to save the cost of communication and control, the impulsive control mechanism was also used in the control protocol. Thirdly, based on the dynamic model and the control protocol, we derived a state error system. Further, we used Lyapunov stability theory to analyze the state error system and obtained two theorems that enable the new multi-agent system to achieve consensus. Finally, we compared the proposed model with several related studies. Meanwhile, we also applied the results we obtained into the real scenario of symmetric secure communication to verify their practicability and efficiency.},
  archive      = {J_APIN},
  author       = {Hu, Xiang and Zhang, Zufan and Li, Chuandong},
  doi          = {10.1007/s10489-021-02644-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4041-4056},
  shortjournal = {Appl. Intell.},
  title        = {Consensus of a new multi-agent system with impulsive control which can heuristically construct the communication network topology},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-efficient multi-service task offloading scheduling for
mobile edge computing. <em>APIN</em>, <em>52</em>(4), 4028–4040. (<a
href="https://doi.org/10.1007/s10489-021-02549-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading in edge computing has become an effective way to expand the computing power of user equipment, since it migrates computing-intensive applications from user equipment to edge servers. The execution of a task may require multiple services. Today, many works study the edge computing about service placement or migration with single service tasks. However, it may not meet the need of applications on large scale. In this paper, we study a computational offloading method for multi-service tasks. Here, the execution of each task requires the collaboration of multiple services, and each service is indispensable. Specifically, we design an evaluation metric about system cost, and aim to find the decision to minimize this metric to solve the mobile edge computing (MEC) problem with multi-services tasks. Since this problem is NP-hard, we design the multi-service task computing offload algorithm (MTCOA) to realize the optimal solution. The simulation results show that the algorithm can effectively reduce the cost of computing offloading, and it has higher resource utilization than the existing algorithms.},
  archive      = {J_APIN},
  author       = {Song, Shudian and Ma, Shuyue and Zhao, Jingmei and Yang, Feng and Zhai, Linbo},
  doi          = {10.1007/s10489-021-02549-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4028-4040},
  shortjournal = {Appl. Intell.},
  title        = {Cost-efficient multi-service task offloading scheduling for mobile edge computing},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ba-PSO: A balanced PSO to solve multi-objective grid
scheduling problem. <em>APIN</em>, <em>52</em>(4), 4015–4027. (<a
href="https://doi.org/10.1007/s10489-021-02625-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a computational grid, environment is dynamic in nature and has distributed resources spread across multiple administrative domains. Therefore, it becomes necessary to provide an effective scheduling mechanism for the applications submitted to the computational grid. Particle Swarm Optimization (PSO) is very popular meta-heuristic in finding solutions to complex problems. Compared to other meta-heuristics, PSO has less parameters and better computational efficiency. In the paper, an advanced form of PSO i.e. balanced PSO (Ba-PSO) has been proposed to solve the scheduling problem of computational grid. The proposed algorithm decreases the jobs’ execution time and improves utilization of resources. The proposed, Ba-PSO, is scalable and works for small as well as large datasets. The role of a standard dataset is significant in testing a new algorithm because it helps in investigating the working of algorithm and provides important insights about the algorithm being tested. This paper uses a standard dataset generated by Czech National Grid Infrastructure i.e. Metacentrum. The proposed Ba-PSO algorithm is evaluated using the standard dataset and its results outperforms other considered deterministic and heuristic approaches.},
  archive      = {J_APIN},
  author       = {Ankita and Sahana, Sudip Kumar},
  doi          = {10.1007/s10489-021-02625-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {4015-4027},
  shortjournal = {Appl. Intell.},
  title        = {Ba-PSO: A balanced PSO to solve multi-objective grid scheduling problem},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SybilSort algorithm - a friend request decision tracking
recommender system in online social networks. <em>APIN</em>,
<em>52</em>(4), 3995–4014. (<a
href="https://doi.org/10.1007/s10489-021-02578-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sybils are detected and prevented from making friends on Online Social Networks (OSN) by the proposed SybilSort algorithm. SybilSort operates in the following manner: Task 1 (OSN registration): In order to register with OSN, a person must upload a Government Identity (GI). Task 2 (sort model): OSN is randomly clustered into equal-sized clusters. When one person receives a friend request from another, the decision to accept or reject is based on the dynamic thresholds determined by the cluster members and Cluster Monitor (CM). The person can then decide whether to accept or reject the request. If the variation is detected, the CM tracks the decision taken despite the recommendation, and the second suggestion is presented as an alert, along with proof of convergence, Sybil similarity, and priority assignment scores. Task 3 (cluster shuffle): The cluster is examined, and if there are any Sybil activities, the cluster is shuffled. To establish cluster shuffle, the cumulative difference is calculated using Belief Level Deviation (BLD), the Jaccard coefficient, and the Hamming distance. The research goal is to avoid the creation of the Sybil profile, to remove vote collisions, to track decisions after recommendation, and to avoid Sybil density. The proposed SybilSort is compared to existing Sybil detection algorithms using performance evaluations for varying numbers of persons and average network sizes. Using the 2019 Facebook dataset, the precision, recall, F1-score, and Region Of Curve (ROC) metrics are examined. The final results reveal that the proposed SybilSort algorithm significantly outperforms the already validated Sybil detection algorithms.},
  archive      = {J_APIN},
  author       = {Nedunchezhian, Poornima and Mahalingam, Murugan},
  doi          = {10.1007/s10489-021-02578-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3995-4014},
  shortjournal = {Appl. Intell.},
  title        = {SybilSort algorithm - a friend request decision tracking recommender system in online social networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). W-net and inception residual network for skin lesion
segmentation and classification. <em>APIN</em>, <em>52</em>(4),
3976–3994. (<a
href="https://doi.org/10.1007/s10489-021-02652-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma is a serious skin disease. Automatic recognition of this lesion by dermoscopic images is a difficult task. Recently, the deep learning paradigm has become a good alternative to detect different types of diseases. In this article, we propose a new deep learning system for melanoma detection. The system consists of three steps: pre-processing, segmentation and classification. The particularity of this work is the introduction of two new deep learning network architectures, W-net and Inception-Resnet, to solve, respectively, the problem of segmentation and the problem of classification. W-net is composed of a ResNet Encoder-Decoder, a ConvNet Encoder-Decoder and a Feature Pyramid network. The use of two concatenated encoder-decoder architectures has significantly improved the segmentation results. Inception-Resnet includes an Inception-Resnet block, which is a fusion of inception with the residual neural network. With this architecture, classification is much more robust. We evaluated the proposed system on the PH2 dataset and on the International Skin Imaging Collaboration datasets (ISIC 2016, ISIC 2017 and ISIC 2018). The results are discussed in terms of accuracy, sensitivity, specificity, dice coefficient and precision. The comparison of the proposed approach with other related work confirmed the advantages of our technique.},
  archive      = {J_APIN},
  author       = {Khouloud, Sahib and Ahlem, Melouah and Fadel, Touré and Amel, Slim},
  doi          = {10.1007/s10489-021-02652-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3976-3994},
  shortjournal = {Appl. Intell.},
  title        = {W-net and inception residual network for skin lesion segmentation and classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive visualization-based surveillance video synopsis.
<em>APIN</em>, <em>52</em>(4), 3954–3975. (<a
href="https://doi.org/10.1007/s10489-021-02636-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video synopsis is an effective technique for the efficient analysis of long videos in a short time. To generate a compact video, multiple tracks of moving objects, which we call as tubes are displayed simultaneously by rearranging them along the time axis. Contemporaneous video synopsis approaches focus on collision avoidance, or preservation of chronological order among tubes. However, generation of an adaptive personalized user-oriented synopsis video congruent to users’ preferences has yet not been thoroughly experimented. This paper propounds a framework for personalized visualization of synopsis video, integrating pertinent object attributes such as color, type, size, speed, travel path and direction towards generation of synopsis video for precise inference of user needs. The framework motivates users to interactively define queries for creation of the targeted synopsis. User queries are classified into visual-queries, temporal-queries, spatial-queries, and spatio-temporal queries concomitant with the visual and spatio-temporal attributes. Tubes relevant to a user-query are selected, and grouped according to original behavioral interactions followed by their rearrangement, to generate synopsis video with fewer false collisions. To evaluate the proffered technique, two evaluation metrics are proposed and extensive experiments of publicly available surveillance videos are conducted. The experimental results demonstrate the propriety and usability of the newer approach.},
  archive      = {J_APIN},
  author       = {Namitha, K. and Narayanan, Athi and Geetha, M.},
  doi          = {10.1007/s10489-021-02636-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3954-3975},
  shortjournal = {Appl. Intell.},
  title        = {Interactive visualization-based surveillance video synopsis},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new non-adaptive optimization method: Stochastic gradient
descent with momentum and difference. <em>APIN</em>, <em>52</em>(4),
3939–3953. (<a
href="https://doi.org/10.1007/s10489-021-02224-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive optimization methods (such as AdaGrad, RMSProp, Adam, and RAdam) and non-adaptive optimization methods (such as SGD and SGD with momentum) have recently been used in deep learning. The former has the characteristics of fast convergence speed but low convergence accuracy. The convergence speed of the latter is relatively slow, but the convergence accuracy is high. We propose a new non-adaptive method, stochastic gradient descent with momentum and difference (SGD(MD)), which is based on the idea of difference. We make a difference between the adjacent mini-batch gradient, and then update exponential moving averages of the gradient variations. That is, the exponential moving averages of the gradient variation is updated, and then the accumulated mean variation is added to the current gradient so as to adjust the convergence direction of our algorithm and accelerate its convergence speed. Compared with other popular optimization methods, the experimental results show that by using the method of difference, our SGD(MD) is significantly superior to SGD(M) and close to and sometimes even better than the adaptive optimization method including Adam and RAdam.},
  archive      = {J_APIN},
  author       = {Yuan, Wei and Hu, Fei and Lu, Liangfu},
  doi          = {10.1007/s10489-021-02224-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3939-3953},
  shortjournal = {Appl. Intell.},
  title        = {A new non-adaptive optimization method: Stochastic gradient descent with momentum and difference},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High average-utility itemsets mining: A survey.
<em>APIN</em>, <em>52</em>(4), 3901–3938. (<a
href="https://doi.org/10.1007/s10489-021-02611-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HUIM (High utility itemsets mining) is a sub-division of data mining dealing with the task to obtain promising patterns in the quantitative datasets. A variant of HUIM is to discover the HAUIM (High average-utility itemsets mining) where average-utility measure is used to obtain the utility of itemsets. HAUIM is the refined version of FIM (Frequent itemset mining) problem and has various applications in the field of market basket analysis, bio-informatics, text mining, network traffic analysis, product recommendation and e-learning among others. In this paper, we provide a comprehensive survey of the state-of-the-art methods of HAUIM to mine the HAUIs (High average-utility itemsets) from the static and dynamic datasets since the induction of the HAUIM problem. We discuss the pros and cons of each category of mining approaches in detail. The taxonomy of HAUIM is presented according to the mining approaches. Finally,various extensions, future directions and research opportunities of HAUIM algorithms are discussed.},
  archive      = {J_APIN},
  author       = {Singh, Kuldeep and Kumar, Rajiv and Biswas, Bhaskar},
  doi          = {10.1007/s10489-021-02611-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3901-3938},
  shortjournal = {Appl. Intell.},
  title        = {High average-utility itemsets mining: A survey},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal distributed parallel algorithms for deep learning
framework tensorflow. <em>APIN</em>, <em>52</em>(4), 3880–3900. (<a
href="https://doi.org/10.1007/s10489-021-02588-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its release, the Tensorflow framework has been widely used in various fields due to its advantages in deep learning. However, it is still at its early state. Its native distributed implementation has difficulty in expanding for large models because it has issues of low utilization of multiple GPUs and slow distribution compared with running on single machine. It is of great significance to reduce the training time through parallel models. In view of this, we firstly provided an in-depth analysis of the implementation principle of Tensorflow and identify the bottlenecks of its native distributed parallel models to improve. Then, two optimal algorithms are designed and implemented based on data parallelism and model parallelism modes of Tensorflow. For data parallelism, the proposed algorithm is implemented to replace the native linear execution mode with pipeline execution mode. As for model parallelism, the native random partitioning mode is replaced by our proposed novel greedy algorithm. Finally, we built a homogeneous distributed cluster and a heterogeneous distributed cluster respectively to verify the effectiveness of the proposed algorithms. Through a number of comparative experiments, we showed that the proposed optimal parallel algorithms can effectively reduce model training time by an average of 26.5%(or average 1.5x speedup than native distributed algorithms) and improve the utilization of the cluster while keeping the same accuracy level of native Tensorflow.},
  archive      = {J_APIN},
  author       = {Xie, Yuanlun and He, Majun and Ma, Tingsong and Tian, Wenhong},
  doi          = {10.1007/s10489-021-02588-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3880-3900},
  shortjournal = {Appl. Intell.},
  title        = {Optimal distributed parallel algorithms for deep learning framework tensorflow},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reference-guided deep deblurring via a selective attention
network. <em>APIN</em>, <em>52</em>(4), 3867–3879. (<a
href="https://doi.org/10.1007/s10489-021-02585-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deblurring is an important problem encountered in many image restoration tasks. To remove the motion blur of images captured from dynamic scenes, various Convolutional Neural Networks (CNNs) based methods are developed to restore the latent sharp image via an end-to-end trainable. However, these CNNs-based methods cannot restore enough structure details as no significant information is provided by the blurry input only. In this work, we propose a reference-guided deep deblurring method by incorporating the high-quality reference image into the deep network for better deblurring effect. Concretely, the correlation between the blurry input and the reference image is computed in the high-level feature space, and further represented by the similarity maps. To pick up the most relevant similarity maps to the input, the selective attention module is designed, and the selected similarity maps are decoded to reconstruct the deblurred sharp image. Extensive experimental results on two public datasets demonstrate that the proposed method achieves superior performance to existing methods quantitatively and qualitatively. More ablation experiments also validate that the favourable deblurred results can still be obtained even if the reference image is not similar with the input image.},
  archive      = {J_APIN},
  author       = {Li, Yaowei and Luo, Ye and Lu, Jianwei},
  doi          = {10.1007/s10489-021-02585-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3867-3879},
  shortjournal = {Appl. Intell.},
  title        = {Reference-guided deep deblurring via a selective attention network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-dynamic markov random fields for price outcome
prediction in the presence of lobbying. <em>APIN</em>, <em>52</em>(4),
3846–3866. (<a
href="https://doi.org/10.1007/s10489-021-02599-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a mathematical/Artificial Intelligence (AI) model for the prediction of price outcomes in markets with the presence of lobbying, whose outputs are pricing trends that aggregate the opinions of lobbies on future prices. Our proposal succeeds in unraveling this complex real-world problem by reducing the solution to straightforward probability computations. We tested our method on real olive oil prices (Andalusia, Spain) with encouraging results in a challenging sector, where opacity in the entry of oil shipments which are stored while waiting for the price to rise, makes it very difficult to forecast the prices. Specifically, understanding by minimum price that the price level is at least reached, specific formulas for computing the likelihood of both the aggregate and the minimum market price are provided. These formulas are based on the price levels that lobbies expect which in turn, can be calculated using the probability that each lobby gives to market prices. An innovative quantitative study of the lobbies is also carried out by explicitly computing the weight of each lobby in the process thus solving a problem for which there were only qualitative references up until now. The structural model is based on Time Dynamic Markov random fields (TD-MRFs). This model requires significantly less information to produce an output and enjoys transparency during the process when compared with other approaches, such as neural networks (known as black boxes). Transparency also ensures that the internal structures can be fine tuned to fit to each context as well as possible.},
  archive      = {J_APIN},
  author       = {Cabello, Julia García},
  doi          = {10.1007/s10489-021-02599-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3846-3866},
  shortjournal = {Appl. Intell.},
  title        = {Time-dynamic markov random fields for price outcome prediction in the presence of lobbying},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Steepest deep bipolar cascade correlation for finger-vein
verification. <em>APIN</em>, <em>52</em>(4), 3825–3845. (<a
href="https://doi.org/10.1007/s10489-021-02619-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finger-vein verification is a considerable problem to be addressed in a biometric system. A lot of research works have been intended for finger-vein authentication with aid of diverse data mining algorithms. But, the verification accuracy using conventional algorithms was minimal. Also, the time complexity involved during the finger-vein verification was maximal. To overcome the above drawbacks, Steepest Deep Bipolar Cascade Correlative Machine Learning (SDBCCML) technique is proposed. The proposed SDBCCML technique is designed to efficiently perform the finger-vein verification process when considering the large size of the dataset as input. The proposed SDBCCML technique contains three main components namely input, hidden, and output units for effective finger-vein authentication. The input unit in the proposed SDBCCML technique takes a number of finger vein images as input and then sent it to the hidden units. The proposed SDBCCML technique employs more numbers of hidden units with aiming to deeply learn the input finger vein images and thereby find the significant vein features by using the Gabor filter. Subsequently, the discovered vein features at hidden units are forwarded to the output unit. In the proposed SDBCCML technique, the output unit applies a bipolar activation function that compares the extracted vein features with pre-stored templates in the dataset. After that, the output unit gives the verification result. If the output unit result is +1, then the input finger vein image is classified as an authorized person. If the output unit result is −1, then the input finger vein image is classified as an unauthorized person. Thus, the main contribution of the proposed SDBCCML technique increases the authentication performance of finger-vein with higher accuracy and minimal time. The simulation of the proposed SDBCCML technique is conducted using metrics such as accuracy, time complexity, error rate, F-Score, and space complexity for a diverse number of finger-vein images.},
  archive      = {J_APIN},
  author       = {Muthusamy, Dharmalingam and Rakkimuthu, P.},
  doi          = {10.1007/s10489-021-02619-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3825-3845},
  shortjournal = {Appl. Intell.},
  title        = {Steepest deep bipolar cascade correlation for finger-vein verification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-aware recommendation model with dynamic
co-attention and attribute regularize. <em>APIN</em>, <em>52</em>(4),
3807–3824. (<a
href="https://doi.org/10.1007/s10489-021-02598-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As important information provided by recommender systems, knowledge graphs are widely applied in computer science and many other fields. The recommender system performance can be significantly improved by leveraging the knowledge graph between the user and item. Various recommendation approaches have been proposed based on the knowledge graph in recent years; however, most of the existing models only apply item-level user representations or attention mechanisms to users and items in the same way and ignore the fact that user and item attributes are significantly different. Hence, these models are not an effectively exploited attribute information and circumscribe the further improvement of recommender performance. In this paper, a novel approach of dynamic co-attention with an attribute regularizer (DCAR) for a knowledge-aware recommender system is proposed to explore the latent connections between the user level and item level. The model dynamically adjusts the dynamic co-attention mechanism through the attribute similarity between the target user and the candidate item. Specifically, an attribute regularizer between user and item is designed to improve the quality of attribute embedding. Experimental results on two realistic datasets show that our proposed model can significantly improve recommender system effectiveness and represents an advancement beyond the compared deep models.},
  archive      = {J_APIN},
  author       = {Yin, Guisheng and Chen, Fukun and Dong, Yuxin and Li, Gesu},
  doi          = {10.1007/s10489-021-02598-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3807-3824},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge-aware recommendation model with dynamic co-attention and attribute regularize},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New approaches for mining regular high utility sequential
patterns. <em>APIN</em>, <em>52</em>(4), 3781–3806. (<a
href="https://doi.org/10.1007/s10489-021-02536-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular pattern mining has been emerged as one of the promising sub-domains of data mining by discovering patterns with regular occurrences throughout a complete database. In contrast, utility-based pattern mining considers non-binary frequencies of items along with their importance values, and hence reveals more significance than traditional frequent pattern mining. Though regular patterns carry interesting knowledge, considering the utility values of the patterns would unveil more interesting and practical information. In sequence databases, the task of mining regular high utility patterns is more useful and challenging. In the recent time of big data, handling the incremental nature of databases to avoid mining from scratch when new updates appear, will bring effective results in a lot of applications. Moreover, databases can be dynamically updated in the form of data streams where new batches of data are added to the database at a higher rate. A window consisting of several recent batches can be of great interest to some end-users. To address all these important problems, here, we first introduce the concept of regular high utility sequential patterns and develop an algorithm for mining these patterns from static databases. Afterwards, we extend our algorithm to mine regular high utility sequential patterns from incremental databases and sliding-window based data streams. These two approaches produce approximate results in order to generate our intended patters faster and thus boost the performance. Extensive performance analyses of all the algorithms are observed over several real-life datasets and impressive results are found compared to the existing research.},
  archive      = {J_APIN},
  author       = {Ishita, Sabrina Zaman and Ahmed, Chowdhury Farhan and Leung, Carson K.},
  doi          = {10.1007/s10489-021-02536-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3781-3806},
  shortjournal = {Appl. Intell.},
  title        = {New approaches for mining regular high utility sequential patterns},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Dictionary learning and face recognition based on sample
expansion. <em>APIN</em>, <em>52</em>(4), 3766–3780. (<a
href="https://doi.org/10.1007/s10489-021-02557-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning has become a research hotspot. How to construct a robust dictionary is a key issue. In face recognition problem, differences in expressions, postures, and lighting conditions are key factors that affect the accuracy. Therefore, images of the same face can be very different in different situations. In real-world scenario, the samples of each face are very limited, which make it hard for the network to generalize well. Therefore, To solve the problem mentioned above, this paper proposes a method to construct a robust dictionary. In the method, virtual samples are generated to appropriately reflect the diversity of the face images, and based on this, two dictionaries are constructed and a corresponding fusion classification scheme is designed. The main advantages of this method are as follows: firstly, the simultaneous use of virtual samples and original samples can better reflect the facial appearance of each morphology, and the dictionaries obtained help to improve the robustness of the dictionary learning algorithm. Secondly, the proposed fusion classification scheme can give full play to the performance of the double dictionary learning algorithm. The results of out experiments show that the proposed algorithm is superior to some existing algorithms.},
  archive      = {J_APIN},
  author       = {Zhang, Yongjun and Liu, Wenjie and Fan, Haisheng and Zou, Yongjie and Cui, Zhongwei and Wang, Qian},
  doi          = {10.1007/s10489-021-02557-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3766-3780},
  shortjournal = {Appl. Intell.},
  title        = {Dictionary learning and face recognition based on sample expansion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized soft likelihood function in combining
multi-source belief distribution functions. <em>APIN</em>,
<em>52</em>(4), 3748–3765. (<a
href="https://doi.org/10.1007/s10489-021-02366-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood function has significant advantages in the fields of statistical inference. Based on this theory, Yager proposed a soft likelihood function to make it more widely used. However, Yager’s method can only deal with probabilities expressed by crisp values, and has strict restrictions on the form of data. Due to human subjectivity and lack of effective information, it is inevitable that data uncertainty will be involved. In order to deal with the uncertain data more flexibly and intuitively and solve the complex problems faced in real-world applications, a generalized soft likelihood function in combining multi-source belief distribution functions is proposed in this paper. Different from other existing methods, this paper uses a distribution function to represent uncertain information, which can retain more original information and improve the credibility of the results. The expectation and variance are used to rank the obtained evidences, and the evidence that contributes more to the results is ranked higher. Finally, the reliable likelihood results are obtained. The proposed method extends the method of Yager and can work well in more uncertain environment. Several numerical examples and comparative experimental simulation are used to illustrate the efficiency of the proposed soft likelihood function.},
  archive      = {J_APIN},
  author       = {Zhang, Pengdan and Zhu, Ruonan and Chen, Jiaqi and Kang, Bingyi},
  doi          = {10.1007/s10489-021-02366-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3748-3765},
  shortjournal = {Appl. Intell.},
  title        = {A generalized soft likelihood function in combining multi-source belief distribution functions},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A human-like decision intelligence for obstacle avoidance in
autonomous vehicle parking. <em>APIN</em>, <em>52</em>(4), 3728–3747.
(<a href="https://doi.org/10.1007/s10489-021-02653-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The autonomous vehicle parking problem has drawn increased attention in recent times. It can resolve the parking-related issues that involve minimizing human driving errors and saving fuel and time. In practice, parallel parking needs extra care when dynamicity is present in the environment. Many human drivers have the expertise to take quick action when obstacles appear on their driving path. An autonomous system should possess such intelligence that can mimic human-like behavior in the presence of obstacles. This work focuses on providing an intelligent autonomous parking design using a car-like mobile robot (CLMR) that efficiently parks the vehicle in dynamic environmental conditions. A novel fuzzy-based obstacle avoidance controller is proposed that integrates sensor information into the parking problem, obtained from the surrounding of a CLMR. Ultrasonic sensors’ arrangement and their grouping provide inputs for the fuzzy system. A fuzzy-based obstacle avoidance controller can execute intelligent parking like a human by avoiding static as well as moving obstacles. The proposed work is tested in different challenging environmental conditions. Simulation results demonstrate that the proposed algorithm accomplishes autonomous parallel parking reasonably well in the presence of static and moving obstacles and can be used to park the vehicle in real-time. The proposed work helps to solve the autonomous parking problem with safety, especially in dynamic environmental conditions.},
  archive      = {J_APIN},
  author       = {Nakrani, Naitik M. and Joshi, Maulin M.},
  doi          = {10.1007/s10489-021-02653-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3728-3747},
  shortjournal = {Appl. Intell.},
  title        = {A human-like decision intelligence for obstacle avoidance in autonomous vehicle parking},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Drug-target interaction prediction via an ensemble of
weighted nearest neighbors with interaction recovery. <em>APIN</em>,
<em>52</em>(4), 3705–3727. (<a
href="https://doi.org/10.1007/s10489-021-02495-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug-target interactions (DTI) via reliable computational methods is an effective and efficient way to mitigate the enormous costs and time of the drug discovery process. Structure-based drug similarities and sequence-based target protein similarities are the commonly used information for DTI prediction. Among numerous computational methods, neighborhood-based chemogenomic approaches that leverage drug and target similarities to perform predictions directly are simple but promising ones. However, existing similarity-based methods need to be re-trained to predict interactions for any new drugs or targets and cannot directly perform predictions for both new drugs, new targets, and new drug-target pairs. Furthermore, a large amount of missing (undetected) interactions in current DTI datasets hinders most DTI prediction methods. To address these issues, we propose a new method denoted as Weighted k-Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can WkNNIR estimate interactions of any new drugs and/or new targets without any need of re-training, but it can also recover missing interactions (false negatives). In addition, WkNNIR exploits local imbalance to promote the influence of more reliable similarities on the interaction recovery and prediction processes. We also propose a series of ensemble methods that employ diverse sampling strategies and could be coupled with WkNNIR as well as any other DTI prediction method to improve performance. Experimental results over five benchmark datasets demonstrate the effectiveness of our approaches in predicting drug-target interactions. Lastly, we confirm the practical prediction ability of proposed methods to discover reliable interactions that were not reported in the original benchmark datasets.},
  archive      = {J_APIN},
  author       = {Liu, Bin and Pliakos, Konstantinos and Vens, Celine and Tsoumakas, Grigorios},
  doi          = {10.1007/s10489-021-02495-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3705-3727},
  shortjournal = {Appl. Intell.},
  title        = {Drug-target interaction prediction via an ensemble of weighted nearest neighbors with interaction recovery},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving sample efficiency in multi-agent actor-critic
methods. <em>APIN</em>, <em>52</em>(4), 3691–3704. (<a
href="https://doi.org/10.1007/s10489-021-02554-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of multi-agent deep reinforcement learning (MADRL) is growing rapidly with the demand for large-scale real-world tasks that require swarm intelligence, and many studies have improved MADRL from the perspective of network structures or reinforcement learning methods. However, the application of MADRL in the real world is hampered by the low sample efficiency of the models and the high cost to collect data. To improve the practicability, an extension to the current training paradigm of MADRL that improves the sample efficiency is imperative. To this end, this paper proposes PEDMA, a flexible plugin unit for MADRL. It consists of three techniques: (i)Parallel Environments (PE), to accelerate the data acquisition; (ii)Experience Augmentation (EA), a novel data augmentation method that utilizes the permutation invariance property of the multi-agent system to reduce the cost of acquiring data; and (iii)Delayed Updated Policies (DUP), to improve the data utilization efficiency of the MADRL model. The proposed EA method could improve the performance, data efficiency, and convergence speed of MADRL models, which is theoretically and empirically demonstrated. Experiments on three multi-agent benchmark tasks show that the MAAC model trained with PEDMA outperforms the baselines and state-of-the-art algorithms, and ablation studies show the contribution and necessity of each component in PEDMA.},
  archive      = {J_APIN},
  author       = {Ye, Zhenhui and Chen, Yining and Jiang, Xiaohong and Song, Guanghua and Yang, Bowei and Fan, Sheng},
  doi          = {10.1007/s10489-021-02554-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3691-3704},
  shortjournal = {Appl. Intell.},
  title        = {Improving sample efficiency in multi-agent actor-critic methods},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning general temporal point processes based on dynamic
weight generation. <em>APIN</em>, <em>52</em>(4), 3678–3690. (<a
href="https://doi.org/10.1007/s10489-021-02590-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most real-world events relate to historical cases scholastically and chronologically. Some increase the probability of the next event in the future, while some do not. Many approaches leverage temporal point processes with explicitly defined probability intensity functions concerning time t and history ${\mathscr{H}}$ , such as Poisson Process and Hawkes Process, to measure the time-varying probability. However, fixed-form intensity functions can limit the performance of temporal point process models, owing to the lack of prior knowledge of required intensity functions and the complexity of the real world. Neural networks’ ability to approximate functions makes the neural temporal point process a promising approach compared with the traditional temporal point process. In the paper, we identify several drawbacks the previous works have in meeting the mathematical constraints before designing a matrix-multiplication-based model to tackle these drawbacks. The experimental results reveal that our model is superior to other neural temporal point processes with better mathematical interpretability and extrapolation capability.},
  archive      = {J_APIN},
  author       = {Liu, Sishun and Li, Li},
  doi          = {10.1007/s10489-021-02590-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3678-3690},
  shortjournal = {Appl. Intell.},
  title        = {Learning general temporal point processes based on dynamic weight generation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-hard voting combiner scheme to ensemble multi-class
probabilistic classifiers. <em>APIN</em>, <em>52</em>(4), 3653–3677. (<a
href="https://doi.org/10.1007/s10489-021-02447-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembling of probabilistic classifiers is a technique that has been widely applied in classification, allowing to build a new classifier combining a set of base classifiers. Of the different schemes that can be used to construct the ensemble, we focus on the simple majority vote (MV), which is one of the most popular combiner schemes, being the foundation of the meta-algorithm bagging. We propose a non-trainable weighted version of the simple majority vote rule that, instead of assign weights to each base classifier based on their respective estimated accuracies, uses the confidence level CL, which is the standard measure of the degree of support that each one of the base classifiers gives to its prediction. In the binary case, we prove that if the number of base classifiers is odd, the accuracy of this scheme is greater than that of the majority vote. Moreover, through a sensitivity analysis, we show in the multi-class setting that its resilience to the estimation error of the probabilities assigned by the classifiers to each class is greater than that of the average scheme. We also consider another simple measure of the degree of support that incorporates additional knowledge of the probability distribution over the classes, namely the modified confidence level MCL. The usefulness for bagging of the proposed weighted majority vote based on CL or MCL is checked through a series of experiments with different databases of public access, resulting that it outperforms the simple majority vote in the sense of a statistically significant improvement regarding two performance measures: Accuracy and Matthews Correlation Coefficient (MCC), while holding up against the average combiner, which majority vote does not, being less computationally demanding.},
  archive      = {J_APIN},
  author       = {Delgado, Rosario},
  doi          = {10.1007/s10489-021-02447-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3653-3677},
  shortjournal = {Appl. Intell.},
  title        = {A semi-hard voting combiner scheme to ensemble multi-class probabilistic classifiers},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video prediction: A step-by-step improvement of a video
synthesis network. <em>APIN</em>, <em>52</em>(4), 3640–3652. (<a
href="https://doi.org/10.1007/s10489-021-02500-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although focusing on the field of video generation has made some progress in network performance and computational efficiency, there is still much room for improvement in terms of the predicted frame number and clarity. In this paper, a depth learning model is proposed to predict future video frames. The model can predict video streams with complex pixel distributions of up to 32 frames. Our framework is mainly composed of two modules: a fusion image prediction generator and an image-video translator. The fusion picture prediction generator is realized by a U-Net neural network built by a 3D convolution, and the image-video translator is composed of a conditional generative adversarial network built by a 2D convolution network. In the proposed framework, given a set of fusion images and labels, the image picture prediction generator can learn the pixel distribution of the fitted label pictures from the fusion images. The image-video translator then translates the output of the fused image prediction generator into future video frames. In addition, this paper proposes an accompanying convolution model and corresponding algorithm for improving image sharpness. Our experimental results prove the effectiveness of this framework.},
  archive      = {J_APIN},
  author       = {Jing, Beibei and Ding, Hongwei and Yang, Zhijun and Li, Bo and Bao, Liyong},
  doi          = {10.1007/s10489-021-02500-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3640-3652},
  shortjournal = {Appl. Intell.},
  title        = {Video prediction: A step-by-step improvement of a video synthesis network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bi-directional class-wise adversaries for unsupervised
domain adaptation. <em>APIN</em>, <em>52</em>(4), 3623–3639. (<a
href="https://doi.org/10.1007/s10489-021-02609-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation relies on well-labeled auxiliary source domain information to get better performance on the unlabeled target domain. It has shown tremendous importance for various classification and segmentation problems. Classical methods rely on diminishing the domain discrepancy in the latent space but ignore class-wise information, which will lead to elimination of the inherent data structure. To avoid destroying the inherent structure during unsupervised domain adaptation, we propose a Bi-Directional Class-level Adversaries cross-domain model (BDCA) with two symmetric classifiers interpolating two latent spaces to build a tunnel between the source domain and target domain. Specifically, we propose a class-level discrepancy metric to enforce domain consistency during the trend of domain adaption. We also employ two symmetric classifiers that are collectively optimized to maximize the discrepancy on target sample prediction. Extensive experiments are conducted on four publicly available datasets (i.e. office-31, office-home, GTAV and Cityscapes) and two challenging computer vision prediction problems, i.e., image classification and semantic segmentation. Quantitative and qualitative results demonstrate the effectiveness of our proposed model.},
  archive      = {J_APIN},
  author       = {Yang, Guanglei and Ding, Mingli and Zhang, Yongqiang},
  doi          = {10.1007/s10489-021-02609-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3623-3639},
  shortjournal = {Appl. Intell.},
  title        = {Bi-directional class-wise adversaries for unsupervised domain adaptation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised learning for k-dependence bayesian
classifiers. <em>APIN</em>, <em>52</em>(4), 3604–3622. (<a
href="https://doi.org/10.1007/s10489-021-02531-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network classifiers (BNCs) are powerful tools for graphically encoding the dependency relationships among variables in a directed acyclic graph and reasoning under conditions of uncertainty. Ever increasing data quantity makes ever more urgent the need of BNCs that are highly scalable and can perform significantly better in terms of classification. Numerous approaches have been proposed to mine conditional dependencies among attributes implicated in labeled training data under the framework of supervised learning, whereas the specific characteristics of unlabeled testing instances receive less attention. That may lead to overfitting and degradation in classification performance. In this paper, we argue that the knowledge learned from labeled training dataset and that from unlabeled testing instance are complementary in nature. The testing instance is pre-assigned with any possible label to make it complete, then log-likelihood function is introduced and redefined to measure the extents to which the learned BNC fits training or testing data. Heuristic search strategy is applied to learn two kinds of arbitrary k-dependence BNCs (general BNC for modeling training dataset and local BNC for modeling testing instance), which will work as an ensemble to make the final prediction under the framework of semi-supervised learning. The experimental evaluation on 40 publicly available datasets from the UCI machine learning repository reveals that the proposed algorithm achieves competitive classification performance compared with state-of-the-art BNCs and their variants, such as CFWNB, WATAN, FKDB, SKDB and IWAODE.},
  archive      = {J_APIN},
  author       = {Wang, LiMin and Zhang, XinHao and Li, Kuo and Zhang, Shuai},
  doi          = {10.1007/s10489-021-02531-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3604-3622},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised learning for k-dependence bayesian classifiers},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An opponent-adaptive strategy to increase utility and
fairness in agents’ negotiation. <em>APIN</em>, <em>52</em>(4),
3587–3603. (<a
href="https://doi.org/10.1007/s10489-021-02638-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In automatic negotiation, intelligent agents try to reach the best deal possible on behalf of their owners. In previous studies, opponent modeling of a negotiator agent has been used to tune the final bid out of a group of bids chosen by the agent’s strategy. In this research, a time-based bidding strategy has been introduced, which uses the opponent model to concede more adaptively to the opponents, thereby achieving an improved utility, social welfare, and fairness for the agent. By modeling the preference profile of the opponent during the negotiation session, this strategy sets its concession factor proportional to the model. Experiments show that in comparison to state-of-the-art agents, this agent makes better agreements in terms of individual utility and social welfare in small and medium-sized domains and can, in some cases, increase the performance up to 10%. The proposed agent successfully gets the deal up to 37% closer to best social bids in terms of distance to the Pareto frontier and the Nash point. An implementation based on the proposed strategy was used in an agent called AgreeableAgent, which participated in the international ANAC 2018 and won first place in individual utility rankings.},
  archive      = {J_APIN},
  author       = {Mirzayi, Sahar and Taghiyareh, Fattaneh and Nassiri-Mofakham, Faria},
  doi          = {10.1007/s10489-021-02638-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3587-3603},
  shortjournal = {Appl. Intell.},
  title        = {An opponent-adaptive strategy to increase utility and fairness in agents’ negotiation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale feature aggregation network for image
super-resolution. <em>APIN</em>, <em>52</em>(4), 3577–3586. (<a
href="https://doi.org/10.1007/s10489-021-02593-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For single image super-resolution technology, Convolutional Neural Network (CNN) has an excellent capability to improve reconstruction results. However, the existing CNN-based SR methods maintain high-quality reconstruction with excessive amounts of parameters and very deep network structures, which not only leads to higher requirements for computational resource and memory storage but also makes it difficult to apply resource-constrained devices. To solve these problems, a multi-scale feature aggregation network (MFAN) is proposed in this paper. In the MFAN, global dual-path is put into use to forward the feature information in low resolution space and medium resolution space respectively. For the sake of enhancing the ability of MFAN feature extraction, we come up a with multi-scale feature extraction module (MSFE), which also contains a multi-path structure. This module is embedded in each global path for image feature extraction. Through the effective combination of global path and MSFE, multi-scale information is extracted and enhanced step by step. Moreover, we design an inter-scale feature projection module (ISFP) based on the back-projection mechanism to effectively integrate the multi-scale feature information from MSFE. Extensive experiments show that the proposed MFAN has achieved competitive results in the qualitative and quantitative evaluation.},
  archive      = {J_APIN},
  author       = {Chen, Wenlong and Yao, Pengcheng and Gai, Shaoyan and Da, Feipeng},
  doi          = {10.1007/s10489-021-02593-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3577-3586},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale feature aggregation network for image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). N2PN: Non-reference two-pathway network for low-light image
enhancement. <em>APIN</em>, <em>52</em>(4), 3559–3576. (<a
href="https://doi.org/10.1007/s10489-021-02627-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing low-light image enhancement methods produce unnatural, low-contrast and color-distorted results. This paper presents a non-reference two-pathway network to solve the above problems. First, the frequency decomposition block is designed to decompose the input image into structure and detail pathways. Then, these two pathways are processed by different networks to improve the brightness and enhance the contrast. For the structure pathway, a light enhancement net is utilized to map the decomposed structure component to a set of adjustment parameters, which can adjust the range of the pixels. For the detail pathway, a contrast enhancement net is proposed to map the decomposed detail component into a series of parameters, which are iteratively applied in logarithmic image processing to obtain the final enhanced image. In these two pathways, the guide mechanism is adopted to maintain the color and spatial consistency of the enhanced result. In addition, the proposed method does not require any reference images during training, and it is a lightweight network that achieves low-light image enhancement. Extensive experiments demonstrate the advantages of the proposed method compared with state-of-the-art methods in both subjective and objective assessments.},
  archive      = {J_APIN},
  author       = {Wu, Yahong and Song, Wanru and Zheng, Jieying and Liu, Feng},
  doi          = {10.1007/s10489-021-02627-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3559-3576},
  shortjournal = {Appl. Intell.},
  title        = {N2PN: Non-reference two-pathway network for low-light image enhancement},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Betweenness centrality-based community adaptive network
representation for link prediction. <em>APIN</em>, <em>52</em>(4),
3545–3558. (<a
href="https://doi.org/10.1007/s10489-021-02633-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction is a fundamental problem in biological network analysis, personalized recommendation, network evolution modeling, etc. It aims at discovering links in the network that are unknown, missing, or will be formed in the future. Network representation learning-based link prediction approaches have drawn extensive attention, due to its high efficiency. The previous approaches use random or hyper parameters to select nodes from neighbors or communities when generating walk sequences. However, they do not fully consider the contribution of nodes to the embedded representation and hence impairing the affect the role of community structure in link prediction. To overcome this limitation and utilize community structure, we propose a betweenness centrality-based community adaptive network representation for link prediction method called CALP, which forms network representation by using betweenness centrality to measure the different contribution of community nodes and neighbor nodes for embedding and then applies it to link prediction. CALP first divides the network into communities. Then, it selects a node from the community nodes or neighbor nodes to join the walk sequence by the contribution of the node to embedding. Finally, it generates the corresponding network representation for link prediction. Experiments on realistic networks such as Cora, Citeseer, etc. show that the accuracy of CALP is much better than other approaches.},
  archive      = {J_APIN},
  author       = {Zhou, Mingqiang and Jin, Haijiang and Wu, Quanwang and Xie, Hong and Han, Qizhi},
  doi          = {10.1007/s10489-021-02633-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3545-3558},
  shortjournal = {Appl. Intell.},
  title        = {Betweenness centrality-based community adaptive network representation for link prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight intelligent network intrusion detection system
using OCSVM and pigeon inspired optimizer. <em>APIN</em>,
<em>52</em>(4), 3527–3544. (<a
href="https://doi.org/10.1007/s10489-021-02621-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the widespread of Internet services, all around the world, service providers are facing a major problem defending their systems, especially from new breaches and attacks. Network Intrusion Detection System (NIDS) analyzes network packets and reports low-level security violations to system administrators. In large networks, these reports become unmanageable. Moreover, state-of-the-art systems suffer from high false alarms. A NIDS should be anomaly-based to have the ability to discover zero-day attacks. Most NIDSs proposed by researchers that were based on such techniques suffered from high false alarms. This paper introduces an intelligent lightweight IDS that has a low false alarm rate while maintaining a high detection rate. The proposed NIDS is a fusion between two main subsystems that work in parallel. Each subsystem is trained using One-Class Support Vector Machine (OCSVM). One of the systems is trained over normal packets, while the other is trained over attack packets. The results of both subsystems are combined to give a good judgment for each packet that passes through the network. The proposed NIDS has been evaluated and compared with state-of-the-art systems using three popular IDS datasets (KDDCUP-99, NSL-KDD, and UNSW-NB15) in terms of detection rate, accuracy, f-measure and false alarms. The results show that the proposed NIDS outperformed the examined IDSs proposed by the previous researches.},
  archive      = {J_APIN},
  author       = {Alazzam, Hadeel and Sharieh, Ahmad and Sabri, Khair Eddin},
  doi          = {10.1007/s10489-021-02621-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3527-3544},
  shortjournal = {Appl. Intell.},
  title        = {A lightweight intelligent network intrusion detection system using OCSVM and pigeon inspired optimizer},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale object detection for high-speed railway
clearance intrusion. <em>APIN</em>, <em>52</em>(4), 3511–3526. (<a
href="https://doi.org/10.1007/s10489-021-02534-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clearance intrusion of foreign objects is a great threat to high-speed railway operation safety. An accurate and fast detection system of foreign object intrusion is important. In this paper, a multi-scale foreign object detection algorithm named feature fusion CenterNet with variable focus multi-scale augmentation (FFCN-VFMS) is proposed for the high-speed railway clearance scene. We render the ground truth with two-dimensional Gaussian distributions to generate the confidence score of each region in the image. In addition, a variable focus multi-scale augmentation (VFMS) method is proposed for multi-scale object detection, which takes detection results as prior knowledge to find the range of subsequent detection that contains most small objects. Moreover, feature fusion CenterNet (FFCN) adopts bidirectional iterative deep aggregation (BiIDA) to fuse the features in different convolutional layers and a spatial pyramid pooling (SPP) module to fuse feature maps extracted by different receptive fields. Our method was tested on public PASCAL VOC2007 datasets and our railway clearance intrusion (RCI) datasets. In comparison with related methods, FFCN-VFMS achieves better performance than comparison detectors with respect to accuracy and speed.},
  archive      = {J_APIN},
  author       = {Tian, Runliang and Shi, Hongmei and Guo, Baoqing and Zhu, Liqiang},
  doi          = {10.1007/s10489-021-02534-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3511-3526},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale object detection for high-speed railway clearance intrusion},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust multi-objective visual bayesian personalized ranking
for multimedia recommendation. <em>APIN</em>, <em>52</em>(4), 3499–3510.
(<a href="https://doi.org/10.1007/s10489-021-02355-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning classifiers are susceptible to adversarial perturbations, and their existence raises security concerns with a focus on recommendation systems. While there is a substantial effort to investigate attacks and defensive techniques in recommendation systems, Basic Iterative perturbation strategies (BIM) have been under-researched in multimedia recommendation. In this work, we adapt the iterative approach for multimedia recommendation. We proposed a novel Dynamic Collaborative Filtering with Aesthetic (DCFA) approach which leverages aesthetic features of clothing images into a multi-objective pairwise ranking to capture consumer aesthetic taste at a specific time through adversarial training (ADCFA). The DCFA method extends visual recommendation to make three key contributions: (1) incorporate aesthetic features into multimedia recommender system to model consumers’ preferences in the aesthetic aspect. (2) Design a multi-objective personalized ranking for the visual recommendation. (3) Use the aesthetic features to optimize the learning strategy to capture the temporal dynamics of image aesthetic preferences. To reduce the impact of perturbation, we train a DCFA objective function using minimax adversarial training. Extensive experiments on three datasets demonstrate the effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Paul, Agyemang and Wu, Zhefu and Liu, Kai and Gong, Shufeng},
  doi          = {10.1007/s10489-021-02355-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3499-3510},
  shortjournal = {Appl. Intell.},
  title        = {Robust multi-objective visual bayesian personalized ranking for multimedia recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on highway vehicle detection based on faster r-CNN
and domain adaptation. <em>APIN</em>, <em>52</em>(4), 3483–3498. (<a
href="https://doi.org/10.1007/s10489-021-02552-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problems of the high missing detection rate of small target vehicles, the low detection ability and the single application scene when the traditional target detection model in the actual highway scene that due to factors such as bad weather, light changes, occlusion. This paper proposes an improved domain adaptive Faster R-CNN algorithm. By adding image-level and instance-level domain classifiers and consistency loss components to solve the problem of domain offset caused by the inconsistent distribution between training samples and actual samples. And the RPN network is improved by using multi-scale training and mining difficult samples for secondary training during the training process to improve the performance of the model. The improved model can increase the gain by 4.8%. The experimental results show that the domain adaptive component is effective for the migration between different sample domains, and the performance of small-scale target detection is significantly improved. The improved method can effectively improve the accuracy and robustness of the model, and has certain generalization ability.},
  archive      = {J_APIN},
  author       = {Yin, Guanxiang and Yu, Meng and Wang, Meng and Hu, Yong and Zhang, Yuejin},
  doi          = {10.1007/s10489-021-02552-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3483-3498},
  shortjournal = {Appl. Intell.},
  title        = {Research on highway vehicle detection based on faster R-CNN and domain adaptation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical analysis of the community lockdown for COVID-19
pandemic. <em>APIN</em>, <em>52</em>(4), 3465–3482. (<a
href="https://doi.org/10.1007/s10489-021-02615-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global pandemic of the COVID-19 continues, the statistical modeling and analysis of the spreading process of COVID-19 have attracted widespread attention. Various propagation simulation models have been proposed to predict the spread of the epidemic and the effectiveness of related control measures. These models play an indispensable role in understanding the complex dynamic situation of the epidemic. Most existing work studies the spread of epidemic at two levels including population and agent. However, there is no comprehensive statistical analysis of community lockdown measures and corresponding control effects. This paper performs a statistical analysis of the effectiveness of community lockdown based on the Agent-Level Pandemic Simulation (ALPS) model. We propose a statistical model to analyze multiple variables affecting the COVID-19 pandemic, which include the timings of implementing and lifting lockdown, the crowd mobility, and other factors. Specifically, a motion model followed by ALPS and related basic assumptions is discussed first. Then the model has been evaluated using the real data of COVID-19. The simulation study and comparison with real data have validated the effectiveness of our model.},
  archive      = {J_APIN},
  author       = {Wu, Shaocong and Wang, Xiaolong and Su, Jingyong},
  doi          = {10.1007/s10489-021-02615-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {4},
  pages        = {3465-3482},
  shortjournal = {Appl. Intell.},
  title        = {Statistical analysis of the community lockdown for COVID-19 pandemic},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified model for collective and point anomaly detection
using stacked temporal convolution networks. <em>APIN</em>,
<em>52</em>(3), 3118–3131. (<a
href="https://doi.org/10.1007/s10489-021-02559-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series anomaly detection utilizing deep learning methods is widely used in fraud detection, network intrusion detection, and medical anomaly detection. Most deep learning methods exclusively focus on models based on recurrent neural networks (RNNs), such as long short-term memory (LSTM) or gated recurring units (GRUs), rather than on models based on convolutional neural networks (CNNs) or integrated ones. Inspired by the success of CNN-based models in many scenarios, we propose a single model that can be used for detecting both collective and point anomalies using stacked temporal convolution networks (CPA-TCN). Compared with state-of-the-art models, the CPA-TCN model boasts the following advantages. First, the CPA-TCN model reconstructs sequential features with current inputs and historical features and is only trained on normal datasets. Second, the CPA-TCN model outperforms RNN-based models in terms of speed and accuracy across diverse tasks and datasets and demonstrates more effective memory. Third, the CPA-TCN model can effectively detect both collective and point anomalies by detecting point anomalies before collective anomalies, overcoming the shortcomings of models that can either detect point or collective anomalies. Fourth, the two-part anomaly detection module can significantly improve the accuracy of point anomaly detection. Extensive experiments on real-world datasets demonstrate that our CPA-TCN model achieves better prediction results with the ROC-AUC of 98%–99% compared to state-of-the-art methods and thus has a competitive advantage.},
  archive      = {J_APIN},
  author       = {Li, Zehui and Xiang, Zhijie and Gong, Weijia and Wang, Hong},
  doi          = {10.1007/s10489-021-02559-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3118-3131},
  shortjournal = {Appl. Intell.},
  title        = {Unified model for collective and point anomaly detection using stacked temporal convolution networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OGM: Online gaussian graphical models on the fly.
<em>APIN</em>, <em>52</em>(3), 3103–3117. (<a
href="https://doi.org/10.1007/s10489-021-02563-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Graphical Model is widely used to understand the dependencies between variables from high-dimensional data and can enable a wide range of applications such as principal component analysis, discriminant analysis, and canonical analysis. With respect to the streaming nature of big data, we study a novel Online Gaussian Graphical Model (OGM) that can estimate the inverse covariance matrix over the high-dimensional streaming data, in this paper. Specifically, given a small number of samples to initialize the learning process, OGM first estimates a low-rank estimation of inverse covariance matrix; then, when each individual new sample arrives, it updates the estimation of inverse covariance matrix using a low-complexity updating rule, without using the past data and matrix inverse. The significant edges of Gaussian graphical models can be discovered through thresholding the inverse covariance matrices. Theoretical analysis shows the convergence rate of OGM to the true parameters is guaranteed under Bernstein-style with mild conditions. We evaluate OGM using extensive experiments. The evaluation results backup our theory.},
  archive      = {J_APIN},
  author       = {Yang, Sijia and Xiong, Haoyi and Zhang, Yunchao and Ling, Yi and Wang, Licheng and Xu, Kaibo and Sun, Zeyi},
  doi          = {10.1007/s10489-021-02563-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3103-3117},
  shortjournal = {Appl. Intell.},
  title        = {OGM: Online gaussian graphical models on the fly},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MoMAC: Multi-objective optimization to combine multiple
association rules into an interpretable classification. <em>APIN</em>,
<em>52</em>(3), 3090–3102. (<a
href="https://doi.org/10.1007/s10489-021-02595-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial characteristic of machine learning models in various domains (such as medical diagnosis, financial analysis, or real-time process monitoring) is the interpretability. The interpretation supports humans in understanding the meaning behind every single prediction made by the machine, and enables the user to assess trustworthiness before acting on the predictions. This article presents our work in building an interpretable classification model based on association rule mining and multi-objective optimization. The classification model itself is a rule list, making a single prediction based on multiple rules. The rule list consists of If ... THEN statements that are understandable to humans. We choose these rules from a large set of pre-mined rules according to an interestingness measure which is formulated as a function of basic probabilities related to the rules. We learned the interestingness measure through multi-objective optimization, concentrating on two objectives: the classifier’s size in terms of number of rules and prediction accuracy. The model is called MoMAC, “Multi-Objective optimization to combine Multiple Association rules into an interpretable Classification”. The experimental results on benchmark datasets demonstrate that MoMAC outperforms other existing rule-based classification methods in terms of classification accuracy.},
  archive      = {J_APIN},
  author       = {Bui-Thi, Danh and Meysman, Pieter and Laukens, Kris},
  doi          = {10.1007/s10489-021-02595-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3090-3102},
  shortjournal = {Appl. Intell.},
  title        = {MoMAC: Multi-objective optimization to combine multiple association rules into an interpretable classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-stage attention and center triplet loss for person
re-identication. <em>APIN</em>, <em>52</em>(3), 3077–3089. (<a
href="https://doi.org/10.1007/s10489-021-02511-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification(Re-ID) has been a hot topic in the field of computer vision for the past few years. In order to solve the problem of misalignment between different images, most of the existing algorithms use a certain method to manually divide the image into several parts(such as uniform block, segmentation, and pose estimation model), and then extract local features through a multi-branch network structure. However, the compulsory region division does not give full play to the advantages of the deep learning network’s automatic feature extraction, and will bring additional calculations. This paper designs the architecture of Multi-stage Attention which achieves the goal of automatically extracting the discriminative features with a single branch structure. Meanwhile, there are few such studies that enhance the power of discriminating features by designing loss functions. Triplet loss, one of the most commonly used loss functions, suffers from difficulty of mining hard triplets and time consuming. To address this issue, we propose an innovative loss function, namely Center Triplet Loss(CTL), to learn a center of each class and to find the closest negative sample to the center. The triplet of CTL consists of an anchor and the corresponding center, and the closest negative sample. As a result, the model is easy to train and stable because the sample pairs are no longer randomly selected. Our algorithm(MACTL: Multi-stage Attention and Center Triplet Loss for Person Re-Identication) outperforms state-of-the-art(sota) on the datasets of Market-1501 and DukeMTMC-reID, with less branches and more stable.},
  archive      = {J_APIN},
  author       = {Zhao, Dandan and Chen, Chunyu and Li, Dongfang},
  doi          = {10.1007/s10489-021-02511-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3077-3089},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stage attention and center triplet loss for person re-identication},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel dense capsule network based on dense capsule layers.
<em>APIN</em>, <em>52</em>(3), 3066–3076. (<a
href="https://doi.org/10.1007/s10489-021-02630-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule network, which performs feature presentations for classification tasks via novel capsule forms, has attracted more and more attention. However, its performance on complex datasets has not been fully utilized. Through an in-depth exploration of Dense Convolutional Network (DenseNet), we propose a novel dense capsule network based on dense capsule layers, named DenseCaps. As far as we know, this is the first attempt to achieve a cross-capsule feature concatenations. This architecture enhances feature reuse by realizing dense connections at capsule-level, and captures different levels of detailed features to improve the performance on color datasets. Extensive experiments and ablation studies prove the proposed model achieves competitive results on multiple benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10, and SVHN).},
  archive      = {J_APIN},
  author       = {Sun, Guangcong and Ding, Shifei and Sun, Tongfeng and Zhang, Chenglong and Du, Wei},
  doi          = {10.1007/s10489-021-02630-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3066-3076},
  shortjournal = {Appl. Intell.},
  title        = {A novel dense capsule network based on dense capsule layers},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An intelligent fault diagnosis method for rotating machinery
based on data fusion and deep residual neural network. <em>APIN</em>,
<em>52</em>(3), 3051–3065. (<a
href="https://doi.org/10.1007/s10489-021-02555-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotating machinery is a very important mechanical device widely used in critical industrial applications. Efficient fault detection and diagnosis are key challenges in the maintenance and operational reliability of rotating machinery. To overcome this problem, a novel fault diagnosis method for rotating machinery based on deep residual neural network (DRNN) and data fusion is proposed. First, the time domain and frequency domain features of the original signal are extracted through the Short-time Fourier transform (STFT) layer, and then the deep residual network and the fusion embedding layer are used to fuse the time domain, frequency domain and spatial domain features to obtain high-quality low-dimensional fusion features. Finally, the fault type is obtained through the classifier. The proposed method is applied to the fault diagnosis of rolling bearing and gearbox, and the performance of the model has been tested comprehensively, including model training test, anti-noise test, fault tolerance test. The results confirm that the proposed method is much more effective and robust for feature learning, model training, anti-noise, fault tolerance and fault diagnosis than other fusion learning methods and single sensor-based methods. This fully reflects the advantages of multi-source information fusion in ensuring the reliable operation of rotating machinery.},
  archive      = {J_APIN},
  author       = {Peng, Binsen and Xia, Hong and Lv, Xinzhi and Annor-Nyarko, M. and Zhu, Shaomin and Liu, Yongkuo and Zhang, Jiyu},
  doi          = {10.1007/s10489-021-02555-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3051-3065},
  shortjournal = {Appl. Intell.},
  title        = {An intelligent fault diagnosis method for rotating machinery based on data fusion and deep residual neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive parallel algorithm for finite language
decomposition. <em>APIN</em>, <em>52</em>(3), 3029–3050. (<a
href="https://doi.org/10.1007/s10489-021-02488-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computationally hard problem of finite language decomposition is investigated. A finite language L is decomposable if there are two languages L1 and L2 such that L = L1L2. Otherwise, L is prime. The main contribution of the paper is an adaptive parallel algorithm for finding all decompositions L1L2 of L. The algorithm is based on an exhaustive search and incorporates several original methods for pruning the search space. Moreover, the algorithm is adaptive since it changes its behavior based on the runtime acquired data related to its performance. Comprehensive computational experiments on more than 4000 benchmark languages generated over alphabets of various sizes have been carried out. The experiments showed that by using the power of parallel computing the decompositions of languages containing more than 200000 words can be found. Decompositions of languages of that size have not been reported in the literature so far.},
  archive      = {J_APIN},
  author       = {Jastrzęb, Tomasz and Czech, Zbigniew J. and Wieczorek, Wojciech},
  doi          = {10.1007/s10489-021-02488-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3029-3050},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive parallel algorithm for finite language decomposition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic pedestrian trajectory forecasting with LSTM-based
delaunay triangulation. <em>APIN</em>, <em>52</em>(3), 3018–3028. (<a
href="https://doi.org/10.1007/s10489-021-02562-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is important for understanding human social behavior. Since the complex nature of the crowd dynamics, it remains a challenging work. Recent studies based on LSTM or GAN have made great progress in sequence prediction, but they still suffer from limitations of modeling neighborhood and handling pedestrian interaction. To address these problems, we propose a conflict-avoiding approach to predict pedestrians’ trajectories based on the Delaunay triangulation graph, which can model the crowd hierarchically. Meanwhile, the middle-level semantic feature is adopted to represent pedestrians’ dynamic interactions in Delaunay triangulation graph. Besides, to evaluate the effect of an additional semantic feature for LSTM, we add an information selection mechanism of pedestrian motion which updates the cell state of LSTM with a new social conflict gate. Furthermore, the results on two public datasets, BIWI and UCY, reveal that the proposed conflict-avoiding approach is excellent in terms of stability and validity. Our experimental results demonstrate that our method can predict the same time span using shorter observation period than state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Ma, Qiulin and Zou, Qi and Huang, Yaping and Wang, Nan},
  doi          = {10.1007/s10489-021-02562-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3018-3028},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic pedestrian trajectory forecasting with LSTM-based delaunay triangulation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainable deep neural networks for novel viral genome
prediction. <em>APIN</em>, <em>52</em>(3), 3002–3017. (<a
href="https://doi.org/10.1007/s10489-021-02572-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral infection causes a wide variety of human diseases including cancer and COVID-19. Viruses invade host cells and associate with host molecules, potentially disrupting the normal function of hosts that leads to fatal diseases. Novel viral genome prediction is crucial for understanding the complex viral diseases like AIDS and Ebola. While most existing computational techniques classify viral genomes, the efficiency of the classification depends solely on the structural features extracted. The state-of-the-art DNN models achieved excellent performance by automatic extraction of classification features, but the degree of model explainability is relatively poor. During model training for viral prediction, proposed CNN, CNN-LSTM based methods (EdeepVPP, EdeepVPP-hybrid) automatically extracts features. EdeepVPP also performs model interpretability in order to extract the most important patterns that cause viral genomes through learned filters. It is an interpretable CNN model that extracts vital biologically relevant patterns (features) from feature maps of viral sequences. The EdeepVPP-hybrid predictor outperforms all the existing methods by achieving 0.992 mean AUC-ROC and 0.990 AUC-PR on 19 human metagenomic contig experiment datasets using 10-fold cross-validation. We evaluate the ability of CNN filters to detect patterns across high average activation values. To further asses the robustness of EdeepVPP model, we perform leave-one-experiment-out cross-validation. It can work as a recommendation system to further analyze the raw sequences labeled as ‘unknown’ by alignment-based methods. We show that our interpretable model can extract patterns that are considered to be the most important features for predicting virus sequences through learned filters.},
  archive      = {J_APIN},
  author       = {Dasari, Chandra Mohan and Bhukya, Raju},
  doi          = {10.1007/s10489-021-02572-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {3002-3017},
  shortjournal = {Appl. Intell.},
  title        = {Explainable deep neural networks for novel viral genome prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain person re-identification by hybrid supervised
and unsupervised learning. <em>APIN</em>, <em>52</em>(3), 2987–3001. (<a
href="https://doi.org/10.1007/s10489-021-02551-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the single-domain person re-identification (Re-ID) method has achieved great accuracy, the dependence on the label in the same image domain severely limits the scalability of this method. Therefore, cross-domain Re-ID has received more and more attention. In this paper, a novel cross-domain Re-ID method combining supervised and unsupervised learning is proposed, which includes two models: a triple-condition generative adversarial network (TC-GAN) and a dual-task feature extraction network (DFE-Net). We first use TC-GAN to generate labeled images with the target style, and then we combine supervised and unsupervised learning to optimize DFE-Net. Specifically, we use labeled generated data for supervised learning. In addition, we mine effective information in the target data from two perspectives for unsupervised learning. To effectively combine the two types of learning, we design a dynamic weighting function to dynamically adjust the weights of these two approaches. To verify the validity of TC-GAN, DFE-Net, and the dynamic weight function, we conduct multiple experiments on Market-1501 and DukeMTMC-reID. The experimental results show that the dynamic weight function can improve the performance of the models, and our method is better than many state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Pang, Zhiqi and Guo, Jifeng and Sun, Wenbo and Xiao, Yanbang and Yu, Ming},
  doi          = {10.1007/s10489-021-02551-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2987-3001},
  shortjournal = {Appl. Intell.},
  title        = {Cross-domain person re-identification by hybrid supervised and unsupervised learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Common-possible concept analysis: A granule description
viewpoint. <em>APIN</em>, <em>52</em>(3), 2975–2986. (<a
href="https://doi.org/10.1007/s10489-021-02499-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concepts are basic units of human cognitive activities and concept based granule description is one of the possible ways to realize explainable AI through information granules. However, the existing types of concepts cannot work effectively when it is necessary to simultaneously investigate the common attributes and possible attributes of granules. In order to solve this problem, we propose common-possible concept analysis based on a granule description viewpoint. Concretely, common-possible concepts are proposed, which can concurrently describe the common attributes and possible attributes of granules. Then, a method to acquire common-possible concepts from formal contexts is described. Besides, the connections between common-possible concepts and the other types of concepts are explored. Analysis results show that common-possible concept analysis can effectively deal with a type of problems and provides some details of granules different from that of formal concepts and property oriented concepts.},
  archive      = {J_APIN},
  author       = {Zhi, Huilai and Qi, Jianjun},
  doi          = {10.1007/s10489-021-02499-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2975-2986},
  shortjournal = {Appl. Intell.},
  title        = {Common-possible concept analysis: A granule description viewpoint},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bonobo optimizer (BO): An intelligent heuristic with
self-adjusting parameters over continuous spaces and its applications to
engineering problems. <em>APIN</em>, <em>52</em>(3), 2942–2974. (<a
href="https://doi.org/10.1007/s10489-021-02444-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an intelligent optimization technique, namely Bonobo Optimizer (BO), is proposed. It mimics several interesting reproductive strategies and social behaviour of Bonobos. Bonobos live in a fission-fusion type of social organization, where they form several groups (fission) of different sizes and compositions within the society and move throughout the territory. Afterward, they merge (fusion) again with their society members for conducting specific activities. Bonobos adopt four different reproductive strategies, like restrictive mating, promiscuous mating, extra-group mating, and consortship mating to maintain a proper harmony in the society. These natural strategies are mathematically modeled in the proposed BO to solve an optimization problem. The searching mechanism with self-adjusting controlling parameters of the BO is designed in such a way that it can cope with various situations efficiently, while solving a variety of problems. Moreover, fission-fusion strategy is followed to select the mating partner, which is a unique approach in the literature of meta-heuristics. The performance of BO has been tested on CEC’13 and CEC’14 test functions and compared to that of other efficient and popular optimization algorithms of recent times. The comparisons show some comparable results and statistically superior performances of the proposed BO. Besides these, five complex real-life optimization problems are solved using BO and the results are compared with those reported in the literature. Here also, the performance of BO is found to be either better or comparable than that of others. These results establish the applicability of proposed BO to solve optimization problems.},
  archive      = {J_APIN},
  author       = {Das, Amit Kumar and Pratihar, Dilip Kumar},
  doi          = {10.1007/s10489-021-02444-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2942-2974},
  shortjournal = {Appl. Intell.},
  title        = {Bonobo optimizer (BO): An intelligent heuristic with self-adjusting parameters over continuous spaces and its applications to engineering problems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PBDE: An effective post-processing method based on box
density for object detection. <em>APIN</em>, <em>52</em>(3), 2930–2941.
(<a href="https://doi.org/10.1007/s10489-021-02540-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An inevitable of object detection is the existence of false positive detection boxes. The existence of a large number of false detection boxes greatly reduces the precision and compromises the desired effect. In this paper, we propose a post-processing method named Prediction Box Density Evaluation (PBDE). During applying object detect models to actual application scenarios, we summarize box density characteristics of true positive (TP) and false positive (FP) boxes. Then we set a threshold of box density to filter out a large number of FP boxes. After applying the PBDE algorithm, we obtain a significant improvement in precision and F1-Score. We have verified the effectiveness of our post-processing method in different application scenarios and models. The entire algorithm is carried out on the post-processing of object detection. There is no need to change the original training method and network structure, which is of great practicality and generality.},
  archive      = {J_APIN},
  author       = {Li, Zhishan and Jia, Baozhi and He, Yifan and Xie, Lei},
  doi          = {10.1007/s10489-021-02540-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2930-2941},
  shortjournal = {Appl. Intell.},
  title        = {PBDE: An effective post-processing method based on box density for object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new multi-feature fusion based convolutional neural
network for facial expression recognition. <em>APIN</em>,
<em>52</em>(3), 2918–2929. (<a
href="https://doi.org/10.1007/s10489-021-02575-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using lightweight networks for facial expression recognition (FER) is becoming an important research topic in recent years. The key to the success of FER with lightweight networks is to explore the potentials of expression features in distinct abstract levels and regions, and design robust features to characterize the facial appearance. This paper proposes a lightweight network called Multi-feature Fusion Based Convolutional Neural Network (MFF-CNN), for image-based FER. The proposed model uses the Image Branch to extract both mid-level and high-level global features from the whole input image and utilizes the Patch Branch to extract local features from sixteen image patches of the original image. In MFF-CNN, feature selection based on L2 norm is performed to obtain more discriminative local features. Joint tuning is employed to integrate the two branches and fuse features. Experiment results on three widely used datasets, CK+, JAFFE and Oulu-CASIA show the proposed MFF-CNN outperforms the state-of-the-art methods in terms of average recognition accuracy. Compared to other competitive models with similar or larger number of parameters, our MFF-CNN improves the average recognition accuracy by 9.80% to 15.05%.},
  archive      = {J_APIN},
  author       = {Zou, Wei and Zhang, Dong and Lee, Dah-Jye},
  doi          = {10.1007/s10489-021-02575-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2918-2929},
  shortjournal = {Appl. Intell.},
  title        = {A new multi-feature fusion based convolutional neural network for facial expression recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated detection of cyclic alternating pattern and
classification of sleep stages using deep neural network. <em>APIN</em>,
<em>52</em>(3), 2903–2917. (<a
href="https://doi.org/10.1007/s10489-021-02597-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual sleep stages scoring by human experts is the current gold standard for sleep analysis. However, this method is tedious, time-consuming, prone to human errors, and unable to detect microstructure of sleep such as cyclic alternating pattern (CAP) which is an important diagnostic factor for the detection of sleep disorders such as insomnia and obstructive sleep apnea (OSA). The CAP is only observed as subtle changes in the electroencephalogram (EEG) signals during non-rapid eye movement (NREM) sleep, making it very difficult for human experts to discern. Hence, it is important to have an automated system developed using artificial intelligence for accurate and robust detection of CAP and sleep stages classification. In this study, a deep learning model based on 1-dimensional convolutional neural network (1D-CNN) is proposed for CAP detection and homogenous 3-class sleep stages classification, namely wakefulness (W), rapid eye movement (REM) and NREM sleep. The proposed model is developed using standardized EEG recordings. Our developed CNN network achieved good model performance for 3-class sleep stages classification with a classification accuracy of 90.46%. Our proposed model also yielded a classification accuracy of 73.64% using balanced CAP dataset, and sensitivity of 92.06% with unbalanced CAP dataset. Our proposed model correctly identified majority of A-phases which comprised of only 12.6% in the unbalanced dataset. The performance of the developed prototype is ready to be tested with more data before clinical application.},
  archive      = {J_APIN},
  author       = {Loh, Hui Wen and Ooi, Chui Ping and Dhok, Shivani G. and Sharma, Manish and Bhurane, Ankit A. and Acharya, U. Rajendra},
  doi          = {10.1007/s10489-021-02597-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2903-2917},
  shortjournal = {Appl. Intell.},
  title        = {Automated detection of cyclic alternating pattern and classification of sleep stages using deep neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A few-shot transfer learning approach using text-label
embedding with legal attributes for law article prediction.
<em>APIN</em>, <em>52</em>(3), 2884–2902. (<a
href="https://doi.org/10.1007/s10489-021-02516-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed an intelligent law article prediction method to address the data imbalance and missing value problems in law text analysis. The proposed method could predict the various articles of laws that are relevant to a case based on fact description. Typically, the prediction of articles of law has two problems. First, no uniform format exists for law texts. Therefore, some attributes and features may be missing. This problem is termed the missing value problem. Second, certain laws may be referred to infrequently. This problem is called the data imbalance problem. To solve the missing value problem, we applied nonstatic word embedding methods to obtain the weighted vector similarity between words. For addressing the data imbalance problem, we used a weight sharing classification layer to classify the labels according to the relevance of the fact vector to the law article vector of the vector space. We used frequently occurring articles of various laws to train a transfer learning model and shared the weight as the prior knowledge to low-frequency ones to improve classification performance. We compared the functionality of our approach with others for law article prediction. By transferring prior knowledge from frequent cases to rare ones, our method saves legal workers’ time by automatically inferring law articles for cases they seldom deal with. Our experimental results revealed that the proposed article prediction method outperformed the state-of-the-art few-shot article prediction method.},
  archive      = {J_APIN},
  author       = {Chen, Yuh-Shyan and Chiang, Shin-Wei and Wu, Meng-Luen},
  doi          = {10.1007/s10489-021-02516-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2884-2902},
  shortjournal = {Appl. Intell.},
  title        = {A few-shot transfer learning approach using text-label embedding with legal attributes for law article prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilayer feature fusion with parallel convolutional block
for fine-grained image classification. <em>APIN</em>, <em>52</em>(3),
2872–2883. (<a
href="https://doi.org/10.1007/s10489-021-02573-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image classification aims at classifying the image subclass under a certain category. It is a challenging task due to the similar features, different gestures and background interference of the images. A key issue in fine-grained image classification is to extract the discriminative regions of images accurately. This paper proposed a multilayer feature fusion (MFF) network with parallel convolutional block (PCB) mechanism to solve this problem. We use the bilinear matrix product to mix different layers’ feature matrixes and then add them to the fully connection layer and the softmax function. In addition, the original convolutional blocks are replaced by the proposed PCB, which has more effective residual connection ability in extracting the region of interest (ROI) and the parallel convolutions with different sizes of kernels. Experimental results on three international available fine-grained datasets demonstrate the effectiveness of the proposed model. Quantitative and visualized experimental results show that our model has higher classification precision compared with the state-of-the-arts ones. Our classification accuracy reaches 87.1%, 91.4% and 93.4% on the dataset CUB-200-2011, FGVC Aircraft and Stanford Cars, respectively.},
  archive      = {J_APIN},
  author       = {Wang, Lei and He, Kai and Feng, Xu and Ma, Xitao},
  doi          = {10.1007/s10489-021-02573-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2872-2883},
  shortjournal = {Appl. Intell.},
  title        = {Multilayer feature fusion with parallel convolutional block for fine-grained image classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel dynamic graph evolution network for salient object
detection. <em>APIN</em>, <em>52</em>(3), 2854–2871. (<a
href="https://doi.org/10.1007/s10489-021-02479-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advanced deep convolution neural networks (CNNs) based salient object detection (SOD) models still suffer from the coarse object edge. The traditional graph-based SOD models can preserve the object edge well benefitting from the superpixels, but they perform weaker in highlighting the whole object compared to recent deep learning models. To tackle this problem, we attempt to find a new way to address this issue under the framework of graph convolution networks (GCNs). Specifically, we first model the image as a set of superpixels and construct the graph structure by connecting the k nearest neighbors for each node. For the connected nodes, rather than only leveraging on the predefined edges, we propose a multi-relations edge convolution operation, expecting to learn multiple implicit relations in the pair-wise nodes and aggregate the information from their neighbors relying on the learned edges. Then, a channel-wise attention operation is also proposed to boost the intra-node massage propagation across channels within the same node. In order to optimize the graph structure from layer to layer, we learn a new metric to re-measure the distance between any pair of nodes, and the graph structure evolves dynamically by recomputing the k nearest neighbors in the saliency feature space at different layers. Finally, a residual structure is applied to enable our graph network to go as deep as CNNs models. The graph nodes (superpixels) inherently belonging to the same class will be ideally clustered together in the learned embedding space. Experiments show that this work is a good practice for designing GCNs for image SOD and achieves the comparable performance with the recent state-of-the-art deep CNNs-based models.},
  archive      = {J_APIN},
  author       = {Xu, Mingzhu and Fu, Ping and Liu, Bing and Yin, Hongtao and Li, Junbao},
  doi          = {10.1007/s10489-021-02479-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2854-2871},
  shortjournal = {Appl. Intell.},
  title        = {A novel dynamic graph evolution network for salient object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADA-INCVAE: Improved data generation using variational
autoencoder for imbalanced classification. <em>APIN</em>,
<em>52</em>(3), 2838–2853. (<a
href="https://doi.org/10.1007/s10489-021-02566-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the number of minority samples by data generation can effectively improve the performance of mining minority samples using a classifier in imbalanced problems. In this paper, we proposed an effective data generation algorithm for minority samples called the Adaptive Increase dimension of Variational AutoEncoder (ADA-INCVAE). Complementary to prior studies, a theoretical study is conducted from the perspective of multi-task learning to solve the posterior collapse for VAE. Afterward, by using the theoretical support, it proposed a novel training method by increasing the dimension of data to avoid the occurrence of posterior collapse. Aiming at restricting the range of synthetic data for different minority samples, an adaptive reconstruction loss weight is proposed according to the distance distribution of majority samples around the minority class samples. In the data generation stage, the generation proportion of different sample points is determined by the local information of the minority class. The experimental results based on 12 imbalanced datasets indicated that the algorithm could help the classifier to effectively improve F1-measure and G-mean, which verifies the effectiveness of synthetic data generated by ADA-INCVAE.},
  archive      = {J_APIN},
  author       = {Huang, Kai and Wang, Xiaoguo},
  doi          = {10.1007/s10489-021-02566-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2838-2853},
  shortjournal = {Appl. Intell.},
  title        = {ADA-INCVAE: Improved data generation using variational autoencoder for imbalanced classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label classifier for protein sequence using
heuristic-based deep convolution neural network. <em>APIN</em>,
<em>52</em>(3), 2820–2837. (<a
href="https://doi.org/10.1007/s10489-021-02529-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques are found very useful to classify sequential data in recent times. The protein sequences belong to the functional classes based on the structure of their sequences. The annotation task of protein sequences into corresponding functional classes is multi-label in nature. The primary structure of protein contains a notable amount of vast data compared to the other secondary, tertiary, and quaternary structures. The clustering-based techniques require expert domain knowledge from the extensive data samples. Traditional methods use the n-gram features of amino acids while ignoring the relationship of motifs and amino acid sequence. This paper proposes an efficient method to classify the proteins into their functional classes using a convolution neural network based on heuristic rules. The proposed approach works on the primary structure of protein sequences which considers the relationship among motifs and amino acids. The proposed approach also takes into account the amino acid locations in the protein sequence. The proposed approach considers the affinity information between amino acids and motifs. Along with achieving high performance in the classification of protein sequences, we propose a heuristic approach to improve the precision and recall of the individual functional classes. The proposed heuristic approach improves the performance and handles the data imbalance problem. The proposed approach is compared with other competitive approaches, and our approach provides better performance metrics in terms of precision, recall, AUC, and subset accuracy. The greatest challenge with multi-label classification is to handle the data imbalance, which appears due to variance in frequencies of the labels in the data. This data imbalance is dealt with weight modulation in the loss function to influence the learning process.},
  archive      = {J_APIN},
  author       = {Chauhan, Vikas and Tiwari, Aruna and Joshi, Niranjan and Khandelwal, Sahaj},
  doi          = {10.1007/s10489-021-02529-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2820-2837},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label classifier for protein sequence using heuristic-based deep convolution neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image super-resolution using global enhanced upscale
network. <em>APIN</em>, <em>52</em>(3), 2813–2819. (<a
href="https://doi.org/10.1007/s10489-021-02565-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current works on super-resolution have obtained satisfactory results since the advance of the convolution neural network. Nevertheless, most previous works use one network for one integer scale factor so ignore the super-resolution of the arbitrary scale factor. In this work, we propose a novel approach called Global Enhanced Upscale Network (GEUN) to tackle super-resolution with a single model adapting the arbitrary scale factor. In our GEUN, we propose the Global Enhanced Upscale module to replace the conventional upscale module. Our GEUN can upscale low-resolution images with an arbitrary scale factor through only one model. Extensive experimental results demonstrate the superiority of our GEUN.},
  archive      = {J_APIN},
  author       = {Du, Xiaobiao},
  doi          = {10.1007/s10489-021-02565-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2813-2819},
  shortjournal = {Appl. Intell.},
  title        = {Single image super-resolution using global enhanced upscale network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid spatial-spectral feature in broad learning system for
hyperspectral image classification. <em>APIN</em>, <em>52</em>(3),
2801–2812. (<a
href="https://doi.org/10.1007/s10489-021-02320-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) classification have aroused a great deal of attention recently due to their wide range of practical prospects in numerous fields. Spatial-spectral fusion feature is widely used in HSI classification to get better performance. These methods are mostly based on a simple linear addition with the combined hyper-parameter to fuse the spatial and spectral information. It is necessary to fuse the features in a more suitable method. To solve this problem, we propose a novel HSI classification approach based on Hybrid spatial-spectral feature in broad learning system (HSFBLS). First, we employ an adaptive weighted mean filter to obtain spatial feature. Computing the weights of spatial and spectral channels in hybrid module by two BLS and uniting them with a weighted linear function. Then, we fuse the spectral-spatial feature by sparse autoencoder to get weighted fusion feature as the feature nodes to classify HSI data in BLS. By a two-stage fusion of spatial and spectral information, it can increase the classification accuracy contrast to simple combination. Very satisfactory classification results on typical HSI datasets illustrate the availability of proposed HSFBLS. Moreover, HSFBLS also reduce training time greatly contrast to time-consuming network.},
  archive      = {J_APIN},
  author       = {Ma, You and Liu, Zhi and Chen Chen, C. L. Philip},
  doi          = {10.1007/s10489-021-02320-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2801-2812},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid spatial-spectral feature in broad learning system for hyperspectral image classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared target tracking based on proximal robust principal
component analysis method. <em>APIN</em>, <em>52</em>(3), 2785–2800. (<a
href="https://doi.org/10.1007/s10489-021-02414-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared target tracking plays an important role in both civil and military fields. The main challenges in designing a robust and high-precision tracker for infrared sequences include overlap, occlusion, and appearance change. To this end, this paper proposes an infrared target tracker based on the proximal robust principal component analysis method. Firstly, the observation matrix is decomposed into a sparse occlusion matrix and a low-rank target matrix, and the constraint optimization is carried out with an approaching proximal norm which is better than l1-norm. Then, the Alternating Direction Method of Multipliers (ADMM) is employed to solve this convex optimization problem by estimating the variables alternately. Finally, the framework of particle filter with model update strategy is exploited to locate the target. Through a series of experiments on real infrared target sequences, the effectiveness and robustness of our algorithm are proved.},
  archive      = {J_APIN},
  author       = {Ma, Chao and Wan, Minjie and Xu, Yunkai and Ren, Kan and Qian, Weixian and Chen, Qian and Gu, Guohua},
  doi          = {10.1007/s10489-021-02414-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2785-2800},
  shortjournal = {Appl. Intell.},
  title        = {Infrared target tracking based on proximal robust principal component analysis method},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time detector design for small targets based on
bi-channel feature fusion mechanism. <em>APIN</em>, <em>52</em>(3),
2775–2784. (<a
href="https://doi.org/10.1007/s10489-021-02545-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {YOLOv4-tiny is a simplified version of YOLOv4 detector, which is extremely fast and with few parameters. However, the detection performance of YOLOv4-tiny is poor while the recognition of small targets and occluded objects is weak. It is mainly attributed to the lack of feature extraction ability and learning ability of the backbone. Furthermore, the feature pyramid network (FPN) cannot adequately fuse adjacent feature maps in the process of multi-scale feature fusion. In this paper, a real-time detector with a bi-channel feature fusion mechanism is proposed based on YOLOv4-tiny, called BFF-YOLO, which effectively improves the detection of small targets and occluded objects. BFF-YOLO is composed of two main components: feature extraction and feature fusion. In the process of feature extraction, inspired by the idea of cross-stage partial connections (CSP), an enhanced CSP block (ECSPBlock) is proposed for enhancing the feature extraction of the backbone and the learning capability of the network. Moreover, the Maxpool layer in YOLOv4-tiny, which is used for downsampling and tends to lose fine-grained information, is replaced with a convolutional layer. In the process of feature fusion, a bi-channel feature fusion pyramid network (BFPN) is proposed to adequately fuse adjacent feature maps of different scales so that each detection head has both shallow and deep features. Finally, with a small increase in parameters, BFF-YOLO has achieved 36.5% AP and 85.1% mAP on the COCO and VOC datasets, respectively.},
  archive      = {J_APIN},
  author       = {Zhang, Xiuling and Wan, Tingbo and Wu, Ziyun and Du, Bingce},
  doi          = {10.1007/s10489-021-02545-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2775-2784},
  shortjournal = {Appl. Intell.},
  title        = {Real-time detector design for small targets based on bi-channel feature fusion mechanism},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal graph neural network for traffic
forecasting: An overview and open research issues. <em>APIN</em>,
<em>52</em>(3), 2763–2774. (<a
href="https://doi.org/10.1007/s10489-021-02587-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic forecasting plays an important role of modern Intelligent Transportation Systems (ITS). With the recent rapid advancement in deep learning, graph neural networks (GNNs) have become an emerging research issue for improving the traffic forecasting problem. Specifically, one of the main types of GNNs is the spatial-temporal GNN (ST-GNN), which has been applied to various time-series forecasting applications. This study aims to provide an overview of recent ST-GNN models for traffic forecasting. Particularly, we propose a new taxonomy of ST-GNN by dividing existing models into four approaches such as graph convolutional recurrent neural network, fully graph convolutional network, graph multi-attention network, and self-learning graph structure. Sequentially, we present experimental results based on the reconstruction of representative models using selected benchmark datasets to evaluate the main contributions of the key components in each type of ST-GNN. Finally, we discuss several open research issues for further investigations.},
  archive      = {J_APIN},
  author       = {Bui, Khac-Hoai Nam and Cho, Jiho and Yi, Hongsuk},
  doi          = {10.1007/s10489-021-02587-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2763-2774},
  shortjournal = {Appl. Intell.},
  title        = {Spatial-temporal graph neural network for traffic forecasting: An overview and open research issues},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social velocity based spatio-temporal anomalous daily
activity discovery of social media users. <em>APIN</em>, <em>52</em>(3),
2745–2762. (<a
href="https://doi.org/10.1007/s10489-021-02535-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous daily activities are the activities that do not fit into normal daily behavior of social media users. Discovering anomalous daily activities is important for protecting social media users from harmful content and providing correct information about populated accounts, products, or hashtags. However, discovering anomalous daily activities is challenging due to hardness of detection of bot applications, complexity of anomalous activities, and the big data nature of social media datasets. In this study, a novel method that discovers anomalous daily activities with respect to spatio-temporal information of social media datasets is proposed. For this purpose, an interest measure, named as social velocity, is proposed to discover anomalous daily activities that is based on spatial distance and temporal difference of successive posts. Two novel algorithms are proposed that use proposed method and interest measure and experimentally evaluated on a real Twitter dataset. The experimental results show that proposed algorithms are successful for discovering anomalous activities of social media users with respect to spatio-temporal information.},
  archive      = {J_APIN},
  author       = {Dokuz, Ahmet Sakir},
  doi          = {10.1007/s10489-021-02535-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2745-2762},
  shortjournal = {Appl. Intell.},
  title        = {Social velocity based spatio-temporal anomalous daily activity discovery of social media users},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent wavelet fuzzy brain emotional controller using
dual function-link network for uncertain nonlinear control systems.
<em>APIN</em>, <em>52</em>(3), 2720–2744. (<a
href="https://doi.org/10.1007/s10489-021-02482-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to propose a more efficient hybrid algorithm to achieve favorable control performance for uncertain nonlinear systems. The proposed algorithm comprises a dual function-link network-based multilayer wavelet fuzzy brain emotional controller and a sign(.) functional compensator. The proposed algorithm estimates the judgment and emotion of a brain that includes two fuzzy inference systems for the amygdala network and the prefrontal cortex network via using a dual-function-link network and three sub-structures. Three sub-structures are a dual-function-link network, an amygdala network, and a prefrontal cortex network. Particularly, the dual-function-link network is used to adjust the amygdala and orbitofrontal weights separately so that the proposed algorithm can efficiently reduce the tracking error, follow the reference signal well, and achieve good performance. A Lyapunov stability function is used to determine the adaptive laws, which are used to efficiently tune the system parameters online. Simulation and experimental studies for an antilock braking system and a magnetic levitation system are presented to verify the effectiveness and advantage of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Huynh, Tuan-Tu and Lin, Chih-Min and Le, Nguyen-Quoc-Khanh and Vu, Mai The and Nguyen, Ngoc Phi and Chao, Fei},
  doi          = {10.1007/s10489-021-02482-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2720-2744},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent wavelet fuzzy brain emotional controller using dual function-link network for uncertain nonlinear control systems},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automatic hyperparameter optimization DNN model for
precipitation prediction. <em>APIN</em>, <em>52</em>(3), 2703–2719. (<a
href="https://doi.org/10.1007/s10489-021-02507-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNN) have gained remarkable success on many rainfall predictions tasks in recent years. However, the performance of DNN highly relies upon the hyperparameter setting. In order to design DNNs with the best performance, extensive expertise in both the DNN and the problem domain under investigation is required. But many DNN users have not met this requirement. Therefore, it is difficult for the users who have no extensive expertise in DNN to design optimal DNN architectures for their rainfall prediction problems that is to solve. In this paper, we proposed a novel automatic hyperparameters optimization method for DNN by using an improved Gene Expression Programming. The proposed method can automatically optimize the hyperparameters of DNN for precipitation modeling and prediction. Extensive experiments are conducted with three real precipitation datasets to verify the performance of the proposed algorithm in terms of four metrics, including MAE, MSE, RMSE, and R-Squared. The results show that: 1) the DNN optimized by the proposed method outperforms the existing precipitation prediction methods including Multiple Linear Regression (MLR), Back Propagation (BP), Support Vector Machine (SVM), Random Forest (RF) and DNN; 2) the proposed DNN hyperparameter optimization method outperforms state-of-the-art DNN hyperparameter optimization methods, including Genetic Algorithm, Bayes Search, Grid Search, Randomized Search, and Quasi Random Search.},
  archive      = {J_APIN},
  author       = {Peng, Yuzhong and Gong, Daoqing and Deng, Chuyan and Li, Hongya and Cai, Hongguo and Zhang, Hao},
  doi          = {10.1007/s10489-021-02507-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2703-2719},
  shortjournal = {Appl. Intell.},
  title        = {An automatic hyperparameter optimization DNN model for precipitation prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An attention network via pronunciation, lexicon and syntax
for humor recognition. <em>APIN</em>, <em>52</em>(3), 2690–2702. (<a
href="https://doi.org/10.1007/s10489-021-02580-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humor is one of the most common and attractive expressions in our daily life. It is usually witty and funny. Humor recognition is an interesting but difficult task in natural language processing. Some recent works have used deep neural networks to recognize humorous text. In a different approach, we start from a new perspective based on humor linguistics, including pronunciation, lexicon, and syntax, for recognizing humor based on neural networks, in order to capture humorous incongruity and ambiguity. Specifically, we propose an attention network via pronunciation, lexicon, and syntax (ANPLS) for humor recognition. The ANPLS model contains four units, namely, the pronunciation understanding unit, the lexicon understanding unit, the syntax analysis unit, and the context understanding unit. The pronunciation understanding unit is used to extract the pronunciation-based humor features. The lexicon understanding unit is used to solve the polysemy in humor. The syntax analysis unit aims to capture the syntax information of humor. The context understanding unit is used to obtain the contextual humor features. These four units may have different levels of importance for humor recognition so that we further apply an attention mechanism to assign different weights to these four units. We conduct experiments on three popular datasets, namely, the SemEval2017 Task7 dataset, the 16000 One-Liners dataset, and the Pun of the Day dataset. The experimental results demonstrate that our model can achieve comparable or state-of-the-art performance compared with the existing models.},
  archive      = {J_APIN},
  author       = {Ren, Lu and Xu, Bo and Lin, Hongfei and Zhang, Jinhui and Yang, Liang},
  doi          = {10.1007/s10489-021-02580-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2690-2702},
  shortjournal = {Appl. Intell.},
  title        = {An attention network via pronunciation, lexicon and syntax for humor recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trustworthiness two-way games via margin policy in
e-commerce platforms. <em>APIN</em>, <em>52</em>(3), 2671–2689. (<a
href="https://doi.org/10.1007/s10489-021-02553-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The portability and low cost of e-commerce platforms have attracted more and more businesses and consumers. However, due to the virtual nature of e-commerce platforms, there is a serious information asymmetry between merchants and consumers–resulting in a serious credit problem of e-commerce platforms. The trust crisis remains a challenging issue for the development of e-commerce. Existing researches propose to strengthen trust supervision to protect the consumers. However, the trust crisis often hurts multiple parties’ interests. We can neither attribute the cause of dishonest transactions to a single party, nor supervise only a single party. To solve the problem of commercial breach of trust, we propose a margin policy-based consumer-to-business-to-consumer (or MP-C2B2C) model for e-commerce platforms. First, we add a third-party platform supervision mechanism to the C2C e-commerce model by presenting different margin policies. Second, we give the unit price trading mode, dual price trading mode and consumer reporting behavior mode under the margin policies based on two-way games. We investigate the game strategies of each participant in the e-commerce transaction process under MP-C2B2C model by comparing them with different representative game models. The research results show that participants in the transaction will be influenced by the margin system to choose the game strategy with better trustworthiness. Trustworthiness of the e-commerce platform is enhanced under the MP-C2B2C model.},
  archive      = {J_APIN},
  author       = {Wang, Lei and Wan, Jing and Zhang, Yunqiu and Chen, Shuhan and Zhu, Zhixiang and Tao, Yuqian},
  doi          = {10.1007/s10489-021-02553-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2671-2689},
  shortjournal = {Appl. Intell.},
  title        = {Trustworthiness two-way games via margin policy in e-commerce platforms},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A PL-MCDM method based on the decision-making reliability of
multi-group for patients with chronic diseases requiring downward
referral. <em>APIN</em>, <em>52</em>(3), 2655–2670. (<a
href="https://doi.org/10.1007/s10489-021-02436-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the number of patients with chronic diseases increasing, a hierarchical diagnosis and treatment system is a feature of long-term treatment. During the treatment, if patients with chronic diseases are relieved, they can be referred to community hospitals for rehabilitation treatment, and to promote the rational allocation of hospital medical resources. Therefore, hospitals need referral standards to encourage patients to refer down. Existing ranking research which is mainly based on referral criteria,has not considered the low reliability due to different opinions within each group when making referral standard. In view of this, we propose a probabilistic linguistic-multiple criteria decision-making (PL-MCDM) method based on the decision-making reliability of multi-group for patients with chronic diseases requiring downward referral. First, the decision information of different groups on referral criteria is expressed as probabilistic linguistic preferences, and then the reliability levels of groups are measured when setting standards respectively. After that, the decision information and referral standards are re-adjusted for the least reliable group to improve the whole reliability. Secondly, the patients requiring downward referral are ranked by the similarity measure based on statistics of probability preference to acquire the formal referral standards. Finally, compared with the methods without reliability improvement, the real case of coronary heart disease in First Hospital of Qinhuangdao is used to verify the effectiveness of the proposed method, which illustrates its superiority.},
  archive      = {J_APIN},
  author       = {Zhao, Meng and Wang, Xiaoran and Xu, Zeshui and Lin, Mei},
  doi          = {10.1007/s10489-021-02436-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2655-2670},
  shortjournal = {Appl. Intell.},
  title        = {A PL-MCDM method based on the decision-making reliability of multi-group for patients with chronic diseases requiring downward referral},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universum parametric-margin ν-support vector machine for
classification using the difference of convex functions algorithm.
<em>APIN</em>, <em>52</em>(3), 2634–2654. (<a
href="https://doi.org/10.1007/s10489-021-02402-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universum data that do not belong to any class of a classification problem can be exploited to utilize prior knowledge to improve generalization performance. In this paper, we design a novel parametric ν-support vector machine with universum data ( $ \mathfrak {U} $ Par-ν-SVM). Unlabeled samples can be integrated into supervised learning by means of $ \mathfrak {U} $ Par-ν-SVM. We propose a fast method to solve the suggested problem of $ \mathfrak {U} $ Par-ν-SVM. The primal problem of $ \mathfrak {U} $ Par-ν-SVM, which is a nonconvex optimization problem, is transformed into an unconstrained optimization problem so that the objective function can be treated as a difference of two convex functions (DC). To solve this unconstrained problem, a boosted difference of convex functions algorithm (BDCA) based on a generalized Newton method is suggested (named DC- $\mathfrak {U} $ Par-ν-SVM). We examined our approach on UCI benchmark data sets, NDC data sets, a handwritten digit recognition data set, and a landmine detection data set. The experimental results confirmed the effectiveness and superiority of the proposed method for solving classification problems in comparison with other methods.},
  archive      = {J_APIN},
  author       = {Moosaei, Hossein and Bazikar, Fatemeh and Ketabchi, Saeed and Hladík, Milan},
  doi          = {10.1007/s10489-021-02402-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2634-2654},
  shortjournal = {Appl. Intell.},
  title        = {Universum parametric-margin ν-support vector machine for classification using the difference of convex functions algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context augmentation for object detection. <em>APIN</em>,
<em>52</em>(3), 2621–2633. (<a
href="https://doi.org/10.1007/s10489-020-02037-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current two-stage object detectors, which mainly consist of a region proposal stage and a proposal recognition stage, may produce unreliable results for objects appearing with little information such as small and occluded objects. This is caused by poor region proposals and inaccurate proposal recognition. To address this problem, we propose a context augmentation algorithm that fully utilizes contextual information to generate high-quality region proposals and detection results. First, Region proposals are produced by two steps: 1) generate a coarse set of region proposals, some of which are reliable and some of which are ambiguous, and 2) the ambiguous region proposals are re-estimated using appearance and geometry information with respect to the reliable region proposals from step 1). Second, similar types of pair-wise relations between region proposals are used to produce global feature information associated with the region proposals in order to enhance recognition results. In practice, our method effectively improves the quality of region proposals as well as recognition results. Empirical studies show that the proposed context augmentation yields substantial and consistent improvements over baseline Faster R-CNN. Moreover, there is around 1.3% mAP improvement over Mask R-CNN on COCO dataset.},
  archive      = {J_APIN},
  author       = {Leng, Jiaxu and Liu, Ying},
  doi          = {10.1007/s10489-020-02037-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2621-2633},
  shortjournal = {Appl. Intell.},
  title        = {Context augmentation for object detection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Many-objective optimization for large-scale EVs charging and
discharging schedules considering travel convenience. <em>APIN</em>,
<em>52</em>(3), 2599–2620. (<a
href="https://doi.org/10.1007/s10489-021-02494-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncontrolled charging behaviors of large-scale electric vehicles (EVs) increase the security risk of the power grid and bring a new challenge for the computing ability of the power system. Using vehicle to grid (V2G) technology, most control systems coordinate the power interaction between EVs and power grid by minimizing the load fluctuation and user cost, but their optimization results are often achieved at the expense of reducing personal travel time. EVs should first meet basic travel needs and then obey the scheduling arrangement. Based on this idea, a four-objective optimal control method for EV charging and discharging schedules considering travel convenience is proposed, including minimization of the load fluctuation and user cost and maximization of the flexible travel time and state of charge (SOC). To solve this large-scale many-objective problem, a resource allocation-based preference-inspired coevolutionary algorithm (PICEAg-EV) is presented. Taking the IEEE 33-node system as an example, the simulation and analysis verify the effectiveness of the proposed control strategy and optimization algorithm. The experimental results show that PICEAg-EV outperforms seven popular intelligence algorithms under EV participation rate setting of 10%, 25%, 50%, 100%. Compared with 2- and 3-objective optimization models, the 4-objective optimization model can provide sufficient flexible travel time and a higher SOC for traveling, which is a better match for the user needs.},
  archive      = {J_APIN},
  author       = {Pan, Xiaotian and Wang, Liping and Qiu, Qicang and Qiu, Feiyue and Zhang, Guodao},
  doi          = {10.1007/s10489-021-02494-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2599-2620},
  shortjournal = {Appl. Intell.},
  title        = {Many-objective optimization for large-scale EVs charging and discharging schedules considering travel convenience},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilayer neural network based asymptotic motion control of
saturated uncertain robotic manipulators. <em>APIN</em>, <em>52</em>(3),
2586–2598. (<a
href="https://doi.org/10.1007/s10489-021-02318-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite influences coming from signal measurement noises, unknown nonlinear dynamics, external disturbances and input saturation nonlinearity make it challenging to synthesize high-performance closed-loop control algorithms for uncertain robotic manipulators. In the face of these challenges, we employ the nonlinear multilayer neural networks to approach uncertain nonlinear dynamics and exploit the robust adaptive control to deal with external disturbances without knowing their bounds in advance. More importantly, robust adaptive based auxiliary functions are creatively introduced to offset the possible input saturation nonlinearity. Furthermore, the desired trajectory based model compensation technology is integrated into the control scheme to reduce measurement noises as much as possible. In theory, the global closed-loop stability of the dynamical uncertain system is testified and significant asymptotic tracking result can be acquired. The application verification under different working conditions including severe high-frequency working conditions is implemented to indicate the high-performance effect of the synthesized intelligent controller.},
  archive      = {J_APIN},
  author       = {Yang, Guichao and Wang, Hua},
  doi          = {10.1007/s10489-021-02318-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2586-2598},
  shortjournal = {Appl. Intell.},
  title        = {Multilayer neural network based asymptotic motion control of saturated uncertain robotic manipulators},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient chaos based image encryption algorithm using
enhanced thorp shuffle and chaotic convolution function. <em>APIN</em>,
<em>52</em>(3), 2556–2585. (<a
href="https://doi.org/10.1007/s10489-021-02508-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Unique chaotic image encryption on the basis of Enhanced Thorp shuffle and Zig-zag Scan based Convolution (ETS-ZSC) is proposed in this paper. A one-dimensional chaotic map is utilized for both shuffling the plain image and producing the critical grid for the convolution activity. The substitution operation is performed in two ways: forward substitution and reverse substitution with zigzag scan. The original seed of the logistic map is created from the hyper chaotic system by matching with the plain image to overcome the differential attacks. The security analyzes are held for the proposed method to prove safe against the chosen plain text/known plain text attack. From the simulations results, it is observed that the proposed methodology is having enough high key sensitivity, key space, good randomness, and equal sharing of pixels in cipher image.},
  archive      = {J_APIN},
  author       = {Kumar, C. Madan and Vidhya, R. and Brindha, M.},
  doi          = {10.1007/s10489-021-02508-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2556-2585},
  shortjournal = {Appl. Intell.},
  title        = {An efficient chaos based image encryption algorithm using enhanced thorp shuffle and chaotic convolution function},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequent itemset hiding revisited: Pushing hiding
constraints into mining. <em>APIN</em>, <em>52</em>(3), 2539–2555. (<a
href="https://doi.org/10.1007/s10489-021-02490-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new theoretical scheme for the solution of the frequent itemset hiding problem. We propose an algorithmic approach that consists of a novel constraint-based hiding model which encompasses hiding into one pass mining, along with a solution methodology that relies on Linear Programming. The induced patterns by the constraint-based mining algorithm are, in this way, utilized to build a minimal linear program whose solution dictates the construction of a database extension that delivers the sought-for hiding. This extension should be appended to the original database and released as a whole for mining, with that resulting extended database hiding the sensitive knowledge that we want to protect. Our proposed theory outdoes both in space complexity and accuracy, all the existing approaches which have been proposed so far in this domain and we proved that superiority with a series of experiments against other existing approaches. Our proposal sheds a new light on the exploration of new algorithmic techniques which can be handily applied to model hiding problems by providing solutions that computationally outperform all existing modeling approaches for hiding.},
  archive      = {J_APIN},
  author       = {Verykios, Vassilios S. and Stavropoulos, Elias C. and Krasadakis, Panteleimon and Sakkopoulos, Evangelos},
  doi          = {10.1007/s10489-021-02490-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2539-2555},
  shortjournal = {Appl. Intell.},
  title        = {Frequent itemset hiding revisited: Pushing hiding constraints into mining},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self attention mechanism of bidirectional information
enhancement. <em>APIN</em>, <em>52</em>(3), 2530–2538. (<a
href="https://doi.org/10.1007/s10489-021-02492-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self attention mechanism is widely used in relation extraction, emotion classification and other tasks. It can extract a wide range of relevance information in the text. The attention mode of the existing self attention mechanism is soft attention mode, that is, a dense attention matrix is generated by softmax function. However, if the sentence length is long, the weight of important information will be too small. At the same time, the softmax function assumes that all elements have a positive impact on the results by default, which makes the model unable to extract the negative effect information. We use hard attention mechanism, namely sparse attention matrix, to improve the existing self attention model and fully extract the positive and negative information of text. Our model can not only enhance the extraction of positive information, but also makes up for the blank that the traditional attention matrix cannot be negative. We evaluated our model in three tasks and seven data sets. The experimental results show that our model is superior to the traditional self attention model and superior to state-of-the-art models in some tasks.},
  archive      = {J_APIN},
  author       = {Li, Qibin and Yao, Nianmin and Zhao, Jian and Zhang, Yanan},
  doi          = {10.1007/s10489-021-02492-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2530-2538},
  shortjournal = {Appl. Intell.},
  title        = {Self attention mechanism of bidirectional information enhancement},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formation control of multiple mecanum-wheeled mobile robots
with physical constraints and uncertainties. <em>APIN</em>,
<em>52</em>(3), 2510–2529. (<a
href="https://doi.org/10.1007/s10489-021-02459-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the formation control of multiple Mecanum-wheeled mobile robots (MWMRs) with physical constraints and model uncertainties, a novel robust control scheme that combines model predictive control (MPC) and extended state observer-based adaptive sliding mode control (ESO-ASMC) is proposed in this paper. First, a linear MPC strategy is proposed to address the motion constraints of MWMRs, which can transform the robot formation model based on leader-follower into a constrained quadratic programming (QP) problem. The QP problem can be solved iteratively online by a delay neural network (DNN) to obtain the optimal control velocity of the follower robot. Then, to address the input saturation constraints, model uncertainties and unknown disturbances in the dynamic model, an improved ESO-ASMC is proposed and compared with the robust adaptive terminal sliding mode control (RATSMC) and the conventional sliding mode control (SMC) to prove the effectiveness. The proposed scheme, considering the optimal control velocity obtained by the kinematics controller as the given desired velocity of the dynamics controller, can implement precise formation control, while solving various physical constraints of the robot, and eliminating the effects of model uncertainties and disturbances. Finally, through a comparative simulation case, the effectiveness and robustness of the proposed method are verified.},
  archive      = {J_APIN},
  author       = {Wang, Dongliang and Wei, Wu and Wang, Xinmei and Gao, Yong and Li, Yanjie and Yu, Qiuda and Fan, Zhun},
  doi          = {10.1007/s10489-021-02459-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2510-2529},
  shortjournal = {Appl. Intell.},
  title        = {Formation control of multiple mecanum-wheeled mobile robots with physical constraints and uncertainties},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finder: A novel approach of change point detection for
multivariate time series. <em>APIN</em>, <em>52</em>(3), 2496–2509. (<a
href="https://doi.org/10.1007/s10489-021-02532-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate time series often contain complex mixed inputs, with complex correlations between them. Detecting change points in multivariate time series is of great importance, which can find anomalies early and reduce losses, yet very challenging as it is affected by many complex factors, i.e., dynamic correlations and external factors. The performance of traditional methods typically scales poorly. In this paper, we propose Finder, a novel approach of change point detection via multivariate fusion attention networks. Our model consists of two key modules. First, in the time series prediction module, we employ multi-level attention networks based on the Transformer and integrate the external factor fusion component, achieving feature extraction and fusion of multivariate data. Secondly, in the change point detection module, a deep learning classifier is used to detect change points, improving efficiency and accuracy. Extensive experiments prove the superiority and effectiveness of Finder on two real-world datasets. Our approach outperforms the state-of-the-art methods by up to 10.50% on the F1 score.},
  archive      = {J_APIN},
  author       = {Du, Haizhou and Duan, Ziyi},
  doi          = {10.1007/s10489-021-02532-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2496-2509},
  shortjournal = {Appl. Intell.},
  title        = {Finder: A novel approach of change point detection for multivariate time series},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting viewer’s watching behavior and live streaming
content change for anchor recommendation. <em>APIN</em>, <em>52</em>(3),
2480–2495. (<a
href="https://doi.org/10.1007/s10489-021-02560-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, live streaming services attract millions of users’ participation and billions of capital investment. In each prevailing live streaming platform, there are thousands of anchors who are broadcasting concurrently, which means it is necessary for the platform to make recommendation to improve user experience. In such platforms, viewers change their watching preference dynamically, and anchors adjust their live content meanwhile. While there are many studies about predicting user’s (i.e., viewer’s) preference in literature, few methods proposed in literature can be used to predict live content’s change. As the recommendation target is online anchor’s live streaming that will be broadcasted in the next moment, we believe the prediction of the live streaming content is necessary for accurate recommendation. Therefore, in this paper, we study how to combine the prediction of viewer’s watching behavior and live content change for recommendation. We define a multi-task learning problem and propose a deep learning-based recommendation model, where we design two novel attention modules to capture viewer’s watching preference, anchor’s broadcasting preference, and loyal viewer’s preference related to each anchor. Experiments conducted on real datasets demonstrate the effectiveness of our proposed model.},
  archive      = {J_APIN},
  author       = {Zhang, Shuai and Liu, Hongyan and Mei, Lang and He, Jun and Du, Xiaoyong},
  doi          = {10.1007/s10489-021-02560-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2480-2495},
  shortjournal = {Appl. Intell.},
  title        = {Predicting viewer’s watching behavior and live streaming content change for anchor recommendation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new method for positive and unlabeled learning with
privileged information. <em>APIN</em>, <em>52</em>(3), 2465–2479. (<a
href="https://doi.org/10.1007/s10489-021-02528-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and unlabeled learning (PU learning) has been studied to address the situation in which only positive and unlabeled examples are available. Most of the previous work has been devoted to identifying negative examples from the unlabeled data, so that the supervised learning approaches can be applied to build a classifier. However, for the remaining unlabeled data, they either exclude them from the learning phase or force them to belong to a class, and this always limits the performance of PU learning. In addition, previous PU methods assume the training data and the testing data have the same features representations. However, we can always collect the features that the training data have while the test data do not have, these kinds of features are called privileged information. In this paper, we propose a new method, which is based on similarity approach for the problem of positive and unlabeled learning with privileged information (SPUPIL), which consists of two steps. The proposed SPUPIL method first conducts KNN method to generate the similarity weights and then the similarity weights and privileged information are incorporated to the learning model based on Ranking SVM to build a more accurate classifier. We also use the Lagrangian method to transform the original model into its dual problem, and solve it to obtain the classifier. Extensive experiments on the real data sets show that the performance of the SPUPIL is better than the state-of-the-art PU learning methods.},
  archive      = {J_APIN},
  author       = {Liu, Bo and Liu, Qian and Xiao, Yanshan},
  doi          = {10.1007/s10489-021-02528-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2465-2479},
  shortjournal = {Appl. Intell.},
  title        = {A new method for positive and unlabeled learning with privileged information},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). C-CADZ: Computational intelligence system for coronary
artery disease detection using z-alizadeh sani dataset. <em>APIN</em>,
<em>52</em>(3), 2436–2464. (<a
href="https://doi.org/10.1007/s10489-021-02467-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) is one of the most lethal diseases which is major cause of deaths around the globe. CAD is among such diseases with mortality rate approximately 7 million per annum. Though, early detection, prognostication and timely diagnosis can help in mortality rate reduction. Conventional CAD detection systems are cumbersome and expensive. Moreover, scarcity or uneven distribution of radiologists in different geographical locations is a hindrance in early diagnosis. Therefore, this is the time when researchers and doctors are collaboratively looking forward for developing a computational intelligence system in the area of medical imaging systems for prognostication, identification, treatment and disease diagnosis. To support the vision of researchers, a computational intelligence system for coronary artery disease diagnosis, C-CADZ, has been proposed. To validate the model, C-CADZ, the dataset namely, Z-Alizadeh Sani CAD dataset from UCI repository is considered. C-CADZ utilizes the fixed analysis of mixed data (FAMD) for feature extraction. FAMD extracts 96 features. In order to retrieve significant features, nature-inspired algorithms are utilized. C-CADZ implemented Synthetic Minority Oversampling Technique (SMOTE) to handle class-imbalanced data as machine learning (ML) predictive models are built to handle class-balanced datasets. Z-Score normalization technique is used for normalizing the dataset. Furthermore, C-CADZ is trained using ML classifiers, Random Forest (RF) and Extra Trees (ET) and validated using holdout validation scheme with hold-out ratio 3 : 1. Experimentation results show that C-CADZ outperforms state-of-the-art methods of last decades in terms of accuracy. C-CADZ has gained an increase in accuracy from state-of-the-art methods published in 2020 by 5.17% with performance metric 〈Acc, Sens, Spec〉≡〈97.37, 98.15, 95.45〉. The performance analysis shows that achieving highest accuracy and the stable nature of boxplot and ROC-AUC curve of RF-ET makes it suitable for heart disease prediction.},
  archive      = {J_APIN},
  author       = {Gupta, Ankur and Kumar, Rahul and Arora, Harkirat Singh and Raman, Balasubramanian},
  doi          = {10.1007/s10489-021-02467-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2436-2464},
  shortjournal = {Appl. Intell.},
  title        = {C-CADZ: Computational intelligence system for coronary artery disease detection using Z-alizadeh sani dataset},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modality person re-identification via channel-based
partition network. <em>APIN</em>, <em>52</em>(3), 2423–2435. (<a
href="https://doi.org/10.1007/s10489-021-02548-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared cross-modality person re-identification is an important task in the night video surveillance system, the huge difference between infrared and visible light images makes this work quite challenging. Unlike traditional person re-identification, a cross-modality mission needs to solve intra-class differences and inter-class variations. To solve the problem of huge modality discrepancy, in this paper, we propose a channel-based partition network which can unify the features of the two modes in an end-to-end way. Firstly, to handle the lack of discriminative information, we introduce newly generated samples to help the network improve its ability to learn cross modal features. Secondly, at the feature level, we propose a distinctive method of learning local features, in which the set of feature maps is parted on the channel. At the end of the proposed framework, we add a lightweight feature converter to further eliminate modality differences. The experimental results on the two popular datasets prove the effectiveness of our work.},
  archive      = {J_APIN},
  author       = {Liu, Jiachang and Song, Wanru and Chen, Changhong and Liu, Feng},
  doi          = {10.1007/s10489-021-02548-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2423-2435},
  shortjournal = {Appl. Intell.},
  title        = {Cross-modality person re-identification via channel-based partition network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A patient network-based machine learning model for disease
prediction: The case of type 2 diabetes mellitus. <em>APIN</em>,
<em>52</em>(3), 2411–2422. (<a
href="https://doi.org/10.1007/s10489-021-02533-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the prevalence of chronic diseases such as type 2 diabetes mellitus (T2DM) has increased, bringing a heavy burden to healthcare systems. While regular monitoring of patients is expensive and impractical, understanding chronic disease progressions and identifying patients at risk of developing comorbidities are crucial. This research used a real-world administrative claim dataset of T2DM to develop an ensemble of innovative patient network and machine learning approach for disease prediction. The healthcare data of 1,028 T2DM patients and 1,028 non-T2DM patients are extracted from the de-identified data to predict the risk of T2DM. The proposed model is based on the ‘patient network’, which represents the underlying relationships among health conditions for a group of patients diagnosed with the same disease using the graph theory. Besides patients’ socio-demographic and behaviour characteristics, the attributes of the ‘patient network’ (e.g., centrality measure) discover patients’ latent features, which are effective in risk prediction. We apply eight machine learning models (Logistic Regression, K-Nearest Neighbours, Support Vector Machine, Naïve Bayes, Decision Tree, Random Forest, XGBoost and Artificial Neural Network) to the extracted features to predict the chronic disease risk. The extensive experiments show that the proposed framework with machine learning classifiers performance with the Area Under Curve (AUC) ranged from 0.79 to 0.91. The Random Forest model outperformed the other models; whereas, eigenvector centrality and closeness centrality of the network and patient age are the most important features for the model. The outstanding performance of our model provides promising potential applications in healthcare services. Also, we provide strong evidence that the extracted latent features are essential in the disease risk prediction. The proposed approach offers vital insight into chronic disease risk prediction that could benefit healthcare service providers and their stakeholders.},
  archive      = {J_APIN},
  author       = {Lu, Haohui and Uddin, Shahadat and Hajati, Farshid and Moni, Mohammad Ali and Khushi, Matloob},
  doi          = {10.1007/s10489-021-02533-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2411-2422},
  shortjournal = {Appl. Intell.},
  title        = {A patient network-based machine learning model for disease prediction: The case of type 2 diabetes mellitus},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pseudo-parallel chaotic self-learning antelope migration
algorithm based on mobility models. <em>APIN</em>, <em>52</em>(3),
2369–2410. (<a
href="https://doi.org/10.1007/s10489-021-02510-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-learning Antelopes Migration Algorithm (SAMA) is a self-heuristic algorithm that simulates the local exploitation of ordinary antelopes and the global exploration of scout antelopes. Aiming at the imbalance between exploitation and exploration of SAMA, it is easy to fall into local optimum, and the convergence speed and precision will be affected. A pseudo-parallel chaotic self-learning antelope migration algorithm based on mobility model is proposed. Nine chaotic self-learning antelope migration algorithms are formed by incorporating nine kinds of chaotic local searching operators, and the total population is divided into several sub-populations through meme grouping, and the internal circulation optimization is carried out by using nine chaotic SAMAs. After the internal cycle is completed, the five migration models will perform their migration operations and mutation operations to form the pseudo-parallel chaotic SAMA to increase the diversity of the population, improve the optimization accuracy and the ability of the algorithm to balance exploitation and exploration. Three simulation experiments are carried out to verify the effectiveness of the proposed algorithm. Firstly, the chaotic SAMA and the pseudo-parallel chaotic SAMA based on the mobility models are used to optimize 17 benchmark test functions. Secondly, 25 test functions in CEC-BC-2017 are optimized respectively. Finally, the four engineering design problems are optimized, including three-bar truss design, welded beam design, pressure vessel design and spring design problems. Experimental results show that the improved algorithm can better solve the function optimization and engineering optimization problems. The pseudo-parallel chaotic SAMA based on the mobility models has the advantage of balancing exploitation and exploration in the optimization process, and improves the convergence accuracy.},
  archive      = {J_APIN},
  author       = {Guo, Meng-wei and Wang, Jie-sheng and Xie, Wei and Guo, Sha-sha and Zhu, Ling-feng},
  doi          = {10.1007/s10489-021-02510-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2369-2410},
  shortjournal = {Appl. Intell.},
  title        = {Pseudo-parallel chaotic self-learning antelope migration algorithm based on mobility models},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fuzzy-weighted gaussian kernel-based machine learning
approach for body fat prediction. <em>APIN</em>, <em>52</em>(3),
2359–2368. (<a
href="https://doi.org/10.1007/s10489-021-02421-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obesity is a critical public health problem associated with various complications and diseases. Accurate prediction of body fat is crucial for diagnosing obesity. Various measurement methods, including underwater weighing, dual energy X-ray absorptiometry, bioelectrical impedance analysis, magnetic resonance imaging, air displacement plethysmography, and near infrared interactance, have been used to assess body fat. These measurement methods, however, require special equipment associated with high-cost tests. The aim of this study is to investigate the use of machine learning-based models to accurately predict the body fat percentage. Considering the fact that off-the-shelf machine learning-based models are typically sensitive to noise data, we propose a fuzzy-weighted Gaussian kernel-based Relative Error Support Vector Machine (RE-SVM) for body fat prediction. We first design a fuzzy-weighted operation, which applies fuzzy weights to the error constraints of the RE-SVM, to alleviate the influence of noise data. Next, we also apply the fuzzy weights to improve the Gaussian kernel by considering the importance of different samples. Computational experiments and statistical tests conducted confirm that our proposed approach is able to significantly outperform other models being compared for body fat prediction across different performance metrics used. The proposed approach offers a viable alternative for diagnosing obesity when high-cost measurement methods are not available.},
  archive      = {J_APIN},
  author       = {Fan, Zongwen and Chiong, Raymond and Chiong, Fabian},
  doi          = {10.1007/s10489-021-02421-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2359-2368},
  shortjournal = {Appl. Intell.},
  title        = {A fuzzy-weighted gaussian kernel-based machine learning approach for body fat prediction},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Predict industrial equipment failure with time windows and
transfer learning. <em>APIN</em>, <em>52</em>(3), 2346–2358. (<a
href="https://doi.org/10.1007/s10489-021-02441-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensors, while more widely implemented in industry, have generated a large number of high-dimension unlabeled time series data during the process of the complicated producing. If putting these data to use, we can predict and preclude malfunctions of specific industrial facilities so that there will be less pecuniary lost. In this paper, we propose a malfunction predicting algorithm based on transfer learning. We use time windows due to the periodicity of industrial data, targeting at transfer learning among pieces of equipment with different sampling rate to address the problem of learning from unlabeled data. Rationale proofs and experiments indicate the efficacy of the algorithm and the prediction accuracy reaches 97%.},
  archive      = {J_APIN},
  author       = {Wang, Hongzhi and Lu, Wenbo and Tang, Shihan and Song, Yang},
  doi          = {10.1007/s10489-021-02441-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2346-2358},
  shortjournal = {Appl. Intell.},
  title        = {Predict industrial equipment failure with time windows and transfer learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural networks for automatic grain-matrix segmentation
in plane and cross-polarized sandstone photomicrographs. <em>APIN</em>,
<em>52</em>(3), 2332–2345. (<a
href="https://doi.org/10.1007/s10489-021-02530-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grain segmentation of sandstone that is partitioning the grain from its surrounding matrix/cement in the thin section is the primary step for computer-aided mineral identification and sandstone classification. The photomicrograph of sandstone contain many mineral grains and their surrounding matrix/cement. The distinction between adjacent grains and the matrix is often ambiguous, making grain segmentation difficult. Various solutions exist in literature to handle these problems; however, they are not robust against sandstone petrography’s varied pattern. In this paper, we formulate grain segmentation as a pixel-wise two-class (i.e., grain and background) semantic segmentation task. We develop a deep learning-based end-to-end trainable framework named Deep Semantic Grain Segmentation network (dsgsn), a data-driven method, and provide a generic solution. As per the authors’ knowledge, this is the first work where the deep neural network is explored to solve the grain segmentation problem. Extensive experiments on the images highlight that our method obtains better segmentation accuracy than various segmentation architectures with more parameters.},
  archive      = {J_APIN},
  author       = {Das, Rajdeep and Mondal, Ajoy and Chakraborty, Tapan and Ghosh, Kuntal},
  doi          = {10.1007/s10489-021-02530-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2332-2345},
  shortjournal = {Appl. Intell.},
  title        = {Deep neural networks for automatic grain-matrix segmentation in plane and cross-polarized sandstone photomicrographs},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictively encoded graph convolutional network for
noise-robust skeleton-based action recognition. <em>APIN</em>,
<em>52</em>(3), 2317–2331. (<a
href="https://doi.org/10.1007/s10489-021-02487-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based action recognition, graph convolutional networks (GCNs), which model human body skeletons using graphical components such as nodes and connections, have recently achieved remarkable performance. While the current state-of-the-art methods for skeleton-based action recognition usually assume that completely observed skeletons will be provided, it is problematic to realize this assumption in real-world scenarios since the captured skeletons may be incomplete or noisy. In this work, we propose a skeleton-based action recognition method that is robust to noise interference for the given skeleton features. The key insight of our approach is to train a model by maximizing the mutual information between normal and noisy skeletons using predictive coding in the latent space. We conducted comprehensive skeleton-based action recognition experiments with defective skeletons using the NTU-RGB+D and Kinetics-Skeleton datasets. The experimental results demonstrate that when the skeleton samples are noisy, our approach achieves outstanding performances compared with the existing state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yoon, Yongsang and Yu, Jongmin and Jeon, Moongu},
  doi          = {10.1007/s10489-021-02487-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {3},
  pages        = {2317-2331},
  shortjournal = {Appl. Intell.},
  title        = {Predictively encoded graph convolutional network for noise-robust skeleton-based action recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active learning using a self-correcting neural network
(ALSCN). <em>APIN</em>, <em>52</em>(2), 1956–1968. (<a
href="https://doi.org/10.1007/s10489-021-02515-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data labeling represents a major obstacle in the development of new models because the performance of machine learning models directly depends on the quality of the datasets used to train these models and labeling requires substantial manual effort. Labeling the entire dataset is not always necessary, and not every item from the image dataset contributes equally to the training process. Active learning or guided labeling is one of the attempts to automate and speed up labeling as much as possible. In this study we present a novel active learning algorithm (ALSCN) that contains two networks, convolutional neural network and self-correcting neural network (SCN). The convolutional network is trained using only manually labeled data, and after training that network it predicts labels for unlabeled items. The SCN network is trained with all available items, some of those items are manually labeled and remaining items are automatically labeled with previous network. After training SCN network, it predicts new labels for all available items, and the new labels are compared with the labels used for training. Items in which differences have been identified are selected for manual labeling and then added to dataset of previously manually labeled items. After that, the convolutional network is trained with extended dataset and previously described steps are repeated. Our experiments show that the network trained using items selected by the proposed method exceeds the performance of a network trained with the same number of items randomly selected from the set of available items. Items from the complete datasets are selected in several iterations, and used for training the models. The accuracy of the models trained with selected items matched or exceeded the accuracy of models trained with the entire dataset, which shows the extent of reduction in the required manual labeling effort. The efficiency of presented algorithm is tested on three datasets (MNIST, Fashion MNIST and CIFAR-10). The final results show that manual labeling is required for only 6.11% (3667/60,000), 23.92% (14,353/60,000) and 59.4% (29,704/50,000) items, in case of MNIST, Fashion MNIST and CIFAR-10 dataset, respectively.},
  archive      = {J_APIN},
  author       = {Ilić, Velibor and Tadić, Jovan},
  doi          = {10.1007/s10489-021-02515-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1956-1968},
  shortjournal = {Appl. Intell.},
  title        = {Active learning using a self-correcting neural network (ALSCN)},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Change point detection for compositional multivariate data.
<em>APIN</em>, <em>52</em>(2), 1930–1955. (<a
href="https://doi.org/10.1007/s10489-021-02321-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change point detection algorithms have numerous applications in areas of medical condition monitoring, fault detection in industrial processes, human activity analysis, climate change detection, and speech recognition. We consider the problem of change point detection on compositional multivariate data (each sample is a probability mass function), which is a practically important sub-class of general multivariate data. While the problem of change-point detection is well studied in univariate setting, and there are few viable implementations for a general multivariate data, the existing methods do not perform well on compositional data. In this paper, we propose a parametric approach for change point detection in compositional data. Moreover, using simple transformations on data, we extend our approach to handle any general multivariate data. Experimentally, we show that our method performs significantly better on compositional data and is competitive on general data compared to the available state of the art implementations.},
  archive      = {J_APIN},
  author       = {K. J., Prabuchandran and Singh, Nitin and Dayama, Pankaj and Agarwal, Ashutosh and Pandit, Vinayaka},
  doi          = {10.1007/s10489-021-02321-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1930-1955},
  shortjournal = {Appl. Intell.},
  title        = {Change point detection for compositional multivariate data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The graph-based behavior-aware recommendation for
interactive news. <em>APIN</em>, <em>52</em>(2), 1913–1929. (<a
href="https://doi.org/10.1007/s10489-021-02497-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive news recommendation has been launched and attracted much attention recently. In this scenario, user’s behavior evolves from single click behavior to multiple behaviors including like, comment, share etc. However, most of the existing methods still use single click behavior as the unique criterion of judging user’s preferences. Further, although heterogeneous graphs have been applied in different areas, a proper way to construct a heterogeneous graph for interactive news data with an appropriate learning mechanism on it is still desired. To address the above concerns, we propose a graph-based behavior-aware network, which simultaneously considers six different types of behaviors as well as user’s demand on the news diversity. We have three mainsteps. First, we build an interaction behavior graph for multi-level and multi-category data. Second, we apply DeepWalk on the behavior graph to obtain entity semantics, then build a graph-based convolutional neural network called G-CNN to learn news representations, and an attention-based LSTM to learn behavior sequence representations. Third, we introduce core and coritivity features for the behavior graph, which measure the concentration degree of user’s interests. These features affect the trade-off between accuracy and diversity of our personalized recommendation system. Taking these features into account, our system finally achieves recommending news to different users at their different levels of concentration degrees.},
  archive      = {J_APIN},
  author       = {Ma, Mingyuan and Na, Sen and Wang, Hongyu and Chen, Congzhou and Xu, Jin},
  doi          = {10.1007/s10489-021-02497-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1913-1929},
  shortjournal = {Appl. Intell.},
  title        = {The graph-based behavior-aware recommendation for interactive news},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted statistical binary patterns for facial feature
representation. <em>APIN</em>, <em>52</em>(2), 1893–1912. (<a
href="https://doi.org/10.1007/s10489-021-02477-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel framework for efficient and robust facial feature representation based upon Local Binary Pattern (LBP), called Weighted Statistical Binary Pattern, wherein the descriptors utilize the straight-line topology along with different directions. The input image is initially divided into mean and variance moments. A new variance moment, which contains distinctive facial features, is prepared by extracting root k-th. Then, when Sign and Magnitude components along four different directions using the mean moment are constructed, a weighting approach according to the new variance is applied to each component. Finally, the weighted histograms of Sign and Magnitude components are concatenated to build a novel histogram of Complementary LBP along with different directions. A comprehensive evaluation using six public face datasets suggests that the present framework outperforms the state-of-the-art methods and achieves 98.51% for ORL, 98.72% for YALE, 98.83% for Caltech, 99.52% for AR, 94.78% for FERET, and 99.07% for KDEF in terms of accuracy, respectively. The influence of color spaces and the issue of degraded images are also analyzed with our descriptors. Such a result with theoretical underpinning confirms that our descriptors are robust against noise, illumination variation, diverse facial expressions, and head poses.},
  archive      = {J_APIN},
  author       = {Truong, Hung Phuoc and Nguyen, Thanh Phuong and Kim, Yong-Guk},
  doi          = {10.1007/s10489-021-02477-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1893-1912},
  shortjournal = {Appl. Intell.},
  title        = {Weighted statistical binary patterns for facial feature representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MKPM: Multi keyword-pair matching for natural language
sentences. <em>APIN</em>, <em>52</em>(2), 1878–1892. (<a
href="https://doi.org/10.1007/s10489-021-02306-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence matching is widely used in various natural language tasks, such as natural language inference, paraphrase identification and question answering. For these tasks, we need to understand the logical and semantic relationship between two sentences. Most current methods use all information within a sentence to build a model and hence determine its relationship to another sentence. However, the information contained in some sentences may cause redundancy or introduce noise, impeding the performance of the model. Therefore, we propose a sentence matching method based on multi keyword-pair matching (MKPM), which uses keyword pairs in two sentences to represent the semantic relationship between them, avoiding the interference of redundancy and noise. Specifically, we first propose a sentence-pair-based attention mechanism sp-attention to select the most important word pair from the two sentences as a keyword pair, and then propose a Bi-task architecture to model the semantic information of these keyword pairs. The Bi-task architecture is as follows: 1. In order to understand the semantic relationship at the word level between two sentences, we design a word-pair task (WP-Task), which uses these keyword pairs to complete sentence matching independently. 2. We design a sentence-pair task (SP-Task) to understand the sentence level semantic relationship between the two sentences by sentence denoising. Through the integration of the two tasks, our model can understand sentences more accurately from the two granularities of word and sentence. Experimental results show that our model can achieve state-of-the-art performance in several tasks. Our source code is publicly available1.},
  archive      = {J_APIN},
  author       = {Lu, Xin and Deng, Yao and Sun, Ting and Gao, Yi and Feng, Jun and Sun, Xia and Sutcliffe, Richard},
  doi          = {10.1007/s10489-021-02306-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1878-1892},
  shortjournal = {Appl. Intell.},
  title        = {MKPM: Multi keyword-pair matching for natural language sentences},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive hierarchical update particle swarm optimization
algorithm with a multi-choice comprehensive learning strategy.
<em>APIN</em>, <em>52</em>(2), 1853–1877. (<a
href="https://doi.org/10.1007/s10489-021-02413-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since many PSO variants are easily trapped in local optima from which they can barely break free, this paper proposes an adaptive hierarchical update particle swarm optimization (AHPSO) algorithm. The new term “local optimum early warning” is first defined to reflect the risk of being trapped in a local optimum. It plays a key role in the global coordinated control to determine the paradigm evolution direction and adjust the trajectory of particles in different risk environments. After that, the adaptive hierarchical update method generates two-layer and three-layer update formulas for the global exploration subpopulation and the local exploitation subpopulation, respectively, in order to improve the capability to resist the temptation of local optima. Consisting of the weighted synthesis sub-strategy and the mean evolution sub-strategy, the multi-choice comprehensive learning strategy is then employed to develop the most suitable learning paradigm to guide the motion path. Moreover, 18 benchmark functions and one real-world optimization problem are employed to evaluate the AHPSO against eight typical PSO variants. According to the experimental results, the AHPSO outperformed other methods in solving different types of functions by yielding high solution accuracy and high convergence speed.},
  archive      = {J_APIN},
  author       = {Zhou, Shangbo and Sha, Long and Zhu, Shufang and Wang, Limin},
  doi          = {10.1007/s10489-021-02413-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1853-1877},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive hierarchical update particle swarm optimization algorithm with a multi-choice comprehensive learning strategy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying influential nodes in weighted complex networks
using an improved WVoteRank approach. <em>APIN</em>, <em>52</em>(2),
1838–1852. (<a
href="https://doi.org/10.1007/s10489-021-02403-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is an important research problem in the field of network science because of its business value. It requires the strategic selection of seed nodes called “influential nodes,” such that information originating from these nodes can reach numerous nodes in the network. Many real-world networks, such as transportation, communication, and social networks, are weighted networks. Influence maximization in a weighted network is more challenging compared to that in an unweighted network. Many methods, such as weighted degree rank, weighted h-index, weighted betweenness, and weighted VoteRank techniques, have been used to order the nodes based on their spreading capabilities in weighted networks. The VoteRank method is a popular method for finding influential nodes in an unweighted network using the idea of a voting scheme. Recently, the WVoteRank method was proposed to find the seed nodes; it extends the idea of the VoteRank method by considering the edge weights. This method considers only 1-hop neighbors to calculate the voting score of every node. In this study, we propose an improved WVoteRank method based on an extended neighborhood concept, which takes the 1-hop neighbors as well as 2-hop neighbors into account for the voting process to decide influential nodes in a weighted network. We also extend our proposed approach to unweighted networks. We compare the performance of the proposed improved WVoteRank method against the popular centrality measures, weighted degree, weighted closeness, weighted betweenness, weighted h-index, and weighted VoteRank on several real-life and synthetic datasets of diverse sizes and properties. We utilize the widely used stochastic susceptible–infected–recovered information diffusion model to calculate the infection scale, the final infected scale as a function of time, and the average distance between spreaders. The simulation results reveal that the proposed method, improved WVoteRank, considerably outperforms the other methods described above, including the recent WVoteRank.},
  archive      = {J_APIN},
  author       = {Kumar, Sanjay and Panda, Ankit},
  doi          = {10.1007/s10489-021-02403-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1838-1852},
  shortjournal = {Appl. Intell.},
  title        = {Identifying influential nodes in weighted complex networks using an improved WVoteRank approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Pyramid-dilated deep convolutional neural network for crowd
counting. <em>APIN</em>, <em>52</em>(2), 1825–1837. (<a
href="https://doi.org/10.1007/s10489-021-02537-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistics on crowds in crowded scenes can reflect the density level of crowds and provide safety warnings. This is a laborious task if conducted manually. In recent years, automated crowd counting has received extensive attention in the computer vision field. However, this task is still challenging mainly due to the serious occlusion in crowds and large appearance variations caused by the viewing angles of cameras. To overcome these difficulties, a pyramid-dilated deep convolutional neural network for accurate crowd counting called PDD-CNN is proposed. PDD-CNN is based on a VGG-16 network that is designed to generate dense attribute feature maps from an image with an arbitrary size or resolution. Then, two pyramid dilated modules are adopted, each consisting of four parallel dilated convolutional layers with different rates and a parallel average pooling layer to capture the multiscale features. Finally, three cascading dilated convolutions are used to regress the density map and perform accurate count estimation. In addition, a novel training loss, combining the Euclidean loss with the structural similarity loss, is employed to attenuate the blurry effects of density map estimation. The experimental results on three datasets (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed PDD-CNN produces high-quality density maps and achieves a good counting performance.},
  archive      = {J_APIN},
  author       = {Wang, Weixing and Liu, Quanli and Wang, Wei},
  doi          = {10.1007/s10489-021-02537-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1825-1837},
  shortjournal = {Appl. Intell.},
  title        = {Pyramid-dilated deep convolutional neural network for crowd counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using ontology to guide reinforcement learning agents in
unseen situations. <em>APIN</em>, <em>52</em>(2), 1808–1824. (<a
href="https://doi.org/10.1007/s10489-021-02449-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-agent systems, goal achievement is challenging when agents operate in ever-changing environments and face unseen situations, where not all the goals are known or predefined. In such cases, agents need to identify the changes and adapt their behaviour, by evolving their goals or even generating new goals to address the emerging requirements. Learning and practical reasoning techniques have been used to enable agents with limited knowledge to adapt to new circumstances. However, they depend on the availability of large amounts of data, require long exploration periods, and cannot help agents to set new goals. Furthermore, the accuracy of agents’ actions is improved by introducing added intelligence through integrating conceptual features extracted from ontologies. However, the concerns related to taking suitable actions when unseen situations occur are not addressed. This paper proposes a new Automatic Goal Generation Model (AGGM) that enables agents to create new goals to handle unseen situations and to adapt to their ever-changing environment on a real-time basis. AGGM is compared to Q-learning, SARSA, and Deep Q Network in a Traffic Signal Control System case study. The results show that AGGM outperforms the baseline algorithms in unseen situations while handling the seen situations as well as the baseline algorithms.},
  archive      = {J_APIN},
  author       = {Ghanadbashi, Saeedeh and Golpayegani, Fatemeh},
  doi          = {10.1007/s10489-021-02449-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1808-1824},
  shortjournal = {Appl. Intell.},
  title        = {Using ontology to guide reinforcement learning agents in unseen situations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Axiom selection over large theory based on new first-order
formula metrics. <em>APIN</em>, <em>52</em>(2), 1793–1807. (<a
href="https://doi.org/10.1007/s10489-021-02469-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Axiom selection is a task that selects the most likely useful axioms from a large-scale axiom set for proving a given conjecture. Existing axiom selection methods either solely take shallow symbols into account or strongly dependent on previous successful proofs from homologous problems. To address these problems, we introduce a new metric to evaluate the dissimilarity between formulae and utilize it as an evaluator in the selection task. Firstly, we propose a substitution-based metric to compute the dissimilarity between terms. It is a pseudo-metric and can capture the in-depth syntactic difference trigged by both functional and variable subterms. We then extend it to atoms and prove the atom metric also to be a pseudo-metric. Treating formulae as atom sets, we define three kinds of dissimilarity metrics between formulae. Finally, we design and implement conjecture-oriented axiom selection methods based on newly proposed formula metrics. The experimental evaluation is conducted on the MPTP2078 benchmark and demonstrates dissimilarity-based axiom selection improves E prover’s performance. In the best case, it increases the ratio of successful proofs from 30.90% to 42.25%.},
  archive      = {J_APIN},
  author       = {Liu, Qinghua and Xu, Yang},
  doi          = {10.1007/s10489-021-02469-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1793-1807},
  shortjournal = {Appl. Intell.},
  title        = {Axiom selection over large theory based on new first-order formula metrics},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive enhancement algorithm based on visual saliency
for low illumination images. <em>APIN</em>, <em>52</em>(2), 1770–1792.
(<a href="https://doi.org/10.1007/s10489-021-02466-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the brightness and contrast of low illumination color images and avoid over enhancement, an adaptive image enhancement algorithm based on visual saliency is proposed. Firstly, the original low illumination image is transformed from Red Green Blue (RGB) color space to Hue Saturation Intensity (HSI) color space. Secondly, the bilateral gamma adjustment (BIGA) function combined with the cuckoo search algorithm is used for adaptively increasing the overall brightness of image. In addition, the brightness preserving Bi-histogram construction based on visual salience algorithm (BBHCVS) is proposed to respectively conserve the brightness and improve the contrast of low illuminance color images. Finally, the processed HSI color space is transformed into RGB color space to get the enhanced image. Experimental results demonstrate that the proposed BBHCVS algorithm can effectively enhance the visual salient areas of human perception, and also significantly improve the contrast and brightness of image compared with other well-known and state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Qian, Shenyi and Shi, Yongsheng and Wu, Huaiguang and Liu, Jinhua and Zhang, Weiwei},
  doi          = {10.1007/s10489-021-02466-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1770-1792},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive enhancement algorithm based on visual saliency for low illumination images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ant colony-based algorithm for integrated scheduling on
batch machines with non-identical capacities. <em>APIN</em>,
<em>52</em>(2), 1752–1769. (<a
href="https://doi.org/10.1007/s10489-021-02336-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a production–distribution scheduling problem with non-identical batch machines and multiple vehicles is considered. In the production stage, n jobs are grouped into batches, which are processed on m parallel non-identical batch machines. In the distribution stage, there are multiple vehicles with identical capacities to deliver jobs to customers after the jobs are processed. The objective is to minimize the total weighted tardiness of the jobs. Considering the NP-hardness of the studied problem, an algorithm based on ant colony optimization is presented. A new local optimization strategy called LOC is proposed to improve the local exploitation ability of the algorithm and further search the neighborhood solution to improve the quality of the solution. Moreover, two interval candidate lists are proposed to reduce the search for the feasible solution space and improve the search speed. Furthermore, three objective-oriented heuristics are developed to accelerate the convergence of the algorithm. To verify the performance of the proposed algorithm, extensive experiments are carried out. The experimental results demonstrate that the proposed algorithm can provide better solutions than the state-of-the-art algorithms within a reasonable time.},
  archive      = {J_APIN},
  author       = {Jia, Zhao-hong and Cui, Yu-fei and Li, Kai},
  doi          = {10.1007/s10489-021-02336-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1752-1769},
  shortjournal = {Appl. Intell.},
  title        = {An ant colony-based algorithm for integrated scheduling on batch machines with non-identical capacities},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ISSATA: An algorithm for solving the 3-satisfiability
problem based on improved strategy. <em>APIN</em>, <em>52</em>(2),
1740–1751. (<a
href="https://doi.org/10.1007/s10489-021-02493-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic local search algorithm with configuration check strategy can effectively solve random satisfiability instances, so configuration check strategy is widely used in combinatorial optimization problems. Inspired by this, we proposed an ISSATA algorithm to solve the 3-satisfiability problem. In this algorithm, a new initialization strategy is given, which can assign initial values to variables more efficiently. At the same time, a new variable selection strategy and a new neighbor priority strategy are proposed to improve the performance of selecting flipped variables. Comparative experiments conducted in public datasets show that ISSATA has better solution accuracy and efficiency.},
  archive      = {J_APIN},
  author       = {Guo, Ping and Zhang, Yang},
  doi          = {10.1007/s10489-021-02493-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1740-1751},
  shortjournal = {Appl. Intell.},
  title        = {ISSATA: An algorithm for solving the 3-satisfiability problem based on improved strategy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast detection method of green peach for application of
picking robot. <em>APIN</em>, <em>52</em>(2), 1718–1739. (<a
href="https://doi.org/10.1007/s10489-021-02456-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the picking robot picks green peaches, there are problems such as the color of the fruit being similar to the background color, overlapping fruits, and small fruit size, uneven lighting, and branches and leaves occlusion. As a result, the picking robot cannot quickly detect green peaches. In order to solve the above problems, a lightweight object detection network for fast detection of green peaches is proposed, which is composed of a backbone network, feature enhancement network, Lightweight Self-Attention (LSA) network, and four-scale prediction network. First, the lightweight detection unit LeanNet of the backbone network is designed, which uses the idea of deep separable convolution to achieve fast detection. Secondly, the feature enhancement module (P-Enhance) is designed, which uses convolution kernels of different receptive fields to extract different perceptual information in the feature map, which enhances the network’s feature extraction ability for green peach. Then, the LSA module is designed to generate a local saliency map based on green peach features, which effectively suppressed the irrelevant area of the branch and leaf background. Finally, a four-scale prediction network is designed, in which the Four-scale Pyramid Fusion (FSPF) module can generate a four-scale feature pyramid, which includes the color and shape of the green peach at different network depths, and is conducive to the detection of small volume green peaches. The experimental results show that precision, recall, and F1 of our method in the green peach test set reached 97.3%, 99.7%, and 98.5%, respectively. In the actual picking scenes, Qualcomm Snapdragon 865 embedded devices equipped with different state-of-the-art methods are used. Through comparative experiments in various scenarios, compared with the state-of-the-art method, both in terms of experimental data and visual effects, there is a significant improvement, which can meet the real-time object detection needs of picking robots.},
  archive      = {J_APIN},
  author       = {Cui, Zhe and Sun, Hong-Mei and Yu, Jin-Tao and Yin, Ruo-Nan and Jia, Rui-Sheng},
  doi          = {10.1007/s10489-021-02456-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1718-1739},
  shortjournal = {Appl. Intell.},
  title        = {Fast detection method of green peach for application of picking robot},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-supervised transferable LSTM with feature evaluation
for fault diagnosis of rotating machinery. <em>APIN</em>,
<em>52</em>(2), 1703–1717. (<a
href="https://doi.org/10.1007/s10489-021-02504-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the issue of impracticality or costliness of collecting enough labeled signals under all working conditions, the performance of a method usually suffers a significant loss when the model trained on one working condition is directly applied to another working condition. Thus, the entropy gain ratio combined with a semi-supervised transferable LSTM (EGR-STLSTM) network is established for fault diagnosis of rotating machinery under variable working conditions. First, the ERG is devoted to evaluating the multi-domain features extracted based on the characteristics of rotating machinery. Then, the optimal feature subset is fed into the STLSTM to obtain a pre-trained network. Finally, a semi-supervised transfer learning strategy, that is, with the aid of a small number of target labeled samples, is applied to the pre-trained model to achieve competitive performance in target tasks. The proposed EGR-STLSTM network fully uses the vibration characteristics of rotating machinery and retains the specialized knowledge derived from the source domain. The fault diagnosis method is verified with raw vibration signals from a bearing test rig and a gearbox test rig. The experimental results indicate that compared with the well-known methods, the proposed method significantly improves the diagnostic performance by reusing the pre-trained model and reduces the demand for massive labeled samples under variable working conditions.},
  archive      = {J_APIN},
  author       = {Tang, Zhi and Bo, Lin and Liu, Xiaofeng and Wei, Daiping},
  doi          = {10.1007/s10489-021-02504-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1703-1717},
  shortjournal = {Appl. Intell.},
  title        = {A semi-supervised transferable LSTM with feature evaluation for fault diagnosis of rotating machinery},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microblog summarization using self-adaptive multi-objective
binary differential evolution. <em>APIN</em>, <em>52</em>(2), 1686–1702.
(<a href="https://doi.org/10.1007/s10489-020-02178-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms become paramount for gathering relevant information during the occurrence of any natural disaster. Twitter has emerged as a platform which is heavily used for the purpose of communication during disaster events. Therefore, it becomes necessary to design a technique which can summarize the relevant tweets and thus, can help in the decision-making process of disaster management authority. In this paper, the problem of summarizing the relevant tweets is posed as an optimization problem where a subset of tweets is selected using the search capability of multi-objective binary differential evolution (MOBDE) by optimizing different perspectives of the summary. MOBDE deals with a set of solutions in its population, and each solution encodes a subset of tweets. Three versions of the proposed approach, namely, MOOTS1, MOOTS2, and MOOTS3, are developed in this paper. They differ in the way of working and the adaptive selection of parameters. Recently developed self-organizing map based genetic operator is explored in the optimization process. Two measures capturing the similarity/dissimilarity between tweets, word mover distance and BM25 are explored in the optimization process. The proposed approaches are evaluated on four datasets related to disaster events containing only relevant tweets. It has been observed that all versions of the developed MOBDE framework outperform the state-of-the-art (SOA) techniques. In terms of improvements, our best-proposed approach (MOOST3) improves by 8.5% and 3.1% in terms of ROUGE− 2 and ROUGE−L, respectively, over the existing techniques and these improvements are further validated using statistical significance t-test.},
  archive      = {J_APIN},
  author       = {Saini, Naveen and Saha, Sriparna and Bhattacharyya, Pushpak},
  doi          = {10.1007/s10489-020-02178-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1686-1702},
  shortjournal = {Appl. Intell.},
  title        = {Microblog summarization using self-adaptive multi-objective binary differential evolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distant context aware text generation from abstract meaning
representation. <em>APIN</em>, <em>52</em>(2), 1672–1685. (<a
href="https://doi.org/10.1007/s10489-021-02431-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text generation from abstract meaning representation is a fundamental task in natural language generation. An interesting challenge is that distant context could influence the surface realization for each node. In the previous encoder-decoder based approaches, graph neural networks have been commonly used to encode abstract meaning representation graphs and exhibited superior performance over the sequence and tree encoders. However, most of them cannot stack numerous layers, thus being too shallow to capture distant context. In this paper, we propose solutions from three aspects. Firstly, we introduce a Transformer based graph encoder to embed abstract meaning representation graphs. This encoder can stack more layers to encode larger context, while without performance degrading. Secondly, we expand the receptive field of each node, i.e. building direct connections between node pairs, to capture the information of its distant neighbors. We also exploit relative position embedding to make the model aware of the original hierarchy of graphs. Thirdly, we encode the linearized version of abstract meaning representation with the pre-trained language model to get the sequence encoding and incorporate it into graph encoding to enrich features. We conduct experiments on LDC2015E86 and LDC2017T10. Experimental results demonstrate that our method outperforms previous strong baselines. Especially, we investigate the performance of our model on large graphs, finding a larger performance gain. Our best model achieves 31.99 of BLEU and 37.02 of METEOR on LDC2015E86, 34.21 of BLEU, and 39.26 of METEOR on LDC2017T10, which are new states of the art.},
  archive      = {J_APIN},
  author       = {Yang, Sen and Feng, Dawei and Liu, Yang and Li, Dongsheng},
  doi          = {10.1007/s10489-021-02431-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1672-1685},
  shortjournal = {Appl. Intell.},
  title        = {Distant context aware text generation from abstract meaning representation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local consistency adjustment strategy and DEA – driven
interval type-2 trapezoidal fuzzy decision-making model and its
application for fog-haze factor assessment problem. <em>APIN</em>,
<em>52</em>(2), 1653–1671. (<a
href="https://doi.org/10.1007/s10489-021-02354-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop a novel decision-making method with interval type-2 trapezoidal fuzzy preference (IT2TrFPR), which can deal with the complex decision information presented in the form of interval type-2 trapezoidal fuzzy numbers. In this paper, we mainly propose a novel interval type-2 trapezoidal fuzzy decision-making method with local consistency adjustment strategy and data envelopment analysis (DEA). Considering the harm of fog-haze pollution to the environment and human life, we apply the decision-making method to the problem about influence factors of for-haze weather. Firstly, we introduce the definition of IT2TrFPR that sufficiently expresses the uncertainty of original decision-making information. After that, we show the definition of the order consistency and additive consistency for IT2TrFPR. Considering that the original IT2TrFPR given by decision-makers usually does not satisfy the characteristic of consistency, to transform the unacceptable additive consistent IT2TrFPRs into acceptable ones, we design a consistency-improving algorithm that uses the local adjustment approach to preserve the original decision-making information as much as possible and avoids the original information loss. Then, an output-oriented interval type-2 trapezoidal fuzzy DEA model and the concept for quasi interval type-2 trapezoidal fuzzy priority weight are developed to derive the interval type-2 trapezoidal fuzzy priority weight vector (IT2TrFPW) and obtain the final ranking result of alternatives. Finally, the effectiveness of the proposed decision-making method is demonstrated by a numerical example of selecting the most crucial fog-haze influence factor. Meanwhile, we also conduct a comparative analysis by comparing our method with the existing methods to show some merits of the proposed method.},
  archive      = {J_APIN},
  author       = {Liu, Jinpei and Zheng, Yun and Jin, Feifei and Chen, Huayou},
  doi          = {10.1007/s10489-021-02354-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1653-1671},
  shortjournal = {Appl. Intell.},
  title        = {Local consistency adjustment strategy and DEA – driven interval type-2 trapezoidal fuzzy decision-making model and its application for fog-haze factor assessment problem},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid robust system considering outliers for electric
load series forecasting. <em>APIN</em>, <em>52</em>(2), 1630–1652. (<a
href="https://doi.org/10.1007/s10489-021-02473-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric load forecasting has become crucial to the safe operation of power grids and cost reduction in the production of power. Although numerous electric load forecasting models have been proposed, most of them are still limited by poor effectiveness in the model training and a sensitivity to outliers. The limitations of current methods may lead to extra operational costs of a power system or even disrupt its power distribution and network safety. To this end, we propose a new hybrid load-forecasting model, which is based on a robust extreme-learning machine and an improved whale optimization algorithm. Specifically, Huber loss, which is insensitive to outliers, is proposed as the objective function in extreme learning machine (ELM) training. In addition, an improved whale optimization algorithm is designed for the robust ELM training, in which a cellular automaton mechanism is used to enhance the local search. To verify our improved whale optimization algorithm, some experiments were then conducted based on seven benchmark test functions. Due to the enhancement of the local search, the improved optimizer was around 7% superior to the basic. Finally, our proposed hybrid forecasting model was validated by two real electric load datasets (Nanjing and New South Wales), and the experimental results confirmed that the proposed hybrid load-forecasting model could achieve satisfying improvements in both datasets.},
  archive      = {J_APIN},
  author       = {Yang, Yang and Tao, Zhenghang and Qian, Chen and Gao, Yuchao and Zhou, Hu and Ding, Zhe and Wu, Jinran},
  doi          = {10.1007/s10489-021-02473-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1630-1652},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid robust system considering outliers for electric load series forecasting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive fuzzy-SIFT rule-based registration for 3D cardiac
motion estimation. <em>APIN</em>, <em>52</em>(2), 1615–1629. (<a
href="https://doi.org/10.1007/s10489-021-02430-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Speckle tracking techniques are used to quantify cardiac deformation in 3D echocardiographic images. Elastic image registration methods are successful in solving 3D speckle tracking problems. However, a suitable solution should be exploited to deal with the poor spatio-temporal resolution in the echocardiographic images. That is why the registration problem may encounter some challenges in representing accurate features and defining suitable geometric transformation. The strong modeling ability of a fuzzy rule-based inference system can aid the challenge in geometric modeling. This paper, thus, aims to solve the 3D speckle tracking problem in a new scheme through a fuzzy modeling procedure. The algorithm begins to work by extracting a well-suited local feature descriptor, scale- invariant feature transform (SIFT). Then, the relevant features are aligned with sets of fuzzy rules the optimum parameters of which are adaptively learned in the hybrid learning process of adaptive-neuro fuzzy inference system (ANFIS) structure. Applying the adaptive fuzzy method on STRAUS synthetic dataset yields an acceptable tracking error below 1 mm. Further, strain analysis indicates the capacity of the proposed method in discriminating pathological diagnosis from a healthy one.},
  archive      = {J_APIN},
  author       = {Hosseini, Monire Sheikh and Moradi, Mahammad Hassan},
  doi          = {10.1007/s10489-021-02430-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1615-1629},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive fuzzy-SIFT rule-based registration for 3D cardiac motion estimation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving distribution problems in content-based
recommendation system with gaussian mixture model. <em>APIN</em>,
<em>52</em>(2), 1602–1614. (<a
href="https://doi.org/10.1007/s10489-021-02429-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play an important role in boosting purchasing consumption for many manufacturers by helping consumers find the most appropriate items. Furthermore, there are quite range of recommendation algorithms so far that can be efficient; however, a content-based algorithm is always the most popular, powerful, and productive method taken at the begin time of any project. In the negative aspect, somehow the accuracy of content-based algorithm results is still a concern that correlates to probabilistic similarity. In addition, the similarity calculation method is another crucial that affect the accuracy of content-based recommendation in probabilistic problems. In order to solve these problems, we propose a new content-based recommendation based on the Gaussian Mixture Model to improve the accuracy with more sensitive results for probabilistic recommendation problems. Our proposed method is experimented on a liquor dataset including six main flavour tastes, liquor main taste tags, and some other criteria. The method clusters n liquor records relied on n vectors of six dimensions into k group (k &lt; n) before applying a formula to sort the results. Compared our proposed algorithm with three other popular models on the above dataset, the accuracy of the experimental results not only outweighs the comparison to those of three other models but also attain a very speedy response time in real-life applications.},
  archive      = {J_APIN},
  author       = {Van Dat, Nguyen and Van Toan, Pham and Thanh, Ta Minh},
  doi          = {10.1007/s10489-021-02429-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1602-1614},
  shortjournal = {Appl. Intell.},
  title        = {Solving distribution problems in content-based recommendation system with gaussian mixture model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed task reassignment method in dynamic
environment for multi-UAV system. <em>APIN</em>, <em>52</em>(2),
1582–1601. (<a
href="https://doi.org/10.1007/s10489-021-02502-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the task reassignment problem for distributed multiple Unmanned Aerial Vehicle (multi-UAV) systems in dynamic environment. For a dynamic reassignment problem in a multi-UAV system, the task information may be subject to different dynamic events, and many existing task allocation algorithms require much computation and communication resource to achieve a feasible solution. Hence, this paper proposes a distributed method to cope with dynamic events that occur online during the execution of original schedules. First, a distributed framework for determining the processing strategy according to the types of dynamic events is introduced. Second, a partial reassignment algorithm (PRA) is proposed to support the framework and an incremental subteam formation mechanism and a partial releasing mechanism are developed to release the computation and communication burden. Furthermore, a modified inclusion phase to maximize assignment (MIP-MA) is also proposed in PRA to maximize the number of task allocations. Numerical simulations demonstrate that the proposed method is able to provide a conflict-free solution with less data exchanges and runtime.},
  archive      = {J_APIN},
  author       = {Yang, Mi and Bi, Wenhao and Zhang, An and Gao, Fei},
  doi          = {10.1007/s10489-021-02502-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1582-1601},
  shortjournal = {Appl. Intell.},
  title        = {A distributed task reassignment method in dynamic environment for multi-UAV system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Enhancing attributed network embedding via
enriched attribute representations. <em>APIN</em>, <em>52</em>(2), 1581.
(<a href="https://doi.org/10.1007/s10489-021-02706-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Correction to this paper has been published: https://doi.org/10.1007/s10489-021-02706-7},
  archive      = {J_APIN},
  author       = {Kakisim, Arzu Gorgulu},
  doi          = {10.1007/s10489-021-02706-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1581},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Enhancing attributed network embedding via enriched attribute representations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Enhancing attributed network embedding via enriched
attribute representations. <em>APIN</em>, <em>52</em>(2), 1566–1580. (<a
href="https://doi.org/10.1007/s10489-021-02498-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed network embedding enables to generate low-dimensional representations of network objects by leveraging both network structure and attribute data. However, how to properly combine two different information to achieve better vector representations remains still unclear. While some methods learn the embeddings from graph structure and attribute data separately, and then joint them, some existing methods use attribute data as an auxiliary information. However, the problem of integrating attribute data into an embedding process is an open problem due to the sparsity of attribute space. Especially in social networks such as Twitter and Flickr, the contexts may be short and the number of attributes defining objects may be very few, which cause that the contextual proximity among objects are not discovered properly. To address these issues, in this work, we present an enhanced attributed network embedding method via enriched attribute representations (ANEA) which generates low-dimensional representations of the network objects. ANEA incorporates attribute data into the embedding process by mapping the data to two different graph structures. To deal with the sparsity problem, our method provides to capture high-order semantic relations between attributes by performing random walks on these graphs. ANEA learns the embeddings through a joint space composed of the network structure and attributes. Therefore, it allows to discover latent attribute representations of the objects, which is helpful to explain what the common contextual interests are effective in modelling the proximity among nodes. Experiments on real-world networks confirm that ANEA outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Kakisim, Arzu Gorgulu},
  doi          = {10.1007/s10489-021-02498-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1566-1580},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing attributed network embedding via enriched attribute representations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly localization in regular textures based on deep
convolutional generative adversarial networks. <em>APIN</em>,
<em>52</em>(2), 1556–1565. (<a
href="https://doi.org/10.1007/s10489-021-02475-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixel-level anomaly localization is a challenging problem due to the lack of abnormal training samples. The existing adversarial network methods attempt to segment anomalies by reconstructing the image then comparing the reconstructed image with the original. However, reconstructing an image with adversarial networks involve complex training procedures and result in long run-times. This paper proposes a simpler and intuitive anomaly localization approach based on generative adversarial networks (GAN) for regular textured images. In the proposed method, a discriminator network generates an anomaly map and is trained by a generator network that generates imitations of anomalous samples. To lower computational costs, strided convolutions are used in the discriminator network to produce anomaly map for pixel blocks instead of individual pixels. Discriminator that is trained in the proposed scheme gains ability to segment the anomalies in images. The experimental results show that the performance of the proposed method is almost equivalent to that of the state-of-the-art methods. Besides, with an accompanying low-cost training phase it is faster and simpler to implement.},
  archive      = {J_APIN},
  author       = {Oz, Muhammed Ali Nur and Mercimek, Muharrem and Kaymakci, Ozgur Turay},
  doi          = {10.1007/s10489-021-02475-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1556-1565},
  shortjournal = {Appl. Intell.},
  title        = {Anomaly localization in regular textures based on deep convolutional generative adversarial networks},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed graph convolution and residual transformation network
for skeleton-based action recognition. <em>APIN</em>, <em>52</em>(2),
1544–1555. (<a
href="https://doi.org/10.1007/s10489-021-02517-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition based on a human skeleton is an extremely challenging research problem. The temporal information contained in the human skeleton is more difficult to extract than the spatial information. Many researchers focus on graph convolution networks and apply them to action recognition. In this study, an action recognition method based on a two-stream network called RNXt-GCN is proposed on the basis of the Spatial-Temporal Graph Convolutional Network (ST-GCN). The human skeleton is converted first into a spatial-temporal graph and a SkeleMotion image which are input into ST-GCN and ResNeXt, respectively, for performing the spatial-temporal convolution. The convolved features are then fused. The proposed method models the temporal information in action from the amplitude and direction of the action and addresses the shortcomings of isolated temporal information in the ST-GCN. The experiments are comprehensively performed on the four datasets: 1) UTD-MHAD, 2) Northwestern-UCLA, 3) NTU RGB-D 60, and 4) NTU RGB-D 120. The proposed model shows very competitive results compared with other models in our experiments. On the experiments of NTU RGB + D 120 dataset, our proposed model outperforms those of the state-of-the-art two-stream models.},
  archive      = {J_APIN},
  author       = {Liu, Shuhua and Bai, Xiaoying and Fang, Ming and Li, Lanting and Hung, Chih-Cheng},
  doi          = {10.1007/s10489-021-02517-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1544-1555},
  shortjournal = {Appl. Intell.},
  title        = {Mixed graph convolution and residual transformation network for skeleton-based action recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scientific document summarization in multi-objective
clustering framework. <em>APIN</em>, <em>52</em>(2), 1520–1543. (<a
href="https://doi.org/10.1007/s10489-021-02376-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth in the number of scientific articles has made it difficult for the researchers to keep themselves updated with the new developments. Scientific document summarization solves this problem by providing a summary of essential contributions. In this paper, we have presented a novel method of scientific document summarization using a multi-objective differential evolution technique. Here, firstly distinct important sentences are extracted by using citation contextualization. These sentences are further clustered using the concept of multi-objective clustering. Two objective functions, PBM index, and XB index, measuring the compactness and separation of sentence clusters, are simultaneously optimized utilizing the search capability of multi-objective differential evolution. We have conducted our experiments on CL-SciSumm 2016, CL-SciSumm 2017, CL-SciSumm 2018, and CL-SciSumm 2019 datasets. Obtained results of CL-SciSumm 2016 and CL-SciSumm 2017 are compared with the state-of-the-art methods. Evaluation results demonstrate that our method outperforms others in terms of ROUGE-2, ROUGE-3, and ROUGE-SU4 scores.},
  archive      = {J_APIN},
  author       = {Mishra, Santosh Kumar and Saini, Naveen and Saha, Sriparna and Bhattacharyya, Pushpak},
  doi          = {10.1007/s10489-021-02376-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1520-1543},
  shortjournal = {Appl. Intell.},
  title        = {Scientific document summarization in multi-objective clustering framework},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MC-net: Multi-scale context-attention network for medical CT
image segmentation. <em>APIN</em>, <em>52</em>(2), 1508–1519. (<a
href="https://doi.org/10.1007/s10489-021-02506-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The encoder-decoder CNN architecture has greatly improved CT medical image segmentation, but it encounters a bottleneck due to the loss of details in the encoding process, which limits the accuracy improvement. To address this problem, we propose a multi-scale context-attention network (MC-Net). The key idea is to explore the useful information across multiple scales and the context for the segmentation of objects of medical interest. Through the introduction of multi-scale and context-attention modules, MC-Net gains the ability to extract local and global semantic information around targets. To further improve the segmentation accuracy, we weight the pixels depending on whether they belong to targets. Many experiments on a lung dataset and a bladder dataset demonstrate that the proposed MC-Net outperforms state-of-the-art methods in terms of accuracy, sensitivity, the area under the receiver operating characteristic curve and the Dice score.},
  archive      = {J_APIN},
  author       = {Xia, Haiying and Ma, Mingjun and Li, Haisheng and Song, Shuxiang},
  doi          = {10.1007/s10489-021-02506-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1508-1519},
  shortjournal = {Appl. Intell.},
  title        = {MC-net: Multi-scale context-attention network for medical CT image segmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RED: Learning the role embedding in networks via
discrete-time quantum walk. <em>APIN</em>, <em>52</em>(2), 1493–1507.
(<a href="https://doi.org/10.1007/s10489-021-02342-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Role embedding aims to embed role-similar nodes into similar representations. Role embedding is significant in graph mining, providing a key bridge between traditional role analysis and machine learning. However, current methods suffer from information loss due to the inherent drawbacks, thus failing to capture role information comprehensively from both global and local perspectives. This paper proposes RED (Role Embedding via Discrete-time quantum walk) to address the above issue via quantum walks, whose characters are naturally applicable to role embedding. Based on the superposition, RED simultaneously learns global role representations by evolving features in a global evolution. Besides, RED uses the quasi-periodicity to capture long-term evolving features within steps. To represent local role information, RED simulates a wave-like diffusion by biased walks, where it learns the closeness from accumulated probabilities for local role representations. To the best of our knowledge, RED is the first to apply quantum walks to the role embedding. Substantial experiments demonstrate that RED significantly outperforms state-of-the-art methods by up to 2300.00% in role detection, 90.93% in equivalency identification, and is overwhelmingly superior in robustness.},
  archive      = {J_APIN},
  author       = {Wang, Xin and Jian, Songlei and Lu, Kai and Zhang, Yi and Liu, Kai},
  doi          = {10.1007/s10489-021-02342-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1493-1507},
  shortjournal = {Appl. Intell.},
  title        = {RED: Learning the role embedding in networks via discrete-time quantum walk},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization framework and applications of training
multi-state influence nets. <em>APIN</em>, <em>52</em>(2), 1477–1492.
(<a href="https://doi.org/10.1007/s10489-021-02514-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence nets (INs) are proposed on the basis of causal logic for the purpose of depicting causal relationship strengths in complex systems. However, it is difficult to accurately determine the causal strength (CAST) parameters on the basis of expert knowledge only, particularly for practical problems with multiple attributes and multiple states. In this paper, the original IN with binary states is first extended into a multi-state IN, endowing the IN with the ability to model and infer under multiple states. Based on this method, an optimization framework is proposed to optimize the CAST parameters of multi-state INs. Finally, two practical cases are studied to verify the feasibility and efficiency of the proposed multi-state IN with an optimization framework in regression and classification.},
  archive      = {J_APIN},
  author       = {Sun, Jianbin and You, Yaqian and Ge, Bingfeng and Tan, Yuejin and Yang, Kewei},
  doi          = {10.1007/s10489-021-02514-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1477-1492},
  shortjournal = {Appl. Intell.},
  title        = {Optimization framework and applications of training multi-state influence nets},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An immune optimization based deterministic dendritic cell
algorithm. <em>APIN</em>, <em>52</em>(2), 1461–1476. (<a
href="https://doi.org/10.1007/s10489-020-02098-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is an important issue, which has been deeply studied in different research domains and application fields. The dendritic cell algorithm (DCA) is one of the most popular artificial immune system inspired approaches to handle anomaly detection problems. The performance of DCA depends significantly on the parameters used to compute the relationship between input instance and detectors. However, we find that while the DCA’s performance is good in practical applications, it is difficult to analyze due to the empirical based parameters and lacks adaptability. This paper studies how to effectively learn appropriate parameters for deterministic DCA (dDCA) for anomaly detection tasks. In particular, we propose a novel immune optimization based dDCA (IO-dDCA) for anomaly detection. It consists of dDCA classification, T cell (TC) classification, gradient descent optimization and immune nonlinear dynamic optimization. First, the dDCA is regarded as a binary classifier, and the data instances which are labeled as normal will be classified by a T cell inspired classification method, so as to improve the classification performance of dDCA. Then, to improve dDCA’s adaptability, gradient descent is adopted for dDCA parameters’ optimization. Finally, the immune nonlinear model is introduced to adjust learning rate in gradient descent to find the optimal parameters. The theoretical and experimental performance analysis of IO-dDCA show effectiveness of the novel approach through simulations, and the experimental results show that the proposed IO-dDCA has good classification accuracy.},
  archive      = {J_APIN},
  author       = {Zhou, Wen and Liang, Yiwen},
  doi          = {10.1007/s10489-020-02098-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1461-1476},
  shortjournal = {Appl. Intell.},
  title        = {An immune optimization based deterministic dendritic cell algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An accurate cell tracking approach with self-regulated
foraging behavior of ant colonies in dynamic microscopy images.
<em>APIN</em>, <em>52</em>(2), 1448–1460. (<a
href="https://doi.org/10.1007/s10489-021-02424-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic analysis of cell behavior is fundamental to the evaluation of the correlation between disease and abnormal cell migration. In this study, a self-regulated foraging behavior for cell tracking is proposed under the framework of dual prediction and update of an ant colony and its pheromone field. In the regulated behavior of ant foraging, three strategies are employed: range of foraging, re-sampling, based on the initial distribution of the ant colony; and the stopping criteria of the foraging. The foraging movement of an ant colony is confined to its relevant range determined by the corresponding pheromone field and dynamically varies over iterations. An initial distribution of the ant colonies at each iteration is generated by a Gaussian based re-sampling strategy to regulate an effective search in a centralized manner as a colony. An adaptive stopping criterion of foraging is put forward on the basis of Kullback-Leibler divergence of two approximately Gaussian pheromone fields between two consecutive iterations. The experimental results of the application to cell tracking show that the effectiveness of the algorithm, and demonstrate that it is better than the compared methods.},
  archive      = {J_APIN},
  author       = {Lu, Mingli and Xu, Benlian and Nener, Brett and Cong, Jinliang and Shi, Jian},
  doi          = {10.1007/s10489-021-02424-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1448-1460},
  shortjournal = {Appl. Intell.},
  title        = {An accurate cell tracking approach with self-regulated foraging behavior of ant colonies in dynamic microscopy images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional mutual information-based feature selection
algorithm for maximal relevance minimal redundancy. <em>APIN</em>,
<em>52</em>(2), 1436–1447. (<a
href="https://doi.org/10.1007/s10489-021-02412-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many feature selection algorithms based on mutual information and three-dimensional mutual information (TDMI) among features and the class label, since these algorithms do not consider TDMI among features, feature selection performance can be influenced. In view of the problem, this paper investigates feature selection based on TDMI among features. According to the maximal relevance minimal redundancy criterion, joint mutual information among the class label and feature set is adopted to describe relevance, and mutual information between feature set is exploited to describe redundancy. Then, joint mutual information among the class label and feature set as well as mutual information between feature set is decomposed. In the process of decomposing, TDMI among features is considered and an objective function is obtained. Finally, a feature selection algorithm based on conditional mutual information for maximal relevance minimal redundancy (CMI-MRMR) is proposed. To validate the performance, we compare CMI-MRMR with several feature selection algorithms. Experimental results show that CMI-MRMR can achieve better feature selection performance.},
  archive      = {J_APIN},
  author       = {Gu, Xiangyuan and Guo, Jichang and Xiao, Lijun and Li, Chongyi},
  doi          = {10.1007/s10489-021-02412-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1436-1447},
  shortjournal = {Appl. Intell.},
  title        = {Conditional mutual information-based feature selection algorithm for maximal relevance minimal redundancy},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to: The impact of time windows constraints on
metaheuristics implementation: A study for the discrete and dynamic
berth allocation problem. <em>APIN</em>, <em>52</em>(2), 1435. (<a
href="https://doi.org/10.1007/s10489-021-02571-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Correction to this paper has been published: https://doi.org10.1007/s10489-021-02571-4},
  archive      = {J_APIN},
  author       = {Barbosa, Flávia and Berbert Rampazzo, Priscila C. and de Azevedo, Anibal Tavares and Yamakami, Akebo},
  doi          = {10.1007/s10489-021-02571-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1435},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: the impact of time windows constraints on metaheuristics implementation: a study for the discrete and dynamic berth allocation problem},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impact of time windows constraints on metaheuristics
implementation: A study for the discrete and dynamic berth allocation
problem. <em>APIN</em>, <em>52</em>(2), 1406–1434. (<a
href="https://doi.org/10.1007/s10489-021-02420-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the development of a mechanism to deal with time windows constraints. To the best of our knowledge, the time windows constraints are difficult to be fulfilled even for state-of-the-art methods. Therefore, the main contribution of this paper is to propose a new computational technique to deal with such constraints. Such technique was tested combined with two metaheuristics to solve the discrete and dynamic Berth Allocation Problem. The technique ensures obtaining feasible solutions in terms of vessels time windows constraints, which are treated as hard constraints. A data set generator was created, resulting in a diversity of problems in terms of time windows constraints. A detailed computational analysis was carried out to compare the performance of both metaheuristics considering the technique.},
  archive      = {J_APIN},
  author       = {Barbosa, Flávia and Rampazzo, Priscila C. Berbert and de Azevedo, Anibal Tavares and Yamakami, Akebo},
  doi          = {10.1007/s10489-021-02420-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1406-1434},
  shortjournal = {Appl. Intell.},
  title        = {The impact of time windows constraints on metaheuristics implementation: A study for the discrete and dynamic berth allocation problem},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental fuzzy temporal association rule mining using
fuzzy grid table. <em>APIN</em>, <em>52</em>(2), 1389–1405. (<a
href="https://doi.org/10.1007/s10489-021-02407-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional temporal association rules mining algorithms cannot dynamically update the temporal association rules within the valid time interval with increasing data. In this paper, a new algorithm called incremental fuzzy temporal association rule mining using fuzzy grid table (IFTARMFGT) is proposed by combining the advantages of boolean matrix with incremental mining. First, multivariate time series data are transformed into discrete fuzzy values that contain the time intervals and fuzzy membership. Second, in order to improve the mining efficiency, the concept of boolean matrices was introduced into the fuzzy membership to generate a fuzzy grid table to mine the frequent itemsets. Finally, in view of the Fast UPdate (FUP) algorithm, fuzzy temporal association rules are incrementally mined and updated without repeatedly scanning the original database by considering the lifespan of each item and inheriting the information from previous mining results. The experiments show that our algorithm provides better efficiency and interpretability in mining temporal association rules than other algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Ling and Gui, Lingpeng and Zhu, Hui},
  doi          = {10.1007/s10489-021-02407-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1389-1405},
  shortjournal = {Appl. Intell.},
  title        = {Incremental fuzzy temporal association rule mining using fuzzy grid table},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Average up-sample network for crowd counting. <em>APIN</em>,
<em>52</em>(2), 1376–1388. (<a
href="https://doi.org/10.1007/s10489-021-02470-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of crowd counting is receiving increased attention recently, but it still faces many challenges, such as extremely dense scene, scale variation and background clutter. The quality of generated density map plays an important role in counting performance. In this paper, we propose an encoder-decoder structure network called Average Up-sample Convolution Neural Network (AU-CNN), for high-quality density map and accurate counting estimation. The encoder extracts the features of input image while the decoder gradually recovers the size of feature map to the original size of input image by developing a simple but effective average up-sample module. The average up-sample module takes the average of interpolation results from three different up-sample methods, without adding any other redundant parameters. Moreover, compared with most existing counting algorithm using only Euclidean loss, we use a combined loss function of Euclidean loss and count loss to optimize the network, which is demonstrated effective in performance improving. Experiments on the ShanghaiTech, UCF_CC_50, and UCF_QNRF demonstrate the great counting performance and robustness of our proposed method.},
  archive      = {J_APIN},
  author       = {Wu, Di and Fan, Zheyi and Cui, Mengjie},
  doi          = {10.1007/s10489-021-02470-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1376-1388},
  shortjournal = {Appl. Intell.},
  title        = {Average up-sample network for crowd counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). DECA: A novel multi-scale efficient channel attention
module for object detection in real-life fire images. <em>APIN</em>,
<em>52</em>(2), 1362–1375. (<a
href="https://doi.org/10.1007/s10489-021-02496-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel attention mechanisms have attracted more and more researchers because of their generality and effectiveness in deep convolutional neural networks(DCNNs). However, the signal encoding methods of the current popular channel attention mechanisms are limited. For example, SENet uses the full-connection method to encode channel relevance, which is parameters-costly; ECANet uses 1D-Convolution to encode channel relevance, which is parameter fewer but can only encode per k adjacent channels in a fixed scale. This paper proposes a novel dilated efficient channel attention module(DECA), which consists of a novel multi-scale channel encoding method and a novel channel relevance feature fusion method. We empirically show that different scale channel relevance also contributes to performance, and fusing various scale channel relevance features can obtain more powerful channel feature representation. Besides, we widely use the weight-sharing method in the DECA module to make it more efficient. Specifically, we have applied our module to the real-life fire image detection task to evaluate its effectiveness. Extensive experiments on different backbone depths, detectors, and fire datasets have shown that the average performance boost of DECA module is more than 4.5% compare to the baselines. Meanwhile, DECA outperforms other state-of-art attention modules while keeping lower or comparable parameters in the experiments. The experimental results on different datasets also shown that the DECA module holds great generalization ability.},
  archive      = {J_APIN},
  author       = {Wang, Junjie and Yu, Jiong and He, Zhu},
  doi          = {10.1007/s10489-021-02496-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1362-1375},
  shortjournal = {Appl. Intell.},
  title        = {DECA: A novel multi-scale efficient channel attention module for object detection in real-life fire images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Faster heuristics for graph burning. <em>APIN</em>,
<em>52</em>(2), 1351–1361. (<a
href="https://doi.org/10.1007/s10489-021-02411-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph burning is a process of information spreading through the network by an agent in discrete steps. The problem is to find an optimal sequence of nodes that have to be given information so that the network is covered in least number of steps. Graph burning problem is NP-Hard for which two approximation algorithms and a few heuristics have been proposed in the literature. In this work, we propose three heuristics, namely, Backbone Based Greedy Heuristic (BBGH), Improved Cutting Corners Heuristic (ICCH), and Component Based Recursive Heuristic (CBRH). These are mainly based on Eigenvector centrality measure. BBGH finds a backbone of the network and picks vertex to be burned greedily from the vertices of the backbone. ICCH is a shortest path based heuristic and picks vertex to burn greedily from best central nodes. The burning number problem on disconnected graphs is harder than on the connected graphs. For example, burning number problem is easy on a path where as it is NP-Hard on disjoint paths. In practice, large networks are generally disconnected and moreover even if the input graph is connected, during the burning process the graph among the unburned vertices may be disconnected. For disconnected graphs, ordering the components is crucial. Our CBRH works well on disconnected graphs as it prioritizes the components. All the heuristics have been implemented and tested on several bench-mark networks including large networks of size more than 50K nodes. The experimentation also includes comparison to the approximation algorithms. The advantages of our algorithms are that they are much simpler to implement and also several orders faster than the heuristics proposed in the literature.},
  archive      = {J_APIN},
  author       = {Gautam, Rahul Kumar and Kare, Anjeneya Swami and S., Durga Bhavani},
  doi          = {10.1007/s10489-021-02411-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1351-1361},
  shortjournal = {Appl. Intell.},
  title        = {Faster heuristics for graph burning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed learning automata based approach to inferring
urban structure via traffic flow. <em>APIN</em>, <em>52</em>(2),
1338–1350. (<a
href="https://doi.org/10.1007/s10489-021-02465-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow can be used as a reference for knowledge generation, which is highly important in urban planning. One of the significant applications of traffic data is decision making about the structure of roads connecting zones of a city. It leads us to an optimal connection between important areas like business centers, shopping malls, construction sites, residential complexes, and other parts of a city which is the motivation of this research. The main question is how to infer the optimal connectivity network considering the current structure of an urban area and time-varying traffic dynamics. Therefore a novel formulation is created in this paper to solve the optimization problem using available data. A proposed algorithm is presented to infer the optimal structure that is a distributed learning automata-based approach. A matrix called estimated optimal connectivity represents the favorite structure and it is optimized utilizing signals about the current system and traffic dynamics from the environment. Two types of data, including synthetic and real-world, are used to show the algorithm’s ability. After many experiments, the algorithm showed capability of optimizing the structure by finding new paths connecting the most correlated areas.},
  archive      = {J_APIN},
  author       = {Yasinian, Hamid and Esmaeilpour, Mansour},
  doi          = {10.1007/s10489-021-02465-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1338-1350},
  shortjournal = {Appl. Intell.},
  title        = {Distributed learning automata based approach to inferring urban structure via traffic flow},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated sleep apnea detection using optimal
duration-frequency concentrated wavelet-based features of pulse oximetry
signals. <em>APIN</em>, <em>52</em>(2), 1325–1337. (<a
href="https://doi.org/10.1007/s10489-021-02422-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep apnea is a potential sleep disorder, which deteriorates the quality of sleep. It is characterized by the obstruction in nasal airflow, which results in a low concentration of oxygen in the blood. Though polysomnography (PSG) is considered as a gold standard for diagnosing sleep apnea, it is arduous, demanding, expensive and inconvenient to the patients. This study presents an effective, efficient and sustainable sleep apnea automated detection system using pulse oximetry signals (SpO2), which indicate the percentage of oxygen content in the blood. The conventional methods, which employ PSG recordings are computationally intensive and costly. Nowadays, the focus is on non-invasive and portable devices for higher convenience and cost-effective diagnosis. In this work, we have used optimal duration-bandwidth concentrated wavelet transform to decompose the SpO2 signals into various sub-bands (SBs). The Shannon entropy features are extracted from various SBs coefficients. These features are then fed to various supervised machine learning algorithms, including decision trees and ensemble algorithms for automated detection of sleep apnea. The proposed model has attained the highest accuracy of 95.97%, and area under the receivers operating characteristics curve (AUC) of 0.98 for optimal wavelet-based Shannon entropy features when an ensemble boosting technique called random under-sampling boosting (RUSBoost) is employed with ten-fold cross-validation strategy. Thus, the proposed model is portable, economical, and accurate which can be used even at homes.},
  archive      = {J_APIN},
  author       = {Sharma, Manish and Kumbhani, Divyash and Yadav, Anuj and Acharya, U. Rajendra},
  doi          = {10.1007/s10489-021-02422-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1325-1337},
  shortjournal = {Appl. Intell.},
  title        = {Automated sleep apnea detection using optimal duration-frequency concentrated wavelet-based features of pulse oximetry signals},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A group consensus-based travel destination evaluation method
with online reviews. <em>APIN</em>, <em>52</em>(2), 1306–1324. (<a
href="https://doi.org/10.1007/s10489-021-02410-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of the massive online information, non-expert decision-making problems (such as travel destination selection) can be solved. Online reviews provide decision-making opinions and weight information for tourists who have never been to the alternative travel destinations. To deal with the increasing tourism products and group tourists, this study proposes a novel group consensus-based travel destination evaluation method with online reviews, which considers the missing preference estimating and group consensus reaching process. Firstly, decision opinions are represented through the sentiment matrix with the percentage distribution. Secondly, to obtain the weight vector of attributes, the incomplete complementary matrices with the preference of attributes are given by group users. Subsequently, the missing preference values in the matrix are estimated. Thirdly, all users are required to reach group consensus based on the minimum adjustment cost feedback mechanism. Finally, the sentiment matrix with the percentage distribution can be aggregated by the weight vectors. So that the ranking of alternatives can be obtained. In this study, an example of travel destination evaluation based on the online reviews of Dazhong.com and Ctrip.com is given to illustrate the use of the proposed method.},
  archive      = {J_APIN},
  author       = {Wu, Jian and Hong, Qing and Cao, Mingshuo and Liu, Yujia and Fujita, Hamido},
  doi          = {10.1007/s10489-021-02410-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1306-1324},
  shortjournal = {Appl. Intell.},
  title        = {A group consensus-based travel destination evaluation method with online reviews},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel approach to generating high-resolution adversarial
examples. <em>APIN</em>, <em>52</em>(2), 1289–1305. (<a
href="https://doi.org/10.1007/s10489-021-02371-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have improved expressive performance in many artificial intelligence (AI) fields in recent years. However, they can easily induce incorrect behavior due to adversarial examples. The state-of-the-art strategies for generating adversarial examples were established as generative adversarial nets (GAN). Due to a large amount of data and the high computational resources required, previous GAN-based work has only generated adversarial examples for small datasets, resulting in a less favorable visualization of the generated images. To address this problem, we propose a feasible approach, which improves on the AdvGAN framework through data augmentation, combined with PCA and KPCA to map the input instance’s main features onto the latent variables. Experimental results indicate that our approach can generate more natural perturbations on high-resolution images while maintaining 96% + of the features of the original input instance. Moreover, we measured 90.30% attack success rates on CIFAR-10 against the target model ResNet152, a small improvement compared to 88.69% for AdvGAN. We applied the same idea to ImageNet and LSUN, and the results showed that it not only achieves a high attack success rate,but can generate strongly semantically adversarial examples with better transferability on prevailing DNNs classification models. We also show that our approach yields competitive results compared to sensitivity analysis-based or optimization-based attacks notable in the literature.},
  archive      = {J_APIN},
  author       = {Fang, Xianjin and Li, Zhiwei and Yang, Gaoming},
  doi          = {10.1007/s10489-021-02371-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1289-1305},
  shortjournal = {Appl. Intell.},
  title        = {A novel approach to generating high-resolution adversarial examples},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online multi-object tracking using multi-function
integration and tracking simulation training. <em>APIN</em>,
<em>52</em>(2), 1268–1288. (<a
href="https://doi.org/10.1007/s10489-021-02457-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the development of deep-learning, the performance of multi-object tracking algorithms based on deep neural networks has been greatly improved. However, most methods separate different functional modules into multiple networks and train them independently on specific tasks. When these network modules are used directly, they are not compatible with each other effectively, nor can they be better adapted to the multi-object tracking task, which leads to a poor tracking effect. Therefore, a network structure is designed to aggregate the regression of objects between frames and the extraction of appearance features into one model to improve the harmony between various functional modules of multi-object tracking. To improve the support for the multi-object tracking task, an end-to-end training method is also proposed to simulate the multi-object tracking process during the training and expand the training data by using the historical position of the target combined with the prediction of the motion model. A metric loss that can take advantage of the historical appearance features of the target is also used to train the extraction module of appearance features to improve the temporal correlation of extracted appearance features. Evaluation results on the MOTChallenge benchmark datasets show that the proposed approach achieves state-of-the-art performance.},
  archive      = {J_APIN},
  author       = {Yang, Jieming and Ge, Hongwei and Yang, Jinlong and Tong, Yubing and Su, Shuzhi},
  doi          = {10.1007/s10489-021-02457-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1268-1288},
  shortjournal = {Appl. Intell.},
  title        = {Online multi-object tracking using multi-function integration and tracking simulation training},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented low-rank methods for gaussian process regression.
<em>APIN</em>, <em>52</em>(2), 1254–1267. (<a
href="https://doi.org/10.1007/s10489-021-02481-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents techniques to improve the prediction accuracy of approximation methods used in Gaussian process regression models. Conventional methods such as Nyström and subset of data methods rely on low-rank approximations to the kernel matrix derived from a set of representative data points. Prediction accuracy suffers when the number of representative points is small or when the length scale is small. The techniques proposed here augment the set of representative points with neighbors of each test input to improve accuracy. Our approach leverages the general structure of the problem through the low-rank approximation and improves its accuracy further by exploiting locality at each test input. Computations involving neighbor points are cast as updates to the base approximation which result in significant savings. To ensure numerical stability, prediction is done via orthogonal projection onto the subspace of the kernel approximation derived from the augmented set. Experiments on synthetic and real datasets show that our approach is robust with respect to changes in length scale and matches the prediction accuracy of the full kernel matrix while using fewer points for kernel approximation. This results in faster and more accurate predictions compared to conventional methods.},
  archive      = {J_APIN},
  author       = {Thomas, Emil and Sarin, Vivek},
  doi          = {10.1007/s10489-021-02481-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1254-1267},
  shortjournal = {Appl. Intell.},
  title        = {Augmented low-rank methods for gaussian process regression},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local community detection algorithm based on local
modularity density. <em>APIN</em>, <em>52</em>(2), 1238–1253. (<a
href="https://doi.org/10.1007/s10489-020-02052-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to global community detection, local community detection aims to find communities that contain a given node. Therefore, it can be regarded as a specific and personalized community detection task. Local community detection algorithms based on modularity are widely studied and applied because of their concise strategies and prominent effects. However, they also face challenges, such as sensitivity to seed node selection and unstable communities. In this paper, a local community detection algorithm based on local modularity density is proposed. The algorithm divides the formation process of local communities into a core area detection stage and a local community extension stage according to community tightness based on the Jaccard coefficient. In the core area detection stage, the modularity density is used to ensure the quality of the communities. In the local community extension stage, the influence of nodes and the similarity between the nodes and the local community are utilized to determine boundary nodes to reduce the sensitivity to seed node selection. Experimental results on real and artificial networks demonstrated that the proposed algorithm can detect local communities with high accuracy and stability.},
  archive      = {J_APIN},
  author       = {Guo, Kun and Huang, Xintong and Wu, Ling and Chen, Yuzhong},
  doi          = {10.1007/s10489-020-02052-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1238-1253},
  shortjournal = {Appl. Intell.},
  title        = {Local community detection algorithm based on local modularity density},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A new distance-based total uncertainty measure in
dempster-shafer evidence theory. <em>APIN</em>, <em>52</em>(2),
1209–1237. (<a
href="https://doi.org/10.1007/s10489-021-02378-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty measure in Dempster-Shafer (D-S) evidence theory is crucial to assess the quality of information conveyed by belief structures. Most of the previous studies consider this issue from the perspective of viewing the D-S evidence theory as a generalization of probability theory. However, the inconsistency between D-S evidence theory and probability theory may lead to some limitations to existing measures. Deng et al proposed an improved total uncertainty measure which is directly based on D-S evidence theory. In their measure, the belief structures are transformed to belief interval which is constructed by the belief function and plausibility function of a proposition. Inspired by the previous research, a new total uncertainty measure (NTU) is proposed in this paper. The proposed measure is based on the Euclidean distance between the belief interval of the singleton subset and the most uncertain interval. It not only retains the properties and advantages of the previous measure, but also presents a higher sensitivity and greater extent to the change of evidences. Some numerical examples, practical applications and related analyses are used to verify the rationality and sensitivity of the proposed measure.},
  archive      = {J_APIN},
  author       = {Li, Rongfei and Chen, Zhiyuan and Li, Hao and Tang, Yongchuan},
  doi          = {10.1007/s10489-021-02378-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1209-1237},
  shortjournal = {Appl. Intell.},
  title        = {A new distance-based total uncertainty measure in dempster-shafer evidence theory},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stable community detection approach for complex network
based on density peak clustering and label propagation. <em>APIN</em>,
<em>52</em>(2), 1188–1208. (<a
href="https://doi.org/10.1007/s10489-021-02287-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dividing a network into communities has great benefits in understanding the characteristics of the network. The label propagation algorithm (LPA) is a fast and convenient community detection algorithm. However, the community initialization of LPA does not take advantage of topological information of networks, and its robustness is poor. In this paper, we propose a stable community detection algorithm based on density peak clustering and label propagation (DS-LPA). First, the local density calculation method in density peak clustering algorithm is improved in finding the community center of the network, so as to build a suitable initial community, which can improve the quality of community partition. Then, the label update order is determined reasonably by computing the information transmission power of nodes, and the solutions for multiple candidate labels are provided, which greatly improved the robustness of the algorithm. DS-LPA is compared with other seven algorithms on the synthetic network and real-world networks. NMI, ARI, and modularity are used to evaluate these algorithms. It can be concluded that DS-LPA has a higher performance than most comparison algorithms on synthetic network with ten different mixed parameters by statistical testing. And DS-LPA can quickly calculate the best community partition on different sizes of real-world networks.},
  archive      = {J_APIN},
  author       = {Li, Chuanwei and Chen, Hongmei and Li, Tianrui and Yang, Xiaoling},
  doi          = {10.1007/s10489-021-02287-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1188-1208},
  shortjournal = {Appl. Intell.},
  title        = {A stable community detection approach for complex network based on density peak clustering and label propagation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved multiobjective cultural algorithm with a
multistrategy knowledge base. <em>APIN</em>, <em>52</em>(2), 1157–1187.
(<a href="https://doi.org/10.1007/s10489-021-02313-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the dual-inheritance framework of cultural evolution, an improved multiobjective cultural algorithm (IMOCA) with a multistrategy knowledge base is presented in this paper. Inspired by the original versions of the cultural algorithm (CA), four basic types of knowledge sources, i.e., normative, situational, topographical and historical knowledge, are effectively utilized in the proposed IMOCA. Several modifications with the knowledge base of the IMOCA are made to tackle the characteristics of the multiobjective problem. Situational knowledge is used as an external repository for storing elite individuals, and the redesigned topographical knowledge functions as a search engine to broaden the expansion of the obtained solution set. The historical knowledge used in the IMOCA aims to select a productive knowledge source to generate new individuals. Furthermore, a simple mutation scheme is introduced into the knowledge base as an influence function for the purpose of fine tuning in the late stage of search. After configuring the parameters used in IMOCA, two classic benchmark suites, i.e., WFG and MaF, are used to assess the performance of the IMOCA in approaching the Pareto fronts (PFs) with accuracy and diversity. Nondominated solution sets obtained by the IMOCA are compared with 8 state-of-the-art multiobjective algorithms available in the literature. A statistical analysis is conducted, which reveals that, by modifying the basic knowledge structure of the CA, the proposed multiobjective cultural algorithm is competent enough to handle multiobjective problems with competitive performance.},
  archive      = {J_APIN},
  author       = {Mao, Zhengyan and Liu, Mandan},
  doi          = {10.1007/s10489-021-02313-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1157-1187},
  shortjournal = {Appl. Intell.},
  title        = {An improved multiobjective cultural algorithm with a multistrategy knowledge base},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental study of objective pain measurement using
pupillary response based on genetic algorithm and artificial neural
network. <em>APIN</em>, <em>52</em>(2), 1145–1156. (<a
href="https://doi.org/10.1007/s10489-021-02458-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining an objective measurement of the pain level of a patient has always been challenging for health care providers. The most common method of pain assessment in the hospital setting is asking the patients’ verbal ratings, which is considered to be a subjective approach. In order to get an objective pain level of a patient, we propose measuring pain level objectively using the pupillary response and machine learning algorithms. Thirty-two healthy subjects were enrolled in this study at Northeastern University. A painful stimulus was applied to healthy subjects by asking them to place their hands inside a bucket filled with iced water. We extracted 11 features from the pupil diameter data. To get the optimal subset of the features, a genetic algorithm (GA) was used to select features for the artificial neural network (ANN) classifier. Before feature selection, the f1-score of ANN was 54.0 ± 0.25% with all 11 features. After feature selection, ANN had the best performance with an accuracy of 81.0% using the selected feature subset, namely the Mean, the Root Mean Square (RMS), and the Pupillary Area Under Curve (PAUC). The experimental results suggested that pupillary response together with machine learning algorithms could be a promising method of objective pain level assessment. The outcomes of this study could improve patients’ experience of pain measurement in telehealthcare, especially during a pandemic when most people had to stay at home.},
  archive      = {J_APIN},
  author       = {Wang, Li and Guo, Yikang and Dalip, Biren and Xiao, Yan and Urman, Richard D. and Lin, Yingzi},
  doi          = {10.1007/s10489-021-02458-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {2},
  pages        = {1145-1156},
  shortjournal = {Appl. Intell.},
  title        = {An experimental study of objective pain measurement using pupillary response based on genetic algorithm and artificial neural network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estate price prediction system based on temporal and spatial
features and lightweight deep learning model. <em>APIN</em>,
<em>52</em>(1), 808–834. (<a
href="https://doi.org/10.1007/s10489-021-02472-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of estate price prediction systems is one of the issues that researchers are paying the most attention to. A good estate price prediction system can shorten the time it takes buyers to consider estates and invigorate the estate market. Generally speaking, an estate price prediction system considers the temporal and spatial features of the estate. In addition, the estate price prediction system can also be launched online for users to make instant online queries with, which means that it needs short run time. However, most existing studies only considered either temporal or spatial features and could not consider both, thereby resulting in questionable prediction accuracy. Although deep learning may increase prediction accuracy, it does not meet the short run time requirement. We therefore presented three ideas in this study to overcome these issues: (1) designing a novel spatiotemporal data structure, the Space-Time Influencing Figure (STIF), to quantify the influence of changes in the facilities surrounding each estate on estate price, (2) designing a novel CNN-LSTM model to go with the STIFs for estate price prediction, and (3) designing a new framework to extract the most important features to estate price for certain types of estates and combining these features with a shallow RNN for modeling. The computation cost of this model is far lower than that of a CNN-LSTM model, making it suitable for practical application. Finally, we used actual estate data from Taiwan to verify that the proposed approach can effectively and swiftly predict estate prices.},
  archive      = {J_APIN},
  author       = {Chiu, Sheng-Min and Chen, Yi-Chung and Lee, Chiang},
  doi          = {10.1007/s10489-021-02472-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {808-834},
  shortjournal = {Appl. Intell.},
  title        = {Estate price prediction system based on temporal and spatial features and lightweight deep learning model},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cure models to estimate time until hospitalization due to
COVID-19. <em>APIN</em>, <em>52</em>(1), 794–807. (<a
href="https://doi.org/10.1007/s10489-021-02311-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A short introduction to survival analysis and censored data is included in this paper. A thorough literature review in the field of cure models has been done. An overview on the most important and recent approaches on parametric, semiparametric and nonparametric mixture cure models is also included. The main nonparametric and semiparametric approaches were applied to a real time dataset of COVID-19 patients from the first weeks of the epidemic in Galicia (NW Spain). The aim is to model the elapsed time from diagnosis to hospital admission. The main conclusions, as well as the limitations of both the cure models and the dataset, are presented, illustrating the usefulness of cure models in this kind of studies, where the influence of age and sex on the time to hospital admission is shown.},
  archive      = {J_APIN},
  author       = {Pedrosa-Laza, Maria and López-Cheda, Ana and Cao, Ricardo},
  doi          = {10.1007/s10489-021-02311-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {794-807},
  shortjournal = {Appl. Intell.},
  title        = {Cure models to estimate time until hospitalization due to COVID-19},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSTM-TC: Bitcoin coin mixing detection method with a high
recall. <em>APIN</em>, <em>52</em>(1), 780–793. (<a
href="https://doi.org/10.1007/s10489-021-02453-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coin mixing is a class of techniques used to enhance Bitcoin transaction privacy, and those well-performing coin mixing algorithms can effectively prevent most transaction analysis attacks. Based on this premise, to have a well-functioning transaction analysis algorithm requires coin mixing detection with a high recall to ensure accuracy. Most practical coin mixing algorithms do not change the Bitcoin protocol. Therefore, the transactions they generate are not fundamentally different from regular transactions. Existing coin mixing detection methods are commonly rule-based that can only identify coin mixing classes with well-defined patterns, which leads to an overall low recall rate. Multiple rules could improve the recall in this situation, yet they are ineffective for new classes and classes with ambiguous patterns. This paper considers coin mixing detection as a transaction classification problem and proposes an LSTM Transaction Tree Classifier (LSTM-TC) solution, which includes feature extraction and classification of Bitcoin transactions based on deep learning. We also build a dataset to validate our solution. Experiments show that our approach has better performance and the potential for discovering new classes of coin mixing transactions than rule-based approaches and graph neural network-based Bitcoin transaction classification algorithms.},
  archive      = {J_APIN},
  author       = {Sun, Xiaowen and Yang, Tan and Hu, Bo},
  doi          = {10.1007/s10489-021-02453-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {780-793},
  shortjournal = {Appl. Intell.},
  title        = {LSTM-TC: Bitcoin coin mixing detection method with a high recall},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised deformable image registration network for 3D
medical images. <em>APIN</em>, <em>52</em>(1), 766–779. (<a
href="https://doi.org/10.1007/s10489-021-02196-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration aims to establish an active correspondence between a pair of images. Such correspondence is critical for many significant applications, such as image fusion, tumor growth monitoring, and atlas generation. In this study, we propose an unsupervised deformable image registration network (UDIR-Net) for 3D medical images. The proposed UDIR-Net is designed in an encoder-decoder architecture and directly estimates the complex deformation field between input pairwise images without any supervised information. In particular, we recalibrate the feature slice of each feature map that is propagated between the encoder and the decoder in accordance with the importance of each feature slice and the correlation between feature slices. This method enhances the representational power of feature maps. To achieve efficient and robust training, we design a novel hierarchical loss function that evaluates multiscale similarity loss between registered image pairs. The proposed UDIR-Net is tested on different public magnetic resonance image datasets of the human brain. Experimental results show that UDIR-Net exhibits competitive performance against several state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Ma, Yingjun and Niu, Dongmei and Zhang, Jinshuo and Zhao, Xiuyang and Yang, Bo and Zhang, Caiming},
  doi          = {10.1007/s10489-021-02196-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {766-779},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised deformable image registration network for 3D medical images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A scalable parallel preconditioned conjugate gradient method
for bundle adjustment. <em>APIN</em>, <em>52</em>(1), 753–765. (<a
href="https://doi.org/10.1007/s10489-021-02349-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle adjustment is a fundamental problem in computer vision, with important applications such as 3D structure reconstruction from 2D images. This paper focuses on large-scale bundle adjustment tasks, e.g., city-wide 3D reconstruction, which require highly efficient solutions. For this purpose, it is common to apply the Levenberg-Marquardt algorithm, whose bottleneck lies in solving normal equations. The majority of recent methods focus on achieving scalability through modern hardware such as GPUs and distributed systems. On the other hand, the core of the solution, i.e., the math underlying the optimizer for the normal equations, remains largely unimproved since the proposal of the classic parallel bundle adjustment (PBA) algorithm, which increasingly becomes a major limiting factor for the scalability of bundle adjustment. This paper proposes parallel preconditioned conjugate gradient (PPCG) method, a novel parallel method for bundle adjustment based on preconditioned conjugate gradient, which achieves significantly higher efficiency and scalability than existing methods on the algorithmic level. The main idea is to exploit the sparsity of the Hessian matrix and reduce its structure parameters through an effective parallel Schur complement method; the result of this step is then fed into our carefully designed PPCG method which reduces matrix operations that are either expensive (e.g., large matrix reverse or multiplications) or scales poorly to multi-processors (e.g., parallel Reduce operators). Extensive experiments demonstrate that PPCG outperforms existing optimizers by large margins, on a wide range of datasets.},
  archive      = {J_APIN},
  author       = {Peng, Jiaxin and Liu, Jie and Wei, Hua},
  doi          = {10.1007/s10489-021-02349-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {753-765},
  shortjournal = {Appl. Intell.},
  title        = {A scalable parallel preconditioned conjugate gradient method for bundle adjustment},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new froth image classification method based on the
MRMR-SSGMM hybrid model for recognition of reagent dosage condition in
the coal flotation process. <em>APIN</em>, <em>52</em>(1), 732–752. (<a
href="https://doi.org/10.1007/s10489-021-02328-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent separation is a core technology in the transformation, upgradation, and high-quality development of coal. Realising the intelligent recognition and accurate classification of coal flotation froth is a key technology of intelligent separation. At present, the coal flotation process relies on artificial recognition of froth features for adjusting the reagent dosage. However, owing to the low accuracy and subjectivity of artificial recognition, some problems arise, such as reagent wastage and unqualified product quality. Thus, this paper proposes a new froth image classification method based on the maximal-relevance-minimal-redundancy (MR MR)-semi-supervised Gaussian mixture model (SSGMM) hybrid model for recognition of reagent dosage condition in the coal flotation process. First, the features of morphology, colour, and texture are extracted, and the optimal froth image features are screened out using the maximal-relevance-minimal-redundancy (MRMR) feature selection algorithm based on class information. Second, the traditional GMM clusterer is improved, called SSGMM, by introducing a small number of marked samples, the traditional GMM’ problems of unclear training goals, invisible clustering results, and artificially judged clustering results are solved. Then a new hybrid classification model is proposed by combining the MRMR with the modified GMM (SSGMM) which can be named as (MRMR - SSGMM). The optimal froth image features are screened by MRMR to provide the SSGMM classifier. In the process of training and learning the feature samples, using the marked feature samples of froth images to guide the unmarked feature samples. The information of marked feature samples of froth images is mapped to the unmarked feature samples, the classification of the froth images were realised. Finally, the accuracy of the SSGMM classifier is used as the evaluation criterion for the screened features by MRMR. By automatically executing the entire learning process to find the best number of froth image features and the optimal image features, so that the classifier achieves the maximum classification accuracy. Experimental results show that the proposed classification method achieves the best results in accuracy and time, compared with other benchmark classification methods. Application results show that the method can provide reliable guidance for the adjustment of the reagent dosage, realize the accurate and timely control of the reagent dosage, reduce the consumption of the reagent and the incidence of production accidents, and stabilize the product quality in the coal flotation production process.},
  archive      = {J_APIN},
  author       = {Cao, Wenyan and Wang, Ranfeng and Fan, Minqiang and Fu, Xiang and Wang, Haoran and Wang, Yulong},
  doi          = {10.1007/s10489-021-02328-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {732-752},
  shortjournal = {Appl. Intell.},
  title        = {A new froth image classification method based on the MRMR-SSGMM hybrid model for recognition of reagent dosage condition in the coal flotation process},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernelized multi-view subspace clustering via auto-weighted
graph learning. <em>APIN</em>, <em>52</em>(1), 716–731. (<a
href="https://doi.org/10.1007/s10489-021-02365-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has been an important and powerful tool for partitioning multi-view data, especially multi-view high-dimensional data. Despite great success, most of the existing multi-view subspace clustering methods still suffer from three limitations. First, they often recover the subspace structure in the original space, which can not guarantee the robustness when handling multi-view data with nonlinear structure. Second, these methods mostly regard subspace clustering and affinity matrix learning as two independent steps, which may not well discover the latent relationships among data samples. Third, many of them ignore the different importance of multiple views, whose performance may be badly affected by the low-quality views in multi-view data. To overcome these three limitations, this paper develops a novel subspace clustering method for multi-view data, termed Kernelized Multi-view Subspace Clustering via Auto-weighted Graph Learning (KMSC-AGL). Specifically, the proposed method implicitly maps the multi-view data from linear space into nonlinear space via kernel-induced functions, so as to exploit the nonlinear structure hidden in data. Furthermore, our method aims to enhance the clustering performance by learning a set of view-specific representations and their affinity matrix in a general framework. By integrating the view weighting strategy into this framework, our method can automatically assign the weights to different views, while learning an optimal affinity matrix that is well-adapted to the subsequent spectral clustering. Extensive experiments are conducted on a variety of multi-view data sets, which have demonstrated the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Zhang, Guang-Yu and Chen, Xiao-Wei and Zhou, Yu-Ren and Wang, Chang-Dong and Huang, Dong and He, Xiao-Yu},
  doi          = {10.1007/s10489-021-02365-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {716-731},
  shortjournal = {Appl. Intell.},
  title        = {Kernelized multi-view subspace clustering via auto-weighted graph learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mathematical modelling for decision making of lockdown
during COVID-19. <em>APIN</em>, <em>52</em>(1), 699–715. (<a
href="https://doi.org/10.1007/s10489-021-02463-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the recent worldwide outbreak of COVID-19, there has been an enormous change in our lifestyle and it has a severe impact in different fields like finance, education, business, travel, tourism, economy, etc., in all the affected countries. In this scenario, people must be careful and cautious about the symptoms and should act accordingly. Accurate predictions of different factors, like the end date of the pandemic, duration of lockdown and spreading trend can guide us through the pandemic and precautions can be taken accordingly. Multiple attempts have been made to model the virus transmission, but none of them has investigated it at a global level. The novelty of the proposed work lies here. In this paper, first, authors have analysed spreading of the said disease using data collected from various platforms and then, have presented a predictive mathematical model for fifteen countries from first, second and third world for probable future projections of this pandemic. The prediction can be used by planning commission, healthcare organizations and the government agencies as well for creating suitable arrangements against this pandemic.},
  archive      = {J_APIN},
  author       = {Ghosh, Ahona and Roy, Sandip and Mondal, Haraprasad and Biswas, Suparna and Bose, Rajesh},
  doi          = {10.1007/s10489-021-02463-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {699-715},
  shortjournal = {Appl. Intell.},
  title        = {Mathematical modelling for decision making of lockdown during COVID-19},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new QoC parameter and corresponding context inconsistency
elimination algorithms for sensed contexts and non-sensed contexts.
<em>APIN</em>, <em>52</em>(1), 681–698. (<a
href="https://doi.org/10.1007/s10489-021-02226-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the key products of ubiquitous computing, context-aware systems have been widely used in many fields such as digital home, smart healthcare and so on. However, in the face of the typical application environment formed by multiple sensors and intelligent devices, the inconsistency of contexts that hinders the normal operation of the systems has become an inevitable and urgent problem that needs to be resolved. In this paper, we propose a new quality of context (QoC) parameter relevance to enrich the comprehensive assessment of the context quality. Moreover, on this basis, we put forward novel context inconsistency elimination algorithms that use multiple QoC parameters and Dempster-Shafer theory to solve the inconsistency problem of sensed contexts and non-sensed contexts, respectively. Experimental analyses from multiple dimensions fully show that the proposed algorithms have obvious advantages over the other algorithms in terms of accuracy, stability, and robustness.},
  archive      = {J_APIN},
  author       = {Fan, Shidi and Xu, Hongji and Xiong, Hailiang and Chen, Min and Liu, Qiang and Xing, Qinghua and Li, Tiankuo},
  doi          = {10.1007/s10489-021-02226-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {681-698},
  shortjournal = {Appl. Intell.},
  title        = {A new QoC parameter and corresponding context inconsistency elimination algorithms for sensed contexts and non-sensed contexts},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Deep learning frameworks to learn prediction
and simulation focused control system models. <em>APIN</em>,
<em>52</em>(1), 680. (<a
href="https://doi.org/10.1007/s10489-021-02538-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Correction to this paper has been published: https://doi.org/10.1007/10489.1573-7497},
  archive      = {J_APIN},
  author       = {Tuna, Turcan and Beke, Aykut and Kumbasar, Tufan},
  doi          = {10.1007/s10489-021-02538-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {680},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Deep learning frameworks to learn prediction and simulation focused control system models},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Deep learning frameworks to learn prediction and simulation
focused control system models. <em>APIN</em>, <em>52</em>(1), 662–679.
(<a href="https://doi.org/10.1007/s10489-021-02416-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) methods have brought world-shattering breakthroughs, especially in computer vision and classification problems. Yet, the design and deployment of DL methods in time series prediction and nonlinear system identification applications still need more progress. In this paper, we present DL frameworks that are developed to provide novel approaches as solutions to the aforementioned engineering problems. The proposed DL frameworks leverage the advantages of autoencoders and long-short term memory network, which are known being data compression and recurrent structures, respectively, to design Deep Neural Networks (DNN) for modeling time series and nonlinear systems with high performance. We provide recommendations on how deep AEs and LSTMs should be utilized to end up with efficient Prediction-focused (Pf) and Simulation-focused (Sf) DNNs for time series and system identification problems. We present systematic learning methods for the DL frameworks that allow straightforward learning of Pf-DNN and Sf-DNN models in detail. To demonstrate the efficiency of the developed DNNs, we present various comparative results conducted on the benchmark and real-world datasets in comparison with their conventional, shallow, and deep neural network counterparts. The results clearly show that the deployment of the proposed DL frameworks results with DNNs that have high accuracy, even with a low dimensional feature vector.},
  archive      = {J_APIN},
  author       = {Tuna, Turcan and Beke, Aykut and Kumbasar, Tufan},
  doi          = {10.1007/s10489-021-02416-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {662-679},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning frameworks to learn prediction and simulation focused control system models},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient residual attention network for single image
super-resolution. <em>APIN</em>, <em>52</em>(1), 652–661. (<a
href="https://doi.org/10.1007/s10489-021-02489-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of deep convolutional neural networks (CNNs) for image super-resolution (SR) from low-resolution (LR) input has achieved remarkable reconstruction performance with the utilization of residual structures and visual attention mechanisms. However, existing single image super-resolution (SISR) methods with deeper network architectures can encounter representational bottlenecks in CNN-based networks and neglect model efficiency in model statistical inference. To solve these issues, in this paper, we design a channel hourglass residual structure (CHRS) and explore an efficient channel attention (ECA) mechanism to extract more representative features and ease the computational burden. Specifically, our CHRS, consisting of several nested residual modules, is developed to learn more discriminative representations with fewer model parameters, and the ECA is presented to efficiently capture local cross-channel interaction by subtly applying 1D convolution. Finally, we propose an efficient residual attention network (ERAN), which not only fully learns more representative features but also pays special attention to network learning efficiency. Extensive experiments demonstrate that our ERAN achieves certain improvements in model performance and implementation efficiency compared to other previous state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Hao, Fangwei and Zhang, Taiping and Zhao, Linchang and Tang, Yuanyan},
  doi          = {10.1007/s10489-021-02489-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {652-661},
  shortjournal = {Appl. Intell.},
  title        = {Efficient residual attention network for single image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing a BERT based triple classification model using
knowledge graph embedding for question answering system. <em>APIN</em>,
<em>52</em>(1), 636–651. (<a
href="https://doi.org/10.1007/s10489-021-02460-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current BERT-based question answering systems use a question and a contextual text to find the answer. This causes the systems to return wrong answers or nothing if the text contains irrelevant contents with the input question. Besides, the systems haven’t answered yes-no and aggregate questions yet. Besides that, the systems only concentrate on the contents of text regardless of the relationship between entities in the corpus. This systems cannot validate the answer. In this paper, we presented a solution to solve these issues by using the BERT model and the knowledge graph to enhance a question answering system. We combined content-based and linked-based information for knowledge graph representation learning and classified triples into one of three classes such as base class, derived class, or non-existent class. We then used the BERT model to build two classifiers: BERT-based text classification for content information and BERT-based triple classification for link information. The former was able to make a contextual embedding vector for representing triples that were used to classify into the three above classes. The latter generated all path instances from all meta paths of a large heterogeneous information network by running the Motif Search method of Apache Spark on a distributed environment. After creating the path instances, we produced triples from these path instances. We made content-based information by converting triples into natural language text with labels and considered them as a text classification problem. Our proposed solution outperformed other embedding methods with an average accuracy of 92.34% on benchmark datasets and the Motif Finding algorithm with an average executive time improvement of 37% on the distributed environment.},
  archive      = {J_APIN},
  author       = {Do, Phuc and Phan, Truong H. V.},
  doi          = {10.1007/s10489-021-02460-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {636-651},
  shortjournal = {Appl. Intell.},
  title        = {Developing a BERT based triple classification model using knowledge graph embedding for question answering system},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative least squares regression for multiclass
classification based on within-class scatter minimization.
<em>APIN</em>, <em>52</em>(1), 622–635. (<a
href="https://doi.org/10.1007/s10489-021-02258-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least square regression has been widely used in pattern classification, due to the compact form and efficient solution. However, two main issues limit its performance for solving the multiclass classification problems. The first one is that employing the hard discrete labels as the regression targets is inappropriate for multiclass classification. The second one is that it focus only on exactly fitting the instances to the target matrix while ignoring the within-class similarity of the instances, resulting in overfitting. To address this issues, we propose a discriminative least squares regression for multiclass classification based on within-class scatter minimization (WCSDLSR). Specifically, a ε-dragging technique is first introduced to relax the hard discrete labels into the slack soft labels, which enlarges the between-class margin for the soft labels as much as possible. The within-class scatter for the soft labels is then constructed as a regularization term to make the transformed instances of the same class closer to each other. These factors ensure WCSDLSR can learn a more compact and discriminative transformation for classification, thus avoiding the overfitting problems. Furthermore, the proposed WCSDLSR can obtain a closed-form solution in each iteration with the lower computational costs. Experimental results on the benchmark datasets demonstrate that the proposed WCSDLSR achieves the better classification performance with the lower computational costs.},
  archive      = {J_APIN},
  author       = {Ma, Jiajun and Zhou, Shuisheng},
  doi          = {10.1007/s10489-021-02258-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {622-635},
  shortjournal = {Appl. Intell.},
  title        = {Discriminative least squares regression for multiclass classification based on within-class scatter minimization},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust zero-watermarking algorithm for lossless copyright
protection of medical images. <em>APIN</em>, <em>52</em>(1), 607–621.
(<a href="https://doi.org/10.1007/s10489-021-02476-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous orthogonal moments of images have become a main tool used in zero-watermarking algorithms in recent years. However, there is a problem that should not be neglected for existing zero-watermarking algorithms, which is known as numerical instability. Because the continuous orthogonal moments used in existing zero-watermarking algorithms are integer-order moments, this problem affects the computational accuracy of such moments and thus affects the performance of zero-watermarking algorithms. Therefore, in this paper, the integer-order radial harmonic Fourier moments (IoRHFMs) are firstly extended to fractional-order radial harmonic Fourier moments (FoRHFMs) to effectively mitigate the problem of numerical instability and improve the computational accuracy of IoRHFMs, and then a FoRHFM-based zero-watermarking algorithm for medical images is proposed to achieve lossless copyright protection of medical images. The experimental results show that this algorithm is highly robust to geometric attacks and common attacks, and is superior to IoRHFM-based and other zero-watermarking algorithms.},
  archive      = {J_APIN},
  author       = {Xia, Zhiqiu and Wang, Xingyuan and Wang, Chunpeng and Wang, Changxu and Ma, Bin and Li, Qi and Wang, Mingxu and Zhao, Tingting},
  doi          = {10.1007/s10489-021-02476-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {607-621},
  shortjournal = {Appl. Intell.},
  title        = {A robust zero-watermarking algorithm for lossless copyright protection of medical images},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-19 cases prediction in multiple areas via shapelet
learning. <em>APIN</em>, <em>52</em>(1), 595–606. (<a
href="https://doi.org/10.1007/s10489-021-02391-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the number of COVID-19 cases in a geographical area is important for the management of health resources and decision making. Several methods have been proposed for COVID-19 case predictions but they have important limitations in terms of model interpretability, related to COVID-19’s incubation period and major trends of disease transmission. To be able to explain prediction results in terms of incubation period and transmission trends, this paper presents the Multivariate Shapelet Learning (MSL) model to learn shapelets from historical observations in multiple areas. An experimental evaluation was done to compare the prediction performance of eleven algorithms, using the data collected from 50 US provinces/states. Results show that the proposed method is effective and efficient. The learned shapelets explain increasing and decreasing trends of new confirmed cases, and reveal that the COVID-19 incubation period in the USA is around 28 days.},
  archive      = {J_APIN},
  author       = {Wang, Zhijin and Cai, Bing},
  doi          = {10.1007/s10489-021-02391-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {595-606},
  shortjournal = {Appl. Intell.},
  title        = {COVID-19 cases prediction in multiple areas via shapelet learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint pyramid attention network for real-time semantic
segmentation of urban scenes. <em>APIN</em>, <em>52</em>(1), 580–594.
(<a href="https://doi.org/10.1007/s10489-021-02446-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is an advanced research topic in computer vision and can be regarded as a fundamental technique for image understanding and analysis. However, most of the current semantic segmentation networks only focus on segmentation accuracy while ignoring the requirements for high processing speed and low computational complexity in mobile terminal fields such as autonomous driving systems, drone applications, and fingerprint recognition systems. Aiming at the problems that the current semantic segmentation task are facing, it is difficult to meet the actual industrial needs due to its high computational cost. We propose a joint pyramid attention network (JPANet) for real-time semantic segmentation. First, we propose a joint feature pyramid (JFP) module, which can combine multiple network stages with learning multi-scale feature representations with strong semantic information, hence improving pixel classification performance. Second, we built a spatial detail extraction (SDE) module to capture the shallow network multi-level local features and make up for the geometric information lost in the down-sampling stage. Finally, we design a bilateral feature fusion (BFF) module, which properly integrates spatial information and semantic information through a hybrid attention mechanism in spatial dimensions and channel dimensions, making full use of the correspondence between high-level features and low-level features. We conducted a series of experiments on two challenging urban road scene datasets (Cityscapes and CamVid) and achieved excellent results. Among them, the experimental results on the Cityscapes dataset show that for 512 × 1024 high-resolution images, our method achieves 71.62% Mean Intersection over Union (mIoU) with 109.9 frames per second (FPS) on a single 1080Ti GPU.},
  archive      = {J_APIN},
  author       = {Hu, Xuegang and Jing, Liyuan and Sehar, Uroosa},
  doi          = {10.1007/s10489-021-02446-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {580-594},
  shortjournal = {Appl. Intell.},
  title        = {Joint pyramid attention network for real-time semantic segmentation of urban scenes},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and efficient asymmetric network design for
real-time semantic segmentation. <em>APIN</em>, <em>52</em>(1), 564–579.
(<a href="https://doi.org/10.1007/s10489-021-02437-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for application scenarios such as autonomous driving and drone aerial photography, it has become a challenging problem that how to achieve the best trade-off between segmentation accuracy and inference speed while reducing the number of parameters. In this paper, a lightweight and efficient asymmetric network (LEANet) for real-time semantic segmentation is proposed to address this problem. Specifically, LEANet adopts an asymmetric encoder-decoder architecture. In the encoder, a depth-wise asymmetric bottleneck module with separation and shuffling operations (SS-DAB module) is proposed to jointly extract local and context information. In the decoder, a pyramid pooling module based on channel-wise attention (CA-PP module) is proposed to aggregate multi-scale context information and guide feature selection. Without any pre-training and post-processing, LEANet respectively achieves the accuracy of 71.9% and 67.5% mean Intersection over Union (mIoU) with the speed of 77.3 and 98.6 Frames Per Second (FPS) on the Cityscapes and CamVid test sets. These experimental results show that LEANet achieves an optimal trade-off between segmentation accuracy and inference speed with only 0.74 million parameters.},
  archive      = {J_APIN},
  author       = {Zhang, Xiu-Ling and Du, Bing-Ce and Luo, Zhao-Ci and Ma, Kai},
  doi          = {10.1007/s10489-021-02437-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {564-579},
  shortjournal = {Appl. Intell.},
  title        = {Lightweight and efficient asymmetric network design for real-time semantic segmentation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual adaptive alignment and partitioning network for visible
and infrared cross-modality person re-identification. <em>APIN</em>,
<em>52</em>(1), 547–563. (<a
href="https://doi.org/10.1007/s10489-021-02390-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible and infrared person re-identification (VI-ReID) describes the task of matching the images of a person, captured by visible-light and infrared cameras; this is a particular challenge in night time surveillance applications. Existing cross-modality recognition studies have been conducted mainly with a focus on learning the global and shareable feature representation of pedestrians to handle cross-modality discrepancies. However, the global features of pedestrian images cannot solve the unaligned image pairs efficiently, particularly when encountering the human appearance or posture misalignment caused by inaccurate pedestrian detection boxes. To mitigate the impact of these problems, we propose an end-to-end dual alignment and partitioning network to simultaneously learn global and local modal invariant features of pedestrians. First, we use two adaptive spatial transform modules to align the visible and infrared input images. Subsequently, the aligned image is divided horizontally, and the features of each local block are extracted. Then, we fuse these local features with global features. To alleviate the differences between heterogeneous modals and learn the common feature representation of heterogeneous modals, we map the features of heterogeneous modes into the same feature embedding space. Finally, we use the combination of identity loss and weighted regularized TriHard loss to improve the recognition accuracy. Extensive experimental results on two cross-modality datasets, RegDB and SYSU-MM01, demonstrate the superiority of the proposed method over other existing state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liu, Qiang and Teng, Qizhi and Chen, Honggang and Li, Bo and Qing, Linbo},
  doi          = {10.1007/s10489-021-02390-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {547-563},
  shortjournal = {Appl. Intell.},
  title        = {Dual adaptive alignment and partitioning network for visible and infrared cross-modality person re-identification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupled low rank representation and subspace clustering.
<em>APIN</em>, <em>52</em>(1), 530–546. (<a
href="https://doi.org/10.1007/s10489-021-02409-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is a technique utilized to find clusters within multiple subspaces. However, most existing methods cannot obtain an accurate block diagonal clustering structure to improve clustering performance. This drawback exists because these methods learn the similarity matrix in advance by utilizing a low dimensional matrix obtained directly from the data, where two unrelated data samples can stay related easily due to the influence of noise. This paper proposes a novel method based on coupled low-rank representation to tackle the above problem. First, our method constructs a manifold recovery structure to correct inadequacy in the low-rank representation of data. Then it obtains a clustering projection matrix that obeys the k-block diagonal property to learn an ideal similarity matrix. This similarity matrix denotes our clustering structure with a rank constraint on its normalized Laplacian matrix. Therefore, we avoid k-means spectral post-processing of the low dimensional embedding matrix, unlike most existing methods. Furthermore, we couple our method to allow the clustering structure to adaptively approximate the low-rank representation so as to find more optimal solutions. Several experiments on benchmark datasets demonstrate that our method outperforms similar state-of-the-art methods in Accuracy, Normalized Mutual Information, F-score, Recall, Precision, and Adjusted Rand Index evaluation metrics.},
  archive      = {J_APIN},
  author       = {Abhadiomhen, Stanley Ebhohimhen and Wang, ZhiYang and Shen, XiangJun},
  doi          = {10.1007/s10489-021-02409-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {530-546},
  shortjournal = {Appl. Intell.},
  title        = {Coupled low rank representation and subspace clustering},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene text detection by adaptive feature selection with text
scale-aware loss. <em>APIN</em>, <em>52</em>(1), 514–529. (<a
href="https://doi.org/10.1007/s10489-021-02331-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since convolutional neural networks(CNNs) were applied to scene text detection, the accuracy of text detection has been improved a lot. However, limited by the receptive fields of regular CNNs and due to the large scale variations of texts in images, current text detection methods may fail to detect some texts well when dealing with more challenging text instances, such as arbitrarily shaped texts and extremely small texts. In this paper, we propose a new segmentation based scene text detector, which is equipped with deformable convolution and global channel attention. In order to detect texts of arbitrary shapes, our method replaces traditional convolutions with deformable convolutions, the sampling locations of deformable convolutions are deformed with augmented offsets so that it can better adapt to any shapes of texts, especially curved texts. To get more representative features for texts, an Adaptive Feature Selection module is introduced to better exploit text content through global channel attention. Meanwhile, a scale-aware loss, which adjusts the weights of text instances with different sizes, is formulated to solve the text scale variation problem. Experiments on several standard benchmarks, including ICDAR2015, SCUT-CTW1500, ICDAR2017-MLT and MSRA-TD500 verify the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Wu, Qin and Luo, Wenli and Chai, Zhilei and Guo, Guodong},
  doi          = {10.1007/s10489-021-02331-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {514-529},
  shortjournal = {Appl. Intell.},
  title        = {Scene text detection by adaptive feature selection with text scale-aware loss},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-instance cancellable biometrics schemes based on
generative adversarial network. <em>APIN</em>, <em>52</em>(1), 501–513.
(<a href="https://doi.org/10.1007/s10489-021-02401-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main role of cancellable biometric schemes is to protect the privacy of the enrolled users. The protected biometric data are generated by applying a parametrized transformation function to the original biometric data. Although cancellable biometric schemes achieve high security levels, they may degrade the recognition accuracy. One of the mostwidely used approaches to enhance the recognition accuracy in biometric systems is to combine several instances of the same biometric modality. In this paper, two multi-instance cancellable biometric schemes based on iris traits are presented. The iris biometric trait is used in both schemes because of the reliability and stability of iris traits compared to the other biometric traits. A generative adversarial network (GAN) is used as a transformation function for the biometric features. The first scheme is based on a pre-transformation feature-level fusion, where the binary features of multiple instances are concatenated and inputted to the transformation phase. On the other hand, the second scheme is based on a post-transformation feature-level fusion, where each instance is separately inputted to the transformation phase. Experiments conducted on the CASIA Iris-V3-Internal database confirm the high recognition accuracy of the two proposed schemes. Moreover, the security of the proposed schemes is analyzed, and their robustness against two well-known types of attacks is proven.},
  archive      = {J_APIN},
  author       = {Tarek, Mayada and Hamouda, Eslam and Abohamama, A. S.},
  doi          = {10.1007/s10489-021-02401-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {501-513},
  shortjournal = {Appl. Intell.},
  title        = {Multi-instance cancellable biometrics schemes based on generative adversarial network},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Using known nonself samples to improve negative selection
algorithm. <em>APIN</em>, <em>52</em>(1), 482–500. (<a
href="https://doi.org/10.1007/s10489-021-02323-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative selection algorithm is the core algorithm of artificial immune system. It only uses the self for training and generates detectors to detect abnormalities. Holes are feature space areas that the detector fails to cover, it is the root cause of the performance degradation of the negative selection algorithm. The conventional method generates a large number of detectors randomly to repair the holes, which is time-consuming and not effective. To alleviate the problem, we propose a V-Detector-KN algorithm in this paper. V-Detector is the abbreviation of the real-valued negative selection algorithm with Variable-sized Detectors, KN represents Known Nonself. The V-Detector-KN algorithm uses the known nonself as the candidate detector to further generate the detector based on the V-Detector randomly generated detector, so as to realize the repair of holes. Compared with the conventional method to randomly generate detectors to repair holes, our proposed V-Detector-KN method uses known nonself to repair holes, reducing the randomness and blindness of hole repair. Theoretical analysis shows that the detection rate of our algorithm is not lower than that of the conventional V-Detector algorithm. The results of experiment comparing with other 6 algorithms on 7 UCI data sets show the superiority of our proposed algorithm.},
  archive      = {J_APIN},
  author       = {Li, Zhiyong and Li, Tao},
  doi          = {10.1007/s10489-021-02323-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {482-500},
  shortjournal = {Appl. Intell.},
  title        = {Using known nonself samples to improve negative selection algorithm},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight surrogate random forest support for model
simplification and feature relevance. <em>APIN</em>, <em>52</em>(1),
471–481. (<a href="https://doi.org/10.1007/s10489-021-02451-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a lightweight surrogate random forest (L-SRF) algorithm that can be interpreted through a new rule distillation method. The common surrogate models replace the existing heavy and deep but high-performance black box model using a teacher–student learning framework. However, the student model obtained in this way must maintain the performance of the teacher model, and thus the degree of model simplification and transparency is extremely limited. Therefore, to increase model transparency while maintaining the performance of the surrogate model, we propose two methods. First, we propose a cross-entropy Shapley value to evaluate the contribution of each rule in the student surrogate model. Second, a random mini-grouping method is devised to effectively distilless important rules while minimizing the overfitting problem caused by a model simplification. The proposed L-SRF based on a rule contribution has the advantage of improving the degree of simplification and transparency of the model by realizing the large distillation ratio against the initial SRF model. In addition, because the proposed L-SRF removes unnecessary rules, it is possible to minimize the loss of the importance and relevance of each feature. To demonstrate the superior performance of the proposed L-SRF method, several comparative experiments were conducted on various data sets. We proved experimentally that the proposed method achieves a more effective performance than black box AI models in terms of model transparency and memory requirement, as well as the interpretation of the feature relevance.},
  archive      = {J_APIN},
  author       = {Kim, Sangwon and Jeong, Mira and Ko, Byoung Chul},
  doi          = {10.1007/s10489-021-02451-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {471-481},
  shortjournal = {Appl. Intell.},
  title        = {Lightweight surrogate random forest support for model simplification and feature relevance},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear dynamical systems approach for human action
recognition with dual-stream deep features. <em>APIN</em>,
<em>52</em>(1), 452–470. (<a
href="https://doi.org/10.1007/s10489-021-02367-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition with a dual-stream architecture using linear dynamical systems (LDSs) approach is discussed in this paper. First, a slice process is established to extract original slices from video sequences. Two slicing methods are adopted to subtract or reserve the remaining frames in the video sequences. By applying background subtraction to adjacent frames of the original slices, difference slices are also expressed. To capture the spatial component of the background and difference expressed in each slice simultaneously, a framework based on pre-trained convolutional neural networks (CNNs) is introduced for dual-stream deep feature extraction. Subsequently, LDSs are established to model the timing relationship between adjacent slices and obtain the temporal component of the background and difference features, which are expressed as linear dynamical background feature (LD-BF) and linear dynamical difference feature (LD-DF). Practical experiments were conducted to demonstrate the effectiveness and robustness of the proposed approach using different datasets. Specifically, our experiments were conducted on the UCF50, UCF101, and hmdb51 datasets. The impact of retaining various principal component analysis (PCA) feature dimensions and distinct slicing methods in terms of detail recognition were evaluated. In particular, combining LD-BF with LD-DF under appropriate feature dimensions and slicing methods further improved the accuracy for the UCF50, UCF101, and hmdb51 datasets. In addition, the computational cost of the feature extraction process was evaluated to illustrate the efficiency of the proposed approach. The experimental results show that the proposed approach is competitive with state-of-the-art approaches in the three datasets.},
  archive      = {J_APIN},
  author       = {Du, Zhouning and Mukaidani, Hiroaki},
  doi          = {10.1007/s10489-021-02367-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {452-470},
  shortjournal = {Appl. Intell.},
  title        = {Linear dynamical systems approach for human action recognition with dual-stream deep features},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prioritized planning algorithm for multi-robot collision
avoidance based on artificial untraversable vertex. <em>APIN</em>,
<em>52</em>(1), 429–451. (<a
href="https://doi.org/10.1007/s10489-021-02397-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method to avoid collisions and deadlocks between mobile robots working collaboratively in a shared physical environment. Based on the shared knowledge of the robot’s direction and coordinates, we define five conflict types between robots and propose a new concept named Artificial Untraversable Vertex (AUV) to resolve the potential conflicts. Since conflict avoidance between robots is typically a real-time process, a heuristic search algorithm D* Lite with fast replanning characteristics is introduced. Once a robot finds that it may collide with another robot while moving along the preplanned path, a new conflict-free path can be calculated based on the AUV and D* Lite. The experimental results demonstrate that the proposed Multi-Robot Path Planning (MRPP) method can effectively avoid collisions and deadlocks between mobile robots.},
  archive      = {J_APIN},
  author       = {Li, Haodong and Zhao, Tao and Dian, Songyi},
  doi          = {10.1007/s10489-021-02397-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {429-451},
  shortjournal = {Appl. Intell.},
  title        = {Prioritized planning algorithm for multi-robot collision avoidance based on artificial untraversable vertex},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RD-hand: A real-time regression-based detector for dynamic
hand gesture. <em>APIN</em>, <em>52</em>(1), 417–428. (<a
href="https://doi.org/10.1007/s10489-021-02380-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing gesture-recognition methods have poor performance on the mobile terminal, because of their limited computing resources and noisy environments such as Spatio-temporal variations and light variations. In this paper, we first draw the idea of regression in object detection tasks into dynamic gesture trajectory recognition. We regard gesture trajectory recognition as an object detection task and propose a regression-based dynamic hand gesture detector named RD-Hand, which contains a special full connection layer for trajectory regression. RD-Hand only needs to perform classification and location once at the same time, and the target trajectory can be detected by regression. Firstly, we use a network combined with CNN and LSTM to extract the features of the trajectory. Then we use a full connection layer with special meaning to predict the classification and location. Finally, trajectory-based non-maximum suppression is used to eliminate redundant solutions. Experiments show that RD-Hand has high accuracy, good real-time performance without GPU.},
  archive      = {J_APIN},
  author       = {Jian, Chengfeng and Liu, Xingze and Zhang, Meiyu},
  doi          = {10.1007/s10489-021-02380-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {417-428},
  shortjournal = {Appl. Intell.},
  title        = {RD-hand: A real-time regression-based detector for dynamic hand gesture},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted multi-view co-clustering (WMVCC) for sparse data.
<em>APIN</em>, <em>52</em>(1), 398–416. (<a
href="https://doi.org/10.1007/s10489-021-02405-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has gained importance in recent times due to the large-scale generation of data, often from multiple sources. Multi-view clustering refers to clustering a set of objects which are expressed by multiple set of features, known as views, such as movies being expressed by the list of actors or by a textual summary of its plot. Co-clustering, on the other hand, refers to the simultaneous grouping of data samples and features under the assumption that samples exhibit a pattern only under a subset of features. This paper combines multi-view clustering with co-clustering and proposes a new Weighted Multi-View Co-Clustering (WMVCC) algorithm. The motivation behind the approach is to use the diversity of features provided by multiple sources of information while exploiting the power of co-clustering. The proposed method expands the clustering objective function to a unified co-clustering objective function across all the multiple views. The algorithm follows the k-means strategy and iteratively optimizes the clustering by updating cluster labels, features, and view weights. A local search is also employed to optimize the clustering result using weighted multi-step paths in a graph. Experiments are conducted on several benchmark datasets. The results show that the proposed approach converges quickly, and the clustering performance significantly outperforms other recent and state-of-the-art algorithms on sparse datasets.},
  archive      = {J_APIN},
  author       = {Hussain, Syed Fawad and Khan, Khadija and Jillani, Rashad},
  doi          = {10.1007/s10489-021-02405-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {398-416},
  shortjournal = {Appl. Intell.},
  title        = {Weighted multi-view co-clustering (WMVCC) for sparse data},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on a new optimization algorithm simulating multi-
states of matter inspired by finite element analysis approach.
<em>APIN</em>, <em>52</em>(1), 378–397. (<a
href="https://doi.org/10.1007/s10489-021-02190-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new optimization algorithm is proposed, since a huge problem that many algorithms faced was not being able to effectively balance the global and local search ability. Matter exists in three states: solid, liquid, and gas, which presents different motion characteristics. Inspired by multi- states of matter, individuals of optimization algorithm have different motion characteristics of matter, which could present different search ability. The Finite Element Analysis (FEA) approach can simulate multi- states of matter, which can be adopted to effectively balance the global search ability and local search ability in new optimization algorithm. The new algorithm is creative application of Finite Element Analysis at optimization algorithm field. Artificial Physics Optimization (APO) and Gravitational Search Algorithm (GSA) belongs to the algorithm types defined by force and mass. According to FEA approach, node displacement caused by force and stiffness could be equivalent to motion caused by force and mass of APO and GSA. In the new algorithm framework, stiffness replaces mass of APO and GSA algorithm. This paper performs research on two different algorithms based on APO and GSA respectively. The individuals of new optimization algorithm are divided into solid state, liquid state, and gas state. The effects of main parameters on the performance were studied through experiments of 6 static test functions. The performance is compared with PSO, basic APO, or GSA for four complex models which made up of solid individual, liquid individual, and gas individual in iterative process. The reasonable complex model can be confirmed experimentally. Based on the reasonable complex model, the article conducted complete experiments against Enhancing artificial bee colony algorithm with multi-elite guidance (MGABC), Artificial bee colony algorithm with an adaptive greedy position update strategy (AABC), Multi-strategy ensemble artificial bee colony (MEABC), Self-adaptive heterogeneous PSO (fk-PSO), and APO with 28 CEC2013 test problem. Experimental results show that the proposed method achieves a good performance in comparison to its counterparts as a consequence of its better exploration– exploitation balance. The algorithm supplies a new method to improve physics optimization algorithm.},
  archive      = {J_APIN},
  author       = {Ning, Zhiqiang and Gao, Youshan and Wang, Aihong},
  doi          = {10.1007/s10489-021-02190-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {378-397},
  shortjournal = {Appl. Intell.},
  title        = {Research on a new optimization algorithm simulating multi- states of matter inspired by finite element analysis approach},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning from synthetic labels for
histopathological images classification. <em>APIN</em>, <em>52</em>(1),
358–377. (<a href="https://doi.org/10.1007/s10489-021-02425-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new strategy that combines unsupervised learning (clustering) and transfer learning. Clustering methods are employed to generate synthetic labels for the source dataset (ICAR-2018). The generated dataset is then used for transfer learning to other histopathological datasets (KimiaPath960, CRC, Biomaging− 2015, Breakhis, and Lymphoma). The comparative study based on two clustering algorithms (K-means and multi-objective clustering stream) demonstrates the efficiency of MOC-Stream. The generated synthetic histopathological dataset by this clustering algorithm outperformed the original labeled dataset and the imageNet models in transfer learning.},
  archive      = {J_APIN},
  author       = {Dif, Nassima and Attaoui, Mohammed Oualid and Elberrichi, Zakaria and Lebbah, Mustapha and Azzag, Hanene},
  doi          = {10.1007/s10489-021-02425-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {358-377},
  shortjournal = {Appl. Intell.},
  title        = {Transfer learning from synthetic labels for histopathological images classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A multi-focus image fusion method based on attention
mechanism and supervised learning. <em>APIN</em>, <em>52</em>(1),
339–357. (<a href="https://doi.org/10.1007/s10489-021-02358-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion is always a difficult problem in digital image processing. To achieve efficient integration, we propose a new end-to-end network. This network uses the residual atrous spatial pyramid pooling module to extract multi-level features from the space of different scales and share parameters to ensure the consistency and correspondence of features. We also introduced a disparities attention module for the network which allows for information retention. These two parts can make our method overcome the difficulties of target edge artifacts, small range blur, poor detail capture, and so on. In addition, in order to improve the semantic ambiguity easily caused by unsupervised learning, we also proposed a new multi-focus image fusion dataset with groundtruth for supervised learning. We performed sufficient experiments, and the results show that the network can quickly capture the corresponding features of multi-focus images, and improve the fusion performance with less computation and lower storage cost. Compared with the existing nine fusion methods, our network is superior to other methods in subjective visual evaluation and objective evaluation, reaching a higher level.},
  archive      = {J_APIN},
  author       = {Jiang, Limai and Fan, Hui and Li, Jinjiang},
  doi          = {10.1007/s10489-021-02358-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {339-357},
  shortjournal = {Appl. Intell.},
  title        = {A multi-focus image fusion method based on attention mechanism and supervised learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft adaptive loss based laplacian eigenmaps. <em>APIN</em>,
<em>52</em>(1), 321–338. (<a
href="https://doi.org/10.1007/s10489-021-02300-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Laplacian eigenmaps (LE) is one of the most commonly used nonlinear dimensionality reduction methods and aims to find a low-dimensional representation to preserve the topological relationship between sample points in the original data. However, the ℓ2-norm based loss function makes LE unable to preserve the relationship in many cases. Additionally, the topological relationship does not represent the real intrinsic structure of data. For example, the overemphasis of the topological relationship by LE easily breaks the manifold structure into multiple local areas in the embedding space, which makes the spectral clustering analysis of multi-manifold data more difficult to carry out. To solve this problem, we propose the soft adaptive loss based LE (SALE). With the soft adaptive loss, SALE can adaptively emphasize the topological relationship between sample points and the clustering structure of data. The model is tested and validated on UCI, face and gene expression data sets, and compared with some state-of-the-art models. The experimental results show that the method is robust to noise.},
  archive      = {J_APIN},
  author       = {Chen, Baihua and Gao, Yunlong and Wu, Shunxiang and Pan, Jinyan and Liu, Jinghua and Fan, Yuling},
  doi          = {10.1007/s10489-021-02300-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {321-338},
  shortjournal = {Appl. Intell.},
  title        = {Soft adaptive loss based laplacian eigenmaps},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolved fuzzy min-max neural network for new-labeled data
classification. <em>APIN</em>, <em>52</em>(1), 305–320. (<a
href="https://doi.org/10.1007/s10489-021-02259-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern classification is a fundamental problem in many data-driven application domains. New-labeled data refers to the data with the labels that are new and different from source labels. How to learn the new-labeled data is a crucial research in the data classification. In this paper, an evolved fuzzy min-max neural network for new-labeled data classification (FMM-NLA) is proposed. In FMM-NLA, the network can be self-evolved. Unlike the traditional FMM methods, the trained network of FMM-NLA can be expanded when new-labeled data added. FMM-NLA is a continuing-learning method, which can realize the continuing training process without retraining all the data. In order to verify the superiority of the proposed method, benchmark data sets are used. The experimental results show that FMM-NLA is effective in handling new-labeled data. Moreover, the application result on the pipeline defect recognition in depth shows that FMM-NLA is effective in solving the new-labeled defect recognition problem.},
  archive      = {J_APIN},
  author       = {Ma, Yanjuan and Liu, Jinhai and Qu, Fuming and Zhu, Hongfei},
  doi          = {10.1007/s10489-021-02259-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {305-320},
  shortjournal = {Appl. Intell.},
  title        = {Evolved fuzzy min-max neural network for new-labeled data classification},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deformable and residual convolutional network for image
super-resolution. <em>APIN</em>, <em>52</em>(1), 295–304. (<a
href="https://doi.org/10.1007/s10489-021-02246-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research on image super-resolution (SR) has greatly progressed with the development of convolutional neural networks (CNNs). However, the fixed geometric structures of standard convolution filters largely limit the learning capacity of CNNs for image SR. To effectively address this problem, we propose a deformable and residual convolutional network (DefRCN) for image SR. Specifically, a deformable residual convolution block (DRCB) is developed to augment spatial sampling locations and enhance the transformation modelling capability of CNNs. In addition, we optimize the residual convolution block to reduce the model redundancy and alleviate the vanishing-gradient in backpropagation. In addition, the proposed upsample block allows the network to directly process low-resolution images, which reduces the computational resource cost. Extensive experiments on benchmark datasets verify that the proposed method achieves a high quantitative and qualitative performance.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Sun, Yemei and Liu, Shudong},
  doi          = {10.1007/s10489-021-02246-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {295-304},
  shortjournal = {Appl. Intell.},
  title        = {Deformable and residual convolutional network for image super-resolution},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ETBRec: A novel recommendation algorithm combining the
double influence of trust relationship and expert users. <em>APIN</em>,
<em>52</em>(1), 282–294. (<a
href="https://doi.org/10.1007/s10489-021-02419-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recommendation system has become the primary tool used by many Internet application platforms to solve the problem of information overload, and it faces issues such as data sparsity, cold start, and scalability. At present, most social recommendation algorithms only consider the influence of the trust relationship on the user’s feature vector, which indirectly affects the predicted rating, or consider directly trusting friends as neighbor users, which directly affects the predicted rating, but does not consider the direct influence and indirect combining influences to make rating predictions. Therefore, this paper proposes a collaborative filtering recommendation algorithm (ETBRec), which not only considers the trust difference between users but also proposes the definition of experts and considers the direct impact of expert users on prediction ratings and the trustees’ indirect impact of ratings. Among them, the trust difference is realized through trust metrics, including direct trust metrics and indirect trust metrics; the selection of expert users takes into account the user’s degree of trust and user rating attitude; experimental comparisons with various social recommendation algorithms and related recommendation algorithms on the Ciao and Douban datasets. The experimental results show that the ETBRec algorithm performs better on some evaluation indexes such as mean absolute error (MAE) and root mean squared error (RMSE).},
  archive      = {J_APIN},
  author       = {Duan, Zhenchun and Xu, Weihong and Chen, Yuantao and Ding, Lin},
  doi          = {10.1007/s10489-021-02419-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {282-294},
  shortjournal = {Appl. Intell.},
  title        = {ETBRec: A novel recommendation algorithm combining the double influence of trust relationship and expert users},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted frequent sequential pattern mining. <em>APIN</em>,
<em>52</em>(1), 254–281. (<a
href="https://doi.org/10.1007/s10489-021-02290-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trillions of bytes of data are generated every day in different forms, and extracting useful information from that massive amount of data is the study of data mining. Sequential pattern mining is a major branch of data mining that deals with mining frequent sequential patterns from sequence databases. Due to items having different importance in real-life scenarios, they cannot be treated uniformly. With today’s datasets, the use of weights in sequential pattern mining is much more feasible. In most cases, as in real-life datasets, pushing weights will give a better understanding of the dataset, as it will also measure the importance of an item inside a pattern rather than treating all the items equally. Many techniques have been introduced to mine weighted sequential patterns, but typically these algorithms generate a massive number of candidate patterns and take a long time to execute. This work aims to introduce a new pruning technique and a complete framework that takes much less time and generates a small number of candidate sequences without compromising with completeness. Performance evaluation on real-life datasets shows that our proposed approach can mine weighted patterns substantially faster than other existing approaches.},
  archive      = {J_APIN},
  author       = {Islam, Md Ashraful and Rafi, Mahfuzur Rahman and Azad, Al-amin and Ovi, Jesan Ahammed},
  doi          = {10.1007/s10489-021-02290-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {254-281},
  shortjournal = {Appl. Intell.},
  title        = {Weighted frequent sequential pattern mining},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep belief networks with self-adaptive sparsity.
<em>APIN</em>, <em>52</em>(1), 237–253. (<a
href="https://doi.org/10.1007/s10489-021-02361-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To have the sparsity of deep neural networks is crucial, which can improve the learning ability of them, especially for application to high-dimensional data with small sample size. Commonly used regularization terms for keeping the sparsity of deep neural networks are based on L1-norm or L2-norm; however, they are not the most reasonable substitutes of L0-norm. In this paper, based on the fact that the minimization of a log-sum function is one effective approximation to that of L0-norm, the sparse penalty term on the connection weights with the log-sum function is introduced. By embedding the corresponding iterative re-weighted-L1 minimization algorithm with k-step contrastive divergence, the connections of deep belief networks can be updated in a way of sparse self-adaption. Experiments on two kinds of biomedical datasets which are two typical small sample size datasets with a large number of variables, i.e., brain functional magnetic resonance imaging data and single nucleotide polymorphism data, show that the proposed deep belief networks with self-adaptive sparsity can learn the layer-wise sparse features effectively. And results demonstrate better performances including the identification accuracy and sparsity capability than several typical learning machines.},
  archive      = {J_APIN},
  author       = {Qiao, Chen and Yang, Lan and Shi, Yan and Fang, Hanfeng and Kang, Yanmei},
  doi          = {10.1007/s10489-021-02361-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {237-253},
  shortjournal = {Appl. Intell.},
  title        = {Deep belief networks with self-adaptive sparsity},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CFENet: Content-aware feature enhancement network for
multi-person pose estimation. <em>APIN</em>, <em>52</em>(1), 215–236.
(<a href="https://doi.org/10.1007/s10489-021-02383-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose estimation is a fundamental yet challenging task in computer vision. Although great success has been made in this field due to the rapid development of deep learning, complex situations (e.g., extreme poses, occlusions, overlapped persons, and crowded scenes) are still not well solved. To further mitigate these issues, we propose a novel Content-aware Feature Enhancement Network (CFENet), which consists of three effective modules: Feature Aggregation and Selection Module (FASM), Feature Fusion Module (FFM) and Dense Upsampling Convolution (DUC) module. The FASM includes Feature Aggregation Module (FAM) and Information Selection Module (ISM). The FAM constructs the hierarchical multi-scale feature aggregations in a granular level to capture more accurate fine-grained representations. The ISM makes the aggregated representations more distinguished, which adaptively highlights the discriminative human part representations both in the spatial location and channel context. Then, we perform FFM which effectively fuses high-resolution spatial features and low-resolution semantic features to obtain more reliable context information for well-estimated joints. Finally, we adopt DUC module to generate more precise prediction, which can recover missing joint details that are usually unavailable in common upsampling process. Comprehensive experiments demonstrate that the proposed approach outperforms most of the popular methods and achieves a competitive performance with the state-of-the-art methods over three benchmark datasets: the recent big dataset CrowdPose, the COCO keypoint detection dataset and the MPII Human Pose dataset. Our code will be released upon acceptance.},
  archive      = {J_APIN},
  author       = {Xu, Xixia and Zou, Qi and Lin, Xue},
  doi          = {10.1007/s10489-021-02383-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {215-236},
  shortjournal = {Appl. Intell.},
  title        = {CFENet: Content-aware feature enhancement network for multi-person pose estimation},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial electric field algorithm with inertia and
repulsion for spherical minimum spanning tree. <em>APIN</em>,
<em>52</em>(1), 195–214. (<a
href="https://doi.org/10.1007/s10489-021-02415-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial electric field algorithm (AEFA) is a potential global optimization algorithm proposed in recent years and has been successfully applied to various engineering optimizations. However, precocious convergence tends to occur when solving complex engineering optimization problems. To avoid premature convergence to some extent, an artificial electric field algorithm with inertia and repulsion (IRAEFA) is proposed. The IRAEFA algorithm introduces the inertia mechanism and the repulsion between charges, expands the search space, increases the diversity of population, balances the exploration and development ability of the algorithm, and avoids the algorithm falling into the local optimal solution. Finally, the IRAEFA algorithm is used to solve the spherical mining spanning tree (MST) problem, and the results obtained are compared and analyzed with the results of other well-known metaheuristics optimization algorithms. Experimental results show that the proposed algorithm has better performance than other algorithms in solving spherical MST problems.},
  archive      = {J_APIN},
  author       = {Bi, Jian and Zhou, Yongquan and Tang, Zhonghua and Luo, Qifang},
  doi          = {10.1007/s10489-021-02415-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {195-214},
  shortjournal = {Appl. Intell.},
  title        = {Artificial electric field algorithm with inertia and repulsion for spherical minimum spanning tree},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DWE-IL: A new incremental learning algorithm for
non-stationary time series prediction via dynamically weighting ensemble
learning. <em>APIN</em>, <em>52</em>(1), 174–194. (<a
href="https://doi.org/10.1007/s10489-021-02385-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an Incremental Learning Algorithm via Dynamically Weighting Ensemble Learning (DWE-IL) is proposed to solve the problem of Non-Stationary Time Series Prediction (NS-TSP). The basic principle of DWE-IL is to track real-time data changes by dynamically establishing and maintaining a knowledge base composed of multiple basic models. It trains the base model for each non-stationary time series subset, and finally combine each base model with dynamically weighting rules. The emphasis of the DWE-IL algorithm lies in the update of data weights and base model weights and the training of the base model. Finally, the experimental results of the DWE-IL algorithm on six non-stationary time series datasets are presented and compared with those of several other excellent algorithms. It can be concluded from the experimental results that the DWE-IL algorithm provides a good solution to the challenges of the NS-TSP tasks and has significantly superior performance over other comparative algorithms.},
  archive      = {J_APIN},
  author       = {Yu, Huihui and Dai, Qun},
  doi          = {10.1007/s10489-021-02385-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {174-194},
  shortjournal = {Appl. Intell.},
  title        = {DWE-IL: A new incremental learning algorithm for non-stationary time series prediction via dynamically weighting ensemble learning},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MapReduce based parallel fuzzy-rough attribute reduction
using discernibility matrix. <em>APIN</em>, <em>52</em>(1), 154–173. (<a
href="https://doi.org/10.1007/s10489-021-02253-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy-rough set theory is an efficient method for attribute reduction. It can effectively handle the imprecision and uncertainty of the data in the attribute reduction. Despite its efficacy, current approaches to fuzzy-rough attribute reduction are not efficient for the processing of large data sets due to the requirement of higher space complexities. A limited number of accelerators and parallel/distributed approaches have been proposed for fuzzy-rough attribute reduction in large data sets. However, all of these approaches are dependency measure based methods in which fuzzy similarity matrices are used for performing attribute reduction. Alternative discernibility matrix based attribute reduction methods are found to have less space requirements and more amicable to parallelization in building parallel/distributed algorithms. This paper therefore introduces a fuzzy discernibility matrix-based attribute reduction accelerator (DARA) to accelerate the attribute reduction. DARA is used to build a sequential approach and the corresponding parallel/distributed approach for attribute reduction in large data sets. The proposed approaches are compared to the existing state-of-the-art approaches with a systematic experimental analysis to assess computational efficiency. The experimental study, along with theoretical validation, shows that the proposed approaches are effective and perform better than the current approaches.},
  archive      = {J_APIN},
  author       = {Sowkuntla, Pandu and Prasad, P. S. V. S. Sai},
  doi          = {10.1007/s10489-021-02253-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {154-173},
  shortjournal = {Appl. Intell.},
  title        = {MapReduce based parallel fuzzy-rough attribute reduction using discernibility matrix},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parallel memetic algorithm with explicit management of
diversity for the job shop scheduling problem. <em>APIN</em>,
<em>52</em>(1), 141–153. (<a
href="https://doi.org/10.1007/s10489-021-02406-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job shop scheduling problem (JSSP) is a very popular NP-hard optimization problem that involves assigning jobs to resources. Recent advances in the field of memetic algorithms show that explicitly managing the diversity of the population by taking into account the stopping criterion with the aim of dynamically adapting the balance between exploration and exploitation is key to their success. This is especially the case in long-term executions. However, this design principle has not yet been applied to the JSSP. This paper proposes a novel memetic algorithm that integrates some of the most advanced components devised in the literature for the JSSP with a replacement strategy that explicitly manages the diversity by considering a novel dissimilarity measure. To properly address large instances, a parallel master-worker model is used. Experimental validation shows the important advances attained by our proposal when compared to two state-of-the-art optimizers. The advantages are clear in both sequential and parallel cases, with more impressive achievements appearing in the parallel case. The parallel proposal has yielded new best-known solutions in 30 well-known JSSP instances, matching the lower bound in two of them, meaning that at least two new optimal solutions have been discovered.},
  archive      = {J_APIN},
  author       = {Constantino, Oscar Hernández and Segura, Carlos},
  doi          = {10.1007/s10489-021-02406-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {141-153},
  shortjournal = {Appl. Intell.},
  title        = {A parallel memetic algorithm with explicit management of diversity for the job shop scheduling problem},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model and algorithm for identifying driver pathways based
on weighted non-binary mutation matrix. <em>APIN</em>, <em>52</em>(1),
127–140. (<a href="https://doi.org/10.1007/s10489-021-02330-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally acknowledged that driver pathway plays a decisive role in the occurrence and progress of tumors, and the identification of driver pathways has become imperative for precision medicine or personalized medicine. Due to the inevitable sequencing error, the noise contained in single omics cancer data usually plays a negative effect on identification. It is a feasible approach to take advantage of multi-omics cancer data rather than a single one now that large amounts of multi-omics cancer data have become available. The identification of driver pathways by integrating multi-omics cancer data has attracted attention of researchers in bioinformatics recently. In this paper, a weighted non-binary mutation matrix is constructed by integrating copy number variations, somatic mutations and gene expressions. Based on the weighted non-binary mutation matrix, a new identification model is proposed through defining new measurements of coverage and exclusivity. Then, a cooperative coevolutionary algorithm CGA-MWS is put forward for solving the presented model. Both real cancer data and simulated one were used to conduct comparisons among methods Dendrix, GA, iMCMC, MOGA, PGA-MWS and CGA-MWS. Compared with the pathways identified by the other five methods, more genes, belonging to the pathway identified by the CGA-MWS method, are enriched in a known signaling pathway in most cases. Simultaneously, the high efficiency of method CGA-MWS makes it practical in realistic applications. All of which have been verified through a number of experiments.},
  archive      = {J_APIN},
  author       = {Wu, Jingli and Zhu, Kai and Li, Gaoshi and Wang, Jinyan and Cai, Qirong},
  doi          = {10.1007/s10489-021-02330-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {127-140},
  shortjournal = {Appl. Intell.},
  title        = {A model and algorithm for identifying driver pathways based on weighted non-binary mutation matrix},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triplet attention multiple spacetime-semantic graph
convolutional network for skeleton-based action recognition.
<em>APIN</em>, <em>52</em>(1), 113–126. (<a
href="https://doi.org/10.1007/s10489-021-02370-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has recently attracted widespread attention in the field of computer vision. Previous studies on skeleton-based action recognition are susceptible to interferences from redundant video frames in judging complex actions but ignore the fact that the spatial-temporal features of different actions are extremely different. To solve these problems, we propose a triplet attention multiple spacetime-semantic graph convolutional network for skeleton-based action recognition (AM-GCN), which can not only capture the multiple spacetime-semantic feature from the video images to avoid limited information diversity from single-layer feature representation but can also improve the generalization ability of the network. We also present the triplet attention mechanism to apply an attention mechanism to different key points, key channels, and key frames of the actions, improving the accuracy and interpretability of the judgement of complex actions. In addition, different kinds of spacetime-semantic feature information are combined through the proposed fusion decision for comprehensive prediction in order to improve the robustness of the algorithm. We validate AM-GCN with two standard datasets, NTU-RGBD and Kinetics, and compare it with other mainstream models. The results show that the proposed model achieves tremendous improvement.},
  archive      = {J_APIN},
  author       = {Sun, Yanjing and Huang, Han and Yun, Xiao and Yang, Bin and Dong, Kaiwen},
  doi          = {10.1007/s10489-021-02370-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {113-126},
  shortjournal = {Appl. Intell.},
  title        = {Triplet attention multiple spacetime-semantic graph convolutional network for skeleton-based action recognition},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-shaped and v-shaped gaining-sharing knowledge-based
algorithm for feature selection. <em>APIN</em>, <em>52</em>(1), 81–112.
(<a href="https://doi.org/10.1007/s10489-021-02233-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, searching for the optimal feature subset from the original datasets is a very challenging and prominent task. The metaheuristic algorithms are used in finding out the relevant, important features, that enhance the classification accuracy and save the resource time. Most of the algorithms have shown excellent performance in solving feature selection problems. A recently developed metaheuristic algorithm, gaining-sharing knowledge-based optimization algorithm (GSK), is considered for finding out the optimal feature subset. GSK algorithm was proposed over continuous search space; therefore, a total of eight S-shaped and V-shaped transfer functions are employed to solve the problems into binary search space. Additionally, a population reduction scheme is also employed with the transfer functions to enhance the performance of proposed approaches. It explores the search space efficiently and deletes the worst solutions from the search space, due to the updation of population size in every iteration. The proposed approaches are tested over twenty-one benchmark datasets from UCI repository. The obtained results are compared with state-of-the-art metaheuristic algorithms including binary differential evolution algorithm, binary particle swarm optimization, binary bat algorithm, binary grey wolf optimizer, binary ant lion optimizer, binary dragonfly algorithm, binary salp swarm algorithm. Among eight transfer functions, V4 transfer function with population reduction on binary GSK algorithm outperforms other optimizers in terms of accuracy, fitness values and the minimal number of features. To investigate the results statistically, two non-parametric statistical tests are conducted that concludes the superiority of the proposed approach.},
  archive      = {J_APIN},
  author       = {Agrawal, Prachi and Ganesh, Talari and Oliva, Diego and Mohamed, Ali Wagdy},
  doi          = {10.1007/s10489-021-02233-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {81-112},
  shortjournal = {Appl. Intell.},
  title        = {S-shaped and V-shaped gaining-sharing knowledge-based algorithm for feature selection},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From SIR to SEAIRD: A novel data-driven modeling approach
based on the grey-box system theory to predict the dynamics of COVID-19.
<em>APIN</em>, <em>52</em>(1), 71–80. (<a
href="https://doi.org/10.1007/s10489-021-02379-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common compartmental modeling for COVID-19 is based on a priori knowledge and numerous assumptions. Additionally, they do not systematically incorporate asymptomatic cases. Our study aimed at providing a framework for data-driven approaches, by leveraging the strengths of the grey-box system theory or grey-box identification, known for its robustness in problem solving under partial, incomplete, or uncertain data. Empirical data on confirmed cases and deaths, extracted from an open source repository were used to develop the SEAIRD compartment model. Adjustments were made to fit current knowledge on the COVID-19 behavior. The model was implemented and solved using an Ordinary Differential Equation solver and an optimization tool. A cross-validation technique was applied, and the coefficient of determination R2 was computed in order to evaluate the goodness-of-fit of the model. Key epidemiological parameters were finally estimated and we provided the rationale for the construction of SEAIRD model. When applied to Brazil’s cases, SEAIRD produced an excellent agreement to the data, with an R2 ≥ 90%. The probability of COVID-19 transmission was generally high (≥ 95%). On the basis of a 20-day modeling data, the incidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed persons in Brazil and France. Within the same time frame, the fatality rate of COVID-19 was the highest in France (16.4%) followed by Brazil (6.9%), and the lowest in Russia (≤ 1%). SEAIRD represents an asset for modeling infectious diseases in their dynamical stable phase, especially for new viruses when pathophysiology knowledge is very limited.},
  archive      = {J_APIN},
  author       = {Pekpe, K. Midzodzi and Zitouni, Djamel and Gasso, Gilles and Dhifli, Wajdi and Guinhouya, Benjamin C.},
  doi          = {10.1007/s10489-021-02379-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {71-80},
  shortjournal = {Appl. Intell.},
  title        = {From SIR to SEAIRD: A novel data-driven modeling approach based on the grey-box system theory to predict the dynamics of COVID-19},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic kernel CNN-LR model for people counting.
<em>APIN</em>, <em>52</em>(1), 55–70. (<a
href="https://doi.org/10.1007/s10489-021-02375-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People Counting in images is a worthwhile task as it is widely used for public safety, emergency people planning, intelligent crowd flow, and countless other reasons. Counting the objects manually in images does not make practical sense, since it is very time-consuming, and it never gives accurate results for dense crowded images. In crowded images, as the density of the people increases, object appear to be partially encircling each other. This occlusion problem of objects limits the crowd counting ability of any traditional computer vision model. To overcome this problem, here we addressed a dynamic kernel convolution neural network-linear regression (DKCNN-LR) model for counting the exact number of people in image frames even if crowd is very dense and occlusion problem. The proposed model works in two phases, first a DKCNN model use convolution layers in such a fashion that the kernel weight of each subsequent successive layer is half of its previous convolution layer’s weight. The first three heavy kernel weight layers identify far camera regions (low-level) features, and the later light kernel weight layers help identify near-camera region (high-level) features. Second, a linear regression model is employed to perform parametric regression between the actual people count (ground truth) and the estimated count (predicted values). The performance of the proposed model tested on three challenging and different quality benchmark datasets in terms of MAE, RMSE, Pearson-R and R2. The DKCNN-LR model secured MAE, RMSE on Mall dataset is 1.65, 2.76, on Beijing-BRT 1.43, 1.87 and on SmartCity dataset it is 2.69 and 10.69. These results confirm that the proposed model is quite reliable, effective and robust for real situations.},
  archive      = {J_APIN},
  author       = {Tomar, Ankit and Kumar, Santosh and Pant, Bhaskar and Tiwari, Umesh Kumar},
  doi          = {10.1007/s10489-021-02375-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {55-70},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic kernel CNN-LR model for people counting},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intra-class low-rank regularization for supervised and
semi-supervised cross-modal retrieval. <em>APIN</em>, <em>52</em>(1),
33–54. (<a href="https://doi.org/10.1007/s10489-021-02308-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to retrieve related items across different modalities, for example, using an image query to retrieve related text. The existing deep methods ignore both the intra-modal and inter-modal intra-class low-rank structures when fusing various modalities, which decreases the retrieval performance. In this paper, two deep models (denoted as ILCMR and Semi-ILCMR) based on intra-class low-rank regularization are proposed for supervised and semi-supervised cross-modal retrieval, respectively. Specifically, ILCMR integrates the image network and text network into a unified framework to learn a common feature space by imposing three regularization terms to fuse the cross-modal data. First, to align them in the label space, we utilize semantic consistency regularization to convert the data representations to probability distributions over the classes. Second, we introduce an intra-modal low-rank regularization, which encourages the intra-class samples that originate from the same space to be more relevant in the common feature space. Third, an inter-modal low-rank regularization is applied to reduce the cross-modal discrepancy. To enable the low-rank regularization to be optimized using automatic gradients during network back-propagation, we propose the rank-r approximation and specify the explicit gradients for theoretical completeness. In addition to the three regularization terms that rely on label information incorporated by ILCMR, we propose Semi-ILCMR in the semi-supervised regime, which introduces a low-rank constraint before projecting the general representations into the common feature space. Extensive experiments on four public cross-modal datasets demonstrate the superiority of ILCMR and Semi-ILCMR over other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Kang, Peipei and Lin, Zehang and Yang, Zhenguo and Fang, Xiaozhao and Bronstein, Alexander M. and Li, Qing and Liu, Wenyin},
  doi          = {10.1007/s10489-021-02308-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {33-54},
  shortjournal = {Appl. Intell.},
  title        = {Intra-class low-rank regularization for supervised and semi-supervised cross-modal retrieval},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural TV program recommendation with label and user dual
attention. <em>APIN</em>, <em>52</em>(1), 19–32. (<a
href="https://doi.org/10.1007/s10489-021-02241-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TV program recommendation is very important for users to find interesting TV programs and avoid confusing users with a lot of information. Currently, they are basically traditional collaborative filtering algorithms, which only recommend through the interactive data between users and programs ignoring the important value of some auxiliary information. In addition, the neural network method based on attention mechanism can well capture the relationship between program labels to obtain accurate program and user representations. In this paper, we propose a neural TV program recommendation with label and user dual attention (NPR-LUA), which can focus on auxiliary information in program and user modules. In the program encoder module, we learn the auxiliary information from program labels through neural networks and word attention to identify important program labels. In the user encoder module, we learn the user representation through the programs that the user watches and use personalized attention mechanism to distinguish the importance of programs for each user. Experiments on real data sets show that our method can effectively improve the effectiveness of TV program recommendations than other existing models.},
  archive      = {J_APIN},
  author       = {Yin, Fulian and Li, Sitong and Ji, Meiqi and Wang, Yanyan},
  doi          = {10.1007/s10489-021-02241-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {19-32},
  shortjournal = {Appl. Intell.},
  title        = {Neural TV program recommendation with label and user dual attention},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel cause analysis approach of grey reasoning petri net
based on matrix operations. <em>APIN</em>, <em>52</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10489-021-02377-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cause analysis makes great contributions to identifying the priorities of the causes in fault diagnosis system. A fuzzy Petri net (FPN) is a preferable model for knowledge representation and reasoning and has become an effective fault diagnosis tool. However, the existing FPN has some limitations in cause analysis. It is criticized for the inability to fully consider incomplete and unknown knowledge in uncertain situations. In this paper, an enhanced grey reasoning Petri net (EGRPN) based on matrix operations is presented to address the limitations and improves the flexibility of the existing FPN. The proposed EGRPN model uses grey numbers to handle the greyness and inaccuracy of uncertain knowledge. Then, the EGRPN inference algorithm is executed based on the matrix operations, which can express the relevance of uncertain events in the form of grey numbers and improve the reliability of the knowledge reasoning process. Finally, industrial examples of cause diagnosis are used to illustrate the feasibility and reliability of the EGRPN model. The experimental results show that the new EGRPN model is promising for cause analysis.},
  archive      = {J_APIN},
  author       = {Li, Li and Xie, Yongfang and Cen, Lihui and Zeng, Zhaohui},
  doi          = {10.1007/s10489-021-02377-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A novel cause analysis approach of grey reasoning petri net based on matrix operations},
  volume       = {52},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
