<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---86">COAP - 86</h2>
<ul>
<li><details>
<summary>
(2022). A stabilized sequential quadratic semidefinite programming
method for degenerate nonlinear semidefinite programs. <em>COAP</em>,
<em>83</em>(3), 1027–1064. (<a
href="https://doi.org/10.1007/s10589-022-00402-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new sequential quadratic semidefinite programming (SQSDP) method for solving degenerate nonlinear semidefinite programs (NSDPs), in which we produce iteration points by solving a sequence of stabilized quadratic semidefinite programming (QSDP) subproblems, which we derive from the minimax problem associated with the NSDP. Unlike the existing SQSDP methods, the proposed one allows us to solve those QSDP subproblems inexactly, and each QSDP is feasible. One more remarkable point of the proposed method is that constraint qualifications or boundedness of Lagrange multiplier sequences are not required in the global convergence analysis. Specifically, without assuming such conditions, we prove the global convergence to a point satisfying any of the following: the stationary conditions for the feasibility problem, the approximate-Karush–Kuhn–Tucker (AKKT) conditions, and the trace-AKKT conditions. Finally, we conduct some numerical experiments to examine the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Yamakawa, Yuya and Okuno, Takayuki},
  doi          = {10.1007/s10589-022-00402-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {1027-1064},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stabilized sequential quadratic semidefinite programming method for degenerate nonlinear semidefinite programs},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computable centering methods for spiraling algorithms and
their duals, with motivations from the theory of lyapunov functions.
<em>COAP</em>, <em>83</em>(3), 999–1026. (<a
href="https://doi.org/10.1007/s10589-022-00413-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many problems, some of which are reviewed in the paper, popular algorithms like Douglas–Rachford (DR), ADMM, and FISTA produce approximating sequences that show signs of spiraling toward the solution. We present a meta-algorithm that exploits such dynamics to potentially enhance performance. The strategy of this meta-algorithm is to iteratively build and minimize surrogates for the Lyapunov function that captures those dynamics. As a first motivating application, we show that for prototypical feasibility problems the circumcentered-reflection method, subgradient projections, and Newton–Raphson are all describable as gradient-based methods for minimizing Lyapunov functions constructed for DR operators, with the former returning the minimizers of spherical surrogates for the Lyapunov function. As a second motivating application, we introduce a new method that shares these properties but with the added advantages that it: (1) does not rely on subproblems (e.g. reflections) and so may be applied for any operator whose iterates have the spiraling property; (2) provably has the aforementioned Lyapunov properties with few structural assumptions and so is generically suitable for primal/dual implementation; and (3) maps spaces of reduced dimension into themselves whenever the original operator does. This makes possible the first primal/dual implementation of a method that seeks the center of spiraling iterates. We describe this method, and provide a computed example (basis pursuit).},
  archive      = {J_COAP},
  author       = {Lindstrom, Scott B.},
  doi          = {10.1007/s10589-022-00413-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {999-1026},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Computable centering methods for spiraling algorithms and their duals, with motivations from the theory of lyapunov functions},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An extrapolated iteratively reweighted <span
class="math display"><em>ℓ</em><sub>1</sub></span> method with
complexity analysis. <em>COAP</em>, <em>83</em>(3), 967–997. (<a
href="https://doi.org/10.1007/s10589-022-00416-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The iteratively reweighted $$\ell _1$$ algorithm is a widely used method for solving various regularization problems, which generally minimize a differentiable loss function combined with a convex/nonconvex regularizer to induce sparsity in the solution. However, the convergence and the complexity of iteratively reweighted $$\ell _1$$ algorithms is generally difficult to analyze, especially for non-Lipschitz differentiable regularizers such as $$\ell _p$$ norm regularization with $$0&lt;p&lt;1$$ . In this paper, we propose, analyze and test a reweighted $$\ell _1$$ algorithm combined with the extrapolation technique under the assumption of Kurdyka-Łojasiewicz (KL) property on the proximal function of the perturbed objective. Our method does not require the Lipschitz differentiability on the regularizers nor the smoothing parameters in the weights bounded away from 0. We show the proposed algorithm converges uniquely to a stationary point of the regularization problem and has local linear convergence for KL exponent at most 1/2 and local sublinear convergence for KL exponent greater than 1/2. We also provide results on calculating the KL exponents and discuss the cases when the KL exponent is at most 1/2. Numerical experiments show the efficiency of our proposed method.},
  archive      = {J_COAP},
  author       = {Wang, Hao and Zeng, Hao and Wang, Jiashan},
  doi          = {10.1007/s10589-022-00416-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {967-997},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An extrapolated iteratively reweighted $$\ell _1$$ method with complexity analysis},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Completely positive factorization by a riemannian smoothing
method. <em>COAP</em>, <em>83</em>(3), 933–966. (<a
href="https://doi.org/10.1007/s10589-022-00417-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copositive optimization is a special case of convex conic programming, and it consists of optimizing a linear function over the cone of all completely positive matrices under linear constraints. Copositive optimization provides powerful relaxations of NP-hard quadratic problems or combinatorial problems, but there are still many open problems regarding copositive or completely positive matrices. In this paper, we focus on one such problem; finding a completely positive (CP) factorization for a given completely positive matrix. We treat it as a nonsmooth Riemannian optimization problem, i.e., a minimization problem of a nonsmooth function over a Riemannian manifold. To solve this problem, we present a general smoothing framework for solving nonsmooth Riemannian optimization problems and show convergence to a stationary point of the original problem. An advantage is that we can implement it quickly with minimal effort by directly using the existing standard smooth Riemannian solvers, such as Manopt. Numerical experiments show the efficiency of our method especially for large-scale CP factorizations.},
  archive      = {J_COAP},
  author       = {Lai, Zhijian and Yoshise, Akiko},
  doi          = {10.1007/s10589-022-00417-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {933-966},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Completely positive factorization by a riemannian smoothing method},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New bregman proximal type algorithms for solving DC
optimization problems﻿. <em>COAP</em>, <em>83</em>(3), 893–931. (<a
href="https://doi.org/10.1007/s10589-022-00411-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Difference of Convex (DC) optimization problems have objective functions that are differences between two convex functions. Representative ways of solving these problems are the proximal DC algorithms, which require that the convex part of the objective function have L-smoothness. In this article, we propose the Bregman Proximal DC Algorithm (BPDCA) for solving large-scale DC optimization problems that do not possess L-smoothness. Instead, it requires that the convex part of the objective function has the L-smooth adaptable property that is exploited in Bregman proximal gradient algorithms. In addition, we propose an accelerated version, the Bregman Proximal DC Algorithm with extrapolation (BPDCAe), with a new restart scheme. We show the global convergence of the iterates generated by BPDCA(e) to a limiting critical point under the assumption of the Kurdyka-Łojasiewicz property or subanalyticity of the objective function and other weaker conditions than those of the existing methods. We applied our algorithms to phase retrieval, which can be described both as a nonconvex optimization problem and as a DC optimization problem. Numerical experiments showed that BPDCAe outperformed existing Bregman proximal-type algorithms because the DC formulation allows for larger admissible step sizes.},
  archive      = {J_COAP},
  author       = {Takahashi, Shota and Fukuda, Mituhiro and Tanaka, Mirai},
  doi          = {10.1007/s10589-022-00411-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {893-931},
  shortjournal = {Comput. Optim. Appl.},
  title        = {New bregman proximal type algorithms for solving DC optimization problems﻿},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “FISTA” in banach spaces with adaptive discretisations.
<em>COAP</em>, <em>83</em>(3), 845–892. (<a
href="https://doi.org/10.1007/s10589-022-00418-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FISTA is a popular convex optimisation algorithm which is known to converge at an optimal rate whenever a minimiser is contained in a suitable Hilbert space. We propose a modified algorithm where each iteration is performed in a subset which is allowed to change at every iteration. Sufficient conditions are provided for guaranteed convergence, although at a reduced rate depending on the conditioning of the specific problem. These conditions have a natural interpretation when a minimiser exists in an underlying Banach space. Typical examples are L1-penalised reconstructions where we provide detailed theoretical and numerical analysis.},
  archive      = {J_COAP},
  author       = {Chambolle, Antonin and Tovey, Robert},
  doi          = {10.1007/s10589-022-00418-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {845-892},
  shortjournal = {Comput. Optim. Appl.},
  title        = {“FISTA” in banach spaces with adaptive discretisations},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving constrained nonsmooth group sparse optimization via
group capped- <span class="math display"><em>ℓ</em><sub>1</sub></span>
relaxation and group smoothing proximal gradient algorithm.
<em>COAP</em>, <em>83</em>(3), 801–844. (<a
href="https://doi.org/10.1007/s10589-022-00419-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the constrained group sparse regularization optimization problem, where the loss function is convex but nonsmooth, and the penalty term is the group sparsity which is then proposed to be relaxed by the group Capped- $$\ell _1$$ for the convenience of computation. Firstly, we introduce three kinds of stationary points for the continuous relaxation problem and describe the relationship of the three kinds of stationary points. We give the optimality conditions for the group Capped- $$\ell _1$$ problem and the original group sparse regularization problem, and investigate the link between the relaxation problem and the original problem in terms of global and local optimal solutions. The results reveal the equivalence of the original problem and the relaxation problem in some sense. Secondly, we propose a group smoothing proximal gradient (GSPG) algorithm for the constrained group sparse optimization, and prove that the proposed GSPG algorithm globally converges to a lifted stationary point of the relaxation problem. Finally, we present some numerical results on recovery of the simulated group sparse signals and the real group sparse images to illustrate the efficiency of the continuous relaxation model and the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Zhang, Xian and Peng, Dingtao},
  doi          = {10.1007/s10589-022-00419-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {801-844},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Solving constrained nonsmooth group sparse optimization via group capped- $$\ell _1$$ relaxation and group smoothing proximal gradient algorithm},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling design and control problems involving neural
network surrogates. <em>COAP</em>, <em>83</em>(3), 759–800. (<a
href="https://doi.org/10.1007/s10589-022-00404-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonlinear optimization problems that involve surrogate models represented by neural networks. We demonstrate first how to directly embed neural network evaluation into optimization models, highlight a difficulty with this approach that can prevent convergence, and then characterize stationarity of such models. We then present two alternative formulations of these problems in the specific case of feedforward neural networks with ReLU activation: as a mixed-integer optimization problem and as a mathematical program with complementarity constraints. For the latter formulation we prove that stationarity at a point for this problem corresponds to stationarity of the embedded formulation. Each of these formulations may be solved with state-of-the-art optimization methods, and we show how to obtain good initial feasible solutions for these methods. We compare our formulations on three practical applications arising in the design and control of combustion engines, in the generation of adversarial attacks on classifier networks, and in the determination of optimal flows in an oil well network.},
  archive      = {J_COAP},
  author       = {Yang, Dominic and Balaprakash, Prasanna and Leyffer, Sven},
  doi          = {10.1007/s10589-022-00404-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {759-800},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Modeling design and control problems involving neural network surrogates},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General-purpose preconditioning for regularized interior
point methods. <em>COAP</em>, <em>83</em>(3), 727–757. (<a
href="https://doi.org/10.1007/s10589-022-00424-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present general-purpose preconditioners for regularized augmented systems, and their corresponding normal equations, arising from optimization problems. We discuss positive definite preconditioners, suitable for CG and MINRES. We consider “sparsifications&quot; which avoid situations in which eigenvalues of the preconditioned matrix may become complex. Special attention is given to systems arising from the application of regularized interior point methods to linear or nonlinear convex programming problems.},
  archive      = {J_COAP},
  author       = {Gondzio, Jacek and Pougkakiotis, Spyridon and Pearson, John W.},
  doi          = {10.1007/s10589-022-00424-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {727-757},
  shortjournal = {Comput. Optim. Appl.},
  title        = {General-purpose preconditioning for regularized interior point methods},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COAP 2021 best paper prize. <em>COAP</em>, <em>83</em>(3),
723–726. (<a href="https://doi.org/10.1007/s10589-022-00426-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Lechner, Theresa},
  doi          = {10.1007/s10589-022-00426-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {723-726},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2021 best paper prize},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimization over the <span
class="math display"><em>ℓ</em><sub>1</sub></span> -ball using an
active-set non-monotone projected gradient. <em>COAP</em>,
<em>83</em>(2), 693–721. (<a
href="https://doi.org/10.1007/s10589-022-00407-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $$\ell _1$$ -ball is a nicely structured feasible set that is widely used in many fields (e.g., machine learning, statistics and signal analysis) to enforce some sparsity in the model solutions. In this paper, we devise an active-set strategy for efficiently dealing with minimization problems over the $$\ell _1$$ -ball and embed it into a tailored algorithmic scheme that makes use of a non-monotone first-order approach to explore the given subspace at each iteration. We prove global convergence to stationary points. Finally, we report numerical experiments, on two different classes of instances, showing the effectiveness of the algorithm.},
  archive      = {J_COAP},
  author       = {Cristofari, Andrea and De Santis, Marianna and Lucidi, Stefano and Rinaldi, Francesco},
  doi          = {10.1007/s10589-022-00407-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {693-721},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Minimization over the $$\ell _1$$ -ball using an active-set non-monotone projected gradient},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some modified fast iterative shrinkage thresholding
algorithms with a new adaptive non-monotone stepsize strategy for
nonsmooth and convex minimization problems. <em>COAP</em>,
<em>83</em>(2), 651–691. (<a
href="https://doi.org/10.1007/s10589-022-00396-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “ fast iterative shrinkage-thresholding algorithm &quot; (FISTA) is one of the most famous first order optimization schemes, and the stepsize, which plays an important role in theoretical analysis and numerical experiment, is always determined by a constant relating to the Lipschitz constant or by a backtracking strategy. In this paper, we design a new adaptive non-monotone stepsize strategy (NMS), which allows the stepsize to increase monotonically after finite iterations. It is remarkable that NMS can be successfully implemented without knowing the Lipschitz constant or without backtracking. And the additional cost of NMS is less than the cost of some existing backtracking strategies. For using NMS to the original FISTA (FISTA_NMS) and the modified FISTA (MFISTA_NMS), we show that the convergence results stay the same. Moreover, under the error bound condition, we show that FISTA_NMS achieves the rate of convergence to $$o\left( {\frac{1}{{{k^6}}}} \right) $$ and MFISTA_NMS enjoys the convergence rate related to the value of parameter of $$t_k$$ , that is $$o\left( {\frac{1}{{{k^{2\left( {a + 1} \right) }}}}} \right) ;$$ and the iterates generated by the above two algorithms are convergent. In addition, by taking advantage of the restart technique to accelerate the above two methods, we establish the linear convergences of the function values and iterates under the error bound condition. We conduct some numerical experiments to examine the effectiveness of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Liu, Hongwei and Wang, Ting and Liu, Zexian},
  doi          = {10.1007/s10589-022-00396-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {651-691},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Some modified fast iterative shrinkage thresholding algorithms with a new adaptive non-monotone stepsize strategy for nonsmooth and convex minimization problems},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized nesterov’s accelerated proximal gradient
algorithms with convergence rate of order o(1/k2). <em>COAP</em>,
<em>83</em>(2), 615–649. (<a
href="https://doi.org/10.1007/s10589-022-00401-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated gradient method initiated by Nesterov is now recognized to be one of the most powerful tools for solving smooth convex optimization problems. This method improves significantly the convergence rate of function values from O(1/k) of the standard gradient method down to $$O(1/k^2)$$ . In this paper, we present two generalized variants of Nesterov’s accelerated proximal gradient method for solving composition convex optimization problems in which the objective function is represented by the sum of a smooth convex function and a nonsmooth convex part. We show that with suitable ways to pick the sequences of parameters, the convergence rate for the function values of this proposed method is actually of order $$o(1/k^2)$$ . Especially, when the objective function is p-uniformly convex for $$p&gt;2$$ , the convergence rate is of order $$O\left( \ln k/k^{2p/(p-2)}\right)$$ , and the convergence is linear if the objective function is strongly convex. By-product, we derive a forward–backward algorithm generalizing the one by Attouch–Peypouquet (SIAM J Optim 26(3):1824–1834, 2016), which produces a convergence sequence with a convergence rate of the function values of order $$o(1/k^2)$$ . Initial computational experiments for solving linear inverse problems with the $$l_1$$ -regularization demonstrate the capabilities of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Van Ngai, Huynh and Son, Ta Anh},
  doi          = {10.1007/s10589-022-00401-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {615-649},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Generalized nesterov’s accelerated proximal gradient algorithms with convergence rate of order o(1/k2)},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized shortest path tour problem with time windows.
<em>COAP</em>, <em>83</em>(2), 593–614. (<a
href="https://doi.org/10.1007/s10589-022-00405-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a generalization of the shortest path tour problem with time windows (GSPTPTW). The aim is to find a single-origin single-destination shortest path, which has to pass through an ordered sequence of not necessarily disjoint node-subsets. Each node has a time window for each node-subset to which it belongs. We investigate the theoretical properties of GSPTPTW and propose a dynamic programming approach to solve it. Numerical results collected on a large set of new benchmark instances highlight the effectiveness of the proposed solution approach.},
  archive      = {J_COAP},
  author       = {Di Puglia Pugliese, L. and Ferone, D. and Festa, P. and Guerriero, F.},
  doi          = {10.1007/s10589-022-00405-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {593-614},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A generalized shortest path tour problem with time windows},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shortest path with acceleration constraints: Complexity and
approximation algorithms. <em>COAP</em>, <em>83</em>(2), 555–592. (<a
href="https://doi.org/10.1007/s10589-022-00403-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a variant of the Shortest Path Problem (SPP), in which we impose additional constraints on the acceleration over the arcs, and call it Bounded Acceleration SPP (BASP). This variant is inspired by an industrial application: a vehicle needs to travel from its current position to a target one in minimum-time, following pre-defined geometric paths connecting positions within a facility, while satisfying some speed and acceleration constraints depending on the vehicle position along the currently traveled path. We characterize the complexity of BASP, proving its NP-hardness. We also show that, under additional hypotheses on problem data, the problem admits a pseudo-polynomial time-complexity algorithm. Moreover, we present an approximation algorithm with polynomial time-complexity with respect to the data of the original problem and the inverse of the approximation factor $$\epsilon$$ . Finally, we present some computational experiments to evaluate the performance of the proposed approximation algorithm.},
  archive      = {J_COAP},
  author       = {Ardizzoni, S. and Consolini, L. and Laurini, M. and Locatelli, M.},
  doi          = {10.1007/s10589-022-00403-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {555-592},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Shortest path with acceleration constraints: Complexity and approximation algorithms},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimum cost b-matching problems with neighborhoods.
<em>COAP</em>, <em>83</em>(2), 525–553. (<a
href="https://doi.org/10.1007/s10589-022-00406-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we deal with minimum cost b-matching problems on graphs where the nodes are assumed to belong to non-necessarily convex regions called neighborhoods, and the costs are given by the distances between points of the neighborhoods. The goal in the proposed problems is twofold: (i) finding a b-matching in the graph and (ii) determining a point in each neighborhood to be the connection point among the edges defining the b-matching. Different variants of the minimum cost b-matching problem are considered depending on the criteria to match neighborhoods: perfect, maximum cardinality, maximal and the a–b-matching problems. The theoretical complexity of solving each one of these problems is analyzed. Different mixed integer non-linear programming formulations are proposed for each one of the considered problems and then reformulated as Second Order Cone formulations. An extensive computational experience shows the efficiency of the proposed formulations to solve the problems under study.},
  archive      = {J_COAP},
  author       = {Espejo, I. and Páez, R. and Puerto, J. and Rodríguez-Chía, A. M.},
  doi          = {10.1007/s10589-022-00406-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {525-553},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Minimum cost b-matching problems with neighborhoods},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic relaxed inertial forward-backward-forward
splitting for monotone inclusions in hilbert spaces. <em>COAP</em>,
<em>83</em>(2), 465–524. (<a
href="https://doi.org/10.1007/s10589-022-00399-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider monotone inclusions defined on a Hilbert space where the operator is given by the sum of a maximal monotone operator T and a single-valued monotone, Lipschitz continuous, and expectation-valued operator V. We draw motivation from the seminal work by Attouch and Cabot (Attouch in AMO 80:547–598, 2019, Attouch in MP 184: 243–287) on relaxed inertial methods for monotone inclusions and present a stochastic extension of the relaxed inertial forward–backward-forward method. Facilitated by an online variance reduction strategy via a mini-batch approach, we show that our method produces a sequence that weakly converges to the solution set. Moreover, it is possible to estimate the rate at which the discrete velocity of the stochastic process vanishes. Under strong monotonicity, we demonstrate strong convergence, and give a detailed assessment of the iteration and oracle complexity of the scheme. When the mini-batch is raised at a geometric (polynomial) rate, the rate statement can be strengthened to a linear (suitable polynomial) rate while the oracle complexity of computing an $$\epsilon $$ -solution improves to $${\mathcal {O}}(1/\epsilon )$$ . Importantly, the latter claim allows for possibly biased oracles, a key theoretical advancement allowing for far broader applicability. By defining a restricted gap function based on the Fitzpatrick function, we prove that the expected gap of an averaged sequence diminishes at a sublinear rate of $${\mathcal {O}}(1/k)$$ while the oracle complexity of computing a suitably defined $$\epsilon $$ -solution is $${\mathcal {O}}(1/\epsilon ^{1+a})$$ where $$a &gt; 1$$ . Numerical results on two-stage games and an overlapping group Lasso problem illustrate the advantages of our method compared to competitors.},
  archive      = {J_COAP},
  author       = {Cui, Shisheng and Shanbhag, Uday and Staudigl, Mathias and Vuong, Phan},
  doi          = {10.1007/s10589-022-00399-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {465-524},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic relaxed inertial forward-backward-forward splitting for monotone inclusions in hilbert spaces},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient scalarization in multiobjective optimal control of
a nonsmooth PDE. <em>COAP</em>, <em>83</em>(2), 435–464. (<a
href="https://doi.org/10.1007/s10589-022-00390-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work deals with the efficient numerical characterization of Pareto stationary fronts for multiobjective optimal control problems with a moderate number of cost functionals and a mildly nonsmooth, elliptic, semilinear PDE-constraint. When “ample” controls are considered, strong stationarity conditions that can be used to numerically characterize the Pareto stationary fronts are known for our problem. We show that for finite dimensional controls, a sufficient adjoint-based stationarity system remains obtainable. It turns out that these stationarity conditions remain useful when numerically characterizing the fronts, because they correspond to strong stationarity systems for problems obtained by application of weighted-sum and reference point techniques to the multiobjective problem. We compare the performance of both scalarization techniques using quantifiable measures for the approximation quality. The subproblems of either method are solved with a line-search globalized pseudo-semismooth Newton method that appears to remove the degenerate behavior of the local version of the method employed previously. We apply a matrix-free, iterative approach to deal with the memory and complexity requirements when solving the subproblems of the reference point method and compare several preconditioning approaches.},
  archive      = {J_COAP},
  author       = {Bernreuther, Marco and Müller, Georg and Volkwein, Stefan},
  doi          = {10.1007/s10589-022-00390-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {435-464},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient scalarization in multiobjective optimal control of a nonsmooth PDE},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Globally convergent newton-type methods for multiobjective
optimization. <em>COAP</em>, <em>83</em>(2), 403–434. (<a
href="https://doi.org/10.1007/s10589-022-00414-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two Newton-type methods for solving (possibly) nonconvex unconstrained multiobjective optimization problems. The first is directly inspired by the Newton method designed to solve convex problems, whereas the second uses second-order information of the objective functions with ingredients of the steepest descent method. One of the key points of our approaches is to impose some safeguard strategies on the search directions. These strategies are associated to the conditions that prevent, at each iteration, the search direction to be too close to orthogonality with the multiobjective steepest descent direction and require a proportionality between the lengths of such directions. In order to fulfill the demanded safeguard conditions on the search directions of Newton-type methods, we adopt the technique in which the Hessians are modified, if necessary, by adding multiples of the identity. For our first Newton-type method, it is also shown that, under convexity assumptions, the local superlinear rate of convergence (or quadratic, in the case where the Hessians of the objectives are Lipschitz continuous) to a local efficient point of the given problem is recovered. The global convergences of the aforementioned methods are based, first, on presenting and establishing the global convergence of a general algorithm and, then, showing that the new methods fall in this general algorithm. Numerical experiments illustrating the practical advantages of the proposed Newton-type schemes are presented.},
  archive      = {J_COAP},
  author       = {Gonçalves, M. L. N. and Lima, F. S. and Prudente, L. F.},
  doi          = {10.1007/s10589-022-00414-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {403-434},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Globally convergent newton-type methods for multiobjective optimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Levenberg–marquardt method based on probabilistic jacobian
models for nonlinear equations. <em>COAP</em>, <em>83</em>(2), 381–401.
(<a href="https://doi.org/10.1007/s10589-022-00393-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Levenberg–Marquardt method based on probabilistic models for nonlinear equations for which the Jacobian cannot be computed accurately or the computation is very expensive. We introduce the definition of the first-order accurate probabilistic Jacobian model, and show how to construct such a model with sample points generated by standard Gaussian distribution. Under certain conditions, we prove that the proposed method converges to a first order stationary point with probability one. Numerical results show the efficiency of the method.},
  archive      = {J_COAP},
  author       = {Zhao, Ruixue and Fan, Jinyan},
  doi          = {10.1007/s10589-022-00393-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {381-401},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Levenberg–Marquardt method based on probabilistic jacobian models for nonlinear equations},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abstract strongly convergent variants of the proximal point
algorithm. <em>COAP</em>, <em>83</em>(1), 349–380. (<a
href="https://doi.org/10.1007/s10589-022-00397-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove an abstract form of the strong convergence of the Halpern-type and Tikhonov-type proximal point algorithms in CAT(0) spaces. In addition, we derive uniform and computable rates of metastability (in the sense of Tao) for these iterations using proof mining techniques.},
  archive      = {J_COAP},
  author       = {Sipoş, Andrei},
  doi          = {10.1007/s10589-022-00397-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {349-380},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Abstract strongly convergent variants of the proximal point algorithm},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A product space reformulation with reduced dimension for
splitting algorithms. <em>COAP</em>, <em>83</em>(1), 319–348. (<a
href="https://doi.org/10.1007/s10589-022-00395-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a product space reformulation to transform monotone inclusions described by finitely many operators on a Hilbert space into equivalent two-operator problems. Our approach relies on Pierra’s classical reformulation with a different decomposition, which results in a reduction of the dimension of the outcoming product Hilbert space. We discuss the case of not necessarily convex feasibility and best approximation problems. By applying existing splitting methods to the proposed reformulation we obtain new parallel variants of them with a reduction in the number of variables. The convergence of the new algorithms is straightforwardly derived with no further assumptions. The computational advantage is illustrated through some numerical experiments.},
  archive      = {J_COAP},
  author       = {Campoy, Rubén},
  doi          = {10.1007/s10589-022-00395-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {319-348},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A product space reformulation with reduced dimension for splitting algorithms},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast inertial dynamic algorithm with smoothing method for
nonsmooth convex optimization. <em>COAP</em>, <em>83</em>(1), 287–317.
(<a href="https://doi.org/10.1007/s10589-022-00388-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the minimization of a nonsmooth convex function, we design an inertial second-order dynamic algorithm, which is obtained by approximating the nonsmooth function by a class of smooth functions. By studying the asymptotic behavior of the dynamic algorithm, we prove that each trajectory of it weakly converges to an optimal solution under some appropriate conditions on the smoothing parameters, and the convergence rate of the objective function values is $$o\left( t^{-2}\right)$$ . We also show that the algorithm is stable, that is, this dynamic algorithm with a perturbation term owns the same convergence properties when the perturbation term satisfies certain conditions. Finally, we verify the theoretical results by some numerical experiments.},
  archive      = {J_COAP},
  author       = {Qu, Xin and Bian, Wei},
  doi          = {10.1007/s10589-022-00388-6},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {287-317},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Fast inertial dynamic algorithm with smoothing method for nonsmooth convex optimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inertial alternating direction method of multipliers for
non-convex non-smooth optimization. <em>COAP</em>, <em>83</em>(1),
247–285. (<a href="https://doi.org/10.1007/s10589-022-00394-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an algorithmic framework, dubbed inertial alternating direction methods of multipliers (iADMM), for solving a class of nonconvex nonsmooth multiblock composite optimization problems with linear constraints. Our framework employs the general minimization-majorization (MM) principle to update each block of variables so as to not only unify the convergence analysis of previous ADMM that use specific surrogate functions in the MM step, but also lead to new efficient ADMM schemes. To the best of our knowledge, in the nonconvex nonsmooth setting, ADMM used in combination with the MM principle to update each block of variables, and ADMM combined with inertial terms for the primal variables have not been studied in the literature. Under standard assumptions, we prove the subsequential convergence and global convergence for the generated sequence of iterates. We illustrate the effectiveness of iADMM on a class of nonconvex low-rank representation problems.},
  archive      = {J_COAP},
  author       = {Hien, Le Thi Khanh and Phan, Duy Nhat and Gillis, Nicolas},
  doi          = {10.1007/s10589-022-00394-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {247-285},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inertial alternating direction method of multipliers for non-convex non-smooth optimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projected orthogonal vectors in two-dimensional search
interior point algorithms for linear programming. <em>COAP</em>,
<em>83</em>(1), 211–246. (<a
href="https://doi.org/10.1007/s10589-022-00385-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of linear programming interior point algorithms successively move from an interior solution to an improved interior solution by following a single search direction, which corresponds to solving a one-dimensional subspace linear program at each iteration. On the other hand, two-dimensional search interior point algorithms select two search directions, and determine a new and improved interior solution by solving a two-dimensional subspace linear program at each step. This paper presents primal and dual two-dimensional search interior point algorithms derived from affine and logarithmic barrier search directions. Both search directions are determined by randomly partitioning the objective function into two orthogonal vectors. Computational experiments performed on benchmark instances demonstrate that these new methods improve the average CPU time by approximately 12\% and the average number of iterations by 14\%.},
  archive      = {J_COAP},
  author       = {Vitor, Fabio and Easton, Todd},
  doi          = {10.1007/s10589-022-00385-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {211-246},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Projected orthogonal vectors in two-dimensional search interior point algorithms for linear programming},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sub-linear convergence of a stochastic proximal iteration
method in hilbert space. <em>COAP</em>, <em>83</em>(1), 181–210. (<a
href="https://doi.org/10.1007/s10589-022-00380-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic version of the proximal point algorithm for convex optimization problems posed on a Hilbert space. A typical application of this is supervised learning. While the method is not new, it has not been extensively analyzed in this form. Indeed, most related results are confined to the finite-dimensional setting, where error bounds could depend on the dimension of the space. On the other hand, the few existing results in the infinite-dimensional setting only prove very weak types of convergence, owing to weak assumptions on the problem. In particular, there are no results that show strong convergence with a rate. In this article, we bridge these two worlds by assuming more regularity of the optimization problem, which allows us to prove convergence with an (optimal) sub-linear rate also in an infinite-dimensional setting. In particular, we assume that the objective function is the expected value of a family of convex differentiable functions. While we require that the full objective function is strongly convex, we do not assume that its constituent parts are so. Further, we require that the gradient satisfies a weak local Lipschitz continuity property, where the Lipschitz constant may grow polynomially given certain guarantees on the variance and higher moments near the minimum. We illustrate these results by discretizing a concrete infinite-dimensional classification problem with varying degrees of accuracy.},
  archive      = {J_COAP},
  author       = {Eisenmann, Monika and Stillfjord, Tony and Williamson, Måns},
  doi          = {10.1007/s10589-022-00380-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {181-210},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Sub-linear convergence of a stochastic proximal iteration method in hilbert space},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stochastic primal-dual method for a class of nonconvex
constrained optimization. <em>COAP</em>, <em>83</em>(1), 143–180. (<a
href="https://doi.org/10.1007/s10589-022-00384-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a class of nonconvex optimization which involves uncertainty in the objective and a large number of nonconvex functional constraints. Challenges often arise when solving this type of problems due to the nonconvexity of the feasible set and the high cost of calculating function value and gradient of all constraints simultaneously. To handle these issues, we propose a stochastic primal-dual method in this paper. At each iteration, a proximal subproblem based on a stochastic approximation to an augmented Lagrangian function is solved to update the primal variable, which is then used to update dual variables. We explore theoretical properties of the proposed algorithm and establish its iteration and sample complexities to find an $$\epsilon$$ -stationary point of the original problem. Numerical tests on a weighted maximin dispersion problem and a nonconvex quadratically constrained optimization problem demonstrate the promising performance of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Jin, Lingzi and Wang, Xiao},
  doi          = {10.1007/s10589-022-00384-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {143-180},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stochastic primal-dual method for a class of nonconvex constrained optimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust min-max regret covering problems. <em>COAP</em>,
<em>83</em>(1), 111–141. (<a
href="https://doi.org/10.1007/s10589-022-00391-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with two min-max regret covering problems: the min-max regret Weighted Set Covering Problem (min-max regret WSCP) and the min-max regret Maximum Benefit Set Covering Problem (min-max regret MSCP). These problems are the robust optimization counterparts, respectively, of the Weighted Set Covering Problem and of the Maximum Benefit Set Covering Problem. In both problems, uncertainty in data is modeled by using an interval of continuous values, representing all the infinite values every uncertain parameter can assume. This study has the following major contributions: (i) a proof that MSCP is $$\varSigma _p^2$$ -Hard, (ii) a mathematical formulation for the min-max regret MSCP, (iii) exact and (iv) heuristic algorithms for the min-max regret WSCP and the min-max regret MSCP. We reproduce the main exact algorithms for the min-max regret WSCP found in the literature: a Logic-based Benders decomposition, an extended Benders decomposition and a branch-and-cut. In addition, such algorithms have been adapted for the min-max regret MSCP. Moreover, five heuristics are applied for both problems: two scenario-based heuristics, a path relinking, a pilot method and a linear programming-based heuristic. The goal is to analyze the impact of such methods on handling robust covering problems in terms of solution quality and performance.},
  archive      = {J_COAP},
  author       = {Coco, Amadeu A. and Santos, Andréa Cynthia and Noronha, Thiago F.},
  doi          = {10.1007/s10589-022-00391-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {111-141},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Robust min-max regret covering problems},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polyhedral analysis and a new algorithm for the length
constrained k–drones rural postman problem. <em>COAP</em>,
<em>83</em>(1), 67–109. (<a
href="https://doi.org/10.1007/s10589-022-00383-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Length Constrained K–Drones Rural Postman Problem (LC K–DRPP) is a continuous optimization problem where a set of curved or straight lines of a network have to be traversed, in order to be serviced, by a fleet of homogeneous drones, with total minimum cost. Since the range and endurance of drones is limited, we consider here that the length of each route is constrained to a given limit L. Drones are not restricted to travel on the network, and they can enter and exit a line through any of its points, servicing only a portion of that line. Therefore, shorter solutions are obtained with “aerial” drones than with “ground” vehicles that are restricted to the network. If a LC K–DRPP instance is digitized by approximating each line with a polygonal chain, and it is assumed that drones can only enter and exit a line through the points of the chain, an instance of the Length Constrained K–vehicles Rural Postman Problem (LC K–RPP) is obtained. This is a discrete arc routing problem, and therefore can be solved with combinatorial optimization techniques. However, when the number of points in each polygonal chain is very large, the LC K–RPP instance can be so large that it is very difficult to solve, even for heuristic algorithms. Therefore, it is necessary to implement a procedure that generates smaller LC K–RPP instances by approximating each line by a few but “significant” points and segments. In this paper, we present a new formulation for the LC K–RPP with two binary variables for each edge and each drone representing the first and second traversals of the edge, respectively. We make a polyhedral study of the set of solutions of a relaxed formulation and prove that several families of inequalities induce facets of the polyhedron. We design and implement a branch–and–cut algorithm for the LC K–RPP that incorporates the separation of these inequalities. This B &amp;C is the main routine of an iterative algorithm that, by solving a LC K–RPP instance at each step, finds good solutions for the original LC K–DRPP. The computational results show that the proposed method is effective in finding good solutions for LC K–DRPP, and that the branch–and–cut algorithm for the LC K–RPP outperforms the only published exact method for this problem.},
  archive      = {J_COAP},
  author       = {Campbell, James and Corberán, Ángel and Plana, Isaac and Sanchis, José M. and Segura, Paula},
  doi          = {10.1007/s10589-022-00383-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {67-109},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Polyhedral analysis and a new algorithm for the length constrained k–drones rural postman problem},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying uncertainty with ensembles of surrogates for
blackbox optimization. <em>COAP</em>, <em>83</em>(1), 29–66. (<a
href="https://doi.org/10.1007/s10589-022-00381-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blackbox optimization tackles problems where the functions are expensive to evaluate and where no analytical information is available. In this context, a tried and tested technique is to build surrogates of the objective and the constraints in order to conduct the optimization at a cheaper computational cost. This work introduces an extension to a specific type of surrogates: ensembles of surrogates, enabling them to quantify the uncertainty on the predictions they produce. The resulting extended ensembles of surrogates behave as stochastic models and allow the use of efficient Bayesian optimization tools. The method is incorporated in the search step of the mesh adaptive direct search (MADS) algorithm to improve the exploration of the search space. Computational experiments are conducted on seven analytical problems, two multi-disciplinary optimization problems and two simulation problems. The results show that the proposed approach solves expensive simulation-based problems at a greater precision and with a lower computational effort than stochastic models.},
  archive      = {J_COAP},
  author       = {Audet, Charles and Le Digabel, Sébastien and Saltet, Renaud},
  doi          = {10.1007/s10589-022-00381-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {29-66},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quantifying uncertainty with ensembles of surrogates for blackbox optimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Block coordinate descent for smooth nonconvex constrained
minimization. <em>COAP</em>, <em>83</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s10589-022-00389-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At each iteration of a block coordinate descent method one minimizes an approximation of the objective function with respect to a generally small set of variables subject to constraints in which these variables are involved. The unconstrained case and the case in which the constraints are simple were analyzed in the recent literature. In this paper we address the problem in which block constraints are not simple and, moreover, the case in which they are not defined by global sets of equations and inequations. A general algorithm that minimizes quadratic models with quadratic regularization over blocks of variables is defined and convergence and complexity are proved. In particular, given tolerances $$\delta &gt;0$$ and $$\varepsilon &gt;0$$ for feasibility/complementarity and optimality, respectively, it is shown that a measure of $$(\delta ,0)$$ -criticality tends to zero; and the number of iterations and functional evaluations required to achieve $$(\delta ,\varepsilon )$$ -criticality is $$O(\varepsilon ^{-2})$$ . Numerical experiments in which the proposed method is used to solve a continuous version of the traveling salesman problem are presented.},
  archive      = {J_COAP},
  author       = {Birgin, E. G. and Martínez, J. M.},
  doi          = {10.1007/s10589-022-00389-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Block coordinate descent for smooth nonconvex constrained minimization},
  volume       = {83},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A path-following inexact newton method for PDE-constrained
optimal control in BV. <em>COAP</em>, <em>82</em>(3), 753–794. (<a
href="https://doi.org/10.1007/s10589-022-00370-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a PDE-constrained optimal control problem that involves functions of bounded variation as controls and includes the TV seminorm of the control in the objective. We apply a path-following inexact Newton method to the problems that arise from smoothing the TV seminorm and adding an $$H^1$$ regularization. We prove in an infinite-dimensional setting that, first, the solutions of these auxiliary problems converge to the solution of the original problem and, second, that an inexact Newton method enjoys fast local convergence when applied to a reformulation of the auxiliary optimality systems in which the control appears as implicit function of the adjoint state. We show convergence of a Finite Element approximation, provide a globalized preconditioned inexact Newton method as solver for the discretized auxiliary problems, and embed it into an inexact path-following scheme. We construct a two-dimensional test problem with fully explicit solution and present numerical results to illustrate the accuracy and robustness of the approach.},
  archive      = {J_COAP},
  author       = {Hafemeyer, D. and Mannel, F.},
  doi          = {10.1007/s10589-022-00370-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {753-794},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A path-following inexact newton method for PDE-constrained optimal control in BV},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a heuristic algorithm for the generalized
multi-objective set covering problem. <em>COAP</em>, <em>82</em>(3),
717–751. (<a href="https://doi.org/10.1007/s10589-022-00379-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set covering optimization problems (SCPs) are important and of broad interest because of their extensive applications in the real world. This study addresses the generalized multi-objective SCP (GMOSCP), which is an augmentation of the well-known multi-objective SCP problem. A mathematically driven heuristic algorithm, which uses a branching approach of the feasible region to approximate the Pareto set of the GMOSCP, is proposed. The algorithm consists of a number of components including an initial stage, a constructive stage, and an improvement stage. Each of these stages contributes significantly to the performance of the algorithm. In the initial stage, we use an achievement scalarization approach to scalarize the objective vector of the GMOSCP, which uses a reference point and a combination of weighted $$l_1$$ and $$l_\infty$$ norms of the objective function vector. Uniformly distributed weight vectors, defined with respect to this reference point, support the constructive stage to produce more widely and uniformly distributed Pareto set approximations. The constructive stage identifies feasible solutions to the problem based on a lexicographic set of selection rules. The improvement stage reduces the total cost of selected feasible solutions, which benefits the convergence of the approximations. We propose multiple cost-efficient rules in the constructive stage and investigate how they affect approximating the Pareto set. We used a diverse set of GMOSCP instances with different parameter settings for the computational experiments.},
  archive      = {J_COAP},
  author       = {Weerasena, Lakmali and Ebiefung, Aniekan and Skjellum, Anthony},
  doi          = {10.1007/s10589-022-00379-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {717-751},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Design of a heuristic algorithm for the generalized multi-objective set covering problem},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated inexact composite gradient methods for nonconvex
spectral optimization problems. <em>COAP</em>, <em>82</em>(3), 673–715.
(<a href="https://doi.org/10.1007/s10589-022-00377-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two inexact composite gradient methods, one inner accelerated and another doubly accelerated, for solving a class of nonconvex spectral composite optimization problems. More specifically, the objective function for these problems is of the form $$f_{1}+f_{2}+h$$ , where $$f_{1}$$ and $$f_{2}$$ are differentiable nonconvex matrix functions with Lipschitz continuous gradients, $$h$$ is a proper closed convex matrix function, and both $$f_{2}$$ and $$h$$ can be expressed as functions that operate on the singular values of their inputs. The methods essentially use an accelerated composite gradient method to solve a sequence of proximal subproblems involving the linear approximation of $$f_{1}$$ and the singular value functions underlying $$f_{2}$$ and $$h$$ . Unlike other composite gradient-based methods, the proposed methods take advantage of both the composite and spectral structure underlying the objective function in order to efficiently generate their solutions. Numerical experiments are presented to demonstrate the practicality of these methods on a set of real-world and randomly generated spectral optimization problems.},
  archive      = {J_COAP},
  author       = {Kong, Weiwei and Monteiro, Renato D. C.},
  doi          = {10.1007/s10589-022-00377-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {673-715},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerated inexact composite gradient methods for nonconvex spectral optimization problems},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DC semidefinite programming and cone constrained DC
optimization i: theory. <em>COAP</em>, <em>82</em>(3), 649–671. (<a
href="https://doi.org/10.1007/s10589-022-00374-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this two-part study, we discuss possible extensions of the main ideas and methods of constrained DC optimization to the case of nonlinear semidefinite programming problems and more general nonlinear cone constrained optimization problems. In the first paper, we analyse two different approaches to the definition of DC matrix-valued functions (namely, order-theoretic and componentwise), study some properties of convex and DC matrix-valued mappings and demonstrate how to compute DC decompositions of some nonlinear semidefinite constraints appearing in applications. We also compute a DC decomposition of the maximal eigenvalue of a DC matrix-valued function. This DC decomposition can be used to reformulate DC semidefinite constraints as DC inequality constrains. Finally, we study local optimality conditions for general cone constrained DC optimization problems.},
  archive      = {J_COAP},
  author       = {Dolgopolik, M. V.},
  doi          = {10.1007/s10589-022-00374-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {649-671},
  shortjournal = {Comput. Optim. Appl.},
  title        = {DC semidefinite programming and cone constrained DC optimization i: Theory},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cut-sharing across trees and efficient sequential sampling
for SDDP with uncertainty in the RHS. <em>COAP</em>, <em>82</em>(3),
617–647. (<a href="https://doi.org/10.1007/s10589-022-00376-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistage stochastic optimization problems (MSOP) are a commonly used paradigm to model many decision processes in energy and finance. Usually, a set of scenarios (the so-called tree) describing the stochasticity of the problem are obtained and the Stochastic Dual Dynamic Programming (SDDP) algorithm is often used to compute policies. Quite often, the uncertainty affects only the right-hand side (RHS) of the optimization problems in consideration. After solving a MSOP, one naturally wants to know if the solution obtained depends on the scenarios and by how much. In this paper we show that when a MSOP with stage-wise independent realizations has only RHS uncertainties, solving one tree using SDDP provides a valid lower bound for all trees with the same number of scenarios per stage without any additional computational effort. The only change to the traditional SDDP is the way cuts are calculated. Once the first tree is solved approximately, a computational assessment of the statistical significance of the current number of scenarios per stage is performed, solving for each new sampled tree, an easy LP to get a valid lower bound for the new tree. The objective of the paper is to estimate by how much the lower bound of the first tree depends on chance. The result of the computational assessment are fast estimates of the mean, variance and max variation of lower bounds across many trees. If the variance of the calculated lower bounds is small, we conclude that the cutting planes model has a small sensitivity to the trees sampled. Otherwise, we increase the number of scenarios per stage and repeat. We do not make assumptions on the distributions of the random variables. The results are not asymptotic. Our method has applications to the determination of the correct number of scenarios per stage. Extensions for uncertainties in the objective only are possible via the dual SDDP. We test our method numerically and verify the correctness of the cut-sharing technique.},
  archive      = {J_COAP},
  author       = {Borges, Pedro},
  doi          = {10.1007/s10589-022-00376-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {617-647},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cut-sharing across trees and efficient sequential sampling for SDDP with uncertainty in the RHS},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Randomized kaczmarz methods for tensor complementarity
problems. <em>COAP</em>, <em>82</em>(3), 595–615. (<a
href="https://doi.org/10.1007/s10589-022-00382-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we equivalently reformulate the tensor complementarity problem as a system of fixed point equations. Based on this system, we propose the (extend) randomized Kaczmarz methods for solving the tensor complementarity problem associated with nonnegative $$\mathcal {P}$$ -tensors and nonsingular $$\mathcal {M}$$ -tensors. We also analyze the upper bounds of the mean squared error and the estimate of the convergence rate for these two iterative methods. The computer simulation results further substantiate that the presented two randomized Kaczmarz type methods can be used to solve TCP with these two cases of tensors.},
  archive      = {J_COAP},
  author       = {Wang, Xuezhong and Che, Maolin and Wei, Yimin},
  doi          = {10.1007/s10589-022-00382-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {595-615},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Randomized kaczmarz methods for tensor complementarity problems},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-sum smooth optimization with SARAH. <em>COAP</em>,
<em>82</em>(3), 561–593. (<a
href="https://doi.org/10.1007/s10589-022-00375-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NC-SARAH for non-convex optimization as a practical modified version of the original SARAH algorithm that was developed for convex optimization. NC-SARAH is the first to achieve two crucial performance properties at the same time—allowing flexible minibatch sizes and large step sizes to achieve fast convergence in practice as verified by experiments. NC-SARAH has a close to optimal asymptotic convergence rate equal to existing prior variants of SARAH called SPIDER and SpiderBoost that either use an order of magnitude smaller step size or a fixed minibatch size. For convex optimization, we propose SARAH++ with sublinear convergence for general convex and linear convergence for strongly convex problems; and we provide a practical version for which numerical experiments on various datasets show an improved performance.},
  archive      = {J_COAP},
  author       = {Nguyen, Lam M. and van Dijk, Marten and Phan, Dzung T. and Nguyen, Phuong Ha and Weng, Tsui-Wei and Kalagnanam, Jayant R.},
  doi          = {10.1007/s10589-022-00375-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {561-593},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finite-sum smooth optimization with SARAH},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QUAntum particle swarm optimization: An auto-adaptive PSO
for local and global optimization. <em>COAP</em>, <em>82</em>(2),
525–559. (<a href="https://doi.org/10.1007/s10589-022-00362-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Swarm Optimization (PSO) is a population-based metaheuristic belonging to the class of Swarm Intelligence (SI) algorithms. Nowadays, its effectiveness on many hard problems is no longer to be proven. Nevertheless, it is known to be strongly sensitive on the choice of its settings and weak for local search. In this paper, we propose a new algorithm, called QUAntum Particle Swarm Optimization (QUAPSO) based on quantum superposition to set the velocity PSO parameters, simplifying the settings of the algorithm. Another improvement, inspired by Kangaroo Algorithm (KA), was added to PSO in order to optimize its efficiency in local search. QUAPSO was compared with a set of six well-known algorithms from the literature (two parameter sets of classical PSO, KA, Differential Evolution, Simulated Annealing Particle Swarm Optimization, Bat Algorithm and Simulated Annealing Gaussian Bat Algorithm). The experimental results show that QUAPSO outperforms the competing algorithms on a set of 30 test functions.},
  archive      = {J_COAP},
  author       = {Flori, Arnaud and Oulhadj, Hamouche and Siarry, Patrick},
  doi          = {10.1007/s10589-022-00362-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {525-559},
  shortjournal = {Comput. Optim. Appl.},
  title        = {QUAntum particle swarm optimization: An auto-adaptive PSO for local and global optimization},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A level set method for laplacian eigenvalue optimization
subject to geometric constraints. <em>COAP</em>, <em>82</em>(2),
499–524. (<a href="https://doi.org/10.1007/s10589-022-00371-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider to solve numerically the shape optimization problems of Dirichlet Laplace eigenvalues subject to volume and perimeter constraints. By combining a level set method with the relaxation approach, the algorithm can perform shape and topological changes on a fixed grid. We use the volume expressions of Eulerian derivatives in shape gradient descent algorithms. Finite element methods are used for discretizations. Two and three-dimensional numerical examples are presented to illustrate the effectiveness of the algorithms.},
  archive      = {J_COAP},
  author       = {Qian, Meizhi and Zhu, Shengfeng},
  doi          = {10.1007/s10589-022-00371-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {499-524},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A level set method for laplacian eigenvalue optimization subject to geometric constraints},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Second order semi-smooth proximal newton methods in hilbert
spaces. <em>COAP</em>, <em>82</em>(2), 465–498. (<a
href="https://doi.org/10.1007/s10589-022-00369-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a globalized Proximal Newton method for composite and possibly non-convex minimization problems in Hilbert spaces. Additionally, we impose less restrictive assumptions on the composite objective functional considering differentiability and convexity than in existing theory. As far as differentiability of the smooth part of the objective function is concerned, we introduce the notion of second order semi-smoothness and discuss why it constitutes an adequate framework for our Proximal Newton method. However, both global convergence as well as local acceleration still pertain to hold in our scenario. Eventually, the convergence properties of our algorithm are displayed by solving a toy model problem in function space.},
  archive      = {J_COAP},
  author       = {Pötzl, Bastian and Schiela, Anton and Jaap, Patrick},
  doi          = {10.1007/s10589-022-00369-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {465-498},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Second order semi-smooth proximal newton methods in hilbert spaces},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Malitsky-tam forward-reflected-backward splitting method for
nonconvex minimization problems. <em>COAP</em>, <em>82</em>(2), 441–463.
(<a href="https://doi.org/10.1007/s10589-022-00364-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the Malitsky-Tam forward-reflected-backward (FRB) splitting method for inclusion problems of monotone operators to nonconvex minimization problems. By assuming the generalized concave Kurdyka-Łojasiewicz (KL) property of a quadratic regularization of the objective, we show that the FRB method converges globally to a stationary point of the objective and enjoys the finite length property. Convergence rates are also given. The sharpness of our approach is guaranteed by virtue of the exact modulus associated with the generalized concave KL property. Numerical experiments suggest that FRB is competitive compared to the Douglas-Rachford method and the Boţ-Csetnek inertial Tseng’s method.},
  archive      = {J_COAP},
  author       = {Wang, Xianfu and Wang, Ziyuan},
  doi          = {10.1007/s10589-022-00364-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {441-463},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Malitsky-tam forward-reflected-backward splitting method for nonconvex minimization problems},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Douglas–rachford splitting and ADMM for nonconvex
optimization: Accelerated and newton-type linesearch algorithms.
<em>COAP</em>, <em>82</em>(2), 395–440. (<a
href="https://doi.org/10.1007/s10589-022-00366-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the performance of popular optimization algorithms such as the Douglas–Rachford splitting (DRS) and the ADMM is satisfactory in convex and well-scaled problems, ill conditioning and nonconvexity pose a severe obstacle to their reliable employment. Expanding on recent convergence results for DRS and ADMM applied to nonconvex problems, we propose two linesearch algorithms to enhance and robustify these methods by means of quasi-Newton directions. The proposed algorithms are suited for nonconvex problems, require the same black-box oracle of DRS and ADMM, and maintain their (subsequential) convergence properties. Numerical evidence shows that the employment of L-BFGS in the proposed framework greatly improves convergence of DRS and ADMM, making them robust to ill conditioning. Under regularity and nondegeneracy assumptions at the limit point, superlinear convergence is shown when quasi-Newton Broyden directions are adopted.},
  archive      = {J_COAP},
  author       = {Themelis, Andreas and Stella, Lorenzo and Patrinos, Panagiotis},
  doi          = {10.1007/s10589-022-00366-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {395-440},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Douglas–Rachford splitting and ADMM for nonconvex optimization: Accelerated and newton-type linesearch algorithms},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated gradient sliding for structured convex
optimization. <em>COAP</em>, <em>82</em>(2), 361–394. (<a
href="https://doi.org/10.1007/s10589-022-00365-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our main goal in this paper is to show that one can skip gradient computations for gradient descent type methods applied to certain structured convex programming (CP) problems. To this end, we first present an accelerated gradient sliding (AGS) method for minimizing the summation of two smooth convex functions with different Lipschitz constants. We show that the AGS method can skip the gradient computation for one of these smooth components without slowing down the overall optimal rate of convergence. This result is much sharper than the classic black-box CP complexity results especially when the difference between the two Lipschitz constants associated with these components is large. We then consider an important class of bilinear saddle point problem whose objective function is given by the summation of a smooth component and a nonsmooth one with a bilinear saddle point structure. Using the aforementioned AGS method for smooth composite optimization and Nesterov’s smoothing technique, we show that one only needs $${{\mathcal{O}}}(1/\sqrt{\varepsilon })$$ gradient computations for the smooth component while still preserving the optimal $${{\mathcal{O}}}(1/\varepsilon )$$ overall iteration complexity for solving these saddle point problems. We demonstrate that even more significant savings on gradient computations can be obtained for strongly convex smooth and bilinear saddle point problems.},
  archive      = {J_COAP},
  author       = {Lan, Guanghui and Ouyang, Yuyuan},
  doi          = {10.1007/s10589-022-00365-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {361-394},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerated gradient sliding for structured convex optimization},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On initial point selection of the steepest descent algorithm
for general quadratic functions. <em>COAP</em>, <em>82</em>(2), 329–360.
(<a href="https://doi.org/10.1007/s10589-022-00372-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove some new results about the asymptotic behavior of the steepest descent algorithm for general quadratic functions. Some well-known results of this theory are developed and extended to non-convex functions. We propose an efficient strategy for choosing initial points in the algorithm and show that this strategy can dramatically enhance the performance of the method. Furthermore, a modified version of the steepest descent algorithm equipped with a pre-initialization step is introduced. We show that an initial guess near the optimal solution does not necessarily imply fast convergence. We also propose a new approach to investigate the behavior of the method for non-convex quadratic functions. Moreover, some interesting results about the role of initial points in convergence to saddle points are presented. Finally, we investigate the probability of divergence for uniform random initial points.},
  archive      = {J_COAP},
  author       = {Fatemi, Masoud},
  doi          = {10.1007/s10589-022-00372-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {329-360},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On initial point selection of the steepest descent algorithm for general quadratic functions},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Derivative-free methods for mixed-integer nonsmooth
constrained optimization. <em>COAP</em>, <em>82</em>(2), 293–327. (<a
href="https://doi.org/10.1007/s10589-022-00363-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, mixed-integer nonsmooth constrained optimization problems are considered, where objective/constraint functions are available only as the output of a black-box zeroth-order oracle that does not provide derivative information. A new derivative-free linesearch-based algorithmic framework is proposed to suitably handle those problems. First, a scheme for bound constrained problems that combines a dense sequence of directions to handle the nonsmoothness of the objective function with primitive directions to handle discrete variables is described. Then, an exact penalty approach is embedded in the scheme to suitably manage nonlinear (possibly nonsmooth) constraints. Global convergence properties of the proposed algorithms toward stationary points are analyzed and results of an extensive numerical experience on a set of mixed-integer test problems are reported.},
  archive      = {J_COAP},
  author       = {Giovannelli, Tommaso and Liuzzi, Giampaolo and Lucidi, Stefano and Rinaldi, Francesco},
  doi          = {10.1007/s10589-022-00363-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {293-327},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Derivative-free methods for mixed-integer nonsmooth constrained optimization},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDP-based bounds for graph partition via extended ADMM.
<em>COAP</em>, <em>82</em>(1), 251–291. (<a
href="https://doi.org/10.1007/s10589-022-00355-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study two NP-complete graph partition problems, k-equipartition problems and graph partition problems with knapsack constraints (GPKC). We introduce tight SDP relaxations with nonnegativity constraints to get lower bounds, the SDP relaxations are solved by an extended alternating direction method of multipliers (ADMM). In this way, we obtain high quality lower bounds for k-equipartition on large instances up to $$n =1000$$ vertices within as few as 5 min and for GPKC problems up to $$n=500$$ vertices within as little as 1 h. On the other hand, interior point methods fail to solve instances from $$n=300$$ due to memory requirements. We also design heuristics to generate upper bounds from the SDP solutions, giving us tighter upper bounds than other methods proposed in the literature with low computational expense.},
  archive      = {J_COAP},
  author       = {Wiegele, Angelika and Zhao, Shudian},
  doi          = {10.1007/s10589-022-00355-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {251-291},
  shortjournal = {Comput. Optim. Appl.},
  title        = {SDP-based bounds for graph partition via extended ADMM},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust output-feedback stabilization for incompressible
flows using low-dimensional <span
class="math display">ℋ<sub>∞</sub></span> -controllers. <em>COAP</em>,
<em>82</em>(1), 225–249. (<a
href="https://doi.org/10.1007/s10589-022-00359-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Output-based controllers are known to be fragile with respect to model uncertainties. The standard $$\mathcal {H}_{\infty }$$ -control theory provides a general approach to robust controller design based on the solution of the $$\mathcal {H}_{\infty }$$ -Riccati equations. In view of stabilizing incompressible flows in simulations, two major challenges have to be addressed: the high-dimensional nature of the spatially discretized model and the differential-algebraic structure that comes with the incompressibility constraint. This work demonstrates the synthesis of low-dimensional robust controllers with guaranteed robustness margins for the stabilization of incompressible flow problems. The performance and the robustness of the reduced-order controller with respect to linearization and model reduction errors are investigated and illustrated in numerical examples.},
  archive      = {J_COAP},
  author       = {Benner, Peter and Heiland, Jan and Werner, Steffen W. R.},
  doi          = {10.1007/s10589-022-00359-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {225-249},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Robust output-feedback stabilization for incompressible flows using low-dimensional $$\mathcal {H}_{\infty }$$ -controllers},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complexity of an inexact proximal-point penalty method for
constrained smooth non-convex optimization. <em>COAP</em>,
<em>82</em>(1), 175–224. (<a
href="https://doi.org/10.1007/s10589-022-00358-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an inexact proximal-point penalty method is studied for constrained optimization problems, where the objective function is non-convex, and the constraint functions can also be non-convex. This method approximately solves a sequence of subproblems, each of which is formed by adding to the original objective function a proximal term and quadratic penalty terms associated to the constraint functions. Under a weak-convexity assumption, each subproblem is made strongly convex and can be solved effectively to a required accuracy by an optimal gradient-based method. The computational complexity of this approach is analyzed separately for the cases of convex constraint and non-convex constraint. For both cases, the complexity results are established in terms of the number of proximal gradient steps needed to find an $$\varepsilon$$ -stationary point. When the constraint functions are convex, we show a complexity result of $$\tilde{O}(\varepsilon ^{-5/2})$$ to produce an $$\varepsilon$$ -stationary point under the Slater’s condition. When the constraint functions are non-convex, the complexity becomes $${\tilde{O}}(\varepsilon ^{-3})$$ if a non-singularity condition holds on constraints and otherwise $$\tilde{O}(\varepsilon ^{-4})$$ if a feasible initial solution is available.},
  archive      = {J_COAP},
  author       = {Lin, Qihang and Ma, Runchao and Xu, Yangyang},
  doi          = {10.1007/s10589-022-00358-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {175-224},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Complexity of an inexact proximal-point penalty method for constrained smooth non-convex optimization},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inexact successive quadratic approximation method for a
class of difference-of-convex optimization problems. <em>COAP</em>,
<em>82</em>(1), 141–173. (<a
href="https://doi.org/10.1007/s10589-022-00357-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method for a class of difference-of-convex (DC) optimization problems, whose objective is the sum of a smooth function and a possibly non-prox-friendly DC function. The method sequentially solves subproblems constructed from a quadratic approximation of the smooth function and a linear majorization of the concave part of the DC function. We allow the subproblem to be solved inexactly, and propose a new inexact rule to characterize the inexactness of the approximate solution. For several classical algorithms applied to the subproblem, we derive practical termination criteria so as to obtain solutions satisfying the inexact rule. We also present some convergence results for our method, including the global subsequential convergence and a non-asymptotic complexity analysis. Finally, numerical experiments are conducted to illustrate the efficiency of our method.},
  archive      = {J_COAP},
  author       = {Liu, Tianxiang and Takeda, Akiko},
  doi          = {10.1007/s10589-022-00357-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {141-173},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact successive quadratic approximation method for a class of difference-of-convex optimization problems},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Parametric shape optimization using the
support function. <em>COAP</em>, <em>82</em>(1), 139. (<a
href="https://doi.org/10.1007/s10589-022-00367-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Antunes, Pedro R. S. and Bogosel, Beniamin},
  doi          = {10.1007/s10589-022-00367-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {139},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: Parametric shape optimization using the support function},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Parametric shape optimization using the support function.
<em>COAP</em>, <em>82</em>(1), 107–138. (<a
href="https://doi.org/10.1007/s10589-022-00360-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization of shape functionals under convexity, diameter or constant width constraints shows numerical challenges. The support function can be used in order to approximate solutions to such problems by finite dimensional optimization problems under various constraints. We propose a numerical framework in dimensions two and three and we present applications from the field of convex geometry. We consider the optimization of functionals depending on the volume, perimeter and Dirichlet Laplace eigenvalues under the aforementioned constraints. In particular we confirm numerically Meissner’s conjecture, regarding three dimensional bodies of constant width with minimal volume.},
  archive      = {J_COAP},
  author       = {Antunes, Pedro R. S. and Bogosel, Beniamin},
  doi          = {10.1007/s10589-022-00360-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {107-138},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Parametric shape optimization using the support function},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local saddle points for unconstrained polynomial
optimization. <em>COAP</em>, <em>82</em>(1), 89–106. (<a
href="https://doi.org/10.1007/s10589-022-00361-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives an algorithm for computing local saddle points for unconstrained polynomial optimization. It is based on optimality conditions and Lasserre’s hierarchy of semidefinite relaxations. It can determine the existence of local saddle points. When there are several different local saddle point values, the algorithm can get them from the smallest one to the largest one.},
  archive      = {J_COAP},
  author       = {Zhao, Wenjie and Zhou, Guangming},
  doi          = {10.1007/s10589-022-00361-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {89-106},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Local saddle points for unconstrained polynomial optimization},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A regularized limited memory BFGS method for large-scale
unconstrained optimization and its efficient implementations.
<em>COAP</em>, <em>82</em>(1), 61–88. (<a
href="https://doi.org/10.1007/s10589-022-00351-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited memory BFGS (L-BFGS) method is one of the popular methods for solving large-scale unconstrained optimization. Since the standard L-BFGS method uses a line search to guarantee its global convergence, it sometimes requires a large number of function evaluations. To overcome the difficulty, we propose a new L-BFGS with a certain regularization technique. We show its global convergence under the usual assumptions. In order to make the method more robust and efficient, we also extend it with several techniques such as the nonmonotone technique and simultaneous use of the Wolfe line search. Finally, we present some numerical results for test problems in CUTEst, which show that the proposed method is robust in terms of solving more problems.},
  archive      = {J_COAP},
  author       = {Tankaria, Hardik and Sugimoto, Shinji and Yamashita, Nobuo},
  doi          = {10.1007/s10589-022-00351-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {61-88},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A regularized limited memory BFGS method for large-scale unconstrained optimization and its efficient implementations},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive trust-region method without function
evaluations. <em>COAP</em>, <em>82</em>(1), 31–60. (<a
href="https://doi.org/10.1007/s10589-022-00356-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose an adaptive trust-region method for smooth unconstrained optimization. The update rule for the trust-region radius relies only on gradient evaluations. Assuming that the gradient of the objective function is Lipschitz continuous, we establish worst-case complexity bounds for the number of gradient evaluations required by the proposed method to generate approximate stationary points. As a corollary, we establish a global convergence result. We also present numerical results on benchmark problems. In terms of the number of calls of the oracle, the proposed method compares favorably with trust-region methods that use evaluations of the objective function.},
  archive      = {J_COAP},
  author       = {Grapiglia, Geovani N. and Stella, Gabriel F. D.},
  doi          = {10.1007/s10589-022-00356-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {31-60},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An adaptive trust-region method without function evaluations},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging special-purpose hardware for local search
heuristics. <em>COAP</em>, <em>82</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10589-022-00354-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we approach the physical limits predicted by Moore’s law, a variety of specialized hardware is emerging to tackle specialized tasks in different domains. Within combinatorial optimization, adiabatic quantum computers, complementary metal-oxide semiconductor annealers, and optical parametric oscillators are a few emerging specialized hardware technologies to solve optimization problems. The Ising optimization model unifies all of these emerging special-purpose hardware for optimization in terms of mathematical framework. In other words, they are all designed to solve optimization problems expressed in the Ising model or equivalently as a quadratic unconstrained binary optimization model. Due to various constraints specific to each type of hardware, they usually suffer from a major challenge: the number of variables that the hardware can manage to solve is very limited. The local search meta-heuristic is one of the approaches to tackle large-scale problems. However, a general optimization step within local search is not traditionally formulated in the Ising form. In this work, we introduce a new modeling framework for modeling local search heuristics for special-purpose hardware. In particular, we propose models that take the limitations of the Ising model and current hardware into account. As such, we demonstrate the advantage of our approach compared to previous methods by carrying out experiments to show that our local search models produce higher-quality solutions.},
  archive      = {J_COAP},
  author       = {Liu, Xiaoyuan and Ushijima-Mwesigwa, Hayato and Mandal, Avradip and Upadhyay, Sarvagya and Safro, Ilya and Roy, Arnab},
  doi          = {10.1007/s10589-022-00354-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Leveraging special-purpose hardware for local search heuristics},
  volume       = {82},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding the global optimum of a class of quartic
minimization problem. <em>COAP</em>, <em>81</em>(3), 923–954. (<a
href="https://doi.org/10.1007/s10589-021-00345-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a special nonconvex quartic minimization problem over a single spherical constraint, which includes the discretized energy functional minimization problem of non-rotating Bose-Einstein condensates (BECs) as one of the important applications. Such a problem is studied by exploiting its characterization as a nonlinear eigenvalue problem with eigenvector nonlinearity (NEPv). Firstly, we show that the NEPv has a unique nonnegative eigenvector, corresponding to the smallest nonlinear eigenvalue of NEPv, which is exactly the global minimizer to the optimization problem. Secondly, with these properties, we obtain that any algorithm converging to the nonnegative stationary point of this optimization problem finds its global optimum, such as the regularized Newton method. In particular, we obtain the convergence to the global optimum of the inexact alternating direction method of multipliers for this problem. Numerical experiments for applications in non-rotating BECs validate our theories.},
  archive      = {J_COAP},
  author       = {Huang, Pengfei and Yang, Qingzhi and Yang, Yuning},
  doi          = {10.1007/s10589-021-00345-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {923-954},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding the global optimum of a class of quartic minimization problem},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A priori error estimate of perturbation method for optimal
control problem governed by elliptic PDEs with small uncertainties.
<em>COAP</em>, <em>81</em>(3), 889–921. (<a
href="https://doi.org/10.1007/s10589-022-00352-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the first-order and second-order perturbation approximation schemes for an optimal control problem governed by elliptic PDEs with small uncertainties. The optimal control minimizes the expectation of a cost functional with a deterministic constrained control. First, using a perturbation method, we expand the state and co-state variables up to a certain order with respect to a parameter that controls the magnitude of uncertainty in the input. Then we take the expansions into the known deterministic parametric optimality system to derive the first-order and second-order optimality systems which are both deterministic problems. After that, the two systems are discretized by finite element method directly. The strong and weak error estimates are derived for the state, co-state and control variables, respectively. We finally illustrate the theoretical results by two numerical examples.},
  archive      = {J_COAP},
  author       = {Feng, Mengya and Sun, Tongjun},
  doi          = {10.1007/s10589-022-00352-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {889-921},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A priori error estimate of perturbation method for optimal control problem governed by elliptic PDEs with small uncertainties},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Method for solving bang-bang and singular optimal control
problems using adaptive radau collocation. <em>COAP</em>,
<em>81</em>(3), 857–887. (<a
href="https://doi.org/10.1007/s10589-022-00350-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method is developed for solving bang-bang and singular optimal control problems using adaptive Legendre–Gauss–Radau collocation. The method is divided into several parts. First, a structure detection method is developed that identifies switch times in the control and analyzes the corresponding switching function for segments where the solution is either bang-bang or singular. Second, after the structure has been detected, the domain is decomposed into multiple domains such that the multiple-domain formulation includes additional decision variables that represent the switch times in the optimal control. In domains classified as bang-bang, the control is set to either its upper or lower limit. In domains identified as singular, the objective function is augmented with a regularization term to avoid the singular arc. An iterative procedure is then developed for singular domains to obtain a control that lies in close proximity to the singular control. The method is demonstrated on four examples, three of which have either a bang-bang and/or singular optimal control while the fourth has a smooth and nonsingular optimal control. The results demonstrate that the method of this paper provides accurate solutions to problems whose solutions are either bang-bang or singular when compared against previously developed mesh refinement methods that are not tailored for solving nonsmooth and/or singular optimal control problems, and produces results that are equivalent to those obtained using previously developed mesh refinement methods for optimal control problems whose solutions are smooth.},
  archive      = {J_COAP},
  author       = {Pager, Elisha R. and Rao, Anil V.},
  doi          = {10.1007/s10589-022-00350-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {857-887},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Method for solving bang-bang and singular optimal control problems using adaptive radau collocation},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagonal BFGS updates and applications to the limited memory
BFGS method. <em>COAP</em>, <em>81</em>(3), 829–856. (<a
href="https://doi.org/10.1007/s10589-022-00353-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two diagonal BFGS-type updates. One is the diagonal part of the ordinary BFGS update on a diagonal matrix. The other is its inverse version. Both diagonal updates preserve the positive definiteness as the ordinary BFGS update. The related diagonal BFGS methods can be regarded as extensions of the well-known Barzilai-Borwein method. Under appropriate conditions, we prove that both diagonal BFGS methods are globally convergent when applied to minimizing a convex or non-convex function. In addition, the diagonal quasi-Newton method with inverse diagonal BFGS update can be even superlinearly convergent if the function to be minimized is uniformly convex and completely separable. We apply the proposed diagonal BFGS updates to the limited memory BFGS (L-BFGS) method using the diagonal BFGS matrix as initial matrix. Our numerical results show the efficiency of the L-BFGS methods with diagonal BFGS updates.},
  archive      = {J_COAP},
  author       = {Li, Donghui and Wang, Xiaozhou and Huang, Jiajian},
  doi          = {10.1007/s10589-022-00353-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {829-856},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Diagonal BFGS updates and applications to the limited memory BFGS method},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering-based multipopulation approaches in MOEA/d for
many-objective problems. <em>COAP</em>, <em>81</em>(3), 789–828. (<a
href="https://doi.org/10.1007/s10589-022-00348-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a new multipopulation framework for the multiobjective evolutionary algorithm based on decomposition (MOEA/D). In this case, clustering methods are used to reinforce mating restrictions by splitting the MOEA/D evolutionary population into multiple subpopulations of similar individuals for independent evolution. Using subpopulations leads to a natural parallel implementation by assigning each subpopulation to a different processor. The proposed multipopulation MOEA/D (mpMOEA/D) is evaluated using three clustering methods: k-Means, spectral-based clustering, and a method based on the shape of objective vectors. Additionally, a random partitioning approach is tested. Metrics measuring convergence, diversity and computation time are used to compare the results of the mpMOEA/D alternatives and the original MOEA/D using DTLZ and WFG problems with 3, 4, 8 and 10 objectives. Evaluation using the Wilcoxon test and the Friedman rank reveals the importance of using clustering procedures for population division, especially in cases with many objectives. The results show the viability of the clustering-based multipopulation approach in enhancing the performance of evolutionary methods for many-objective problems.},
  archive      = {J_COAP},
  author       = {von Lücken, Christian and Brizuela, Carlos A. and Barán, Benjamín},
  doi          = {10.1007/s10589-022-00348-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {789-828},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Clustering-based multipopulation approaches in MOEA/D for many-objective problems},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonmonotone trust region algorithm for solving the
unconstrained multiobjective optimization problems. <em>COAP</em>,
<em>81</em>(3), 769–788. (<a
href="https://doi.org/10.1007/s10589-021-00346-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work an iterative method to solve the nonlinear multiobjective problem is presented. The goal is to find locally optimal points for the problem, that is, points that cannot simultaneously improve all functions when we compare the value at the point with those in their neighborhood. The algorithm uses a strategy developed in previous works by several authors but globalization is obtained through a nonmonotone technique. The construction of a new ratio between the actual descent and predicted descent plays a key role for selecting the new point and updating the trust region radius. On the other hand, we introduce a modification in the quadratic model used to determine if the point is accepted or not, which is fundamental for the convergence of the method. The combination of this strategy with a Newton-type method leads to an algorithm whose convergence properties are proved. The numerical experimentation is performed using a known set of test problems. Preliminary numerical results show that the nonmonotone method can be more efficient when it is compared to another algorithm that use the classic trust region approach.},
  archive      = {J_COAP},
  author       = {Ramirez, V. A. and Sottosanto, G. N.},
  doi          = {10.1007/s10589-021-00346-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {769-788},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonmonotone trust region algorithm for solving the unconstrained multiobjective optimization problems},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A successive relaxation algorithm to solve a MILP involving
piecewise linear functions with application to road design.
<em>COAP</em>, <em>81</em>(3), 741–767. (<a
href="https://doi.org/10.1007/s10589-021-00347-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new algorithm to build feasible solutions to a MILP formulation of the vertical alignment problem in road design. This MILP involves a large number of special ordered set of type 2 variables used to describe piecewise linear functions. The principle of the algorithm is to successively solve LPs adapted from the MILP by replacing the special ordered set of type 2 constraints by linear constraints. Proof that the solutions to the successive linear relaxations of the MILP converge to a feasible solution to the MILP is provided. Numerical results emphasize that the algorithm performs better than CPLEX   for large scale vertical alignment problems.},
  archive      = {J_COAP},
  author       = {Monnet, Dominique and Hare, Warren and Lucet, Yves},
  doi          = {10.1007/s10589-021-00347-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {741-767},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A successive relaxation algorithm to solve a MILP involving piecewise linear functions with application to road design},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the acceleration of the barzilai–borwein method.
<em>COAP</em>, <em>81</em>(3), 717–740. (<a
href="https://doi.org/10.1007/s10589-022-00349-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Barzilai–Borwein (BB) gradient method is efficient for solving large-scale unconstrained problems to modest accuracy due to its ingenious stepsize which generally yields nonmonotone behavior. In this paper, we propose a new stepsize to accelerate the BB method by requiring finite termination for minimizing the two-dimensional strongly convex quadratic function. Based on this new stepsize, we develop an efficient gradient method for quadratic optimization which adaptively takes the nonmonotone BB stepsizes and certain monotone stepsizes. Two variants using retard stepsizes associated with the new stepsize are also presented. Numerical experiments show that our strategies of properly inserting monotone gradient steps into the nonmonotone BB method could significantly improve its performance and our new methods are competitive with the most successful gradient descent methods developed in the recent literature.},
  archive      = {J_COAP},
  author       = {Huang, Yakui and Dai, Yu-Hong and Liu, Xin-Wei and Zhang, Hongchao},
  doi          = {10.1007/s10589-022-00349-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {717-740},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the acceleration of the Barzilai–Borwein method},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Accelerated derivative-free nonlinear least-squares applied
to the estimation of manning coefficients. <em>COAP</em>,
<em>81</em>(3), 689–715. (<a
href="https://doi.org/10.1007/s10589-021-00344-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general framework for solving nonlinear least squares problems without the employment of derivatives is proposed in the present paper together with a new general global convergence theory. With the aim to cope with the case in which the number of variables is big (for the standards of derivative-free optimization), two dimension-reduction procedures are introduced. One of them is based on iterative subspace minimization and the other one is based on spline interpolation with variable nodes. Each iteration based on those procedures is followed by an acceleration step inspired in the Sequential Secant Method. The practical motivation for this work is the estimation of parameters in Hydraulic models applied to dam breaking problems. Numerical examples of the application of the new method to those problems are given.},
  archive      = {J_COAP},
  author       = {Birgin, E. G. and Martínez, J. M.},
  doi          = {10.1007/s10589-021-00344-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {689-715},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerated derivative-free nonlinear least-squares applied to the estimation of manning coefficients},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general variable neighborhood search for the cyclic
antibandwidth problem. <em>COAP</em>, <em>81</em>(2), 657–687. (<a
href="https://doi.org/10.1007/s10589-021-00334-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Layout Problems refer to a family of optimization problems where the aim is to assign the vertices of an input graph to the vertices of a structured host graph, optimizing a certain objective function. In this paper, we tackle one of these problems, named Cyclic Antibandwidth Problem, where the objective is to maximize the minimum distance of all adjacent vertices, computed in a cycle host graph. Specifically, we propose a General Variable Neighborhood Search which combines an efficient Variable Neighborhood Descent with a novel destruction–reconstruction shaking procedure. Additionally, our proposal takes advantage of two new exploration strategies for this problem: a criterion for breaking the tie of solutions with the same objective function and an efficient evaluation of neighboring solutions. Furthermore, two new neighborhood reduction strategies are proposed. We conduct a thorough computational experience by comparing the algorithm proposed with the current state-of-the-art methods over a set of previously reported instances. The associated results show the merit of the introduced algorithm, emerging as the best performance method in those instances where the optima are unknown. These results are further confirmed with nonparametric statistical tests.},
  archive      = {J_COAP},
  author       = {Cavero, Sergio and Pardo, Eduardo G. and Duarte, Abraham},
  doi          = {10.1007/s10589-021-00334-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {657-687},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A general variable neighborhood search for the cyclic antibandwidth problem},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An alternate approach to solve two-level priority based
assignment problem. <em>COAP</em>, <em>81</em>(2), 613–656. (<a
href="https://doi.org/10.1007/s10589-021-00340-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-level priority based assignment problem (2PBAP) is an important optimization problem due to the timely delivery requirement in project management. Only four methods with deficiency are available in the literature to solve 2PBAP. In this paper, 2PBAP is reduced to a series of finding the feasible flow in a network with lower and upper arc capacities, and consequently two iterative algorithms are developed to solve 2PBAP. It is proved that both iterative algorithms find the optimal solution to 2PBAP in a strongly polynomial time. Owing to having fully utilized the network flow structural characteristic inherent to 2PBAP, both iterative algorithms have the advantages such as easy implementation on computer, no memory overflow in implementation on computer for large scale instances, high computational efficiency, and easy extension to the priority based assignment problem with priority level more than two, and successfully overcome the deficiency of existing approaches. Computational experiments validate that in terms of computational time, one of our proposed iterative algorithms has higher efficiency than the other, and rivals the existing best approach.},
  archive      = {J_COAP},
  author       = {Xie, Fanrong and Sharma, Anuj and Li, Zuoan},
  doi          = {10.1007/s10589-021-00340-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {613-656},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An alternate approach to solve two-level priority based assignment problem},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative regularization for constrained minimization
formulations of nonlinear inverse problems. <em>COAP</em>,
<em>81</em>(2), 569–611. (<a
href="https://doi.org/10.1007/s10589-021-00343-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the formulation of inverse problems as constrained minimization problems and their iterative solution by gradient or Newton type methods. We carry out a convergence analysis in the sense of regularization methods and discuss applicability to the problem of identifying the spatially varying diffusivity in an elliptic PDE from different sets of observations. Among these is a novel hybrid imaging technology known as impedance acoustic tomography, for which we provide numerical experiments.},
  archive      = {J_COAP},
  author       = {Kaltenbacher, Barbara and Van Huynh, Kha},
  doi          = {10.1007/s10589-021-00343-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {569-611},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Iterative regularization for constrained minimization formulations of nonlinear inverse problems},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reduced proximal-point homotopy method for large-scale
non-convex BQP. <em>COAP</em>, <em>81</em>(2), 539–567. (<a
href="https://doi.org/10.1007/s10589-021-00330-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a reduced proximal-point homotopy (RPP-Hom) method is presented for large-scale non-convex box constrained quadratic programming (BQP) problems. As the outer iteration, at each step, the reduced proximal-point (RPP) algorithm applies the proximal point algorithm to a reduced BQP problem. The variables of the reduced subproblem include all free variables and variables at bound with respect to which the optimality conditions are violated. The RPP subproblem is solved by, as the inner iteration, an efficient piecewise linear homotopy path following method. A special termination criterion for the RPP algorithm is given and the global convergence as well as the locally linear convergence to a Karush-Kuhn-Tucker point is proved. Furthermore, a random perturbation procedure is given to modify RPP such that it converges to a local minimizer with probability 1. An accelerated version of RPP is also presented. Numerical experiments show that the RPP-Hom method outperforms the state-of-the-art algorithms for most of the benchmark problems, especially for training non-convex support vector machine.},
  archive      = {J_COAP},
  author       = {Liang, Xiubo and Wang, Guoqiang and Yu, Bo},
  doi          = {10.1007/s10589-021-00330-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {539-567},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A reduced proximal-point homotopy method for large-scale non-convex BQP},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A smoothing proximal gradient algorithm for matrix rank
minimization problem. <em>COAP</em>, <em>81</em>(2), 519–538. (<a
href="https://doi.org/10.1007/s10589-021-00337-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the low-rank matrix minimization problem, where the loss function is convex but nonsmooth and the penalty term is defined by the cardinality function. We first introduce an exact continuous relaxation, that is, both problems have the same minimizers and the same optimal value. In particular, we introduce a class of lifted stationary points of the relaxed problem and show that any local minimizer of the relaxed problem must be a lifted stationary point. In addition, we derive lower bound property for the nonzero singular values of the lifted stationary point and hence also of the local minimizers of the relaxed problem. Then the smoothing proximal gradient (SPG) algorithm is proposed to find a lifted stationary point of the continuous relaxation model. Moreover, it is shown that any accumulating point of the sequence generated by SPG algorithm is a lifted stationary point. At last, numerical examples show the efficiency of the SPG algorithm.},
  archive      = {J_COAP},
  author       = {Yu, Quan and Zhang, Xinzhen},
  doi          = {10.1007/s10589-021-00337-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {519-538},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A smoothing proximal gradient algorithm for matrix rank minimization problem},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inexact accelerated stochastic ADMM for separable convex
optimization. <em>COAP</em>, <em>81</em>(2), 479–518. (<a
href="https://doi.org/10.1007/s10589-021-00338-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An inexact accelerated stochastic Alternating Direction Method of Multipliers (AS-ADMM) scheme is developed for solving structured separable convex optimization problems with linear constraints. The objective function is the sum of a possibly nonsmooth convex function and a smooth function which is an average of many component convex functions. Problems having this structure often arise in machine learning and data mining applications. AS-ADMM combines the ideas of both ADMM and the stochastic gradient methods using variance reduction techniques. One of the ADMM subproblems employs a linearization technique while a similar linearization could be introduced for the other subproblem. For a specified choice of the algorithm parameters, it is shown that the objective error and the constraint violation are $$\mathcal {O}(1/k)$$ relative to the number of outer iterations k. Under a strong convexity assumption, the expected iterate error converges to zero linearly. A linearized variant of AS-ADMM and incremental sampling strategies are also discussed. Numerical experiments with both stochastic and deterministic ADMM algorithms show that AS-ADMM can be particularly effective for structured optimization arising in big data applications.},
  archive      = {J_COAP},
  author       = {Bai, Jianchao and Hager, William W. and Zhang, Hongchao},
  doi          = {10.1007/s10589-021-00338-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {479-518},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact accelerated stochastic ADMM for separable convex optimization},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linearization and parallelization schemes for convex
mixed-integer nonlinear optimization. <em>COAP</em>, <em>81</em>(2),
423–478. (<a href="https://doi.org/10.1007/s10589-021-00335-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and test linearization and parallelization schemes for convex mixed-integer nonlinear programming. Several linearization approaches are proposed for LP/NLP based branch-and-bound. Some of these approaches strengthen the linear approximation to nonlinear constraints at the root node and some at the other branch-and-bound nodes. Two of the techniques are specifically applicable to commonly found univariate nonlinear functions and are more effective than other general approaches. These techniques have been implemented in the Minotaur toolkit. Tests on benchmark instances show up to 12\% improvement in the average time to solve the instances. Shared-memory parallel versions of NLP based branch-and-bound and LP/NLP based branch-and-bound algorithms have also been developed in the toolkit. These implementations solve different nodes of branch-and-bound concurrently. About 44\% improvement in the speed and an increase in the number of instances solved within the time limit are observed when the two schemes are used together on a computer with 16 cores. These parallelization methods are compared to alternate approaches that exploit parallelism in existing commercial MILP solvers. The latter approaches are seen to perform better thus highlighting the importance of MILP techniques.},
  archive      = {J_COAP},
  author       = {Sharma, Meenarli and Palkar, Prashant and Mahajan, Ashutosh},
  doi          = {10.1007/s10589-021-00335-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {423-478},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Linearization and parallelization schemes for convex mixed-integer nonlinear optimization},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential optimality conditions for nonlinear optimization
on riemannian manifolds and a globally convergent augmented lagrangian
method. <em>COAP</em>, <em>81</em>(2), 397–421. (<a
href="https://doi.org/10.1007/s10589-021-00336-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the approximate Karush–Kuhn–Tucker (AKKT) conditions, also called the sequential optimality conditions, have been proposed for nonlinear optimization in Euclidean spaces, and several methods to find points satisfying such conditions have been developed by researchers. These conditions are known as genuine necessary optimality conditions because all local optima satisfy them with no constraint qualification (CQ). In this paper, we extend the AKKT conditions to nonlinear optimization on Riemannian manifolds and propose an augmented Lagrangian (AL) method that globally converges to points satisfying such conditions. In addition, we prove that the AKKT and KKT conditions are indeed equivalent under a certain CQ. Finally, we examine the effectiveness of the proposed AL method via several numerical experiments.},
  archive      = {J_COAP},
  author       = {Yamakawa, Yuya and Sato, Hiroyuki},
  doi          = {10.1007/s10589-021-00336-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {397-421},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Sequential optimality conditions for nonlinear optimization on riemannian manifolds and a globally convergent augmented lagrangian method},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a primal-dual newton proximal method for convex quadratic
programs. <em>COAP</em>, <em>81</em>(2), 369–395. (<a
href="https://doi.org/10.1007/s10589-021-00342-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces QPDO, a primal-dual method for convex quadratic programs which builds upon and weaves together the proximal point algorithm and a damped semismooth Newton method. The outer proximal regularization yields a numerically stable method, and we interpret the proximal operator as the unconstrained minimization of the primal-dual proximal augmented Lagrangian function. This allows the inner Newton scheme to exploit sparse symmetric linear solvers and multi-rank factorization updates. Moreover, the linear systems are always solvable independently from the problem data and exact linesearch can be performed. The proposed method can handle degenerate problems, provides a mechanism for infeasibility detection, and can exploit warm starting, while requiring only convexity. We present details of our open-source C implementation and report on numerical results against state-of-the-art solvers. QPDO proves to be a simple, robust, and efficient numerical method for convex quadratic programming.},
  archive      = {J_COAP},
  author       = {De Marchi, Alberto},
  doi          = {10.1007/s10589-021-00342-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {369-395},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a primal-dual newton proximal method for convex quadratic programs},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math display"><em>ρ</em></span> -regularization
subproblems: Strong duality and an eigensolver-based algorithm.
<em>COAP</em>, <em>81</em>(2), 337–368. (<a
href="https://doi.org/10.1007/s10589-021-00341-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust-region (TR) type method, based on a quadratic model such as the trust-region subproblem (TRS) and p-regularization subproblem (pRS), is arguably one of the most successful methods for unconstrained minimization. In this paper, we study a general regularized subproblem (named $$\rho$$ RS), which covers TRS and pRS as special cases. We derive a strong duality theorem for $$\rho$$ RS, and also its necessary and sufficient optimality condition under general assumptions on the regularization term. We then define the Rendl–Wolkowicz (RW) dual problem of $$\rho$$ RS, which is a maximization problem whose objective function is concave, and differentiable except possibly at two points. It is worth pointing out that our definition is based on an alternative derivation of the RW-dual problem for TRS. Then we propose an eigensolver-based algorithm for solving the RW-dual problem of $$\rho$$ RS. The algorithm is carried out by finding the smallest eigenvalue and its unit eigenvector of a certain matrix in each iteration. Finally, we present numerical results on randomly generated pRS’s, and on a new class of regularized problem that combines TRS and pRS, to illustrate our algorithm.},
  archive      = {J_COAP},
  author       = {Zeng, Liaoyuan and Pong, Ting Kei},
  doi          = {10.1007/s10589-021-00341-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {337-368},
  shortjournal = {Comput. Optim. Appl.},
  title        = {$$\rho$$ -regularization subproblems: Strong duality and an eigensolver-based algorithm},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding a hamiltonian cycle by finding the global minimizer
of a linearly constrained problem. <em>COAP</em>, <em>81</em>(1),
309–336. (<a href="https://doi.org/10.1007/s10589-021-00326-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that a global minimizer of a smooth determinant of a matrix function corresponds to the largest cycle of a graph. When it exists, this is a Hamiltonian cycle. Finding global minimizers even of a smooth function is a challenge. The difficulty is often exacerbated by the existence of many global minimizers. One may think this would help, but in the case of Hamiltonian cycles the ratio of the number of global minimizers to the number of local minimizers is typically astronomically small. There are various equivalent forms of the problem and here we report on two. Although the focus is on finding Hamiltonian cycles, and this has an interest in and of itself, this is just a proxy for a class of problems that have discrete variables. The solution of relaxations of these problems is typically at a degenerate vertex, and in the neighborhood of the solution the Hessian is indefinite. The form of the Hamiltonian cycle problem we address has the virtue of being an ideal test problem for algorithms designed for discrete nonlinear problems in general. It is easy to generate problems of varying size and varying character, and they have the advantage of being able to determine if a global solution has been found. A feature of many discrete problems is that there are many solutions. For example, in the frequency assignment problem any permutation of a solution is also a solution. A consequence is that a common characteristic of the relaxed problems is that they have large numbers of global minimizers and even larger numbers of both local minimizers, and saddle points whose reduced Hessian has only a single negative eigenvalue. Efficient algorithms that seek to find global minimizers for this type of problem are described. Results using BONMIN, a solver for nonlinear problems with continuous and discrete variables, are also included.},
  archive      = {J_COAP},
  author       = {Haythorpe, Michael and Murray, Walter},
  doi          = {10.1007/s10589-021-00326-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {309-336},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding a hamiltonian cycle by finding the global minimizer of a linearly constrained problem},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding best approximation pairs for two intersections of
closed convex sets. <em>COAP</em>, <em>81</em>(1), 289–308. (<a
href="https://doi.org/10.1007/s10589-021-00324-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding a best approximation pair of two sets, which in turn generalizes the well known convex feasibility problem, has a long history that dates back to work by Cheney and Goldstein in 1959. In 2018, Aharoni, Censor, and Jiang revisited this problem and proposed an algorithm that can be used when the two sets are finite intersections of halfspaces. Motivated by their work, we present alternative algorithms that utilize projection and proximity operators. Our modeling framework is able to accommodate even convex sets. Numerical experiments indicate that these methods are competitive and sometimes superior to the one proposed by Aharoni et al.},
  archive      = {J_COAP},
  author       = {Bauschke, Heinz H. and Singh, Shambhavi and Wang, Xianfu},
  doi          = {10.1007/s10589-021-00324-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {289-308},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding best approximation pairs for two intersections of closed convex sets},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A piecewise conservative method for unconstrained convex
optimization. <em>COAP</em>, <em>81</em>(1), 251–288. (<a
href="https://doi.org/10.1007/s10589-021-00332-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a continuous-time optimization method based on a dynamical system, where a massive particle starting at rest moves in the conservative force field generated by the objective function, without any kind of friction. We formulate a restart criterion based on the mean dissipation of the kinetic energy, and we prove a global convergence result for strongly-convex functions. Using the Symplectic Euler discretization scheme, we obtain an iterative optimization algorithm. We have considered a discrete mean dissipation restart scheme, but we have also introduced a new restart procedure based on ensuring at each iteration a decrease of the objective function greater than the one achieved by a step of the classical gradient method. For the discrete conservative algorithm, this last restart criterion is capable of guaranteeing a qualitative convergence result. We apply the same restart scheme to the Nesterov Accelerated Gradient (NAG-C), and we use this restarted NAG-C as benchmark in the numerical experiments. In the smooth convex problems considered, our method shows a faster convergence rate than the restarted NAG-C. We propose an extension of our discrete conservative algorithm to composite optimization: in the numerical tests involving non-strongly convex functions with $$\ell ^1$$ -regularization, it has better performances than the well known efficient Fast Iterative Shrinkage-Thresholding Algorithm, accelerated with an adaptive restart scheme.},
  archive      = {J_COAP},
  author       = {Scagliotti, A. and Colli Franzone, P.},
  doi          = {10.1007/s10589-021-00332-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {251-288},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A piecewise conservative method for unconstrained convex optimization},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive partition-based SDDP algorithms for multistage
stochastic linear programming with fixed recourse. <em>COAP</em>,
<em>81</em>(1), 201–250. (<a
href="https://doi.org/10.1007/s10589-021-00323-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we extend the adaptive partition-based approach for solving two-stage stochastic programs with fixed recourse matrix and fixed cost vector to the multistage stochastic programming setting where the stochastic process is assumed to be stage-wise independent. The proposed algorithms integrate the adaptive partition-based strategy with a popular approach for solving multistage stochastic programs, the stochastic dual dynamic programming (SDDP) algorithm, according to two main strategies. These two strategies are distinct from each other in the manner by which they refine the partitions during the solution process. In particular, we propose a refinement outside SDDP strategy whereby we iteratively solve a coarse scenario tree induced by the partitions, and refine the partitions in a separate step outside of SDDP, only when necessary. We also propose a refinement within SDDP strategy where the partitions are refined in conjunction with the machinery of the SDDP algorithm. We then use, within the two different refinement schemes, different tree-traversal strategies which allow us to have some control over the size of the partitions. We performed numerical experiments on a hydro-thermal power generation planning problem. Numerical results show the effectiveness of the proposed algorithms that use the refinement outside SDDP strategy in comparison to the standard SDDP algorithm and algorithms that use the refinement within SDDP strategy.},
  archive      = {J_COAP},
  author       = {Siddig, Murwan and Song, Yongjia},
  doi          = {10.1007/s10589-021-00323-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {201-250},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Adaptive partition-based SDDP algorithms for multistage stochastic linear programming with fixed recourse},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Expected complexity analysis of stochastic direct-search.
<em>COAP</em>, <em>81</em>(1), 179–200. (<a
href="https://doi.org/10.1007/s10589-021-00329-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents the convergence rate analysis of stochastic variants of the broad class of direct-search methods of directional type. It introduces an algorithm designed to optimize differentiable objective functions f whose values can only be computed through a stochastically noisy blackbox. The proposed stochastic directional direct-search (SDDS) algorithm accepts new iterates by imposing a sufficient decrease condition on so called probabilistic estimates of the corresponding unavailable objective function values. The accuracy of such estimates is required to hold with a sufficiently large but fixed probability $$\beta$$ . The analysis of this method utilizes an existing supermartingale-based framework proposed for the convergence rates analysis of stochastic optimization methods that use adaptive step sizes. It aims to show that the expected number of iterations required to drive the norm of the gradient of f below a given threshold $$\epsilon$$ is bounded in $${\mathcal {O}}\left( \epsilon ^{\frac{-p}{\min (p-1,1)}}/(2\beta -1)\right)$$ with $$p&gt;1$$ . Unlike prior analysis using the same aforementioned framework such as those of stochastic trust-region methods and stochastic line search methods, SDDS does not use any gradient information to find descent directions. However, its convergence rate is similar to those of both latter methods with a dependence on $$\epsilon$$ that also matches that of the broad class of deterministic directional direct-search methods which accept new iterates by imposing a sufficient decrease condition.},
  archive      = {J_COAP},
  author       = {Dzahini, Kwassi Joseph},
  doi          = {10.1007/s10589-021-00329-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {179-200},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Expected complexity analysis of stochastic direct-search},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On r-linear convergence analysis for a class of gradient
methods. <em>COAP</em>, <em>81</em>(1), 161–177. (<a
href="https://doi.org/10.1007/s10589-021-00333-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient method is a simple optimization approach using minus gradient of the objective function as a search direction. Its efficiency highly relies on the choices of the stepsize. In this paper, the convergence behavior of a class of gradient methods, where the stepsize has an important property introduced in (Dai in Optimization 52:395–415, 2003), is analyzed. Our analysis is focused on minimization on strictly convex quadratic functions. We establish the R-linear convergence and derive an estimate for the R-factor. Specifically, if the stepsize can be expressed as a collection of Rayleigh quotient of the inverse Hessian matrix, we are able to show that these methods converge R-linearly and their R-factors are bounded above by $$1-\frac{1}{\varkappa }$$ , where $$\varkappa$$ is the associated condition number. Preliminary numerical results demonstrate the tightness of our estimate of the R-factor.},
  archive      = {J_COAP},
  author       = {Huang, Na},
  doi          = {10.1007/s10589-021-00333-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {161-177},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On R-linear convergence analysis for a class of gradient methods},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bregman primal–dual first-order method and application to
sparse semidefinite programming. <em>COAP</em>, <em>81</em>(1), 127–159.
(<a href="https://doi.org/10.1007/s10589-021-00339-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new variant of the Chambolle–Pock primal–dual algorithm with Bregman distances, analyze its convergence, and apply it to the centering problem in sparse semidefinite programming. The novelty in the method is a line search procedure for selecting suitable step sizes. The line search obviates the need for estimating the norm of the constraint matrix and the strong convexity constant of the Bregman kernel. As an application, we discuss the centering problem in large-scale semidefinite programming with sparse coefficient matrices. The logarithmic barrier function for the cone of positive semidefinite completable sparse matrices is used as the distance-generating kernel. For this distance, the complexity of evaluating the Bregman proximal operator is shown to be roughly proportional to the cost of a sparse Cholesky factorization. This is much cheaper than the standard proximal operator with Euclidean distances, which requires an eigenvalue decomposition.},
  archive      = {J_COAP},
  author       = {Jiang, Xin and Vandenberghe, Lieven},
  doi          = {10.1007/s10589-021-00339-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {127-159},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Bregman primal–dual first-order method and application to sparse semidefinite programming},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the inexact scaled gradient projection method.
<em>COAP</em>, <em>81</em>(1), 91–125. (<a
href="https://doi.org/10.1007/s10589-021-00331-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to present an inexact version of the scaled gradient projection method on a convex set, which is inexact in two sense. First, an inexact projection on the feasible set is computed, allowing for an appropriate relative error tolerance. Second, an inexact nonmonotone line search scheme is employed to compute a step size which defines the next iteration. It is shown that the proposed method has similar asymptotic convergence properties and iteration-complexity bounds as the usual scaled gradient projection method employing monotone line searches.},
  archive      = {J_COAP},
  author       = {Ferreira, O. P. and Lemes, M. and Prudente, L. F.},
  doi          = {10.1007/s10589-021-00331-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {91-125},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the inexact scaled gradient projection method},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A riemannian rank-adaptive method for low-rank matrix
completion. <em>COAP</em>, <em>81</em>(1), 67–90. (<a
href="https://doi.org/10.1007/s10589-021-00328-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank matrix completion problem can be solved by Riemannian optimization on a fixed-rank manifold. However, a drawback of the known approaches is that the rank parameter has to be fixed a priori. In this paper, we consider the optimization problem on the set of bounded-rank matrices. We propose a Riemannian rank-adaptive method, which consists of fixed-rank optimization, rank increase step and rank reduction step. We explore its performance applied to the low-rank matrix completion problem. Numerical experiments on synthetic and real-world datasets illustrate that the proposed rank-adaptive method compares favorably with state-of-the-art algorithms. In addition, it shows that one can incorporate each aspect of this rank-adaptive framework separately into existing algorithms for the purpose of improving performance.},
  archive      = {J_COAP},
  author       = {Gao, Bin and Absil, P.-A.},
  doi          = {10.1007/s10589-021-00328-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {67-90},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A riemannian rank-adaptive method for low-rank matrix completion},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sublevel moment-SOS hierarchy for polynomial optimization.
<em>COAP</em>, <em>81</em>(1), 31–66. (<a
href="https://doi.org/10.1007/s10589-021-00325-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a sublevel Moment-SOS hierarchy where each SDP relaxation can be viewed as an intermediate (or interpolation) between the d-th and $$(d+1)$$ -th order SDP relaxations of the Moment-SOS hierarchy (dense or sparse version). With the flexible choice of determining the size (level) and number (depth) of subsets in the SDP relaxation, one is able to obtain different improvements compared to the d-th order relaxation, based on the machine memory capacity. In particular, we provide numerical experiments for $$d=1$$ and various types of problems both in combinatorial optimization (Max-Cut, Mixed Integer Programming) and deep learning (robustness certification, Lipschitz constant of neural networks), where the standard Lasserre’s relaxation (or its sparse variant) is computationally intractable. In our numerical results, the lower bounds from the sublevel relaxations improve the bound from Shor’s relaxation (first order Lasserre’s relaxation) and are significantly closer to the optimal value or to the best-known lower/upper bounds.},
  archive      = {J_COAP},
  author       = {Chen, Tong and Lasserre, Jean-Bernard and Magron, Victor and Pauwels, Edouard},
  doi          = {10.1007/s10589-021-00325-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {31-66},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A sublevel moment-SOS hierarchy for polynomial optimization},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On large-scale unconstrained optimization and arbitrary
regularization. <em>COAP</em>, <em>81</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10589-021-00322-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm for large-scale unconstrained minimization that, at each iteration, minimizes, approximately, a quadratic model of the objective function plus a regularization term, not necessarily based on a norm. We prove convergence assuming only gradient continuity and complexity results assuming Lipschitz conditions. For solving the subproblems in the case of regularizations based on the 3-norm, we introduce a new method that quickly obtains the approximate solutions required by the theory. We present numerical experiments.},
  archive      = {J_COAP},
  author       = {Martínez, J. M. and Santos, L. T.},
  doi          = {10.1007/s10589-021-00322-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On large-scale unconstrained optimization and arbitrary regularization},
  volume       = {81},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
