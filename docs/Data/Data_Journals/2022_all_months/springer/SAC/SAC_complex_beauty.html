<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---110">SAC - 110</h2>
<ul>
<li><details>
<summary>
(2022). Changepoint detection in non-exchangeable data.
<em>SAC</em>, <em>32</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10176-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changepoint models typically assume the data within each segment are independent and identically distributed conditional on some parameters that change across segments. This construction may be inadequate when data are subject to local correlation patterns, often resulting in many more changepoints fitted than preferable. This article proposes a Bayesian changepoint model that relaxes the assumption of exchangeability within segments. The proposed model supposes data within a segment are m-dependent for some unknown $$m \geqslant 0$$ that may vary between segments, resulting in a model suitable for detecting clear discontinuities in data that are subject to different local temporal correlations. The approach is suited to both continuous and discrete data. A novel reversible jump Markov chain Monte Carlo algorithm is proposed to sample from the model; in particular, a detailed analysis of the parameter space is exploited to build proposals for the orders of dependence. Two applications demonstrate the benefits of the proposed model: computer network monitoring via change detection in count data, and segmentation of financial time series.},
  archive      = {J_SAC},
  author       = {Hallgren, Karl L. and Heard, Nicholas A. and Adams, Niall M.},
  doi          = {10.1007/s11222-022-10176-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Changepoint detection in non-exchangeable data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic and robust bayesian graphical models. <em>SAC</em>,
<em>32</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10177-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are widely popular for studying the conditional dependence among random variables. By encoding conditional dependence as an undirected graph, Gaussian graphical models provide interpretable representations and insightful visualizations of the relationships among variables. However, time series data present additional challenges: the graphs can evolve over time—with changes occurring at unknown time points—and the data often exhibit heavy-tailed characteristics. To address these challenges, we propose dynamic and robust Bayesian graphical models that employ state-of-the-art hidden Markov models (HMMs) to introduce dynamics in the graph and heavy-tailed multivariate t-distributions for model robustness. The HMM latent states are linked both temporally and hierarchically for greater information sharing across time and between states. The proposed methods are computationally efficient and demonstrate excellent graph estimation on simulated data with substantial improvements over non-robust graphical models. We demonstrate our proposed approach on human hand gesture tracking data, and discover edges and dynamics with well explained practical meanings.},
  archive      = {J_SAC},
  author       = {Liu, Chunshan and Kowal, Daniel R. and Vannucci, Marina},
  doi          = {10.1007/s11222-022-10177-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Dynamic and robust bayesian graphical models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic enumeration of definitive screening designs.
<em>SAC</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-022-10171-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conference designs are $$n \times k$$ matrices, $$k \le n$$ , with orthogonal columns, one zero in each column, at most one zero in each row, and $$-1$$ and $$+1$$ entries elsewhere. Conference designs with $$k=n$$ are called conference matrices. Definitive screening designs (DSDs) are constructed by folding over a conference design and adding a row vector of zeros. We propose methodology for the systematic enumeration of conference designs with a specified number of rows and columns, and thereby for the systematic enumeration of the corresponding DSDs. We demonstrate its potential by enumerating all conference designs with up to 24 rows and columns, and thus all DSDs with up to 49 runs. A large fraction of these DSDs cannot be obtained from conference matrices and is therefore new to the literature. We identify DSDs that minimize the correlation among contrast vectors of second-order effects and provide them in supplementary files.},
  archive      = {J_SAC},
  author       = {Schoen, Eric D. and Eendebak, Pieter T. and Vazquez, Alan R. and Goos, Peter},
  doi          = {10.1007/s11222-022-10171-6},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Systematic enumeration of definitive screening designs},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uniform calibration tests for forecasting systems with small
lead time. <em>SAC</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-022-10144-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long noted difficulty when assessing calibration (or reliability) of forecasting systems is that calibration, in general, is a hypothesis not about a finite dimensional parameter but about an entire functional relationship. A calibrated probability forecast for binary events for instance should equal the conditional probability of the event given the forecast, whatever the value of the forecast. A new class of tests is presented that are based on estimating the cumulative deviations from calibration. The supremum of those deviations is taken as a test statistic, and the asymptotic distribution of the test statistic is established rigorously. It turns out to be universal, provided the forecasts “look one step ahead” only, or in other words, verify at the next time step in the future. The new tests apply to various different forecasting problems and are compared with established approaches which work in a regression based framework. In comparison to those approaches, the new tests develop power against a wider class of alternatives. Numerical experiments for both artificial data as well as operational weather forecasting systems are presented, and possible extensions to longer lead times are discussed.},
  archive      = {J_SAC},
  author       = {Bröcker, Jochen},
  doi          = {10.1007/s11222-022-10144-9},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Uniform calibration tests for forecasting systems with small lead time},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parameter estimation method for multivariate binned hawkes
processes. <em>SAC</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-022-10121-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often assumed that events cannot occur simultaneously when modelling data with point processes. This raises a problem as real-world data often contains synchronous observations due to aggregation or rounding, resulting from limitations on recording capabilities and the expense of storing high volumes of precise data. In order to gain a better understanding of the relationships between processes, we consider modelling the aggregated event data using multivariate Hawkes processes, which offer a description of mutually-exciting behaviour and have found wide applications in areas including seismology and finance. Here we generalise existing methodology on parameter estimation of univariate aggregated Hawkes processes to the multivariate case using a Monte Carlo expectation–maximization (MC-EM) algorithm and through a simulation study illustrate that alternative approaches to this problem can be severely biased, with the multivariate MC-EM method outperforming them in terms of MSE in all considered cases.},
  archive      = {J_SAC},
  author       = {Shlomovich, Leigh and Cohen, Edward A. K. and Adams, Niall},
  doi          = {10.1007/s11222-022-10121-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A parameter estimation method for multivariate binned hawkes processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpolating log-determinant and trace of the powers of
matrix <span
class="math display"><strong>A</strong> + <em>t</em><strong>B</strong></span>.
<em>SAC</em>, <em>32</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10173-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop heuristic interpolation methods for the functions $$t\mapsto \log \det \left( \textbf{A} + t\textbf{B} \right) $$ and $$t\mapsto {{\,\textrm{trace}\,}}\left( (\textbf{A} + t\textbf{B})^{p} \right) $$ where the matrices $$\textbf{A}$$ and $$\textbf{B}$$ are Hermitian and positive (semi) definite and $$p$$ and $$t$$ are real variables. These functions are featured in many applications in statistics, machine learning, and computational physics. The presented interpolation functions are based on the modification of sharp bounds for these functions. We demonstrate the accuracy and performance of the proposed method with numerical examples, namely, the marginal maximum likelihood estimation for Gaussian process regression and the estimation of the regularization parameter of ridge regression with the generalized cross-validation method.},
  archive      = {J_SAC},
  author       = {Ameli, Siavash and Shadden, Shawn C.},
  doi          = {10.1007/s11222-022-10173-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Interpolating log-determinant and trace of the powers of matrix $$\textbf{A} + t\textbf{B}$$},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spline estimation of functional principal components via
manifold conjugate gradient algorithm. <em>SAC</em>, <em>32</em>(6),
1–18. (<a href="https://doi.org/10.1007/s11222-022-10175-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional principal component analysis has become the most important dimension reduction technique in functional data analysis. Based on B-spline approximation, functional principal components (FPCs) can be efficiently estimated by the expectation-maximization (EM) and the geometric restricted maximum likelihood (REML) algorithms under the strong assumption of Gaussianity on the principal component scores and observational errors. When computing the solution, the EM algorithm does not exploit the underlying geometric manifold structure, while the performance of REML is known to be unstable. In this article, we propose a conjugate gradient algorithm over the product manifold to estimate FPCs. This algorithm exploits the manifold geometry structure of the overall parameter space, thus improving its search efficiency and estimation accuracy. In addition, a distribution-free interpretation of the loss function is provided from the viewpoint of matrix Bregman divergence, which explains why the proposed method works well under general distribution settings. We also show that a roughness penalization can be easily incorporated into our algorithm with a potentially better fit. The appealing numerical performance of the proposed method is demonstrated by simulation studies and the analysis of a Type Ia supernova light curve dataset.},
  archive      = {J_SAC},
  author       = {He, Shiyuan and Ye, Hanxuan and He, Kejun},
  doi          = {10.1007/s11222-022-10175-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Spline estimation of functional principal components via manifold conjugate gradient algorithm},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A phase transition for finding needles in nonlinear
haystacks with LASSO artificial neural networks. <em>SAC</em>,
<em>32</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10169-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fit sparse linear associations, a LASSO sparsity inducing penalty with a single hyperparameter provably allows to recover the important features (needles) with high probability in certain regimes even if the sample size is smaller than the dimension of the input vector (haystack). More recently learners known as artificial neural networks (ANN) have shown great successes in many machine learning tasks, in particular fitting nonlinear associations. Small learning rate, stochastic gradient descent algorithm and large training set help to cope with the explosion in the number of parameters present in deep neural networks. Yet few ANN learners have been developed and studied to find needles in nonlinear haystacks. Driven by a single hyperparameter, our ANN learner, like for sparse linear associations, exhibits a phase transition in the probability of retrieving the needles, which we do not observe with other ANN learners. To select our penalty parameter, we generalize the universal threshold of Donoho and Johnstone (Biometrika 81(3):425–455, 1994) which is a better rule than the conservative (too many false detections) and expensive cross-validation. In the spirit of simulated annealing, we propose a warm-start sparsity inducing algorithm to solve the high-dimensional, non-convex and non-differentiable optimization problem. We perform simulated and real data Monte Carlo experiments to quantify the effectiveness of our approach.},
  archive      = {J_SAC},
  author       = {Ma, Xiaoyu and Sardy, Sylvain and Hengartner, Nick and Bobenko, Nikolai and Lin, Yen Ting},
  doi          = {10.1007/s11222-022-10169-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A phase transition for finding needles in nonlinear haystacks with LASSO artificial neural networks},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic zig-zag sampling in practice. <em>SAC</em>,
<em>32</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10142-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel Monte Carlo methods to generate samples from a target distribution, such as a posterior from a Bayesian analysis, have rapidly expanded in the past decade. Algorithms based on Piecewise Deterministic Markov Processes (PDMPs), non-reversible continuous-time processes, are developing into their own research branch, thanks their important properties (e.g., super-efficiency). Nevertheless, practice has not caught up with the theory in this field, and the use of PDMPs to solve applied problems is not widespread. This might be due, firstly, to several implementational challenges that PDMP-based samplers present with and, secondly, to the lack of papers that showcase the methods and implementations in applied settings. Here, we address both these issues using one of the most promising PDMPs, the Zig-Zag sampler, as an archetypal example. After an explanation of the key elements of the Zig-Zag sampler, its implementation challenges are exposed and addressed. Specifically, the formulation of an algorithm that draws samples from a target distribution of interest is provided. Notably, the only requirement of the algorithm is a closed-form differentiable function to evaluate the log-target density of interest, and, unlike previous implementations, no further information on the target is needed. The performance of the algorithm is evaluated against canonical Hamiltonian Monte Carlo, and it is proven to be competitive, in simulation and real-data settings. Lastly, we demonstrate that the super-efficiency property, i.e. the ability to draw one independent sample at a lesser cost than evaluating the likelihood of all the data, can be obtained in practice.},
  archive      = {J_SAC},
  author       = {Corbella, Alice and Spencer, Simon E. F. and Roberts, Gareth O.},
  doi          = {10.1007/s11222-022-10142-x},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Automatic zig-zag sampling in practice},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math display"><em>π</em></span> VAE: A
stochastic process prior for bayesian deep learning with MCMC.
<em>SAC</em>, <em>32</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10151-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic processes provide a mathematically elegant way to model complex data. In theory, they provide flexible priors over function classes that can encode a wide range of interesting assumptions. However, in practice efficient inference by optimisation or marginalisation is difficult, a problem further exacerbated with big data and high dimensional input spaces. We propose a novel variational autoencoder (VAE) called the prior encoding variational autoencoder ( $$\pi $$ VAE). $$\pi $$ VAE is a new continuous stochastic process. We use $$\pi $$ VAE to learn low dimensional embeddings of function classes by combining a trainable feature mapping with generative model using a VAE. We show that our framework can accurately learn expressive function classes such as Gaussian processes, but also properties of functions such as their integrals. For popular tasks, such as spatial interpolation, $$\pi $$ VAE achieves state-of-the-art performance both in terms of accuracy and computational efficiency. Perhaps most usefully, we demonstrate an elegant and scalable means of performing fully Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.},
  archive      = {J_SAC},
  author       = {Mishra, Swapnil and Flaxman, Seth and Berah, Tresnia and Zhu, Harrison and Pakkanen, Mikko and Bhatt, Samir},
  doi          = {10.1007/s11222-022-10151-w},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {$$\pi $$ VAE: A stochastic process prior for bayesian deep learning with MCMC},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limit theory and robust evaluation methods for the extremal
properties of GARCH(p, q) processes. <em>SAC</em>, <em>32</em>(6), 1–22.
(<a href="https://doi.org/10.1007/s11222-022-10164-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized autoregressive conditionally heteroskedastic (GARCH) processes are widely used for modelling financial returns, with their extremal properties being of interest for market risk management. For GARCH( $$p,q$$ ) processes with $$\max (p,q) = 1$$ all extremal features have been fully characterised, but when $$\max (p,q)\ge 2$$ much remains to be found. Previous research has identified that both marginal and dependence extremal features of strictly stationary GARCH( $$p,q$$ ) processes are determined by a multivariate regular variation property and tail processes. Currently there are no reliable methods for evaluating these characterisations, or even assessing the stationarity, for the classes of GARCH( $$p,q$$ ) processes that are used in practice, i.e., with unbounded and asymmetric innovations. By developing a mixture of new limit theory and particle filtering algorithms for fixed point distributions we produce novel and robust evaluation methods for all extremal features for all GARCH( $$p,q$$ ) processes, including ARCH and IGARCH processes. We investigate our methods’ performance when evaluating the marginal tail index, the extremogram and the extremal index, the latter two being measures of temporal dependence.},
  archive      = {J_SAC},
  author       = {Laurini, Fabrizio and Fearnhead, Paul and Tawn, Jonathan},
  doi          = {10.1007/s11222-022-10164-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Limit theory and robust evaluation methods for the extremal properties of GARCH(p, q) processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based algorithms for phase-type distributions.
<em>SAC</em>, <em>32</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10174-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase-type distributions model the time until absorption in continuous or discrete-time Markov chains on a finite state space. The multivariate phase-type distributions have diverse and important applications by modeling rewards accumulated at visited states. However, even moderately sized state spaces make the traditional matrix-based equations computationally infeasible. State spaces of phase-type distributions are often large but sparse, with only a few transitions from a state. This sparseness makes a graph-based representation of the phase-type distribution more natural and efficient than the traditional matrix-based representation. In this paper, we develop graph-based algorithms for analyzing phase-type distributions. In addition to algorithms for state space construction, reward transformation, and moments calculation, we give algorithms for the marginal distribution functions of multivariate phase-type distributions and for the state probability vector of the underlying Markov chains of both time-homogeneous and time-inhomogeneous phase-type distributions. The algorithms are available as a numerically stable and memory-efficient open source software package written in C named ptdalgorithms. This library exposes all methods in the programming languages C and R. We compare the running time of ptdalgorithms to the fastest tools using a traditional matrix-based formulation. This comparison includes the computation of the probability distribution, which is usually computed by exponentiation of the sub-intensity or sub-transition matrix. We also compare time spent calculating the moments of (multivariate) phase-type distributions usually defined by inversion of the same matrices. The numerical results of our graph-based and traditional matrix-based methods are identical, and our graph-based algorithms are often orders of magnitudes faster. Finally, we demonstrate with a classic problem from population genetics how ptdalgorithms serves as a much faster, simpler, and completely general modeling alternative.},
  archive      = {J_SAC},
  author       = {Røikjer, Tobias and Hobolth, Asger and Munch, Kasper},
  doi          = {10.1007/s11222-022-10174-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Graph-based algorithms for phase-type distributions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affine-mapping based variational ensemble kalman filter.
<em>SAC</em>, <em>32</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10156-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an affine-mapping based variational ensemble Kalman filter for sequential Bayesian filtering problems with generic observation models. Specifically, the proposed method is formulated as to construct an affine mapping from the prior ensemble to the posterior one, and the affine mapping is computed via a variational Bayesian formulation, i.e., by minimizing the Kullback–Leibler divergence between the transformed distribution through the affine mapping and the actual posterior. Some theoretical properties of resulting optimization problem are studied and a gradient descent scheme is proposed to solve the resulting optimization problem. With numerical examples we demonstrate that the method has competitive performance against existing methods.},
  archive      = {J_SAC},
  author       = {Wen, Linjie and Li, Jinglai},
  doi          = {10.1007/s11222-022-10156-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Affine-mapping based variational ensemble kalman filter},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representative random sampling: An empirical evaluation of a
novel bin stratification method for model performance estimation.
<em>SAC</em>, <em>32</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s11222-022-10138-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional cancer data can be burdensome to analyze, with complex relationships between molecular measurements, clinical diagnostics, and treatment outcomes. Data-driven computational approaches may be key to identifying relationships with potential clinical or research use. To this end, reliable comparison of feature engineering approaches in their ability to support machine learning survival modeling is crucial. With the limited number of cases often present in multi-omics datasets (“big p, little n,” or many features, few subjects), a resampling approach such as cross validation (CV) would provide robust model performance estimates at the cost of flexibility in intermediate assessments and exploration in feature engineering approaches. A holdout (HO) estimation approach, however, would permit this flexibility at the expense of reliability. To provide more reliable HO-based model performance estimates, we propose a novel sampling procedure: representative random sampling (RRS). RRS is a special case of continuous bin stratification which minimizes significant relationships between random HO groupings (or CV folds) and a continuous outcome. Monte Carlo simulations used to evaluate RRS on synthetic molecular data indicated that RRS-based HO (RRHO) yields statistically significant reductions in error and bias when compared with standard HO. Similarly, more consistent reductions are observed with RRS-based CV. While resampling approaches are the ideal choice for performance estimation with limited data, RRHO can enable more reliable exploratory feature engineering than standard HO.},
  archive      = {J_SAC},
  author       = {Rendleman, Michael C. and Smith, Brian J. and Canahuate, Guadalupe and Braun, Terry A. and Buatti, John M. and Casavant, Thomas L.},
  doi          = {10.1007/s11222-022-10138-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Representative random sampling: An empirical evaluation of a novel bin stratification method for model performance estimation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust approach for comparing two dependent normal
populations through wald-type tests based on rényi’s pseudodistance
estimators. <em>SAC</em>, <em>32</em>(6), 1–34. (<a
href="https://doi.org/10.1007/s11222-022-10162-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the two seminal papers by Fisher (Biometrika 10:507–521, 1915; Metron 1:1–32, 1921) were published, the test under a fixed value correlation coefficient null hypothesis for the bivariate normal distribution constitutes an important statistical problem. In the framework of asymptotic robust statistics, it remains being a topic of great interest to be investigated. For this and other tests, focused on paired correlated normal random samples, Rényi’s pseudodistance estimators are proposed, their asymptotic distribution is established and an iterative algorithm is provided for their computation. From them the Wald-type test statistics are constructed for different problems of interest and their influence function is theoretically studied. For testing null correlation in different contexts, an extensive simulation study and two real data based examples support the robust properties of our proposal.},
  archive      = {J_SAC},
  author       = {Castilla, Elena and Jaenada, María and Martín, Nirian and Pardo, Leandro},
  doi          = {10.1007/s11222-022-10162-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-34},
  shortjournal = {Stat. Comput.},
  title        = {Robust approach for comparing two dependent normal populations through wald-type tests based on rényi’s pseudodistance estimators},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limitations of the wasserstein MDE for univariate data.
<em>SAC</em>, <em>32</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s11222-022-10146-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum Kolmogorov and Wasserstein distance estimates, $$\hat{\theta }_{MKD}$$ and $$\hat{\theta }_{MWD},$$ respectively, of model parameter, $$\theta (\in \Theta ),$$ are empirically compared, obtained assuming the model is intractable. For the Cauchy and Lognormal models, simulations indicate both estimates have expected values nearly $$\theta ,$$ but $$\hat{\theta }_{MKD}$$ has in all repetitions of the experiments smaller SD than $$\hat{\theta }_{MWD},$$ and $$\hat{\theta }_{MKD}$$ ’s relative efficiency with respect to $$\hat{\theta }_{MWD}$$ improves as the sample size, n,  increases. The minimum expected Kolmogorov distance estimate, $$\hat{\theta }_{MEKD},$$ has eventually bias and SD both smaller than the corresponding Wasserstein estimate, $$\hat{\theta }_{MEWD},$$ and $$\hat{\theta }_{MEKD}$$ ’s relative efficiency improves as n increases. These results hold also for stable models with stability index $$\alpha =.5$$ and $$\alpha =1.1.$$ For the Uniform and the Normal models the estimates have similar performance. The disturbing empirical findings for $$\hat{\theta }_{MWD}$$ are due to the unboudedness and non-robustness of the Wasserstein distance and the heavy tails of the underlying univariate models.Theoretical confirmation is provided for stable models with $$1&lt;\alpha &lt;2,$$ which have finite first moment. Similar results are expected to hold for multivariate heavy tail models. Combined with existing results in the literature, the findings do not support the use of Wasserstein distance in statistical inference, especially for intractable and Black Box models with unverifiable heavy tails.},
  archive      = {J_SAC},
  author       = {Yatracos, Yannis G.},
  doi          = {10.1007/s11222-022-10146-7},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Limitations of the wasserstein MDE for univariate data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient reduced-rank methods for gaussian processes with
eigenfunction expansions. <em>SAC</em>, <em>32</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10124-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a reduced-rank algorithm for Gaussian process regression. Our numerical scheme converts a Gaussian process on a user-specified interval to its Karhunen–Loève expansion, the $$L^2$$ -optimal reduced-rank representation. Numerical evaluation of the Karhunen–Loève expansion is performed once during precomputation and involves computing a numerical eigendecomposition of an integral operator whose kernel is the covariance function of the Gaussian process. The Karhunen–Loève expansion is independent of observed data and depends only on the covariance kernel and the size of the interval on which the Gaussian process is defined. The scheme of this paper does not require translation invariance of the covariance kernel. We also introduce a class of fast algorithms for Bayesian fitting of hyperparameters and demonstrate the performance of our algorithms with numerical experiments in one and two dimensions. Extensions to higher dimensions are mathematically straightforward but suffer from the standard curses of high dimensions.},
  archive      = {J_SAC},
  author       = {Greengard, Philip and O’Neil, Michael},
  doi          = {10.1007/s11222-022-10124-z},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Efficient reduced-rank methods for gaussian processes with eigenfunction expansions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistic selection and MCMC for differentially private
bayesian estimation. <em>SAC</em>, <em>32</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10129-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns differentially private Bayesian estimation of the parameters of a population distribution, when a noisy statistic of a sample from that population is shared to provide differential privacy. This work mainly addresses two problems. (1) What statistics of the sample should be shared privately? For this question, we promote using the Fisher information. We find out that the statistic that is most informative in a non-privacy setting may not be the optimal choice under the privacy restrictions. We provide several examples to support that point. We consider several types of data sharing settings and propose several Monte Carlo-based numerical estimation methods for calculating the Fisher information for those settings. The second question concerns inference: (2) Based on the shared statistics, how could we perform effective Bayesian inference? We propose several Markov chain Monte Carlo (MCMC) algorithms for sampling from the posterior distribution of the parameter given the noisy statistic. The proposed MCMC algorithms can be preferred over one another depending on the problem. For example, when the shared statistic is additive and added Gaussian noise, a simple Metropolis-Hasting algorithm that utilises the central limit theorem is a decent choice. We propose more advanced MCMC algorithms for several other cases of practical relevance. Our numerical examples involve comparing several candidate statistics to be shared privately. For each statistic, we perform Bayesian estimation based on the posterior distribution conditional on the privatised version of that statistic. We demonstrate that the relative performance of a statistic, in terms of the mean squared error of the Bayesian estimator based on the corresponding privatised statistic, is adequately predicted by the Fisher information of the privatised statistic.},
  archive      = {J_SAC},
  author       = {Alparslan, Barış and Yıldırım, Sinan},
  doi          = {10.1007/s11222-022-10129-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Statistic selection and MCMC for differentially private bayesian estimation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Density deconvolution under a k-monotonicity constraint.
<em>SAC</em>, <em>32</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10150-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood estimation of a k-monotone density which can be represented as a scale mixture of beta densities with parameters 1 and k is well-studied when no measurement error is present. In this paper, we further study the maximum likelihood method of density deconvolution under a k-monotonicity constraint. Using the mixture representation of a k-monotone density straightforwardly is prevented due to the likelihood unboundedness problem. To counter the problem, a reparameterization trick is applied by creating a scalar tuning parameter. When the tuning parameter is known, we establish the identifiability of the reparameterized mixture model for the observed data and the consistency of the maximum likelihood estimator of a k-monotone density in the presence of measurement error. We also provide a method to choose the tuning parameter. Numerical studies are used to illustrate the proposed estimation and selection procedure.},
  archive      = {J_SAC},
  author       = {Chee, Chew-Seng and Seo, Byungtae},
  doi          = {10.1007/s11222-022-10150-x},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Density deconvolution under a k-monotonicity constraint},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal designs for dose-escalation trials and individual
allocations in cohorts. <em>SAC</em>, <em>32</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10158-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose escalation trials are crucial in the development of new pharmaceutical products to optimize the amount of drug administered while avoiding undesirable side effects. We adopt the framework established by Bailey (Stat Med 28(30):3721–3738, 2009. https://doi.org/10.1002/sim.3646 ) where the individuals are grouped into cohorts, to the subjects in which the placebo or previously defined doses are administered and responses measured. Successive cohorts allow testing higher doses of drug if negative responses have not been observed in earlier cohorts. We propose Mixed Integer Nonlinear Programming formulations for systematically computing optimal experimental designs for dose escalation. We demonstrate its application with i. different optimality criteria; ii. standard and extended designs; and iii. non-constrained (or traditional), strict halving and uniform halving designs. Additionally, we address the allocation of the individuals in a cohort considering previously known prognostic factors. To handle the problem we propose i. an enumerative algorithm; and ii. a Mixed Integer Nonlinear Programming formulation. We demonstrate the application of the enumeration scheme for allocating individuals on an individual arrival basis, and of the latter formulation for allocation on a within cohort basis.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Oliveira, Nuno M. C.},
  doi          = {10.1007/s11222-022-10158-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Optimal designs for dose-escalation trials and individual allocations in cohorts},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantile regression feature selection and estimation with
grouped variables using huber approximation. <em>SAC</em>,
<em>32</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10135-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers model selection and estimation for quantile regression with a known group structure in the predictors. For the median case the model is estimated by minimizing a penalized objective function with Huber loss and the group lasso penalty. While, for other quantiles an M-quantile approach, an asymmetric version of Huber loss, is used which approximates the standard quantile loss function. This approximation allows for efficient implementation of algorithms which rely on a differentiable loss function. Rates of convergence are provided which demonstrate the potential advantages of using the group penalty and that bias from the Huber-type approximation vanishes asymptotically. An efficient algorithm is discussed, which provides fast and accurate estimation for quantile regression models. Simulation and empirical results are provided to demonstrate the effectiveness of the proposed algorithm and support the theoretical results.},
  archive      = {J_SAC},
  author       = {Sherwood, Ben and Li, Shaobo},
  doi          = {10.1007/s11222-022-10135-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Quantile regression feature selection and estimation with grouped variables using huber approximation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kryging: Geostatistical analysis of large-scale datasets
using krylov subspace methods. <em>SAC</em>, <em>32</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10104-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing massive spatial datasets using a Gaussian process model poses computational challenges. This is a problem prevailing heavily in applications such as environmental modeling, ecology, forestry and environmental health. We present a novel approximate inference methodology that uses profile likelihood and Krylov subspace methods to estimate the spatial covariance parameters and makes spatial predictions with uncertainty quantification for point-referenced spatial data. “Kryging” combines Kriging and Krylov subspace methods and applies for both observations on regular grid and irregularly spaced observations, and for any Gaussian process with a stationary isotropic (and certain geometrically anisotropic) covariance function, including the popular Matérn  covariance family. We make use of the block Toeplitz structure with Toeplitz blocks of the covariance matrix and use fast Fourier transform methods to bypass the computational and memory bottlenecks of approximating log-determinant and matrix-vector products. We perform extensive simulation studies to show the effectiveness of our model by varying sample sizes, spatial parameter values and sampling designs. A real data application is also performed on a dataset consisting of land surface temperature readings taken by the MODIS satellite. Compared to existing methods, the proposed method performs satisfactorily with much less computation time and better scalability.},
  archive      = {J_SAC},
  author       = {Majumder, Suman and Guan, Yawen and Reich, Brian J. and Saibaba, Arvind K.},
  doi          = {10.1007/s11222-022-10104-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Kryging: Geostatistical analysis of large-scale datasets using krylov subspace methods},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating directed networks with predetermined
assortativity measures. <em>SAC</em>, <em>32</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10161-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assortativity coefficients are important metrics to analyze both directed and undirected networks. In general, it is not guaranteed that the fitted model will always agree with the assortativity coefficients in the given network, and the structure of directed networks is more complicated than the undirected ones. Therefore, we provide a remedy by proposing a degree-preserving rewiring algorithm, called DiDPR, for generating directed networks with given directed assortativity coefficients. We construct the joint edge distribution of the target network by accounting for the four directed assortativity coefficients simultaneously, provided that they are attainable, and obtain the desired network by solving a convex optimization problem. Our algorithm also helps check the attainability of the given assortativity coefficients. We assess the performance of the proposed algorithm by simulation studies with focus on two different network models, namely Erdös–Rényi and preferential attachment random networks. We then apply the algorithm to a Facebook wall post network as a real data example. The codes for implementing our algorithm are publicly available in R package wdnet (Yuan et al. in wdnet: Weighted and Directed Networks, University of Connecticut, R package version 0.0.5, https://CRAN.R-project.org/package=wdnet , 2022).},
  archive      = {J_SAC},
  author       = {Wang, Tiandong and Yan, Jun and Yuan, Yelie and Zhang, Panpan},
  doi          = {10.1007/s11222-022-10161-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Generating directed networks with predetermined assortativity measures},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A random persistence diagram generator. <em>SAC</em>,
<em>32</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10141-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological data analysis (TDA) studies the shape patterns of data. Persistent homology is a widely used method in TDA that summarizes homological features of data at multiple scales and stores them in persistence diagrams (PDs). In this paper, we propose a random persistence diagram generator (RPDG) method that generates a sequence of random PDs from the ones produced by the data. RPDG is underpinned by a model based on pairwise interacting point processes and a reversible jump Markov chain Monte Carlo (RJ-MCMC) algorithm. A first example, which is based on a synthetic dataset, demonstrates the efficacy of RPDG and provides a comparison with another method for sampling PDs. A second example demonstrates the utility of RPDG to solve a materials science problem given a real dataset of small sample size.},
  archive      = {J_SAC},
  author       = {Papamarkou, Theodore and Nasrin, Farzana and Lawson, Austin and Gong, Na and Rios, Orlando and Maroulas, Vasileios},
  doi          = {10.1007/s11222-022-10141-y},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A random persistence diagram generator},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatially relaxed inference on high-dimensional linear
models. <em>SAC</em>, <em>32</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10139-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the inference problem for high-dimensional linear models, when covariates have an underlying spatial organization reflected in their correlation. A typical example of such a setting is high-resolution imaging, in which neighboring pixels are usually very similar. Accurate point and confidence intervals estimation is not possible in this context with many more covariates than samples, furthermore with high correlation between covariates. This calls for a reformulation of the statistical inference problem that takes into account the underlying spatial structure: if covariates are locally correlated, it is acceptable to detect them up to a given spatial uncertainty. We thus propose to rely on the $$\delta $$ -FWER, that is, the probability of making a false discovery at a distance greater than $$\delta $$ from any true positive. With this target measure in mind, we study the properties of ensembled clustered inference algorithms which combine three techniques: spatially constrained clustering, statistical inference, and ensembling to aggregate several clustered inference solutions. We show that ensembled clustered inference algorithms control the $$\delta $$ -FWER under standard assumptions, for $$\delta $$ equal to the largest cluster diameter. We complement the theoretical analysis with empirical results, demonstrating accurate $$\delta $$ -FWER control and decent power achieved by such inference algorithms.},
  archive      = {J_SAC},
  author       = {Chevalier, Jérôme-Alexis and Nguyen, Tuan-Binh and Thirion, Bertrand and Salmon, Joseph},
  doi          = {10.1007/s11222-022-10139-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Spatially relaxed inference on high-dimensional linear models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate laplace importance sampling for the estimation
of expected shannon information gain in high-dimensional bayesian design
for nonlinear models. <em>SAC</em>, <em>32</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10159-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in Bayesian optimal design is to approximate the expected utility function in an accurate and computationally efficient manner. We focus on Shannon information gain, one of the most widely used utilities when the experimental goal is parameter inference. We compare the performance of various methods for approximating expected Shannon information gain in common nonlinear models from the statistics literature, with a particular emphasis on Laplace importance sampling (LIS) and approximate Laplace importance sampling (ALIS), a new method that aims to reduce the computational cost of LIS. Specifically, in order to centre the importance distributions LIS requires computation of the posterior mode for each of a large number of simulated possibilities for the response vector. ALIS substantially reduces the amount of numerical optimization that is required, in some cases eliminating all optimization, by centering the importance distributions on the data-generating parameter values wherever possible. Both methods are thoroughly compared with existing approximations including Double Loop Monte Carlo, nested importance sampling, and Laplace approximation. It is found that LIS and ALIS both give an efficient trade-off between mean squared error and computational cost for utility estimation, and ALIS can be up to 70\% cheaper than LIS. Usually ALIS gives an approximation that is cheaper but less accurate than LIS, while still being efficient, giving a useful addition to the suite of efficient methods. However, we observed one case where ALIS is both cheaper and more accurate. In addition, for the first time we show that LIS and ALIS yield superior designs to existing methods in problems with large numbers of model parameters when combined with the approximate co-ordinate exchange algorithm for design optimization.},
  archive      = {J_SAC},
  author       = {Englezou, Yiolanda and Waite, Timothy W. and Woods, David C.},
  doi          = {10.1007/s11222-022-10159-2},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Approximate laplace importance sampling for the estimation of expected shannon information gain in high-dimensional bayesian design for nonlinear models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint estimation approach to sparse additive ordinary
differential equations. <em>SAC</em>, <em>32</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10117-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinary differential equations (ODEs) are widely used to characterize the dynamics of complex systems in real applications. In this article, we propose a novel joint estimation approach for generalized sparse additive ODEs where observations are allowed to be non-Gaussian. The new method is unified with existing collocation methods by considering the likelihood, ODE fidelity and sparse regularization simultaneously. We design a block coordinate descent algorithm for optimizing the non-convex and non-differentiable objective function. The global convergence of the algorithm is established. The simulation study and two applications demonstrate the superior performance of the proposed method in estimation and improved performance of identifying the sparse structure.},
  archive      = {J_SAC},
  author       = {Zhang, Nan and Nanshan, Muye and Cao, Jiguo},
  doi          = {10.1007/s11222-022-10117-y},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A joint estimation approach to sparse additive ordinary differential equations},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing unconstrained parametrization methods for return
covariance matrix prediction. <em>SAC</em>, <em>32</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10157-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting covariance matrices is a difficult task in many research fields since the predicted matrices should be at least positive semidefinite. This problem can be overcome by including constraints in the predictive model or through a parametrization of the matrices to be predicted. In this paper, we focus on the latter approach in a financial application and analyse four parametrizations of the covariance matrices of asset returns. The aim of the manuscript is to understand if the parametrizations of the covariance matrices exhibit differences in terms of predictive accuracy. To this end, we critically analyse their predictive performance through both a Monte Carlo simulation and an empirical application with daily and weekly realized covariance matrices of stock assets. Our findings highlight that the Cholesky decomposition and the parametrization recently introduced by Archakov and Hansen are the overall best-performing methods in terms of forecasting accuracy.},
  archive      = {J_SAC},
  author       = {Bucci, Andrea and Ippoliti, Luigi and Valentini, Pasquale},
  doi          = {10.1007/s11222-022-10157-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Comparing unconstrained parametrization methods for return covariance matrix prediction},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Summary statistics and discrepancy measures for approximate
bayesian computation via surrogate posteriors. <em>SAC</em>,
<em>32</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10155-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen, and thus their choice can affect the quality of approximations. The choice between discrepancies is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated: a standard L $$_2$$ distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to a functional summary statistics setting and can also be used as an alternative to sample-based approaches. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.},
  archive      = {J_SAC},
  author       = {Forbes, Florence and Nguyen, Hien Duy and Nguyen, TrungTin and Arbel, Julyan},
  doi          = {10.1007/s11222-022-10155-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Summary statistics and discrepancy measures for approximate bayesian computation via surrogate posteriors},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixture of multivariate gaussian processes for
classification of irregularly sampled satellite image time-series.
<em>SAC</em>, <em>32</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10145-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of irregularly sampled Satellite image time-series (SITS) is investigated in this paper. A multivariate Gaussian process mixture model is proposed to address the irregular sampling, the multivariate nature of the time-series and the scalability to large data-sets. The spectral and temporal correlation is handled using a Kronecker structure on the covariance operator of the Gaussian process. The multivariate Gaussian process mixture model allows both for the classification of time-series and the imputation of missing values. Experimental results on simulated and real SITS data illustrate the importance of taking into account the spectral correlation to ensure a good behavior in terms of classification accuracy and reconstruction errors.},
  archive      = {J_SAC},
  author       = {Constantin, Alexandre and Fauvel, Mathieu and Girard, Stéphane},
  doi          = {10.1007/s11222-022-10145-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Mixture of multivariate gaussian processes for classification of irregularly sampled satellite image time-series},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic differentiation of box-cox transformations with
application to random effects models for continuous non-normal response.
<em>SAC</em>, <em>32</em>(5), 1–7. (<a
href="https://doi.org/10.1007/s11222-022-10160-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Box-Cox transformation generates a family of distributions used in the modelling of continuous, non-Normal data. When data are grouped, the unknown parameters of the distribution are modelled as depending on random effects, and inference by Laplace-approximate marginal likelihood involves challenging computations that are aided by modern advances in automatic differentiation methods. Although mathematically continuously differentiable everywhere, the removable singularity present at zero in the standard form of the Box-Cox transformation renders it incompatible with automatic differentiation methods, complicating implementation of Laplace-approximate marginal likelihood. In this paper we consider two automatically-differentiable approximations to the Box-Cox transformation, with Gauss-Legendre quadrature shown to be especially accurate and fast. Application to Hodges’ famous state-level insurance premiums data is used to illustrate the computational aspects of the approximations. In particular, the flexibility of the implementation allows the use of heavy-tailed random effects distributions with essentially no additional user effort, and this is emphasized when we obtain a 92\% reduction in mean-squared error for predicting state-level median premiums, by changing only a single line of code.},
  archive      = {J_SAC},
  author       = {Stringer, Alex},
  doi          = {10.1007/s11222-022-10160-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-7},
  shortjournal = {Stat. Comput.},
  title        = {Automatic differentiation of box-cox transformations with application to random effects models for continuous non-normal response},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proximal nested sampling for high-dimensional bayesian model
selection. <em>SAC</em>, <em>32</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s11222-022-10152-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian model selection provides a powerful framework for objectively comparing models directly from observed data, without reference to ground truth data. However, Bayesian model selection requires the computation of the marginal likelihood (model evidence), which is computationally challenging, prohibiting its use in many high-dimensional Bayesian inverse problems. With Bayesian imaging applications in mind, in this work we present the proximal nested sampling methodology to objectively compare alternative Bayesian imaging models for applications that use images to inform decisions under uncertainty. The methodology is based on nested sampling, a Monte Carlo approach specialised for model comparison, and exploits proximal Markov chain Monte Carlo techniques to scale efficiently to large problems and to tackle models that are log-concave and not necessarily smooth (e.g., involving $$\ell _1$$ or total-variation priors). The proposed approach can be applied computationally to problems of dimension $${\mathcal {O}}(10^6)$$ and beyond, making it suitable for high-dimensional inverse imaging problems. It is validated on large Gaussian models, for which the likelihood is available analytically, and subsequently illustrated on a range of imaging problems where it is used to analyse different choices of dictionary and measurement model.},
  archive      = {J_SAC},
  author       = {Cai, Xiaohao and McEwen, Jason D. and Pereyra, Marcelo},
  doi          = {10.1007/s11222-022-10152-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Proximal nested sampling for high-dimensional bayesian model selection},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry-informed irreversible perturbations for accelerated
convergence of langevin dynamics. <em>SAC</em>, <em>32</em>(5), 1–22.
(<a href="https://doi.org/10.1007/s11222-022-10147-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel geometry-informed irreversible perturbation that accelerates convergence of the Langevin algorithm for Bayesian computation. It is well documented that there exist perturbations to the Langevin dynamics that preserve its invariant measure while accelerating its convergence. Irreversible perturbations and reversible perturbations (such as Riemannian manifold Langevin dynamics (RMLD)) have separately been shown to improve the performance of Langevin samplers. We consider these two perturbations simultaneously by presenting a novel form of irreversible perturbation for RMLD that is informed by the underlying geometry. Through numerical examples, we show that this new irreversible perturbation can improve estimation performance over irreversible perturbations that do not take the geometry into account. Moreover we demonstrate that irreversible perturbations generally can be implemented in conjunction with the stochastic gradient version of the Langevin algorithm. Lastly, while continuous-time irreversible perturbations cannot impair the performance of a Langevin estimator, the situation can sometimes be more complicated when discretization is considered. To this end, we describe a discrete-time example in which irreversibility increases both the bias and variance of the resulting estimator.},
  archive      = {J_SAC},
  author       = {Zhang, Benjamin J. and Marzouk, Youssef M. and Spiliopoulos, Konstantinos},
  doi          = {10.1007/s11222-022-10147-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Geometry-informed irreversible perturbations for accelerated convergence of langevin dynamics},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Split hamiltonian monte carlo revisited. <em>SAC</em>,
<em>32</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s11222-022-10149-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Hamiltonian Monte Carlo (HMC) samplers based on splitting the Hamiltonian H as $$H_0(\theta ,p)+U_1(\theta )$$ , where $$H_0$$ is quadratic and $$U_1$$ small. We show that, in general, such samplers suffer from stepsize stability restrictions similar to those of algorithms based on the standard leapfrog integrator. The restrictions may be circumvented by preconditioning the dynamics. Numerical experiments show that, when the $$H_0(\theta ,p)+U_1(\theta )$$ splitting is combined with preconditioning, it is possible to construct samplers far more efficient than standard leapfrog HMC.},
  archive      = {J_SAC},
  author       = {Casas, Fernando and Sanz-Serna, Jesús María and Shaw, Luke},
  doi          = {10.1007/s11222-022-10149-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Split hamiltonian monte carlo revisited},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive random neighbourhood informed markov chain monte
carlo for high-dimensional bayesian variable selection. <em>SAC</em>,
<em>32</em>(5), 1–52. (<a
href="https://doi.org/10.1007/s11222-022-10137-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for efficient Markov chain Monte Carlo algorithms targeting discrete-valued high-dimensional distributions, such as posterior distributions in Bayesian variable selection problems. We show that many recently introduced algorithms, such as the locally informed sampler of Zanella (J Am Stat Assoc 115(530):852–865, 2020), the locally informed with thresholded proposal of Zhou et al. (Dimension-free mixing for high-dimensional Bayesian variable selection, 2021) and the adaptively scaled individual adaptation sampler of Griffin et al. (Biometrika 108(1):53–69, 2021), can be viewed as particular cases within the framework. We then describe a novel algorithm, the adaptive random neighbourhood informed sampler, which combines ideas from these existing approaches. We show using several examples of both real and simulated data-sets that a computationally efficient point-wise implementation (PARNI) provides more reliable inferences on a range of variable selection problems, particularly in the very large p setting.},
  archive      = {J_SAC},
  author       = {Liang, Xitong and Livingstone, Samuel and Griffin, Jim},
  doi          = {10.1007/s11222-022-10137-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-52},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive random neighbourhood informed markov chain monte carlo for high-dimensional bayesian variable selection},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact simulation of normal tempered stable processes of OU
type with applications. <em>SAC</em>, <em>32</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10153-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Ornstein-Uhlenbeck process having a symmetric normal tempered stable stationary law and represent its transition distribution in terms of the sum of independent laws. In addition, we write the background driving Lévy process as the sum of two independent Lévy components. Accordingly, we can design two alternate algorithms for the simulation of the skeleton of the Ornstein-Uhlenbeck process. The solution based on the transition law turns out to be faster since it is based on a lower number of computational steps, as confirmed by extensive numerical experiments. We also calculate the characteristic function of the transition density which is instrumental for the application of the FFT-based method of Carr and Madan (J Comput Finance 2:61–73, 1999) to the pricing of a strip of call options written on markets whose price evolution is modeled by such an Ornstein-Uhlenbeck dynamics. This setting is indeed common for spot prices in the energy field. Finally, we show how to extend the range of applications to future markets.},
  archive      = {J_SAC},
  author       = {Sabino, Piergiacomo},
  doi          = {10.1007/s11222-022-10153-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Exact simulation of normal tempered stable processes of OU type with applications},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Achieving fairness with a simple ridge penalty.
<em>SAC</em>, <em>32</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10143-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a general framework for estimating regression models subject to a user-defined level of fairness. We enforce fairness as a model selection step in which we choose the value of a ridge penalty to control the effect of sensitive attributes. We then estimate the parameters of the model conditional on the chosen penalty value. Our proposal is mathematically simple, with a solution that is partly in closed form and produces estimates of the regression coefficients that are intuitive to interpret as a function of the level of fairness. Furthermore, it is easily extended to generalised linear models, kernelised regression models and other penalties, and it can accommodate multiple definitions of fairness. We compare our approach with the regression model from Komiyama et al. (in: Proceedings of machine learning research. 35th international conference on machine learning (ICML), vol 80, pp 2737–2746, 2018), which implements a provably optimal linear regression model and with the fair models from Zafar et al. (J Mach Learn Res 20:1–42, 2019). We evaluate these approaches empirically on six different data sets, and we find that our proposal provides better goodness of fit and better predictive accuracy for the same level of fairness. In addition, we highlight a source of bias in the original experimental evaluation in Komiyama et al. (in: Proceedings of machine learning research. 35th international conference on machine learning (ICML), vol 80, pp 2737–2746, 2018).},
  archive      = {J_SAC},
  author       = {Scutari, Marco and Panero, Francesca and Proissl, Manuel},
  doi          = {10.1007/s11222-022-10143-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Achieving fairness with a simple ridge penalty},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Default risk prediction and feature extraction using a
penalized deep neural network. <em>SAC</em>, <em>32</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10140-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online peer-to-peer lending platforms provide loans directly from lenders to borrowers without passing through traditional financial institutions. For lenders on these platforms to avoid loss, it is crucial that they accurately assess default risk so that they can make appropriate decisions. In this study, we develop a penalized deep learning model to predict default risk based on survival data. As opposed to simply predicting whether default will occur, we focus on predicting the probability of default over time. Moreover, by adding an additional one-to-one layer in the neural network, we achieve feature selection and estimation simultaneously by incorporating an $$L_1$$ -penalty into the objective function. The minibatch gradient descent algorithm makes it possible to handle massive data. An analysis of a real-world loan data and simulations demonstrate the model’s competitive practical performance, which suggests favorable potential applications in peer-to-peer lending platforms.},
  archive      = {J_SAC},
  author       = {Lin, Cunjie and Qiao, Nan and Zhang, Wenli and Li, Yang and Ma, Shuangge},
  doi          = {10.1007/s11222-022-10140-z},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Default risk prediction and feature extraction using a penalized deep neural network},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential sampling of junction trees for decomposable
graphs. <em>SAC</em>, <em>32</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10113-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The junction-tree representation provides an attractive structural property for organising a decomposable graph. In this study, we present two novel stochastic algorithms, referred to as the junction-tree expander and junction-tree collapser, for sequential sampling of junction trees for decomposable graphs. We show that recursive application of the junction-tree expander, which expands incrementally the underlying graph with one vertex at a time, has full support on the space of junction trees for any given number of underlying vertices. On the other hand, the junction-tree collapser provides a complementary operation for removing vertices in the underlying decomposable graph of a junction tree, while maintaining the junction tree property. A direct application of the proposed algorithms is demonstrated in the setting of sequential Monte Carlo methods, designed for sampling from distributions on spaces of decomposable graphs. Numerical studies illustrate the utility of the proposed algorithms for combinatorial computations on decomposable graphs and junction trees. All the methods proposed in the paper are implemented in the Python library trilearn.},
  archive      = {J_SAC},
  author       = {Olsson, Jimmy and Pavlenko, Tatjana and Rios, Felix L.},
  doi          = {10.1007/s11222-022-10113-2},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Sequential sampling of junction trees for decomposable graphs},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A non-stationary model for spatially dependent circular
response data based on wrapped gaussian processes. <em>SAC</em>,
<em>32</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10136-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data can be found across many areas of science, for instance meteorology (e.g., wind directions), ecology (e.g., animal movement directions), or medicine (e.g., seasonality in disease onset). The special nature of these data means that conventional methods for non-periodic data are no longer valid. In this paper, we consider wrapped Gaussian processes and introduce a spatial model for circular data that allow for non-stationarity in the mean and the covariance structure of Gaussian random fields. We use the empirical equivalence between Gaussian random fields and Gaussian Markov random fields which allows us to considerably reduce computational complexity by exploiting the sparseness of the precision matrix of the associated Gaussian Markov random field. Furthermore, we develop tunable priors, inspired by the penalized complexity prior framework, that shrink the model toward a less flexible base model with stationary mean and covariance function. Posterior estimation is done via Markov chain Monte Carlo simulation. The performance of the model is evaluated in a simulation study. Finally, the model is applied to analyzing wind directions in Germany.},
  archive      = {J_SAC},
  author       = {Marques, Isa and Kneib, Thomas and Klein, Nadja},
  doi          = {10.1007/s11222-022-10136-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A non-stationary model for spatially dependent circular response data based on wrapped gaussian processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational inference and sparsity in high-dimensional deep
gaussian mixture models. <em>SAC</em>, <em>32</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10132-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian mixture models are a popular tool for model-based clustering, and mixtures of factor analyzers are Gaussian mixture models having parsimonious factor covariance structure for mixture components. There are several recent extensions of mixture of factor analyzers to deep mixtures, where the Gaussian model for the latent factors is replaced by a mixture of factor analyzers. This construction can be iterated to obtain a model with many layers. These deep models are challenging to fit, and we consider Bayesian inference using sparsity priors to further regularize the estimation. A scalable natural gradient variational inference algorithm is developed for fitting the model, and we suggest computationally efficient approaches to the architecture choice using overfitted mixtures where unnecessary components drop out in the estimation. In a number of simulated and two real examples, we demonstrate the versatility of our approach for high-dimensional problems, and demonstrate that the use of sparsity inducing priors can be helpful for obtaining improved clustering results.},
  archive      = {J_SAC},
  author       = {Kock, Lucas and Klein, Nadja and Nott, David J.},
  doi          = {10.1007/s11222-022-10132-z},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Variational inference and sparsity in high-dimensional deep gaussian mixture models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing marginal likelihoods via the fourier integral
theorem and pointwise estimation of posterior densities. <em>SAC</em>,
<em>32</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10131-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel approach to the estimation of a density function at a specific chosen point. With this approach, we can estimate a normalizing constant, or equivalently compute a marginal likelihood, by focusing on estimating a posterior density function at a point. Relying on the Fourier integral theorem, the proposed method is capable of producing quick and accurate estimates of the marginal likelihood, regardless of how samples are obtained from the posterior; that is, it uses the posterior output generated by a Markov chain Monte Carlo sampler to estimate the marginal likelihood directly, with no modification to the form of the estimator on the basis of the type of sampler used. Thus, even for models with complicated specifications, such as those involving challenging hierarchical structures, or for Markov chains obtained from a black-box MCMC algorithm, the method provides a straightforward means of quickly and accurately estimating the marginal likelihood. In addition to developing theory to support the favorable behavior of the estimator, we also present a number of illustrative examples.},
  archive      = {J_SAC},
  author       = {Rotiroti, Frank and Walker, Stephen G.},
  doi          = {10.1007/s11222-022-10131-0},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Computing marginal likelihoods via the fourier integral theorem and pointwise estimation of posterior densities},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving bridge estimators via f-GAN. <em>SAC</em>,
<em>32</em>(5), 1–28. (<a
href="https://doi.org/10.1007/s11222-022-10133-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridge sampling is a powerful Monte Carlo method for estimating ratios of normalizing constants. Various methods have been introduced to improve its efficiency. These methods aim to increase the overlap between the densities by applying appropriate transformations to them without changing their normalizing constants. In this paper, we first give a new estimator of the asymptotic relative mean square error (RMSE) of the optimal Bridge estimator by equivalently estimating an f-divergence between the two densities. We then utilize this framework and propose f-GAN-Bridge estimator (f-GB) based on a bijective transformation that maps one density to the other and minimizes the asymptotic RMSE of the optimal Bridge estimator with respect to the densities. This transformation is chosen by minimizing a specific f-divergence between the densities. We show f-GB is optimal in the sense that within any given set of candidate transformations, the f-GB estimator can asymptotically achieve an RMSE lower than or equal to that achieved by Bridge estimators based on any other transformed densities. Numerical experiments show that f-GB outperforms existing methods in simulated and real-world examples. In addition, we discuss how Bridge estimators naturally arise from the problem of f-divergence estimation.},
  archive      = {J_SAC},
  author       = {Xing, Hanwen},
  doi          = {10.1007/s11222-022-10133-y},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Improving bridge estimators via f-GAN},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence rate bounds for iterative random functions using
one-shot coupling. <em>SAC</em>, <em>32</em>(5), 1–25. (<a
href="https://doi.org/10.1007/s11222-022-10134-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot coupling is a method of bounding the convergence rate between two copies of a Markov chain in total variation distance, which was first introduced in Roberts and Rosenthal (Process Appl 99:195–208, 2002) and generalized in Madras and Sezer (Bernoulli 16:882–908, 2010). The method is divided into two parts: the contraction phase, when the chains converge in expected distance and the coalescing phase, which occurs at the last iteration, when there is an attempt to couple. One-shot coupling does not require the use of any exogenous variables like a drift function or a minorization constant. In this paper, we summarize the one-shot coupling method into the One-Shot Coupling Theorem. We then apply the theorem to two families of Markov chains: the random functional autoregressive process and the autoregressive conditional heteroscedastic process. We provide multiple examples of how the theorem can be used on various models including ones in high dimensions. These examples illustrate how the theorem’s conditions can be verified in a straightforward way. The one-shot coupling method appears to generate tight geometric convergence rate bounds.},
  archive      = {J_SAC},
  author       = {Sixta, Sabrina and Rosenthal, Jeffrey S.},
  doi          = {10.1007/s11222-022-10134-x},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Convergence rate bounds for iterative random functions using one-shot coupling},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A joint latent factor analyzer and functional subspace model
for clustering multivariate functional data. <em>SAC</em>,
<em>32</em>(5), 1–33. (<a
href="https://doi.org/10.1007/s11222-022-10128-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a model-based approach for clustering multivariate functional data observations. We utilize theoretical results regarding a surrogate density on the truncated Karhunen–Loeve expansions along with a direct sum specification of the functional space to define a matrix normal distribution on functional principal components. This formulation allows for individual parsimonious modelling of the function space and coefficient space of the univariate components of the multivariate functional observations in the form a subspace projection and latent factor analyzers, respectively. The approach facilitates interpretation at both the full multivariate level and the component level, which is of specific interest when the component functions have clear meaning. We derive an AECM algorithm for fitting the model, and discuss appropriate initialization strategies, convergence and model selection criteria. We demonstrate the model’s applicability through simulation and two data analyses on observations that have many functional components.},
  archive      = {J_SAC},
  author       = {Sharp, Alex and Browne, Ryan},
  doi          = {10.1007/s11222-022-10128-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Stat. Comput.},
  title        = {A joint latent factor analyzer and functional subspace model for clustering multivariate functional data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A 4D-var method with flow-dependent background covariances
for the shallow-water equations. <em>SAC</em>, <em>32</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10119-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 4D-Var method for filtering partially observed nonlinear chaotic dynamical systems consists of finding the maximum a-posteriori (MAP) estimator of the initial condition of the system given observations over a time window, and propagating it forward to the current time via the model dynamics. This method forms the basis of most currently operational weather forecasting systems. In practice the optimisation becomes infeasible if the time window is too long due to the non-convexity of the cost function, the effect of model errors, and the limited precision of the ODE solvers. Hence the window has to be kept sufficiently short, and the observations in the previous windows can be taken into account via a Gaussian background (prior) distribution. The choice of the background covariance matrix is an important question that has received much attention in the literature. In this paper, we define the background covariances in a principled manner, based on observations in the previous b assimilation windows, for a parameter $$b\ge 1$$ . The method is at most b times more computationally expensive than using fixed background covariances, requires little tuning, and greatly improves the accuracy of 4D-Var. As a concrete example, we focus on the shallow-water equations. The proposed method is compared against state-of-the-art approaches in data assimilation and is shown to perform favourably on simulated data. We also illustrate our approach on data from the recent tsunami of 2011 in Fukushima, Japan.},
  archive      = {J_SAC},
  author       = {Paulin, Daniel and Jasra, Ajay and Beskos, Alexandros and Crisan, Dan},
  doi          = {10.1007/s11222-022-10119-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A 4D-var method with flow-dependent background covariances for the shallow-water equations},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real time anomaly detection and categorisation.
<em>SAC</em>, <em>32</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10112-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to quickly and accurately detect anomalous structure within data sequences is an inference challenge of growing importance. This work extends recently proposed post-hoc (offline) anomaly detection methodology to the sequential setting. The resultant procedure is capable of real-time analysis and categorisation between baseline and two forms of anomalous structure: point and collective anomalies. Various theoretical properties of the procedure are derived. These, together with an extensive simulation study, highlight that the average run length to false alarm and the average detection delay of the proposed online algorithm are very close to that of the offline version. Experiments on simulated and real data are provided to demonstrate the benefits of the proposed method.},
  archive      = {J_SAC},
  author       = {Fisch, Alexander T. M. and Bardwell, Lawrence and Eckley, Idris A.},
  doi          = {10.1007/s11222-022-10112-3},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Real time anomaly detection and categorisation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On automatic bias reduction for extreme expectile
estimation. <em>SAC</em>, <em>32</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10118-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectiles induce a law-invariant risk measure that has recently gained popularity in actuarial and financial risk management applications. Unlike quantiles or the quantile-based Expected Shortfall, the expectile risk measure is coherent and elicitable. The estimation of extreme expectiles in the heavy-tailed framework, which is reasonable for extreme financial or actuarial risk management, is not without difficulties; currently available estimators of extreme expectiles are typically biased and hence may show poor finite-sample performance even in fairly large samples. We focus here on the construction of bias-reduced extreme expectile estimators for heavy-tailed distributions. The rationale for our construction hinges on a careful investigation of the asymptotic proportionality relationship between extreme expectiles and their quantile counterparts, as well as of the extrapolation formula motivated by the heavy-tailed context. We accurately quantify and estimate the bias incurred by the use of these relationships when constructing extreme expectile estimators. This motivates the introduction of classes of bias-reduced estimators whose asymptotic properties are rigorously shown, and whose finite-sample properties are assessed on a simulation study and three samples of real data from economics, insurance and finance.},
  archive      = {J_SAC},
  author       = {Girard, Stéphane and Stupfler, Gilles and Usseglio-Carleve, Antoine},
  doi          = {10.1007/s11222-022-10118-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {On automatic bias reduction for extreme expectile estimation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The computational asymptotics of gaussian variational
inference and the laplace approximation. <em>SAC</em>, <em>32</em>(4),
1–37. (<a href="https://doi.org/10.1007/s11222-022-10125-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian variational inference and the Laplace approximation are popular alternatives to Markov chain Monte Carlo that formulate Bayesian posterior inference as an optimization problem, enabling the use of simple and scalable stochastic optimization algorithms. However, a key limitation of both methods is that the solution to the optimization problem is typically not tractable to compute; even in simple settings, the problem is nonconvex. Thus, recently developed statistical guarantees—which all involve the (data) asymptotic properties of the global optimum—are not reliably obtained in practice. In this work, we provide two major contributions: a theoretical analysis of the asymptotic convexity properties of variational inference with a Gaussian family and the maximum a posteriori (MAP) problem required by the Laplace approximation, and two algorithms—consistent Laplace approximation (CLA) and consistent stochastic variational inference (CSVI)—that exploit these properties to find the optimal approximation in the asymptotic regime. Both CLA and CSVI involve a tractable initialization procedure that finds the local basin of the optimum, and CSVI further includes a scaled gradient descent algorithm that provably stays locally confined to that basin. Experiments on nonconvex synthetic and real-data examples show that compared with standard variational and Laplace approximations, both CSVI and CLA improve the likelihood of obtaining the global optimum of their respective optimization problems.},
  archive      = {J_SAC},
  author       = {Xu, Zuheng and Campbell, Trevor},
  doi          = {10.1007/s11222-022-10125-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-37},
  shortjournal = {Stat. Comput.},
  title        = {The computational asymptotics of gaussian variational inference and the laplace approximation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fitting double hierarchical models with the integrated
nested laplace approximation. <em>SAC</em>, <em>32</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10122-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double hierarchical generalized linear models (DHGLM) are a family of models that are flexible enough as to model hierarchically the mean and scale parameters. In a Bayesian framework, fitting highly parameterized hierarchical models is challenging when this problem is addressed using typical Markov chain Monte Carlo (MCMC) methods due to the potential high correlation between different parameters and effects in the model. The integrated nested Laplace approximation (INLA) could be considered instead to avoid dealing with these problems. However, DHGLM do not fit within the latent Gaussian Markov random field (GMRF) models that INLA can fit. In this paper, we show how to fit DHGLM with INLA by combining INLA and importance sampling (IS) algorithms. In particular, we will illustrate how to split DHGLM into submodels that can be fitted with INLA so that the remainder of the parameters are fit using adaptive multiple IS (AMIS) with the aid of the graphical representation of the hierarchical model. This is illustrated using a simulation study on three different types of models and two real data examples.},
  archive      = {J_SAC},
  author       = {Morales-Otero, Mabel and Gómez-Rubio, Virgilio and Núñez-Antón, Vicente},
  doi          = {10.1007/s11222-022-10122-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Fitting double hierarchical models with the integrated nested laplace approximation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantile hidden semi-markov models for multivariate time
series. <em>SAC</em>, <em>32</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s11222-022-10130-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a quantile hidden semi-Markov regression to jointly estimate multiple quantiles for the analysis of multivariate time series. The approach is based upon the Multivariate Asymmetric Laplace (MAL) distribution, which allows to model the quantiles of all univariate conditional distributions of a multivariate response simultaneously, incorporating the correlation structure among the outcomes. Unobserved serial heterogeneity across observations is modeled by introducing regime-dependent parameters that evolve according to a latent finite-state semi-Markov chain. Exploiting the hierarchical representation of the MAL, inference is carried out using an efficient Expectation-Maximization algorithm based on closed form updates for all model parameters, without parametric assumptions about the states’ sojourn distributions. The validity of the proposed methodology is analyzed both by a simulation study and through the empirical analysis of air pollutant concentrations in a small Italian city.},
  archive      = {J_SAC},
  author       = {Merlo, Luca and Maruotti, Antonello and Petrella, Lea and Punzo, Antonio},
  doi          = {10.1007/s11222-022-10130-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Quantile hidden semi-markov models for multivariate time series},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rate-optimal refinement strategies for local approximation
MCMC. <em>SAC</em>, <em>32</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s11222-022-10123-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Bayesian inference problems involve target distributions whose density functions are computationally expensive to evaluate. Replacing the target density with a local approximation based on a small number of carefully chosen density evaluations can significantly reduce the computational expense of Markov chain Monte Carlo (MCMC) sampling. Moreover, continual refinement of the local approximation can guarantee asymptotically exact sampling. We devise a new strategy for balancing the decay rate of the bias due to the approximation with that of the MCMC variance. We prove that the error of the resulting local approximation MCMC (LA-MCMC) algorithm decays at roughly the expected $$1/\sqrt{T}$$ rate, and we demonstrate this rate numerically. We also introduce an algorithmic parameter that guarantees convergence given very weak tail bounds, significantly strengthening previous convergence results. Finally, we apply LA-MCMC to a computationally intensive Bayesian inverse problem arising in groundwater hydrology.},
  archive      = {J_SAC},
  author       = {Davis, Andrew D. and Marzouk, Youssef and Smith, Aaron and Pillai, Natesh},
  doi          = {10.1007/s11222-022-10123-0},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Rate-optimal refinement strategies for local approximation MCMC},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polya tree-based nearest neighborhood regression.
<em>SAC</em>, <em>32</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11222-021-10076-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric regression, such as linear regression, plays an important role in statistics. The use of parametric regression models typically involves the specification of a regression function of the covariates, the distribution of response and the link between the response and covariates, which are commonly at the risk of misspecification. In this paper, we introduce a fully nonparametric regression model, a Polya tree (PT)-based nearest neighborhood regression. To approximate the true conditional probability measure of the response given the covariate value, we construct a PT-distributed probability measure of the response in the nearest neighborhood of the covariate value of interest. Our proposed method gives consistent and robust estimators, and has a faster convergence rate than the kernel density estimation. We conduct extensive simulation studies and analyze a Combined Cycle Power Plant dataset to compare the performance of our method relative to kernel density estimation, PT density estimation, and linear dependent tail-free process (LDTFP). The studies suggest that the proposed method exhibits the superiority to the kernel and PT density estimation methods in terms of the estimation accuracy and convergence rate and to LDTFP in terms of robustness.},
  archive      = {J_SAC},
  author       = {Zhuang, Haoxin and Diao, Liqun and Yi, Grace},
  doi          = {10.1007/s11222-021-10076-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Polya tree-based nearest neighborhood regression},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptively weighted stochastic gradient MCMC algorithm
for monte carlo simulation and global optimization. <em>SAC</em>,
<em>32</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s11222-022-10120-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an adaptively weighted stochastic gradient Langevin dynamics (AWSGLD) algorithm for Bayesian learning of big data problems. The proposed algorithm is scalable and possesses a self-adjusting mechanism: It adaptively flattens the high-energy region and protrudes the low-energy region during simulations such that both Monte Carlo simulation and global optimization tasks can be greatly facilitated in a single run. The self-adjusting mechanism enables the proposed algorithm to be essentially immune to local traps. Theoretically, by showing the stability of the mean-field system and verifying the existence and regularity properties of the solution of Poisson equation, we establish the convergence of the AWSGLD algorithm, including both the convergence of the self-adapting parameters and the convergence of the weighted averaging estimators. Empirically, the AWSGLD algorithm is tested on multiple benchmark datasets including CIFAR100 and SVHN for both optimization and uncertainty estimation tasks. The numerical results indicate its great potential in Monte Carlo simulation and global optimization for modern machine learning tasks.},
  archive      = {J_SAC},
  author       = {Deng, Wei and Lin, Guang and Liang, Faming},
  doi          = {10.1007/s11222-022-10120-3},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {An adaptively weighted stochastic gradient MCMC algorithm for monte carlo simulation and global optimization},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifiability and parameter estimation of the overlapped
stochastic co-block model. <em>SAC</em>, <em>32</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11222-022-10114-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic block model (SBM) has been extensively studied for undirected network data with community structure, yet its extension to directed network, stochastic co-block model (ScBM), has only been proposed recently. The key difference of the ScBM model is to introduce out- and in-communities to capture different sending and receiving patterns among nodes. In this paper, we further extend the ScBM model so that each node may belong to multiple out- or in-communities. Particularly, we formulate the ScBM model as a generative model, where the unknown community assignment is modeled based on the exclusive or overlapped community. We also establish the corresponding identifiability of the generative ScBM model, and estimate its parameters via an efficient variational EM algorithm. The advantage of the generative ScBM model is demonstrated in a variety of simulated networks and a real political blog network.},
  archive      = {J_SAC},
  author       = {Zhang, Jingnan and Wang, Junhui},
  doi          = {10.1007/s11222-022-10114-1},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Identifiability and parameter estimation of the overlapped stochastic co-block model},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallelizing MCMC sampling via space partitioning.
<em>SAC</em>, <em>32</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s11222-022-10116-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient sampling of many-dimensional and multimodal density functions is a task of great interest in many research fields. We describe an algorithm that allows parallelizing inherently serial Markov chain Monte Carlo (MCMC) sampling by partitioning the space of the function parameters into multiple subspaces and sampling each of them independently. The samples of the different subspaces are then reweighted by their integral values and stitched back together. This approach allows reducing sampling wall-clock time by parallel operation. It also improves sampling of multimodal target densities and results in less correlated samples. Finally, the approach yields an estimate of the integral of the target density function.},
  archive      = {J_SAC},
  author       = {Hafych, Vasyl and Eller, Philipp and Schulz, Oliver and Caldwel, Allen},
  doi          = {10.1007/s11222-022-10116-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Parallelizing MCMC sampling via space partitioning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recursive inversion models for permutations. <em>SAC</em>,
<em>32</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s11222-022-10111-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new exponential family model for permutations that can capture hierarchical structure in preferences, and that has the well known Mallows models as a subclass. The Recursive Inversions Model (RIM), unlike most distributions over permutations of n items, has a flexible structure, represented by a binary tree. We describe how to compute marginals in the RIM, including the partition function, in closed form. Further we introduce methods for the Maximum Likelihood estimation of parameters and structure search for this model. We demonstrate that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.},
  archive      = {J_SAC},
  author       = {Meilă, Marina and Wagner, Annelise and Meek, Christopher},
  doi          = {10.1007/s11222-022-10111-4},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Recursive inversion models for permutations},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parsimonious hidden markov models for matrix-variate
longitudinal data. <em>SAC</em>, <em>32</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10107-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) have been extensively used in the univariate and multivariate literature. However, there has been an increased interest in the analysis of matrix-variate data over the recent years. In this manuscript we introduce HMMs for matrix-variate balanced longitudinal data, by assuming a matrix normal distribution in each hidden state. Such data are arranged in a four-way array. To address for possible overparameterization issues, we consider the eigen decomposition of the covariance matrices, leading to a total of 98 HMMs. An expectation-conditional maximization algorithm is discussed for parameter estimation. The proposed models are firstly investigated on simulated data, in terms of parameter recovery, computational times and model selection. Then, they are fitted to a four-way real data set concerning the unemployment rates of the Italian provinces, evaluated by gender and age classes, over the last 16 years.},
  archive      = {J_SAC},
  author       = {Tomarchio, Salvatore D. and Punzo, Antonio and Maruotti, Antonello},
  doi          = {10.1007/s11222-022-10107-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Parsimonious hidden markov models for matrix-variate longitudinal data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Importance conditional sampling for pitman–yor mixtures.
<em>SAC</em>, <em>32</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10096-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric mixture models based on the Pitman–Yor process represent a flexible tool for density estimation and clustering. Natural generalization of the popular class of Dirichlet process mixture models, they allow for more robust inference on the number of components characterizing the distribution of the data. We propose a new sampling strategy for such models, named importance conditional sampling (ICS), which combines appealing properties of existing methods, including easy interpretability and a within-iteration parallelizable structure. An extensive simulation study highlights the efficiency of the proposed method which, unlike other conditional samplers, shows stable performances for different specifications of the parameters characterizing the Pitman–Yor process. We further show that the ICS approach can be naturally extended to other classes of computationally demanding models, such as nonparametric mixture models for partially exchangeable data.},
  archive      = {J_SAC},
  author       = {Canale, Antonio and Corradin, Riccardo and Nipoti, Bernardo},
  doi          = {10.1007/s11222-022-10096-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Importance conditional sampling for Pitman–Yor mixtures},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional regression with potential prior information
on variable importance. <em>SAC</em>, <em>32</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10110-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a variety of settings where vague prior information may be available on the importance of predictors in high-dimensional regression settings. Examples include the ordering on the variables offered by their empirical variances (which is typically discarded through standardisation), the lag of predictors when fitting autoregressive models in time series settings, or the level of missingness of the variables. Whilst such orderings may not match the true importance of variables, we argue that there is little to be lost, and potentially much to be gained, by using them. We propose a simple scheme involving fitting a sequence of models indicated by the ordering. We show that the computational cost for fitting all models when ridge regression is used is no more than for a single fit of ridge regression, and describe a strategy for Lasso regression that makes use of previous fits to greatly speed up fitting the entire sequence of models. We propose to select a final estimator by cross-validation and provide a general result on the quality of the best performing estimator on a test set selected from among a number M of competing estimators in a high-dimensional linear regression setting. Our result requires no sparsity assumptions and shows that only a $$\log M$$ price is incurred compared to the unknown best estimator. We demonstrate the effectiveness of our approach when applied to missing or corrupted data, and in time series settings. An R package is available on github.},
  archive      = {J_SAC},
  author       = {Stokell, Benjamin G. and Shah, Rajen D.},
  doi          = {10.1007/s11222-022-10110-5},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional regression with potential prior information on variable importance},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complexity of zigzag sampling algorithm for strongly
log-concave distributions. <em>SAC</em>, <em>32</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10109-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the computational complexity of zigzag sampling algorithm for strongly log-concave distributions. The zigzag process has the advantage of not requiring time discretization for implementation, and that each proposed bouncing event requires only one evaluation of partial derivative of the potential, while its convergence rate is dimension independent. Using these properties, we prove that the zigzag sampling algorithm achieves $$\varepsilon $$ error in chi-square divergence with a computational cost equivalent to $$O\bigl (\kappa ^2 d^\frac{1}{2}(\log \frac{1}{\varepsilon })^{\frac{3}{2}}\bigr )$$ gradient evaluations in the regime $$\kappa \ll \frac{d}{\log d}$$ under a warm start assumption, where $$\kappa $$ is the condition number and d is the dimension.},
  archive      = {J_SAC},
  author       = {Lu, Jianfeng and Wang, Lihan},
  doi          = {10.1007/s11222-022-10109-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Complexity of zigzag sampling algorithm for strongly log-concave distributions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selecting the derivative of a functional covariate in
scalar-on-function regression. <em>SAC</em>, <em>32</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s11222-022-10091-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents tests to formally choose between regression models using different derivatives of a functional covariate in scalar-on-function regression. We demonstrate that for linear regression, models using different derivatives can be nested within a model that includes point-impact effects at the end-points of the observed functions. Contrasts can then be employed to test the specification of different derivatives. When nonlinear regression models are employed, we apply a C test to determine the statistical significance of the nonlinear structure between a functional covariate and a scalar response. The finite-sample performance of these methods is verified in simulation, and their practical application is demonstrated using both chemometric and environmental data sets.},
  archive      = {J_SAC},
  author       = {Hooker, Giles and Shang, Han Lin},
  doi          = {10.1007/s11222-022-10091-5},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Selecting the derivative of a functional covariate in scalar-on-function regression},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint latent space models for ranking data and social
network. <em>SAC</em>, <em>32</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10106-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human interaction and communication has become one of the essential features of social life. Individuals’ preference may be influenced by those of their peers or friends in a social network. In the literature, individuals’ rank-order preferences and their social network are often modeled separately. In this article, we propose a new joint probabilistic model for ranking data and social network. With a latent space for all the individuals and items, the proposed model assume that the social network and rankings of items are governed by the locations of individuals and items. Based on an efficient MCMC algorithm, we develop a set of Bayesian inference approaches for the proposed model, including procedures of model selection, criteria to evaluate model fitness and a test for conditional independence between individuals’ rankings and their social network given their positions in the latent space. Simulation studies reveal the usefulness of our proposed methods for parameter estimation, model fitness evaluation, model selection and conditional independence testing. Finally, we apply our model to the CiaoDVD dataset which consists of users’ trust relations and their implicit preferences on DVD categories.},
  archive      = {J_SAC},
  author       = {Gu, Jiaqi and Yu, Philip L. H.},
  doi          = {10.1007/s11222-022-10106-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Joint latent space models for ranking data and social network},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biclustering via structured regularized matrix
decomposition. <em>SAC</em>, <em>32</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s11222-022-10095-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering is a machine learning problem that deals with simultaneously clustering of rows and columns of a data matrix. Complex structures of the data matrix such as overlapping biclusters have challenged existing methods. In this paper, we first provide a unified formulation of biclustering that uses structured regularized matrix decomposition, which synthesizes various existing methods, and then develop a new biclustering method called BCEL based on this formulation. The biclustering problem is formulated as a penalized least-squares problem that approximates the data matrix $$\mathbf {X}$$ by a multiplicative matrix decomposition $$\mathbf {U}\mathbf {V}^T$$ with sparse columns in both $$\mathbf {U}$$ and $$\mathbf {V}$$ . The squared $$\ell _{1,2}$$ -norm penalty, also called the exclusive Lasso penalty, is applied to both $$\mathbf {U}$$ and $$\mathbf {V}$$ to assist identification of rows and columns included in the biclusters. The penalized least-squares problem is solved by a novel computational algorithm that combines alternating minimization and the proximal gradient method. A subsampling based procedure called stability selection is developed to select the tuning parameters and determine the bicluster membership. BCEL is shown to be competitive to existing methods in simulation studies and an application to a real-world single-cell RNA sequencing dataset.},
  archive      = {J_SAC},
  author       = {Zhong, Yan and Huang, Jianhua Z.},
  doi          = {10.1007/s11222-022-10095-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Biclustering via structured regularized matrix decomposition},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated parallel non-conjugate sampling for bayesian
non-parametric models. <em>SAC</em>, <em>32</em>(3), 1–25. (<a
href="https://doi.org/10.1007/s11222-022-10108-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference of latent feature models in the Bayesian nonparametric setting is generally difficult, especially in high dimensional settings, because it usually requires proposing features from some prior distribution. In special cases, where the integration is tractable, we can sample new feature assignments according to a predictive likelihood. We present a novel method to accelerate the mixing of latent variable model inference by proposing feature locations based on the data, as opposed to the prior. First, we introduce an accelerated feature proposal mechanism that we show is a valid MCMC algorithm for posterior inference. Next, we propose an approximate inference strategy to perform accelerated inference in parallel. A two-stage algorithm that combines the two approaches provides a computationally attractive method that can quickly reach local convergence to the posterior distribution of our model, while allowing us to exploit parallelization.},
  archive      = {J_SAC},
  author       = {Zhang, Michael Minyi and Williamson, Sinead A. and Pérez-Cruz, Fernando},
  doi          = {10.1007/s11222-022-10108-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Accelerated parallel non-conjugate sampling for bayesian non-parametric models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Particle gradient descent model for point process
generation. <em>SAC</em>, <em>32</em>(3), 1–25. (<a
href="https://doi.org/10.1007/s11222-022-10099-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a statistical model for stationary ergodic point processes, estimated from a single realization observed in a square window. With existing approaches in stochastic geometry, it is very difficult to model processes with complex geometries formed by a large number of particles. Inspired by recent works on gradient descent algorithms for sampling maximum-entropy models, we describe a model that allows for fast sampling of new configurations reproducing the statistics of the given observation. Starting from an initial random configuration, its particles are moved according to the gradient of an energy, in order to match a set of prescribed moments (functionals). Our moments are defined via a phase harmonic operator on the wavelet transform of point patterns. They allow one to capture multi-scale interactions between the particles, while controlling explicitly the number of moments by the scales of the structures to model. We present numerical experiments on point processes with various geometric structures, and assess the quality of the model by spectral and topological data analysis.},
  archive      = {J_SAC},
  author       = {Brochard, Antoine and Błaszczyszyn, Bartłomiej and Zhang, Sixin and Mallat, Stéphane},
  doi          = {10.1007/s11222-022-10099-x},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Particle gradient descent model for point process generation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel estimation of normalization constants using
ensemble kalman–bucy filters. <em>SAC</em>, <em>32</em>(3), 1–25. (<a
href="https://doi.org/10.1007/s11222-022-10094-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider the application of multilevel Monte Carlo, for the estimation of normalizing constants. In particular we will make use of the filtering algorithm, the ensemble Kalman–Bucy filter (EnKBF), which is an N-particle representation of the Kalman–Bucy filter (KBF). The EnKBF is of interest as it coincides with the optimal filter in the continuous-linear setting, i.e. the KBF. This motivates our particular setup in the linear setting. The resulting methodology we will use is the multilevel ensemble Kalman–Bucy filter (MLEnKBF). We provide an analysis based on deriving $${\mathbb {L}}_q$$ -bounds for the normalizing constants using both the single-level, and the multilevel algorithms, which is largely based on previous work deriving the MLEnKBF Chada et al. (2022). Our results will be highlighted through numerical results, where we firstly demonstrate the error-to-cost rates of the MLEnKBFs comparing it to the EnKBF on a linear Gaussian model. Our analysis will be specific to one variant of the MLEnKBF, whereas the numerics will be tested on different variants. We also exploit this methodology for parameter estimation, where we test this on the models arising in atmospheric sciences, such as the stochastic Lorenz 63 and 96 model.},
  archive      = {J_SAC},
  author       = {Ruzayqat, Hamza and Chada, Neil K. and Jasra, Ajay},
  doi          = {10.1007/s11222-022-10094-2},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel estimation of normalization constants using ensemble Kalman–Bucy filters},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized likelihood-based bayesian approach for
scalable joint regression and covariance selection in high dimensions.
<em>SAC</em>, <em>32</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11222-022-10102-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper addresses joint sparsity selection in the regression coefficient matrix and the error precision (inverse covariance) matrix for high-dimensional multivariate regression models in the Bayesian paradigm. The selected sparsity patterns are crucial to help understand the network of relationships between the predictor and response variables, as well as the conditional relationships among the latter. While Bayesian methods have the advantage of providing natural uncertainty quantification through posterior inclusion probabilities and credible intervals, current Bayesian approaches either restrict to specific sub-classes of sparsity patterns and/or are not scalable to settings with hundreds of responses and predictors. Bayesian approaches that only focus on estimating the posterior mode are scalable, but do not generate samples from the posterior distribution for uncertainty quantification. Using a bi-convex regression-based generalized likelihood and spike-and-slab priors, we develop an algorithm called joint regression network selector (JRNS) for joint regression and covariance selection, which (a) can accommodate general sparsity patterns, (b) provides posterior samples for uncertainty quantification, and (c) is scalable and orders of magnitude faster than the state-of-the-art Bayesian approaches providing uncertainty quantification. We demonstrate the statistical and computational efficacy of the proposed approach on synthetic data and through the analysis of selected cancer data sets. We also establish high-dimensional posterior consistency for one of the developed algorithms.},
  archive      = {J_SAC},
  author       = {Samanta, Srijata and Khare, Kshitij and Michailidis, George},
  doi          = {10.1007/s11222-022-10102-5},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {A generalized likelihood-based bayesian approach for scalable joint regression and covariance selection in high dimensions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparison of likelihood-free methods with and without
summary statistics. <em>SAC</em>, <em>32</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s11222-022-10092-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-free methods are useful for parameter estimation of complex models with intractable likelihood functions for which it is easy to simulate data. Such models are prevalent in many disciplines including genetics, biology, ecology and cosmology. Likelihood-free methods avoid explicit likelihood evaluation by finding parameter values of the model that generate data close to the observed data. The general consensus has been that it is most efficient to compare datasets on the basis of a low dimensional informative summary statistic, incurring information loss in favour of reduced dimensionality. More recently, researchers have explored various approaches for efficiently comparing empirical distributions of the data in the likelihood-free context in an effort to avoid data summarisation. This article provides a review of these full data distance based approaches, and conducts the first comprehensive comparison of such methods, both qualitatively and empirically. We also conduct a substantive empirical comparison with summary statistic based likelihood-free methods. The discussion and results offer guidance to practitioners considering a likelihood-free approach. Whilst we find the best approach to be problem dependent, we also find that the full data distance based approaches are promising and warrant further development. We discuss some opportunities for future research in this space. Computer code to implement the methods discussed in this paper can be found at https://github.com/cdrovandi/ABC-dist-compare .},
  archive      = {J_SAC},
  author       = {Drovandi, Christopher and Frazier, David T.},
  doi          = {10.1007/s11222-022-10092-4},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {A comparison of likelihood-free methods with and without summary statistics},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sklar’s omega: A gaussian copula-based framework for
assessing agreement. <em>SAC</em>, <em>32</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10105-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical measurement of agreement—the most commonly used form of which is inter-coder agreement (also called inter-rater reliability), i.e., consistency of scoring among two or more coders for the same units of analysis—is important in a number of fields, e.g., content analysis, education, computational linguistics, sports. We propose Sklar’s Omega, a Gaussian copula-based framework for measuring not only inter-coder agreement but also intra-coder agreement, inter-method agreement, and agreement relative to a gold standard. We demonstrate the efficacy and advantages of our approach by applying both Sklar’s Omega and Krippendorff’s Alpha (a well-established nonparametric agreement coefficient) to simulated data, to nominal data previously analyzed by Krippendorff, and to continuous data from an imaging study of hip cartilage in femoroacetabular impingement. Application of our proposed methodology is supported by our open-source R package, sklarsomega, which is available for download from the Comprehensive R Archive Network. The package permits users to apply the Omega methodology to nominal scores, ordinal scores, percentages, counts, amounts (i.e., non-negative real numbers), and balances (i.e., any real number); and can accommodate any number of units, any number of coders, and missingness. Classical inference is available for all levels of measurement while Bayesian inference is available for continuous outcomes only.},
  archive      = {J_SAC},
  author       = {Hughes, John},
  doi          = {10.1007/s11222-022-10105-2},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Sklar’s omega: A gaussian copula-based framework for assessing agreement},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rule-based bayesian regression. <em>SAC</em>,
<em>32</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10100-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel rule-based approach for handling regression problems. The new methodology carries elements from two frameworks: (i) it provides information about the uncertainty of the parameters of interest using Bayesian inference, and (ii) it allows the incorporation of expert knowledge through rule-based systems. The blending of those two different frameworks can be particularly beneficial for various domains (e.g., engineering), where even though the significance of uncertainty quantification motivates a Bayesian approach, there is no simple way to incorporate researcher intuition into the model. We validate our models by applying them to synthetic applications: a simple linear regression problem and two more complex structures based on partial differential equations, and we illustrate their use through two cases derived from real data. Finally, we review the advantages of our methodology, which include the simplicity of the implementation, the uncertainty reduction due to the added information and, in some occasions, the derivation of better point predictions, and we outline limitations, mainly from the computational complexity perspective, such as the difficulty in choosing an appropriate algorithm and the added computational burden.},
  archive      = {J_SAC},
  author       = {Botsas, Themistoklis and Mason, Lachlan R. and Pan, Indranil},
  doi          = {10.1007/s11222-022-10100-7},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Rule-based bayesian regression},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimally adaptive bayesian spectral density estimation for
stationary and nonstationary processes. <em>SAC</em>, <em>32</em>(3),
1–22. (<a href="https://doi.org/10.1007/s11222-022-10103-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article improves on existing Bayesian methods to estimate the spectral density of stationary and nonstationary time series assuming a Gaussian process prior. By optimising an appropriate eigendecomposition using a smoothing spline covariance structure, our method more appropriately models data with both simple and complex periodic structure. We further justify the utility of this optimal eigendecomposition by investigating the performance of alternative covariance functions other than smoothing splines. We show that the optimal eigendecomposition provides a material improvement, while the other covariance functions under examination do not, all performing comparatively well as the smoothing spline. During our computational investigation, we introduce new validation metrics for the spectral density estimate, inspired from the physical sciences. We validate our models in an extensive simulation study and demonstrate superior performance with real data.},
  archive      = {J_SAC},
  author       = {James, Nick and Menzies, Max},
  doi          = {10.1007/s11222-022-10103-4},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Optimally adaptive bayesian spectral density estimation for stationary and nonstationary processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-clustering of evolving count matrices with the dynamic
latent block model: Application to pharmacovigilance. <em>SAC</em>,
<em>32</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s11222-022-10098-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous clustering of observations and features of datasets (known as co-clustering) has recently emerged as a central topic in machine learning applications. However, most models focus on continuous data in stationary scenarios, where cluster assignments do not evolve over time. We propose in this paper the dynamic latent block model (dLBM), which extends the classical binary latent block model, making amenable such analysis to dynamic cases where data are counts. Our approach operates on temporal count matrices allowing to detect abrupt changes in the way existing clusters interact with each other. The time breaks detection is performed through clustering of time instants that allows for better model parsimony. The time-dependent counting data are modeled via non-homogeneous Poisson processes (HHPPs), conditionally to the latent variables. In order to handle the model inference, we rely on a SEM-Gibbs algorithm and the ICL criterion is used for model selection. Numerical experiments on simulated data highlight the main features of the proposed approach and show the interest of dLBM with respect to related works. An application to adverse drug reaction in pharmacovigilance is also proposed, where dLBM was able to recognize clusters in a meaningful way that identified safety events that were consistent with retrospective knowledge. Hence, our aim is to propose this dynamic co-clustering method as a tool for automatic safety signal detection, to support medical authorities.},
  archive      = {J_SAC},
  author       = {Marchello, Giulia and Fresse, Audrey and Corneli, Marco and Bouveyron, Charles},
  doi          = {10.1007/s11222-022-10098-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Co-clustering of evolving count matrices with the dynamic latent block model: Application to pharmacovigilance},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The node-wise pseudo-marginal method: Model selection with
spatial dependence on latent graphs. <em>SAC</em>, <em>32</em>(3), 1–31.
(<a href="https://doi.org/10.1007/s11222-022-10101-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by problems from neuroimaging in which existing approaches make use of “mass univariate” analysis which neglects spatial structure entirely, but the full joint modelling of all quantities of interest is computationally infeasible, a novel method for incorporating spatial dependence within a (potentially large) family of model-selection problems is presented. Spatial dependence is encoded via a Markov random field model for which a variant of the pseudo-marginal Markov chain Monte Carlo algorithm is developed and extended by a further augmentation of the underlying state space. This approach allows the exploitation of existing unbiased marginal likelihood estimators used in settings in which spatial independence is normally assumed thereby facilitating the incorporation of spatial dependence using non-spatial estimates with minimal additional development effort. The proposed algorithm can be realistically used for analysis of moderately sized data sets such as 2D slices of whole 3D dynamic PET brain images or other regions of interest. Principled approximations of the proposed method, together with simple extensions based on the augmented spaces, are investigated and shown to provide similar results to the full pseudo-marginal method. Such approximations and extensions allow the improved performance obtained by incorporating spatial dependence to be obtained at negligible additional cost. An application to measured PET image data shows notable improvements in revealing underlying spatial structure when compared to current methods that assume spatial independence.},
  archive      = {J_SAC},
  author       = {Thesingarajah, Denishrouf and Johansen, Adam M.},
  doi          = {10.1007/s11222-022-10101-6},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {The node-wise pseudo-marginal method: Model selection with spatial dependence on latent graphs},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributional anchor regression. <em>SAC</em>,
<em>32</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10097-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models often fail if train and test data do not stem from the same distribution. Out-of-distribution (OOD) generalization to unseen, perturbed test data is a desirable but difficult-to-achieve property for prediction models and in general requires strong assumptions on the data generating process (DGP). In a causally inspired perspective on OOD generalization, the test data arise from a specific class of interventions on exogenous random variables of the DGP, called anchors. Anchor regression models, introduced by Rothenhäusler et al. (J R Stat Soc Ser B 83(2):215–246, 2021. https://doi.org/10.1111/rssb.12398 ), protect against distributional shifts in the test data by employing causal regularization. However, so far anchor regression has only been used with a squared-error loss which is inapplicable to common responses such as censored continuous or ordinal data. Here, we propose a distributional version of anchor regression which generalizes the method to potentially censored responses with at least an ordered sample space. To this end, we combine a flexible class of parametric transformation models for distributional regression with an appropriate causal regularizer under a more general notion of residuals. In an exemplary application and several simulation scenarios we demonstrate the extent to which OOD generalization is possible.},
  archive      = {J_SAC},
  author       = {Kook, Lucas and Sick, Beate and Bühlmann, Peter},
  doi          = {10.1007/s11222-022-10097-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributional anchor regression},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unbiased approximation of posteriors via coupled particle
markov chain monte carlo. <em>SAC</em>, <em>32</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10093-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) is a powerful methodology for the approximation of posterior distributions. However, the iterative nature of MCMC does not naturally facilitate its use with modern highly parallel computation on HPC and cloud environments. Another concern is the identification of the bias and Monte Carlo error of produced averages. The above have prompted the recent development of fully (‘embarrassingly’) parallel unbiased Monte Carlo methodology based on coupling of MCMC algorithms. A caveat is that formulation of effective coupling is typically not trivial and requires model-specific technical effort. We propose coupling of MCMC chains deriving from sequential Monte Carlo (SMC) by considering adaptive SMC methods in combination with recent advances in unbiased estimation for state-space models. Coupling is then achieved at the SMC level and is, in principle, not problem-specific. The resulting methodology enjoys desirable theoretical properties. A central motivation is to extend unbiased MCMC to more challenging targets compared to the ones typically considered in the relevant literature. We illustrate the effectiveness of the algorithm via application to two complex statistical models: (i) horseshoe regression; (ii) Gaussian graphical models.},
  archive      = {J_SAC},
  author       = {van den Boom, Willem and Jasra, Ajay and De Iorio, Maria and Beskos, Alexandros and Eriksson, Johan G.},
  doi          = {10.1007/s11222-022-10093-3},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Unbiased approximation of posteriors via coupled particle markov chain monte carlo},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Eigenfunction martingale estimating functions and filtered
data for drift estimation of discretely observed multiscale diffusions.
<em>SAC</em>, <em>32</em>(2), 1–33. (<a
href="https://doi.org/10.1007/s11222-022-10081-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for drift estimation of multiscale diffusion processes when a sequence of discrete observations is given. For the Langevin dynamics in a two-scale potential, our approach relies on the eigenvalues and the eigenfunctions of the homogenized dynamics. Our first estimator is derived from a martingale estimating function of the generator of the homogenized diffusion process. However, the unbiasedness of the estimator depends on the rate with which the observations are sampled. We therefore introduce a second estimator which relies also on filtering the data, and we prove that it is asymptotically unbiased independently of the sampling rate. A series of numerical experiments illustrate the reliability and efficiency of our different estimators.},
  archive      = {J_SAC},
  author       = {Abdulle, Assyr and Pavliotis, Grigorios A. and Zanoni, Andrea},
  doi          = {10.1007/s11222-022-10081-7},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-33},
  shortjournal = {Stat. Comput.},
  title        = {Eigenfunction martingale estimating functions and filtered data for drift estimation of discretely observed multiscale diffusions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cauchy markov random field priors for bayesian inversion.
<em>SAC</em>, <em>32</em>(2), 1–26. (<a
href="https://doi.org/10.1007/s11222-022-10089-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Cauchy Markov random field priors in statistical inverse problems can potentially lead to posterior distributions which are non-Gaussian, high-dimensional, multimodal and heavy-tailed. In order to use such priors successfully, sophisticated optimization and Markov chain Monte Carlo methods are usually required. In this paper, our focus is largely on reviewing recently developed Cauchy difference priors, while introducing interesting new variants, whilst providing a comparison. We firstly propose a one-dimensional second-order Cauchy difference prior, and construct new first- and second-order two-dimensional isotropic Cauchy difference priors. Another new Cauchy prior is based on the stochastic partial differential equation approach, derived from Matérn type Gaussian presentation. The comparison also includes Cauchy sheets. Our numerical computations are based on both maximum a posteriori and conditional mean estimation. We exploit state-of-the-art MCMC methodologies such as Metropolis-within-Gibbs, Repelling-Attracting Metropolis, and No-U-Turn sampler variant of Hamiltonian Monte Carlo. We demonstrate the models and methods constructed for one-dimensional and two-dimensional deconvolution problems. Thorough MCMC statistics are provided for all test cases, including potential scale reduction factors.},
  archive      = {J_SAC},
  author       = {Suuronen, Jarkko and Chada, Neil K. and Roininen, Lassi},
  doi          = {10.1007/s11222-022-10089-z},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Cauchy markov random field priors for bayesian inversion},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graphical test for discrete uniformity and its applications
in goodness-of-fit evaluation and multiple sample comparison.
<em>SAC</em>, <em>32</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-022-10090-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation- and optimization-based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample, which is useful, for example, for simulation-based calibration. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is useful, for example, as a complementary diagnostic in multi-chain Markov chain Monte Carlo (MCMC) convergence diagnostics, where most currently used convergence diagnostics provide a single diagnostic value, but do not usually offer insight into the nature of the deviation. We provide numerical experiments to assess the properties of the tests using both simulated and real-world data and give recommendations on their practical application in computational statistics workflows.},
  archive      = {J_SAC},
  author       = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
  doi          = {10.1007/s11222-022-10090-6},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast bayesian inversion for high dimensional inverse
problems. <em>SAC</em>, <em>32</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s11222-021-10019-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of learning approaches to handle Bayesian inverse problems in a computationally efficient way when the signals to be inverted present a moderately high number of dimensions and are in large number. We propose a tractable inverse regression approach which has the advantage to produce full probability distributions as approximations of the target posterior distributions. In addition to provide confidence indices on the predictions, these distributions allow a better exploration of inverse problems when multiple equivalent solutions exist. We then show how these distributions can be used for further refined predictions using importance sampling, while also providing a way to carry out uncertainty level estimation if necessary. The relevance of the proposed approach is illustrated both on simulated and real data in the context of a physical model inversion in planetary remote sensing. The approach shows interesting capabilities both in terms of computational efficiency and multimodal inference.},
  archive      = {J_SAC},
  author       = {Kugler, Benoit and Forbes, Florence and Douté, Sylvain},
  doi          = {10.1007/s11222-021-10019-5},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Fast bayesian inversion for high dimensional inverse problems},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse functional partial least squares regression with a
locally sparse slope function. <em>SAC</em>, <em>32</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-021-10066-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The partial least squares approach has been particularly successful in spectrometric prediction in chemometrics. By treating the spectral data as realizations of a stochastic process, the functional partial least squares can be applied. Motivated by the spectral data collected from oriented strand board furnish, we propose a sparse version of the functional partial least squares regression. The proposed method aims at achieving locally sparse (i.e., zero on certain sub-regions) estimates for the functional partial least squares bases, and more importantly, the locally sparse estimate for the slope function. The new approach applies a functional regularization technique to each iteration step of the functional partial least squares and implements a computational method that identifies nonzero sub-regions on which the slope function is estimated. We illustrate the proposed method with simulation studies and two applications on the oriented strand board furnish data and the particulate matter emissions data.},
  archive      = {J_SAC},
  author       = {Guan, Tianyu and Lin, Zhenhua and Groves, Kevin and Cao, Jiguo},
  doi          = {10.1007/s11222-021-10066-y},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Sparse functional partial least squares regression with a locally sparse slope function},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GP-ETAS: Semiparametric bayesian inference for the
spatio-temporal epidemic type aftershock sequence model. <em>SAC</em>,
<em>32</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11222-022-10085-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatio-temporal epidemic type aftershock sequence (ETAS) model is widely used to describe the self-exciting nature of earthquake occurrences. While traditional inference methods provide only point estimates of the model parameters, we aim at a fully Bayesian treatment of model inference, allowing naturally to incorporate prior knowledge and uncertainty quantification of the resulting estimates. Therefore, we introduce a highly flexible, non-parametric representation for the spatially varying ETAS background intensity through a Gaussian process (GP) prior. Combined with classical triggering functions this results in a new model formulation, namely the GP-ETAS model. We enable tractable and efficient Gibbs sampling by deriving an augmented form of the GP-ETAS inference problem. This novel sampling approach allows us to assess the posterior model variables conditioned on observed earthquake catalogues, i.e., the spatial background intensity and the parameters of the triggering function. Empirical results on two synthetic data sets indicate that GP-ETAS outperforms standard models and thus demonstrate the predictive power for observed earthquake catalogues including uncertainty quantification for the estimated parameters. Finally, a case study for the l’Aquila region, Italy, with the devastating event on 6 April 2009, is presented.},
  archive      = {J_SAC},
  author       = {Molkenthin, Christian and Donner, Christian and Reich, Sebastian and Zöller, Gert and Hainzl, Sebastian and Holschneider, Matthias and Opper, Manfred},
  doi          = {10.1007/s11222-022-10085-3},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {GP-ETAS: Semiparametric bayesian inference for the spatio-temporal epidemic type aftershock sequence model},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal scaling of random walk metropolis algorithms using
bayesian large-sample asymptotics. <em>SAC</em>, <em>32</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s11222-022-10080-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional limit theorems have been shown useful to derive tuning rules for finding the optimal scaling in random walk Metropolis algorithms. The assumptions under which weak convergence results are proved are, however, restrictive: the target density is typically assumed to be of a product form. Users may thus doubt the validity of such tuning rules in practical applications. In this paper, we shed some light on optimal scaling problems from a different perspective, namely a large-sample one. This allows to prove weak convergence results under realistic assumptions and to propose novel parameter-dimension-dependent tuning guidelines. The proposed guidelines are consistent with the previous ones when the target density is close to having a product form, and the results highlight that the correlation structure has to be accounted for to avoid performance deterioration if that is not the case, while justifying the use of a natural (asymptotically exact) approximation to the correlation matrix that can be employed for the very first algorithm run.},
  archive      = {J_SAC},
  author       = {Schmon, Sebastian M. and Gagnon, Philippe},
  doi          = {10.1007/s11222-022-10080-8},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Optimal scaling of random walk metropolis algorithms using bayesian large-sample asymptotics},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank tensor reconstruction of concentrated densities
with application to bayesian inversion. <em>SAC</em>, <em>32</em>(2),
1–27. (<a href="https://doi.org/10.1007/s11222-022-10087-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for the accurate functional approximation of possibly highly concentrated probability densities. It is based on the combination of several modern techniques such as transport maps and low-rank approximations via a nonintrusive tensor train reconstruction. The central idea is to carry out computations for statistical quantities of interest such as moments based on a convenient representation of a reference density for which accurate numerical methods can be employed. Since the transport from target to reference can usually not be determined exactly, one has to cope with a perturbed reference density due to a numerically approximated transport map. By the introduction of a layered approximation and appropriate coordinate transformations, the problem is split into a set of independent approximations in seperately chosen orthonormal basis functions, combining the notions h- and p-refinement (i.e. “mesh size” and polynomial degree). An efficient low-rank representation of the perturbed reference density is achieved via the Variational Monte Carlo method. This nonintrusive regression technique reconstructs the map in the tensor train format. An a priori convergence analysis with respect to the error terms introduced by the different (deterministic and statistical) approximations in the Hellinger distance and the Kullback–Leibler divergence is derived. Important applications are presented and in particular the context of Bayesian inverse problems is illuminated which is a main motivation for the developed approach. Several numerical examples illustrate the efficacy with densities of different complexity and degrees of perturbation of the transport to the reference density. The (superior) convergence is demonstrated in comparison to Monte Carlo and Markov Chain Monte Carlo methods.},
  archive      = {J_SAC},
  author       = {Eigel, Martin and Gruhlke, Robert and Marschall, Manuel},
  doi          = {10.1007/s11222-022-10087-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Low-rank tensor reconstruction of concentrated densities with application to bayesian inversion},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential changepoint detection in neural networks with
checkpoints. <em>SAC</em>, <em>32</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10088-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for online changepoint detection and simultaneous model learning which is applicable to highly parametrized models, such as deep neural networks. It is based on detecting changepoints across time by sequentially performing generalized likelihood ratio tests that require only evaluations of simple prediction score functions. This procedure makes use of checkpoints, consisting of early versions of the actual model parameters, that allow to detect distributional changes by performing predictions on future data. We define an algorithm that bounds the Type I error in the sequential testing procedure. We demonstrate the efficiency of our method in challenging continual learning applications with unknown task changepoints, and show improved performance compared to online Bayesian changepoint detection.},
  archive      = {J_SAC},
  author       = {Titsias, Michalis K. and Sygnowski, Jakub and Chen, Yutian},
  doi          = {10.1007/s11222-022-10088-0},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Sequential changepoint detection in neural networks with checkpoints},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent structure blockmodels for bayesian spectral graph
clustering. <em>SAC</em>, <em>32</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-022-10082-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral embedding of network adjacency matrices often produces node representations living approximately around low-dimensional submanifold structures. In particular, hidden substructure is expected to arise when the graph is generated from a latent position model. Furthermore, the presence of communities within the network might generate community-specific submanifold structures in the embedding, but this is not explicitly accounted for in most statistical models for networks. In this article, a class of models called latent structure block models (LSBM) is proposed to address such scenarios, allowing for graph clustering when community-specific one-dimensional manifold structure is present. LSBMs focus on a specific class of latent space model, the random dot product graph (RDPG), and assign a latent submanifold to the latent positions of each community. A Bayesian model for the embeddings arising from LSBMs is discussed, and shown to have a good performance on simulated and real-world network data. The model is able to correctly recover the underlying communities living in a one-dimensional manifold, even when the parametric form of the underlying curves is unknown, achieving remarkable results on a variety of real data.},
  archive      = {J_SAC},
  author       = {Sanna Passino, Francesco and Heard, Nicholas A.},
  doi          = {10.1007/s11222-022-10082-6},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Latent structure blockmodels for bayesian spectral graph clustering},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal bayesian design for model discrimination via
classification. <em>SAC</em>, <em>32</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-022-10078-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing optimal Bayesian design for discriminating between competing models is computationally intensive as it involves estimating posterior model probabilities for thousands of simulated data sets. This issue is compounded further when the likelihood functions for the rival models are computationally expensive. A new approach using supervised classification methods is developed to perform Bayesian optimal model discrimination design. This approach requires considerably fewer simulations from the candidate models than previous approaches using approximate Bayesian computation. Further, it is easy to assess the performance of the optimal design through the misclassification error rate. The approach is particularly useful in the presence of models with intractable likelihoods but can also provide computational advantages when the likelihoods are manageable.},
  archive      = {J_SAC},
  author       = {Hainy, Markus and Price, David J. and Restif, Olivier and Drovandi, Christopher},
  doi          = {10.1007/s11222-022-10078-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Optimal bayesian design for model discrimination via classification},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A numerically stable algorithm for integrating bayesian
models using markov melding. <em>SAC</em>, <em>32</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11222-022-10086-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When statistical analyses consider multiple data sources, Markov melding provides a method for combining the source-specific Bayesian models. Markov melding joins together submodels that have a common quantity. One challenge is that the prior for this quantity can be implicit, and its prior density must be estimated. We show that error in this density estimate makes the two-stage Markov chain Monte Carlo sampler employed by Markov melding unstable and unreliable. We propose a robust two-stage algorithm that estimates the required prior marginal self-density ratios using weighted samples, dramatically improving accuracy in the tails of the distribution. The stabilised version of the algorithm is pragmatic and provides reliable inference. We demonstrate our approach using an evidence synthesis for inferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.},
  archive      = {J_SAC},
  author       = {Manderson, Andrew A. and Goudie, Robert J. B.},
  doi          = {10.1007/s11222-022-10086-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A numerically stable algorithm for integrating bayesian models using markov melding},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the identifiability of bayesian factor analytic models.
<em>SAC</em>, <em>32</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s11222-022-10084-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well known identifiability issue in factor analytic models is the invariance with respect to orthogonal transformations. This problem burdens the inference under a Bayesian setup, where Markov chain Monte Carlo (MCMC) methods are used to generate samples from the posterior distribution. We introduce a post-processing scheme in order to deal with rotation, sign and permutation invariance of the MCMC sample. The exact version of the contributed algorithm requires to solve $$2^q$$ assignment problems per (retained) MCMC iteration, where q denotes the number of factors of the fitted model. For large numbers of factors two approximate schemes based on simulated annealing are also discussed. We demonstrate that the proposed method leads to interpretable posterior distributions using synthetic and publicly available data from typical factor analytic models as well as mixtures of factor analyzers. An R package is available online at CRAN web-page.},
  archive      = {J_SAC},
  author       = {Papastamoulis, Panagiotis and Ntzoufras, Ioannis},
  doi          = {10.1007/s11222-022-10084-4},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {On the identifiability of bayesian factor analytic models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented pseudo-marginal metropolis–hastings for partially
observed diffusion processes. <em>SAC</em>, <em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-022-10083-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of inference for nonlinear, multivariate diffusion processes, satisfying Itô stochastic differential equations (SDEs), using data at discrete times that may be incomplete and subject to measurement error. Our starting point is a state-of-the-art correlated pseudo-marginal Metropolis–Hastings algorithm, that uses correlated particle filters to induce strong and positive correlation between successive likelihood estimates. However, unless the measurement error or the dimension of the SDE is small, correlation can be eroded by the resampling steps in the particle filter. We therefore propose a novel augmentation scheme, that allows for conditioning on values of the latent process at the observation times, completely avoiding the need for resampling steps. We integrate over the uncertainty at the observation times with an additional Gibbs step. Connections between the resulting pseudo-marginal scheme and existing inference schemes for diffusion processes are made, giving a unified inference framework that encompasses Gibbs sampling and pseudo marginal schemes. The methodology is applied in three examples of increasing complexity. We find that our approach offers substantial increases in overall efficiency, compared to competing methods},
  archive      = {J_SAC},
  author       = {Golightly, Andrew and Sherlock, Chris},
  doi          = {10.1007/s11222-022-10083-5},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Augmented pseudo-marginal Metropolis–Hastings for partially observed diffusion processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Point process simulation of generalised inverse gaussian
processes and estimation of the jaeger integral. <em>SAC</em>,
<em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10072-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper novel simulation methods are provided for the generalised inverse Gaussian (GIG) Lévy process. Such processes are intractable for simulation except in certain special edge cases, since the Lévy density associated with the GIG process is expressed as an integral involving certain Bessel functions, known as the Jaeger integral in diffusive transport applications. We here show for the first time how to solve the problem indirectly, using generalised shot-noise methods to simulate the underlying point processes and constructing an auxiliary variables approach that avoids any direct calculation of the integrals involved. The resulting augmented bivariate process is still intractable and so we propose a novel thinning method based on upper bounds on the intractable integrand. Moreover, our approach leads to lower and upper bounds on the Jaeger integral itself, which may be compared with other approximation methods. The shot noise method involves a truncated infinite series of decreasing random variables, and as such is approximate, although the series are found to be rapidly convergent in most cases. We note that the GIG process is the required Brownian motion subordinator for the generalised hyperbolic (GH) Lévy process and so our simulation approach will straightforwardly extend also to the simulation of these intractable processes. Our new methods will find application in forward simulation of processes of GIG and GH type, in financial and engineering data, for example, as well as inference for states and parameters of stochastic processes driven by GIG and GH Lévy processes.},
  archive      = {J_SAC},
  author       = {Godsill, Simon and Kındap, Yaman},
  doi          = {10.1007/s11222-021-10072-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Point process simulation of generalised inverse gaussian processes and estimation of the jaeger integral},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MALA with annealed proposals: A generalization of locally
and globally balanced proposal distributions. <em>SAC</em>,
<em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10063-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a generalized version of the Metropolis-adjusted Langevin algorithm (MALA). The informed proposal distribution of this new sampler features two tuning parameters: the usual step size parameter $$\sigma ^2$$ and an interpolation parameter $$\gamma $$ that may be adjusted to accommodate the dimension of the target distribution. We theoretically study the efficiency of the sampler by making use of the local- and global-balance concepts introduced in Zanella (JASA 115:852–865, 2020) and provide efficient tuning guidelines that work well with a variety of target distributions. Although the usual MALA ( $$\gamma =1$$ ) is shown to be optimal for infinite-dimensional targets, in practice, the generalized MALA ( $$1&lt;\gamma \le 2$$ ) remains the most appealing option, even in high-dimensional contexts. Simulation studies and numerical experiments are presented to illustrate our findings. We apply the new sampler to a Bayesian logistic regression context and show that its efficiency compares favourably to competing algorithms.},
  archive      = {J_SAC},
  author       = {Boisvert-Beaudry, Gabriel and Bédard, Mylène},
  doi          = {10.1007/s11222-021-10063-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {MALA with annealed proposals: A generalization of locally and globally balanced proposal distributions},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble kalman filter based sequential monte carlo sampler
for sequential bayesian inference. <em>SAC</em>, <em>32</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s11222-021-10075-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems require one to estimate parameters of interest, in a Bayesian framework, from data that are collected sequentially in time. Conventional methods for sampling from posterior distributions, such as Markov chain Monte Carlo cannot efficiently address such problems as they do not take advantage of the data’s sequential structure. To this end, sequential methods which seek to update the posterior distribution whenever a new collection of data become available are often used to solve these types of problems. Two popular choices of sequential method are the ensemble Kalman filter (EnKF) and the sequential Monte Carlo sampler (SMCS). While EnKF only computes a Gaussian approximation of the posterior distribution, SMCS can draw samples directly from the posterior. Its performance, however, depends critically upon the kernels that are used. In this work, we present a method that constructs the kernels of SMCS using an EnKF formulation, and we demonstrate the performance of the method with numerical examples.},
  archive      = {J_SAC},
  author       = {Wu, Jiangqi and Wen, Linjie and Green, Peter L. and Li, Jinglai and Maskell, Simon},
  doi          = {10.1007/s11222-021-10075-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Ensemble kalman filter based sequential monte carlo sampler for sequential bayesian inference},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph matching beyond perfectly-overlapping erdős–rényi
random graphs. <em>SAC</em>, <em>32</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-022-10079-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching is a fruitful area in terms of both algorithms and theories. Given two graphs $$G_1 = (V_1, E_1)$$ and $$G_2 = (V_2, E_2)$$ , where $$V_1$$ and $$V_2$$ are the same or largely overlapped upon an unknown permutation $$\pi ^*$$ , graph matching is to seek the correct mapping $$\pi ^*$$ . In this paper, we exploit the degree information, which was previously used only in noiseless graphs and perfectly-overlapping Erdős–Rényi random graphs matching. We are concerned with graph matching of partially-overlapping graphs and stochastic block models, which are more useful in tackling real-life problems. We propose the edge exploited degree profile graph matching method and two refined variations. We conduct a thorough analysis of our proposed methods’ performances in a range of challenging scenarios, including coauthorship data set and a zebrafish neuron activity data set. Our methods are proved to be numerically superior than the state-of-the-art methods. The algorithms are implemented in the R (A language and environment for statistical computing, R Foundation for Statistical Computing, Vienna, 2020) package GMPro (GMPro: graph matching with degree profiles, 2020).},
  archive      = {J_SAC},
  author       = {Hu, Yaofang and Wang, Wanjie and Yu, Yi},
  doi          = {10.1007/s11222-022-10079-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Graph matching beyond perfectly-overlapping Erdős–Rényi random graphs},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to: A two-stage bayesian semiparametricmodel for
novelty detection with robust prior information. <em>SAC</em>,
<em>32</em>(1), 1. (<a
href="https://doi.org/10.1007/s11222-021-10028-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Denti, Francesco and Cappozzo, Andrea and Greselin, Francesca},
  doi          = {10.1007/s11222-021-10028-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction to: A two-stage bayesian semiparametricmodel for novelty detection with robust prior information},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative clustering with representation learning with
any ratio of labeled to unlabeled data. <em>SAC</em>, <em>32</em>(1),
1–24. (<a href="https://doi.org/10.1007/s11222-021-10067-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a discriminative clustering approach in which the feature representation can be learned from data and moreover leverage labeled data. Representation learning can give a similarity-based clustering method the ability to automatically adapt to an underlying, yet hidden, geometric structure of the data. The proposed approach augments the DIFFRAC method with a representation learning capability, using a gradient-based stochastic training algorithm and an optimal transport algorithm with entropic regularization to perform the cluster assignment step. The resulting method is evaluated on several real datasets when varying the ratio of labeled data to unlabeled data and thereby interpolating between the fully unsupervised regime and the fully supervised regime. The experimental results suggest that the proposed method can learn powerful feature representations even in the fully unsupervised regime and can leverage even small amounts of labeled data to improve the feature representations and to obtain better clusterings of complex datasets.},
  archive      = {J_SAC},
  author       = {Jones, Corinne and Roulet, Vincent and Harchaoui, Zaid},
  doi          = {10.1007/s11222-021-10067-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Discriminative clustering with representation learning with any ratio of labeled to unlabeled data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact and computationally efficient bayesian inference for
generalized markov modulated poisson processes. <em>SAC</em>,
<em>32</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11222-021-10074-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modeling of temporal point patterns is an important problem in several areas. The Cox process, a Poisson process where the intensity function is stochastic, is a common model for such data. We present a new class of unidimensional Cox process models in which the intensity function assumes parametric functional forms that switch according to a continuous-time Markov chain. A novel methodology is introduced to perform exact (up to Monte Carlo error) Bayesian inference based on MCMC algorithms. The reliability of the algorithms depends on a variety of specifications which are carefully addressed, resulting in a computationally efficient (in terms of computing time) algorithm and enabling its use with large data sets. Simulated and real examples are presented to illustrate the efficiency and applicability of the methodology. A specific model to fit epidemic curves is proposed and used to analyze data from Dengue Fever in Brazil and COVID-19 in some countries.},
  archive      = {J_SAC},
  author       = {Gonçalves, Flávio B. and Dutra, Lívia M. and Silva, Roger W. C.},
  doi          = {10.1007/s11222-021-10074-y},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Exact and computationally efficient bayesian inference for generalized markov modulated poisson processes},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The recursive variational gaussian approximation (r-VGA).
<em>SAC</em>, <em>32</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11222-021-10068-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of computing a Gaussian approximation to the posterior distribution of a parameter given N observations and a Gaussian prior. Owing to the need of processing large sample sizes N, a variety of approximate tractable methods revolving around online learning have flourished over the past decades. In the present work, we propose to use variational inference to compute a Gaussian approximation to the posterior through a single pass over the data. Our algorithm is a recursive version of variational Gaussian approximation we have called recursive variational Gaussian approximation. We start from the prior, and for each observation, we compute the nearest Gaussian approximation in the sense of Kullback–Leibler divergence to the posterior given this observation. In turn, this approximation is considered as the new prior when incorporating the next observation. This recursive version based on a sequence of optimal Gaussian approximations leads to a novel implicit update scheme which resembles the online Newton algorithm and which is shown to boil down to the Kalman filter for Bayesian linear regression. In the context of Bayesian logistic regression, the implicit scheme may be solved, and the algorithm is shown to perform better than the extended Kalman filter, while being less computationally demanding than its sampling counterparts.},
  archive      = {J_SAC},
  author       = {Lambert, Marc and Bonnabel, Silvère and Bach, Francis},
  doi          = {10.1007/s11222-021-10068-w},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {The recursive variational gaussian approximation (R-VGA)},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variance reduction for additive functionals of markov chains
via martingale representations. <em>SAC</em>, <em>32</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-021-10073-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient variance reduction approach for additive functionals of Markov chains relying on a novel discrete-time martingale representation. Our approach is fully non-asymptotic and does not require the knowledge of the stationary distribution (and even any type of ergodicity) or specific structure of the underlying density. By rigorously analyzing the convergence properties of the proposed algorithm, we show that its cost-to-variance product is indeed smaller than one of the naive algorithms. The numerical performance of the new method is illustrated for the Langevin-type Markov chain Monte Carlo (MCMC) methods.},
  archive      = {J_SAC},
  author       = {Belomestny, D. and Moulines, E. and Samsonov, S.},
  doi          = {10.1007/s11222-021-10073-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Variance reduction for additive functionals of markov chains via martingale representations},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Product-form estimators: Exploiting independence to scale up
monte carlo. <em>SAC</em>, <em>32</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-021-10069-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a class of Monte Carlo estimators that aim to overcome the rapid growth of variance with dimension often observed for standard estimators by exploiting the target’s independence structure. We identify the most basic incarnations of these estimators with a class of generalized U-statistics and thus establish their unbiasedness, consistency, and asymptotic normality. Moreover, we show that they obtain the minimum possible variance amongst a broad class of estimators, and we investigate their computational cost and delineate the settings in which they are most efficient. We exemplify the merger of these estimators with other well known Monte Carlo estimators so as to better adapt the latter to the target’s independence structure and improve their performance. We do this via three simple mergers: one with importance sampling, another with importance sampling squared, and a final one with pseudo-marginal Metropolis–Hastings. In all cases, we show that the resulting estimators are well founded and achieve lower variances than their standard counterparts. Lastly, we illustrate the various variance reductions through several examples.},
  archive      = {J_SAC},
  author       = {Kuntz, Juan and Crucinio, Francesca R. and Johansen, Adam M.},
  doi          = {10.1007/s11222-021-10069-9},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Product-form estimators: Exploiting independence to scale up monte carlo},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An explicit split point procedure in model-based trees
allowing for a quick fitting of GLM trees and GLM forests. <em>SAC</em>,
<em>32</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11222-021-10059-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification and regression trees (CART) prove to be a true alternative to full parametric models such as linear models (LM) and generalized linear models (GLM). Although CART suffer from a biased variable selection issue, they are commonly applied to various topics and used for tree ensembles and random forests because of their simplicity and computation speed. Conditional inference trees and model-based trees algorithms for which variable selection is tackled via fluctuation tests are known to give more accurate and interpretable results than CART, but yield longer computation times. Using a closed-form maximum likelihood estimator for GLM, this paper proposes a split point procedure based on the explicit likelihood in order to save time when searching for the best split for a given splitting variable. A simulation study for non-Gaussian response is performed to assess the computational gain when building GLM trees. We also propose a benchmark on simulated and empirical datasets of GLM trees against CART, conditional inference trees and LM trees in order to identify situations where GLM trees are efficient. This approach is extended to multiway split trees and log-transformed distributions. Making GLM trees possible through a new split point procedure allows us to investigate the use of GLM in ensemble methods. We propose a numerical comparison of GLM forests against other random forest-type approaches. Our simulation analyses show cases where GLM forests are good challengers to random forests.},
  archive      = {J_SAC},
  author       = {Dutang, Christophe and Guibert, Quentin},
  doi          = {10.1007/s11222-021-10059-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {An explicit split point procedure in model-based trees allowing for a quick fitting of GLM trees and GLM forests},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical sparse cholesky decomposition with applications
to high-dimensional spatio-temporal filtering. <em>SAC</em>,
<em>32</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11222-021-10077-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial statistics often involves Cholesky decomposition of covariance matrices. To ensure scalability to high dimensions, several recent approximations have assumed a sparse Cholesky factor of the precision matrix. We propose a hierarchical Vecchia approximation, whose conditional-independence assumptions imply sparsity in the Cholesky factors of both the precision and the covariance matrix. This remarkable property is crucial for applications to high-dimensional spatiotemporal filtering. We present a fast and simple algorithm to compute our hierarchical Vecchia approximation, and we provide extensions to nonlinear data assimilation with non-Gaussian data based on the Laplace approximation. In several numerical comparisons, including a filtering analysis of satellite data, our methods strongly outperformed alternative approaches.},
  archive      = {J_SAC},
  author       = {Jurek, Marcin and Katzfuss, Matthias},
  doi          = {10.1007/s11222-021-10077-9},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Hierarchical sparse cholesky decomposition with applications to high-dimensional spatio-temporal filtering},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet-based robust estimation and variable selection in
nonparametric additive models. <em>SAC</em>, <em>32</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11222-021-10065-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies M-type estimators for fitting robust additive models in the presence of anomalous data. The components in the additive model are allowed to have different degrees of smoothness. We introduce a new class of wavelet-based robust M-type estimators for performing simultaneous additive component estimation and variable selection in such inhomogeneous additive models. Each additive component is approximated by a truncated series expansion of wavelet bases, making it feasible to apply the method to nonequispaced data and sample sizes that are not necessarily a power of 2. Sparsity of the additive components together with sparsity of the wavelet coefficients within each component (group), results into a bi-level group variable selection problem. In this framework, we discuss robust estimation and variable selection. A two-stage computational algorithm, consisting of a fast accelerated proximal gradient algorithm of coordinate descend type, and thresholding, is proposed. When using nonconvex redescending loss functions, and appropriate nonconvex penalty functions at the group level, we establish optimal convergence rates of the estimates. We prove variable selection consistency under a weak compatibility condition for sparse additive models. The theoretical results are complemented with some simulations and real data analysis, as well as a comparison to other existing methods.},
  archive      = {J_SAC},
  author       = {Amato, Umberto and Antoniadis, Anestis and Feis, Italia De and Gijbels, Irène},
  doi          = {10.1007/s11222-021-10065-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Wavelet-based robust estimation and variable selection in nonparametric additive models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from missing data with the binary latent block
model. <em>SAC</em>, <em>32</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-021-10058-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data can be informative. Ignoring this information can lead to misleading conclusions when the data model does not allow information to be extracted from the missing data. We propose a co-clustering model, based on the binary Latent Block Model, that aims to take advantage of this nonignorable nonresponses, also known as Missing Not At Random data. A variational expectation–maximization algorithm is derived to perform inference and a model selection criterion is presented. We assess the proposed approach on a simulation study, before using our model on the voting records from the lower house of the French Parliament, where our analysis brings out relevant groups of MPs and texts, together with a sensible interpretation of the behavior of non-voters.},
  archive      = {J_SAC},
  author       = {Frisch, Gabriel and Leger, Jean-Benoist and Grandvalet, Yves},
  doi          = {10.1007/s11222-021-10058-y},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Learning from missing data with the binary latent block model},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A riemannian newton trust-region method for fitting gaussian
mixture models. <em>SAC</em>, <em>32</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10071-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Mixture Models are a powerful tool in Data Science and Statistics that are mainly used for clustering and density approximation. The task of estimating the model parameters is in practice often solved by the expectation maximization (EM) algorithm which has its benefits in its simplicity and low per-iteration costs. However, the EM converges slowly if there is a large share of hidden information or overlapping clusters. Recent advances in Manifold Optimization for Gaussian Mixture Models have gained increasing interest. We introduce an explicit formula for the Riemannian Hessian for Gaussian Mixture Models. On top, we propose a new Riemannian Newton Trust-Region method which outperforms current approaches both in terms of runtime and number of iterations. We apply our method on clustering problems and density approximation tasks. Our method is very powerful for data with a large share of hidden information compared to existing methods.},
  archive      = {J_SAC},
  author       = {Sembach, Lena and Burgard, Jan Pablo and Schulz, Volker},
  doi          = {10.1007/s11222-021-10071-1},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {A riemannian newton trust-region method for fitting gaussian mixture models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic approximation cut algorithm for inference in
modularized bayesian models. <em>SAC</em>, <em>32</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-021-10070-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian modelling enables us to accommodate complex forms of data and make a comprehensive inference, but the effect of partial misspecification of the model is a concern. One approach in this setting is to modularize the model and prevent feedback from suspect modules, using a cut model. After observing data, this leads to the cut distribution which normally does not have a closed form. Previous studies have proposed algorithms to sample from this distribution, but these algorithms have unclear theoretical convergence properties. To address this, we propose a new algorithm called the stochastic approximation cut (SACut) algorithm as an alternative. The algorithm is divided into two parallel chains. The main chain targets an approximation to the cut distribution; the auxiliary chain is used to form an adaptive proposal distribution for the main chain. We prove convergence of the samples drawn by the proposed algorithm and present the exact limit. Although SACut is biased, since the main chain does not target the exact cut distribution, we prove this bias can be reduced geometrically by increasing a user-chosen tuning parameter. In addition, parallel computing can be easily adopted for SACut, which greatly reduces computation time.},
  archive      = {J_SAC},
  author       = {Liu, Yang and Goudie, Robert J. B.},
  doi          = {10.1007/s11222-021-10070-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Stochastic approximation cut algorithm for inference in modularized bayesian models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Erlang mixture modeling for poisson process intensities.
<em>SAC</em>, <em>32</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-021-10064-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a prior probability model for temporal Poisson process intensities through structured mixtures of Erlang densities with common scale parameter, mixing on the integer shape parameters. The mixture weights are constructed through increments of a cumulative intensity function which is modeled nonparametrically with a gamma process prior. Such model specification provides a novel extension of Erlang mixtures for density estimation to the intensity estimation setting. The prior model structure supports general shapes for the point process intensity function, and it also enables effective handling of the Poisson process likelihood normalizing term resulting in efficient posterior simulation. The Erlang mixture modeling approach is further elaborated to develop an inference method for spatial Poisson processes. The methodology is examined relative to existing Bayesian nonparametric modeling approaches, including empirical comparison with Gaussian process prior based models, and is illustrated with synthetic and real data examples.},
  archive      = {J_SAC},
  author       = {Kim, Hyotae and Kottas, Athanasios},
  doi          = {10.1007/s11222-021-10064-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Erlang mixture modeling for poisson process intensities},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained parsimonious model-based clustering.
<em>SAC</em>, <em>32</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-021-10061-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new methodology for constrained parsimonious model-based clustering is introduced, where some tuning parameter allows to control the strength of these constraints. The methodology includes the 14 parsimonious models that are often applied in model-based clustering when assuming normal components as limit cases. This is done in a natural way by filling the gap among models and providing a smooth transition among them. The methodology provides mathematically well-defined problems and is also useful to prevent us from obtaining spurious solutions. Novel information criteria are proposed to help the user in choosing parameters. The interest of the proposed methodology is illustrated through simulation studies and a real-data application on COVID data.},
  archive      = {J_SAC},
  author       = {García-Escudero, Luis A. and Mayo-Iscar, Agustín and Riani, Marco},
  doi          = {10.1007/s11222-021-10061-3},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Constrained parsimonious model-based clustering},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Updating variational bayes: Fast sequential posterior
inference. <em>SAC</em>, <em>32</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s11222-021-10062-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper, we propose the use of recursive algorithms to update a sequence of VB posterior approximations in an online, time series setting, with the computation of each posterior update requiring only the data observed since the previous update. We show how importance sampling can be incorporated into online variational inference allowing the user to trade accuracy for a substantial increase in computational speed. The proposed methods and their properties are detailed in two separate simulation studies. Additionally, two empirical illustrations are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.},
  archive      = {J_SAC},
  author       = {Tomasetti, Nathaniel and Forbes, Catherine and Panagiotelis, Anastasios},
  doi          = {10.1007/s11222-021-10062-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Updating variational bayes: Fast sequential posterior inference},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emulation-accelerated hamiltonian monte carlo algorithms for
parameter estimation and uncertainty quantification in differential
equation models. <em>SAC</em>, <em>32</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11222-021-10060-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to accelerate Hamiltonian and Lagrangian Monte Carlo algorithms by coupling them with Gaussian processes for emulation of the log unnormalised posterior distribution. We provide proofs of detailed balance with respect to the exact posterior distribution for these algorithms, and validate the correctness of the samplers’ implementation by Geweke consistency tests. We implement these algorithms in a delayed acceptance (DA) framework, and investigate whether the DA scheme can offer computational gains over the standard algorithms. A comparative evaluation study is carried out to assess the performance of the methods on a series of models described by differential equations, including a real-world application of a 1D fluid-dynamics model of the pulmonary blood circulation. The aim is to identify the algorithm which gives the best trade-off between accuracy and computational efficiency, to be used in nonlinear DE models, which are computationally onerous due to repeated numerical integrations in a Bayesian analysis. Results showed no advantage of the DA scheme over the standard algorithms with respect to several efficiency measures based on the effective sample size for most methods and DE models considered. These gradient-driven algorithms register a high acceptance rate, thus the number of expensive forward model evaluations is not significantly reduced by the first emulator-based stage of DA. Additionally, the Lagrangian Dynamical Monte Carlo and Riemann Manifold Hamiltonian Monte Carlo tended to register the highest efficiency (in terms of effective sample size normalised by the number of forward model evaluations), followed by the Hamiltonian Monte Carlo, and the No U-turn sampler tended to be the least efficient.},
  archive      = {J_SAC},
  author       = {Paun, L. Mihaela and Husmeier, Dirk},
  doi          = {10.1007/s11222-021-10060-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Emulation-accelerated hamiltonian monte carlo algorithms for parameter estimation and uncertainty quantification in differential equation models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
