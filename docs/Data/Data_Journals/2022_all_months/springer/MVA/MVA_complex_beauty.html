<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---97">MVA - 97</h2>
<ul>
<li><details>
<summary>
(2022). Graph convolutional networks and LSTM for first-person
multimodal hand action recognition. <em>MVA</em>, <em>33</em>(6), 1–16.
(<a href="https://doi.org/10.1007/s00138-022-01328-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been successfully introduced in skeleton-based human action recognition. Both human skeletons and hand skeletons are composed of open-loop chains, and each chain is composed of rigid links (corresponding to bones) and revolving pairs (corresponding to joints). Despite this similarity, there has been no skeleton-based hand action recognition method that represents hand skeletons using GCNs. We first evaluate the effectiveness of traditional spatial–temporal GCNs for skeleton-based hand action recognition. Then, we propose to improve the traditional spatial–temporal GCNs by incorporating the third-order node information (geometric relationships between neighbor connected bones in a hand skeleton), and the geometric relationships are described by a Lie group, including relative translations and rotations. Finally, we study first-person multimodal hand action recognition with hand skeletons, RGB images, and depth maps jointly used as visual input. We propose to fuse the multimodal features by customized long short-term memory (LSTM) units, rather than simply concatenating them as a feature vector. Extensive ablation studies are conducted to demonstrate the improvements due to the use of the third-order node information and the advantages of our multimodal fusion strategy. Our method markedly outperforms recent baselines on a public first-person hand action recognition dataset.},
  archive      = {J_MVA},
  author       = {Li, Rui and Wang, Hongyu},
  doi          = {10.1007/s00138-022-01328-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Graph convolutional networks and LSTM for first-person multimodal hand action recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond homography: Nonparametric image alignment via graph
convolutional networks. <em>MVA</em>, <em>33</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01331-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an image alignment algorithm based on weak supervision, which aims to identify the correspondence between a pair of reference and target images with no supervision of individual pixels. Since most existing methods have relied on a predefined geometric model such as homography, they often suffer from a lack of model flexibility and generalizability. To tackle the challenge, we propose a novel nonparametric transformation model based on graph convolutional networks without an explicit geometric constraint. The proposed method is generic and flexible in the sense that it is applicable to the image pairs undergoing diverse local and/or global transformations. To make the algorithm more suitable for real-world scenarios having potential noises from moving objects, we disregard those objects with an off-the-shelf semantic segmentation model. The proposed algorithm is evaluated on the Cityscapes dataset with annotated pixel-level correspondences and outperforms baseline methods relying on global parametric transformations.},
  archive      = {J_MVA},
  author       = {Kim, Mijeong and Chu, Sanghyeok and Han, Bohyung},
  doi          = {10.1007/s00138-022-01331-9},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Beyond homography: Nonparametric image alignment via graph convolutional networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of stereo vision baseline and effects of canopy
structure, pre-processing and imaging parameters for 3D reconstruction
of trees. <em>MVA</em>, <em>33</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01333-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision farming requires tree-canopy information for better management. Stereo vision is the technique to create a 3D model, and it needs to be adequately setup to avoid extreme data processing and unreliable results. Features detection is very important. Different parameters affect features in images. Because 3D accuracy is necessary, this study focused to investigate the effects of various baselines of a stereo camera on the well-known combination of feature detectors and descriptors and optimization of a stereo-vision-system for obtaining 3D-model of tree-canopy. Also, the effects of different parameters were investigated in RGB and Y color spaces. These parameters were three levels of density, two shapes of canopy (conic and ellipse), image rectification and un-distortion, metering mode, exposure time and ISO speed. The results showed that the best system was stereo-system with baseline of 12 cm and the best combination was SURF-BRISK. Also, SURF-FREAK and SURF-SURF combinations were appropriate afterwards. The precision value was 1 for the SURF-BRISK combination in the system with the baseline of 12 cm. The parameters including image rectification, metering mode, exposure time and ISO speed were affected by combinations performance. Images must be rectified before the implementation of detector algorithms. Use of the pattern mode and same exposure times and ISO speeds for both pair images were better. The recall values were decreased for various exposure times and ISO speeds. The results of algorithms were not affected by the tree-canopy shapes and density. So results can be used successfully for trees with larger size and different shapes and densities.},
  archive      = {J_MVA},
  author       = {Jafari Malekabadi, Ayoub and Khojastehpour, Mehdi},
  doi          = {10.1007/s00138-022-01333-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Optimization of stereo vision baseline and effects of canopy structure, pre-processing and imaging parameters for 3D reconstruction of trees},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). U-shaped spatial–temporal transformer network for 3D human
pose estimation. <em>MVA</em>, <em>33</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01334-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose estimation has achieved much progress with the development of convolution neural networks. There still have some challenges to accurately estimate 3D joint locations from single-view images or videos due to depth ambiguity and severe occlusion. Motivated by the effectiveness of introducing vision transformer into computer vision tasks, we present a novel U-shaped spatial–temporal transformer-based network (U-STN) for 3D human pose estimation. The core idea of the proposed method is to process the human joints by designing a multi-scale and multi-level U-shaped transformer model. We construct a multi-scale architecture with three different scales based on the human skeletal topology, in which the local and global features are processed through three different scales with kinematic constraints. Furthermore, a multi-level feature representations is introduced by fusing intermediate features from different depths of the U-shaped network. With a skeletal constrained pooling and unpooling operations devised for U-STN, the network can transform features across different scales and extract meaningful semantic features at all levels. Experiments on two challenging benchmark datasets show that the proposed method achieves a good performance on 2D-to-3D pose estimation. The code is available at https://github.com/l-fay/Pose3D .},
  archive      = {J_MVA},
  author       = {Yang, Honghong and Guo, Longfei and Zhang, Yumei and Wu, Xiaojun},
  doi          = {10.1007/s00138-022-01334-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {U-shaped spatial–temporal transformer network for 3D human pose estimation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolution algorithm of parametric active contour model based
on gaussian smoothing filter. <em>MVA</em>, <em>33</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s00138-022-01336-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the parametric active contour model is one of the most well-known and widely used image segmentation techniques in image processing and computer vision. However, its evolution computation is slow, which is a great obstacle to some applications such as real-time motion tracking. This paper not only reveals its bottleneck including the high computation cost of the inverse operation of matrix and the matrix multiplication in each iteration, but also proposes a novel scheme that transfers these time-consuming matrix operations into vector convolution operations for better performance. As shown by simulation results the proposed algorithm is always much faster than the conventional algorithm, and the velocity gain increases with the snaxels on the curve, from several times to over 2 orders of magnitude.},
  archive      = {J_MVA},
  author       = {Tang, Kelun and Zhou, Xiaojun},
  doi          = {10.1007/s00138-022-01336-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Evolution algorithm of parametric active contour model based on gaussian smoothing filter},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale convolution underwater image restoration
network. <em>MVA</em>, <em>33</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01337-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complex underwater imaging environment and illumination conditions, underwater images have some quality degradation problems, such as low contrast, color distortion, texture blur and uneven illumination, which seriously restrict the application in underwater work. In order to solve these problems, we proposed a multi-scale feature fusion CNN based on underwater imaging model in this paper called Multi-Scale Convolution Underwater Image Restoration Network (MSCUIR-Net). Unlike most previous models that estimated the background light and transmittance, respectively, our model unifies the two parameters into one, predicts the univariate linear physical model through lightweight CNN, and directly generates end-to-end clean images. Based on the underwater imaging model, we synthesized the underwater image training set can simulate the shallow water to deep water environment. Then, we do experiments on synthetic images and real underwater images, and prove the superiority of this method through image evaluation indexes. The experimental results show that MSCUIR-Net has a good effect on underwater image restoration.},
  archive      = {J_MVA},
  author       = {Tang, Zhijie and Li, Jianda and Huang, Jingke and Wang, Zhanhua and Luo, Zhihang},
  doi          = {10.1007/s00138-022-01337-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-scale convolution underwater image restoration network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ising granularity image analysis on VAE–GAN. <em>MVA</em>,
<em>33</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01338-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a variational autoencoder (VAE) and a VAE-generative adversarial net (GAN) trained to generate from 12000 Ising granularity images, new and appropriate images, which can retain the former $${}&#39;s$$ global chaotic structure to some extent. Via VAE, we project high-dimensional Ising granularity images onto a two-dimensional latent space in which some spatial distribution patterns are explored. The observed particles in latent space electronic cloud are similar to that of the quantum dynamics integrable pattern. The resulting VAE latent space is a new measurement space to explore both the spatial particle distribution patterns and the structural topology clusters, leading to recognition of new classification/clustering patterns of the physical state/phase, which extend those found via traditional approaches which consider pixels of an image as physical particles. In addition, we propose a multiple-level structural similarity image quality assessment (IQA) scheme to measure inter- and intra-patch similarities on VAE and VAE–GAN generate images when they are split into patches. The results show that this novel IQA scheme can both maximize the distances of the samples among inter-classes and minimize those of the intra-classes, without compromising the image fidelity and features.},
  archive      = {J_MVA},
  author       = {Chen, Guoming and Long, Shun and Yuan, Zeduo and Zhu, Weiheng and Chen, Qiang and Wu, Yilin},
  doi          = {10.1007/s00138-022-01338-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ising granularity image analysis on VAE–GAN},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinematic calibration of a hexapod robot based on monocular
vision. <em>MVA</em>, <em>33</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01339-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot kinematic calibration is an effective way to reduce the errors of kinematic parameters and improve the positioning accuracy of a robot. This paper presents a cost-effective kinematic calibration method for a hexapod robot that only needs a monocular camera and two planar markers. The markers are attached to the body and the foot-tip of the robot separately, and the robot’s six legs are calibrated one by one. The kinematic model and error model of the robot are established based on the local product of exponential (POE) model, and the calibration task is formulated as a nonlinear least squares problem where 24 unknown parameters are estimated for each leg. The proposed calibration procedure is successfully evaluated on a real hexapod robot, and the experimental results show that the robot can have a better walking performance after calibration.},
  archive      = {J_MVA},
  author       = {Wang, Qian and Jin, Bo and Zhang, Ce},
  doi          = {10.1007/s00138-022-01339-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Kinematic calibration of a hexapod robot based on monocular vision},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspectral image classification based on clustering
dimensionality reduction and multi-scale feature fusion. <em>MVA</em>,
<em>33</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01340-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSI) contain rich ground object information, which has great potential in classification. However, the large amount of data and noise also pose a challenge to HSI classification. In this paper, a new framework based on band selection and multi-scale structure features is proposed, which mainly consists of the following steps. Firstly, the spectral dimension of the HSI is reduced with the clustering average method based on information divergence. Secondly, the detailed multi-scale structure features of HSI are extracted by using multi-parameter relative total variation. Thirdly, in order to reduce noise and highlight structural features, bilateral filtering is used to fine-tune the extracted structural features. Finally, the improved quantum particle swarm optimization algorithm is proposed to optimize the parameters of SVM. A lot of experiment results on two hyperspectral datasets show that the proposed method performs better than several state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Wang, Cailing and Song, Xiaonan and Zhang, Jing},
  doi          = {10.1007/s00138-022-01340-8},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hyperspectral image classification based on clustering dimensionality reduction and multi-scale feature fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Family classification and kinship verification from facial
images in the wild. <em>MVA</em>, <em>33</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01341-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification from facial images in the wild based on one-to-one classification has gathered a promising attention by image processing and computer vision researchers. While family classification based on one-to-many classification is relatively the least explored domain in computer vision. This paper first performs family classification on different family-sets based on number of family members. Second, we perform kinship verification on different kinship relations covering parent–child and siblings. We present a new kinship database named KinIndian dedicated for these two tasks of family classification and kinship verification. KinIndian database comprises 1926 images of 813 individuals from 230 unique Indian families with 2–7 members. KinIndian is designed into two levels: the first is family-level for family classification, and the second is photo-level for kinship verification. We propose a novel weighted nearest member metric leaning (WNMML) method to evaluate family classification on different family-sets. Proposed WNMML method is based on minimizing intraclass separation by characterizing compactness for positive families and maximizing interclass separation by pushing members of negative families as far as possible. WNMML achieves competitive accuracy on different family-sets and hence shows that WNMML could be effectively used in real-world scenarios. Furthermore, we also perform kinship verification on KinIndian using baseline multimetric learning methods and achieves promising and encouraging kinship accuracy.},
  archive      = {J_MVA},
  author       = {Goyal, Aarti and Meenpal, Toshanlal and Mukherjee, Moumita},
  doi          = {10.1007/s00138-022-01341-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Family classification and kinship verification from facial images in the wild},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Welding splash and arc noise reduction imaging model based
on computationally efficient pairwise response serving welding process
library. <em>MVA</em>, <em>33</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01342-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of vision-assisted robot automatic welding, splash and arc noise will interfere with the useful information in the image, which has a serious impact on the subsequent vision-based weld seam recognition and tracking algorithm. Although various methods have been proposed to achieve noise reduction, those are implemented from a perspective of image processing or intelligent algorithm. In this paper, we propose a pairwise response noise reduction model (PRNM) from the imaging perspective of seam tracking camera, which provides an image processing-free method to suppress welding splash and arc. Inverse response law and the loss function is established and solved by using singular value decomposition, and then the irradiance of reference scene is restored. A tone mapping approach is proposed based on the advantages of full-range compression and joint estimator, followed by mapping the obtained irradiance distribution to a high dynamic range (HDR) image. The grayscale correspondences of two reference images and the HDR image are reflected in the form of a two-dimensional lookup table (LUT). For welding noise characteristics, an alignment strategy is proposed to further correct the LUT to form a PRNM, which responds to all pairwise inputs with high computational efficiency. A batch of PRNMs have potential to form imaging solutions serving a welding process library. Experimental results reveal that the proposed model has a good effect in noise reduction, and the real-time performance is suitable for weld seam tracking.},
  archive      = {J_MVA},
  author       = {Qin, Zhonghao and Wang, Ke and Li, Ruifeng},
  doi          = {10.1007/s00138-022-01342-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Welding splash and arc noise reduction imaging model based on computationally efficient pairwise response serving welding process library},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminant distance template matching for image
recognition. <em>MVA</em>, <em>33</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01343-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of arrow markings in intelligent transportation system can be posed as a template matching problem, because their production has to conform with strict national standards. However, appearance variations often arise when capturing the arrow images due to different eye views. In this paper, we propose a discriminant distance template matching (DDTM) method for image recognition. It is compatible with not only hand-crafted features, but also DCNNs for features auto-learning. DDTM is able to discriminate whether an instance and a template matched. Consequently it can realize the arrow recognition through template matching. We have also shown some traditional image recognition tasks can be solved by DDTM. Experimental results on an arrow and the MNIST datasets validate its advantages compared to classification-based methods.},
  archive      = {J_MVA},
  author       = {Wang, Liantao and Liu, Qingrui},
  doi          = {10.1007/s00138-022-01343-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Discriminant distance template matching for image recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved two-stage image inpainting with perceptual color
loss and modified region normalization. <em>MVA</em>, <em>33</em>(6),
1–10. (<a href="https://doi.org/10.1007/s00138-022-01344-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a two-stage architecture to perform image inpainting from coarse to fine. The framework extracts advantages from different designs in the literature and integrates them into the inpainting network. We apply region normalization to generate coarse blur results with the correct structure. Then, contextual attention is applied to utilize the texture information of background regions to generate the final result. Although using region normalization can improve the performance and quality of the network, it often results in visible color shifts. To solve this problem, we introduce perceptual color distance in the loss function. In quantitative comparison experiments, the proposed method is superior to the existing similar methods in Inception Score, Fréchet Inception Distance, and perceptual color distance. In qualitative comparison experiments, the proposed method can effectively resolve the problem of color shifts.},
  archive      = {J_MVA},
  author       = {Cheng, Hsu-Yung and Yu, Chih-Chang and Li, Cheng-Ying},
  doi          = {10.1007/s00138-022-01344-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Improved two-stage image inpainting with perceptual color loss and modified region normalization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level receptive field feature reuse for multi-focus
image fusion. <em>MVA</em>, <em>33</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01345-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion, which is the fusion of two or more images focused on different targets into one clear image, is a worthwhile problem in digital image processing. Traditional methods are usually based on frequency domain or space domain, but they cannot guarantee the accurate measurement of all the image details of the activity level, and also cannot perfect the selection of image fusion rules. Therefore, the deep learning method with strong feature representation ability is called the mainstream of multi-focus image fusion. However, until now, most of the deep learning frameworks have not balanced the relationship between the two input features, the shallow features and the feature fusion. In order to improve the defects of previous work, we propose an end-to-end deep network, which includes an encoder and a decoder. Encoder is a pseudo-Siamese network. It extracts the same and different feature sets by using the features of double encoder, then reuses the shallow features and finally forms the coding. In decoder, the coding will be analyzed and dimensionally reduced enough to generate high-quality fusion image. We carried out extensive experiments. The results show that our network structure is better. Compared with various image fusion methods based on deep learning and traditional multi-focus image fusion methods in recent years, our method is slightly better than theirs in both objective metric contrast and subjective visual contrast.},
  archive      = {J_MVA},
  author       = {Jiang, Limai and Fan, Hui and Li, Jinjiang},
  doi          = {10.1007/s00138-022-01345-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-level receptive field feature reuse for multi-focus image fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose is all you need: The pose only group activity
recognition system (POGARS). <em>MVA</em>, <em>33</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01346-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel deep learning-based group activity recognition approach called the Pose Only Group Activity Recognition System (POGARS), designed to use only tracked poses of people to predict the performed group activity. In contrast to existing approaches for group activity recognition, POGARS uses 1D CNNs to learn spatiotemporal dynamics of individuals involved in a group activity and forgo learning features from pixel data. The proposed model uses a spatial and temporal attention mechanism to infer person-wise importance and multi-task learning for simultaneously performing group and individual action classification. Experimental results confirm that POGARS achieves highly competitive results compared to state-of-the-art methods on a widely used public volleyball dataset despite only using tracked pose as input. Further, our experiments show by using pose only as input, POGARS has better generalization capabilities compared to methods that use RGB as input.},
  archive      = {J_MVA},
  author       = {Thilakarathne, Haritha and Nibali, Aiden and He, Zhen and Morgan, Stuart},
  doi          = {10.1007/s00138-022-01346-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pose is all you need: The pose only group activity recognition system (POGARS)},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MURA-objects: A radioactive bone imaging lesion detection
dataset. <em>MVA</em>, <em>33</em>(6), 1–7. (<a
href="https://doi.org/10.1007/s00138-022-01347-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer-aided diagnosis technology needs to determine whether the bone image is abnormal and needs to locate the location of the lesions accurately. However, there are few publicly available detection datasets of bone lesions. Therefore, for the first, this paper proposes the abnormal detection dataset MURA-objects, which is relabeled based on the large-scale radioactive bone image dataset Musculoskeletal Radiograph. MURA-objects consist of 8431 images, including 7579 in the training set and 852 in the validation set. There are 8933 metal objects and 740 fracture objects in the training images and 1107 metal objects and 124 fracture objects in the validation images. We also give the baseline results using state-of-the-art methods such as faster RCNN, SSD, and YOLOv3, which lays a foundation for future bone imaging lesion detection research. The MURA-objects dataset can be found at https://github.com/wangxin1216/MURA-Objects .},
  archive      = {J_MVA},
  author       = {Shao, Yunxue and Wang, Xin},
  doi          = {10.1007/s00138-022-01347-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-7},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MURA-objects: A radioactive bone imaging lesion detection dataset},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast re-OBJ: Real-time object re-identification in rigid
scenes. <em>MVA</em>, <em>33</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01349-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Re-identifying objects in a rigid scene across varying viewpoints (object Re-ID) is a challenging task, in particular when there are similar, even identical objects coexist in the same environment. Discriminative features play no doubt an essential role in addressing this challenge, while for practical deployment, real-time performance is another desired attribute. We therefore propose a novel framework, named Fast re-OBJ, that is able to improve both Re-ID accuracy and processing speed via tight coupling between the instance segmentation module and embedding generation module. The rich object encoding in the instance segmentation backbone is directly shared to the embedding generation module for training a more discriminative representation via a triplet network. Moreover, we create datasets with the segmentation outputs using real-time object detectors to train and evaluate our object embedding module. With extensive experiments, we prove that our proposed Fast re-OBJ improves the object Re-ID accuracy by 5% and the speed is $$5\times $$ faster compared to the state-of-the-art methods. The dataset and code repository are publicly available at: https://tinyurl.com/bdsb53c4 .},
  archive      = {J_MVA},
  author       = {Bayraktar, Ertugrul and Wang, Yiming and DelBue, Alessio},
  doi          = {10.1007/s00138-022-01349-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fast re-OBJ: Real-time object re-identification in rigid scenes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight image super-resolution network using involution.
<em>MVA</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01307-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the single image super-resolution methods with deep and complex convolutional neural network structures have achieved remarkable performance. However, those approaches improve the performance at the cost of higher memory occupation, which are difficult to be applied for some resource-constrained devices. With the goal of minimizing parameters, an effective and efficient operator named involution is introduced in our proposed model, delivering enhanced performance at reduced cost compared to convolution-based counterparts. On the basis of involution, we propose two building blocks named RMFDB(Residual Mixed Feature Distillation Block) and CICB(Conv-Invo-Conv Block) for the main module and the reconstruction module respectively. RMFDB has the similar structure as the RFDB but with our involution layers. This block is much more lightweight and efficient than conventional convolution-based blocks. CICB combines the nearest-neighbor upsampling, convolution and involution layers. The final reconstruction quality is improved with little parameter cost. Experimental results demonstrate the effectiveness of the proposed model against the state-of-the-art (SOTA) SR methods. Our final model could achieve similar performance as the lightweight networks RFDN and PAN, but with only 224K parameters and 64.2G Multi-Adds with the scale factor of 2. The effectiveness of each proposed components is also validated by ablation study.},
  archive      = {J_MVA},
  author       = {Liang, Jiu and Zhang, Yu and Xue, Jiangbo and Hu, Yanda},
  doi          = {10.1007/s00138-022-01307-9},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Lightweight image super-resolution network using involution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial structured prediction for domain-adaptive
semantic segmentation. <em>MVA</em>, <em>33</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01308-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a structured prediction problem that heavily relies on expensive annotated image data to train supervised models. Unsupervised domain adaptation has been successful in leveraging synthetic (source) images to build models that generalize well to real (target) image data without annotations. However, previous methods mainly utilize source ground truth for segmentation loss and do not fully utilize them for learning segmentation output structures to guide the target domain. In this work, we exploit similar output structures across domains in order to better segment the target images. Toward this end, we devise an adversarial structured prediction by utilizing a regularizer. This regularizer outputs structured predictions on provided image features. Using an adversarial training setup, we make the structured predictions follow the spatial layout learned from the source ground truth. As a result, even without an explicit alignment between source and target features, our proposed method can adapt well from a source to a target domain. We evaluate our method on different challenging synthetic-2-real benchmarks and validate the effectiveness of the proposed method when compared with the state of the arts.},
  archive      = {J_MVA},
  author       = {Yarram, Sudhir and Yuan, Junsong and Yang, Ming},
  doi          = {10.1007/s00138-022-01308-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial structured prediction for domain-adaptive semantic segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A global activated feature pyramid network for tiny pest
detection in the wild. <em>MVA</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01310-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection techniques have been developed for decades, but one of key remaining open challenges is detecting tiny objects in wild or nature scenes. While recent works on deep learning techniques have shown a promising potential direction on common object detection in the wild, their accuracy and robustness on tiny object detection in the wild are still unsatisfactory. In this paper, we target at studying the problem of tiny pest detection in the wild and propose a new effective deep learning approach. It builds up a global activated feature pyramid network on convolutional neural network backbone for detecting tiny pests across a large range of scales over both positions and pyramid levels. The network enables retrieving the depth and spatial intension information over different levels in the feature pyramid. It makes variance or changes of spatial or depth-sensitive features in tiny pest images more visible. Besides, a hard example enhancement strategy is also proposed to implement fast and efficient training in this approach. The approach is evaluated on our newly built large-scale wide tiny pest dataset containing 27.8K images with 145.6K manually labelled pest objects. The results show that our approach perform well on pest detection with over 71% mAP, which outweighs other state-of-the-art object detection methods.},
  archive      = {J_MVA},
  author       = {Liu, Liu and Wang, Rujing and Xie, Chengjun and Li, Rui and Wang, Fangyuan and Qi, Long},
  doi          = {10.1007/s00138-022-01310-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A global activated feature pyramid network for tiny pest detection in the wild},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parametric regularization loss in super-resolution
reconstruction. <em>MVA</em>, <em>33</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s00138-022-01315-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A noise-enhanced super-resolution generative adversarial network plus (nESRGAN+) was proposed to improve the enhanced super-resolution GAN (ESRGAN). The contributions of nESRGAN+ generate an impressive reconstructed image with more texture details and greater sharpness. However, the perceptual quality of the output lacks hallucinated details and undesirable artifacts and takes a long time to converge. To address these problems, we propose four types of parametric regularization algorithms as loss functions of the model to enable the iterative weight adjustment of the network gradient. Several experiments were conducted to confirm that the generator can achieve a better-quality reconstructed image, including restoring the unseen texture. Our method accomplished the average peak signal-to-noise ratio (PSNR) of the reconstructed image at 27.96 dB, the average Structural Similarity Index Measure (SSIM) at 0.8303, and the average Learned Perceptual Image Patch Similarity (LPIPS) at 0.1949. It took seven times less training time than the state of the art. In addition to the better visual quality of the reconstructed result, the proposed loss functions allow the generator to converge faster.},
  archive      = {J_MVA},
  author       = {Viriyavisuthisakul, Supatta and Kaothanthong, Natsuda and Sanguansat, Parinya and Nguyen, Minh Le and Haruechaiyasak, Choochart},
  doi          = {10.1007/s00138-022-01315-9},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Parametric regularization loss in super-resolution reconstruction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning framework for finding illicit images/videos
of children. <em>MVA</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01318-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have led to tremendous achievements in computer vision applications. Specifically for the tasks of automated human age estimation and nudity detection, modern machine learning can predict whether or not an image contains nudity or the presence of a minor with startling accuracy. Fusing together separate models can make possible to identify instances of child pornography without ever coming into contact with the illicit material during model training. In this paper, a novel framework for automatically identifying Sexually Exploitative Imagery of Children is introduced. It is a synthesis of models for modeling human apparent age and nudity detection. The performance of this approach is thoroughly evaluated on several widely used age estimation and nudity detection datasets. Additionally, preliminary tests were conducted with the help of a local law enforcement agency on a private dataset of SEIC taken from real-world cases with up to $$97\%$$ accuracy of SEIC video classification.},
  archive      = {J_MVA},
  author       = {Rondeau, Jared and Deslauriers, Douglas and Howard III, Thomas and Alvarez, Marco},
  doi          = {10.1007/s00138-022-01318-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A deep learning framework for finding illicit images/videos of children},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based domain adaptation for single-stage
detectors. <em>MVA</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01320-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While domain adaptation has been used to improve the performance of object detectors when the training and test data follow different distributions, previous work has mostly focused on two-stage detectors. This is because their use of region proposals makes it possible to perform local adaptation, which has been shown to significantly improve the adaptation effectiveness. Here, by contrast, we target single-stage architectures, which are better suited to resource-constrained detection than two-stage ones but do not provide region proposals. To nonetheless benefit from the strength of local adaptation, we introduce an attention mechanism that lets us identify the important regions on which adaptation should focus. Our method gradually adapts the features from global, image level to local, instance level. Our approach is generic and can be integrated into any Single-Shot Detector. We demonstrate this on standard benchmark datasets by applying it to both the single-shot detector (SSD) and a recent variant of the You Only Look Once detector (YOLOv5). Furthermore, for equivalent single-stage architectures, our method outperforms the state-of-the-art domain adaptation techniques even though they were designed for specific detectors.},
  archive      = {J_MVA},
  author       = {Vidit, Vidit and Salzmann, Mathieu},
  doi          = {10.1007/s00138-022-01320-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-based domain adaptation for single-stage detectors},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure–texture decomposition-based dehazing of a single
image with large sky area. <em>MVA</em>, <em>33</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-022-01321-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional dehazing methods based on restoration are prone to color distortion and noise amplification when dealing with hazy image with large sky area. To improve dehazing effect, we propose a dehazing algorithm based on image structure–texture decomposition and reconstruction. Hazy image is decomposed into high-frequency texture layer and low-frequency structure layer by total variation. Discrete cosine transform is used to generate an image mask to separate sky area and non-sky area. The texture layer is denoised by the mask, and the structure layer is dehazed by dark channel prior. The media transmission is corrected by color attenuation prior. Finally, the denoised texture layer and the dehazed structure layer are reconstructed to obtain the dehazed image. A no-reference image quality assessment is also proposed to evaluate the dehazed images. Experiment results show that, compared with the state-of-the-art methods, our algorithm has better dehazing effect on non-sky area, and the sky area after dehazing is smooth without color distortion and noise.},
  archive      = {J_MVA},
  author       = {Tang, Chaoying and Jia, Ru and Ren, Xue and Cui, Yun and Wang, Biao},
  doi          = {10.1007/s00138-022-01321-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Structure–texture decomposition-based dehazing of a single image with large sky area},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint patch clustering-based adaptive dictionary and sparse
representation for multi-modality image fusion. <em>MVA</em>,
<em>33</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01322-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the image fusion method using sparse representation, the adaptive dictionary and fusion rule have a great influence on the multi-modality image fusion, and the maximum $$L_{1}$$ norm fusion rule may cause gray inconsistency in the fusion result. In order to solve this problem, we proposed an improved multi-modality image fusion method by combining the joint patch clustering-based adaptive dictionary and sparse representation in this study. First, we used a Gaussian filter to separate the high- and low-frequency information. Second, we adopted the local energy-weighted strategy to complete the low-frequency fusion. Third, we used the joint patch clustering algorithm to reconstruct an over-complete adaptive learning dictionary, designed a hybrid fusion rule depending on the similarity of multi-norm of sparse representation coefficients, and completed the high-frequency fusion. Last, we obtained the fusion result by transforming the frequency domain into the spatial domain. We adopted the fusion metrics to evaluate the fusion results quantitatively and proved the superiority of the proposed method by comparing the state-of-the-art image fusion methods. The results showed that this method has the highest fusion metrics in average gradient, general image quality, and edge preservation. The results also showed that this method has the best performance in subjective vision. We demonstrated that this method has strong robustness by analyzing the parameter’s influence on the fusion result and consuming time. We extended this method to the infrared and visible image fusion and multi-focus image fusion perfectly. In summary, this method has the advantages of good robustness and wide application.},
  archive      = {J_MVA},
  author       = {Wang, Chang and Wu, Yang and Yu, Yi and Zhao, Jun Qiang},
  doi          = {10.1007/s00138-022-01322-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Joint patch clustering-based adaptive dictionary and sparse representation for multi-modality image fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thin section analysis for ceramic petrography using motion
analysis and segmentation techniques. <em>MVA</em>, <em>33</em>(5),
1–29. (<a href="https://doi.org/10.1007/s00138-022-01324-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mineral segmentation in ceramic thin sections containing different minerals, in which there are no evident and close boundaries, is a rather complex process. The results of such a process are used in archaeology for analyzing the origin and manufacturing techniques of ancient ceramics. In this paper we present a methodology for the segmentation and analysis of thin sections of material segments and reaching some conclusions in a fully automatic way. We employ machine learning and computer vision techniques to analyze a video of the thin section sample, acquired under an optical microscope. When examined under polarized light, the color of segments may vary during sample rotation. This variation is due to the optical properties of the materials and it provides valuable information about the material inclusions in the sample. Using the video as our input, we perform an entire-video segmentation. To accomplish this task, we developed a hierarchical categorical mean-shift-based algorithm. Using the entire-video segmentation we examine the detected segments and gather statistical information about their sizes, shapes and colors and present an overall report about the sample. We tested the algorithm on nine specimens of ancient ceramics, taken from three different Mediterranean sites. The results show clear differences between the sites in the amounts, sizes and shapes of the segments present in the specimens.},
  archive      = {J_MVA},
  author       = {Lerner, Jenny and Shimshoni, Ilan},
  doi          = {10.1007/s00138-022-01324-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Thin section analysis for ceramic petrography using motion analysis and segmentation techniques},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global-guided cross-reference network for co-salient object
detection. <em>MVA</em>, <em>33</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01325-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-salient object detection aims to find common salient objects from an image group, which is a branch of salient object detection. This paper proposes a global-guided cross-reference network. The cross-reference module is designed to enhance the multi-level features from two perspectives. From the spatial perspective, the location information of objects with similar appearances must be highlighted. From the channel perspective, more attention must be assigned to channels that indicate the same object category. After spatial and channel cross-reference, the features are enhanced to possess the consensus representation of image group. Next, a global co-semantic guidance module is built to provide hierarchical features with the location information of co-salient objects. Compared with state-of-the-art co-salient object detection methods, our proposed method extracts collaborative information and obtains better co-saliency maps on several challenging co-saliency detection datasets.},
  archive      = {J_MVA},
  author       = {Liu, Zhengyi and Dong, Hao and Zhang, Zhili and Xiao, Yun},
  doi          = {10.1007/s00138-022-01325-7},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Global-guided cross-reference network for co-salient object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-focus image fusion based on unsupervised learning.
<em>MVA</em>, <em>33</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01326-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-focus image fusion task, how to better balance the clear region information of the original image with different focus positions is the key. In this paper, a multi-focus image fusion model based on unsupervised learning is designed, and the image fusion task is carried out by two-stage processing. In the training phase, the encoder–decoder structure is adopted and the multi-scale structural similarity is introduced as the loss function for image reconstruction. In the fusion stage, the trained encoder is used to encode the feature of the original image. The spatial frequency is used to distinguish the clear area of the image from the two scales of channel and space, and the pixels with inconsistent discrimination are checked and processed to generate the initial decision diagram. The final image fusion task is carried out after mathematical morphology optimization. The experimental results show that this method has good effect on preserving the texture details and edge information of the focused area of the original image. Compared with the five advanced fusion algorithms, the proposed algorithm has achieved preferential fusion performance.},
  archive      = {J_MVA},
  author       = {Wu, Kaijun and Mei, Yuan},
  doi          = {10.1007/s00138-022-01326-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-focus image fusion based on unsupervised learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Micro-concrete crack detection of underwater structures
based on convolutional neural network. <em>MVA</em>, <em>33</em>(5),
1–19. (<a href="https://doi.org/10.1007/s00138-022-01327-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-cracks are often generated on the concrete structures of long-distance water conveyance projects. Without early detection and timely maintenance, micro-cracks may expand and deteriorate continuously, leading to major structural failure and disastrous results. However, due to the complexity of the underwater environment, many vision-based methods for concrete crack detection cannot be directly applied to the interior surface of water conveyance structures. In view of this, this paper proposes a three-step method to automatically detect concrete micro-cracks of underwater structures during the operation period. First, underwater optical images were preprocessed by a series of algorithms such as global illumination balance, image color correction, and detail enhancement. Second, the preprocessed images were sliced to image patches, which are sent to a convolutional neural network for crack recognition and crack boundary localization. Finally, the image patches containing cracks were segmented by the Otsu algorithm to localize the cracks precisely. The proposed method can overcome issues such as uneven illumination, color distortion, and detail blurring, and can effectively detect and localize cracks in underwater optical images with low illumination, low signal-to-noise ratio and low contrast. The experimental results show that this method can achieve a true positive rate of 93.9% for crack classification, and the identification accuracy of the crack width can reach 0.2 mm.},
  archive      = {J_MVA},
  author       = {Qi, ZhiLong and Liu, Donghai and Zhang, Jinyue and Chen, Junjie},
  doi          = {10.1007/s00138-022-01327-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Micro-concrete crack detection of underwater structures based on convolutional neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiTransformer: Augmenting semantic context in video
captioning via bidirectional decoder. <em>MVA</em>, <em>33</em>(5), 1–9.
(<a href="https://doi.org/10.1007/s00138-022-01329-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning is an important problem involved in many applications. It aims to generate some descriptions of the content of a video. Most of existing methods for video captioning are based on the deep encoder–decoder models, particularly, the attention-based models (say Transformer). However, the existing transformer-based models may not fully exploit the semantic context, that is, only using the left-to-right style of context but ignoring the right-to-left counterpart. In this paper, we introduce a bidirectional (forward-backward) decoder to exploit both the left-to-right and right-to-left styles of context for the Transformer-based video captioning model. Thus, our model is called bidirectional Transformer (dubbed BiTransformer). Specifically, in the bridge of the encoder and forward decoder (aiming to capture the left-to-right context) used in the existing Transformer-based models, we plug in a backward decoder to capture the right-to-left context. Equipped with such bidirectional decoder, the semantic context of videos will be more fully exploited, resulting in better video captions. The effectiveness of our model is demonstrated over two benchmark datasets, i.e., MSVD and MSR-VTT,via comparing to the state-of-the-art methods. Particularly, in terms of the important evaluation metric CIDEr, the proposed model outperforms the state-of-the-art models with improvements of 1.2% in both datasets.},
  archive      = {J_MVA},
  author       = {Zhong, Maosheng and Zhang, Hao and Wang, Yong and Xiong, Hao},
  doi          = {10.1007/s00138-022-01329-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {BiTransformer: Augmenting semantic context in video captioning via bidirectional decoder},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stream lightweight sign language transformer.
<em>MVA</em>, <em>33</em>(5), 1–8. (<a
href="https://doi.org/10.1007/s00138-022-01330-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent progress of continuous sign language translation-based video, a variety of deep learning models are difficult to apply to the real-time translation in the limit computing resource. We present the two-stream lightweight sign transformer network model for recognizing and translating continuous sign language. This lightweight framework can obtain both static spatial information and all body dynamic features of signer, and the transformer-style decoder architecture to real-time translate sentences from the spatio-temporal context around the signer. Additionally its attention mechanism focus on moving hands and mouth of signer, which is often crucial for semantic understanding of sign language. In this paper, we introduce the Chinese sign language corpus of the business scene which consists of 3080 videos of high quality. The Chinese sign language corpus of the business scene has enormous impetuses for further research on the Chinese sign language translation. Experiments are carried out the PHOENIX-Weather 2014T (Camgoz et al, in: Proceedings of IEEE/CVF conference on computer vision and pattern recognition (CVPR 2018), pp 7784–7793, 2018), Chinese Sign Language dataset Huang et al, in: The thirty-second AAAI conference on artificial intelligence (AAAI-18), pp 2257–2264, 2018) and our CSLBS, the proposed model outperforms the state-of-the-art in inference times and accuracy using only raw RGB and RGB difference frames as input.},
  archive      = {J_MVA},
  author       = {Chen, Yuming and Mei, Xue and Qin, Xuan},
  doi          = {10.1007/s00138-022-01330-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Two-stream lightweight sign language transformer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized hand pose estimation CrossInfoNet-based
architecture for embedded devices. <em>MVA</em>, <em>33</em>(5), 1–12.
(<a href="https://doi.org/10.1007/s00138-022-01332-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present CrossInfoMobileNet, a hand pose estimation convolutional neural network based on CrossInfoNet, specifically tuned to mobile phone processors through the optimization, modification, and replacement of computationally critical CrossInfoNet components. By introducing a state-of-the-art MobileNetV3 network as a feature extractor and refiner, replacing ReLU activation with a better performing H-Swish activation function, we have achieved a network that requires 2.37 times less multiply-add operations and 2.22 times less parameters than the CrossInfoNet network, while maintaining the same error on the state-of-the-art datasets. This reduction of multiply-add operations resulted in an average 1.56 times faster real-world performance on both desktop and mobile devices, making it more suitable for embedded applications. The full source code of CrossInfoMobileNet including the sample dataset and its evaluation is available online through Code Ocean.},
  archive      = {J_MVA},
  author       = {Šimoník, Marek and Krumnikl, Michal},
  doi          = {10.1007/s00138-022-01332-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Optimized hand pose estimation CrossInfoNet-based architecture for embedded devices},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An annotated image database of building facades categorized
into land uses for object detection using deep learning. <em>MVA</em>,
<em>33</em>(5), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01335-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a machine learning approach to automatic land use categorization based on a convolutional artificial neural network architecture. It is intended to support the detection and classification of building facades in order to associate each building with its respective land use. Replacing the time-consuming manual acquisition of images in the field and subsequent interpretation of the data with computer-aided techniques facilitates the creation of useful maps for urban planning. A specific future objective of this study is to monitor the commercial evolution in the city of Vila Velha, Brazil. The initial step is object detection based on a deep network architecture called Faster R-CNN. The model is trained on a collection of street-level photographs of buildings of desired land uses, from a database of annotated images of building facades. Images are extracted from Google Street View scenes. Furthermore, in order to save manual annotation time, a semi-supervised dual pipeline method is proposed that uses a pre-trained predictor model from the Places365 database to learn unannotated images. Several backbones were connected to the Faster R-CNN architecture for comparisons. The experimental results with the VGG backbone show an improvement over published works, with an average accuracy of 86.49%.},
  archive      = {J_MVA},
  author       = {Bortoloti, Frederico Damasceno and Tavares, Jonivane and Rauber, Thomas Walter and Ciarelli, Patrick Marques and Botelho, Rayane Cardozo Gama},
  doi          = {10.1007/s00138-022-01335-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An annotated image database of building facades categorized into land uses for object detection using deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triple attention and global reasoning siamese networks for
visual tracking. <em>MVA</em>, <em>33</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01301-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental problem in computer vision, the aim of object tracking is to capture the accurate information of the given target in the video sequence, with the initial information determined in the first frame. Despite its significant improvement in the past decades, however, they are still facing various challenges, including occlusion, deformation, fast motion, etc. To attain robust performance, a tracking algorithm based on triple attention mechanism and global reasoning model is presented in this work, which is inspired by the progress of the Siamese network recently. First, in order to solve the problem of insufficient feature extraction, a triple attention model is proposed, which consists of three parts: squeeze-and-excitation (SE) block, spatial SE (sSE) block, and channel SE (cSE) block. Second, to tackle the lack of context information in the tracking procedure, a global reasoning model was added into the template branch and search branch, which will generate two different score maps. As the tracking process continued, these two score maps were summed to construct a regression confidence map with their weight, respectively. Extensive experiments on exited benchmarks including OTB50, OTB100, VOT 2016, VOT2018, GOT-10k, LaSOT, NFS, and TC128 demonstrate that the proposed method achieves competitive results.},
  archive      = {J_MVA},
  author       = {Shu, Ping and Xu, Keying and Bao, Hua},
  doi          = {10.1007/s00138-022-01301-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Triple attention and global reasoning siamese networks for visual tracking},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel method for 3D knee anatomical landmark localization
by combining global and local features. <em>MVA</em>, <em>33</em>(4),
1–13. (<a href="https://doi.org/10.1007/s00138-022-01303-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landmark localization with neural networks had gained popularity in recent years. However, due to the high dimensionality and large size of medical images, current neural network models still have problems such as information loss with deeper network, low accuracy and robustness. To address these issues, a 3D anatomical landmark localization method with a two-stage strategy was proposed in this study. The 3D spatial information between landmarks and the local information of each single feature point were extracted in these two stages. Additionally, new inception and attention modules were designed for the second stage to combine convolutional kernels of different sizes and weight labeling to strengthen the effective features extraction while weakening the invalid features. The proposed model was evaluated on a collected knee image dataset. The results outperformed state-of-the-art models with a mean error of 3.29 mm and a standard deviation of 2.17 mm. The outlier rates at error radius 3 mm, 5 mm and 7 mm were 53%, 22% and 5%, respectively, indicating good robustness of the model. The study provides a new neural network model with good accuracy for landmark localization tasks.},
  archive      = {J_MVA},
  author       = {Zhu, Junjun and Zhao, Qijie and Zhu, Junhao and Zhou, Anwen and Shao, Hui},
  doi          = {10.1007/s00138-022-01303-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel method for 3D knee anatomical landmark localization by combining global and local features},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel multi-feature fusion deep neural network using HOG
and VGG-face for facial expression classification. <em>MVA</em>,
<em>33</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s00138-022-01304-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are a prevalent way to recognize human emotions, and automatic facial expression recognition (FER) has been a significant task in cognitive science, artificial intelligence, and computer vision. The critical issue with the design of the FER model is the strong intra-class correlation of different emotions. The accuracy of the FER model is reduced due to other problems such as the variations in expressing the emotions, variations in lighting, and different ethnic biases. The latest convolutional neural network-based FER models have shown significant improvement in accuracy score but lack distinguishing the micro-expressions. This paper proposed a multi-input hybrid FER model that considers both hand-engineered and self-learnt features to classify facial expressions. The VGG-Face and the histogram of oriented gradients (HOG) features are derived from the faces to distinguish various facial expression patterns. The fusion of deep (VGG-Face) and hand-engineered (HOG) features has shown improved accuracy compared to the conventional CNN models. The results obtained showed that the proposed model’s accuracy scores outperformed the accuracy scores of the other popular FER models on three facial expression datasets. Extended Cohn–Kanade (CK $$+$$ ), Yale-Face, and Karolinska directed emotional faces (KDEF) datasets are used to determine the model’s classification efficiency. The proposed model scored 98.12%, 95.26%, and 96.36% accuracy using a fivefold cross-validation process on the CK $$+$$ , Yale-Face and KDEF datasets.},
  archive      = {J_MVA},
  author       = {Ahadit, Alagesan Bhuvaneswari and Jatoth, Ravi Kumar},
  doi          = {10.1007/s00138-022-01304-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel multi-feature fusion deep neural network using HOG and VGG-face for facial expression classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based global context network for driving maneuvers
prediction. <em>MVA</em>, <em>33</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01305-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving maneuvers prediction is one of the most challenging tasks in modern Advanced Driver Assistance System. Such predictions can improve driving safety by alerting the driver to the danger of unsafe or risky traffic situations. In this research, we presents a novel Attention-based Global Context Network (AGCNet) for driving maneuvers prediction from multiple modality data, including front view video data and driver physiological signals. Firstly, with Global Context block, the AGCNet has an ability of modeling the long-range dependency contextual features from multi-modal data with lightweight computation. Secondly, the Channel-wise Attention is introduced in AGCNet to focus on valuable features. Finally, a custom-built Dual attention-based Long Short-Term Memory (DaLSTM) network is designed to learn co-occurrence features and predict driving maneuvers. Specially, the DaLSTM can employ attention mechanisms over heterogeneous features and time steps simultaneously. The experimental results show that the AGCNet is capable of learning the latent features of driving maneuvers and achieving significantly better performance than other advanced models on a real-world driving dataset.},
  archive      = {J_MVA},
  author       = {Gao, Jun and Yi, Jiangang and Murphey, Yi Lu},
  doi          = {10.1007/s00138-022-01305-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-based global context network for driving maneuvers prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive overview of dynamic visual SLAM and deep
learning: Concepts, methods and challenges. <em>MVA</em>,
<em>33</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s00138-022-01306-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual SLAM (vSLAM) is a research topic that has been developing rapidly in recent years, especially with the renewed interest in machine learning and, more particularly, deep-learning-based approaches. Nowadays, main research is carried out to improve accuracy and robustness in complex and dynamic environments. This scorching topic has reached a significant level of maturity. This paper presents a relatively detailed and easily understood survey of vSLAM within deep learning. This study attempts to meet this challenge by better organizing the literature, explaining the basic concepts and tools, and presenting the current trends. The contributions of this study can be summarized in three essential steps. The first one is to provide the state-of-the-art in an incremental way following the classical processes of vSLAM-based systems. The second is to give our short- and medium-term view of the development of this very active and evolving field. Finally, we share our opinions on this subject and its interactions with new trends and, more particularly, the deep learning paradigm. We believe that this contribution will be an overview and, more importantly, a critical and detailed vision that serves as a roadmap in the field of vSLAMs both in terms of models and concepts and in terms of associated technologies.},
  archive      = {J_MVA},
  author       = {Beghdadi, Ayman and Mallem, Malik},
  doi          = {10.1007/s00138-022-01306-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A comprehensive overview of dynamic visual SLAM and deep learning: Concepts, methods and challenges},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editor’s note. <em>MVA</em>, <em>33</em>(4), 1. (<a
href="https://doi.org/10.1007/s00138-022-01309-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  doi          = {10.1007/s00138-022-01309-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editor’s note},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamically throttleable neural networks. <em>MVA</em>,
<em>33</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-022-01311-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional computation for deep neural networks reduces overall computational load and improves model accuracy by running a subset of the network. In this work, we present a runtime dynamically throttleable neural network (DTNN) that can self-regulate its own performance target and computing resources by dynamically activating neurons in response to a single control signal, called utilization. We describe a generic formulation of throttleable neural networks (TNNs) by grouping and gating partial neural modules with various gating strategies. To directly optimize arbitrary application-level performance metrics and model complexity, a controller network is trained separately to predict a context-aware utilization via deep contextual bandits. Extensive experiments and comparisons on image classification and object detection tasks show that TNNs can be effectively throttled across a wide range of utilization settings, while having peak accuracy and lower cost that are comparable to corresponding vanilla architectures such as VGG, ResNet, ResNeXt, and DenseNet. We further demonstrate the effectiveness of the controller network on throttleable 3D convolutional networks (C3D) for video-based hand gesture recognition, which outperforms the vanilla C3D and all fixed utilization settings.},
  archive      = {J_MVA},
  author       = {Liu, Hengyue and Parajuli, Samyak and Hostetler, Jesse and Chai, Sek and Bhanu, Bir},
  doi          = {10.1007/s00138-022-01311-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dynamically throttleable neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EAF-net: An enhancement and aggregation–feedback network for
RGB-t salient object detection. <em>MVA</em>, <em>33</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01312-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) aims at highlighting important foreground objects automatically from the background. Most existing SOD methods only employ visible images (RGB images) for salient detection, which limits the performance of real-life applications when encountering challenging scenarios such as low illumination, haze, and smog. In this paper, we take advantage of the RGB and thermal images and propose an Enhancement and Aggregation–Feedback Network (EAF-Net) for SOD. Specifically, to achieve effective complementation between modalities and prevent the interference from noises, we first treat RGB and thermal images equally in the Feature Enhancement Block (FEB), and further, the Global Context Module expands receptive field to obtain the global features and the Top-Feature Enhancement Module suppresses the redundant information that may destroy the original features from the top layer. Subsequently, we embed several Cross Feature Aggregation Modules (CFAMs) into the Aggregation-and-Feedback Decoder to fuse different level features and compensation features for further obtaining comprehensive feature expression. Moreover, a feedback mechanism is adopted to propagate these fused features back into previous layers for refinement and generate saliency maps to decode features in a progressive way. Comprehensive experiments on RGB-T datasets demonstrate that EAF-Net achieves outstanding performance against the state-of-the-art models.},
  archive      = {J_MVA},
  author       = {He, Haiyang and Wang, Jing and Li, Xiaolin and Hong, Minglin and Huang, Shiguo and Zhou, Tao},
  doi          = {10.1007/s00138-022-01312-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {EAF-net: An enhancement and aggregation–feedback network for RGB-T salient object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ConsInstancy: Learning instance representations for
semi-supervised panoptic segmentation of concrete aggregate particles.
<em>MVA</em>, <em>33</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01313-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a semi-supervised method for panoptic segmentation based on ConsInstancy regularisation, a novel strategy for semi-supervised learning. It leverages completely unlabelled data by enforcing consistency between predicted instance representations and semantic segmentations during training in order to improve the segmentation performance. To this end, we also propose new types of instance representations that can be predicted by one simple forward path through a fully convolutional network (FCN), delivering a convenient and simple-to-train framework for panoptic segmentation. More specifically, we propose the prediction of a three-dimensional instance orientation map as intermediate representation and two complementary distance transform maps as final representation, providing unique instance representations for a panoptic segmentation. We test our method on two challenging data sets of both, hardened and fresh concrete, the latter being proposed by the authors in this paper demonstrating the effectiveness of our approach, outperforming the results achieved by state-of-the-art methods for semi-supervised segmentation. In particular, we are able to show that by leveraging completely unlabelled data in our semi-supervised approach the achieved overall accuracy (OA) is increased by up to 5% compared to an entirely supervised training using only labelled data. Furthermore, we exceed the OA achieved by state-of-the-art semi-supervised methods by up to 1.5%.},
  archive      = {J_MVA},
  author       = {Coenen, Max and Schack, Tobias and Beyer, Dries and Heipke, Christian and Haist, Michael},
  doi          = {10.1007/s00138-022-01313-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ConsInstancy: Learning instance representations for semi-supervised panoptic segmentation of concrete aggregate particles},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth estimation from a single SEM image using pixel-wise
fine-tuning with multimodal data. <em>MVA</em>, <em>33</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s00138-022-01314-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the ongoing size reduction in integrated circuits, the need for accurate depth measurements of on-chip structures becomes increasingly important. Unfortunately, present metrology tools do not offer a practical solution. In the semiconductor industry, critical dimension scanning electron microscopes (CD-SEMs) are predominantly used for 2D imaging at a local scale. The main objective of this work is to investigate whether sufficient 3D information is present in a single SEM image for accurate surface reconstruction of the device topology. In this work, we present a method that is able to produce depth maps from synthetic and experimental SEM images. We demonstrate that the proposed neural network architecture, together with a tailored training procedure, leads to accurate depth predictions. The training procedure includes a weakly supervised domain adaptation step, which is further referred to as pixel-wise fine-tuning. This step employs scatterometry data to address the ground-truth scarcity problem. We have tested this method first on a synthetic contact hole dataset, where a mean relative error smaller than 6.2% is achieved at realistic noise levels. Additionally, it is shown that this method is well suited for other important semiconductor metrics, such as top critical dimension (CD), bottom CD and sidewall angle. To the extent of our knowledge, we are the first to achieve accurate depth estimation results on real experimental data, by combining data from SEM and scatterometry measurements. An experiment on a dense line space dataset yields a mean relative error smaller than 1%.},
  archive      = {J_MVA},
  author       = {Houben, Tim and Huisman, Thomas and Pisarenco, Maxim and van der Sommen, Fons and de With, Peter H. N.},
  doi          = {10.1007/s00138-022-01314-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depth estimation from a single SEM image using pixel-wise fine-tuning with multimodal data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The development of ViBe foreground detection algorithm using
lévy flights random update strategy and kinect laser imaging sensor.
<em>MVA</em>, <em>33</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01316-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the development of a Lévy flights-based ViBe algorithm for foreground detection. It is based on a novel approach, using a particular class of the generalized random walk known as Lévy flights, to improve the spatial update mechanism. This mechanism originally used the uniform probability distribution, and it is responsible for handling the new objects that appear in the scene. The proposed approach speeds up the inclusion process of ghost regions in the background model and makes it faster than the inclusion of real static foreground objects while maintaining the classification performance. The developed detection algorithm was evaluated using inclusion speed and classification tests, the results showing the efficacy of using Lévy flights with ViBe’s updating mechanism. Experimental tests were also undertaken on the proposed algorithm to validate its ability with real images, obtained through a series of experiments performed using a multi-spectral Kinect laser imaging sensor, and also from a public dataset. The experimental results show the high adaptation capability of this algorithm against the background modification and validate its ability to deal with multi-spectral real images. The developed algorithm achieved a better performance in comparison with traditional ViBe algorithms when extracting background and detecting foreground objects.},
  archive      = {J_MVA},
  author       = {Al-Temeemy, Ali A.},
  doi          = {10.1007/s00138-022-01316-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {The development of ViBe foreground detection algorithm using lévy flights random update strategy and kinect laser imaging sensor},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical contrastive adaptation for cross-domain object
detection. <em>MVA</em>, <em>33</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01317-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection based on deep learning has been enormously developed in recent years. However, applying the detectors trained on a label-rich domain to an unseen domain results in performance drop due to the domain-shift. To deal with this problem, we propose a novel unsupervised domain adaptation method to adapt from a labeled source domain to an unlabeled target domain. Recent approaches based on adversarial learning show some effect for aligning the feature distributions of different domains, but the decision boundary would be strongly source-biased for the complex detection task when merely training with source labels and aligning in the entire feature distribution. In this paper, we suggest utilizing image translation to generate translated images of source and target domains to fill in the large domain gap and facilitate a paired adaptation. We propose a hierarchical contrastive adaptation method between the original and translated domains to encourage the detectors to learn domain-invariant but discriminative features. To attach importance to foreground instances and tackle the noises of translated images, we further propose foreground attention reweighting for instance-aware adaptation . Experiments are carried out on 3 cross-domain detection scenarios, and we achieve the state-of-the-art results against other approaches, showing the effectiveness of our proposed method.},
  archive      = {J_MVA},
  author       = {Deng, Ziwei and Kong, Quan and Akira, Naoto and Yoshinaga, Tomoaki},
  doi          = {10.1007/s00138-022-01317-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hierarchical contrastive adaptation for cross-domain object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ellipse detection using the edges extracted by deep
learning. <em>MVA</em>, <em>33</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01319-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing edge detection methods are based on fixed logics, which are not intelligent enough to distinguish useful edges and useless/noise edges. Recent ellipse detection methods developed some excellent algorithms that can still detect ellipses, while a large number of noise edges exist. However, these algorithms are compromised that will lose some precision and recall. This paper proposes a deep learning model that can intelligently distinguish useful edges and useless edges. Therefore, high-quality edge maps with low noise can be obtained. An arc-growing-based ellipse detection method is also proposed to take full advantage of the high-quality edge maps. Experiments are performed to reveal the mechanism of the deep learning model and to verify the performance of the proposed method. The experimental results demonstrate that the proposed method performs far better than the state-of-the-art in terms of precision, recall and the F-measure on industrial images and performs slightly better on natural images.},
  archive      = {J_MVA},
  author       = {Liu, Chicheng and Chen, Rui and Chen, Ken and Xu, Jing},
  doi          = {10.1007/s00138-022-01319-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ellipse detection using the edges extracted by deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D pedestrian localization using multiple cameras: A
generalizable approach. <em>MVA</em>, <em>33</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01323-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is a critical problem in many areas, such as smart cities, surveillance, monitoring, autonomous driving, and robotics. AI-based methods have made tremendous progress in the field in the last few years, but good performance is limited to data that match the training datasets. We present a multi-camera 3D pedestrian detection method that does not need to be trained using data from the target scene. The core idea of our approach consists in formulating consistency in multiple views as a graph clique cover problem. We estimate pedestrian ground location on the image plane using a novel method based on human body poses and person’s bounding boxes from an off-the-shelf monocular detector. We then project these locations onto the ground plane and fuse them with a new formulation of a clique cover problem from graph theory. We propose a new vertex ordering strategy to define fusion priority based on both detection distance and vertex degree. We also propose an optional step for exploiting pedestrian appearance during fusion by using a domain-generalizable person re-identification model. Finally, we compute the final 3D ground coordinates of each detected pedestrian with a method based on keypoint triangulation. We evaluated the proposed approach on the challenging WILDTRACK and MultiviewX datasets. Our proposed method significantly outperformed state of the art in terms of generalizability. It obtained a MODA that was approximately 15% and 2% better than the best existing generalizable detection technique on WILDTRACK and MultiviewX, respectively.},
  archive      = {J_MVA},
  author       = {Lima, João Paulo and Roberto, Rafael and Figueiredo, Lucas and Simões, Francisco and Thomas, Diego and Uchiyama, Hideaki and Teichrieb, Veronica},
  doi          = {10.1007/s00138-022-01323-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D pedestrian localization using multiple cameras: A generalizable approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-validation of a semantic segmentation network for
natural history collection specimens. <em>MVA</em>, <em>33</em>(3),
1–31. (<a href="https://doi.org/10.1007/s00138-022-01276-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has been proposed as a tool to accelerate the processing of natural history collection images. However, developing a flexible and resilient segmentation network requires an approach for adaptation which allows processing different datasets with minimal training and validation. This paper presents a cross-validation approach designed to determine whether a semantic segmentation network possesses the flexibility required for application across different collections and institutions. Consequently, the specific objectives of cross-validating the semantic segmentation network are to (a) evaluate the effectiveness of the network for segmenting image sets derived from collections different from the one in which the network was initially trained on; and (b) test the adaptability of the segmentation network for use in other types of collections. The resilience to data variations from different institutions and the portability of the network across different types of collections are required to confirm its general applicability. The proposed validation method is tested on the Natural History Museum semantic segmentation network, designed to process entomological microscope slides. The proposed semantic segmentation network is evaluated through a series of cross-validation experiments designed to test using data from two types of collections: microscope slides (from three institutions) and herbarium sheets (from seven institutions). The main contribution of this work is the method, software and ground truth sets created for this cross-validation as they can be reused in testing similar segmentation proposals in the context of digitization of natural history collections. The cross-validation of segmentation methods should be a required step in the integration of such methods into image processing workflows for natural history collections.},
  archive      = {J_MVA},
  author       = {de la Hidalga, Abraham Nieva and Rosin, Paul L. and Sun, Xianfang and Livermore, Laurence and Durrant, James and Turner, James and Dillen, Mathias and Musson, Alicia and Phillips, Sarah and Groom, Quentin and Hardisty, Alex},
  doi          = {10.1007/s00138-022-01276-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cross-validation of a semantic segmentation network for natural history collection specimens},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WPC-SS: Multi-label wear particle classification based on
semantic segmentation. <em>MVA</em>, <em>33</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s00138-022-01287-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel wear particle online images multi-label classification based on semantic segmentation (WPC-SS) is proposed. In this model, both semantic labels and class labels are applied to guide network training, which make the regions with wear particles attain more attention during the process of network training. It solves the problem that it is difficult to classify the small wear particles when they are compared with the background in the online image. In addition, chain channel attention and class attention unit are added to optimize the network to improve the recognition accuracy. The important channels of the network are monitored by chain channel attention, so that the extracted features can be better prepared for the subsequent classification work. Class attention unit can refine the segmentation results and further optimize the classification results. Comparison experiments are executed with the standard image classification method (multi-CNN). The results of experiments show that WPC-SS model surpasses the standard image classification methods in solving the problem of multi-label classification of online wear particle images.},
  archive      = {J_MVA},
  author       = {Fan, Suli and Zhang, Taohong and Guo, Xuxu and Zhang, Ying and Wulamu, Aziguli},
  doi          = {10.1007/s00138-022-01287-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WPC-SS: Multi-label wear particle classification based on semantic segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FERGCN: Facial expression recognition based on graph
convolution network. <em>MVA</em>, <em>33</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01288-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the problems of occlusion, pose change, illumination change, and image blur in the wild facial expression dataset, it is a challenging computer vision problem to recognize facial expressions in a complex environment. To solve this problem, this paper proposes a deep neural network called facial expression recognition based on graph convolution network (FERGCN), which can effectively extract expression information from the face in a complex environment. The proposed FERGCN includes three essential parts. First, a feature extraction module is designed to obtain the global feature vectors from convolutional neural networks branch with triplet attention and the local feature vectors from key point-guided attention branch. Then, the proposed graph convolutional network uses the correlation between global features and local features to enhance the expression information of the non-occluded part, based on the topology graph of key points. Furthermore, the graph-matching module uses the similarity between images to enhance the network’s ability to distinguish different expressions. Results on public datasets show that our FERGCN can effectively recognize facial expressions in real environment, with RAF-DB of 88.23%, SFEW of 56.15% and AffectNet of 62.03%.},
  archive      = {J_MVA},
  author       = {Liao, Lei and Zhu, Yu and Zheng, Bingbing and Jiang, Xiaoben and Lin, Jiajun},
  doi          = {10.1007/s00138-022-01288-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FERGCN: Facial expression recognition based on graph convolution network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated building and evaluation of 2D as-built floor
plans. <em>MVA</em>, <em>33</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s00138-022-01289-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Site inspection is a notably tedious, time-consuming, and error-prone process when carried out manually by construction inspectors. One portion of site inspection is the generation and comparison of as-built drawings to their as-planned counterparts to ensure conformity. Current as-built evaluation systems rely on manually positioning and transporting scanning stations around the site to collect a cohesive map of the environment and then followed by a manual assessment of the site status using the built drawings. This paper proposes an automated alternative to the generation and assessment of 2D as-built floor maps by relying on robotic simultaneous localisation and mapping (SLAM) to build the 2D as-built maps, followed by automated evaluation of the as-built maps using machine vision. The components of the proposed system have been tested through proof of concept experiments inside 2 different constructed sites. Results indicate average errors below 4 cm (site of 30 m x 15 m) between the disparities identified by our proposed system versus ground truth measured disparities.},
  archive      = {J_MVA},
  author       = {Asmar, Daniel and Daher, Rema and Hawari, Yasmine and Khoury, Hiam and Elhajj, Imad H.},
  doi          = {10.1007/s00138-022-01289-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automated building and evaluation of 2D as-built floor plans},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating human body orientation from image depth data and
its implementation. <em>MVA</em>, <em>33</em>(3), 1–19. (<a
href="https://doi.org/10.1007/s00138-022-01290-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a human body orientation estimation method using the Kinect camera depth data. The input of our system consists of three one-dimensional distance-based signals which reflect the body’s surface contours of the human upper body portion, i.e., the upper chest, upper abdomen, and lower abdomen. Such signals are then normalized using their distances to achieve the same amount of the lower parts. All normalized signals are concatenated to provide a mix of contour features. We used Support Vector Regression (SVR) to classify the feature and Kalman Filter to estimate the continuous orientations instead of using discrete orientations. We also extend our work by adding human motion direction to the robust estimate of human body orientation when walking. We conducted two evaluation schemes, i.e., body orientation at static position and body orientation when moving. The experimental results show that our system achieves impressive results by achieving mean average of angle error (MAAE) of $$0.097^{\circ }$$ and $$5.82^{\circ }$$ for estimating body continuous orientation at static position and estimating body continuous orientation when moving, respectively. Therefore, it is very promising to be applied in real implementations.},
  archive      = {J_MVA},
  author       = {Dewantara, Bima Sena Bayu and Saputra, Rizka Wahyu Aditiya and Pramadihanto, Dadet},
  doi          = {10.1007/s00138-022-01290-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Estimating human body orientation from image depth data and its implementation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human action interpretation using convolutional neural
network: A survey. <em>MVA</em>, <em>33</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s00138-022-01291-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action interpretation (HAI) is one of the trending domains in the era of computer vision. It can further be divided into human action recognition (HAR) and human action detection (HAD). The HAR analyzes frames and provides label(s) to overall video, whereas the HAD localizes actor first, in each frame, and then estimates the action score for the detected region. The effectiveness of a HAI model is highly dependent on the representation of spatiotemporal features and the model’s architectural design. For the effective representation of these features, various studies have been carried out. Moreover, to better learn these features and to get the action score on the basis of these features, different designs of deep architectures have also been proposed. Among various deep architectures, convolutional neural network (CNN) is relatively more explored for HAI due to its lesser computational cost. To provide overview of these efforts, various surveys have been published to date; however, none of these surveys is focusing the features’ representation and design of proposed architectures in detail. Secondly, none of these studies is focusing the pose assisted HAI techniques. This study provides a more detailed survey on existing CNN-based HAI techniques by incorporating the frame level as well as pose level spatiotemporal features-based techniques. Besides these, it offers comparative study on different publicly available datasets used to evaluate HAI models based on various spatiotemporal features’ representations. Furthermore, it also discusses the limitations and challenges of the HAI and concludes that human action interpretation from visual data is still very far from the actual interpretation of human action in realistic videos which are continuous in nature and may contain multiple human beings performing multiple actions sequentially or in parallel.},
  archive      = {J_MVA},
  author       = {Malik, Zainab and Shapiai, Mohd Ibrahim Bin},
  doi          = {10.1007/s00138-022-01291-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Human action interpretation using convolutional neural network: A survey},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight convolutional neural network for pose
estimation of a planar model. <em>MVA</em>, <em>33</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s00138-022-01292-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D pose estimation problem consists of calculating the position and orientation of a three-dimensional object from its projection onto a two-dimensional image relative to a given reference frame. In recent years, convolutional neural networks (CNNs) have achieved impressive results in addressing some of the traditional problems of computer vision, including 3D pose estimation. In general, CNNs employed contain convolutional and fully connected layers with many neurons and trainable parameters. That is, they are heavyweight architectures. Such models are difficult to train, highly memory-consuming, and, as the number of trainable parameters increases, they tend to suffer from overfitting. In this work, we present a lightweight CNN called Pose Network with Spatial Pyramid Pooling (PNSPP), capable of estimating the six-degree-of-freedom pose of a planar model from a single RGB image. Inspired by PoseNet, our CNN employs almost the same architecture but contains 4X fewer parameters (for a chosen image size) thanks to its optimized regression layers. In all tests, PNSPP outperformed PoseNet in the pose predictions. The overall relative improvements were in the ranges of 24–40%, and 9–33% for the estimated position and orientation errors, respectively. Other performance metrics, such as RMSE and ADD, also favored PNSPP. Finally, we propose a method that estimates the scale factor $$\beta $$ used in the pose error functions to balance the contributions of the position and orientation terms. Unlike other approaches that perform potentially expensive grid or random searches, our method uses simple heuristics to adjust this value as the neural network training progress. At the end of each experiment, the estimated $$\beta $$ values deviated roughly ± 10% from the optimal values, which in our case seems reasonable given the computational cost of performing more exhaustive searches.},
  archive      = {J_MVA},
  author       = {Ocegueda-Hernández, Vladimir and Román-Godínez, Israel and Mendizabal-Ruiz, Gerardo},
  doi          = {10.1007/s00138-022-01292-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A lightweight convolutional neural network for pose estimation of a planar model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EfficientLiteDet: A real-time pedestrian and vehicle
detection algorithm. <em>MVA</em>, <em>33</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01293-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since safety plays a crucial role and the top priority, in both unmanned and driver-assistance driving systems, there is a need of efficient and accurate detection of captured objects by object detection algorithms in real-time. Directly applying existing models to tackle real-time pedestrian and vehicle detection tasks captured by high speed moving vehicle scenarios has two problems. First, the target scale varies drastically because the vehicle speed changes greatly. Second, captured images contain both tiny targets and high density targets, which brings in occlusion between targets. To solve the two issues, an efficient light weight real-time detection algorithm is proposed, which is referred to as EfficientLiteDet. Based on Tiny-YOLOv4, one more prediction head is introduced in the proposed model to detect multi-scale targets effectively. In order to detect tiny and occluded denser targets, we used Transformer Prediction Heads (TPH) instead of original anchor detection heads in our model. To explore the potential of self-attention mechanism in TPH, the proposed model integrates “convolutional block attention model” to locate crucial attention region on scenarios with denser targets. Further to improve the detection performance of our model, we applied various data augmentation strategies such as mosaic, mix-up, multi-scale, and random-horizontal-flip during the model training. Extensive experiments are conducted on five challenging pedestrian and vehicle datasets shows that the EfficientLiteDet model has better performance in real-time scenarios. On Pascal Voc-2007, Highway and Udacity datasets, the proposed model achieves mean average precision (mAP) 87.3%, 80.1% and 77.8%, respectively, which is quite better than Tiny-YOLOv4 state-of-the-art algorithm by + 2.4%, 1.8% and + 2.4%, respectively.},
  archive      = {J_MVA},
  author       = {Murthy, Chintakindi Balaram and Hashmi, Mohammad Farukh and Keskar, Avinash G.},
  doi          = {10.1007/s00138-022-01293-y},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {EfficientLiteDet: A real-time pedestrian and vehicle detection algorithm},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional neural network-based cross-corpus speech
emotion recognition with data augmentation and features fusion.
<em>MVA</em>, <em>33</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01294-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) is one of the most challenging and active research topics in data science due to its wide range of applications in human–computer interaction, computer games, mobile services and psychological assessment. In the past, several studies have employed handcrafted features to classify emotions and achieved good classification accuracy. However, such features degrade the classification accuracy in complex scenarios. Thus, recent studies employed deep learning models to automatically extract the local representation from given audio signals. Though, automated feature engineering overcomes the issues of handcrafted feature extraction approach. However, still there is a need to further improve the performance of reported techniques. This is because, in reported techniques, single-layer and two-layer convolutional neural networks (CNNs) were used and these architectures are not capable of learning optimal features from complex speech signals. Thus, to overcome this limitation, this study proposed a novel SER framework, which applies data augmentation methods before extracting seven informative feature sets from each utterance. The extracted feature vector is used as input to the 1D CNN for emotions recognition using the EMO-DB, RAVDESS and SAVEE databases. Moreover, this study also proposed a cross-corpus SER model using the all audio files of common emotions of aforementioned databases. The experimental results showed that our proposed SER framework outperformed existing SER frameworks. Specifically, the proposed SER framework obtained 96.7% accuracy for EMO-DB with all utterances in seven emotions, 90.6% RAVDESS with all utterances in eight emotions, 93.2% for SAVEE with all utterances in seven emotions and 93.3% for cross-corpus with 1930 utterances in six emotions. We believe that our proposed framework will bring significant contribute to SER domain.},
  archive      = {J_MVA},
  author       = {Jahangir, Rashid and Teh, Ying Wah and Mujtaba, Ghulam and Alroobaea, Roobaea and Shaikh, Zahid Hussain and Ali, Ihsan},
  doi          = {10.1007/s00138-022-01294-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Convolutional neural network-based cross-corpus speech emotion recognition with data augmentation and features fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view damage inspection using single-view damage
projection. <em>MVA</em>, <em>33</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01295-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-view computer vision models for vehicle damage inspection often suffer from strong light reflections. To resolve this, multiple images under various viewpoints can be used. However, multiple views increase the complexity as multi-view training data, specialized models, and damage re-identification over different views are required. In addition, traditional point cloud applications require large computational power, being impractical for edge computing. Therefore, multi-view damage inspection has not yet found its way into practical applications. We present a novel approach that projects the results from widely available single-view computer vision models onto 3D representations, to combine the detections from various viewpoints. With this, we leverage all advantages of multi-view damage inspection, without the need for multi-view training data and specialized models or hardware. We conduct a practical evaluation using a drive-through camera setup, to show the applicability of the methods in practice. We show that our proposed method successfully combines similar damages across viewpoints, reducing the number of duplicate damages by almost 99%. In addition, we show that our approach reduces the number of false positives by 96%. The proposed method leverages the existing single-view training data and single-view deep learning models to make multi-view inspection more accessible for practical implementations.},
  archive      = {J_MVA},
  author       = {van Ruitenbeek, R. E. and Bhulai, S.},
  doi          = {10.1007/s00138-022-01295-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-view damage inspection using single-view damage projection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-precision calibration of wide-angle fisheye lens with
radial distortion projection ellipse constraint (RDPEC). <em>MVA</em>,
<em>33</em>(3), 1–23. (<a
href="https://doi.org/10.1007/s00138-022-01296-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel technique for wide-angle fisheye lens calibration which requires neither metric information nor particular reference pattern. First, the fisheye imaging model with the interior Orientation parameters (IOPs)—principal point (u0,v0), focal length f, aspect ratio λ and radial distortion coefficients (k1, k2), is established. Then, upon the fisheye imaging model and the parameter dependency between f and (k1, k2), the radial distortion projection ellipse constraint (RDPEC) for space lines in fisheye image is mathematically formulated to build a non-linear calibration model for high-precision estimation of the IOPs. In this step, parameter initialization based on the geometry of fisheye image outline ellipse (FIOE) is discussed as well. Finally, initial IOPs are further optimized though least square technique by taking the projection ellipse arcs of space lines in fisheye image as observation. The proposed calibration technique was tested on two kinds of fisheye images: (a) simulated image with a set of ground-truth IOPs, (b) internet images with unknown IOPs. Experimental results show that the calibration parameters in this paper are in the best agreement with the fisheye imaging model, compared with the ground-truth parameters and the parameters estimated by two state-of-the-art literature. Compared to that by a state-of-the-art CNN and the well-known software DXO, the proposed technique can enable a high-quality correction of fisheye images in different regions. This makes it very useful in application scenarios containing space lines, such as urban panorama surveillance, auto-parking and, robot navigation.},
  archive      = {J_MVA},
  author       = {Huang, Mingyi and Wu, Jun and Zhiyong, Peng and Zhao, Xuemei},
  doi          = {10.1007/s00138-022-01296-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {High-precision calibration of wide-angle fisheye lens with radial distortion projection ellipse constraint (RDPEC)},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep transfer learning algorithms applied to synthetic
drawing images as a tool for supporting alzheimer’s disease prediction.
<em>MVA</em>, <em>33</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s00138-022-01297-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases, such as Alzheimer’s Disease or Parkinson’s disease, are unfortunately still incurable, although there are many therapies that can slow down the progression of the disease and improve patients’ lives. An essential condition, however, is the early diagnosis of these disorders to begin therapies as soon as possible: In fact, when the signs of the disease become evident, damages may be already significant and irreversible. In this context, it is generally agreed that handwriting is one of the first skills affected by the onset of cognitive disorders. For this reason, in a preliminary study, we considered a database of handwriting and drawing specimens and proposed a method for selecting the most relevant information for diagnosing neurodegenerative disorders. The basic idea was to generate, for each handwriting sample, a color image to exploit the ability of convolutional neural network to automatically extract features from raw images. In the generated images, the color of each elementary trait encodes, in the three RGB channels, the dynamic information associated with that trait. Starting from the very encouraging obtained results, the aim of this study is twofold: On the one hand, we have tried to improve the feature extraction phase, associating further dynamic information with each handwritten trait. On the other hand, we have expanded the database of handwriting samples by adding specimen derived from more complex drawing tasks. Finally, we carried out a large set of experiments for comparing the results obtained by using standard online features with those obtained with our feature extraction approach.},
  archive      = {J_MVA},
  author       = {Cilia, Nicole D. and D’Alessandro, Tiziana and De Stefano, Claudio and Fontanella, Francesco},
  doi          = {10.1007/s00138-022-01297-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep transfer learning algorithms applied to synthetic drawing images as a tool for supporting alzheimer’s disease prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Paired-d++ GAN for image manipulation with text.
<em>MVA</em>, <em>33</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-022-01298-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image manipulation with text is to semantically modify the appearance of an object in a source image based on the given text describing the novel visual attributes while retaining other irrelevant information in the image, such as the background. This has a wide range of applications, such as intelligent image manipulation, and is helpful to those who are not good at painting. We propose a generative adversarial network having a pair of discriminators with different architectures, namely Paired-D++ GAN, for image manipulation with text where the two discriminators make different judgments: one for foreground synthesis and the other for background synthesis. The generator of Paired-D++ GAN has the encoder–decoder architecture with skip-connections and synthesizes an object’s appearance matching the given text description while preserving other parts of the source image. The two discriminators judge the foreground and background of the synthesized image separately to meet the given input text description and the given source image. The Paired-D++ GAN is trained using the effectively unconditional and conditional adversarial learning process in a simultaneous three-player minimax game. Our comprehensively experimental results on the Caltech-200 bird dataset and the Oxford-102 flower dataset show that Paired-D++ GAN can semantically synthesize images to match an input text description while retaining the background in a source image against the state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Vo, Duc Minh and Sugimoto, Akihiro},
  doi          = {10.1007/s00138-022-01298-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Paired-d++ GAN for image manipulation with text},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-world super-resolution under the guidance of optimal
transport. <em>MVA</em>, <em>33</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01299-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, lacking paired training data makes image super-resolution (SR) be a tricky unsupervised task. Existing methods are mainly train models on synthetic datasets and achieve the tradeoff between detail restoration and noise artifact suppression based on a priori knowledge, which indicate it cannot be optimal in both aspects. To solve this problem, we propose OTSR, a single image super-resolution method based on optimal transport theory. OTSR aims to find the optimal solution to the ill-posed SR problem, so that the model can restore high-frequency detail accurately and also suppress noise and artifacts well. Our method consists of three stages: real-world images degradation estimation, LR images generation and model optimization based on quadratic Wasserstein distance. Through the first two stages, the problem of no paired image is solved. In the third stage, under the guidance of optimal transport theory, the optimal mapping from LR to HR image space is learned. Extensive experiments show that our method outperforms the state-of-the-art methods in terms of both detail repair and noise artifact suppression. The source code is available at https://github.com/cognaclee/OTSR .},
  archive      = {J_MVA},
  author       = {Li, Zezeng and Lei, Na and Shi, Ji and Xue, Hao},
  doi          = {10.1007/s00138-022-01299-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real-world super-resolution under the guidance of optimal transport},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep traffic sign detection and recognition without target
domain real images. <em>MVA</em>, <em>33</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01302-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has become a standard approach to machine vision in recent years. Despite several advances, it requires large amounts of annotated data. Nonetheless, in many applications, large-scale data acquisition and annotation is expensive and data imbalance is an intrinsic problem. To address these challenges, we propose a novel synthetic database generation method that only requires (i) arbitrary natural images, i.e., does not demand real images from the target domain, and (ii) templates of the traffic signs. Our method does not aim at overcoming the training with real data but to be a compatible option when there is a lack of real data. Results with data of multiple countries show that the synthetic database generated without human effort is effective for training a deep traffic sign detector. On large datasets, training with a fully synthetic dataset almost matches the performance of training with a real one. When compared to training with a smaller dataset of real images, training with synthetic images increased the accuracy by 12.25%. The proposed method also improves the performance of the detector when target-domain data are available.},
  archive      = {J_MVA},
  author       = {Tabelini, Lucas and Berriel, Rodrigo and Paixão, Thiago M. and De Souza, Alberto F. and Badue, Claudine and Sebe, Nicu and Oliveira-Santos, Thiago},
  doi          = {10.1007/s00138-022-01302-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep traffic sign detection and recognition without target domain real images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identification of facial skin diseases from face phenotypes
using FSDNet in uncontrolled environment. <em>MVA</em>, <em>33</em>(2),
1–10. (<a href="https://doi.org/10.1007/s00138-021-01259-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial skin diseases occur due to multiple reasons. They may have different or similar phenotypic signs and may psychologically and physically impact the affected person. Therefore, early detection, diagnosis, and prognosis of such skin diseases are prerequisite for proper treatment. In particular, an artificial-intelligence-based system helps dermatologists to identify facial skin diseases, even remotely. Within this context, the purpose of this research is to develop an algorithm to identify from a single face image, potential facial diseases. Only facial phenotypes are used regardless of the condition of acquisition (face pose, illumination, image resolution, etc.). Moreover, no segmentation of region of interests (ROIs) is required as commonly considered in the literature. Technically speaking, a calibrated CNN-based deep neural architecture facial skin diseases network (FSDNet) is proposed. It is a fine-tuned version of VGG 16 with modification of the architecture of the fully connected layer to be suitable for facial skin diseases identification. Due to the absence of any standard public dataset for the same, we created a database composed of 20000 images (with labeled pathologies) collected from different sources, which is used to train and validate our network. Our study achieves the identification of eight face skin pathologies, normal skin class, and no-face class with an accuracy of 97%},
  archive      = {J_MVA},
  author       = {Saleh, Rola El and Chantaf, Samer and Nait-ali, Amine},
  doi          = {10.1007/s00138-021-01259-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Identification of facial skin diseases from face phenotypes using FSDNet in uncontrolled environment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An anisotropic non-local attention network for image
segmentation. <em>MVA</em>, <em>33</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01265-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies witness that combining contextual and spatial information significantly improves the performance of segmentation networks. Existing methods differ from each other mainly in the way of extracting contextual and spatial information. To comprehensively utilize spatial details from shallow layers, semantic information of deeper layers, and attention mechanism by special pooling, we propose an Anisotropic Non-local Attention Network (ANANet) to jointly acquire contextual and spatial information in a flexible and efficient way. We first present a spatial contextual module with anisotropic pooling (SCMA) to further encode contextual features by integrating traditional square pooling, anisotropic pooling and attention mechanisms. Our SCMA adopts adaptive spatial pooling to extract multi-scale features and designs an anisotropic pooling attention module (APAM) to compensate for the shortage of square pooling. Our APAM first uses horizontal and vertical pooling, and then multiplies one pooling result by another to generate attention maps for long-shaped and anisotropic objects. Then, we propose a non-local channel contextual module (CCM) to fully reuse shallow features by the backbone network for emphasizing channel interdependency. Our CCM encodes category differences to further reduce erroneous segmentation of ambiguous boundary pixels. Finally, we concatenated the outputs of SCMA and CCM to further improve feature representation. Experiments show that our method achieves obviously better results than existing state-of-the-art methods on public datasets.},
  archive      = {J_MVA},
  author       = {Yuan, Feiniu and Zhu, Yaowen and Li, Kang and Fang, Zhijun and Shi, Jinting},
  doi          = {10.1007/s00138-021-01265-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An anisotropic non-local attention network for image segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time 3D reconstruction using point-dependent pose graph
optimization framework. <em>MVA</em>, <em>33</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01268-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential core of structure from motion, full optimization and pose graph optimization are widely used in most of state-of-the-art 3D reconstruction systems, to estimate the motion trajectory of camera during scanning. Comparing to full optimization, the pose graph optimization has the advantages of low computational complexity and fast convergence, while the practical accuracy of pose graph optimization in applications is intrinsically limited by simple loss function independent of points in scene. In this paper, we proposed a point-dependent pose graph optimization (PDPGO) to address this problem and take it as core to construct a 3D high-precision reconstruction system. In our pipeline, we first construct a hierarchical pose graph by aligning the input frame to its overlapping frames searched by a spatial hashing scheme, which reduces the computational complexity of pairwise alignment. We then derive a loss function of PDPGO from global geometry loss, which improves the accuracy of previous methods. Our system is validated on public benchmarks, and experimental results demonstrate the competing performance against the state-of-the-art systems. And the average reconstruction accuracy in all scenes of ICL-NUIM is up to 0.9 cm.},
  archive      = {J_MVA},
  author       = {Liu, Lei and Tang, Tianhang and Chen, Jie and Liu, Yiguang},
  doi          = {10.1007/s00138-021-01268-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real-time 3D reconstruction using point-dependent pose graph optimization framework},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demographic attribute estimation in face videos combining
local information and quality assessment. <em>MVA</em>, <em>33</em>(2),
1–15. (<a href="https://doi.org/10.1007/s00138-021-01269-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, video analysis applications are gaining popularity given the rise of CCTV systems and the availability of video cameras to the general public, such as cameras in mobile devices. Many image analysis and processing tasks have evolved toward video domain, with the advantage of redundant information obtained from several frames, which can help disambiguating many recognition outputs. In this context, there are also particular video problems to deal with, such as uncontrolled scenarios and poor image quality. Most existing works regarding facial demographic estimation are focused on still image datasets; therefore, we propose to address gender and age estimation in video scenarios. In order to handle known video problems such as low-quality image capture, occlusions and pose variations, we propose a threefold strategy to adapt current image-based attribute recognition algorithms. First, we employ a quality assessment step based on 12 metrics to select relevant good quality frames from a face video sequence. Second, we propose a component-based approach to determine the most discriminant local regions of the face for each specific attribute, under these varying conditions. Third, we evaluate different frame combination strategies to produce the final video prediction. In our experimental validation, conducted in 3 datasets (EURECOM Augmented, UvA-Nemo Smile and YouTube Faces datasets), we show the advantages of our proposed strategy for improving video-based demographic attribute classification.},
  archive      = {J_MVA},
  author       = {Becerra-Riera, Fabiola and Morales-González, Annette and Méndez-Vázquez, Heydi and Dugelay, Jean-Luc},
  doi          = {10.1007/s00138-021-01269-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Demographic attribute estimation in face videos combining local information and quality assessment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Potential escalator-related injury identification and
prevention based on multi-module integrated system for public health.
<em>MVA</em>, <em>33</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01273-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Escalator-related injuries threaten public health with the widespread use of escalators. The existing studies tend to focus on after-the-fact statistics, reflecting on the original design and use of defects to reduce the impact of escalator-related injuries, but few attention has been paid to ongoing and impending injuries. In this study, a multi-module escalator safety monitoring system based on computer vision is designed and proposed to simultaneously monitor and deal with three major injury triggers, including losing balance, not holding on to handrails and carrying large items. The escalator identification module is utilized to determine the escalator region, namely the region of interest. The passenger monitoring module is leveraged to estimate the passengers’ pose to recognize unsafe behaviors on the escalator. The dangerous object detection module detects large items that may enter the escalator and raises alarms. The processing results of the above three modules are summarized in the safety assessment module as the basis for the intelligent decision of the system. The experimental results demonstrate that the proposed system has good performance and great application potential.},
  archive      = {J_MVA},
  author       = {Jiao, Zeyu and Lei, Huan and Zong, Hengshan and Cai, Yingjie and Zhong, Zhenyu},
  doi          = {10.1007/s00138-022-01273-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Potential escalator-related injury identification and prevention based on multi-module integrated system for public health},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GMC_FM: A grid and multi-density-based method for matching
ancient chinese architectural images. <em>MVA</em>, <em>33</em>(2),
1–13. (<a href="https://doi.org/10.1007/s00138-022-01274-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protection and digitization of ancient Chinese Buddhist and Daoist temples is an urgent task due to their wooden structures and usual location on high mountainous sites, sensitive to various natural and manmade damages. Image-based 3D reconstruction technology presents a unique advantage owing to its easy and cheap data acquisition. However, due to the shape complexity, repeated structural and textural patterns, how to reliably establish feature correspondences across images is still a great challenging task. To address this issue, we propose a grid-and-multi-density-based feature matching method, called GMC_FM. Firstly, an image is partitioned into clusters based on grid and multi-density analysis. Secondly, an iterative clustering strategy is proposed to adaptively determine the suitable grid size. Thirdly, corresponding clusters from two images are determined by a proposed new matching measure. Finally, new features within the corresponding clusters are extracted by Canny operator and feature correspondences are established through a new similarity measure combining local geometry and gray-level distribution. Our proposed GMC_FM is evaluated on the Buddhist and Taoist Temple Dataset, and satisfactory feature matching results are obtained in terms of matching precision and robustness.},
  archive      = {J_MVA},
  author       = {Hu, Lihua and Nie, Yaoyao and Zhang, Jifu and Zhang, Sulan},
  doi          = {10.1007/s00138-022-01274-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {GMC_FM: A grid and multi-density-based method for matching ancient chinese architectural images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGBD mapping solution for low-cost robot. <em>MVA</em>,
<em>33</em>(2), 1–27. (<a
href="https://doi.org/10.1007/s00138-022-01275-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is focused on the proposal and verification of the RGBD mapping system for a small, low-cost mobile robot. The solution&#39;s requested properties were easy to replicate and easy to extend for further development on commonly available personal computers. The proposed solution is based on a Kinect sensor. Furthermore, 14 feature detectors were evaluated, and an ORB detector was chosen for the final implementation. In the image, pre-processing CLAHE filter was applied. Post-processing used the modification of the RANSAC method. The final solution proves a globally consistent SLAM based on an RGBD sensor. The article also presents research, which suggests a parallelization scheme of the computational process using a multi-core CPU to achieve real-time processing.},
  archive      = {J_MVA},
  author       = {Beňo, Peter and Duchoň, František and Hubinský, Peter and Dekan, Martin and Tölgyessy, Michal and Dobiš, Michal},
  doi          = {10.1007/s00138-022-01275-0},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RGBD mapping solution for low-cost robot},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contextual guided segmentation framework for semi-supervised
video instance segmentation. <em>MVA</em>, <em>33</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01278-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose contextual guided segmentation (CGS) framework for video instance segmentation in three passes. In the first pass, i.e.,preview segmentation, we propose Instance Re-Identification Flow to estimate main properties of each instance (i.e., human/non-human, rigid/deformable, known/unknown category) by propagating its preview mask to other frames. In the second pass, i.e.,contextual segmentation, we introduce multiple contextual segmentation schemes. For human instance, we develop skeleton-guided segmentation in a frame along with object flow to correct and refine the result across frames. For non-human instance, if the instance has a wide variation in appearance and belongs to known categories (which can be inferred from the initial mask), we adopt instance segmentation. If the non-human instance is nearly rigid, we train FCNs on synthesized images from the first frame of a video sequence. In the final pass, i.e.,guided segmentation, we develop a novel fined-grained segmentation method on non-rectangular regions of interest (ROIs). The natural-shaped ROI is generated by applying guided attention from the neighbor frames of the current one to reduce the ambiguity in the segmentation of different overlapping instances. Forward mask propagation is followed by backward mask propagation to further restore missing instance fragments due to re-appeared instances, fast motion, occlusion, or heavy deformation. Finally, instances in each frame are merged based on their depth values, together with human and non-human object interaction and rare instance priority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate the effectiveness of our proposed framework. We achieved the 3rd consistently in the DAVIS Challenges 2017–2019 with 75.4%, 72.4%, and 78.4% in terms of global score, region similarity, and contour accuracy, respectively.},
  archive      = {J_MVA},
  author       = {Le, Trung-Nghia and Nguyen, Tam V. and Tran, Minh-Triet},
  doi          = {10.1007/s00138-022-01278-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Contextual guided segmentation framework for semi-supervised video instance segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delaunay walk for fast nearest neighbor: Accelerating
correspondence matching for ICP. <em>MVA</em>, <em>33</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-022-01279-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point set registration algorithms such as Iterative Closest Point (ICP) are commonly utilized in time-constrained environments like robotics. Finding the nearest neighbor of a point in a reference 3D point set is a common operation in ICP and frequently consumes at least 90% of the computation time. We introduce a novel approach to performing the distance-based nearest neighbor step based on Delaunay triangulation. This greedy algorithm finds the nearest neighbor of a query point by traversing the edges of the Delaunay triangulation created from a reference 3D point set. Our work integrates the Delaunay traversal into the correspondences search of ICP and exploits the iterative aspect of ICP by caching previous correspondences to expedite each iteration. An algorithmic analysis and comparison is conducted showing an order of magnitude speedup for both serial and vector processor implementation.},
  archive      = {J_MVA},
  author       = {Anderson, James D. and Raettig, Ryan M. and Larson, Josh and Nykl, Scott L. and Taylor, Clark N. and Wischgoll, Thomas},
  doi          = {10.1007/s00138-022-01279-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Delaunay walk for fast nearest neighbor: Accelerating correspondence matching for ICP},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCA-IUnet: A residual cross-spatial attention-guided
inception u-net model for tumor segmentation in breast ultrasound
imaging. <em>MVA</em>, <em>33</em>(2), 1–10. (<a
href="https://doi.org/10.1007/s00138-022-01280-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in deep learning technologies have produced immense contributions to biomedical image analysis applications. With breast cancer being the common deadliest disease among women, early detection is the key means to improve survivability. Medical imaging like ultrasound presents an excellent visual representation of the functioning of the organs; however, for any radiologist analysing such scans is challenging and time consuming which delays the diagnosis process. Although various deep learning-based approaches are proposed that achieved promising results, the present article introduces an efficient residual cross-spatial attention-guided inception U-Net (RCA-IUnet) model with minimal training parameters for tumor segmentation using breast ultrasound imaging to further improve the segmentation performance of varying tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception depth-wise separable convolution and hybrid pooling (max pooling and spectral pooling) layers. In addition, cross-spatial attention filters are added to suppress the irrelevant features and focus on the target structure. The segmentation performance of the proposed model is validated on two publicly available datasets using standard segmentation evaluation metrics, where it outperformed the other state-of-the-art segmentation models.},
  archive      = {J_MVA},
  author       = {Punn, Narinder Singh and Agarwal, Sonali},
  doi          = {10.1007/s00138-022-01280-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RCA-IUnet: A residual cross-spatial attention-guided inception U-net model for tumor segmentation in breast ultrasound imaging},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mitigating adversarial perturbations via weakly supervised
object location and regions recombination. <em>MVA</em>, <em>33</em>(2),
1–16. (<a href="https://doi.org/10.1007/s00138-022-01281-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are the most widely used technology in many fields, but they are vulnerable to adversarial attacks which can manipulate their outputs by adding imperceptible perturbation to images. In this paper, we introduce the theory of adversarial examples and the well-known defense methods at present. Then, we propose a universal defense strategy based on weakly supervised object localization and regions recombination. We analyze the distribution of adversarial perturbations, finding that there are more perturbations in the foreground region. Therefore, we first propose a weighted heatmap extraction method based on weakly supervised object localization to label foreground region and background region. Then, we propose a boundary exploration method to separate these two regions. After that, we eliminate the adversarial perturbations in the foreground by bicubic interpolation filtering. Finally, we recombine regions to highlight the rectified foreground region and weaken the background region to get the rectification of adversarial examples which can be correctly classified. We perform comprehensive experiments indicating the proposed method provides better protection than other defense methods, and the average rectification rate is up to 92%.},
  archive      = {J_MVA},
  author       = {Wu, Fei and Guo, Tong and Zhang, Jingjing and Xiao, Limin},
  doi          = {10.1007/s00138-022-01281-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Mitigating adversarial perturbations via weakly supervised object location and regions recombination},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clarity method of fog and dust image in fully mechanized
mining face. <em>MVA</em>, <em>33</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-022-01282-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the abnormal state of equipment and surrounding rocks in the fully mechanized mining face is mainly detected by visual methods. However, the vision sensor works in a low-light environment and it is affected by factors such as water fog and dust, which lead to blurred images. The defogging algorithm of image based on boundary constraint and context regularization has a good effect on image restoration in the daily environment, but the recovery quality is poor in low illumination environment. Therefore, a method based on boundary constraint and nonlinear context regularization is proposed. The model of fog and dust image is established, and the transmittance function is roughly estimated by boundary constraint method. Then, the nonlinear context regularization method based on logarithmic transformation is used to estimate and optimize the scene transmission model to improve the brightness of the image, and the low illumination fog and dust image is restored by the optimized transmittance function. The logarithmic transformation multiple is selected according to the peak value of image brightness. In order to highlight the effectiveness of our method, the widely used and improved Dark Channel Prior or other methods are used for comparison. The experiment results indicate that our method can effectively remove fog and dust and improve the brightness of the image of the fully mechanized face. It is of great significance to ensure safe production and safety of workers and equipment in coal mine.},
  archive      = {J_MVA},
  author       = {Mao, Qinghua and Wang, Yufei and Zhang, Xuhui and Zhao, Xiaoyong and Zhang, Guangming and Mushayi, Kundayi},
  doi          = {10.1007/s00138-022-01282-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Clarity method of fog and dust image in fully mechanized mining face},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic football video production system with edge
processing. <em>MVA</em>, <em>33</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-022-01283-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic video production of sports aims at producing an aesthetic broadcast of sporting events. This is an enabler of low-cost solutions for TV-like streaming of sports events. We present a new video system able to automatically produce a smooth and pleasant broadcast of football games using a camera rig composed of three fixed 4K cameras. The system automatically detects and localizes the main action of a football game and frame it, so it yields a professional cameraman-like production of the football event. We compute the actionness of a football game using a ball detector and an occupancy map representation of players based on a saliency map. The action framing is computed through geometrically pitch modeling. The whole video processing pipeline is done in the edge using smartphones and we report a 25 FPS throughput.},
  archive      = {J_MVA},
  author       = {Carrillo, Henry and Quiroga, Julian and Zapata, Luis and Maldonado, Edisson},
  doi          = {10.1007/s00138-022-01283-0},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic football video production system with edge processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utilizing incremental branches on a one-stage object
detection framework to avoid catastrophic forgetting. <em>MVA</em>,
<em>33</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s00138-022-01284-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous success of deep learning on object detection tasks compels researchers to adopt deep learning models for autonomous driving vehicles. As autonomous driving vehicles grows sophisticated, the top models are expected to detect novel classes beyond its prior objectives. Thus, incremental learning for object detection essentially ensures that a model is able to detect additional classes on the fly. In this work, we demonstrate how to update a model on new data and an existing model to append new classes on the existing model. The proposed method utilizes episodic memory to save finite samples of data and replay them during incremental learning. The results on PASCAL VOC2007 have suggested that the proposed method obtains the least mAP reduction, at 4.3%, compared against the all-classes learning in the 10+10 classes scenario, which is the lowest amongst other prior arts. Our method also has the highest backward and forward transfer among incremental learning strategies, indicating better memorization and adaptability.},
  archive      = {J_MVA},
  author       = {Shieh, Jeng-Lun and Haq, Muhamad Amirul and Haq, Qazi Mazhar ul and Ruan, Shanq-Jang and Chondro, Peter},
  doi          = {10.1007/s00138-022-01284-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Utilizing incremental branches on a one-stage object detection framework to avoid catastrophic forgetting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image dehazing based on multi-scale segmentation and
deep learning. <em>MVA</em>, <em>33</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s00138-022-01285-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image dehazing methods suffer from problems of insufficient dehazing, distortion, and low color contrast. Aiming at this problem, a deep learning single-image dehazing method based on multi-scale segmentation is proposed in this paper. The study found that the haze information in the haze image will decrease with the increase of frequency. Therefore, the haze image is first decomposed into four sub-images of different frequency domains through image segmentation in this article. A dehazing network model composed of four sub-network channels with different complexity is then constructed to extract the haze information contained in each sub-image. After the transmission sub-images are generated, the image fusion technology is used to obtain the final transmittance map. Finally, the haze-free image is obtained based on the physical model of atmospheric scattering. Experimental results on the synthetic and real images dataset show that the proposed method achieves significant dehazing effect and high color contrast with no distortion, showing superior performance than other dehazing methods.},
  archive      = {J_MVA},
  author       = {Yu, Tianhe and Zhu, Ming and Chen, Haiming},
  doi          = {10.1007/s00138-022-01285-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Single image dehazing based on multi-scale segmentation and deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-supervised learning method for surface defect
classification of magnetic tiles. <em>MVA</em>, <em>33</em>(2), 1–14.
(<a href="https://doi.org/10.1007/s00138-022-01286-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect inspection is a crucial step to ensure the quality of magnetic tiles. Recently, deep learning methods have shown excellent performance on many vision tasks. Some deep learning-based methods have been applied to the surface defect inspection of magnetic tiles as well. However, related methods are based on supervised learning, which requires plenty of labeled samples to train deep neural networks. In industrial application scenarios, the annotation of large labeled datasets is extremely expensive, time-consuming, and error-prone. A semi-supervised learning method based on pseudo-labeling is proposed in this paper to address the problem of surface defect classification of magnetic tiles with limited labeled samples. The proposed method consists of two models: the teacher model and the student model. The training procedure is divided into two stages: pseudo-label generation and student model training. In the pseudo-label generation stage, the teacher model parameters and the pseudo-labels of unlabeled samples are alternatively optimized based on the idea of transductive learning. Curriculum learning is employed to reduce the impact of label noise so that high-quality pseudo-labels can be obtained. In the student model training stage, labeled samples and unlabeled samples with pseudo-labels are jointly used to train the classifier, with mixup to achieve information fusion and regularization. The experimental results show that the proposed method outperforms the supervised-only and semi-supervised baselines. With only 4.4% of labeled samples in the training set, the proposed method can still achieve the defect classification accuracy of 90.13%.},
  archive      = {J_MVA},
  author       = {Liu, Tao and Ye, Wei},
  doi          = {10.1007/s00138-022-01286-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A semi-supervised learning method for surface defect classification of magnetic tiles},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic convolutional features for face detection.
<em>MVA</em>, <em>33</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01245-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been extensively used as the key role to address many computer vision applications. Traditionally, learning convolutional features is performed in a hierarchical manner along the dimension of network depth to create multi-scale feature maps. As a result, strong semantic features are derived at the top-level layers only. This paper proposes a novel feature pyramid fashion to produce semantic features at all levels of the network for specially addressing the problem of face detection. Particularly, a Semantic Convolutional Box (SCBox) is presented by merging the features from different layers in a bottom-up fashion. The proposed lightweight detector is stacked of alternating SCBox and Inception residual modules to learn the visual features in both the dimensions of network depth and width. In addition, the newly introduced objective functions (e.g., focal and CIoU losses) are incorporated to effectively address the problem of unbalanced data, resulting in stable training. The proposed model has been validated on the standard benchmarks FDDB and WIDER FACES, in comparison with the state-of-the-art methods. Experiments showed promising results in terms of both processing time and detection accuracy. For instance, the proposed network achieves an average precision of $$96.8\%$$ on FDDB, $$82.4\%$$ on WIDER FACES, and gains an inference speed of 106 FPS on a moderate GPU configuration or 20 FPS on a CPU machine.},
  archive      = {J_MVA},
  author       = {Pham, The-Anh},
  doi          = {10.1007/s00138-021-01245-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semantic convolutional features for face detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural networks in node classification: Survey and
evaluation. <em>MVA</em>, <em>33</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00138-021-01251-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have been proved efficient in improving many machine learning tasks such as convolutional neural networks and recurrent neural networks for computer vision and natural language processing, respectively. However, the inputs of these deep learning paradigms all belong to the type of Euclidean structure, e.g., images or texts. It is difficult to directly apply these neural networks to graph-based applications such as node classification since graph is a typical non-Euclidean structure in machine learning domain. Graph neural networks are designed to deal with the particular graph-based input and have received great developments because of more and more research attention. In this paper, we provide a comprehensive review about applying graph neural networks to the node classification task. First, the state-of-the-art methods are discussed and divided into three main categories: convolutional mechanism, attention mechanism and autoencoder mechanism. Afterward, extensive comparative experiments are conducted on several benchmark datasets, including citation networks and co-author networks, to compare the performance of different methods with diverse evaluation metrics. Finally, several suggestions are provided for future research based on the experimental results.},
  archive      = {J_MVA},
  author       = {Xiao, Shunxin and Wang, Shiping and Dai, Yuanfei and Guo, Wenzhong},
  doi          = {10.1007/s00138-021-01251-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Graph neural networks in node classification: Survey and evaluation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Viewpoint placement for inspection planning. <em>MVA</em>,
<em>33</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s00138-021-01252-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspection planning approaches so far have focused on automatically obtaining an optimal set of viewpoints required to cover a given object. While research has provided interesting results, the automatic inspection planning has still not been made a part of the everyday inspection system development process. This is mostly because the plans are difficult to verify and it is impossible to compare them to laboratory-developed plans. In this work, we give an overview of available generate-and-test approaches, evaluate their results for various objects and finally compare them to plans created by inspection system development experts. The comparison emphasizes both benefits and downsides of automated approaches and highlights problems which need to be tackled in the future in order to make the automated inspection planning more applicable.},
  archive      = {J_MVA},
  author       = {Gospodnetić, Petra and Mosbach, Dennis and Rauhut, Markus and Hagen, Hans},
  doi          = {10.1007/s00138-021-01252-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Viewpoint placement for inspection planning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-planar geometry and latent image recovery from a
single motion-blurred image. <em>MVA</em>, <em>33</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01254-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing works for depth estimation and image deblurring in the presence of depth-dependent blur work with the assumption of a multi-layered scene wherein each layer is modeled in the form of a fronto-parallel plane. In this work, we attempt to relax these constraints by considering more generalized settings of a 3D scene with piecewise planar structure, i.e., a scene that can be modeled as a combination of multiple planes with arbitrary orientations. To this end, we first propose a novel approach to estimate the normal of a planar surface from a single motion-blurred image. We then extend this idea and develop an algorithm for automatic recovery of the number of planes, the parameters corresponding to each plane, and camera motion from a single motion-blurred image of a multi-planar 3D scene. Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal that our proposed method achieves state-of-the-art results on the dual problem of depth recovery and image deblurring.},
  archive      = {J_MVA},
  author       = {Purohit, Kuldeep and Vasu, Subeesh and Rao, M. Purnachandra and Rajagopalan, A. N.},
  doi          = {10.1007/s00138-021-01254-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-planar geometry and latent image recovery from a single motion-blurred image},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image projection method for vehicle speed estimation model
in video system. <em>MVA</em>, <em>33</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01255-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in Information and Communication Technology have facilitated a new concept of Smart City to the world. The Smart City concept is driven by technology intervention in every aspect of city life, including the most dynamic and unpredictable transport management. Intelligent transport management system (ITMS) is the most essential component of a Smart City ecosystem. The primary function is to ensure smooth and accident-free transport on the city roads. ITMS can prompt drivers of possible traffic jams; ITMS can be used to detect speed violations of vehicles. One of the primary inputs to ITMS is fed from closed circuit television (CCTV) installations on the roads. The main objective of the paper is to detect vehicles violating traffic rules especially over-speeding. The detection of over-speeding of a vehicle involves detection of vehicle, calculation and calibration of the distance traveled by the vehicle both on an image plane and real world. To calibrate the distance traveled by the vehicle, the geometric plane of the real world is projected onto the image plane. The projection onto the image plane helps in determining the actual distance traveled by the vehicle in the real world. After calibration of the distance traveled by the vehicle, speed calculation is performed. The accuracy of the algorithm to speed detection is 90.8%.},
  archive      = {J_MVA},
  author       = {Anandhalli, Mallikarjun and Baligar, Pavana and Saraf, Santosh S. and Deepsir, Pooja},
  doi          = {10.1007/s00138-021-01255-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Image projection method for vehicle speed estimation model in video system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The overlapping effect and fusion protocols of data
augmentation techniques in iris PAD. <em>MVA</em>, <em>33</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s00138-021-01256-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris Presentation Attack Detection (PAD) algorithms address the vulnerability of iris recognition systems to presentation attacks. With the great success of deep learning methods in various computer vision fields, neural network-based iris PAD algorithms emerged. However, most PAD networks suffer from overfitting due to insufficient iris data variability. Therefore, we explore the impact of various data augmentation techniques on performance and the generalizability of iris PAD. We apply several data augmentation methods to generate variability, such as shift, rotation, and brightness. We provide in-depth analyses of the overlapping effect of these methods on performance. In addition to these widely used augmentation techniques, we also propose an augmentation selection protocol based on the assumption that various augmentation techniques contribute differently to the PAD performance. Moreover, two fusion methods are performed for more comparisons: the strategy-level and the score-level combination. We demonstrate experiments on two fine-tuned models and one trained from the scratch network and perform on the datasets in the Iris-LivDet-2017 competition designed for generalizability evaluation. Our experimental results show that augmentation methods improve iris PAD performance in many cases. Our least overlap-based augmentation selection protocol achieves the lower error rates for two networks. Besides, the shift augmentation strategy also exceeds state-of-the-art (SoTA) algorithms on the Clarkson and IIITD-WVU datasets.},
  archive      = {J_MVA},
  author       = {Fang, Meiling and Damer, Naser and Boutros, Fadi and Kirchbuchner, Florian and Kuijper, Arjan},
  doi          = {10.1007/s00138-021-01256-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {The overlapping effect and fusion protocols of data augmentation techniques in iris PAD},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object detection by crossing relational reasoning based on
graph neural network. <em>MVA</em>, <em>33</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01257-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing relational representations to facilitate object detection has attracted growing research attention in recent years. However, previous studies mainly focus on relationships within the region proposals or within the label embeddings and pay less attention to the relationships between them. To fill this gap, we propose a novel object detection framework that fully explores the relationships across visual feature space and label embedding space to facilitate the proposal classification in object detection. Specifically, we model the region proposals and class labels into a uniform relation graph, where the extracted proposals and labels are regarded as nodes and each pair of them is associated by an assignment edge, and convert the problem of classifying proposals to the problem of selecting reliable edges from the constructed relation graph. Furthermore, a graph convolutional module is developed to perform relational reasoning on the graph, which finally predicts a label for each assignment edge to indicate whether the classification is reliable or not. The updated relational representations for proposals are used for bounding box regression. Embedding our framework into state-of-the-art baselines, we perform extensive comparison experiments on two public benchmarks, i.e., Pascal VOC and COCO2017. And the experimental results demonstrate the flexibility and effectiveness of the proposed framework.},
  archive      = {J_MVA},
  author       = {You, XiuTing and Liu, He and Wang, Tao and Feng, Songhe and Lang, Congyan},
  doi          = {10.1007/s00138-021-01257-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Object detection by crossing relational reasoning based on graph neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep-plane sweep generative adversarial network for
consistent multi-view depth estimation. <em>MVA</em>, <em>33</em>(1),
1–10. (<a href="https://doi.org/10.1007/s00138-021-01258-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the improved representation ability, recent deep learning-based methods enable to estimate scene depths accurately. However, these methods still have difficulty in estimating consistent scene depths under real-world environments containing severe illumination changes, occlusions, and texture-less regions. To solve this problem, in this paper, we propose a novel depth-estimation method for unstructured multi-view images. Accordingly, we present a plane sweep generative adversarial network, where the proposed adversarial loss significantly improves the depth-estimation accuracy under real-world settings, and the consistency loss makes the depth-estimation results insensitive to the changes in viewpoints and the number of input images. In addition, 3D convolution layers are inserted into the network to enrich feature representation. Experimental results indicate that the proposed plane sweep generative adversarial network quantitatively and qualitatively outperforms state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Shu, Dong Wook and Jang, Wonbeom and Yoo, Heebin and Shin, Hong-Chang and Kwon, Junseok},
  doi          = {10.1007/s00138-021-01258-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep-plane sweep generative adversarial network for consistent multi-view depth estimation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective triplet mining improves training of multi-scale
pooled CNN for image retrieval. <em>MVA</em>, <em>33</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01260-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of content-based image retrieval (CBIR) by learning images representations based on the activations of a Convolutional Neural Network. We propose an end-to-end trainable network architecture that exploits a novel multi-scale local pooling based on the trainable aggregation layer NetVLAD (Arandjelovic et al in Proceedings of the IEEE conference on computer vision and pattern recognition CVPR, NetVLAD, 2016) and bags of local features obtained by splitting the activations, allowing to reduce the dimensionality of the descriptor and to increase the performance of retrieval. Training is performed using an improved triplet mining procedure that selects samples based on their difficulty to obtain an effective image representation, reducing the risk of overfitting and loss of generalization. Extensive experiments show that our approach, that can be effectively used with different CNN architectures, obtains state-of-the-art results on standard and challenging CBIR datasets.},
  archive      = {J_MVA},
  author       = {Vaccaro, Federico and Bertini, Marco and Uricchio, Tiberio and Bimbo, Alberto Del},
  doi          = {10.1007/s00138-021-01260-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Effective triplet mining improves training of multi-scale pooled CNN for image retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MLMT-CNN for object detection and segmentation in
multi-layer and multi-spectral images. <em>MVA</em>, <em>33</em>(1),
1–15. (<a href="https://doi.org/10.1007/s00138-021-01261-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely localising solar Active Regions (AR) from multi-spectral images is a challenging but important task in understanding solar activity and its influence on space weather. A main challenge comes from each modality capturing a different location of the 3D objects, as opposed to typical multi-spectral imaging scenarios where all image bands observe the same scene. Thus, we refer to this special multi-spectral scenario as multi-layer. We present a multi-task deep learning framework that exploits the dependencies between image bands to produce 3D AR localisation (segmentation and detection) where different image bands (and physical locations) have their own set of results. Furthermore, to address the difficulty of producing dense AR annotations for training supervised machine learning (ML) algorithms, we adapt a training strategy based on weak labels (i.e. bounding boxes) in a recursive manner. We compare our detection and segmentation stages against baseline approaches for solar image analysis (multi-channel coronal hole detection, SPOCA for ARs) and state-of-the-art deep learning methods (Faster RCNN, U-Net). Additionally, both detection and segmentation stages are quantitatively validated on artificially created data of similar spatial configurations made from annotated multi-modal magnetic resonance images. Our framework achieves an average of 0.72 IoU (segmentation) and 0.90 F1 score (detection) across all modalities, comparing to the best performing baseline methods with scores of 0.53 and 0.58, respectively, on the artificial dataset, and 0.84 F1 score in the AR detection task comparing to baseline of 0.82 F1 score. Our segmentation results are qualitatively validated by an expert on real ARs.},
  archive      = {J_MVA},
  author       = {Almahasneh, Majedaldein and Paiement, Adeline and Xie, Xianghua and Aboudarham, Jean},
  doi          = {10.1007/s00138-021-01261-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MLMT-CNN for object detection and segmentation in multi-layer and multi-spectral images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of different machine learning techniques
for brain tumor classification and subsequent segmentation using hybrid
texture feature. <em>MVA</em>, <em>33</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01262-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor classification and segmentation for different weighted MRIs are among the most tedious tasks for many researchers due to the high variability of tumor tissues based on texture, structure, and position. Our study is divided into two stages: supervised machine learning-based tumor classification and image processing-based region of tumor extraction. For this job, seven methods have been used for texture feature generation. We have experimented with various state-of-the-art supervised machine learning classification algorithms such as support vector machines (SVMs), K-nearest neighbors (KNNs), binary decision trees (BDTs), random forest (RF), and ensemble methods. Then considering texture features into account, we have tried for fuzzy C-means (FCM), K-means, and hybrid image segmentation algorithms for our study. The experimental results achieved a classification accuracy of 94.25%, 87.88%, 89.57%, 96.99%, and 97% with SVM, KNN, BDT, RF, and Ensemble methods, respectively, on FLAIR-, T1C-, and T2-weighted MRI, and the hybrid segmentation attaining 90.16% mean dice score for tumor area segmentation against ground-truth images.},
  archive      = {J_MVA},
  author       = {Jena, Biswajit and Nayak, Gopal Krishna and Saxena, Sanjay},
  doi          = {10.1007/s00138-021-01262-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An empirical study of different machine learning techniques for brain tumor classification and subsequent segmentation using hybrid texture feature},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet and PCA-based glaucoma classification through novel
methodological enhanced retinal images. <em>MVA</em>, <em>33</em>(1),
1–42. (<a href="https://doi.org/10.1007/s00138-021-01263-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have proposed a systematic retinal image enhancement and classification method. The proposed method deals with balancing all the visual and technical aspects of the image for glaucoma diagnosis. Initially, similar 3D image blocks are obtained for each retinal image using novel block-matching and grouping techniques. The proposed enhancement technique emphasizes these blocks, which constitute careful estimation of low frequency for each 3D block followed by enhancement using the new alpha-rooting method with adaptive alpha value. During this, the image may over-enhance in some areas, which can be corrected through the image polishing phase that uses cumulative distribution function (CDF) transformations. The enhanced retinal images are qualitatively compared with the outcomes of some existing methods and are employed in glaucoma classification using principal component analysis (PCA) and its variants using discrete wavelet transformations (DWT). We have carried out a deep investigation to find the best combination of DWT and PCA variants. The results obtained from the implementation proved that the performance of the proposed method is highly satisfactory.},
  archive      = {J_MVA},
  author       = {Santosh, N. Krishna and Barpanda, Soubhagya Sankar},
  doi          = {10.1007/s00138-021-01263-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Wavelet and PCA-based glaucoma classification through novel methodological enhanced retinal images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inflated 3D ConvNet context analysis for violence detection.
<em>MVA</em>, <em>33</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01264-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the Wall Street Journal, one billion surveillance cameras will be deployed around the world by 2021. This amount of information can be hardly managed by humans. Using a Inflated 3D ConvNet as backbone, this paper introduces a novel automatic violence detection approach that outperforms state-of-the-art existing proposals. Most of those proposals consider a pre-processing step to only focus on some regions of interest in the scene, i.e., those actually containing a human subject. In this regard, this paper also reports the results of an extensive analysis on whether and how the context can affect or not the adopted classifier performance. The experiments show that context-free footage yields substantial deterioration of the classifier performance (2% to 5%) on publicly available datasets. However, they also demonstrate that performance stabilizes in context-free settings, no matter the level of context restriction applied. Finally, a cross-dataset experiment investigates the generalizability of results obtained in a single-collection experiment (same dataset used for training and testing) to cross-collection settings (different datasets used for training and testing).},
  archive      = {J_MVA},
  author       = {Freire-Obregón, David and Barra, Paola and Castrillón-Santana, Modesto and Marsico, Maria De},
  doi          = {10.1007/s00138-021-01264-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Inflated 3D ConvNet context analysis for violence detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UP-net: Unique keyPoint description and detection net.
<em>MVA</em>, <em>33</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01266-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer vision tasks, such as simultaneous localization and mapping, visual localization, image retrieval, pose estimation, structure-from-motion, rely on the keypoints matching relationship between image pairs. Recently, jointly learned keypoint descriptor and detector nets with simple structure have shown highly competitive performance. However, most of them have two limitations: 1) The positioning accuracy of detected keypoints is poor, which has a negative impact on many applications; 2) by only emphasizing repeatability in keypoint detection, mismatches may occur easily in the texture areas. In this work, we make two enhancements on D2-Net to address these two problems: Firstly, feature fusion is used to enrich feature information of different levels and solve the problem of positioning accuracy of keypoints; secondly, the uniqueness index of keypoints is added in the keypoint detection, and keypoints of the repeated pattern in the texture region are eliminated, which makes keypoints more effective and accurate. Furthermore, we use homography to build the correspondence between image pairs and use it to achieve unsupervised training. Our method achieves leading performance on the HPatches dataset for image matching, especially on its illumination sequences, with a 5 $$\%$$ improvement over the state-of-the-art ASLFeat method at a projection error threshold of 10 px. Meanwhile, our keypoint positioning accuracy is twice than that of D2-Net with the strict projection error threshold. It also exhibits competitive performance in 3D reconstruction and visual localization experiments.},
  archive      = {J_MVA},
  author       = {Yang, Ning and Han, Yunlong and Fang, Jun and Zhong, Weijun and Xu, Anlin},
  doi          = {10.1007/s00138-021-01266-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {UP-net: Unique keyPoint description and detection net},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orchid classification using homogeneous ensemble of small
deep convolutional neural network. <em>MVA</em>, <em>33</em>(1), 1–13.
(<a href="https://doi.org/10.1007/s00138-021-01267-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orchids are flowering plants in the large and diverse family Orchidaceae. Orchid flowers may share similar visual characteristics even they are from different species. Thus, classifying orchid species from images is a hugely challenging task. Motivated by the inadequacy of the current state-of-the-art general-purpose image classification methods in differentiating subtle differences between orchid flower images, we propose a hybrid model architecture to better classify the orchid species from images. The model architecture is composed of three parts: the global prediction network (GPN), the local prediction network (LPN), and the ensemble neural network (ENN). The GPN predicts the orchid species by global features of orchid flowers. The LPN looks into local features such as the organs of orchid plant via a spatial transformer network. Finally, the ENN fuses the intermediate predictions from the GPN and the LPN modules and produces the final prediction. All modules are implemented based on a robust convolutional neural network with transfer learning methodology from notable existing models. Due to the interplay between the modules, we also guidelined the training steps necessary for achieving higher predictive performance. The classification results based on an extensive in-house Orchids-52 dataset demonstrated the superiority of the proposed method compared to the state of the art.},
  archive      = {J_MVA},
  author       = {Sarachai, Watcharin and Bootkrajang, Jakramate and Chaijaruwanich, Jeerayut and Somhom, Samerkae},
  doi          = {10.1007/s00138-021-01267-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Orchid classification using homogeneous ensemble of small deep convolutional neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Teacher–student training and triplet loss to reduce the
effect of drastic face occlusion. <em>MVA</em>, <em>33</em>(1), 1–19.
(<a href="https://doi.org/10.1007/s00138-021-01270-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a series of recognition tasks in two realistic scenarios requiring the analysis of faces under strong occlusion. On the one hand, we aim to recognize facial expressions of people wearing virtual reality headsets. On the other hand, we aim to estimate the age and identify the gender of people wearing surgical masks. For all these tasks, the common ground is that half of the face is occluded. In this challenging setting, we show that convolutional neural networks trained on fully visible faces exhibit very low performance levels. While fine-tuning the deep learning models on occluded faces is extremely useful, we show that additional performance gains can be obtained by distilling knowledge from models trained on fully visible faces. To this end, we study two knowledge distillation methods, one based on teacher–student training and one based on triplet loss. Our main contribution consists in a novel approach for knowledge distillation based on triplet loss, which generalizes across models and tasks. Furthermore, we consider combining distilled models learned through conventional teacher–student training or through our novel teacher–student training based on triplet loss. We provide empirical evidence showing that, in most cases, both individual and combined knowledge distillation methods bring statistically significant performance improvements. We conduct experiments with three different neural models (VGG-f, VGG-face and ResNet-50) on various tasks (facial expression recognition, gender recognition, age estimation), showing consistent improvements regardless of the model or task.},
  archive      = {J_MVA},
  author       = {Georgescu, Mariana-Iuliana and Duţǎ, Georgian-Emilian and Ionescu, Radu Tudor},
  doi          = {10.1007/s00138-021-01270-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Teacher–student training and triplet loss to reduce the effect of drastic face occlusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pattern recognition methodologies for pollen grain image
classification: A survey. <em>MVA</em>, <em>33</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00138-021-01271-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large number of scientific areas, such as immunology, forensics, paleoecology, and archeology, the study of pollen, i.e., palynology, plays an important role: from tracking climate changes, studying allergies, to forensic investigations or honey origin analysis. Since the mid-nineties of the last century, the idea for an automated solution to the problem of pollen identification and classification was formulated and since then, several attempts and proposals have been made and presented, based on different technologies, in particular in the field of Computer Vision. However, as of 2021 microscopic analyses are performed mainly manually by highly trained specialists, although the capabilities of artificial intelligence, especially Deep Neural Networks, are steadily increasing. In this work, we analyzed various state-of-the-art research work concerning pollen detection and classification and compared their methods and results. The problems, such as data accessibility, different methods of Machine Learning, and the intended applicability of the proposed solutions are explored. We also identified crucial issues that require further work and research. Our work will provide a thorough view on the current state of the art, its issues, and possibilities for the future.},
  archive      = {J_MVA},
  author       = {Viertel, Philipp and König, Matthias},
  doi          = {10.1007/s00138-021-01271-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pattern recognition methodologies for pollen grain image classification: A survey},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple feature-based contrast enhancement of ROI of
backlit images. <em>MVA</em>, <em>33</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01272-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backlit image is obtained when image is captured with intense reflection of light. It is a frequently observed condition of lighting that can cause significant image quality deterioration. They are a combination of dark and bright regions, and the objects in the image generally appear to be dark. The region of interest (ROI) depicts some dark image regions or objects present in the image. Such ROI has low contrast in backlit images; therefore, visualization is uncertain. In order to visualize the contents properly, enhancement of ROI in backlit images is essential. A novel and simplified approach based on the multiple features of ROI of backlit images is proposed. The proposed approach’s fundamental idea is to blend different features into a single one to enhance the ROI. Global tone mappings, namely gamma correction and logarithmic transform, are performed while preserving global and local contrast effectively to improve the visual quality. In the next step, gradient map and filter-based operations were performed to preserve the image’s naturalness. Furthermore, the proposed method introduces weight maps based on the exposedness to increase the visibility and the fusion of the results. Experimental results based on the contrast measure (CM), discrete entropy (DE), and balanced mean magnitude of relative error (BMMRE) of discrete entropy reveal the proposed approach’s effectiveness and its gains in visual consistency over existing backlit image enhancement algorithms.},
  archive      = {J_MVA},
  author       = {Yadav, Gaurav and Yadav, Dilip Kumar},
  doi          = {10.1007/s00138-021-01272-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multiple feature-based contrast enhancement of ROI of backlit images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated diagnosis of diverse coffee leaf images through a
stage-wise aggregated triple deep convolutional neural network.
<em>MVA</em>, <em>33</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s00138-022-01277-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the struggles of developing countries in coping with widespread coffee leaf diseases and infestations, the quality and quantity of coffee-based commodities have reduced significantly. This paper proposes a solution to this problem using Deep Convolutional Neural Networks (DCNN) that classifies seven coffee leaf conditions. Unlike other studies, this work proposed a novel Triple-DCNN (T-DCNN) composed of three aggregated DCNN models formed in an ensemble to produce lesser bias and better accuracy than standard models. Added to the proposed T-DCNN, an employed stage-wise approach narrowed down the classification options through a multi-staged structure and diversified the entire feature pool. Upon evaluation, the proposed Stage-Wise Aggregated T-DCNN (SWAT-DCNN) yielded successful diagnoses of diverse coffee leaf conditions in various environmental settings. Furthermore, with an overall accuracy of 95.98%, the SWAT-DCNN outperformed most state-of-the-art DCNNs that performed the same task.},
  archive      = {J_MVA},
  author       = {Montalbo, Francis Jesmar P.},
  doi          = {10.1007/s00138-022-01277-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automated diagnosis of diverse coffee leaf images through a stage-wise aggregated triple deep convolutional neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
