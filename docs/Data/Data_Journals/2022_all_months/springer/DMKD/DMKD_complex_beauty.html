<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---69">DMKD - 69</h2>
<ul>
<li><details>
<summary>
(2022). Wisdom of the contexts: Active ensemble learning for
contextual anomaly detection. <em>DMKD</em>, <em>36</em>(6), 2410–2458.
(<a href="https://doi.org/10.1007/s10618-022-00868-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contextual anomaly detection, an object is only considered anomalous within a specific context. Most existing methods use a single context based on a set of user-specified contextual features. However, identifying the right context can be very challenging in practice, especially in datasets with a large number of attributes. Furthermore, in real-world systems, there might be multiple anomalies that occur in different contexts and, therefore, require a combination of several “useful” contexts to unveil them. In this work, we propose a novel approach, called wisdom of the contexts (WisCon), to effectively detect complex contextual anomalies in situations where the true contextual and behavioral attributes are unknown. Our method constructs an ensemble of multiple contexts, with varying importance scores, based on the assumption that not all useful contexts are equally so. We estimate the importance of each context using an active learning approach with a novel query strategy. Experiments show that WisCon significantly outperforms existing baselines in different categories (i.e., active learning methods, unsupervised contextual and non-contextual anomaly detectors) on 18 datasets. Furthermore, the results support our initial hypothesis that there is no single perfect context that successfully uncovers all kinds of contextual anomalies, and leveraging the “wisdom” of multiple contexts is necessary.},
  archive      = {J_DMKD},
  author       = {Calikus, Ece and Nowaczyk, Sławomir and Bouguelia, Mohamed-Rafik and Dikmen, Onur},
  doi          = {10.1007/s10618-022-00868-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2410-2458},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Wisdom of the contexts: Active ensemble learning for contextual anomaly detection},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). POI recommendation with queuing time and user interest
awareness. <em>DMKD</em>, <em>36</em>(6), 2379–2409. (<a
href="https://doi.org/10.1007/s10618-022-00865-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-interest (POI) recommendation is a challenging problem due to different contextual information and a wide variety of human mobility patterns. Prior studies focus on recommendation that considers user travel spatiotemporal and sequential patterns behaviours. These studies do not pay attention to user personal interests, which is a significant factor for POI recommendation. Besides user interests, queuing time also plays a significant role in affecting user mobility behaviour, e.g., having to queue a long time to enter a POI might reduce visitor’s enjoyment. Recently, attention-based recurrent neural networks-based approaches show promising performance in the next POI recommendation task. However, they are limited to single head attention, which can have difficulty in finding the appropriate user mobility behaviours considering complex relationships among POI spatial distances, POI check-in time, user interests and POI queuing times. In this research work, we are the first to consider queuing time and user interest awareness factors for next POI recommendation. We demonstrate how it is non-trivial to recommend a next POI and simultaneously predict its queuing time. To solve this problem, we propose a multi-task, multi-head attention transformer model called TLR-M_UI. The model recommends the next POIs to the target users and predicts queuing time to access the POIs simultaneously by considering user mobility behaviours. The proposed model utilises POIs description-based user personal interest that can also solve the new categorical POI cold start problem. Extensive experiments on six real-world datasets show that the proposed models outperform the state-of-the-art baseline approaches in terms of precision, recall, and F1-score evaluation metrics. The model also predicts and minimizes the queuing time. For the reproducibility of the proposed model, we have publicly shared our implementation code at GitHub ( https://github.com/sajalhalder/TLR-M_UI ).},
  archive      = {J_DMKD},
  author       = {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and Zhang, Xiuzhen},
  doi          = {10.1007/s10618-022-00865-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2379-2409},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {POI recommendation with queuing time and user interest awareness},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing polarization and increasing diverse navigability in
graphs by inserting edges and swapping edge weights. <em>DMKD</em>,
<em>36</em>(6), 2334–2378. (<a
href="https://doi.org/10.1007/s10618-022-00875-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sets of hyperlinks in web pages, relationship ties in social networks, or sets of recommendations in recommender systems, have a major impact on the diversity of content accessed by the user in a browsing session. Bias induced by the graph structure may trap a reader in a polarized bubble with no access to other opinions. It is widely accepted that exposure to diverse opinions creates more informed citizens and consumers. We introduce the concept of the polarized bubble radius of a node, as the expected length of a random walk from it to a node of different opinion. Using the bubble radius, we define the measures of structural bias and diverse navigability to quantify the effect of links and recommendations on the diversity of content visited in a browsing session. We then propose algorithmic techniques to reduce the structural bias of the graph or improve the diverse navigability of the system through minimal modifications, such as edge insertions or flipping the order of existing links or recommendations, corresponding to switching the edge traversal probabilities. Under mild conditions, our techniques obtain a constant factor-approximation of their respective tasks. In our extensive experimental evaluation, we show that our algorithms reduce the structural bias or improve the diverse navigability faster than appropriate baselines, including some designed with the goal of reducing the polarization of a graph.},
  archive      = {J_DMKD},
  author       = {Haddadan, Shahrzad and Menghini, Cristina and Riondato, Matteo and Upfal, Eli},
  doi          = {10.1007/s10618-022-00875-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2334-2378},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Reducing polarization and increasing diverse navigability in graphs by inserting edges and swapping edge weights},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalised meta-path generation for heterogeneous graph
neural networks. <em>DMKD</em>, <em>36</em>(6), 2299–2333. (<a
href="https://doi.org/10.1007/s10618-022-00862-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, increasing attention has been paid to heterogeneous graph representation learning (HGRL), which aims to embed rich structural and semantic information in heterogeneous information networks (HINs) into low-dimensional node representations. To date, most HGRL models rely on hand-crafted meta-paths. However, the dependency on manually-defined meta-paths requires domain knowledge, which is difficult to obtain for complex HINs. More importantly, the pre-defined or generated meta-paths of all existing HGRL methods attached to each node type or node pair cannot be personalised to each individual node. To fully unleash the power of HGRL, we present a novel framework, Personalised Meta-path based Heterogeneous Graph Neural Networks (PM-HGNN), to jointly generate meta-paths that are personalised for each individual node in a HIN and learn node representations for the target downstream task like node classification. Precisely, PM-HGNN treats the meta-path generation as a Markov Decision Process and utilises a policy network to adaptively generate a meta-path for each individual node and simultaneously learn effective node representations. The policy network is trained with deep reinforcement learning by exploiting the performance improvement on a downstream task. We further propose an extension, PM-HGNN++, to better encode relational structure and accelerate the training during the meta-path generation. Experimental results reveal that both PM-HGNN and PM-HGNN++ can significantly and consistently outperform 16 competing baselines and state-of-the-art methods in various settings of node classification. Qualitative analysis also shows that PM-HGNN++ can identify meaningful meta-paths overlooked by human knowledge.},
  archive      = {J_DMKD},
  author       = {Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun},
  doi          = {10.1007/s10618-022-00862-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2299-2333},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Personalised meta-path generation for heterogeneous graph neural networks},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint dynamic topic model for recognition of lead-lag
relationship in two text corpora. <em>DMKD</em>, <em>36</em>(6),
2272–2298. (<a
href="https://doi.org/10.1007/s10618-022-00873-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic evolution modeling has received significant attentions in recent decades. Although various topic evolution models have been proposed, most studies focus on the single document corpus. However in practice, we can easily access data from multiple sources and also observe relationships between them. Then it is of great interest to recognize the relationship between multiple text corpora and further utilize this relationship to improve topic modeling. In this work, we focus on a special type of relationship between two text corpora, which we define as the “lead-lag relationship&quot;. This relationship characterizes the phenomenon that one text corpus would influence the topics to be discussed in the other text corpus in the future. To discover the lead-lag relationship, we propose a joint dynamic topic model and also develop an embedding extension to address the modeling problem of large-scale text corpus. With the recognized lead-lag relationship, the similarities of the two text corpora can be figured out and the quality of topic learning in both corpora can be improved. We numerically investigate the performance of the joint dynamic topic modeling approach using synthetic data. Finally, we apply the proposed model on two text corpora consisting of statistical papers and the graduation theses. Results show the proposed model can well recognize the lead-lag relationship between the two corpora, and the specific and shared topic patterns in the two corpora are also discovered.},
  archive      = {J_DMKD},
  author       = {Zhu, Yandi and Lu, Xiaoling and Hong, Jingya and Wang, Feifei},
  doi          = {10.1007/s10618-022-00873-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2272-2298},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Joint dynamic topic model for recognition of lead-lag relationship in two text corpora},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decision tree boosted varying coefficient models.
<em>DMKD</em>, <em>36</em>(6), 2237–2271. (<a
href="https://doi.org/10.1007/s10618-022-00863-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varying coefficient models are a flexible extension of generic parametric models whose coefficients are functions of a set of effect-modifying covariates instead of fitted constants. They are capable of achieving higher model complexity while preserving the structure of the underlying parametric models, hence generating interpretable predictions. In this paper we study the use of gradient boosted decision trees as those coefficient-deciding functions in varying coefficient models with linearly structured outputs. In contrast to the traditional choices of splines or kernel smoothers, boosted trees are more flexible since they require no structural assumptions in the effect modifier space. We introduce our proposed method from the perspective of a localized version of gradient descent, prove its theoretical consistency under mild assumptions commonly adapted by decision tree research, and empirically demonstrate that the proposed tree boosted varying coefficient models achieve high performance qualified by their training speed, prediction accuracy and intelligibility as compared to several benchmark algorithms.},
  archive      = {J_DMKD},
  author       = {Zhou, Yichen and Hooker, Giles},
  doi          = {10.1007/s10618-022-00863-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2237-2271},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Decision tree boosted varying coefficient models},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer how much: A fine-grained measure of the knowledge
transferability of user behavior sequences in social network.
<em>DMKD</em>, <em>36</em>(6), 2214–2236. (<a
href="https://doi.org/10.1007/s10618-022-00857-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various user behaviors are providing valuable information for user interest modeling in online information platforms. For the phenomenon that some kinds of behavior data are insufficient to express users’ preferences, therefore, some cross-domain or multi-behavior fusion approaches are proposed to solve it. However, we have not yet understood which behaviors can be transferred and which behaviors can be better transferred to the target behavior. In this paper, we propose a novel knowledge transferability metric, TEMCS (Transfer Entropy with Multi-Concept Semantic), to measure the transferability of knowledge from the source to the target behavior sequence. The new metric not only can obtain the maximum semantics of the sequence based on the multi-concept semantic compression mechanism, but also can further achieve the dynamic information transfer between two sequences by modeling the inter-sequence coupling association founded on the transfer entropy. In particular, TEMCS is model-agnostic, calculation-simple, and requires no training on the source and target behavior sequences. Furthermore, TEMCS can be used as the weight of the difference between the source domain and target domain behavior characteristics, thereby reducing the distribution of the source domain and target domain characteristics and improving the performance of target behavior prediction. Extensive experiments on two real datasets demonstrate that our transferability metric is reasonable and effective, which not only can guide the choice of appropriate source behaviors but also can improve the performance of transfer models and multi-behavior models.},
  archive      = {J_DMKD},
  author       = {Li, Nuo and Guo, Bin and Liu, Yan and Ding, Yasan and Xu, En and Yao, Lina and Yu, Zhiwen},
  doi          = {10.1007/s10618-022-00857-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2214-2236},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Transfer how much: A fine-grained measure of the knowledge transferability of user behavior sequences in social network},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fairness in vulnerable attribute prediction on social media.
<em>DMKD</em>, <em>36</em>(6), 2194–2213. (<a
href="https://doi.org/10.1007/s10618-022-00855-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historically, policymakers and practitioners relied exclusively on survey and census data to design and plan for assistive interventions; now, social media offer a timely and cost-effective way to reach out to populations otherwise unobserved. This study was designed to address the needs of a non-for-profit organisation to reach out to the young unemployed individuals in Italy with educational and job opportunities via communication channels that are more likely to appeal to younger generations. To this extend, we developed an ad-hoc Facebook application which administers questionnaires while gathering data about the Likes on Facebook Pages. Then, we developed a machine learning framework that successfully predicts the unemployment status of an unseen individual (.74 AUC). However, blindly delegating to the machine learning model the communication intervention may lead to digital discrimination on the basis of socio-demographic characteristics. Here, we propose a framework that aims to optimising both for the prediction performance as well as the most adequate fairness metric. Our framework is based on an adaptive threshold for gender, while we show that it can be expanded for other socio-demographic attributes and generalised for other interventions of assistive character. We present a doubly cross-validated setting that achieves out-of-sample stability and generalisability of results. We compare the behaviour of models that infer on different sets of data and provide an indepth discussion on the most predictive features, demonstrating that the “fairness through unawareness” approach does not suffice to achieve a fair classification since sensitive demographic information can be inferred not only via other sociodemographic attributes but also from behavioural digital patterns. Finally, we thoroughly assess the behaviour of the adaptive threshold approach and provide an in-depth discussion on the advantages but also the implications of such models offering actionable insights. Our results show that careful assessment of fairness metrics should be considered, primarily when AI models are employed for policymaking.},
  archive      = {J_DMKD},
  author       = {Beiró, Mariano G. and Kalimeri, Kyriaki},
  doi          = {10.1007/s10618-022-00855-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2194-2213},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fairness in vulnerable attribute prediction on social media},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An external stability audit framework to test the validity
of personality prediction in AI hiring. <em>DMKD</em>, <em>36</em>(6),
2153–2193. (<a
href="https://doi.org/10.1007/s10618-022-00861-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers’ resumes or social media profiles. We interrogate the validity of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. Crucially, rather than challenging or affirming the assumptions made in psychometric testing — that personality is a meaningful and measurable construct, and that personality traits are indicative of future success on the job — we frame our audit methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves. Our main contribution is the development of a socio-technical framework for auditing the stability of algorithmic systems. This contribution is supplemented with an open-source software library that implements the technical components of the audit, and can be used to conduct similar stability audits of algorithmic systems. We instantiate our framework with the audit of two real-world personality prediction systems, namely, Humantic AI and Crystal. The application of our audit framework demonstrates that both these systems show substantial instability with respect to key facets of measurement, and hence cannot be considered valid testing instruments.},
  archive      = {J_DMKD},
  author       = {Rhea, Alene K. and Markey, Kelsey and D’Arinzo, Lauren and Schellmann, Hilke and Sloane, Mona and Squires, Paul and Arif Khan, Falaah and Stoyanovich, Julia},
  doi          = {10.1007/s10618-022-00861-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2153-2193},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An external stability audit framework to test the validity of personality prediction in AI hiring},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithmic fairness datasets: The story so far.
<em>DMKD</em>, <em>36</em>(6), 2074–2152. (<a
href="https://doi.org/10.1007/s10618-022-00854-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven algorithms are studied and deployed in diverse domains to support critical decisions, directly impacting people’s well-being. As a result, a growing community of researchers has been investigating the equity of existing algorithms and proposing novel ones, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair machine learning and equitable algorithm design hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the algorithmic fairness community, as a whole, suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we target this data documentation debt by surveying over two hundred datasets employed in algorithmic fairness research, and producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS, and German Credit, for which we compile in-depth documentation. This unifying documentation effort supports multiple contributions. Firstly, we summarize the merits and limitations of Adult, COMPAS, and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. Secondly, we document hundreds of available alternatives, annotating their domain and supported fairness tasks, along with additional properties of interest for fairness practitioners and researchers, including their format, cardinality, and the sensitive attributes they encode. We summarize this information, zooming in on the tasks, domains, and roles of these resources. Finally, we analyze these datasets from the perspective of five important data curation topics: anonymization, consent, inclusivity, labeling of sensitive attributes, and transparency. We discuss different approaches and levels of attention to these topics, making them tangible, and distill them into a set of best practices for the curation of novel resources.},
  archive      = {J_DMKD},
  author       = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
  doi          = {10.1007/s10618-022-00854-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2074-2152},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Algorithmic fairness datasets: The story so far},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring potential biases towards blockbuster items in
ranking-based recommendations. <em>DMKD</em>, <em>36</em>(6), 2033–2073.
(<a href="https://doi.org/10.1007/s10618-022-00860-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popularity bias is defined as the intrinsic tendency of recommendation algorithms to feature popular items more than unpopular ones in the ranked lists lists they produced. When investigating the adverse effects of popularity bias, the literature has usually focused on the most frequently rated items only. However, an item’s popularity does not always indicate that it is highly-liked by individuals; in fact, the degree of liking may even introduce biases that are more extreme than the famous popularity bias in terms of beyond-accuracy evaluations. Therefore, in the present study, we attempt to consider items that are both popular and highly-liked, which we refer to as blockbuster items, and to investigate whether the recommendation algorithms impose a considerable bias in favor of the blockbuster items in their ranking-based recommendations. To this end, we first present a practical formulation that measures the degree of the blockbuster level of the items by combining their liking-degree and popularity effectively. Then, based on this formulation, we perform a comprehensive set of experiments with ten different algorithms on five datasets with different characteristics to explore the potential biases towards blockbuster items in recommendations. The experimental outcomes demonstrate that most recommenders propagate an undesirable bias in their recommendations towards the blockbuster items, and such a bias is, in fact, not caused by the item popularity. Moreover, the observed biases to blockbuster items are more harmful and persistent than those to popular ones in terms of beyond-accuracy aspects such as diversity, catalog coverage, and novelty. The obtained results also suggest that conventional popularity-debiasing strategies are not so talented in treating the adverse effects of the observed blockbuster bias in recommendations.},
  archive      = {J_DMKD},
  author       = {Yalcin, Emre},
  doi          = {10.1007/s10618-022-00860-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2033-2073},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exploring potential biases towards blockbuster items in ranking-based recommendations},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SOKNL: A novel way of integrating k-nearest neighbours with
adaptive random forest regression for data streams. <em>DMKD</em>,
<em>36</em>(5), 2006–2032. (<a
href="https://doi.org/10.1007/s10618-022-00858-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most research in machine learning for data streams has focused on classification algorithms, whereas regression methods have received a lot less attention. This paper proposes Self-Optimising K-Nearest Leaves (SOKNL), a novel forest-based algorithm for streaming regression problems. Specifically, the Adaptive Random Forest Regression, a state-of-the-art online regression algorithm is extended like this: in each leaf, a representative data point – also called centroid – is generated by compressing the information from all instances in that leaf. During the prediction step, instead of letting all trees in the forest participate, the distances between the input instance and all centroids from relevant leaves are calculated, only k trees that possess the smallest distances are utilised for the prediction. Furthermore, we simplify the algorithm by introducing a mechanism for tuning the k values, which is dynamically and automatically optimised based on historical information. This new algorithm produces promising predictive results and achieves a superior ranking according to statistical testing when compared with several standard stream regression methods over typical benchmark datasets. This improvement incurs only a small increase in runtime and memory consumption over the basic Adaptive Random Forest Regressor.},
  archive      = {J_DMKD},
  author       = {Sun, Yibin and Pfahringer, Bernhard and Gomes, Heitor Murilo and Bifet, Albert},
  doi          = {10.1007/s10618-022-00858-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2006-2032},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SOKNL: A novel way of integrating K-nearest neighbours with adaptive random forest regression for data streams},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural content-aware collaborative filtering for cold-start
music recommendation. <em>DMKD</em>, <em>36</em>(5), 1971–2005. (<a
href="https://doi.org/10.1007/s10618-022-00859-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art music recommender systems are based on collaborative filtering, which builds upon learning similarities between users and songs from the available listening data. These approaches inherently face the cold-start problem, as they cannot recommend novel songs with no listening history. Content-aware recommendation addresses this issue by incorporating content information about the songs on top of collaborative filtering. However, methods falling in this category rely on a shallow user/item interaction that originates from a matrix factorization framework. In this work, we introduce neural content-aware collaborative filtering, a unified framework which alleviates these limits, and extends the recently introduced neural collaborative filtering to its content-aware counterpart. This model leverages deep learning for both extracting content information from low-level acoustic features and for modeling the interaction between users and songs embeddings. The deep content feature extractor can either directly predict the item embedding, or serve as a regularization prior, yielding two variants (strict and relaxed) of our model. Experimental results show that the proposed method reaches state-of-the-art results for both warm- and cold-start music recommendation tasks. We notably observe that exploiting deep neural networks for learning refined user/item interactions outperforms approaches using a more simple interaction model in a content-aware framework.},
  archive      = {J_DMKD},
  author       = {Magron, Paul and Févotte, Cédric},
  doi          = {10.1007/s10618-022-00859-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1971-2005},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Neural content-aware collaborative filtering for cold-start music recommendation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust subgroup discovery. <em>DMKD</em>, <em>36</em>(5),
1885–1970. (<a
href="https://doi.org/10.1007/s10618-022-00856-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the problem of robust subgroup discovery, i.e., finding a set of interpretable descriptions of subsets that 1) stand out with respect to one or more target attributes, 2) are statistically robust, and 3) non-redundant. Many attempts have been made to mine either locally robust subgroups or to tackle the pattern explosion, but we are the first to address both challenges at the same time from a global modelling perspective. First, we formulate the broad model class of subgroup lists, i.e., ordered sets of subgroups, for univariate and multivariate targets that can consist of nominal or numeric variables, including traditional top-1 subgroup discovery in its definition. This novel model class allows us to formalise the problem of optimal robust subgroup discovery using the Minimum Description Length (MDL) principle, where we resort to optimal Normalised Maximum Likelihood and Bayesian encodings for nominal and numeric targets, respectively. Second, finding optimal subgroup lists is NP-hard. Therefore, we propose SSD++, a greedy heuristic that finds good subgroup lists and guarantees that the most significant subgroup found according to the MDL criterion is added in each iteration. In fact, the greedy gain is shown to be equivalent to a Bayesian one-sample proportion, multinomial, or t-test between the subgroup and dataset marginal target distributions plus a multiple hypothesis testing penalty. Furthermore, we empirically show on 54 datasets that SSD++ outperforms previous subgroup discovery methods in terms of quality, generalisation on unseen data, and subgroup list size.},
  archive      = {J_DMKD},
  author       = {Proença, Hugo M. and Grünwald, Peter and Bäck, Thomas and van Leeuwen, Matthijs},
  doi          = {10.1007/s10618-022-00856-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1885-1970},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Robust subgroup discovery},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-in-the-loop handling of knowledge drift.
<em>DMKD</em>, <em>36</em>(5), 1865–1884. (<a
href="https://doi.org/10.1007/s10618-022-00845-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and study knowledge drift (KD), a special form of concept drift that occurs in hierarchical classification. Under KD the vocabulary of concepts, their individual distributions, and the is-a relations between them can all change over time. The main challenge is that, since the ground-truth concept hierarchy is unobserved, it is hard to tell apart different forms of KD. For instance, the introduction of a new is-a relation between two concepts might be confused with changes to those individual concepts, but it is far from equivalent. Failure to identify the right kind of KD compromises the concept hierarchy used by the classifier, leading to systematic prediction errors. Our key observation is that in human-in-the-loop applications like smart personal assistants the user knows what kind of drift occurred recently, if any. Motivated by this observation, we introduce trckd, a novel approach that combines two automated stages—drift detection and adaptation—with a new interactive disambiguation stage in which the user is asked to refine the machine’s understanding of recently detected KD. In addition, trckd implements a simple but effective knowledge-aware adaptation strategy. Our simulations show that, when the structure of the concept hierarchy drifts, a handful of queries to the user are often enough to substantially improve prediction performance on both synthetic and realistic data.},
  archive      = {J_DMKD},
  author       = {Bontempelli, Andrea and Giunchiglia, Fausto and Passerini, Andrea and Teso, Stefano},
  doi          = {10.1007/s10618-022-00845-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1865-1884},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Human-in-the-loop handling of knowledge drift},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TCMI: A non-parametric mutual-dependence estimator for
multivariate continuous distributions. <em>DMKD</em>, <em>36</em>(5),
1815–1864. (<a
href="https://doi.org/10.1007/s10618-022-00847-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of relevant features, i.e., the driving variables that determine a process or the properties of a system, is an essential part of the analysis of data sets with a large number of variables. A mathematical rigorous approach to quantifying the relevance of these features is mutual information. Mutual information determines the relevance of features in terms of their joint mutual dependence to the property of interest. However, mutual information requires as input probability distributions, which cannot be reliably estimated from continuous distributions such as physical quantities like lengths or energies. Here, we introduce total cumulative mutual information (TCMI), a measure of the relevance of mutual dependences that extends mutual information to random variables of continuous distribution based on cumulative probability distributions. TCMI is a non-parametric, robust, and deterministic measure that facilitates comparisons and rankings between feature sets with different cardinality. The ranking induced by TCMI allows for feature selection, i.e., the identification of variable sets that are nonlinear statistically related to a property of interest, taking into account the number of data samples as well as the cardinality of the set of variables. We evaluate the performance of our measure with simulated data, compare its performance with similar multivariate-dependence measures, and demonstrate the effectiveness of our feature-selection method on a set of standard data sets and a typical scenario in materials science.},
  archive      = {J_DMKD},
  author       = {Regler, Benjamin and Scheffler, Matthias and Ghiringhelli, Luca M.},
  doi          = {10.1007/s10618-022-00847-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1815-1864},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TCMI: A non-parametric mutual-dependence estimator for multivariate continuous distributions},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupled block diagonal regularization for multi-view
subspace clustering. <em>DMKD</em>, <em>36</em>(5), 1787–1814. (<a
href="https://doi.org/10.1007/s10618-022-00852-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object of multi-view subspace clustering is to uncover the latent low-dimensional structure by segmenting a collection of high-dimensional multi-source data into their corresponding subspaces. Existing methods imposed various constraints on the affinity matrix and/or the cluster labels to promote segmentation accuracy, and demonstrated effectiveness in some applications. However, the previous constraints are inefficient to ensure the ideal discriminative capability of the corresponding method. In this paper, we propose to learn view-specific affinity matrices and a common cluster indicator matrix jointly in a unified minimization problem, in which the affinity matrices and the cluster indicator matrix can guide each other to facilitate the final segmentation. To enforce the ideal discrimination, we use a block diagonal inducing regularity to constrain the affinity matrices as well as the cluster indicator matrix. Such coupled regularities are double insurances to promote clustering accuracy. We call it Coupled Block Diagonal Regularized Multi-view Subspace Clustering (CBDMSC). Based on the alternative minimization method, an algorithm is proposed to solve the new model. We evaluate our method by several metrics and compare it with several state-of-the-art related methods on some commonly used datasets. The results demonstrate that our method outperforms the state-of-the-art methods in the vast majority of metrics.},
  archive      = {J_DMKD},
  author       = {Chen, Huazhu and Wang, Weiwei and Luo, Shousheng},
  doi          = {10.1007/s10618-022-00852-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1787-1814},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Coupled block diagonal regularization for multi-view subspace clustering},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic slate recommendation with gated recurrent units and
thompson sampling. <em>DMKD</em>, <em>36</em>(5), 1756–1786. (<a
href="https://doi.org/10.1007/s10618-022-00849-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of recommending relevant content to users of an internet platform in the form of lists of items, called slates. We introduce a variational Bayesian Recurrent Neural Net recommender system that acts on time series of interactions between the internet platform and the user, and which scales to real world industrial situations. The recommender system is tested both online on real users, and on an offline dataset collected from a Norwegian web-based marketplace, FINN.no, that is made public for research. This is one of the first publicly available datasets which includes all the slates that are presented to users as well as which items (if any) in the slates were clicked on. Such a data set allows us to move beyond the common assumption that implicitly assumes that users are considering all possible items at each interaction. Instead we build our likelihood using the items that are actually in the slate, and evaluate the strengths and weaknesses of both approaches theoretically and in experiments. We also introduce a hierarchical prior for the item parameters based on group memberships. Both item parameters and user preferences are learned probabilistically. Furthermore, we combine our model with bandit strategies to ensure learning, and introduce ‘in-slate Thompson sampling’ which makes use of the slates to maximise explorative opportunities. We show experimentally that explorative recommender strategies perform on par or above their greedy counterparts. Even without making use of exploration to learn more effectively, click rates increase simply because of improved diversity in the recommended slates.},
  archive      = {J_DMKD},
  author       = {Eide, Simen and Leslie, David S. and Frigessi, Arnoldo},
  doi          = {10.1007/s10618-022-00849-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1756-1786},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Dynamic slate recommendation with gated recurrent units and thompson sampling},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EmbAssi: Embedding assignment costs for similarity search in
large graph databases. <em>DMKD</em>, <em>36</em>(5), 1728–1755. (<a
href="https://doi.org/10.1007/s10618-022-00850-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph edit distance is an intuitive measure to quantify the dissimilarity of graphs, but its computation is $$\mathsf {NP}$$ -hard and challenging in practice. We introduce methods for answering nearest neighbor and range queries regarding this distance efficiently for large databases with up to millions of graphs. We build on the filter-verification paradigm, where lower and upper bounds are used to reduce the number of exact computations of the graph edit distance. Highly effective bounds for this involve solving a linear assignment problem for each graph in the database, which is prohibitive in massive datasets. Index-based approaches typically provide only weak bounds leading to high computational costs verification. In this work, we derive novel lower bounds for efficient filtering from restricted assignment problems, where the cost function is a tree metric. This special case allows embedding the costs of optimal assignments isometrically into $$\ell _1$$ space, rendering efficient indexing possible. We propose several lower bounds of the graph edit distance obtained from tree metrics reflecting the edit costs, which are combined for effective filtering. Our method termed EmbAssi can be integrated into existing filter-verification pipelines as a fast and effective pre-filtering step. Empirically we show that for many real-world graphs our lower bounds are already close to the exact graph edit distance, while our index construction and search scales to very large databases.},
  archive      = {J_DMKD},
  author       = {Bause, Franka and Schubert, Erich and Kriege, Nils M.},
  doi          = {10.1007/s10618-022-00850-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1728-1755},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {EmbAssi: Embedding assignment costs for similarity search in large graph databases},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The minimum description length principle for pattern mining:
A survey. <em>DMKD</em>, <em>36</em>(5), 1679–1727. (<a
href="https://doi.org/10.1007/s10618-022-00846-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining patterns is a core task in data analysis and, beyond issues of efficient enumeration, the selection of patterns constitutes a major challenge. The Minimum Description Length (MDL) principle, a model selection method grounded in information theory, has been applied to pattern mining with the aim to obtain compact high-quality sets of patterns. After giving an outline of relevant concepts from information theory and coding, we review MDL-based methods for mining different kinds of patterns from various types of data. Finally, we open a discussion on some issues regarding these methods.},
  archive      = {J_DMKD},
  author       = {Galbrun, Esther},
  doi          = {10.1007/s10618-022-00846-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1679-1727},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The minimum description length principle for pattern mining: A survey},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VPint: Value propagation-based spatial interpolation.
<em>DMKD</em>, <em>36</em>(5), 1647–1678. (<a
href="https://doi.org/10.1007/s10618-022-00843-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the common problem of missing data in real-world applications from various fields, such as remote sensing, ecology and meteorology, the interpolation of missing spatial and spatio-temporal data can be of tremendous value. Existing methods for spatial interpolation, most notably Gaussian processes and spatial autoregressive models, tend to suffer from (a) a trade-off between modelling local or global spatial interaction, (b) the assumption there is only one possible path between two points, and (c) the assumption of homogeneity of intermediate locations between points. Addressing these issues, we propose a value propagation-based spatial interpolation method called VPint, inspired by Markov reward processes (MRPs), and introduce two variants thereof: (i) a static discount (SD-MRP) and (ii) a data-driven weight prediction (WP-MRP) variant. Both these interpolation variants operate locally, while implicitly accounting for global spatial relationships in the entire system through recursion. We evaluated our proposed methods by comparing the mean absolute error, root mean squared error, peak signal-to-noise ratio and structural similarity of interpolated grid cells to those of 8 common baselines. Our analysis involved detailed experiments on a synthetic and two real-world datasets, as well as experiments on convergence and scalability. Empirical results demonstrate the competitive advantage of VPint on randomly missing data, where it performed better than baselines in terms of mean absolute error and structural similarity, as well as spatially clustered missing data, where it performed best on 2 out of 3 datasets.},
  archive      = {J_DMKD},
  author       = {Arp, Laurens and Baratchi, Mitra and Hoos, Holger},
  doi          = {10.1007/s10618-022-00843-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1647-1678},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {VPint: Value propagation-based spatial interpolation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MultiRocket: Multiple pooling operators and transformations
for fast and effective time series classification. <em>DMKD</em>,
<em>36</em>(5), 1623–1646. (<a
href="https://doi.org/10.1007/s10618-022-00844-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose MultiRocket, a fast time series classification (TSC) algorithm that achieves state-of-the-art accuracy with a tiny fraction of the time and without the complex ensembling structure of many state-of-the-art methods. MultiRocket improves on MiniRocket, one of the fastest TSC algorithms to date, by adding multiple pooling operators and transformations to improve the diversity of the features generated. In addition to processing the raw input series, MultiRocket also applies first order differences to transform the original series. Convolutions are applied to both representations, and four pooling operators are applied to the convolution outputs. When benchmarked using the University of California Riverside TSC benchmark datasets, MultiRocket is significantly more accurate than MiniRocket, and competitive with the best ranked current method in terms of accuracy, HIVE-COTE 2.0, while being orders of magnitude faster.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Dempster, Angus and Bergmeir, Christoph and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-022-00844-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1623-1646},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MultiRocket: Multiple pooling operators and transformations for fast and effective time series classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic self-paced sampling ensemble for highly imbalanced
and class-overlapped data classification. <em>DMKD</em>, <em>36</em>(5),
1601–1622. (<a
href="https://doi.org/10.1007/s10618-022-00838-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets with imbalanced class distribution are available in various real-world applications. A great number of approaches has been proposed to address the class imbalance challenge, but most of these models perform poorly when datasets are characterized with high class imbalance, class overlap and low data quality. In this study, we propose an effective meta-framework for high imbalance overlapped classification, called DAPS (DynAmic self-Paced sampling enSemble), which (1) leverages reasonable and effective sampling to maximize the utilization of informative instances and to avoid serious information loss and (2) assigns proper instance weights to address the issues of noisy data. Furthermore, most of the existing canonical classifiers (e.g. Decision Tree, Random Forest) can be integrated in DAPS. The comprehensive experimental results on both synthetic and three real-world datasets show that the DAPS model could obtain considerable improvements in F1-score when compared to a broad range of published models.},
  archive      = {J_DMKD},
  author       = {Zhou, Fang and Gao, Suting and Ni, Lyu and Pavlovski, Martin and Dong, Qiwen and Obradovic, Zoran and Qian, Weining},
  doi          = {10.1007/s10618-022-00838-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1601-1622},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Dynamic self-paced sampling ensemble for highly imbalanced and class-overlapped data classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPEck: Mining statistically-significant sequential patterns
efficiently with exact sampling. <em>DMKD</em>, <em>36</em>(4),
1575–1599. (<a
href="https://doi.org/10.1007/s10618-022-00848-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of efficiently mining statistically-significant sequential patterns from large datasets, under different null models. We consider one null model presented in the literature, and introduce two new ones that preserve different properties of the observed dataset. We describe SPEck, a generic framework for significant sequential pattern mining, that can be instantiated with any null model, when given a procedure for sampling datasets according to the null distribution. For the previously-proposed model, we introduce a novel procedure that samples exactly according to the null distribution, while existing procedures are approximate samplers. Our exact sampler is also more computationally efficient and much faster in practice. For the null models we introduce, we give exact and/or almost uniform samplers. Our experimental evaluation shows how exact samplers can be orders of magnitude faster than approximate ones, and scale well.},
  archive      = {J_DMKD},
  author       = {Jenkins, Steedman and Walzer-Goldfeld, Stefan and Riondato, Matteo},
  doi          = {10.1007/s10618-022-00848-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1575-1599},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SPEck: Mining statistically-significant sequential patterns efficiently with exact sampling},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conclusive local interpretation rules for random forests.
<em>DMKD</em>, <em>36</em>(4), 1521–1574. (<a
href="https://doi.org/10.1007/s10618-022-00839-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In critical situations involving discrimination, gender inequality, economic damage, and even the possibility of casualties, machine learning models must be able to provide clear interpretations of their decisions. Otherwise, their obscure decision-making processes can lead to socioethical issues as they interfere with people’s lives. Random forest algorithms excel in the aforementioned sectors, where their ability to explain themselves is an obvious requirement. In this paper, we present LionForests, which relies on a preliminary work of ours. LionForests is a random forest-specific interpretation technique that provides rules as explanations. It applies to binary classification tasks up to multi-class classification and regression tasks, while a stable theoretical background supports it. A time and scalability analysis suggests that LionForests is much faster than our preliminary work and is also applicable to large datasets. Experimentation, including a comparison with state-of-the-art techniques, demonstrate the efficacy of our contribution. LionForests outperformed the other techniques in terms of precision, variance, and response time, but fell short in terms of rule length and coverage. Finally, we highlight conclusiveness, a unique property of LionForests that provides interpretation validity and distinguishes it from previous techniques.},
  archive      = {J_DMKD},
  author       = {Mollas, Ioannis and Bassiliades, Nick and Tsoumakas, Grigorios},
  doi          = {10.1007/s10618-022-00839-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1521-1574},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Conclusive local interpretation rules for random forests},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended missing data imputation via GANs for ranking
applications. <em>DMKD</em>, <em>36</em>(4), 1498–1520. (<a
href="https://doi.org/10.1007/s10618-022-00837-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Conditional Imputation GAN, an extended missing data imputation method based on Generative Adversarial Networks (GANs). The motivating use case is learning-to-rank, the cornerstone of modern search, recommendation system, and information retrieval applications. Empirical ranking datasets do not always follow standard Gaussian distributions or Missing Completely At Random (MCAR) mechanism, which are standard assumptions of classic missing data imputation methods. Our methodology provides a simple solution that offers compatible imputation guarantees while relaxing assumptions for missing mechanisms and sidesteps approximating intractable distributions to improve imputation quality. We prove that the optimal GAN imputation is achieved for Extended Missing At Random and Extended Always Missing At Random mechanisms, beyond the naive MCAR. Our method demonstrates the highest imputation quality on the open-source Microsoft Research Ranking Dataset and a synthetic ranking dataset compared to state-of-the-art benchmarks and across various feature distributions. Using a proprietary Amazon Search ranking dataset, we also demonstrate comparable ranking quality metrics for ranking models trained on GAN-imputed data compared to ground-truth data.},
  archive      = {J_DMKD},
  author       = {Deng, Grace and Han, Cuize and Matteson, David S.},
  doi          = {10.1007/s10618-022-00837-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1498-1520},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Extended missing data imputation via GANs for ranking applications},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing biceps to completely compute in subquadratic time
a new generic type of bicluster in dense and sparse matrices.
<em>DMKD</em>, <em>36</em>(4), 1451–1497. (<a
href="https://doi.org/10.1007/s10618-022-00834-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an m-by-n real matrix, biclustering aims to discover relevant submatrices. This article defines a new type of bicluster. In any of its columns, the values in the rows of the bicluster must be all strictly greater than those in the rows absent from it, hence the discovery of a binary clustering of the rows in the restricted context of the columns of the bicluster. To only keep the best bicluster among those carrying redundant information, its rows must not be a subset or a superset of the rows of another bicluster of greater or equal quality. Any computable function can be chosen to assign qualities to the biclusters. In that respect, the proposed definition is generic. Dynamic programming and appropriate data structures allow to exhaustively list the biclusters satisfying it within $$O(m^2n + mn^2)$$ time, plus the time to compute O(mn) qualities. After some adaptations, the proposed algorithm, Biceps, remains subquadratic if its complexity is expressed in function of $$m_{\text {non-min}}n$$ , where $$m_{\text {non-min}}$$ is the maximal number of non-minimal values in a column, i. e., for sparse matrices. Experiments on three real-world datasets demonstrate the effectiveness of the proposal in different application contexts. They also show its good theoretical efficiency is practical as well: two minutes and 5.3 GB of RAM are enough to list the desired biclusters in a dense 801-by-20,531 matrix; 3.5s and 192 MB of RAM for a sparse 631,532-by-174,559 matrix with 2,575,425 nonzero values.},
  archive      = {J_DMKD},
  author       = {Abreu, Bernardo and Ataide Martins, João Paulo and Cerf, Loïc},
  doi          = {10.1007/s10618-022-00834-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1451-1497},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Developing biceps to completely compute in subquadratic time a new generic type of bicluster in dense and sparse matrices},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grouped feature importance and combined features effect
plot. <em>DMKD</em>, <em>36</em>(4), 1401–1450. (<a
href="https://doi.org/10.1007/s10618-022-00840-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretable machine learning has become a very active area of research due to the rising popularity of machine learning algorithms and their inherently challenging interpretability. Most work in this area has been focused on the interpretation of single features in a model. However, for researchers and practitioners, it is often equally important to quantify the importance or visualize the effect of feature groups. To address this research gap, we provide a comprehensive overview of how existing model-agnostic techniques can be defined for feature groups to assess the grouped feature importance, focusing on permutation-based, refitting, and Shapley-based methods. We also introduce an importance-based sequential procedure that identifies a stable and well-performing combination of features in the grouped feature space. Furthermore, we introduce the combined features effect plot, which is a technique to visualize the effect of a group of features based on a sparse, interpretable linear combination of features. We used simulation studies and real data examples to analyze, compare, and discuss these methods.},
  archive      = {J_DMKD},
  author       = {Au, Quay and Herbinger, Julia and Stachl, Clemens and Bischl, Bernd and Casalicchio, Giuseppe},
  doi          = {10.1007/s10618-022-00840-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1401-1450},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Grouped feature importance and combined features effect plot},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting second-order dissimilarity representations for
hierarchical clustering and visualization. <em>DMKD</em>,
<em>36</em>(4), 1371–1400. (<a
href="https://doi.org/10.1007/s10618-022-00836-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The representation of objects is crucial for the learning process, often having a large impact on the application performance. The dissimilarity space (DS) is one of such representations, which is built by applying a dissimilarity measure between objects (e.g., Euclidean distance). However, other measures can be applied to generate more informative data representations. This paper focuses on the application of second-order dissimilarity measures, namely the Shared Nearest Neighbor (SNN) and the Dissimilarity Increments (Dinc), to produce new DSs that lead to a better description of the data, by reducing the overlap of the classes and by increasing the discriminative power of features. Experimental results show that the application of the proposed DSs provide significant benefits for unsupervised learning tasks. When compared with Feature and Euclidean space, the proposed SNN and Dinc spaces allow improving the performance of traditional hierarchical clustering algorithms, and also help in the visualization task, by leading to higher area under the precision/recall curve values.},
  archive      = {J_DMKD},
  author       = {Aidos, Helena},
  doi          = {10.1007/s10618-022-00836-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1371-1400},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exploiting second-order dissimilarity representations for hierarchical clustering and visualization},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpreting deep learning models with marginal attribution
by conditioning on quantiles. <em>DMKD</em>, <em>36</em>(4), 1335–1370.
(<a href="https://doi.org/10.1007/s10618-022-00841-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vast and growing literature on explaining deep learning models has emerged. This paper contributes to that literature by introducing a global gradient-based model-agnostic method, which we call Marginal Attribution by Conditioning on Quantiles (MACQ). Our approach is based on analyzing the marginal attribution of predictions (outputs) to individual features (inputs). Specifically, we consider variable importance by fixing (global) output levels, and explaining how features marginally contribute to these fixed global output levels. MACQ can be seen as a marginal attribution counterpart to approaches such as accumulated local effects, which study the sensitivities of outputs by perturbing inputs. Furthermore, MACQ allows us to separate marginal attribution of individual features from interaction effects and to visualize the 3-way relationship between marginal attribution, output level, and feature value.},
  archive      = {J_DMKD},
  author       = {Merz, Michael and Richman, Ronald and Tsanakas, Andreas and Wüthrich, Mario V.},
  doi          = {10.1007/s10618-022-00841-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1335-1370},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interpreting deep learning models with marginal attribution by conditioning on quantiles},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive meta-heuristic for music plagiarism detection
based on text similarity and clustering. <em>DMKD</em>, <em>36</em>(4),
1301–1334. (<a
href="https://doi.org/10.1007/s10618-022-00835-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plagiarism is a controversial and debated topic in different fields, especially in the Music one, where the commercial market generates a huge amount of money. The lack of objective metrics to decide whether a song is a plagiarism, makes music plagiarism detection a very complex task: often decisions have to be based on subjective argumentations. Automated music analysis methods that identify music similarities can be of help. In this work, we first propose two novel such methods: a text similarity-based method and a clustering-based method. Then, we show how to combine them to get an improved (hybrid) method. The result is a novel adaptive meta-heuristic for music plagiarism detection. To assess the effectiveness of the proposed methods, considered both singularly and in the combined meta-heuristic, we performed tests on a large dataset of ascertained plagiarism and non-plagiarism cases. Results show that the meta-heuristic outperforms existing methods. Finally, we deployed the meta-heuristic into a tool, accessible as a Web application, and assessed the effectiveness, usefulness, and overall user acceptance of the tool by means of a study involving 20 people, divided into two groups, one of which with access to the tool. The study consisted in having people decide which pair of songs, in a predefined set of pairs, should be considered plagiarisms and which not. The study shows that the group supported by our tool successfully identified all plagiarism cases, performing all tasks with no errors. The whole sample agreed about the usefulness of an automatic tool that provides a measure of similarity between two songs.},
  archive      = {J_DMKD},
  author       = {Malandrino, Delfina and De Prisco, Roberto and Ianulardo, Mario and Zaccagnino, Rocco},
  doi          = {10.1007/s10618-022-00835-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1301-1334},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An adaptive meta-heuristic for music plagiarism detection based on text similarity and clustering},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simplification of genetic programs: A literature survey.
<em>DMKD</em>, <em>36</em>(4), 1279–1300. (<a
href="https://doi.org/10.1007/s10618-022-00830-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic programming (GP), a widely used evolutionary computing technique, suffers from bloat—the problem of excessive growth in individuals’ sizes. As a result, its ability to efficiently explore complex search spaces reduces. The resulting solutions are less robust and generalisable. Moreover, it is difficult to understand and explain models which contain bloat. This phenomenon is well researched, primarily from the angle of controlling bloat: instead, our focus in this paper is to review the literature from an explainability point of view, by looking at how simplification can make GP models more explainable by reducing their sizes. Simplification is a code editing technique whose primary purpose is to make GP models more explainable. However, it can offer bloat control as an additional benefit when implemented and applied with caution. Researchers have proposed several simplification techniques and adopted various strategies to implement them. We organise the literature along multiple axes to identify the relative strengths and weaknesses of simplification techniques and to identify emerging trends and areas for future exploration. We highlight design and integration challenges and propose several avenues for research. One of them is to consider simplification as a standalone operator, rather than an extension of the standard crossover or mutation operators. Its role is then more clearly complementary to other GP operators, and it can be integrated as an optional feature into an existing GP setup. Another proposed avenue is to explore the lack of utilisation of complexity measures in simplification. So far, size is the most discussed measure, with only two pieces of prior work pointing out the benefits of using time as a measure when controlling bloat.},
  archive      = {J_DMKD},
  author       = {Javed, Noman and Gobet, Fernand and Lane, Peter},
  doi          = {10.1007/s10618-022-00830-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1279-1300},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Simplification of genetic programs: A literature survey},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Individualized passenger travel pattern multi-clustering
based on graph regularized tensor latent dirichlet allocation.
<em>DMKD</em>, <em>36</em>(4), 1247–1278. (<a
href="https://doi.org/10.1007/s10618-022-00842-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual passenger travel patterns have significant value in understanding passenger’s behavior, such as learning the hidden clusters of locations, time, and passengers. The learned clusters further enable commercially beneficial actions such as customized services, promotions, data-driven urban-use planning, peak hour discovery, and so on. However, the individualized passenger modeling is very challenging for the following reasons: 1) The individual passenger travel data are multi-dimensional spatiotemporal big data, including at least the origin, destination, and time dimensions; 2) Moreover, individualized passenger travel patterns usually depend on the external environment, such as the distances and functions of locations, which are ignored in most current works. This work proposes a multi-clustering model to learn the latent clusters along the multiple dimensions of Origin, Destination, Time, and eventually, Passenger (ODT-P). We develop a graph-regularized tensor Latent Dirichlet Allocation (LDA) model by first extending the traditional LDA model into a tensor version and then applies to individual travel data. Then, the external information of stations is formulated as semantic graphs and incorporated as the Laplacian regularizations; Furthermore, to improve the model scalability when dealing with massive data, an online stochastic learning method based on tensorized variational Expectation-Maximization algorithm is developed. Finally, a case study based on passengers in the Hong Kong metro system is conducted and demonstrates that a better clustering performance is achieved compared to state-of-the-arts with the improvement in point-wise mutual information index and algorithm convergence speed by a factor of two.},
  archive      = {J_DMKD},
  author       = {Li, Ziyue and Yan, Hao and Zhang, Chen and Tsung, Fugee},
  doi          = {10.1007/s10618-022-00842-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1247-1278},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Individualized passenger travel pattern multi-clustering based on graph regularized tensor latent dirichlet allocation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The area under the ROC curve as a measure of clustering
quality. <em>DMKD</em>, <em>36</em>(3), 1219–1245. (<a
href="https://doi.org/10.1007/s10618-022-00829-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area under the receiver operating characteristics (ROC) Curve, referred to as AUC, is a well-known performance measure in the supervised learning domain. Due to its compelling features, it has been employed in a number of studies to evaluate and compare the performance of different classifiers. In this work, we explore AUC as a performance measure in the unsupervised learning domain, more specifically, in the context of cluster analysis. In particular, we elaborate on the use of AUC as an internal/relative measure of clustering quality, which we refer to as Area Under the Curve for Clustering (AUCC). We show that the AUCC of a given candidate clustering solution has an expected value under a null model of random clustering solutions, regardless of the size of the dataset and, more importantly, regardless of the number or the (im)balance of clusters under evaluation. In addition, we elaborate on the fact that, in the context of internal/relative clustering validation as we consider, AUCC is actually a linear transformation of the Gamma criterion from Baker and Hubert (1975), for which we also formally derive a theoretical expected value for chance clusterings. We also discuss the computational complexity of these criteria and show that, while an ordinary implementation of Gamma can be computationally prohibitive and impractical for most real applications of cluster analysis, its equivalence with AUCC actually unveils a much more efficient algorithmic procedure. Our theoretical findings are supported by experimental results. These results show that, in addition to an effective and robust quantitative evaluation provided by AUCC, visual inspection of the ROC curves themselves can be useful to further assess a candidate clustering solution from a broader, qualitative perspective as well.},
  archive      = {J_DMKD},
  author       = {Jaskowiak, Pablo A. and Costa, Ivan G. and Campello, Ricardo J. G. B.},
  doi          = {10.1007/s10618-022-00829-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1219-1245},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The area under the ROC curve as a measure of clustering quality},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ranking with submodular functions on a budget.
<em>DMKD</em>, <em>36</em>(3), 1197–1218. (<a
href="https://doi.org/10.1007/s10618-022-00833-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Submodular maximization has been the backbone of many important machine-learning problems, and has applications to viral marketing, diversification, sensor placement, and more. However, the study of maximizing submodular functions has mainly been restricted in the context of selecting a set of items. On the other hand, many real-world applications require a solution that is a ranking over a set of items. The problem of ranking in the context of submodular function maximization has been considered before, but to a much lesser extent than item-selection formulations. In this paper, we explore a novel formulation for ranking items with submodular valuations and budget constraints. We refer to this problem as max-submodular ranking ( $$\text {MSR}$$ ). In more detail, given a set of items and a set of non-decreasing submodular functions, where each function is associated with a budget, we aim to find a ranking of the set of items that maximizes the sum of values achieved by all functions under the budget constraints. For the $$\text {MSR}$$ problem with cardinality- and knapsack-type budget constraints we propose practical algorithms with approximation guarantees. In addition, we perform an empirical evaluation, which demonstrates the superior performance of the proposed algorithms against strong baselines.},
  archive      = {J_DMKD},
  author       = {Zhang, Guangyi and Tatti, Nikolaj and Gionis, Aristides},
  doi          = {10.1007/s10618-022-00833-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1197-1218},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Ranking with submodular functions on a budget},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sufficient dimension reduction for average causal effect
estimation. <em>DMKD</em>, <em>36</em>(3), 1174–1196. (<a
href="https://doi.org/10.1007/s10618-022-00832-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of covariates can have a negative impact on the quality of causal effect estimation since confounding adjustment becomes unreliable when the number of covariates is large relative to the number of samples. Propensity score is a common way to deal with a large covariate set, but the accuracy of propensity score estimation (normally done by logistic regression) is also challenged by the large number of covariates. In this paper, we prove that a large covariate set can be reduced to a lower dimensional representation which captures the complete information for adjustment in causal effect estimation. The theoretical result enables effective data-driven algorithms for causal effect estimation. Supported by the result, we develop an algorithm that employs a supervised kernel dimension reduction method to learn a lower dimensional representation from the original covariate space, and then utilises nearest neighbour matching in the reduced covariate space to impute the counterfactual outcomes to avoid the large sized covariate set problem. The proposed algorithm is evaluated on two semisynthetic and three real-world datasets and the results show the effectiveness of the proposed algorithm.},
  archive      = {J_DMKD},
  author       = {Cheng, Debo and Li, Jiuyong and Liu, Lin and Le, Thuc Duy and Liu, Jixue and Yu, Kui},
  doi          = {10.1007/s10618-022-00832-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1174-1196},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sufficient dimension reduction for average causal effect estimation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpretability, personalization and reliability of a
machine learning based clinical decision support system. <em>DMKD</em>,
<em>36</em>(3), 1140–1173. (<a
href="https://doi.org/10.1007/s10618-022-00821-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has achieved notable performances in many fields and its research impact in healthcare has been unquestionable. Nevertheless, the deployment of such computational models in clinical practice is still limited. Some of the major issues recognized as barriers to a successful real-world machine learning applications include lack of: transparency; reliability and personalization. Actually, these aspects are decisive not only for patient safety, but also to assure the confidence of professionals. Explainable AI aims at to achieve solutions for artificial intelligence transparency and reliability concerns, with the capacity to better understand and trust a model, providing the ability to justify its outcomes, thus effectively assisting clinicians in rationalizing the model prediction. This work proposes an innovative machine learning based approach, implementing a hybrid scheme, able to combine in a systematic way knowledge-driven and data-driven techniques. In a first step a global set of interpretable rules is generated, founded on clinical evidence. Then, in a second phase, a machine learning model is trained to select, from the global set of rules, the subset that is more appropriate for a given patient, according to his particular characteristics. This approach addresses simultaneously three of the central requirements of explainable AI—interpretability, personalization, and reliability—without impairing the accuracy of the model’s prediction. The scheme was validated with a real dataset provided by two Portuguese Hospitals, the Santa Cruz Hospital, Lisbon, and the Santo André Hospital, Leiria, comprising a total of N = 1111 patients that suffered an acute coronary syndrome event, where the 30 days mortality was assessed. When compared with standard black-box structures (e.g. feedforward neural network) the proposed scheme achieves similar performances, while ensures simultaneously clinical interpretability and personalization of the model, as well as provides a level of reliability to the estimated mortality risk.},
  archive      = {J_DMKD},
  author       = {Valente, F. and Paredes, S. and Henriques, J. and Rocha, T. and de Carvalho, P. and Morais, J.},
  doi          = {10.1007/s10618-022-00821-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1140-1173},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interpretability, personalization and reliability of a machine learning based clinical decision support system},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using p-values for the comparison of classifiers: Pitfalls
and alternatives. <em>DMKD</em>, <em>36</em>(3), 1102–1139. (<a
href="https://doi.org/10.1007/s10618-022-00828-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical comparison of machine learning classifiers is frequently underpinned by null hypothesis significance testing. Here, we provide a survey and analysis of underrated problems that significance testing entails for classification benchmark studies. The p-value has become deeply entrenched in machine learning, but it is substantially less objective and less informative than commonly assumed. Even very small p-values can drastically overstate the evidence against the null hypothesis. Moreover, the p-value depends on the experimenter’s intentions, irrespective of whether these were actually realized or not. We show how such intentions can lead to experimental designs with more than one stage, and how to calculate a valid p-value for such designs. We discuss two widely used statistical tests for the comparison of classifiers, the Friedman test and the Wilcoxon signed rank test. Some improvements to the use of p-values, such as the calibration with the Bayes factor bound, and alternative methods for the evaluation of benchmark studies are discussed as well.},
  archive      = {J_DMKD},
  author       = {Berrar, Daniel},
  doi          = {10.1007/s10618-022-00828-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1102-1139},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Using p-values for the comparison of classifiers: Pitfalls and alternatives},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel features for time series analysis: A complex networks
approach. <em>DMKD</em>, <em>36</em>(3), 1062–1101. (<a
href="https://doi.org/10.1007/s10618-022-00826-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to capture the characteristics of a time series with a feature vector is a very important task with a multitude of applications, such as classification, clustering or forecasting. Usually, the features are obtained from linear and nonlinear time series measures, that may present several data related drawbacks. In this work we introduce NetF as an alternative set of features, incorporating several representative topological measures of different complex networks mappings of the time series. Our approach does not require data preprocessing and is applicable regardless of any data characteristics. Exploring our novel feature vector, we are able to connect mapped network features to properties inherent in diversified time series models, showing that NetF can be useful to characterize time data. Furthermore, we also demonstrate the applicability of our methodology in clustering synthetic and benchmark time series sets, comparing its performance with more conventional features, showcasing how NetF can achieve high-accuracy clusters. Our results are very promising, with network features from different mapping methods capturing different properties of the time series, adding a different and rich feature set to the literature.},
  archive      = {J_DMKD},
  author       = {Silva, Vanessa Freitas and Silva, Maria Eduarda and Ribeiro, Pedro and Silva, Fernando},
  doi          = {10.1007/s10618-022-00826-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1062-1101},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Novel features for time series analysis: A complex networks approach},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PETSC: Pattern-based embedding for time series
classification. <em>DMKD</em>, <em>36</em>(3), 1015–1061. (<a
href="https://doi.org/10.1007/s10618-022-00822-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and interpretable classification of time series is an essential data mining task with many real-world applications. Recently several dictionary- and shapelet-based time series classification methods have been proposed that employ contiguous subsequences of fixed length. We extend pattern mining to efficiently enumerate long variable-length sequential patterns with gaps. Additionally, we discover patterns at multiple resolutions thereby combining cohesive sequential patterns that vary in length, duration and resolution. For time series classification we construct an embedding based on sequential pattern occurrences and learn a linear model. The discovered patterns form the basis for interpretable insight into each class of time series. The pattern-based embedding for time series classification (PETSC) supports both univariate and multivariate time series datasets of varying length subject to noise or missing data. We experimentally validate that MR-PETSC performs significantly better than baseline interpretable methods such as DTW, BOP and SAX-VSM on univariate and multivariate time series. On univariate time series, our method performs comparably to many recent methods, including BOSS, cBOSS, S-BOSS, ProximityForest and ResNET, and is only narrowly outperformed by state-of-the-art methods such as HIVE-COTE, ROCKET, TS-CHIEF and InceptionTime. Moreover, on multivariate datasets PETSC performs comparably to the current state-of-the-art such as HIVE-COTE, ROCKET, CIF and ResNET, none of which are interpretable. PETSC scales to large datasets and the total time for training and making predictions on all 85 ‘bake off’ datasets in the UCR archive is under 3 h making it one of the fastest methods available. PETSC is particularly useful as it learns a linear model where each feature represents a sequential pattern in the time domain, which supports human oversight to ensure predictions are trustworthy and fair which is essential in financial, medical or bioinformatics applications.},
  archive      = {J_DMKD},
  author       = {Feremans, Len and Cule, Boris and Goethals, Bart},
  doi          = {10.1007/s10618-022-00822-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1015-1061},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {PETSC: Pattern-based embedding for time series classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Who can receive the pass? A computational model for
quantifying availability in soccer. <em>DMKD</em>, <em>36</em>(3),
987–1014. (<a href="https://doi.org/10.1007/s10618-022-00827-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a computational approach to Availability of soccer players. Availability is defined as the probability that a pass reaches the target player without being intercepted by opponents. Clearly, a computational model for this probability grounds on models for ball dynamics, player movements, and technical skills of the pass giver. Our approach aggregates these quantities for all possible passes to the target player to compute a single Availability value. Empirically, our approach outperforms state-of-the-art competitors using data from 58 professional soccer matches. Moreover, our experiments indicate that the model can even outperform soccer coaches in assessing the availability of soccer players from static images.},
  archive      = {J_DMKD},
  author       = {Dick, Uwe and Link, Daniel and Brefeld, Ulf},
  doi          = {10.1007/s10618-022-00827-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {987-1014},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Who can receive the pass? a computational model for quantifying availability in soccer},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted sparse simplex representation: A unified framework
for subspace clustering, constrained clustering, and active learning.
<em>DMKD</em>, <em>36</em>(3), 958–986. (<a
href="https://doi.org/10.1007/s10618-022-00820-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral-based subspace clustering methods have proved successful in many challenging applications such as gene sequencing, image recognition, and motion segmentation. In this work, we first propose a novel spectral-based subspace clustering algorithm that seeks to represent each point as a sparse convex combination of a few nearby points. We then extend the algorithm to a constrained clustering and active learning framework. Our motivation for developing such a framework stems from the fact that typically either a small amount of labelled data are available in advance; or it is possible to label some points at a cost. The latter scenario is typically encountered in the process of validating a cluster assignment. Extensive experiments on simulated and real datasets show that the proposed approach is effective and competitive with state-of-the-art methods.},
  archive      = {J_DMKD},
  author       = {Peng, Hankui and Pavlidis, Nicos G.},
  doi          = {10.1007/s10618-022-00820-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {958-986},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Weighted sparse simplex representation: A unified framework for subspace clustering, constrained clustering, and active learning},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). XEM: An explainable-by-design ensemble method for
multivariate time series classification. <em>DMKD</em>, <em>36</em>(3),
917–957. (<a href="https://doi.org/10.1007/s10618-022-00823-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present XEM, an eXplainable-by-design Ensemble method for Multivariate time series classification. XEM relies on a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation shows that XEM outperforms the state-of-the-art MTS classifiers on the public UEA datasets. Furthermore, XEM provides faithful explainability-by-design and manifests robust performance when faced with challenges arising from continuous data collection (different MTS length, missing data and noise).},
  archive      = {J_DMKD},
  author       = {Fauvel, Kevin and Fromont, Élisa and Masson, Véronique and Faverdin, Philippe and Termier, Alexandre},
  doi          = {10.1007/s10618-022-00823-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {917-957},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {XEM: An explainable-by-design ensemble method for multivariate time series classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introducing the contrast profile: A novel time series
primitive that allows real world classification. <em>DMKD</em>,
<em>36</em>(2), 877–915. (<a
href="https://doi.org/10.1007/s10618-022-00824-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data remains a perennially important datatype considered in data mining. In the last decade there has been an increasing realization that time series data can be best understood by reasoning about time series subsequences on the basis of their similarity to other subsequences: the two most familiar such time series concepts being motifs and discords. Time series motifs refer to two particularly close subsequences, whereas time series discords indicate subsequences that are far from their nearest neighbors. However, we argue that it can sometimes be useful to simultaneously reason about a subsequence’s closeness to certain data and its distance to other data. In this work we introduce a novel primitive called the Contrast Profile that allows us to efficiently compute such a definition in a principled way. As we will show, the Contrast Profile has many downstream uses, including anomaly detection, data exploration, and preprocessing unstructured data for classification. We demonstrate the utility of the Contrast Profile by showing how it allows end-to-end classification in datasets with tens of billions of datapoints, and how it can be used to explore datasets and reveal subtle patterns that might otherwise escape our attention. Moreover, we demonstrate the generality of the Contrast Profile by presenting detailed case studies in domains as diverse as seismology, animal behavior, and cardiology.},
  archive      = {J_DMKD},
  author       = {Mercer, Ryan and Alaee, Sara and Abdoli, Alireza and Senobari, Nader Shakibay and Singh, Shailendra and Murillo, Amy and Keogh, Eamonn},
  doi          = {10.1007/s10618-022-00824-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {877-915},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Introducing the contrast profile: A novel time series primitive that allows real world classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PAC-bayesian lifelong learning for multi-armed bandits.
<em>DMKD</em>, <em>36</em>(2), 841–876. (<a
href="https://doi.org/10.1007/s10618-022-00825-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a PAC-Bayesian analysis of lifelong learning. In the lifelong learning problem, a sequence of learning tasks is observed one-at-a-time, and the goal is to transfer information acquired from previous tasks to new learning tasks. We consider the case when each learning task is a multi-armed bandit problem. We derive lower bounds on the expected average reward that would be obtained if a given multi-armed bandit algorithm was run in a new task with a particular prior and for a set number of steps. We propose lifelong learning algorithms that use our new bounds as learning objectives. Our proposed algorithms are evaluated in several lifelong multi-armed bandit problems and are found to perform better than a baseline method that does not use generalisation bounds.},
  archive      = {J_DMKD},
  author       = {Flynn, Hamish and Reeb, David and Kandemir, Melih and Peters, Jan},
  doi          = {10.1007/s10618-022-00825-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {841-876},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {PAC-bayesian lifelong learning for multi-armed bandits},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Counterfactual inference with latent variable and its
application in mental health care. <em>DMKD</em>, <em>36</em>(2),
811–840. (<a href="https://doi.org/10.1007/s10618-021-00818-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of modeling counterfactual reasoning in scenarios where, apart from the observed endogenous variables, we have a latent variable that affects the outcomes and, consequently, the results of counterfactuals queries. This is a common setup in healthcare problems, including mental health. We propose a new framework where the aforementioned problem is modeled as a multivariate regression and the counterfactual model accounts for both observed and a latent variable, where the latter represents what we call the patient individuality factor ( $$\upvarphi $$ ). In mental health, focusing on individuals is paramount, as past experiences can change how people see or deal with situations, but individuality cannot be directly measured. To the best of our knowledge, this is the first counterfactual approach that considers both observational and latent variables to provide deterministic answers to counterfactual queries, such as: what if I change the social support of a patient, to what extent can I change his/her anxiety? The framework combines concepts from deep representation learning and causal inference to infer the value of $$\upvarphi $$ and capture both non-linear and multiplicative effects of causal variables. Experiments are performed with both synthetic and real-world datasets, where we predict how changes in people’s actions may lead to different outcomes in terms of symptoms of mental illness and quality of life. Results show the model learns the individually factor with errors lower than 0.05 and answers counterfactual queries that are supported by the medical literature. The model has the potential to recommend small changes in people’s lives that may completely change their relationship with mental illness.},
  archive      = {J_DMKD},
  author       = {Marchezini, Guilherme F. and Lacerda, Anisio M. and Pappa, Gisele L. and Meira, Wagner and Miranda, Debora and Romano-Silva, Marco A. and Costa, Danielle S. and Diniz, Leandro Malloy},
  doi          = {10.1007/s10618-021-00818-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {811-840},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Counterfactual inference with latent variable and its application in mental health care},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust regression via error tolerance. <em>DMKD</em>,
<em>36</em>(2), 781–810. (<a
href="https://doi.org/10.1007/s10618-022-00819-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets are often characterised by outliers; data items that do not follow the same structure as the rest of the data. These outliers might negatively influence modelling of the data. In data analysis it is, therefore, important to consider methods that are robust to outliers. In this paper we develop a robust regression method that finds the largest subset of data items that can be approximated using a sparse linear model to a given precision. We show that this can yield the best possible robustness to outliers. However, this problem is NP-hard and to solve it we present an efficient approximation algorithm, termed SLISE. Our method extends existing state-of-the-art robust regression methods, especially in terms of speed on high-dimensional datasets. We demonstrate our method by applying it to both synthetic and real-world regression problems.},
  archive      = {J_DMKD},
  author       = {Björklund, Anton and Henelius, Andreas and Oikarinen, Emilia and Kallonen, Kimmo and Puolamäki, Kai},
  doi          = {10.1007/s10618-022-00819-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {781-810},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Robust regression via error tolerance},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synwalk: Community detection via random walk modelling.
<em>DMKD</em>, <em>36</em>(2), 739–780. (<a
href="https://doi.org/10.1007/s10618-021-00809-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex systems, abstractly represented as networks, are ubiquitous in everyday life. Analyzing and understanding these systems requires, among others, tools for community detection. As no single best community detection algorithm can exist, robustness across a wide variety of problem settings is desirable. In this work, we present Synwalk, a random walk-based community detection method. Synwalk builds upon a solid theoretical basis and detects communities by synthesizing the random walk induced by the given network from a class of candidate random walks. We thoroughly validate the effectiveness of our approach on synthetic and empirical networks, respectively, and compare Synwalk’s performance with the performance of Infomap and Walktrap (also random walk-based), Louvain (based on modularity maximization) and stochastic block model inference. Our results indicate that Synwalk performs robustly on networks with varying mixing parameters and degree distributions. We outperform Infomap on networks with high mixing parameter, and Infomap and Walktrap on networks with many small communities and low average degree. Our work has a potential to inspire further development of community detection via synthesis of random walks and we provide concrete ideas for future research.},
  archive      = {J_DMKD},
  author       = {Toth, Christian and Helic, Denis and Geiger, Bernhard C.},
  doi          = {10.1007/s10618-021-00809-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {739-780},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Synwalk: Community detection via random walk modelling},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Provable randomized rounding for minimum-similarity
diversification. <em>DMKD</em>, <em>36</em>(2), 709–738. (<a
href="https://doi.org/10.1007/s10618-021-00811-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When searching for information in a data collection, we are often interested not only in finding relevant items, but also in assembling a diverse set, so as to explore different concepts that are present in the data. This problem has been researched extensively. However, finding a set of items with minimal pairwise similarities can be computationally challenging, and most existing works striving for quality guarantees assume that item relatedness is measured by a distance function. Given the widespread use of similarity functions in many domains, we believe this to be an important gap in the literature. In this paper we study the problem of finding a diverse set of items, when item relatedness is measured by a similarity function. We formulate the diversification task using a flexible, broadly applicable minimization objective, consisting of the sum of pairwise similarities of the selected items and a relevance penalty term. To find good solutions we adopt a randomized rounding strategy, which is challenging to analyze because of the cardinality constraint present in our formulation. Even though this obstacle can be overcome using dependent rounding, we show that it is possible to obtain provably good solutions using an independent approach, which is faster, simpler to implement and completely parallelizable. Our analysis relies on a novel bound for the ratio of Poisson-Binomial densities, which is of independent interest and has potential implications for other combinatorial-optimization problems. We leverage this result to design an efficient randomized algorithm that provides a lower-order additive approximation guarantee. We validate our method using several benchmark datasets, and show that it consistently outperforms the greedy approaches that are commonly used in the literature.},
  archive      = {J_DMKD},
  author       = {Ordozgoiti, Bruno and Mahadevan, Ananth and Matakos, Antonis and Gionis, Aristides},
  doi          = {10.1007/s10618-021-00811-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {709-738},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Provable randomized rounding for minimum-similarity diversification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequence graph transform (SGT): A feature embedding function
for sequence data mining. <em>DMKD</em>, <em>36</em>(2), 668–708. (<a
href="https://doi.org/10.1007/s10618-021-00813-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence feature embedding is a challenging task due to the unstructuredness of sequences—arbitrary strings of arbitrary length. Existing methods are efficient in extracting short-term dependencies but typically suffer from computation issues for the long-term. Sequence Graph Transform (SGT), a feature embedding function, that can extract a varying amount of short- to long-term dependencies without increasing the computation is proposed. SGT’s properties are analytically proved for interpretation under normal and uniform distribution assumptions. SGT features yield significantly superior results in sequence clustering and classification with higher accuracy and lower computation as compared to the existing methods, including the state-of-the-art sequence/string Kernels and LSTM.},
  archive      = {J_DMKD},
  author       = {Ranjan, Chitta and Ebrahimi, Samaneh and Paynabar, Kamran},
  doi          = {10.1007/s10618-021-00813-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {668-708},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sequence graph transform (SGT): A feature embedding function for sequence data mining},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). INK: Knowledge graph embeddings for node classification.
<em>DMKD</em>, <em>36</em>(2), 620–667. (<a
href="https://doi.org/10.1007/s10618-021-00806-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques are increasingly being applied to solve various machine learning tasks that use Knowledge Graphs as input data. However, these techniques typically learn a latent representation for the entities of interest internally, which is then used to make decisions. This latent representation is often not comprehensible to humans, which is why deep learning techniques are often considered to be black boxes. In this paper, we present INK: Instance Neighbouring by using Knowledge, a novel technique to learn binary feature-based representations, which are comprehensible to humans, for nodes of interest in a knowledge graph. We demonstrate the predictive power of the node representations obtained through INK by feeding them to classical machine learning techniques and comparing their predictive performances for the node classification task to the current state of the art: Graph Convolutional Networks (R-GCN) and RDF2Vec. We perform this comparison both on benchmark datasets and using a real-world use case.},
  archive      = {J_DMKD},
  author       = {Steenwinckel, Bram and Vandewiele, Gilles and Weyns, Michael and Agozzino, Terencio and Turck, Filip De and Ongenae, Femke},
  doi          = {10.1007/s10618-021-00806-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {620-667},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {INK: Knowledge graph embeddings for node classification},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An eager splitting strategy for online decision trees in
ensembles. <em>DMKD</em>, <em>36</em>(2), 566–619. (<a
href="https://doi.org/10.1007/s10618-021-00816-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision tree ensembles are widely used in practice. In this work, we study in ensemble settings the effectiveness of replacing the split strategy for the state-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more eager splitting strategy that we had previously published as Hoeffding AnyTime Tree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine whether the current best candidate split is superior to the current split, with the possibility of revision, while Hoeffding Tree aims to determine whether the top candidate is better than the second best and if a test is selected, fixes it for all posterity. HATT converges to the ideal batch tree while Hoeffding Tree does not. We find that HATT is an efficacious base learner for online bagging and online boosting ensembles. On UCI and synthetic streams, HATT as a base learner outperforms HT at a 0.05 significance level for the majority of tested ensembles on what we believe is the largest and most comprehensive set of testbenches in the online learning literature. Our results indicate that HATT is a superior alternative to Hoeffding Tree in a large number of ensemble settings.},
  archive      = {J_DMKD},
  author       = {Manapragada, Chaitanya and Gomes, Heitor M. and Salehi, Mahsa and Bifet, Albert and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-021-00816-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {566-619},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An eager splitting strategy for online decision trees in ensembles},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient binary embedding of categorical data using
BinSketch. <em>DMKD</em>, <em>36</em>(2), 537–565. (<a
href="https://doi.org/10.1007/s10618-021-00815-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a dimensionality reduction algorithm, aka. sketching, for categorical datasets. Our proposed sketching algorithm Cabin constructs low-dimensional binary sketches from high-dimensional categorical vectors, and our distance estimation algorithm Cham computes a close approximation of the Hamming distance between any two original vectors only from their sketches. The minimum dimension of the sketches required by Cham to ensure a good estimation theoretically depends only on the sparsity of the data points—making it useful for many real-life scenarios involving sparse datasets. We present a rigorous theoretical analysis of our approach and supplement it with extensive experiments on several high-dimensional real-world data sets, including one with over a million dimensions. We show that the Cabin and Cham duo is a significantly fast and accurate approach for tasks such as $$\mathrm {RMSE}$$ , all-pair similarity, and clustering when compared to working with the full dataset and other dimensionality reduction techniques.},
  archive      = {J_DMKD},
  author       = {Verma, Bhisham Dev and Pratap, Rameshwar and Bera, Debajyoti},
  doi          = {10.1007/s10618-021-00815-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {537-565},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient binary embedding of categorical data using BinSketch},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic cyber risk estimation with competitive quantile
autoregression. <em>DMKD</em>, <em>36</em>(2), 513–536. (<a
href="https://doi.org/10.1007/s10618-021-00814-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing value of data held in enterprises makes it an attractive target to attackers. The increasing likelihood and impact of a cyber attack have highlighted the importance of effective cyber risk estimation. We propose two methods for modelling Value-at-Risk (VaR) which can be used for any time-series data. The first approach is based on Quantile Autoregression (QAR), which can estimate VaR for different quantiles, i. e. confidence levels. The second method, we term Competitive Quantile Autoregression (CQAR), dynamically re-estimates cyber risk as soon as new data becomes available. This method provides a theoretical guarantee that it asymptotically performs as well as any QAR at any time point in the future. We show that these methods can predict the size and inter-arrival time of cyber hacking breaches by running coverage tests. The proposed approaches allow to model a separate stochastic process for each significance level and therefore provide more flexibility compared to previously proposed techniques. We provide a fully reproducible code used for conducting the experiments.},
  archive      = {J_DMKD},
  author       = {Dzhamtyrova, Raisa and Maple, Carsten},
  doi          = {10.1007/s10618-021-00814-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {513-536},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Dynamic cyber risk estimation with competitive quantile autoregression},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A recurrent neural network architecture to model physical
activity energy expenditure in older people. <em>DMKD</em>,
<em>36</em>(1), 477–512. (<a
href="https://doi.org/10.1007/s10618-021-00817-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through the quantification of physical activity energy expenditure (PAEE), health care monitoring has the potential to stimulate vital and healthy ageing, inducing behavioural changes in older people and linking these to personal health gains. To be able to measure PAEE in a health care perspective, methods from wearable accelerometers have been developed, however, mainly targeted towards younger people. Since elderly subjects differ in energy requirements and range of physical activities, the current models may not be suitable for estimating PAEE among the elderly. Furthermore, currently available methods seem to be either simple but non-generalizable or require elaborate (manual) feature construction steps. Because past activities influence present PAEE, we propose a modeling approach known for its ability to model sequential data, the recurrent neural network (RNN). To train the RNN for an elderly population, we used the growing old together validation (GOTOV) dataset with 34 healthy participants of 60 years and older (mean 65 years old), performing 16 different activities. We used accelerometers placed on wrist and ankle, and measurements of energy counts by means of indirect calorimetry. After optimization, we propose an architecture consisting of an RNN with 3 GRU layers and a feedforward network combining both accelerometer and participant-level data. Our efforts included switching mean to standard deviation for down-sampling the input data and combining temporal and static data (person-specific details such as age, weight, BMI). The resulting architecture produces accurate PAEE estimations while decreasing training input and time by a factor of 10. Subsequently, compared to the state-of-the-art, it is capable to integrate longer activity data which lead to more accurate estimations of low intensity activities EE. It can thus be employed to investigate associations of PAEE with vitality parameters of older people related to metabolic and cognitive health and mental well-being.},
  archive      = {J_DMKD},
  author       = {Paraschiakos, Stylianos and de Sá, Cláudio Rebelo and Okai, Jeremiah and Slagboom, P. Eline and Beekman, Marian and Knobbe, Arno},
  doi          = {10.1007/s10618-021-00817-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {477-512},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A recurrent neural network architecture to model physical activity energy expenditure in older people},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strengthening ties towards a highly-connected world.
<em>DMKD</em>, <em>36</em>(1), 448–476. (<a
href="https://doi.org/10.1007/s10618-021-00812-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks provide a forum where people make new connections, learn more about the world, get exposed to different points of view, and access information that were previously inaccessible. It is natural to assume that content-delivery algorithms in social networks should not only aim to maximize user engagement but also to offer opportunities for increasing connectivity and enabling social networks to achieve their full potential. Our motivation and aim is to develop methods that foster the creation of new connections, and subsequently, improve the flow of information in the network. To achieve our goal, we propose to leverage the strong triadic closure principle, and consider violations to this principle as opportunities for creating more social links. We formalize this idea as an algorithmic problem related to the densest k-subgraph problem. For this new problem, we establish hardness results and propose approximation algorithms. We identify two special cases of the problem that admit a constant-factor approximation. Finally, we experimentally evaluate our proposed algorithm on real-world social networks, and we additionally evaluate some simpler but more scalable algorithms.},
  archive      = {J_DMKD},
  author       = {Matakos, Antonis and Gionis, Aristides},
  doi          = {10.1007/s10618-021-00812-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {448-476},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Strengthening ties towards a highly-connected world},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential stratified regeneration: MCMC for large state
spaces with an application to subgraph count estimation. <em>DMKD</em>,
<em>36</em>(1), 414–447. (<a
href="https://doi.org/10.1007/s10618-021-00802-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the general task of estimating the sum of a bounded function over the edges of a graph, given neighborhood query access and where access to the entire network is prohibitively expensive. To estimate this sum, prior work proposes Markov chain Monte Carlo (MCMC) methods that use random walks started at some seed vertex and whose equilibrium distribution is the uniform distribution over all edges, eliminating the need to iterate over all edges. Unfortunately, these existing estimators are not scalable to massive real-world graphs. In this paper, we introduce Ripple, an MCMC-based estimator that achieves unprecedented scalability by stratifying the Markov chain state space into ordered strata with a new technique that we denote sequential stratified regenerations. We show that the Ripple estimator is consistent, highly parallelizable, and scales well. We empirically evaluate our method by applying Ripple to the task of estimating connected, induced subgraph counts given some input graph. Therein, we demonstrate that Ripple is accurate and can estimate counts of up to 12-node subgraphs, which is a task at a scale that has been considered unreachable, not only by prior MCMC-based methods but also by other sampling approaches. For instance, in this target application, we present results in which the Markov chain state space is as large as $$10^{43}$$ , for which Ripple computes estimates in less than 4 h, on average.},
  archive      = {J_DMKD},
  author       = {Teixeira, Carlos H. C. and Kakodkar, Mayank and Dias, Vinícius and Meira, Wagner and Ribeiro, Bruno},
  doi          = {10.1007/s10618-021-00802-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {414-447},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sequential stratified regeneration: MCMC for large state spaces with an application to subgraph count estimation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining sequences with exceptional transition behaviour of
varying order using quality measures based on information-theoretic
scoring functions. <em>DMKD</em>, <em>36</em>(1), 379–413. (<a
href="https://doi.org/10.1007/s10618-021-00808-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete Markov chains are frequently used to analyse transition behaviour in sequential data. Here, the transition probabilities can be estimated using varying order Markov chains, where order k specifies the length of the sequence history that is used to model these probabilities. Generally, such a model is fitted to the entire dataset, but in practice it is likely that some heterogeneity in the data exists and that some sequences would be better modelled with alternative parameter values, or with a Markov chain of a different order. We use the framework of Exceptional Model Mining (EMM) to discover these exceptionally behaving sequences. In particular, we propose an EMM model class that allows for discovering subgroups with transition behaviour of varying order. To that end, we propose three new quality measures based on information-theoretic scoring functions. Our findings from controlled experiments show that all three quality measures find exceptional transition behaviour of varying order and are reasonably sensitive. The quality measure based on Akaike’s Information Criterion is most robust for the number of observations. We furthermore add to existing work by seeking for subgroups of sequences, as opposite to subgroups of transitions. Since we use sequence-level descriptive attributes, we form subgroups of entire sequences, which is practically relevant in situations where you want to identify the originators of exceptional sequences, such as patients. We show this relevance by analysing sequences of blood glucose values of adult persons with diabetes type 2. In the experiments, we find subgroups of patients based on age and glycated haemoglobin (HbA1c), a measure known to correlate with average blood glucose values. Clinicians and domain experts confirmed the transition behaviour as estimated by the fitted Markov chain models.},
  archive      = {J_DMKD},
  author       = {Schouten, Rianne M. and Bueno, Marcos L. P. and Duivesteijn, Wouter and Pechenizkiy, Mykola},
  doi          = {10.1007/s10618-021-00808-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {379-413},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining sequences with exceptional transition behaviour of varying order using quality measures based on information-theoretic scoring functions},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient procedure for mining egocentric temporal
motifs. <em>DMKD</em>, <em>36</em>(1), 355–378. (<a
href="https://doi.org/10.1007/s10618-021-00803-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graphs are structures which model relational data between entities that change over time. Due to the complex structure of data, mining statistically significant temporal subgraphs, also known as temporal motifs, is a challenging task. In this work, we present an efficient technique for extracting temporal motifs in temporal networks. Our method is based on the novel notion of egocentric temporal neighborhoods, namely multi-layer structures centered on an ego node. Each temporal layer of the structure consists of the first-order neighborhood of the ego node, and corresponding nodes in sequential layers are connected by an edge. The strength of this approach lies in the possibility of encoding these structures into a unique bit vector, thus bypassing the problem of graph isomorphism in searching for temporal motifs. This allows our algorithm to mine substantially larger motifs with respect to alternative approaches. Furthermore, by bringing the focus on the temporal dynamics of the interactions of a specific node, our model allows to mine temporal motifs which are visibly interpretable. Experiments on a number of complex networks of social interactions confirm the advantage of the proposed approach over alternative non-egocentric solutions. The egocentric procedure is indeed more efficient in revealing similarities and discrepancies among different social environments, independently of the different technologies used to collect data, which instead affect standard non-egocentric measures.},
  archive      = {J_DMKD},
  author       = {Longa, Antonio and Cencetti, Giulia and Lepri, Bruno and Passerini, Andrea},
  doi          = {10.1007/s10618-021-00803-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {355-378},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An efficient procedure for mining egocentric temporal motifs},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controlling hallucinations at word level in data-to-text
generation. <em>DMKD</em>, <em>36</em>(1), 318–354. (<a
href="https://doi.org/10.1007/s10618-021-00801-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-to-Text Generation (DTG) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statements –usually called hallucinations—in their outputs. The control of this phenomenon is today a major challenge for DTG, and is the problem addressed in the paper. Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard WikiBio benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder. Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of ToTTo show that our model could be successfully used on very noisy settings.},
  archive      = {J_DMKD},
  author       = {Rebuffel, Clement and Roberti, Marco and Soulier, Laure and Scoutheeten, Geoffrey and Cancelliere, Rossella and Gallinari, Patrick},
  doi          = {10.1007/s10618-021-00801-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {318-354},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Controlling hallucinations at word level in data-to-text generation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Expected passes. <em>DMKD</em>, <em>36</em>(1), 295–317. (<a
href="https://doi.org/10.1007/s10618-021-00810-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passes are by far football’s (soccer) most frequent event, yet surprisingly little meaningful research has been devoted to quantify them. With the increase in availability of so-called positional data, describing the positioning of players and ball at every moment of the game, our work aims to determine the difficulty of every pass by calculating its success probability based on its surrounding circumstances. As most experts will agree, not all passes are of equal difficulty, however, most traditional metrics count them as such. With our work we can quantify how well players can execute passes, assess their risk profile, and even compute completion probabilities for hypothetical passes by combining physical and machine learning models. Our model uses the first 0.4 seconds of a ball trajectory and the movement vectors of all players to predict the intended target of a pass with an accuracy of $$93.0\%$$ for successful and $$72.0\%$$ for unsuccessful passes much higher than any previously published work. Our extreme gradient boosting model can then quantify the likelihood of a successful pass completion towards the identified target with an area under the curve (AUC) of $$93.4\%$$ . Finally, we discuss several potential applications, like player scouting or evaluating pass decisions.},
  archive      = {J_DMKD},
  author       = {Anzer, Gabriel and Bauer, Pascal},
  doi          = {10.1007/s10618-021-00810-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {295-317},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Expected passes},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal state change bayesian networks for modeling of
evolving multivariate state sequences: Model, structure discovery and
parameter estimation. <em>DMKD</em>, <em>36</em>(1), 240–294. (<a
href="https://doi.org/10.1007/s10618-021-00807-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many systems can be expressed as multivariate state sequences (MSS) in terms of entities and their states with evolving dependencies over time. In order to interpret the temporal dynamics in such data, it is essential to capture relationships between entities and their changes in state and dependence over time under uncertainty. Existing probabilistic models do not explicitly model the evolution of causality between dependent state sequences and mostly result in complex structures when representing complete causal dependencies between random variables. To solve this, Temporal State Change Bayesian Networks (TSCBN) are introduced to effectively model interval relations of MSSs under evolving uncertainty. Our model outperforms competing approaches in terms of parameter complexity and expressiveness. Further, an efficient structure discovery method for TSCBNs is presented, that improves classical approaches by exploiting temporal knowledge and multiple parameter estimation approaches for TSCBNs are introduced. Those are expectation maximization, variational inference and a sampling based maximum likelihood estimation that allow to learn parameters from partially observed MSSs. Lastly, we demonstrate how TSCBNs allow to interpret and infer patterns of captured sequences for specification mining in automotive.},
  archive      = {J_DMKD},
  author       = {Mrowca, Artur and Gyrock, Florian and Günnemann, Stephan},
  doi          = {10.1007/s10618-021-00807-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {240-294},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Temporal state change bayesian networks for modeling of evolving multivariate state sequences: Model, structure discovery and parameter estimation},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized core maintenance of dynamic bipartite graphs.
<em>DMKD</em>, <em>36</em>(1), 209–239. (<a
href="https://doi.org/10.1007/s10618-021-00805-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k-core is important in many graph mining applications, such as community detection and clique finding. As one generalized concept of k-core, (i, j)-core is more suited for bipartite graph analysis since it can identify the functions of two different types of vertices. Because (i, j)-cores evolve as edges are inserted into (removed from) a dynamic bipartite graph, it is more economical to maintain them rather than decompose the graph recursively when only a few edges change. Moreover, many applications (e.g., graph visualization) only focus on some dense (i, j)-cores. Existing solutions are simply insufficiently adaptable. They must maintain all (i, j)-cores rather than just a subset of them, which requires more effort. To solve this issue, we propose novel maintenance methods for updating expected (i, j)-cores. To estimate the influence scope of inserted (removed) edges, we first construct quasi-(i, j)-cores, which loosen the constraint of (i, j)-cores but have similar properties. Second, we present a bottom-up approach for efficiently maintaining all (i, j)-cores, from sparse to dense. Thirdly, because certain applications only focus on dense (i, j)-cores of top-n layers, we also propose a top-down approach to maintain (i, j)-cores from dense to sparse. Finally, we conduct extensive experiments to validate the efficiency of proposed approaches. Experimental results show that our maintenance solutions outperform existing approaches by one order of magnitude.},
  archive      = {J_DMKD},
  author       = {Bai, Wen and Chen, Yadi and Wu, Di and Huang, Zhichuan and Zhou, Yipeng and Xu, Chuan},
  doi          = {10.1007/s10618-021-00805-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {209-239},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Generalized core maintenance of dynamic bipartite graphs},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Matrix sketching for supervised classification with
imbalanced classes. <em>DMKD</em>, <em>36</em>(1), 174–208. (<a
href="https://doi.org/10.1007/s10618-021-00791-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of imbalanced classes is more and more common in practical applications and it is known to heavily compromise the learning process. In this paper we propose a new method aimed at addressing this issue in binary supervised classification. Re-balancing the class sizes has turned out to be a fruitful strategy to overcome this problem. Our proposal performs re-balancing through matrix sketching. Matrix sketching is a recently developed data compression technique that is characterized by the property of preserving most of the linear information that is present in the data. Such property is guaranteed by the Johnson-Lindenstrauss’ Lemma (1984) and allows to embed an n-dimensional space into a reduced one without distorting, within an $$\epsilon $$ -size interval, the distances between any pair of points. We propose to use matrix sketching as an alternative to the standard re-balancing strategies that are based on random under-sampling the majority class or random over-sampling the minority one. We assess the properties of our method when combined with linear discriminant analysis (LDA), classification trees (C4.5) and Support Vector Machines (SVM) on simulated and real data. Results show that sketching can represent a sound alternative to the most widely used rebalancing methods.},
  archive      = {J_DMKD},
  author       = {Falcone, Roberta and Anderlucci, Laura and Montanari, Angela},
  doi          = {10.1007/s10618-021-00791-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {174-208},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Matrix sketching for supervised classification with imbalanced classes},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic change point detection using a mixed bayesian model.
<em>DMKD</em>, <em>36</em>(1), 146–173. (<a
href="https://doi.org/10.1007/s10618-021-00804-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic text documents, including news articles, user reviews, and blogs, are now commonly encountered in many fields. Accordingly, the topics underlying text streams also change over time. To grasp the topic changes in the increasing accumulation of text documents, there is a great need to develop automatic text analysis models to find the key changes in topics. To this end, this study proposes a topic change point detection (Topic-CD) model. Different from previous studies, we define the change point of topics from the perspective of hyperparameters associated with topic-word distributions. This allows the model to detect change points underlying the whole topic set. Under this definition, the topic modeling and change point detection are combined in a unified framework and then performed simultaneously using a Markov chain Monte Carlo algorithm. In addition, the Topic-CD model is free from setting the number of change points in advance, which makes it more convenient for practical use. We investigate the performance of the Topic-CD model numerically using synthetic data and three real datasets. The results show that the Topic-CD model can well identify the change points in topics when compared with several state-of-the-art methods.},
  archive      = {J_DMKD},
  author       = {Lu, Xiaoling and Guo, Yuxuan and Chen, Jiayi and Wang, Feifei},
  doi          = {10.1007/s10618-021-00804-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {146-173},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Topic change point detection using a mixed bayesian model},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mint: MDL-based approach for mining INTeresting numerical
pattern sets. <em>DMKD</em>, <em>36</em>(1), 108–145. (<a
href="https://doi.org/10.1007/s10618-021-00799-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern mining is well established in data mining research, especially for mining binary datasets. Surprisingly, there is much less work about numerical pattern mining and this research area remains under-explored. In this paper we propose Mint, an efficient MDL-based algorithm for mining numerical datasets. The MDL principle is a robust and reliable framework widely used in pattern mining, and as well in subgroup discovery. In Mint we reuse MDL for discovering useful patterns and returning a set of non-redundant overlapping patterns with well-defined boundaries and covering meaningful groups of objects. Mint is not alone in the category of numerical pattern miners based on MDL. In the experiments presented in the paper we show that Mint outperforms competitors among which IPD, RealKrimp, and Slim.},
  archive      = {J_DMKD},
  author       = {Makhalova, Tatiana and Kuznetsov, Sergei O. and Napoli, Amedeo},
  doi          = {10.1007/s10618-021-00799-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {108-145},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mint: MDL-based approach for mining INTeresting numerical pattern sets},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring range of information diffusion based on historical
frequent items. <em>DMKD</em>, <em>36</em>(1), 82–107. (<a
href="https://doi.org/10.1007/s10618-021-00800-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate the range of information diffusion is critical for social network and user behavior analysis. Selecting nodes to constitute the range of information diffusion is challenging by the classic independent cascade and linear threshold models, due to the unknown topology of large-scale online social networks (OSNs). In this paper, we start from the mining of frequent itemsets in historical records of information diffusion, and adopt Bayesian network (BN) as the framework to represent and infer the implied dependence relations among frequent items. To make probabilistic inferences to infer the range, we first propose a greedy algorithm to select the observed nodes as the evidence of BN inference, for which we propose the metric of proximity degree and prove its submodularity. Then, we give the algorithm to construct the item-association BN (IABN) to represent the dependencies among frequent items. Following, we present an approximate algorithm to infer the range of information diffusion w.r.t. the observed nodes. Experimental results show that the observed nodes could be selected and the range of information diffusion could be inferred effectively. Empirical studies also demonstrate that our proposed IABN outperforms some state-of-the-art methods to obtain relatively complete nodes in the range of information diffusion.},
  archive      = {J_DMKD},
  author       = {Liu, Weiyi and Yue, Kun and Li, Jianyu and Li, Jie and Li, Jin and Zhang, Zhijian},
  doi          = {10.1007/s10618-021-00800-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {82-107},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inferring range of information diffusion based on historical frequent items},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end deep representation learning for time series
clustering: A comparative study. <em>DMKD</em>, <em>36</em>(1), 29–81.
(<a href="https://doi.org/10.1007/s10618-021-00796-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are ubiquitous in data mining applications. Similar to other types of data, annotations can be challenging to acquire, thus preventing from training time series classification models. In this context, clustering methods can be an appropriate alternative as they create homogeneous groups allowing a better analysis of the data structure. Time series clustering has been investigated for many years and multiple approaches have already been proposed. Following the advent of deep learning in computer vision, researchers recently started to study the use of deep clustering to cluster time series data. The existing approaches mostly rely on representation learning (imported from computer vision), which consists of learning a representation of the data and performing the clustering task using this new representation. The goal of this paper is to provide a careful study and an experimental comparison of the existing literature on time series representation learning for deep clustering. In this paper, we went beyond the sole comparison of existing approaches and proposed to decompose deep clustering methods into three main components: (1) network architecture, (2) pretext loss, and (3) clustering loss. We evaluated all combinations of these components (totaling 300 different models) with the objective to study their relative influence on the clustering performance. We also experimentally compared the most efficient combinations we identified with existing non-deep clustering methods. Experiments were performed using the largest repository of time series datasets (the UCR/UEA archive) composed of 128 univariate and 30 multivariate datasets. Finally, we proposed an extension of the class activation maps method to the unsupervised case which allows to identify patterns providing highlights on how the network clustered the time series.},
  archive      = {J_DMKD},
  author       = {Lafabregue, Baptiste and Weber, Jonathan and Gançarski, Pierre and Forestier, Germain},
  doi          = {10.1007/s10618-021-00796-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {29-81},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {End-to-end deep representation learning for time series clustering: A comparative study},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-sensitive ensemble learning: A unifying framework.
<em>DMKD</em>, <em>36</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s10618-021-00790-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, a plethora of cost-sensitive methods have been proposed for learning on data when different types of misclassification errors incur different costs. Our contribution is a unifying framework that provides a comprehensive and insightful overview on cost-sensitive ensemble methods, pinpointing their differences and similarities via a fine-grained categorization. Our framework contains natural extensions and generalisations of ideas across methods, be it AdaBoost, Bagging or Random Forest, and as a result not only yields all methods known to date but also some not previously considered.},
  archive      = {J_DMKD},
  author       = {Petrides, George and Verbeke, Wouter},
  doi          = {10.1007/s10618-021-00790-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Cost-sensitive ensemble learning: A unifying framework},
  volume       = {36},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
