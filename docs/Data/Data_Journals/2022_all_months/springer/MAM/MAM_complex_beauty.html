<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---34">MAM - 34</h2>
<ul>
<li><details>
<summary>
(2022). Ethical considerations in the application of artificial
intelligence to monitor social media for COVID-19 data. <em>MAM</em>,
<em>32</em>(4), 759–768. (<a
href="https://doi.org/10.1007/s11023-022-09610-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic and its related policies (e.g., stay at home and social distancing orders) have increased people’s use of digital technology, such as social media. Researchers have, in turn, utilized artificial intelligence to analyze social media data for public health surveillance. For example, through machine learning and natural language processing, they have monitored social media data to examine public knowledge and behavior. This paper explores the ethical considerations of using artificial intelligence to monitor social media to understand the public’s perspectives and behaviors surrounding COVID-19, including potential risks and benefits of an AI-driven approach. Importantly, investigators and ethics committees have a role in ensuring that researchers adhere to ethical principles of respect for persons, beneficence, and justice in a way that moves science forward while ensuring public safety and confidence in the process.},
  archive      = {J_MAM},
  author       = {Flores, Lidia and Young, Sean D.},
  doi          = {10.1007/s11023-022-09610-0},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {759-768},
  shortjournal = {Minds Mach.},
  title        = {Ethical considerations in the application of artificial intelligence to monitor social media for COVID-19 data},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The US algorithmic accountability act of 2022 vs. The EU
artificial intelligence act: What can they learn from each other?
<em>MAM</em>, <em>32</em>(4), 751–758. (<a
href="https://doi.org/10.1007/s11023-022-09612-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the whole, the US Algorithmic Accountability Act of 2022 (US AAA) is a pragmatic approach to balancing the benefits and risks of automated decision systems. Yet there is still room for improvement. This commentary highlights how the US AAA can both inform and learn from the European Artificial Intelligence Act (EU AIA).},
  archive      = {J_MAM},
  author       = {Mökander, Jakob and Juneja, Prathm and Watson, David S. and Floridi, Luciano},
  doi          = {10.1007/s11023-022-09612-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {751-758},
  shortjournal = {Minds Mach.},
  title        = {The US algorithmic accountability act of 2022 vs. the EU artificial intelligence act: What can they learn from each other?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Why indirect harms do not support social robot rights.
<em>MAM</em>, <em>32</em>(4), 735–749. (<a
href="https://doi.org/10.1007/s11023-022-09593-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing evidence to support the claim that we react differently to robots than we do to other objects. In particular, we react differently to robots with which we have some form of social interaction. In this paper I critically assess the claim that, due to our tendency to become emotionally attached to social robots, permitting their harm may be damaging for society and as such we should consider introducing legislation to grant social robots rights and protect them from harm. I conclude that there is little evidence to support this claim and that legislation in this area would restrict progress in areas of social care where social robots are a potentially valuable resource.},
  archive      = {J_MAM},
  author       = {Sweeney, Paula},
  doi          = {10.1007/s11023-022-09593-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {735-749},
  shortjournal = {Minds Mach.},
  title        = {Why indirect harms do not support social robot rights},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ethics of self-driving cars: A naturalistic approach.
<em>MAM</em>, <em>32</em>(4), 717–734. (<a
href="https://doi.org/10.1007/s11023-022-09604-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential development of self-driving cars (also known as autonomous vehicles or AVs – particularly Level 5 AVs) has called the attention of different interested parties. Yet, there are still only a few relevant international regulations on them, no emergency patterns accepted by communities and Original Equipment Manufacturers (OEMs), and no publicly accepted solutions to some of their pending ethical problems. Thus, this paper aims to provide some possible answers to these moral and practical dilemmas. In particular, we focus on what AVs should do in no-win scenarios and on who should be held responsible for these types of decisions. A naturalistic perspective on ethics informs our proposal, which, we argue, could represent a pragmatic and realistic solution to the regulation of AVs. We discuss the proposals already set out in the current literature regarding both policy-making strategies and theoretical accounts. In fact, we consider and reject descriptive approaches to the problem as well as the option of using either a strict deontological view or a solely utilitarian one to set AVs’ ethical choices. Instead, to provide concrete answers to AVs’ ethical problems, we examine three hierarchical levels of decision-making processes: country-wide regulations, OEM policies, and buyers’ moral attitudes. By appropriately distributing ethical decisions and considering their practical implications, we maintain that our proposal based on ethical naturalism recognizes the importance of all stakeholders and allows the most able of them to take actions (the OEMs and buyers) to reflect on the moral leeway and weight of their options.},
  archive      = {J_MAM},
  author       = {Arfini, Selene and Spinelli, Davide and Chiffi, Daniele},
  doi          = {10.1007/s11023-022-09604-y},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {717-734},
  shortjournal = {Minds Mach.},
  title        = {Ethics of self-driving cars: A naturalistic approach},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From pluralistic normative principles to autonomous-agent
rules. <em>MAM</em>, <em>32</em>(4), 683–715. (<a
href="https://doi.org/10.1007/s11023-022-09614-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advancements in systems engineering and artificial intelligence, autonomous agents are increasingly being called upon to execute tasks that have normative relevance. These are tasks that directly—and potentially adversely—affect human well-being and demand of the agent a degree of normative-sensitivity and -compliance. Such norms and normative principles are typically of a social, legal, ethical, empathetic, or cultural (‘SLEEC’) nature. Whereas norms of this type are often framed in the abstract, or as high-level principles, addressing normative concerns in concrete applications of autonomous agents requires the refinement of normative principles into explicitly formulated practical rules. This paper develops a process for deriving specification rules from a set of high-level norms, thereby bridging the gap between normative principles and operational practice. This enables autonomous agents to select and execute the most normatively favourable action in the intended context premised on a range of underlying relevant normative principles. In the translation and reduction of normative principles to SLEEC rules, we present an iterative process that uncovers normative principles, addresses SLEEC concerns, identifies and resolves SLEEC conflicts, and generates both preliminary and complex normatively-relevant rules, thereby guiding the development of autonomous agents and better positioning them as normatively SLEEC-sensitive or SLEEC-compliant.},
  archive      = {J_MAM},
  author       = {Townsend, Beverley and Paterson, Colin and Arvind, T. T. and Nemirovsky, Gabriel and Calinescu, Radu and Cavalcanti, Ana and Habli, Ibrahim and Thomas, Alan},
  doi          = {10.1007/s11023-022-09614-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {683-715},
  shortjournal = {Minds Mach.},
  title        = {From pluralistic normative principles to autonomous-agent rules},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the advantages of distinguishing between predictive and
allocative fairness in algorithmic decision-making. <em>MAM</em>,
<em>32</em>(4), 655–682. (<a
href="https://doi.org/10.1007/s11023-022-09615-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of algorithmic fairness is typically framed as the problem of finding a unique formal criterion that guarantees that a given algorithmic decision-making procedure is morally permissible. In this paper, I argue that this is conceptually misguided and that we should replace the problem with two sub-problems. If we examine how most state-of-the-art machine learning systems work, we notice that there are two distinct stages in the decision-making process. First, a prediction of a relevant property is made. Secondly, a decision is taken based (at least partly) on this prediction. These two stages have different aims: the prediction is aimed at accuracy, while the decision is aimed at allocating a given good in a way that maximizes some context-relative utility measure. Correspondingly, two different fairness issues can arise. First, predictions could be biased in discriminatory ways. This means that the predictions contain systematic errors for a specific group of individuals. Secondly, the system’s decisions could result in an allocation of goods that is in tension with the principles of distributive justice. These two fairness issues are distinct problems that require different types of solutions. I here provide a formal framework to address both issues and argue that this way of conceptualizing them resolves some of the paradoxes present in the discussion of algorithmic fairness.},
  archive      = {J_MAM},
  author       = {Beigang, Fabian},
  doi          = {10.1007/s11023-022-09615-9},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {655-682},
  shortjournal = {Minds Mach.},
  title        = {On the advantages of distinguishing between predictive and allocative fairness in algorithmic decision-making},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamical systems implementation of intrinsic sentence
meaning. <em>MAM</em>, <em>32</em>(4), 627–653. (<a
href="https://doi.org/10.1007/s11023-022-09590-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a model for implementation of intrinsic natural language sentence meaning in a physical language understanding system, where &#39;intrinsic&#39; is understood as &#39;independent of meaning ascription by system-external observers&#39;. The proposal is that intrinsic meaning can be implemented as a point attractor in the state space of a nonlinear dynamical system with feedback which is generated by temporally sequenced inputs. It is motivated by John Searle&#39;s well known (Behavioral and Brain Sciences, 3: 417–57, 1980) critique of the then-standard and currently still influential computational theory of mind (CTM), the essence of which was that CTM representations lack intrinsic meaning because that meaning is dependent on ascription by an observer. The proposed dynamical model comprises a collection of interacting artificial neural networks, and constitutes a radical simplification of the principle of compositional phrase structure which is at the heart of the current standard view of sentence semantics because it is computationally interpretable as a finite state machine.},
  archive      = {J_MAM},
  author       = {Moisl, Hermann},
  doi          = {10.1007/s11023-022-09590-1},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {627-653},
  shortjournal = {Minds Mach.},
  title        = {Dynamical systems implementation of intrinsic sentence meaning},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal and evidential conditionals. <em>MAM</em>,
<em>32</em>(4), 613–626. (<a
href="https://doi.org/10.1007/s11023-022-09606-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We put forth an account for when to believe causal and evidential conditionals. The basic idea is to embed a causal model in an agent’s belief state. For the evaluation of conditionals seems to be relative to beliefs about both particular facts and causal relations. Unlike other attempts using causal models, we show that ours can account rather well not only for various causal but also evidential conditionals.},
  archive      = {J_MAM},
  author       = {Günther, Mario},
  doi          = {10.1007/s11023-022-09606-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {613-626},
  shortjournal = {Minds Mach.},
  title        = {Causal and evidential conditionals},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The end of vagueness: Technological epistemicism,
surveillance capitalism, and explainable artificial intelligence.
<em>MAM</em>, <em>32</em>(3), 585–611. (<a
href="https://doi.org/10.1007/s11023-022-09609-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) pervades humanity in 2022, and it is notoriously difficult to understand how certain aspects of it work. There is a movement—Explainable Artificial Intelligence (XAI)—to develop new methods for explaining the behaviours of AI systems. We aim to highlight one important philosophical significance of XAI—it has a role to play in the elimination of vagueness. To show this, consider that the use of AI in what has been labeled surveillance capitalism has resulted in humans quickly gaining the capability to identify and classify most of the occasions in which languages are used. We show that the knowability of this information is incompatible with what a certain theory of vagueness—epistemicism—says about vagueness. We argue that one way the epistemicist could respond to this threat is to claim that this process brought about the end of vagueness. However, we suggest an alternative interpretation, namely that epistemicism is false, but there is a weaker doctrine we dub technological epistemicism, which is the view that vagueness is due to ignorance of linguistic usage, but the ignorance can be overcome. The idea is that knowing more of the relevant data and how to process it enables us to know the semantic values of our words and sentences with higher confidence and precision. Finally, we argue that humans are probably not going to believe what future AI algorithms tell us about the sharp boundaries of our vague words unless the AI involved can be explained in terms understandable by humans. That is, if people are going to accept that AI can tell them about the sharp boundaries of the meanings of their words, then it is going to have to be XAI.},
  archive      = {J_MAM},
  author       = {Kerr, Alison Duncan and Scharp, Kevin},
  doi          = {10.1007/s11023-022-09609-7},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {585-611},
  shortjournal = {Minds Mach.},
  title        = {The end of vagueness: Technological epistemicism, surveillance capitalism, and explainable artificial intelligence},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defining explanation and explanatory depth in XAI.
<em>MAM</em>, <em>32</em>(3), 563–584. (<a
href="https://doi.org/10.1007/s11023-022-09607-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (XAI) aims to help people understand black box algorithms, particularly of their outputs. But what are these explanations and when is one explanation better than another? The manipulationist definition of explanation from the philosophy of science offers good answers to these questions, holding that an explanation consists of a generalization that shows what happens in counterfactual cases. Furthermore, when it comes to explanatory depth this account holds that a generalization that has more abstract variables, is broader in scope and/or more accurate is better. By applying these definitions and contrasting them with alternative definitions in the XAI literature I hope to help clarify what a good explanation is for AI.},
  archive      = {J_MAM},
  author       = {Buijsman, Stefan},
  doi          = {10.1007/s11023-022-09607-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {563-584},
  shortjournal = {Minds Mach.},
  title        = {Defining explanation and explanatory depth in XAI},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assembled bias: Beyond transparent algorithmic bias.
<em>MAM</em>, <em>32</em>(3), 533–562. (<a
href="https://doi.org/10.1007/s11023-022-09605-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we make the case for the emergence of novel kind of bias with the use of algorithmic decision-making systems. We argue that the distinctive generative process of feature creation, characteristic of machine learning (ML), contorts feature parameters in ways that can lead to emerging feature spaces that encode novel algorithmic bias involving already marginalized groups. We term this bias assembled bias. Moreover, assembled biases are distinct from the much-discussed algorithmic bias, both in source (training data versus feature creation) and in content (mimics of extant societal bias versus reconfigured categories). As such, this problem is distinct from issues arising from bias-encoding training feature sets or proxy features. Assembled bias is not epistemically transparent in source or content. Hence, when these ML models are used as a basis for decision-making in social contexts, algorithmic fairness concerns are compounded.},
  archive      = {J_MAM},
  author       = {Waller, Robyn Repko and Waller, Russell L.},
  doi          = {10.1007/s11023-022-09605-x},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {533-562},
  shortjournal = {Minds Mach.},
  title        = {Assembled bias: Beyond transparent algorithmic bias},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainable artificial intelligence in data science.
<em>MAM</em>, <em>32</em>(3), 485–531. (<a
href="https://doi.org/10.1007/s11023-022-09603-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A widespread need to explain the behavior and outcomes of AI-based systems has emerged, due to their ubiquitous presence. Thus, providing renewed momentum to the relatively new research area of eXplainable AI (XAI). Nowadays, the importance of XAI lies in the fact that the increasing control transference to this kind of system for decision making -or, at least, its use for assisting executive stakeholders- already affects many sensitive realms (as in Politics, Social Sciences, or Law). The decision-making power handover to opaque AI systems makes mandatory explaining those, primarily in application scenarios where the stakeholders are unaware of both the high technology applied and the basic principles governing the technological solutions. The issue should not be reduced to a merely technical problem; the explainer would be compelled to transmit richer knowledge about the system (including its role within the informational ecosystem where he/she works). To achieve such an aim, the explainer could exploit, if necessary, practices from other scientific and humanistic areas. The first aim of the paper is to emphasize and justify the need for a multidisciplinary approach that is beneficiated from part of the scientific and philosophical corpus on Explaining, underscoring the particular nuances of the issue within the field of Data Science. The second objective is to develop some arguments justifying the authors’ bet by a more relevant role of ideas inspired by, on the one hand, formal techniques from Knowledge Representation and Reasoning, and on the other hand, the modeling of human reasoning when facing the explanation. This way, explaining modeling practices would seek a sound balance between the pure technical justification and the explainer-explainee agreement.},
  archive      = {J_MAM},
  author       = {Borrego-Díaz, Joaquín and Galán-Páez, Juan},
  doi          = {10.1007/s11023-022-09603-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {485-531},
  shortjournal = {Minds Mach.},
  title        = {Explainable artificial intelligence in data science},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From representations in predictive processing to degrees of
representational features. <em>MAM</em>, <em>32</em>(3), 461–484. (<a
href="https://doi.org/10.1007/s11023-022-09599-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whilst the topic of representations is one of the key topics in philosophy of mind, it has only occasionally been noted that representations and representational features may be gradual. Apart from vague allusions, little has been said on what representational gradation amounts to and why it could be explanatorily useful. The aim of this paper is to provide a novel take on gradation of representational features within the neuroscientific framework of predictive processing. More specifically, we provide a gradual account of two features of structural representations: structural similarity and decoupling. We argue that structural similarity can be analysed in terms of two dimensions: number of preserved relations and state space granularity. Both dimensions can take on different values and hence render structural similarity gradual. We further argue that decoupling is gradual in two ways. First, we show that different brain areas are involved in decoupled cognitive processes to a greater or lesser degree depending on the cause (internal or external) of their activity. Second, and more importantly, we show that the degree of decoupling can be further regulated in some brain areas through precision weighting of prediction error. We lastly argue that gradation of decoupling (via precision weighting) and gradation of structural similarity (via state space granularity) are conducive to behavioural success.},
  archive      = {J_MAM},
  author       = {Rutar, Danaja and Wiese, Wanja and Kwisthout, Johan},
  doi          = {10.1007/s11023-022-09599-6},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {461-484},
  shortjournal = {Minds Mach.},
  title        = {From representations in predictive processing to degrees of representational features},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Schema-centred unity and process-centred pluralism of the
predictive mind. <em>MAM</em>, <em>32</em>(3), 433–459. (<a
href="https://doi.org/10.1007/s11023-022-09595-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proponents of the predictive processing (PP) framework often claim that one of the framework’s significant virtues is its unificatory power. What is supposedly unified are predictive processes in the mind, and these are explained in virtue of a common prediction error-minimisation (PEM) schema. In this paper, I argue against the claim that PP currently converges towards a unified explanation of cognitive processes. Although the notion of PEM systematically relates a set of posits such as ‘efficiency’ and ‘hierarchical coding’ into a unified conceptual schema, neither the frameworks’ algorithmic specifications nor its hypotheses about their implementations in the brain are clearly unified. I propose a novel way to understand the fruitfulness of the research program in light of a set of research heuristics that are partly shared with those common to Bayesian reverse engineering. An interesting consequence of this proposal is that pluralism is at least as important as unification to promote the positive development of the predictive mind.},
  archive      = {J_MAM},
  author       = {Poth, Nina},
  doi          = {10.1007/s11023-022-09595-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {433-459},
  shortjournal = {Minds Mach.},
  title        = {Schema-centred unity and process-centred pluralism of the predictive mind},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Are propositional attitudes mental states? <em>MAM</em>,
<em>32</em>(3), 417–432. (<a
href="https://doi.org/10.1007/s11023-022-09594-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I present an argument that propositional attitudes are not mental states. In a nutshell, the argument is that if propositional attitudes are mental states, then only minded beings could have them; but there are reasons to think that some non-minded beings could bear propositional attitudes. To illustrate this, I appeal to cases of genuine group intentionality. I argue that these are cases in which some group entities bear propositional attitudes, but they are not subjects of mental states. Although propositional attitudes are not mental states, I propose that they are typically co-instantiated with mental states. In an attempt to explain this co-instantiation, I suggest that propositional attitudes of minded beings are typically realized by mental states.},
  archive      = {J_MAM},
  author       = {Baysan, Umut},
  doi          = {10.1007/s11023-022-09594-x},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {417-432},
  shortjournal = {Minds Mach.},
  title        = {Are propositional attitudes mental states?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Is your neural data part of your mind? Exploring the
conceptual basis of mental privacy. <em>MAM</em>, <em>32</em>(2),
395–415. (<a href="https://doi.org/10.1007/s11023-021-09574-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been argued that neural data (ND) are an especially sensitive kind of personal information that could be used to undermine the control we should have over access to our mental states (i.e. our mental privacy), and therefore need a stronger legal protection than other kinds of personal data. The Morningside Group, a global consortium of interdisciplinary experts advocating for the ethical use of neurotechnology, suggests achieving this by treating legally ND as a body organ (i.e. protecting them through bodily integrity). Although the proposal is currently shaping ND-related policies (most notably, a Neuroprotection Bill of Law being discussed by the Chilean Senate), it is not clear what its conceptual and legal basis is. Treating legally something as something else requires some kind of analogical reasoning, which is not provided by the authors of the proposal. In this paper, I will try to fill this gap by addressing ontological issues related to neurocognitive processes. The substantial differences between ND and body organs or organic tissue cast doubt on the idea that the former should be covered by bodily integrity. Crucially, ND are not constituted by organic material. Nevertheless, I argue that the ND of a subject s are analogous to neurocognitive properties of her brain. I claim that (i) s’ ND are a ‘medium independent’ property that can be characterized as natural semantic personal information about her brain and that (ii) s’ brain not only instantiates this property but also has an exclusive ontological relationship with it: This information constitutes a domain that is unique to her neurocognitive architecture.},
  archive      = {J_MAM},
  author       = {Wajnerman Paz, Abel},
  doi          = {10.1007/s11023-021-09574-7},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {395-415},
  shortjournal = {Minds Mach.},
  title        = {Is your neural data part of your mind? exploring the conceptual basis of mental privacy},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid theory of event memory. <em>MAM</em>,
<em>32</em>(2), 365–394. (<a
href="https://doi.org/10.1007/s11023-021-09578-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amongst philosophers, there is ongoing debate about what successful event remembering requires. Causal theorists argue that it requires a causal connection to the past event. Simulation theorists argue, in contrast, that successful remembering requires only production by a reliable memory system. Both views must contend with the fact that people can remember past events they have experienced with varying degrees of accuracy. The debate between them thus concerns not only the account of successful remembering, but how each account explains the various forms of memory error as well. Advancing the debate therefore must include exploration of the cognitive architecture implicated by each view and whether that architecture is capable of producing the range of event representations seen in human remembering. Our paper begins by exploring these architectures, framing casual theories as best suited to the storage of event instances and simulation theories as best suited to store schemas. While each approach has its advantages, neither can account for the full range of our event remembering abilities. We then propose a novel hybrid theory that combines both instance and schematic elements in the event memory. In addition, we provide an implementation of our theory in the context of a cognitive architecture. We also discuss an agent we developed using this system and its ability to remember events in the blocks world domain.},
  archive      = {J_MAM},
  author       = {Ménager, David H. and Choi, Dongkyu and Robins, Sarah K.},
  doi          = {10.1007/s11023-021-09578-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {365-394},
  shortjournal = {Minds Mach.},
  title        = {A hybrid theory of event memory},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Playing games with ais: The limits of GPT-3 and similar
large language models. <em>MAM</em>, <em>32</em>(2), 341–364. (<a
href="https://doi.org/10.1007/s11023-022-09602-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article contributes to the debate around the abilities of large language models such as GPT-3, dealing with: firstly, evaluating how well GPT does in the Turing Test, secondly the limits of such models, especially their tendency to generate falsehoods, and thirdly the social consequences of the problems these models have with truth-telling. We start by formalising the recently proposed notion of reversible questions, which Floridi &amp; Chiriatti (2020) propose allow one to ‘identify the nature of the source of their answers’, as a probabilistic measure based on Item Response Theory from psychometrics. Following a critical assessment of the methodology which led previous scholars to dismiss GPT’s abilities, we argue against claims that GPT-3 completely lacks semantic ability. Using ideas of compression, priming, distributional semantics and semantic webs we offer our own theory of the limits of large language models like GPT-3, and argue that GPT can competently engage in various semantic tasks. The real reason GPT’s answers seem senseless being that truth-telling is not amongst them. We claim that these kinds of models cannot be forced into producing only true continuation, but rather to maximise their objective function they strategize to be plausible instead of truthful. This, we moreover claim, can hijack our intuitive capacity to evaluate the accuracy of its outputs. Finally, we show how this analysis predicts that a widespread adoption of language generators as tools for writing could result in permanent pollution of our informational ecosystem with massive amounts of very plausible but often untrue texts.},
  archive      = {J_MAM},
  author       = {Sobieszek, Adam and Price, Tadeusz},
  doi          = {10.1007/s11023-022-09602-0},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {341-364},
  shortjournal = {Minds Mach.},
  title        = {Playing games with ais: The limits of GPT-3 and similar large language models},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: What might machines mean? <em>MAM</em>,
<em>32</em>(2), 339. (<a
href="https://doi.org/10.1007/s11023-022-09601-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Green, Mitchell and Michel, Jan G.},
  doi          = {10.1007/s11023-022-09601-1},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {339},
  shortjournal = {Minds Mach.},
  title        = {Correction to: What might machines mean?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). What might machines mean? <em>MAM</em>, <em>32</em>(2),
323–338. (<a href="https://doi.org/10.1007/s11023-022-09589-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This essay addresses the question whether artificial speakers can perform speech acts in the technical sense of that term common in the philosophy of language. We here argue that under certain conditions artificial speakers can perform speech acts so understood. After (§1) explaining some of the issues at stake in these questions, we (§2) elucidate a relatively uncontroversial way in which machines can communicate, namely through what we call verbal signaling. But verbal signaling is not sufficient for the performance of a speech act. To explain the difference, we (§3) elucidate the notion of a speech act developed by Austin (How to Do Things with Words, 1962) in the mid-twentieth century and then discuss Strawson’s (&quot;Intention and Convention in Speech Acts&quot;, 1964) influential proposal for how that notion may be related to Grice’s (&quot;Meaning&quot;, 1957) conception of speaker meaning. We then refine Strawson’s synthesis in light of Armstrong’s (&quot;Meaning and Communication&quot;, 1971) reconceptualization of speaker meaning in terms of objectives rather than intentions. We next (§4) extend this conception of speech acts to the cases of recorded, proxy, and conditional speech acts. On this basis, we propose (§5) that a characteristic role for artificial speakers is as proxies in the performance of speech acts on behalf of their human creators. We (§6) also consider two objections to our position, and compare our approach with others: while other authors appeal to notions such as “quasi-assertion,” we offer a sharp characterization of what artificial speakers can do that does not impute intentions or similarly controversial powers to them. We conclude (§7) by raising doubts that our strategy can be applied to speech acts generally.},
  archive      = {J_MAM},
  author       = {Green, Mitchell and Michel, Jan G.},
  doi          = {10.1007/s11023-022-09589-8},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {323-338},
  shortjournal = {Minds Mach.},
  title        = {What might machines mean?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dretske and informational closure. <em>MAM</em>,
<em>32</em>(2), 311–322. (<a
href="https://doi.org/10.1007/s11023-021-09587-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Christoph Jäger (Erkenntnis 61:187–201, 2004) has argued that Dretske’s (Knowledge and the flow of Information, MIT Press, Cambridge, 1981) information-based account of knowledge is committed to both knowledge and information closure under known entailment. However, in a reply to Jäger, Dretske (Erkenntnis 64:409–413, 2006) defended his view on the basis of a discrepancy between the relation of information and the relation of logical implication. This paper shares Jäger’s criticism that Dretske’s externalist notion of information implies closure, but provides an analysis based on different grounds. By means of a distinction between two perspectives, the mathematical perspective and the epistemological perspective, I present, in the former, a notion of logical implication that is compatible with the notion of information in the mathematical theory of information, and I show how, in the latter, Dretske’s logical reading of the closure principle is incompatible with his information-theoretic epistemological framework.},
  archive      = {J_MAM},
  author       = {Bouchard, Yves},
  doi          = {10.1007/s11023-021-09587-2},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {311-322},
  shortjournal = {Minds Mach.},
  title        = {Dretske and informational closure},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified model of ad hoc concepts in conceptual spaces.
<em>MAM</em>, <em>32</em>(2), 289–309. (<a
href="https://doi.org/10.1007/s11023-021-09586-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ad hoc concepts (like “clothes to wear in the snow”, for instance) are highly-context dependent representations humans construct to deal with novel or uncommon situations and to interpret linguistic stimuli in communication. In the last decades, such concepts have been investigated both in experimental cognitive psychology and within pragmatics by proponents of so-called relevance theory. These two research lines have however proceeded in parallel, proposing two unconnected strategies to account for the construction and use of ad hoc concepts. The present work explores the relations between these two approaches and the possibility of merging them into a unique account of the internal structure of ad hoc representations and of the key processes involved in their constructions. To this purpose, we first present an integrated two-level account of the construction of ad hoc representations from lexical concepts; then, we show how our account can be embedded in a conceptual space framework that allows for a natural, geometrical interpretation of the main steps in such a construction process. After discussing in detail two main examples of the construction of ad hoc concepts within conceptual spaces, we conclude with some remarks on possible extensions of our approach.},
  archive      = {J_MAM},
  author       = {Coraci, Davide},
  doi          = {10.1007/s11023-021-09586-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {289-309},
  shortjournal = {Minds Mach.},
  title        = {A unified model of ad hoc concepts in conceptual spaces},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strictly human: Limitations of autonomous systems.
<em>MAM</em>, <em>32</em>(2), 269–288. (<a
href="https://doi.org/10.1007/s11023-021-09582-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can autonomous systems replace humans in the performance of their activities? How does the answer to this question inform the design of autonomous systems?  The study of technical systems and their features should be preceded by the study of the activities in which they play roles. Each activity can be described by its overall goals, governing norms and the intermediate steps which are taken to achieve the goals and to follow the norms. This paper uses the activity realist approach to conceptualize autonomous systems in the context of human activities. By doing so, it first argues for epistemic and logical conditions that illustrate the limitations of autonomous systems in tasks which they can and cannot perform, and then, it discusses the ramifications of the limitations of system autonomy on the design of autonomous systems.},
  archive      = {J_MAM},
  author       = {Soltanzadeh, Sadjad},
  doi          = {10.1007/s11023-021-09582-7},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {269-288},
  shortjournal = {Minds Mach.},
  title        = {Strictly human: Limitations of autonomous systems},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conformity assessments and post-market monitoring: A guide
to the role of auditing in the proposed european AI regulation.
<em>MAM</em>, <em>32</em>(2), 241–268. (<a
href="https://doi.org/10.1007/s11023-021-09577-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed European Artificial Intelligence Act (AIA) is the first attempt to elaborate a general legal framework for AI carried out by any major global economy. As such, the AIA is likely to become a point of reference in the larger discourse on how AI systems can (and should) be regulated. In this article, we describe and discuss the two primary enforcement mechanisms proposed in the AIA: the conformity assessments that providers of high-risk AI systems are expected to conduct, and the post-market monitoring plans that providers must establish to document the performance of high-risk AI systems throughout their lifetimes. We argue that the AIA can be interpreted as a proposal to establish a Europe-wide ecosystem for conducting AI auditing, albeit in other words. Our analysis offers two main contributions. First, by describing the enforcement mechanisms included in the AIA in terminology borrowed from existing literature on AI auditing, we help providers of AI systems understand how they can prove adherence to the requirements set out in the AIA in practice. Second, by examining the AIA from an auditing perspective, we seek to provide transferable lessons from previous research about how to refine further the regulatory approach outlined in the AIA. We conclude by highlighting seven aspects of the AIA where amendments (or simply clarifications) would be helpful. These include, above all, the need to translate vague concepts into verifiable criteria and to strengthen the institutional safeguards concerning conformity assessments based on internal checks.},
  archive      = {J_MAM},
  author       = {Mökander, Jakob and Axente, Maria and Casolari, Federico and Floridi, Luciano},
  doi          = {10.1007/s11023-021-09577-4},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {241-268},
  shortjournal = {Minds Mach.},
  title        = {Conformity assessments and post-market monitoring: A guide to the role of auditing in the proposed european AI regulation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scientific exploration and explainable artificial
intelligence. <em>MAM</em>, <em>32</em>(1), 219–239. (<a
href="https://doi.org/10.1007/s11023-021-09583-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models developed using machine learning are increasingly prevalent in scientific research. At the same time, these models are notoriously opaque. Explainable AI aims to mitigate the impact of opacity by rendering opaque models transparent. More than being just the solution to a problem, however, Explainable AI can also play an invaluable role in scientific exploration. This paper describes how post-hoc analytic techniques from Explainable AI can be used to refine target phenomena in medical science, to identify starting points for future investigations of (potentially) causal relationships, and to generate possible explanations of target phenomena in cognitive science. In this way, this paper describes how Explainable AI—over and above machine learning itself—contributes to the efficiency and scope of data-driven scientific research.},
  archive      = {J_MAM},
  author       = {Zednik, Carlos and Boelsen, Hannes},
  doi          = {10.1007/s11023-021-09583-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {219-239},
  shortjournal = {Minds Mach.},
  title        = {Scientific exploration and explainable artificial intelligence},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local explanations via necessity and sufficiency: Unifying
theory and practice. <em>MAM</em>, <em>32</em>(1), 185–218. (<a
href="https://doi.org/10.1007/s11023-022-09598-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Necessity and sufficiency are the building blocks of all successful explanations. Yet despite their importance, these notions have been conceptually underdeveloped and inconsistently applied in explainable artificial intelligence (XAI), a fast-growing research area that is so far lacking in firm theoretical foundations. In this article, an expanded version of a paper originally presented at the 37th Conference on Uncertainty in Artificial Intelligence (Watson et al., 2021), we attempt to fill this gap. Building on work in logic, probability, and causality, we establish the central role of necessity and sufficiency in XAI, unifying seemingly disparate methods in a single formal framework. We propose a novel formulation of these concepts, and demonstrate its advantages over leading alternatives. We present a sound and complete algorithm for computing explanatory factors with respect to a given context and set of agentive preferences, allowing users to identify necessary and sufficient conditions for desired outcomes at minimal cost. Experiments on real and simulated data confirm our method’s competitive performance against state of the art XAI tools on a diverse array of tasks.},
  archive      = {J_MAM},
  author       = {Watson, David S. and Gultchin, Limor and Taly, Ankur and Floridi, Luciano},
  doi          = {10.1007/s11023-022-09598-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {185-218},
  shortjournal = {Minds Mach.},
  title        = {Local explanations via necessity and sufficiency: Unifying theory and practice},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The automated laplacean demon: How ML challenges our views
on prediction and explanation. <em>MAM</em>, <em>32</em>(1), 159–183.
(<a href="https://doi.org/10.1007/s11023-021-09575-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Certain characteristics make machine learning (ML) a powerful tool for processing large amounts of data, and also particularly unsuitable for explanatory purposes. There are worries that its increasing use in science may sideline the explanatory goals of research. We analyze the key characteristics of ML that might have implications for the future directions in scientific research: epistemic opacity and the ‘theory-agnostic’ modeling. These characteristics are further analyzed in a comparison of ML with the traditional statistical methods, in order to demonstrate what it is specifically that makes ML methodology substantially unsuitable for reaching explanations. The analysis is given broader philosophical context by connecting it with the views on the role of prediction and explanation in science, their relationship, and the value of explanation. We proceed to show, first, that ML disrupts the relationship between prediction and explanation commonly understood as a functional relationship. Then we show that the value of explanation is not exhausted in purely predictive functions, but rather has a ubiquitously recognized value for both science and everyday life. We then invoke two hypothetical scenarios with different degrees of automatization of science, which help test our intuitions on the role of explanation in science. The main question we address is whether ML will reorient or otherwise impact our standard explanatory practice. We conclude with a prognosis that ML would diversify science into purely predictively oriented research based on ML-like techniques and, on the other hand, remaining faithful to anthropocentric research focused on the search for explanation.},
  archive      = {J_MAM},
  author       = {Srećković, Sanja and Berber, Andrea and Filipović, Nenad},
  doi          = {10.1007/s11023-021-09575-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {159-183},
  shortjournal = {Minds Mach.},
  title        = {The automated laplacean demon: How ML challenges our views on prediction and explanation},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explanations in AI as claims of tacit knowledge.
<em>MAM</em>, <em>32</em>(1), 135–158. (<a
href="https://doi.org/10.1007/s11023-021-09588-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI systems become increasingly complex it may become unclear, even to the designer of a system, why exactly a system does what it does. This leads to a lack of trust in AI systems. To solve this, the field of explainable AI has been working on ways to produce explanations of these systems’ behaviors. Many methods in explainable AI, such as LIME (Ribeiro et al. in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016), offer only a statistical argument for the validity of their explanations. However, some methods instead study the internal structure of the system and try to find components which can be assigned an interpretation. I believe that these methods provide more valuable explanations than those statistical in nature. I will try to identify which explanations can be considered internal to the system using the Chomskyan notion of tacit knowledge. I argue that each explanation expresses a rule, and through the localization of this rule in the system internals, we can take a system to have tacit knowledge of the rule. I conclude that the only methods which are able to sufficiently establish this tacit knowledge are those along the lines of Olah (Distill 2(11): 4901–4911, 2017), and therefore they provide explanations with unique strengths.},
  archive      = {J_MAM},
  author       = {Lam, Nardi},
  doi          = {10.1007/s11023-021-09588-1},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {135-158},
  shortjournal = {Minds Mach.},
  title        = {Explanations in AI as claims of tacit knowledge},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analogue models and universal machines. Paradigms of
epistemic transparency in artificial intelligence. <em>MAM</em>,
<em>32</em>(1), 111–133. (<a
href="https://doi.org/10.1007/s11023-022-09596-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of epistemic opacity in Artificial Intelligence (AI) is often characterised as a problem of intransparent algorithms that give rise to intransparent models. However, the degrees of transparency of an AI model should not be taken as an absolute measure of the properties of its algorithms but of the model’s degree of intelligibility to human users. Its epistemically relevant elements are to be specified on various levels above and beyond the computational one. In order to elucidate this claim, I first contrast computer models and their claims to algorithm-based universality with cybernetics-style analogue models and their claims to structural isomorphism between elements of model and target system (in: Black, Models and metaphors, 1962). While analogue models aim at perceptually or conceptually accessible model-target relations, computer models give rise to a specific kind of underdetermination in these relations that needs to be addressed in specific ways. I then undertake a comparison between two contemporary AI approaches that, although related, distinctly align with the above modelling paradigms and represent distinct strategies towards model intelligibility: Deep Neural Networks and Predictive Processing. I conclude that their respective degrees of epistemic transparency primarily depend on the underlying purposes of modelling, not on their computational properties.},
  archive      = {J_MAM},
  author       = {Greif, Hajo},
  doi          = {10.1007/s11023-022-09596-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {111-133},
  shortjournal = {Minds Mach.},
  title        = {Analogue models and universal machines. paradigms of epistemic transparency in artificial intelligence},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The intriguing relation between counterfactual explanations
and adversarial examples. <em>MAM</em>, <em>32</em>(1), 77–109. (<a
href="https://doi.org/10.1007/s11023-021-09580-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The same method that creates adversarial examples (AEs) to fool image-classifiers can be used to generate counterfactual explanations (CEs) that explain algorithmic decisions. This observation has led researchers to consider CEs as AEs by another name. We argue that the relationship to the true label and the tolerance with respect to proximity are two properties that formally distinguish CEs and AEs. Based on these arguments, we introduce CEs, AEs, and related concepts mathematically in a common framework. Furthermore, we show connections between current methods for generating CEs and AEs, and estimate that the fields will merge more and more as the number of common use-cases grows.},
  archive      = {J_MAM},
  author       = {Freiesleben, Timo},
  doi          = {10.1007/s11023-021-09580-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {77-109},
  shortjournal = {Minds Mach.},
  title        = {The intriguing relation between counterfactual explanations and adversarial examples},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two dimensions of opacity and the deep learning predicament.
<em>MAM</em>, <em>32</em>(1), 43–75. (<a
href="https://doi.org/10.1007/s11023-021-09569-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become increasingly successful in applications from biology to cosmology to social science. Trained DNNs, moreover, correspond to models that ideally allow the prediction of new phenomena. Building in part on the literature on ‘eXplainable AI’ (XAI), I here argue that these models are instrumental in a sense that makes them non-explanatory, and that their automated generation is opaque in a unique way. This combination implies the possibility of an unprecedented gap between discovery and explanation: When unsupervised models are successfully used in exploratory contexts, scientists face a whole new challenge in forming the concepts required for understanding underlying mechanisms.},
  archive      = {J_MAM},
  author       = {Boge, Florian J.},
  doi          = {10.1007/s11023-021-09569-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {43-75},
  shortjournal = {Minds Mach.},
  title        = {Two dimensions of opacity and the deep learning predicament},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simple models in complex worlds: Occam’s razor and
statistical learning theory. <em>MAM</em>, <em>32</em>(1), 13–42. (<a
href="https://doi.org/10.1007/s11023-022-09592-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea that “simplicity is a sign of truth”, and the related “Occam’s razor” principle, stating that, all other things being equal, simpler models should be preferred to more complex ones, have been long discussed in philosophy and science. We explore these ideas in the context of supervised machine learning, namely the branch of artificial intelligence that studies algorithms which balance simplicity and accuracy in order to effectively learn about the features of the underlying domain. Focusing on statistical learning theory, we show that situations exist for which a preference for simpler models (as modeled through the addition of a regularization term in the learning problem) provably slows down, instead of favoring, the supervised learning process. Our results shed new light on the relations between simplicity and truth approximation, which are briefly discussed in the context of both machine learning and the philosophy of science.},
  archive      = {J_MAM},
  author       = {Bargagli Stoffi, Falco J. and Cevolani, Gustavo and Gnecco, Giorgio},
  doi          = {10.1007/s11023-022-09592-z},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {13-42},
  shortjournal = {Minds Mach.},
  title        = {Simple models in complex worlds: Occam’s razor and statistical learning theory},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to: (What) can deep learning contribute to
theoretical linguistics? <em>MAM</em>, <em>32</em>(1), 11. (<a
href="https://doi.org/10.1007/s11023-022-09600-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Dupre, Gabe},
  doi          = {10.1007/s11023-022-09600-2},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {11},
  shortjournal = {Minds Mach.},
  title        = {Correction to: (What) can deep learning contribute to theoretical linguistics?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minds and machines special issue: Machine learning:
Prediction without explanation? <em>MAM</em>, <em>32</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11023-022-09597-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Boge, F. J. and Grünke, P. and Hillerbrand, R.},
  doi          = {10.1007/s11023-022-09597-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Minds Mach.},
  title        = {Minds and machines special issue: machine learning: prediction without explanation?},
  volume       = {32},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
