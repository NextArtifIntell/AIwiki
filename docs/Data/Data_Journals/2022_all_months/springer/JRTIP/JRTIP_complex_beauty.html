<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---93">JRTIP - 93</h2>
<ul>
<li><details>
<summary>
(2022a). Correction to: Reduced complexity and optimized face
recognition approach based on facial symmetry. <em>JRTIP</em>,
<em>19</em>(6), 1225. (<a
href="https://doi.org/10.1007/s11554-022-01231-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Ahmed, Waqas and Bajwa, Usama Ijaz and Anwar, Muhammad Waqas and Sajid, Muhammad},
  doi          = {10.1007/s11554-022-01231-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1225},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: Reduced complexity and optimized face recognition approach based on facial symmetry},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient bicubic interpolation implementation for
real-time image processing using hybrid computing. <em>JRTIP</em>,
<em>19</em>(6), 1211–1223. (<a
href="https://doi.org/10.1007/s11554-022-01254-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bicubic interpolation is a classic algorithm in the real-time image processing systems, which can achieve good quality at a relatively low hardware cost, and is also the fundamental component of many other much more complex algorithms. However, the multiply-accumulate units (MAC) in the bicubic require massive resources in the hardware-based implementation, which limits the use of the bicubic algorithm. In this article, a hybrid architecture of fix-point and stochastic computing is proposed to reduce the hardware resource consumption by computing the low-weight bits ambiguously. The proposed architecture is tested on standard image sets to survey the performance and is implemented on Intel Cyclone V and Xilinx Virtex-II targets to verify the hardware consumption. The experimental results show that the proposed architecture achieves significant resource reduction and even higher image processing speed compared to the existing architectures with comparable performance.},
  archive      = {J_JRTIP},
  author       = {Zhu, Yubin and Dai, Yonghang and Han, Kaining and Wang, Junchao and Hu, Jianhao},
  doi          = {10.1007/s11554-022-01254-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1211-1223},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient bicubic interpolation implementation for real-time image processing using hybrid computing},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards SSD accelerating for embedded environments: A
compressive sensing based approach. <em>JRTIP</em>, <em>19</em>(6),
1199–1210. (<a
href="https://doi.org/10.1007/s11554-022-01255-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the rise of convolutional neural networks (CNN), deep learning-based computer vision has been a dynamic field of research. Nevertheless, modern CNN architectures have not given sufficient consideration to real−time applications within limited computation settings and always compromise speed and accuracy. To this end, a novel approach to CNN design, based on the emerging technology of compressive sensing (CS), is proposed. For instance, CS networks function in a compression−reconstruction approach as an encoder−decoder neural network. This approach transforms the computer vision problem into a multioutput learning problem by incorporating the CS network into a recognition network for joint training. As to the deployment phase, images are obtained from a CS−acquisition device and fed directly, without reconstruction, to the new recognition network. Following such an approach considerably improves transmission bandwidth and reduces the computational burden. Furthermore, the redesigned CNN holds fewer parameters than its original counterpart, thus reducing model complexity. To validate our findings, object detection using the Single−Shot Detector (SSD) network was redesigned to operate in our CS−based ecosystem using different datasets. The results show that the lightweight CS network offers good performance at a faster running speed. For instance, the number of FLOPS was reduced by 57% compared to the SSD baseline. Furthermore, the proposed CS_SSD achieves a compelling accuracy while being 30% faster than its original counterpart on small GPUs. Code is available at: https://github.com/Bouderbal-Imene/CS-SSD .},
  archive      = {J_JRTIP},
  author       = {Bouderbal, Imene and Amamra, Abdenour and Djebbar, M. El-Arbi and Benatia, M. Akrem},
  doi          = {10.1007/s11554-022-01255-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1199-1210},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Towards SSD accelerating for embedded environments: A compressive sensing based approach},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast sand-dust video quality improvement method based on
adaptive dynamic guided filtering and interframe detection strategy.
<em>JRTIP</em>, <em>19</em>(6), 1181–1197. (<a
href="https://doi.org/10.1007/s11554-022-01248-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sand-dust weather seriously reduces the effectiveness of computer vision equipment acquisition. To solve this problem, a fast sand-dust video quality improvement method based on adaptive dynamic guided filtering and an interframe detection strategy is proposed in this paper. First, an adaptive gamma correction with weighting distribution and color balance (AGCWDCB) and adaptive dynamic guided filtering (ADGIF) are used to perform color correction, contrast enhancement and detailed information enhancement on the first frame of the video. Second, the interframe detection model is constructed based on the normalized mean square error information between video frames. Finally, each frame after the first frame of the sand-dust video is processed according to the interframe detection strategy until a sand-dust video with improved quality is obtained. Through qualitative and quantitative comprehensive experiments on sand-dust images and videos, the experimental results are compared with the existing methods, the results of processing sand-dust images using our improved frame method have the best visual effect and the highest total scores in quantitative analysis. The results of interframe detection strategy show average 2.65 × speed up as compared with the frame-wise quality improvement method.},
  archive      = {J_JRTIP},
  author       = {Ni, Dongdong and Jia, Zhenhong and Yang, Jie and Kasabov, Nikola},
  doi          = {10.1007/s11554-022-01248-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1181-1197},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fast sand-dust video quality improvement method based on adaptive dynamic guided filtering and interframe detection strategy},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view aggregation for real-time accurate object
detection of a moving camera. <em>JRTIP</em>, <em>19</em>(6), 1169–1179.
(<a href="https://doi.org/10.1007/s11554-022-01253-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection plays an important role on various mobile robot tasks. However, directly applying existing detectors on videos from a mobile robot will cause a sharp accuracy decline, because such videos introduce some extra difficulties on accurate detection. This paper proposes a viewpoint-based memory mechanism to handle detection performance deterioration and improve detection accuracy of the videos in real time. The mechanism positively organizes previous results from multiple viewpoints of target objects as prior knowledge to enhance detection accuracy for succeeding frames, and it is designed as an extension module of an existing image detector. In experiments, we collect testing dataset from an indoor mobile robot, and compare performance of several sole image detectors and the same detectors extended by the extension module. The result shows the mechanism module achieves 20.7% object localization rate margin in average at a cost of 18.1 ms, and the mechanism can give positive impact on various existing detectors. The result indicates the proposed method achieves good accuracy margin, has acceptable time cost, and gets a degree of universal applicability.},
  archive      = {J_JRTIP},
  author       = {Hu, Jiyuan and Wang, Tao and Zhu, Shiqiang},
  doi          = {10.1007/s11554-022-01253-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1169-1179},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-view aggregation for real-time accurate object detection of a moving camera},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time traffic sign detection based on multiscale
attention and spatial information aggregator. <em>JRTIP</em>,
<em>19</em>(6), 1155–1167. (<a
href="https://doi.org/10.1007/s11554-022-01252-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection, as an important part of intelligent driving, can effectively guide drivers to regulate driving and reduce the occurrence of traffic accidents. Currently, the deep learning-based detection methods have achieved very good performance. However, existing network models do not adequately consider the importance of lower-layer features for traffic sign detection. The lack of information on the lower-layer features is a major obstacle to the accurate detection of traffic signs. To solve the above problems, we propose a novel and efficient traffic sign detection method. First, we remove a prediction branch of the YOLOv3 network model to reduce the redundancy of the network model parameters and improve the real-time performance of detection. After that, we propose a multiscale attention feature module. This module fuses the feature information from different layers and refines the features to enhance the Feature Pyramid Network. In addition, we introduce a spatial information aggregator. This enables the spatial information of the lower-layer feature maps to be fused into the higher-layer feature maps. The robustness of our proposed method is further demonstrated by experiments on GTSDB, CCTSDB2021 and TT100k datasets. Specifically, the average execution time on CCTSDB2021 demonstrates the excellent real-time performance of our method. The experimental results show that the method has better accuracy than the original YOLOv3 and YOLOv5 network models.},
  archive      = {J_JRTIP},
  author       = {Zhang, Jianming and Ye, Zi and Jin, Xiaokang and Wang, Jin and Zhang, Jin},
  doi          = {10.1007/s11554-022-01252-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1155-1167},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time traffic sign detection based on multiscale attention and spatial information aggregator},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A practical super-resolution method for multi-degradation
remote sensing images with deep convolutional neural networks.
<em>JRTIP</em>, <em>19</em>(6), 1139–1154. (<a
href="https://doi.org/10.1007/s11554-022-01245-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have proved that convolutional neural networks (CNNs) have great potential for image super-resolution (SR) tasks. However, most existing methods rely on paired high-resolution (HR) and low-resolution (LR) images to train the CNN, where the LR images are routinely synthesized by applying predefined degradation operations (e.g., bicubic). Because the degradation process of LR images is usually unknown and more complex than those predefined, these methods suffer a significant performance decrease when applied to real-world SR problems. In addition, a deeper and wider network structure enables superior performance while increasing the network parameters and inference time, making it difficult to process real-time data. Inspired by the above motivations, we present an efficient two-step SR method for multi-degradation remote sensing images. Specifically, we first present a novel kernel estimation framework based on generative adversarial networks that can accurately extract the latent blur kernel from the input LR image without any image priors. We then train an efficient SR deep neural network with paired HR and corresponding LR images degraded with the generated kernels. To better balance network parameters and network performance, the densely connected attention mechanism and multi-scale feature extract blocks are introduced in the SR network by increasing the flow of feature information within the network. Extensive experiments indicate that the proposed method outperforms current methods with desired network parameters and complexity, making it feasible to enable real-time image processing.},
  archive      = {J_JRTIP},
  author       = {Zhao, Zhibo and Ren, Chao and Teng, Qizhi and He, Xiaohai},
  doi          = {10.1007/s11554-022-01245-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1139-1154},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A practical super-resolution method for multi-degradation remote sensing images with deep convolutional neural networks},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight target detection algorithm based on YOLOv4.
<em>JRTIP</em>, <em>19</em>(6), 1123–1137. (<a
href="https://doi.org/10.1007/s11554-022-01251-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem that the model parameters of YOLOv4 algorithm are large and difficult to deploy in edge computing devices, a lightweight target detection algorithm (Light-YOLOv4) is proposed based on YOLOv4 algorithm. The algorithm uses the GhostNet structure to replace the backbone feature extraction network in YOLOv4 algorithm, and introduces the depthwise separable convolution to replace the vanilla convolution, which greatly reduces the parameters of the original network model. Light-YOLOv4 also replaces the ReLU activation function in the deep structure of GhostNet with the improved lightweight activation function H-MetaACON, which improves the detection accuracy when the amount of model parameters and calculation are basically unchanged. Finally, the coordinate attention module is added to the effective feature layer and the PANet upsampling module, so that the model captures the cross-channel information while capturing the direction and position awareness information to further improve the detection accuracy. The experimental results show that the detection accuracy of the optimized model is improved by 0.89% and the size of the model is reduced to 17.48% compared to the original YOLOv4 model. The Light-YOLOv4 model can effectively reduce the inference calculation of the original model while maintaining high detection accuracy, and significantly improve the detection speed of the model on devices with insufficient computing power.},
  archive      = {J_JRTIP},
  author       = {Liu, Chuan and Wang, Xianchao and Wu, Qilin and Jiang, Jiabao},
  doi          = {10.1007/s11554-022-01251-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1123-1137},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight target detection algorithm based on YOLOv4},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-power hardware-efficient memory-based DCT processor.
<em>JRTIP</em>, <em>19</em>(6), 1105–1121. (<a
href="https://doi.org/10.1007/s11554-022-01243-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new discrete cosine transform (DCT) processor. The micro-rotation section of the architecture is based on a shared-resource improved coordinate rotation digital computer (CORDIC) unit, in an enhanced scalable DCT engine. To reduce the resources, and utilization area all micro-rotation operations have implemented as one united block in overlapped form. Using one processing element, the memory-based architecture has reduced the power consumption. Inputs and outputs of the processor are in-order which can be taken into account as an advantage for the proposed design. The processor has a low-complexity and distributed controller. Furthermore, due to the shared-resource implementation of CORDIC-II unit, by reduction of adding, shifting operations both in size, and number, the processor has high capabilities in short word lengths in comparison with state-of-the-art DCT processors. Compared to existing prominent DCT processors, the proposed processor achieves better results with limited hardware resources.},
  archive      = {J_JRTIP},
  author       = {Khalili Sadaghiani, AbdolVahab and Forouzandeh, Behjat},
  doi          = {10.1007/s11554-022-01243-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1105-1121},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-power hardware-efficient memory-based DCT processor},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAM: Focal attention module for lesion segmentation of
COVID-19 CT images. <em>JRTIP</em>, <em>19</em>(6), 1091–1104. (<a
href="https://doi.org/10.1007/s11554-022-01249-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus pneumonia (COVID-19) is the world’s most serious public health crisis, posing a serious threat to public health. In clinical practice, automatic segmentation of the lesion from computed tomography (CT) images using deep learning methods provides an promising tool for identifying and diagnosing COVID-19. To improve the accuracy of image segmentation, an attention mechanism is adopted to highlight important features. However, existing attention methods are of weak performance or negative impact to the accuracy of convolutional neural networks (CNNs) due to various reasons (e.g. low contrast of the boundary between the lesion and the surrounding, the image noise). To address this issue, we propose a novel focal attention module (FAM) for lesion segmentation of CT images. FAM contains a channel attention module and a spatial attention module. In the spatial attention module, it first generates rough spatial attention, a shape prior of the lesion region obtained from the CT image using median filtering and distance transformation. The rough spatial attention is then input into two 7 × 7 convolution layers for correction, achieving refined spatial attention on the lesion region. FAM is individually integrated with six state-of-the-art segmentation networks (e.g. UNet, DeepLabV3+, etc.), and then we validated these six combinations on the public dataset including COVID-19 CT images. The results show that FAM improve the Dice Similarity Coefficient (DSC) of CNNs by 2%, and reduced the number of false negatives (FN) and false positives (FP) up to 17.6%, which are significantly higher than that using other attention modules such as CBAM and SENet. Furthermore, FAM significantly improve the convergence speed of the model training and achieve better real-time performance. The codes are available at GitHub ( https://github.com/RobotvisionLab/FAM.git ).},
  archive      = {J_JRTIP},
  author       = {Wu, Xiaoxin and Zhang, Zhihao and Guo, Lingling and Chen, Hui and Luo, Qiaojie and Jin, Bei and Gu, Weiyan and Lu, Fangfang and Chen, Jingjing},
  doi          = {10.1007/s11554-022-01249-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1091-1104},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FAM: Focal attention module for lesion segmentation of COVID-19 CT images},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A high-performance two-dimensional transform architecture of
variable block sizes for the VVC standard. <em>JRTIP</em>,
<em>19</em>(6), 1081–1090. (<a
href="https://doi.org/10.1007/s11554-022-01250-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The versatile video coding standard H.266/VVC release has been accompanied with various new contributions to improve the coding efficiency beyond the high-efficiency video coding (HEVC), particularly in the transformation process. The adaptive multiple transform (AMT) is one of the new tools that was introduced in the transform module. It involves five transform types from the discrete cosine transform/discrete sine transform families with larger block sizes. The DCT-II has a fast computing algorithm, while the DST-VII relies on a complex matrix multiplication. This has led to an additional computational complexity. The approximation of the DST-VII can be used for the transform optimization. At the hardware level, this method can provide a gain in power consumption, logic resources use and speed. In this paper, a unifed two-dimensional transform architecture that enables exact and approximate DST-VII computation of sizes $$8\times 8, 8\times 16, 8\times 32, 16\times 8, 16\times 16, 16\times 32, 32\times 8, 32\times 16$$ and $$32\times 32$$ is proposed. The exact transform computation can be processed using either multipliers or the MCM algorithm, while the approximate transform computation is based on additions and bit-shifting operations. All the designs are implemented under the Arria 10 FPGA device. The synthesis results show that the proposed design implementing the approximate transform matrices is the most efficient method with only 4% of area consumption. It reduces the logic utilization by more than 65% compared to the multipliers-based exact transform design, while about 53% of hardware cost saving is obtained when compared to the MCM-based computation. Furthermore, the approximate-based 2D transform architecture can operate at 78 MHz allowing a real-time coding for 2K and 4K videos at 100 and 25 frames/s, respectively.},
  archive      = {J_JRTIP},
  author       = {Ben Jdidia, Sonda and Belghith, Fatma and Masmoudi, Nouri},
  doi          = {10.1007/s11554-022-01250-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1081-1090},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high-performance two-dimensional transform architecture of variable block sizes for the VVC standard},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational scatter correction in near real-time with a
fast monte carlo photon transport model for high-resolution flat-panel
CT. <em>JRTIP</em>, <em>19</em>(6), 1063–1079. (<a
href="https://doi.org/10.1007/s11554-022-01247-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computed tomography (CT), scattering causes server quality degradation of the reconstructed CT images by introducing streaks and cupping artifacts which reduce the detectability of low contrast objects. Monte Carlo (MC) simulation is considered the most accurate approach for scatter estimation. However, the existing MC estimators are computationally expensive, especially for high-resolution flat-panel CT. In this paper, we propose a fast and accurate MC photon transport model which describes the physics within the 1 keV to 1 MeV range using multiple controllable key parameters. Based on this model, scatter computation for a single projection can be completed within a range of a few seconds under well-defined model parameters. Smoothing and interpolation are performed on the estimated scatter to accelerate the scatter calculation without compromising accuracy too much compared to measured near scatter-free projection images. Combining the fast scatter estimation with the filtered backprojection (FBP), scatter correction is performed effectively in an iterative manner. To evaluate the proposed MC model, we have conducted extensive experiments on the simulated data and real-world high-resolution flat-panel CT. Compared to the state-of-the-art MC simulators, the proposed MC model achieved a 15 $$\times$$ acceleration on a single-GPU compared to the GPU implementation of the Penelope simulator (MCGPU) utilizing several acceleration techniques, and a 202 $$\times$$ speed-up on a multi-GPU system compared to the multi-threaded state-of-the-art EGSnrc MC simulator. Furthermore, it is shown that for high-resolution images, scatter correction with sufficient accuracy is accomplished within one to three iterations using a FBP and the proposed fast MC photon transport model.},
  archive      = {J_JRTIP},
  author       = {Alsaffar, Ammar and Kieß, Steffen and Sun, Kaicong and Simon, Sven},
  doi          = {10.1007/s11554-022-01247-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1063-1079},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Computational scatter correction in near real-time with a fast monte carlo photon transport model for high-resolution flat-panel CT},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new hardware architecture of lightweight and efficient
real-time video chaos-based encryption algorithm. <em>JRTIP</em>,
<em>19</em>(6), 1049–1062. (<a
href="https://doi.org/10.1007/s11554-022-01244-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel chaotic-based encryption scheme for securing real-time video data. The proposed encryption algorithm is based on the One-Time Pad (OTP) scheme and the unified Lorenz chaotic generator. The peculiarity of the latter is that it can change the chaotic system’s and its behaviour as well as its parameters. This provides the system with an important dynamic reconfiguration dimension, especially for real-time applications, in case the key is under attack. As a result, the attacker is obliged to perform these calculations again and again. The 3D unified chaotic generator can switch between three chaotic systems according to a control parameter. As a result, the cryptosystem will offer several advantages, namely a very large dimension of the secret key, low resource and energy consumption and low latency. An extensive security and differential analysis have been performed, demonstrating the high resistance of the proposed scheme to different attacks. The proposed encryption algorithm is validated for real-time video through an experimental implementation of FPGA interfaced with a camera. Experimental results indicate that the proposed hardware architecture is very promising since it provides good performance and can be useful in many embedded applications.},
  archive      = {J_JRTIP},
  author       = {Hadjadj, Mahieddine Anouar and Sadoudi, Said and Azzaz, Mohamed Salah and Bendecheche, Hichem and Kaibou, Redouane},
  doi          = {10.1007/s11554-022-01244-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1049-1062},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new hardware architecture of lightweight and efficient real-time video chaos-based encryption algorithm},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing large 2D convolutions on GPU efficiently with the
im2tensor algorithm. <em>JRTIP</em>, <em>19</em>(6), 1035–1047. (<a
href="https://doi.org/10.1007/s11554-022-01240-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attaining the best possible throughput when computing convolutions is a challenge for signal and image processing systems, be they HPC (High-Performance Computing) machines or embedded real-time targets. This importance is highlighted by the numerous methods and implementations available, often optimized for particular settings: small batched kernels or very large kernels, for example. In the meantime, GPUs (Graphics Processing Units) have become a first-class architecture for real-time and embedded processing. The power offered by those chips stems from their parallel nature, and this advantage has been exploited for convolutions in several libraries. Even more recently, the introduction of tensor cores on NVIDIA GPUs has opened up new limits in terms of attainable FLOPS (Floating-Point Operations per Second). For reaching that performance, GPU applications must use GEMMs (GEneral Matrix Multiplications), that tensor cores accelerate. We then developed an efficient GEMM-based 2D convolution algorithm in a general setting. On relatively large kernels (30–50-pixel wide), im2tensor is, to the best of our knowledge, the fastest method for computing 2D convolutions. We provide detailed performance analysis for different scenarios: small (1024 $$\times$$ 1024) and large (4096 $$\times$$ 4096) images, with convolutions kernels of sizes ranging 1 to 60-pixel wide, on two GPU cards: Jetson AGX Xavier (embedded) and Titan V (server-class). Moreover, the accuracy of im2tensor surpasses non-GEMM based methods, thanks to the larger-precision registers used by tensor cores for intermediate representations.},
  archive      = {J_JRTIP},
  author       = {Seznec, Mickaël and Gac, Nicolas and Orieux, François and Sashala Naik, Alvin},
  doi          = {10.1007/s11554-022-01240-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1035-1047},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Computing large 2D convolutions on GPU efficiently with the im2tensor algorithm},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DDH-YOLOv5: Improved YOLOv5 based on double IoU-aware
decoupled head for object detection. <em>JRTIP</em>, <em>19</em>(6),
1023–1033. (<a
href="https://doi.org/10.1007/s11554-022-01241-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {YOLOv5 is a high-performance real-time object detector that plays an important role in one-stage detectors. However, there are two problems with the design of the YOLOv5 head. The common branch of classification task and regression task of the YOLOv5 head will hurt the training process, and the correlation between classification score and localization accuracy is low. We propose a Double IoU-aware Decoupled Head (DDH) and apply it to YOLOv5. The improved model is named DDH-YOLOv5, which substantially improves the localization accuracy of the model without significantly increasing FLOPS and parameters. Extensive experiments on dataset PASCAL VOC2007 show that DDH-YOLOv5 has good performance. Compared with YOLOv5, DDH-YOLOv5m and DDH-YOLOv5l proposed in this paper achieve 2.4 $$\%$$ and 1.3 $$\%$$ improvement in Average Precision (AP), respectively. Compared with Deformable DETR, which is known for its fast-converging, DDH-YOLOv5 completely outperforms Deformable DETR on COCO2017 Val with half of FLOPS and only a quarter of epochs.},
  archive      = {J_JRTIP},
  author       = {Wang, Hui and Jin, Yang and Ke, Hongchang and Zhang, Xinping},
  doi          = {10.1007/s11554-022-01241-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1023-1033},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DDH-YOLOv5: Improved YOLOv5 based on double IoU-aware decoupled head for object detection},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M-YOLO: An object detector based on global context
information for infrared images. <em>JRTIP</em>, <em>19</em>(6),
1009–1022. (<a
href="https://doi.org/10.1007/s11554-022-01242-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is an important task in computer vision. While visible (VS) images are adequate for detecting objects in most scenarios, infrared (IR) images can extend the capabilities of object detection to night-time or occluded objects. For IR images, we proposes an infrared object detector based on global context information. Combined with the lightweight network (MobileNetV2) to extract features, therefore the detector is named M-YOLO. Then, dedicated to enhancing the global information perception capability of the model, this paper proposes a global contextual information aggregation model. To preserve multi-scale information and enhance expressiveness of features, a top-down and bottom-up parallel feature fusion method is proposed. Only two detection heads are used to implement a lightweight model, which improves detection accuracy and speed. We use the self-built IR dataset (GIR) and the public IR dataset (FLIR) to verify the superiority of the model. Compared with YOLOv4 (78.1%), the average accuracy of M-YOLO (83.4%) is improved by 5.3% on the FLIR dataset. The detection time (4.33 ms) is less, with a detection speed of 30.6 FPS. On the GIR dataset, the detection accuracy (76.1%) is 6.4% higher than that of YOLOv4 (69.7%), and the detection time (6.84 ms) is lower. Our method improves the performance of IR object detection. The method is able to detect IR ground targets in complex environments, and the detection speed meets the real-time requirements.},
  archive      = {J_JRTIP},
  author       = {Hou, Zhiqiang and Sun, Ying and Guo, Hao and Li, Juanjuan and Ma, Sugang and Fan, Jiulun},
  doi          = {10.1007/s11554-022-01242-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1009-1022},
  shortjournal = {J. Real-Time Image Process.},
  title        = {M-YOLO: An object detector based on global context information for infrared images},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Journal of real-time image processing: Sixth issue of
volume 19. <em>JRTIP</em>, <em>19</em>(6), 1007. (<a
href="https://doi.org/10.1007/s11554-022-01246-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-022-01246-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1007},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Sixth issue of volume 19},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time monocular depth estimation for low-power embedded
systems using deep learning. <em>JRTIP</em>, <em>19</em>(5), 997–1006.
(<a href="https://doi.org/10.1007/s11554-022-01237-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted for fast and accurate single-image depth estimation on embedded devices, we propose an improved lightweight PyD-Net model for monocular depth estimation. We use feature pyramids and a depth decoder to extract image features. Our proposed model addresses the problem of checkerboard artifacts in up-sampling and reduces the number of parameters. We optimize the model using an inference engine and deploy it on Xilinx Zynq UltraScale + MPSoCs ZCU102. Experimental results on the KITTI dataset show that the model runs on the FPGA side with a power consumption of only 4.53 W. The inference is 20 times faster than the CPU (AMD Ryzen 5 4600H 3 GHz) and 80 times faster than the Raspberry Pi 3 (ARMv8 processor Cortex-A53 1.2 GHz) with almost the same accuracy as on the PC side. The model proposed in this paper also holds well in real-world scenarios.},
  archive      = {J_JRTIP},
  author       = {Liu, Shuyao and Zhao, Shuo and Zhang, Pu and Cheng, Jingjing},
  doi          = {10.1007/s11554-022-01237-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {997-1006},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time monocular depth estimation for low-power embedded systems using deep learning},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time object detection method of melon leaf diseases
under complex background in greenhouse. <em>JRTIP</em>, <em>19</em>(5),
985–995. (<a href="https://doi.org/10.1007/s11554-022-01239-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early disease detection in greenhouses is an important part of integrated disease management in modern agriculture. A real-time object detection method of melon leaf disease, Pruned-YOLO v5s+Shuffle (PYSS) is proposed in this research. First, for enhancing the feature extraction capability, the backbone of the YOLO v5s is reconstructed with ShuffleNet v2 Inverted Residual block. Then, to further downsize the model, the channel pruning method is used to prune and fine-tune the sparsely trained model. Finally, Pruned-YOLO v5s+Shuffle model is deployed to Jetson Nano, and the real-time performance is confirmed in melon greenhouses. The experimental results show that the proposed model has 93.2% and 98.2% mAP@0.5 for melon (Cucumis melon. L) powdery mildew and melon real leaves, respectively. Compared with YOLO v5s, the performance of our proposed model is improved 6.2% and 6.4% in the term of mAP@0.5 and precision, respectively. The model size and inference time are reduced 85% and 7.5%. In addition, the PYSS demonstrates the higher detection precision and faster inference speed in the comparison of YOLO v3, Faster R-CNN, RetinaNet, Cascade R-CNN, YOLO v4 and YOLO v5s. Being deployed to Jetson Nano, the detection results are displayed on the monitor in real time: mAP@0.5 is 96.7%, the model size is 1.1 MB, and the inference time is 13.8 ms.},
  archive      = {J_JRTIP},
  author       = {Xu, Yanlei and Chen, Qingyuan and Kong, Shuolin and Xing, Lu and Wang, Qi and Cong, Xue and Zhou, Yang},
  doi          = {10.1007/s11554-022-01239-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {985-995},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time object detection method of melon leaf diseases under complex background in greenhouse},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality-power configurable flexible coding order hardware
design for real-time 3D-HEVC intra-frame prediction. <em>JRTIP</em>,
<em>19</em>(5), 969–984. (<a
href="https://doi.org/10.1007/s11554-022-01238-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging of 3D video-capable embedded mobile devices is expected due to the popularization of multimedia services and the demand for novel immersive video technologies. Such devices require efficient hardware-friendly heuristics to deal with strict processing requirements and limited energy supply. To contribute to these requirements, this work presents a complete 3D-HEVC intra-frame prediction hardware design that supports a flexible coding order between texture and depth channels. The developed hardware employs hardware-friendly constraints and novel heuristics to explore inter-channel redundancies and to reduce the computational effort through the novel inter-channel directional structure detector heuristic. The designed 3D-HEVC intra-frame prediction system dissipates 384.6 mW while processing three HD 1080p views (texture + depth) at 30 frames per second in real-time. To the best of our knowledge, this is the first work to propose a complete 3D-HEVC intra-frame prediction system with support to flexible coding order. In addition, this is the only hardware design to process luminance and chrominance texture channels and depth channel.},
  archive      = {J_JRTIP},
  author       = {Perleberg, Murilo R. and Afonso, Vladimir and Borges, Vinicius A. and Zatt, Bruno and Agostini, Luciano V. and Porto, Marcelo},
  doi          = {10.1007/s11554-022-01238-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {969-984},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Quality-power configurable flexible coding order hardware design for real-time 3D-HEVC intra-frame prediction},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforced attention method for real-time traffic line
detection. <em>JRTIP</em>, <em>19</em>(5), 957–968. (<a
href="https://doi.org/10.1007/s11554-022-01236-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of traffic line detection is a fundamental yet challenging problem in computer vision. Previous traffic line segmentation models either tend to increase the network depth to enhance the representation ability to achieve high accuracy, or tend to reduce the number of model layers or hyper-parameters to achieve real-time efficiency, but how to trade off high accuracy and low inference time is still challenging. In this paper, we propose a reinforced attention method (RAM) to increase the saliency of traffic lines in feature abstraction, using RAM to optimize the model can achieve better traffic line detection accuracy without increasing inference time. In the RAM processing, we define the line to context contrast weight (LCCW) to represent the traffic line saliency in the feature map, which can be calculated by the ratio of the traffic line energy to the total feature energy. After LCCW calculation, we add a RAM loss item to the total loss in backward processing, and then retrain the model to obtain the new parameter weights. To validate RAM on real-time traffic line detection models, we applied RAM to seven popular real-time models and evaluate them on two popular traffic line detection benchmarks (CULane and TuSimple). Experimental results show that RAM can increase line detection accuracy by 1–2% on the CULane and TuSimple benchmarks, and the ERFNet and CGNet almost reach state-of-the-art performance after the models are optimized by RAM. The results also show that RAM can be applied to the optimization of almost all encoder–decoder-based models, and the optimized models are more robust to occlusion and extreme lighting conditions.},
  archive      = {J_JRTIP},
  author       = {Liu, Yian and Xu, Ping and Zhu, Lei and Yan, Ming and Xue, Lingyun},
  doi          = {10.1007/s11554-022-01236-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {957-968},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reinforced attention method for real-time traffic line detection},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAM: A balanced attention mechanism to optimize single image
super-resolution. <em>JRTIP</em>, <em>19</em>(5), 941–955. (<a
href="https://doi.org/10.1007/s11554-022-01235-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering texture information from the aliasing regions has always been a major challenge for single image super-resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose an efficient Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large-scale feature maps, while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 state-of-the-art SISR networks. The results demonstrate that BAM can efficiently improve the networks&#39; performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Information multi-distillation network (IMDN), a representative lightweight SISR network with attention, when the input image size is 200 × 200, the FPS of proposed IMDN-BAM precedes IMDN {8.1%, 8.7%, 8.8%} under the three SR magnifications of × 2, × 3, × 4, respectively. Densely residual Laplacian network (DRLN), a representative heavyweight SISR network with attention, when the scale is 60 × 60, the proposed DRLN-BAM is {11.0%, 8.8%, 10.1%} faster than DRLN under × 2, × 3, × 4. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.},
  archive      = {J_JRTIP},
  author       = {Wang, Fanyi and Hu, Haotian and Shen, Cheng and Feng, Tianpeng and Guo, Yandong},
  doi          = {10.1007/s11554-022-01235-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {941-955},
  shortjournal = {J. Real-Time Image Process.},
  title        = {BAM: A balanced attention mechanism to optimize single image super-resolution},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware acceleration for object detection using YOLOv4
algorithm on xilinx zynq platform. <em>JRTIP</em>, <em>19</em>(5),
931–940. (<a href="https://doi.org/10.1007/s11554-022-01234-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the technological improvement in artificial intelligence, particularly deep learning is providing effective outcomes along with hardware platforms such as field-programmable gate arrays (FPGAs) and graphics processing units in various domains. FPGAs with their reconfigurable architectures provide flexibility, better performance and high levels of parallelism. Object detection is one of the prominent areas of research in the fields of computer vision and image processing applications. You Only Look Once (YOLO) is a state-of-the art object detection algorithm which is fast and accurate. However, many applications require accuracy and rapid processing for better results. For such conditions, these algorithms can be implemented on hardware accelerators. This work proposes the implementation of YOLOv4 algorithm on Xilinx® Zynq-7000 System on a chip and is suitable for real-time object detection. The proposed work shows better resource utilization of about 23.2 k (43.6%) of Look-up tables, 45.8 k (43.04%) of Flip-flops, 115 (82.17%) BRAMs and 174 (79%) DSPs achieving at 100 MHz frequency which is more efficient on comparing with other simulation results.},
  archive      = {J_JRTIP},
  author       = {Babu, Praveenkumar and Parthasarathy, Eswaran},
  doi          = {10.1007/s11554-022-01234-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {931-940},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware acceleration for object detection using YOLOv4 algorithm on xilinx zynq platform},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time digital twins end-to-end multi-branch object
detection with feature level selection for healthcare. <em>JRTIP</em>,
<em>19</em>(5), 921–930. (<a
href="https://doi.org/10.1007/s11554-022-01233-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is one of the most significant tasks in recent computer vision and healthcare study, which also has been applied in many areas. Although some detection frameworks show good performance for some specific datasets, the ambiguity in feature levels of anchor-free detectors still limits the performance of both fully-supervised and cross-dataset settings. Hence, a digital twins end-to-end multi-branch object detection framework with feature level selection is presented in this work. First, a five-level feature pyramid is adopted with a set of detection heads to construct an anchor-free detection backbone. Then, a learning-based selection strategy is presented to help obtain better feature level selection performance. Experimental results on general object detection datasets show that our framework can achieve 39.2 average precision (AP) on the COCO dataset and 10.2 miss rate (MR) on the CityPersons dataset. Furthermore, experimental results on cross-dataset settings, including Cityscapes, Caltech, SIM 10k, KITTI datasets, have also proved the good generalization ability of our framework. Through the optimized models in digital twins, it is also been applied in a pneumonia detection dataset with 49.3 AP. In addition, a large number of comparisons with state-of-the-art works also verify the detection performance and real-time efficiency of the proposed framework.},
  archive      = {J_JRTIP},
  author       = {Li, Xiaoqin},
  doi          = {10.1007/s11554-022-01233-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {921-930},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time digital twins end-to-end multi-branch object detection with feature level selection for healthcare},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater trash detection algorithm based on improved
YOLOv5s. <em>JRTIP</em>, <em>19</em>(5), 911–920. (<a
href="https://doi.org/10.1007/s11554-022-01232-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of insufficient storage space and limited computing ability of underwater mobile devices, an underwater garbage detection algorithm based on an improved YOLOv5s algorithm is proposed. The algorithm replaces the feature extraction module of the YOLOv5s network with the lightweight network MobileNetv3; the Convolutional Block Attention Module (CBAM) is embedded in the network to improve the feature extraction ability of the network in two dimensions of space and channel. At the same time, the improved network is pruned to reduce the redundant parameters and further compress the model. The experimental results show that the detection accuracy of the approach can reach 97.5% based on one-ninth of the parameters of YOLOv5s, and the real-time detection speed on the CPU is 2.5 times that of YOLOv5s.},
  archive      = {J_JRTIP},
  author       = {Wu, ChunMing and Sun, YiQian and Wang, TiaoJun and Liu, YaLi},
  doi          = {10.1007/s11554-022-01232-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {911-920},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Underwater trash detection algorithm based on improved YOLOv5s},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A GPU-accelerated light-field super-resolution framework
based on mixed noise model and weighted regularization. <em>JRTIP</em>,
<em>19</em>(5), 893–910. (<a
href="https://doi.org/10.1007/s11554-022-01230-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light-field (LF) super-resolution (SR) plays an essential role in alleviating the current technology challenge in the acquisition of a 4D LF, which assembles both high-density angular and spatial information. Due to the algorithm complexity and data-intensive property of LF images, LFSR demands a significant computational effort and results in a long CPU processing time. This paper presents a GPU-accelerated computational framework for reconstructing high-resolution (HR) LF images under a mixed Gaussian-Impulse noise condition. The main focus is on developing a high-performance approach considering processing speed and reconstruction quality. From a statistical perspective, we derive a joint $$\ell ^1$$ - $$\ell ^2$$ data fidelity term for penalizing the HR reconstruction error taking into account the mixed noise situation. For regularization, we employ the weighted non-local total variation approach, which allows us to effectively realize LF image prior through a proper weighting scheme. We show that the alternating direction method of the multipliers algorithm (ADMM) can be used to simplify the computation complexity and results in a high-performance parallel computation on the GPU Platform. An extensive experiment is conducted on both synthetic 4D LF dataset and natural image dataset to validate the proposed SR model’s robustness and evaluate the accelerated optimizer’s performance. The experimental results show that our approach achieves better reconstruction quality under severe mixed-noise conditions as compared to the state-of-the-art approaches. In addition, the proposed approach overcomes the limitation of the previous work in handling large-scale SR tasks. While fitting within a single off-the-shelf GPU, the proposed accelerator provides an average speedup of 2.46 $${\times }$$ and 1.57 $${\times }$$ for $${\times }2$$ and $${\times }3$$ SR tasks, respectively. In addition, a speedup of $$77{\times }$$ is achieved as compared to CPU execution.},
  archive      = {J_JRTIP},
  author       = {Tran, Trung-Hieu and Sun, Kaicong and Simon, Sven},
  doi          = {10.1007/s11554-022-01230-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {893-910},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A GPU-accelerated light-field super-resolution framework based on mixed noise model and weighted regularization},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised real-time evaluation of optical coherence
tomography (OCT) images of solid oral dosage forms. <em>JRTIP</em>,
<em>19</em>(5), 881–892. (<a
href="https://doi.org/10.1007/s11554-022-01229-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of images, which is now feasible through an increase in available computing power, has become an important challenge in many fields. A key technology for obtaining such images is optical coherence tomography (OCT), which is already widely applied in ophthalmology and more recently in the pharmaceutical industry, as a method for real-time monitoring of solid oral dosage form coating processes. Accurately detecting the boundaries of objects in OCT images is required for a meaningful automatic evaluation. During in-line monitoring, the evaluation time for each image is a crucial factor to enable the real-time analysis of large amounts of data. The segmentation of images has previously been achieved via machine learning methods, which generally require a large number of training examples. This work aims to overcome this limitation by employing unsupervised machine learning for the segmentation of OCT images of coated pharmaceutical tablets. An adapted clustering method was specifically developed to achieve the fast real-time detection of the coating layer’s boundaries in OCT-generated images. A newly developed parallel implementation of DBSCAN, that is well suited for image evaluation, makes it possible to use this novel method for real-time process analytical technology (PAT) applications. This approach has been shown to be significantly faster than so far established methods for segmenting similar OCT images. Furthermore, the image-specific parallelized DBSCAN algorithm has been shown to be around three times faster than other parallel implementations.},
  archive      = {J_JRTIP},
  author       = {Fink, Elisabeth and Clarke, Phillip and Spoerk, Martin and Khinast, Johannes},
  doi          = {10.1007/s11554-022-01229-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {881-892},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Unsupervised real-time evaluation of optical coherence tomography (OCT) images of solid oral dosage forms},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time SC2S-based open-set recognition in remote
sensing imagery. <em>JRTIP</em>, <em>19</em>(5), 867–880. (<a
href="https://doi.org/10.1007/s11554-022-01226-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy and computational time are two crucial parameters influencing the efficacy of classification algorithms for remote sensing applications. Machine learning algorithms are known for achieving notable success for several classification problems in various domains, including remote sensing. However, they are well-recognized and considered accurate and efficient for closed-set recognition (CSR) but may provide suboptimal and erroneous results for open-set recognition (OSR) tasks. Many practical image-driven and computer vision applications have open-set and dynamic scenarios with unknown data where classification algorithms have not yet achieved significant prediction performance. This paper presents a group of class-aware (CA) classification algorithms based on a supervised cascaded classifier system (SC2S), called CA-SC2S, which is accurate for OSR and CSR tasks. We evaluate the prediction accuracy of the proposed methods against the state-of-the-art methods in a multiclass setting using multiple image classification scenarios of OSR and CSR. The test case scenarios use six multispectral and hyperspectral datasets from different sensing platforms. And to assess the computational performance of the methods, we designed various field-programmable gate array (FPGA) architectures of the proposed methods. We evaluated their real-time performance on a low-cost, low-power Artix-7 35 T FPGA.},
  archive      = {J_JRTIP},
  author       = {Gyaneshwar, Dubacharla and Nidamanuri, Rama Rao},
  doi          = {10.1007/s11554-022-01226-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {867-880},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time SC2S-based open-set recognition in remote sensing imagery},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-granularity network for fine-grained image
classification. <em>JRTIP</em>, <em>19</em>(5), 853–866. (<a
href="https://doi.org/10.1007/s11554-022-01228-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is widely used to identify different sub-categories of ships, dogs, flowers, and so on, and aims to help the ordinary people distinguish sub-categories with only slight differences. It mainly faces the challenges of small inter-class differences and large intra-class variations. The current effective methods adopt multi-scale or multi-granularity feature to find the subtle difference. However, these methods pay their attentions to the accuracy while neglecting the computational cost in practice. Therefore, in this paper, an improved efficient Multi-granularity Learning method with Only Forward Once (MLOFO) is proposed. It reduces the forward and back propagation in training from several times to once, and decreases the computational cost several times. And more, an intra-class metric loss, named prototype metric (PM) loss, is proposed to supervise learning the effective features for classification in a multi-granularity network (MGN) framework. The effectiveness of the proposed method is verified on four fine-grained classification datasets (CUB-200-2011, Stanford Cars, FGVC-Aircraft, and AircraftCarrier). Experimental results demonstrate that our method achieves state-of-the-art accuracies, substantially improving FGVC tasks. Furthermore, we discuss that the new PM loss can compress the distribution of the intra-class features as label smoothing to achieve better generalization ability. Our method is helpful to promote the training efficiency of the MGN model and improve the accuracy of fine-grained classification to a certain extent.},
  archive      = {J_JRTIP},
  author       = {Wang, Jiabao and Li, Yang and Li, Hang and Zhao, Xun and Zhang, Rui and Miao, Zhuang},
  doi          = {10.1007/s11554-022-01228-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {853-866},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient multi-granularity network for fine-grained image classification},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Yolov3-pruning(transfer): Real-time object detection
algorithm based on transfer learning. <em>JRTIP</em>, <em>19</em>(4),
839–852. (<a href="https://doi.org/10.1007/s11554-022-01227-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, object detection algorithms have achieved great success in the field of machine vision. To pursue the detection accuracy of the model, the scale of the network is constantly increasing, which leads to the continuous increase in computational cost and a large requirement for memory. The larger network scale allows their execution to take a longer time, facing the balance between the detection accuracy and the speed of execution. Therefore, the developed algorithm is not suitable for real-time applications. To improve the detection performance of small targets, we propose a new method, the real-time object detection algorithm based on transfer learning. Based on the baseline Yolov3 model, pruning is done to reduce the scale of the model, and then migration learning is used to ensure the detection accuracy of the model. The object detection method using transfer learning achieves a good balance between detection accuracy and inference speed and is more conducive to the real-time processing of images. Through the evaluation of the dataset voc2007 + 2012, the experimental results show that the parameters of the Yolov3-Pruning(transfer): model are reduced by 3X compared with the baseline Yolov3 model, and the detection accuracy is improved, realizes real-time processing, and improves the detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Li, Xiaoning and Wang, Zhengzhong and Geng, Shichao and Wang, Lin and Zhang, Huaxiang and Liu, Li and Li, Donghua},
  doi          = {10.1007/s11554-022-01227-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {839-852},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Yolov3-pruning(transfer): Real-time object detection algorithm based on transfer learning},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on license plate location and recognition in
complex environment. <em>JRTIP</em>, <em>19</em>(4), 823–837. (<a
href="https://doi.org/10.1007/s11554-022-01225-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of license plate location difficulty and low character recognition accuracy in complex environments, such as a small number of license plate samples, illumination transformation, changeable weather and motion blur, this paper proposes an end-to-end license plate recognition method to improve the location and recognition accuracy in complex environments. First, the cyclic generative adversarial network is used to synthesize the approximate real license plate image to enrich the training set and solve the problem of data imbalance to facilitate subsequent model training. Second, a MF-RepUnet license plate location method is proposed, which integrates the improved VGG structure and feature pyramid into the U-Net model to improve the feature extraction capability of the network, and effectively solve the problem of missing detection of inclined license plate and small-scale license plate. Finally, the convolutional recurrent neural network is improved to accurately predict the feature sequence through the way of attention mechanism weighting, which solves the problem of blurred semantic structure sequence features caused by image degradation and further improves the accuracy of license plate character recognition. Experiments show that the proposed method can effectively improve the accuracy and efficiency of license plate location and character recognition, and can be applied to license plate recognition in various complex environments.},
  archive      = {J_JRTIP},
  author       = {Yu, Hao and Wang, Xingqi and Shao, Yanli and Qin, Feiwei and Chen, Bin and Gong, Senlin},
  doi          = {10.1007/s11554-022-01225-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {823-837},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on license plate location and recognition in complex environment},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Reduced complexity and optimized face recognition approach
based on facial symmetry. <em>JRTIP</em>, <em>19</em>(4), 809–822. (<a
href="https://doi.org/10.1007/s11554-022-01224-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of face images is still a challenging and open research problem. A number of recent algorithms have shown that there is a vast scope in improving recognition accuracy by utilizing facial symmetry for face recognition task. The lower computational complexity and faster processing times make this method well suited for real-time applications. In this paper, we have used only one half of the face image for recognition task against various facial challenges. Keeping in view all the previous related studies that are limited in their scope, an unbiased comparison is presented between full face images and half face images by applying four subspace-based algorithms with four different distance metrics. Experiments are conducted on the two most challenging face databases. The FERET is a benchmark database, which closely simulates real-life scenarios, and LFW which is developed for the problem of unconstrained face recognition.},
  archive      = {J_JRTIP},
  author       = {Ahmed, Waqas and Bajwa, Usama Ijaz and Anwar, Muhammad Waqas and Sajid, Muhammad},
  doi          = {10.1007/s11554-022-01224-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {809-822},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reduced complexity and optimized face recognition approach based on facial symmetry},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BLINC: Lightweight bimodal learning for low-complexity VVC
intra-coding. <em>JRTIP</em>, <em>19</em>(4), 791–807. (<a
href="https://doi.org/10.1007/s11554-022-01223-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest video coding standard, versatile video coding (VVC), achieves almost twice coding efficiency compared to its predecessor, the high efficiency video coding (HEVC). However, achieving this efficiency (for intra coding) requires 31 × computational complexity compared to HEVC, which makes it challenging for low power and real-time applications. This paper, proposes a novel machine learning approach that jointly and separately employs two modalities of features, to simplify the intra coding decision. To do so, first a set of features are extracted that use the existing DCT core of VVC, to assess the texture characteristics, and forms the first modality of data. This produces high-quality features with almost no extra computational overhead. The distribution of intra modes at the neighboring blocks is also used to form the second modality of data, which provides statistical information about the frame, unlike the first modality. Second, a two-step feature reduction method is designed that reduces the size of feature set, such that a lightweight model with a limited number of parameters can be used to learn the intra mode decision task. Third, three separate training strategies are proposed (1) an offline training strategy using the first (single) modality of data, (2) an online training strategy that uses the second (single) modality, and (3) a mixed online–offline strategy that uses bimodal learning. Finally, a low-complexity encoding algorithms is proposed based on the proposed learning strategies. Extensive experimental results show that the proposed methods can reduce up to 24% of encoding time, with a negligible loss of coding efficiency. Moreover, it is demonstrated how a bimodal learning strategy can boost the performance of learning. Lastly, the proposed method has a very low computational overhead (0.2%), and uses existing components of a VVC encoder, which makes it much more practical compared to competing solutions.},
  archive      = {J_JRTIP},
  author       = {Pakdaman, Farhad and Adelimanesh, Mohammad Ali and Hashemi, Mahmoud Reza},
  doi          = {10.1007/s11554-022-01223-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {791-807},
  shortjournal = {J. Real-Time Image Process.},
  title        = {BLINC: Lightweight bimodal learning for low-complexity VVC intra-coding},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel multi-wing chaotic system with FPGA implementation
and application in image encryption. <em>JRTIP</em>, <em>19</em>(4),
775–790. (<a href="https://doi.org/10.1007/s11554-022-01220-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a two-wing chaotic system is transformed into a four-wing chaotic system and an eight-wing chaotic system using fractal processing and the dynamic characteristics of new multi-wing chaotic systems are analyzed. The encryption of the image is accomplished by combining the eight-wing chaotic system and the improved AES algorithm. The number of AES encryption rounds is reduced to make it more suitable for image encryption. To further improve the encryption efficiency, the chaotic system circuit and AES parallel computation are designed and implemented on FPGA. Finally, the high performance of the chaotic system is demonstrated by the satisfactory encryption effect. This methodology provides a promising direction for the study of real-time image encryption.},
  archive      = {J_JRTIP},
  author       = {Cai, Hong and Sun, Jing-yu and Gao, Zi-bo and Zhang, Hao},
  doi          = {10.1007/s11554-022-01220-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {775-790},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel multi-wing chaotic system with FPGA implementation and application in image encryption},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing distance transform computation by leveraging the
discrete nature of images. <em>JRTIP</em>, <em>19</em>(4), 763–773. (<a
href="https://doi.org/10.1007/s11554-022-01221-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a major reformulation of a widely used solution for computing the exact Euclidean distance transform of n-dimensional discrete binary shapes. Initially proposed by Hirata, the original algorithm is linear in time, separable, and easy to implement. Furthermore, it accounts for the fastest existing solutions, leading to its widespread use in the state of the art, especially in real-time applications. In particular, we focus on the second step of this algorithm, where the lower envelope of a set of parabolas has to be computed. By leveraging the discrete nature of images, we show that some of those parabolas can be merged into line segments. It reduces the computational cost of the algorithm by about 20% in most practical cases, while maintaining its exactness. To evaluate the proposed improvement on different cases, two state-of-the art benchmarks are implemented and discussed.},
  archive      = {J_JRTIP},
  author       = {Fuseiller, Guillaume and Marie, Romain and Mourioux, Gilles and Duno, Erick and Labbani-Igbida, Ouiddad},
  doi          = {10.1007/s11554-022-01221-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {763-773},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhancing distance transform computation by leveraging the discrete nature of images},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shelf control in retail stores via ultra-low and low power
microcontrollers. <em>JRTIP</em>, <em>19</em>(4), 751–762. (<a
href="https://doi.org/10.1007/s11554-022-01222-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart retail stores started to take place in our lives. Several computer vision and sensor-based systems are working together to achieve such a complex and automated operation. Besides, the retail sector has several open and challenging problems which can be solved by embedded computer vision systems. One important problem to be tackled is shelf control or stock out detection. Here, shelves in a store should be controlled regularly such that no item is missing in the shelf. In this study, we propose an embedded computer vision system to solve this problem. To do so, we frame the shelf control operation as change detection. Due to the constraints posed by the retail sector, we formed the system by an ultra-low or low power microcontroller with an embedded camera attached to it. We provided all the implementation details of the system both from software and hardware perspectives. We also tested the proposed shelf control system from different perspectives. Hence, the reader can form such a system for retail sector or for a broad class of change detection problems in which stand-alone embedded system usage is mandatory.},
  archive      = {J_JRTIP},
  author       = {Yücel, M. Erkin and Ünsalan, Cem},
  doi          = {10.1007/s11554-022-01222-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {751-762},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Shelf control in retail stores via ultra-low and low power microcontrollers},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math display">$$\hbox {ABDF}^{2}$$</span> -net:
An adaptive bi-directional features fusion network for real-time
detection of threat object. <em>JRTIP</em>, <em>19</em>(4), 739–749. (<a
href="https://doi.org/10.1007/s11554-022-01219-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve automatic detection of threat objects for X-ray baggage screening, we propose an adaptive bi-directional features fusion network ( $$\hbox {ABDF}^{2}$$ -Net) to detect threat objects on X-ray images. In $$\hbox {ABDF}^{2}$$ -Net, an adaptive bi-directional feature fusion module ( $$\hbox {ABDF}^{2}\hbox {M}$$ ) is introduced to fuse the multi-scale features from two directions, and the adaptive function is used to control the features passing rate. Besides, an atrous convolutional pyramid pooling (ACPP) is employed to capture global contextual information, which can provide global semantic guidance for multi-scale features. Finally, the fused multi-scale features are used to predict the final detection results through prediction modules. Experiments on the GDXray database demonstrate the effectiveness and superiority of our proposed method against the other four object detection methods.},
  archive      = {J_JRTIP},
  author       = {Wei, Yiru and Zhu, Zhiliang and Yu, Hai and Zhang, Wei},
  doi          = {10.1007/s11554-022-01219-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {739-749},
  shortjournal = {J. Real-Time Image Process.},
  title        = {$$\hbox {ABDF}^{2}$$ -net: An adaptive bi-directional features fusion network for real-time detection of threat object},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time double JPEG forensics for mobile devices.
<em>JRTIP</em>, <em>19</em>(4), 727–737. (<a
href="https://doi.org/10.1007/s11554-022-01218-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double JPEG compression is the most common process to hide image manipulation. Therefore, it becomes necessary to detect the double JPEG compression. Several approaches have been developed for double JPEG compression detection with high accuracy, but they do not provide a unified solution in terms of real-time applicability. To address this issue, a new 953-dimensional unified detector is proposed. The unified detector is a combination of 44 spatial domain features and 909 frequency domain features. Extensive experiments are performed on UCID and RAISE databases to evaluate the robustness of the proposed detector. In addition, the proposed detector is evaluated and compared with a state-of-the-art method under a multi-class (9-class) classification.},
  archive      = {J_JRTIP},
  author       = {Agarwal, Aanchal and Gupta, Abhinav},
  doi          = {10.1007/s11554-022-01218-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {727-737},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time double JPEG forensics for mobile devices},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lane line detection based on the codec structure of the
attention mechanism. <em>JRTIP</em>, <em>19</em>(4), 715–726. (<a
href="https://doi.org/10.1007/s11554-022-01217-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For self-driving cars and advanced driver assistance systems, lane detection is imperative. On the one hand, numerous current lane line detection algorithms perform dense pixel-by-pixel prediction followed by complex post-processing. On the other hand, as lane lines only account for a small part of the whole image, there are only very subtle and sparse signals, and information is lost during long-distance transmission. Therefore, it is difficult for an ordinary convolutional neural network to resolve challenging scenes, such as severe occlusion, congested roads, and poor lighting conditions. To address these issues, in this study, we propose an encoder–decoder architecture based on an attention mechanism. The encoder module is employed to initially extract the lane line features. We propose a spatial recurrent feature-shift aggregator module to further enrich the lane line features, which transmits information from four directions (up, down, left, and right). In addition, this module contains the spatial attention feature that focuses on useful information for lane line detection and reduces redundant computations. In particular, to reduce the occurrence of incorrect predictions and the need for post-processing, we add channel attention between the encoding and decoding. It processes encoding and decoding to obtain multidimensional attention information, respectively. Our method achieved novel results on two popular lane detection benchmarks (CULane F1-measure 76.2, TuSimple accuracy 96.85%), which can reach 48 frames per second and meet the real-time requirements of autonomous driving.},
  archive      = {J_JRTIP},
  author       = {Zhao, Qinghua and Peng, Qi and Zhuang, Yiqi},
  doi          = {10.1007/s11554-022-01217-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {715-726},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lane line detection based on the codec structure of the attention mechanism},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SimpleMeshNet: End to end recovery of 3d body mesh with one
fully connected layer. <em>JRTIP</em>, <em>19</em>(3), 703–713. (<a
href="https://doi.org/10.1007/s11554-022-01214-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to recent research, reconstructing high-precision 3D human body shape and pose using neural networks necessitates not just large datasets with ground-truth 3D annotations, but also depends significantly on sophisticated network structures to utilize spatial and temporal information. Employing these strategies will also make training more difficult and time-consuming. We proposed SimpleMeshNet, the simplest frame-based model to present, to estimate 3D human body mesh for in-the-wild images. On the one hand, the SimpleMeshNet contains just one fully connected layer after extracting the features and utilizing a pre-trained ResNet as a regressor to output the SMPL model parameters; on the other hand, it performed well and runs fairly fast. To minimize overfitting concerns when the ground-truth SMPL annotations are missing, SimpleMeshNet employs two different training strategies when training the network with or without ground-truth SMPL parameter annotations. Without bells and whistles, the network is quite easy to train and the results are highly convincing. In comparison to other methods, SimpleMeshNet&#39;s performance is measured using a video with five persons and an RTX3090 GPU. SimpleMeshNet alone can achieve 107 frames per second, whereas the whole system can get 45 frames per second while using YOLOv3-416 as a tracker. Compared with the leading algorithms, the performance of SimpleMeshNet can rival them, sometimes even better. What’s more, SimpleMeshNet can be used to process different in-the-wild images captured by a variety of devices: cell phones, monitors, cameras, and more.},
  archive      = {J_JRTIP},
  author       = {Sun, Wenzhang and Ma, Shaopeng and He, Xuanfang and Ma, Qinwei},
  doi          = {10.1007/s11554-022-01214-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {703-713},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SimpleMeshNet: End to end recovery of 3d body mesh with one fully connected layer},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast identification model for coal and gangue based on the
improved tiny YOLO v3. <em>JRTIP</em>, <em>19</em>(3), 687–701. (<a
href="https://doi.org/10.1007/s11554-022-01215-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of coal mining, the coal quality is greatly reduced due to the mixing of gangue, and the stacking and burning of coal and gangue can cause serious environmental pollution. The fast identification of coal and gangue is an important technology for gangue sorting. Considering tiny YOLO v3 has the advantages of fast running speed, simple network and good effectiveness, an improved tiny-YOLO-v3-based fast identification model is proposed in this paper, which including the spatial pyramid pooling (SPP) net, the squeeze-and-excitation (SE) module and the dilated convolution. Firstly, the SPP net preprocesses the input images into acceptable size via a single convolution layer. Then, the SE module, which can strengthen the attention among channels in RGB image, is adopted to capture the key information and enhance the sensitivity of network accurately. Finally, the dilated convolution, which can enlarge the receptive filed without increasing parameters, is used to further optimize and realize the fast identification of coal and gangue. The experiments show that, compared with the tiny YOLO v3 the average intersection over union (Avg IOU) of proposed model is as high as 0.39%, the required time for each image and the loss reduced by 7.41 and 53.01%, respectively. The mean Average Precision (mAP@0.5) of the proposed network is 3.12% higher than Faster RCNN, reaching 0.964. It is an efficient fast identification model for gangue-sorting.},
  archive      = {J_JRTIP},
  author       = {Pan, Hongguang and Shi, Yuhong and Lei, Xinyu and Wang, Zheng and Xin, Fangfang},
  doi          = {10.1007/s11554-022-01215-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {687-701},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast identification model for coal and gangue based on the improved tiny YOLO v3},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High throughput architecture for multiscale variational
optical flow. <em>JRTIP</em>, <em>19</em>(3), 675–686. (<a
href="https://doi.org/10.1007/s11554-022-01216-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation of high-quality Optical Flow (OF) for fast-moving objects in real time is still an unsolved issue for High Definition (HD) images. The work proposes a novel hybrid hardware architecture for the variational multiscale Horn-Schunck OF algorithm. The dedicated memory banks and the custom access schemes help to achieve superior area reduction and improve parallelism. To the best of the author’s knowledge, this is the first work on deeply pipelined hardware architecture for capturing dense OF of fast-moving objects from HD frames in real time. The Field Programmable Gate Array (FPGA) emulation achieves an effective speed-up of $$2.6\times$$ compared to the state-of-the-art non-variational multiscale OF implementations. A full system evaluation on the Virtex-7 FPGA could compute dense OF of incoming HD frames in real time.},
  archive      = {J_JRTIP},
  author       = {Johnson, Bibin and Thomas, Sachin and Rani, J. Sheeba},
  doi          = {10.1007/s11554-022-01216-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {675-686},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High throughput architecture for multiscale variational optical flow},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and implementation of a real-time LDWS with parameter
space filtering for embedded platforms. <em>JRTIP</em>, <em>19</em>(3),
663–673. (<a href="https://doi.org/10.1007/s11554-022-01213-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a lane departure warning system (LDWS) algorithm for embedded platforms which has restricted resources is proposed. An LDWS consists of two main sub-functions which are lane detection and lane tracking. Although sophisticated methods have been developed for both sub-functions, they usually require high processing power and even GPU processing power. Therefore, they are not applicable for hardware with limited resources. In this work, Hough Transform (HT)-based lane detection algorithm is applied. The vulnerability of HT-based methods against misleading images is eliminated by the proposed filtering algorithm. Main differences of the proposed filtering algorithm from the classical methods in the literature are that it is applied in the parameter space rather than the image, and it is specialized only for determining lanes. In the lane tracking stage, the K-means clustering algorithm has been modified to operate online. In this way, the parameters of the detected lane can be followed adaptively during lane changing or overtaking. Real-time test results on embedded hardware demonstrated that the processing time does not exceed 41.67 ms with an accuracy of over 91.5%.},
  archive      = {J_JRTIP},
  author       = {Selim, Erman and Alci, Musa and Uğur, Aybars},
  doi          = {10.1007/s11554-022-01213-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {663-673},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Design and implementation of a real-time LDWS with parameter space filtering for embedded platforms},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evaluation of EfficientDet for object detection used for
indoor robots assistance navigation. <em>JRTIP</em>, <em>19</em>(3),
651–661. (<a href="https://doi.org/10.1007/s11554-022-01212-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor object detection and recognition present one of the most crucial tasks for computer vision and robotic systems. Developing new intelligent autonomous robots is required in various applications including blind and visually impaired people assistance navigation and smart healthcare. Intelligent robots navigation is still a very challenging problem as it involves various aspects including indoor objects detection, recognition and scene understanding. We propose in this work to develop an indoor object detection system that can be used for intelligent vision of robotics applications. We ensure in this paper a lightweight implementation of the system using EfficientDet neural network. The proposed work presents a vision-based detection system able to work on real mobile robots by studying and considering their limited resources implementations. To ensure a lightweight implementation of the proposed indoor objects detection system and to design a deployable system in mobile robots application, we applied the weights pruning technique. To contribute for an embedded implementation of the proposed system, we used a pruning method which successfully reduced the network size, complexity and computation resources. Experimental results have demonstrated the robustness of the proposed indoor object detection system that can be deployed for indoor robotics assistance navigation systems. Based on the obtained results, we note that the proposed system achieved very competitive results in terms of detection precision as well as processing time. The proposed system can runs in low-end devices as we succeeded to reduce the parameters and FLOPs number, we achieved 89% on the testing set of the proposed indoor data set for EfficientDet D2. We achieved 31 FPS for the basic EfficientDet model and 38 FPS for the pruned model.},
  archive      = {J_JRTIP},
  author       = {Afif, Mouna and Ayachi, Riadh and Said, Yahia and Atri, Mohamed},
  doi          = {10.1007/s11554-022-01212-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {651-661},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An evaluation of EfficientDet for object detection used for indoor robots assistance navigation},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient filter pruning via topology.
<em>JRTIP</em>, <em>19</em>(3), 639–649. (<a
href="https://doi.org/10.1007/s11554-022-01209-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep neural networks, compressing and accelerating deep neural networks without performance deterioration has become a research hotspot. Among all kinds of network compression methods, network pruning is one of the most effective and popular methods. Inspired by several property-based pruning methods and geometric topology, we focus the research of the pruning method on the extraction of feature map information. We predefine a metric, called TopologyHole, used to describe the feature map and associate it with the importance of the corresponding filter. In the exploration experiments, we find out that the average TopologyHole of the feature map for the same filter is relatively stable, regardless of the number of image batches the CNNs receive. This phenomenon proves TopologyHole is a data-independent metric and valid as a criterion for filter pruning. Through a large number of experiments, we have demonstrated that priorly pruning the filters with high-TopologyHole feature maps achieves competitive performance compared to the state-of-the-art. Notably, on ImageNet, TopologyHole reduces 45.0 $$\%$$ FLOPs by removing 40.9 $$\%$$ parameters on ResNet-50 with 75.71 $$\%$$ , only a loss of 0.44 $$\%$$ in top-1 accuracy.},
  archive      = {J_JRTIP},
  author       = {Xu, Xiaozhou and Chen, Jun and Su, Hongye and Xie, Lei},
  doi          = {10.1007/s11554-022-01209-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {639-649},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Towards efficient filter pruning via topology},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward real-time image annotation using marginalized coupled
dictionary learning. <em>JRTIP</em>, <em>19</em>(3), 623–638. (<a
href="https://doi.org/10.1007/s11554-022-01210-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced $${\ell }_1$$ regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental results on various datasets demonstrate the efficiency of the proposed method for image annotation tasks in terms of accuracy and time. The reference implementation is publicly available at https://github.com/hamid-amiri/MCDL-Image-Annotation .},
  archive      = {J_JRTIP},
  author       = {Roostaiyan, Seyed Mahdi and Hosseini, Mohammad Mehdi and Kashani, Mahya Mohammadi and Amiri, S. Hamid},
  doi          = {10.1007/s11554-022-01210-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {623-638},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Toward real-time image annotation using marginalized coupled dictionary learning},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time eye state recognition using dual convolutional
neural network ensemble. <em>JRTIP</em>, <em>19</em>(3), 607–622. (<a
href="https://doi.org/10.1007/s11554-022-01211-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic recognition of the eye states is essential for diverse computer vision applications related to drowsiness detection, facial emotion recognition (FER), human–computer interaction (HCI), etc. Existing solutions for eye state detection are either parameter intensive or suffer from a low recognition rate. This paper presents the design and implementation of a vision-based system for real-time eye state recognition on a resource-constrained embedded platform to tackle these issues. The designed system uses an ensemble of two lightweight convolutional neural networks (CNN), each trained to extract relevant information from the eye patches. We adopted transfer-learning-based fine-tuning to overcome the over-fitting issues when training the CNNs on small sample eye state datasets. Once trained, these CNNs are integrated and jointly fine-tuned to achieve enhanced performance. Experimental results manifest the effectiveness of the proposed eye state recognizer that is robust and computationally efficient. On the ZJU dataset, the proposed DCNNE model delivered the state-of-the-art recognition accuracy of 97.99% and surpassed the prior best recognition accuracy of 97.20% by 0.79%. The designed model also achieved competitive results on the CEW and MRL datasets. Finally, the designed CNNs are optimized and ported on two different embedded platforms for real-world applications with real-time performance. The complete system runs at 62 frames per second (FPS) on an Nvidia Xavier device and 11 FPS on a low-cost Intel NCS2 embedded platform using a frame size of 640 $$\times$$ 480 pixels resolution.},
  archive      = {J_JRTIP},
  author       = {Saurav, Sumeet and Gidde, Prashant and Saini, Ravi and Singh, Sanjay},
  doi          = {10.1007/s11554-022-01211-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {607-622},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time eye state recognition using dual convolutional neural network ensemble},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GUD-canny: A real-time GPU-based unsupervised and
distributed canny edge detector. <em>JRTIP</em>, <em>19</em>(3),
591–605. (<a href="https://doi.org/10.1007/s11554-022-01208-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Canny algorithm is one of the most commonly used edge detectors due to its superior performance, especially in noisy environments. Its main limitation is that it is time consuming due to its multistage nature and the use of complex computational operations, primarily hysteresis thresholding. For this reason, many efficient implementations of the Canny edge detector have been developed on different accelerating platforms, such as ASICs, FPGAs and GPUs. The two main limitations of the GPU implementations developed to date are the bottleneck caused by the hysteresis process, and the use of fixed hysteresis thresholds. To overcome these issues, a novel GPU-based unsupervised and distributed Canny edge detector is proposed in this paper. Experimental evaluation showed that our Canny edge detector fully satisfies real time requirements, as it only requires 0.35 ms on average to detect edges on 512 $$\times$$ 512 images, and that it is faster than existing GPU and FPGA implementations.},
  archive      = {J_JRTIP},
  author       = {Fuentes-Alventosa, Antonio and Gómez-Luna, Juan and Medina-Carnicer, R.},
  doi          = {10.1007/s11554-022-01208-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {591-605},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GUD-canny: A real-time GPU-based unsupervised and distributed canny edge detector},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A large-scale container dataset and a baseline method for
container hole localization. <em>JRTIP</em>, <em>19</em>(3), 577–589.
(<a href="https://doi.org/10.1007/s11554-022-01199-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic container handling plays an important role in improving the efficiency of the container terminal, promoting the globalization of container trade, and ensuring worker safety. Utilizing vision-based methods to assist container handling has recently drawn attention. However, most existing keyhole detection/localization methods still suffer from coarse keyhole boundaries. To solve this problem, we propose a real-time container hole localization algorithm based on a modified salient object segmentation network. Note that there exists no public container dataset for researchers to fairly compare their approaches, which has hindered the advances of related algorithms in this domain. Therefore, we propose the first large-scale container dataset in this work, containing 1700 container images and 4810 container hole images, for benchmarking container hole location and detection. Through extensive quantitative evaluation and computational complexity analysis, we show our method can simultaneously achieve superior results on precision and real-time performance. Especially, the detection and location precision is 100% and 99.3%, surpassing the state-of-the-art-work by 2% and 62% respectively. Further, our proposed method only consumes 70 ms (on GPU) or 1.27s (on CPU) per image. We hope the baseline approach, the first released dataset will help benchmark future work and follow-up research on automatic container handling. The dataset is available at https://github.com/qkicen/A-large-scale-container-dataset-and-a-baseline-method-for-container-hole-localization .},
  archive      = {J_JRTIP},
  author       = {Diao, Yunfeng and Tang, Xin and Wang, He and Taylor, Emma Christophine Florence and Xiao, Shirui and Xie, Mengtian and Cheng, Wenming},
  doi          = {10.1007/s11554-022-01199-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {577-589},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A large-scale container dataset and a baseline method for container hole localization},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional neural networks for geo-localisation with a
single aerial image. <em>JRTIP</em>, <em>19</em>(3), 565–575. (<a
href="https://doi.org/10.1007/s11554-022-01207-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Unmanned Aerial Vehicles (UAVs) navigating outdoors rely heavily on GPS for localisation and autonomous flight or applications for aerial photography re-cording with a GPS coordinate. However, GPS may fail or become unreliable, thus compromising the flight mission. Motivated by this scenario, in this work, we present a study on the use of popular Convolutional Neural Networks (CNN) to address the problem of geo-localisation from a single aerial image. We compare CNN-based architectures from the state-of-the-art, and introduce a compact architecture to speed up the inference process without affecting the estimation error. For our experiments, aerial images were recorded with a monocular camera onboard a UAV, flying outdoors with a height between 20 to 25 metres. On average, our compact network achieves a minimum estimation error of 2.8 metres and a maximum of 6.1 metres, which is comparable to the performance of other networks in the state-of-the-art. However, our network achieves on average an operation frequency of 103 fps versus 69 fps achieved by the fastest network in the comparison analysis. These results are promising since such speed would enable fast geo-localisation with cameras capturing images at those frame rates, which are useful for obtaining neater images than with conventional cameras working at 30 fps.},
  archive      = {J_JRTIP},
  author       = {Cabrera-Ponce, Aldrich A. and Martinez-Carranza, Jose},
  doi          = {10.1007/s11554-022-01207-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {565-575},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Convolutional neural networks for geo-localisation with a single aerial image},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing a real-time social distancing detection system
based on YOLOv4-tiny and bird-eye view for COVID-19. <em>JRTIP</em>,
<em>19</em>(3), 551–563. (<a
href="https://doi.org/10.1007/s11554-022-01203-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a virus, which is transmitted through small droplets during speech, sneezing, coughing, and mostly by inhalation between individuals in close contact. The pandemic is still ongoing and causes people to have an acute respiratory infection which has resulted in many deaths. The risks of COVID-19 spread can be eliminated by avoiding physical contact among people. This research proposes real-time AI platform for people detection, and social distancing classification of individuals based on thermal camera. YOLOv4-tiny is proposed in this research for object detection. It is a simple neural network architecture, which makes it suitable for low-cost embedded devices. The proposed model is a better option compared to other approaches for real-time detection. An algorithm is also implemented to monitor social distancing using a bird’s-eye perspective. The proposed approach is applied to videos acquired through thermal cameras for people detection, social distancing classification, and at the same time measuring the skin temperature for the individuals. To tune up the proposed model for individual detection, the training stage is carried out by thermal images with various indoor and outdoor environments. The final prototype algorithm has been deployed in a low-cost Nvidia Jetson devices (Xavier and Jetson Nano) which are composed of fixed camera. The proposed approach is suitable for a surveillance system within sustainable smart cities for people detection, social distancing classification, and body temperature measurement. This will help the authorities to visualize the fulfillment of the individuals with social distancing and simultaneously monitoring their skin temperature.},
  archive      = {J_JRTIP},
  author       = {Saponara, Sergio and Elhanashi, Abdussalam and Zheng, Qinghe},
  doi          = {10.1007/s11554-022-01203-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {551-563},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Developing a real-time social distancing detection system based on YOLOv4-tiny and bird-eye view for COVID-19},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient parallel-pipelined intra prediction
architecture to support DCT/DST engine of HEVC encoder. <em>JRTIP</em>,
<em>19</em>(3), 539–550. (<a
href="https://doi.org/10.1007/s11554-022-01206-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of intra prediction in high-efficiency video coding (HEVC) is increased due to the addition of five variable sized prediction units (PUs) and 35 directional predictions. In this work, we propose an efficient parallel-pipelined architecture that can process 8 samples in parallel for every clock cycle. The functional units needed to predict the PU samples work in a pipelined fashion. With this balanced combination of parallel-pipelined structure, we are able to achieve higher throughput with limited hardware resources than existing literature works. The samples are processed row-wise, so that they can be directly transform coded, thus eliminating the need for an intermediate memory buffer of 8 K between the two modules. A compact reconfigurable reference buffer of size 0.8 KB is incorporated to reduce the read-write latency associated with reference samples’ fetching. A dedicated module for arithmetic operations is used in the intra engine that ensures the reuse of multipliers to increase the hardware efficiency. The architecture so designed supports all the PU sizes and directional modes. The proposed design is tested and implemented on a field-programmable gate array (FPGA) platform operating at 150 MHz frequency to achieve 8 samples throughput with a hardware cost of 16.2 K Look-Up Tables (LUTs) and 5.7 K registers to support HD 4 K real-time video encoding applications.},
  archive      = {J_JRTIP},
  author       = {Poola, Lakshmi and Aparna, P.},
  doi          = {10.1007/s11554-022-01206-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {539-550},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient parallel-pipelined intra prediction architecture to support DCT/DST engine of HEVC encoder},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPGA-accelerated adaptive cartesian to polar conversion and
efficient MI computation for image registration. <em>JRTIP</em>,
<em>19</em>(3), 529–537. (<a
href="https://doi.org/10.1007/s11554-022-01205-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a high speed cartesian to polar conversion of an image and efficient Mutual Information (MI) computation methods have been proposed for high-speed multi-modal image registration. Further, a complete hardware-based system for computation of existing transformation parameters between two images has been developed. The proposed method speed up MI computation and cartesian to polar conversion of an image by detecting and discarding redundant computation. The proposed system is mapped in Field Programmable Gate Array (FPGA).},
  archive      = {J_JRTIP},
  author       = {Mondal, Pulak},
  doi          = {10.1007/s11554-022-01205-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {529-537},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-accelerated adaptive cartesian to polar conversion and efficient MI computation for image registration},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPU and GPU real-time filtering methods for dense surface
metrology using general matrix to matrix multiplications.
<em>JRTIP</em>, <em>19</em>(3), 517–527. (<a
href="https://doi.org/10.1007/s11554-022-01204-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filtering is a required task in surface metrology for the identification of the components relevant for automated quality control. The calculation of real-time features about the surface is crucial to determining the mechanical and physical properties of the inspected product. The computation efficiency of the filtering operations is a major challenge in surface metrology, as current sensors provide massive volumes of data at very high acquisition rates. To overcome the challenges, this work presents different real-time filtering solutions comparing the performance on the CPU and on the GPU, using modern hardware. The proposed framework is focused on filtering techniques that can be expressed using a finite impulse response (FIR) kernel that includes the Gaussian kernel, the most common filtering technique recommended by ISO and ASME standards. This research work proposes variations of the double FIFO and double circular filters. The filters are transformed into a series of general matrix to matrix multiplications, which can be run extremely efficiently on different architectures. The proposed filtering approach provides superior performance compared with previous works. Additionally, tests are carried out to quantify the performance of the GPU in terms of data transfer and computation capabilities in order to diminish the penalty imposed by data transfer from main memory to the GPU in real-time operations. Based on the results, an efficient batch filtering technique is proposed that can be run on the GPU faster than the CPU even for small profile and kernel sizes, offloading this task from the host CPU for optimal system and application response.},
  archive      = {J_JRTIP},
  author       = {Usamentiaga, R.},
  doi          = {10.1007/s11554-022-01204-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {517-527},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CPU and GPU real-time filtering methods for dense surface metrology using general matrix to matrix multiplications},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight convolutional neural network for real-time 3D
object detection in road and railway environments. <em>JRTIP</em>,
<em>19</em>(3), 499–516. (<a
href="https://doi.org/10.1007/s11554-022-01202-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For smart mobility, and autonomous vehicles (AV), it is necessary to have a very precise perception of the environment to guarantee reliable decision-making, and to be able to extend the results obtained for the road sector to other areas such as rail. To this end, we introduce a new single-stage monocular real-time 3D object detection convolutional neural network (CNN) based on YOLOv5, dedicated to smart mobility applications for both road and rail environments. To perform the 3D parameter regression, we replace YOLOv5’s anchor boxes with our hybrid anchor boxes. Our method is available in different model sizes such as YOLOv5: small, medium, and large. The new model that we propose is optimized for real-time embedded constraints (lightweight, speed, and accuracy) that takes advantage of the improvement brought by split attention (SA) convolutions called small split attention model (Small-SA). To validate our CNN model, we also introduce a new virtual dataset for both road and rail environments by leveraging the video game Grand Theft Auto V (GTAV). We provide extensive results of our different models on both KITTI and our own GTAV datasets. Through our results, we show that our method is the fastest available 3D object detection with accuracy results close to state-of-the-art methods on the KITTI road dataset. We further demonstrate that the pre-training process on our GTAV virtual dataset improves the accuracy on real datasets such as KITTI, thus allowing our method to obtain an even greater accuracy than state-of-the-art approaches with 16.16% 3D average precision on hard car detection with inference time of 11.1 ms/image on an RTX 3080 GPU.},
  archive      = {J_JRTIP},
  author       = {Mauri, A. and Khemmar, R. and Decoux, B. and Haddad, M. and Boutteau, R.},
  doi          = {10.1007/s11554-022-01202-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {499-516},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight convolutional neural network for real-time 3D object detection in road and railway environments},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SlimYOLOv4: Lightweight object detector based on YOLOv4.
<em>JRTIP</em>, <em>19</em>(3), 487–498. (<a
href="https://doi.org/10.1007/s11554-022-01201-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a valuable but challenging technology in computer vision research. Although existing methods could attain satisfactory results on high-performance computers, but the huge number of network parameters brings great operating pressure to the mobile devices with limited computing power. Existing methods are usually in a dilemma between accuracy and speed. The low detection effect brings great difficulties to the implementation of detection tasks. This paper optimizes the classic YOLOv4 and proposes the SlimYOLOv4 network structure. Firstly, we change the feature extraction network from CSPDarknet53 to MobileNetV2. Secondly, more appropriate DO-DConv (depthwise over-parameterized depthwise convolutional layer) and DSC (depthwise separable convolution) were selected to replace the standard convolution in the network structure, which greatly reduces computation and improves network performance. Finally, Leaky ReLU is replaced by ReLU6 to improve the numerical resolution. We evaluate SlimYOLOv4 on Pascal VOC07+12 dataset and MS COCO dataset. The experimental results demonstrate that the parameters of our method account for only 12.6 $$\%$$ of YOLOv4, and the speed is 1.59 times that of YOLOv4, reaching 60.19 frames per second (FPS), which is suitable for real-time detection. It achieve 70.83 $$\%$$ mean average precision (mAP) on PASCAL VOC07+12 and 29.2 $$\%$$ mAP on the MS COCO dataset. As a lightweight object detector, it takes into account both speed and accuracy, which can be comparable to the state-of-the-art detectors as well.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Qian, Huaming and Chu, Shuai},
  doi          = {10.1007/s11554-022-01201-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {487-498},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SlimYOLOv4: Lightweight object detector based on YOLOv4},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time high precision eye center localizer.
<em>JRTIP</em>, <em>19</em>(2), 475–486. (<a
href="https://doi.org/10.1007/s11554-022-01200-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise eye center localization remains a very promising but challenging task, while its real-time performance constitutes a critical constraint in many human interaction applications. In this paper a new hybrid framework that combines the shape-based Modified Fast Radial Symmetry Transform (MFRST) and a Convolutional Neural Network (CNN), is introduced. The motivation of this work is to exploit the circularity of the iris to reduce the search space and consequently, the computational complexity of the fed CNN. Thus, the proposed hybrid scheme not only achieves real-time performance, but also increases substantially the localization accuracy by reducing the false detections of the MFRST. The experimental results that stemmed from the most challenging face databases demonstrated high accuracy, outperforming state of the art techniques even those that are based on end-to-end deep neural networks. To deal with unreliable data and provide valid evaluation, we manually annotated the FERET database, making the annotations publicly available. Moreover, the reduced computational time of the proposed scheme reveals that it can be incorporated in low-cost eye trackers, where the real-time performance is a basic prerequisite.},
  archive      = {J_JRTIP},
  author       = {Poulopoulos, Nikolaos and Psarakis, Emmanouil Z.},
  doi          = {10.1007/s11554-022-01200-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {475-486},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time high precision eye center localizer},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time siamese tracker deployed on UAVs.
<em>JRTIP</em>, <em>19</em>(2), 463–473. (<a
href="https://doi.org/10.1007/s11554-021-01190-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is an essential enabler for the automation of UAVs. Recently, Siamese network based trackers have achieved excellent performance on offline benchmarks. The Siamese network based trackers usually use classic deep and wide networks, such as AlexNet, VggNet, and ResNet, to extract the features of template frame and detection frame. However, due to the poor computing power of embedded devices, these models without modification are too heavy on calculation to be deployed on UAVs. In this paper, we propose a guideline to design a slim backbone: the dimension of output should be smaller than that of the input for every layer. Directed by the guideline, we reduce the computational requirements of AlexNet by 59.4%, while the tracker maintains a comparable accuracy. In addition, we adopt an anchor-free network as the tracking head, which requires less calculation than that of anchor-based method. Based on such approaches, our tracker achieves an AUC of 60.9% on UAV123 data set and reaches 30 frames per second on NVIDIA Jetson TX2, which, therefore, can be embedded in UAVs. To the best of our knowledge, it is the first real-time Siamese tracker deployed on the embedded system of UAVs. The code is available at GitHub.},
  archive      = {J_JRTIP},
  author       = {Shen, Hao and Lin, Defu and Song, Tao},
  doi          = {10.1007/s11554-021-01190-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {463-473},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time siamese tracker deployed on UAVs},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal synchronization framework of machine-vision cameras
for high-speed steel surface inspection systems. <em>JRTIP</em>,
<em>19</em>(2), 445–461. (<a
href="https://doi.org/10.1007/s11554-022-01198-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-speed industrial machine-vision (MV) applications such as surface inspection of steel sheets necessitate synchronous operation of multiple high-resolution cameras. Synchronization of cameras in the microsecond band is necessary to ensure accurate frame matching while melding images together. Existing approaches for synchronization employ dedicated electronic circuits or network-time-protocol (NTP) whose accuracies are in the millisecond band. Conversely, IEEE-1508 precision-time-protocol (PTP) synchronizes computers in highly accurate industrial measurement and control networks. Synchronization algorithms using PTP involve synchronizing computers connected to cameras. Although the computers synchronize in the microsecond band, the cameras synchronize in the millisecond band. Moreover, PTP is practically not used for synchronizing multiple devices due to the high bandwidth utilization of the network. This paper proposes a temporal synchronization algorithm and framework with two-way communication with timestamps and estimates mean path delays. Unicast transmission forms the basis of the synchronization framework, so that the network utilization is minimal, thereby ensuring the necessary bandwidth is available for image transmission. Experimental results show that the proposed approach outperforms the existing methodologies with synchronization accuracies in the microsecond band.},
  archive      = {J_JRTIP},
  author       = {Subramanyam, Vasanth and Kumar, Jayendra and Singh, Shiva Nand},
  doi          = {10.1007/s11554-022-01198-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {445-461},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Temporal synchronization framework of machine-vision cameras for high-speed steel surface inspection systems},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HIDE: Hyperchaotic image encryption using DNA computing.
<em>JRTIP</em>, <em>19</em>(2), 429–443. (<a
href="https://doi.org/10.1007/s11554-021-01194-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital images are encrypted to prevent malicious attempts by unauthorized users from accessing the sensitive information. Recently, a number of chaotic image encryption schemes are cryptanalyzed using known/chosen plaintext attacks (KPA/CPA) and internal state leakage due to less correlation between the plaintext and key stream inherent by design. In this paper, we propose an efficient and secure image encryption scheme HIDE based on a four dimensional Hyperchaotic Rabinovich system and Deoxyribo Nucleic Acid (DNA) computing. To resist against the KPA/CPA attacks, we introduce a sandwich permutation diffusion architecture that consists of DNA-based arithmetic diffusion to uniformly diffuse the plain image. The symmetric diffusion structure counteracts the computational redundancy and powerful cryptanalytic attacks. The key stream is made sufficiently complex and non-linear by dynamically generating and selecting the keystream. HIDE introduces many improvements towards design of robust image encryption scheme that has resulted in increased key space to resist against brute force attack, preventing leakage of the internal state of the hyper chaotic system, the sandwich permutation architecture and the plaintext-related dynamic key generation and selection based on DNA computing can resist KPA/CPA attacks. The standard empirical statistical security tests conducted extensively shows that the proposed method passes all statistical security and performance benchmarks to guarantee security and operational efficiency that are required of a robust and secure encryption scheme.},
  archive      = {J_JRTIP},
  author       = {Elizabeth, B. Lydia and Gayathri, J. and Subashini, S. and Prakash, A. John},
  doi          = {10.1007/s11554-021-01194-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {429-443},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HIDE: Hyperchaotic image encryption using DNA computing},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning correlation filter with fused feature and reliable
response for real-time tracking. <em>JRTIP</em>, <em>19</em>(2),
417–427. (<a href="https://doi.org/10.1007/s11554-022-01195-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is a key component of machine vision system and getting much attention in different walk of life. Recently, correlation filters have been successfully applied to visual tracking. However, how to design effective features and deal with model drifts remain open issues for online tracking. This paper tackles these challenges by proposing a real-time correlation tracking algorithm (RCT) based on two ideas. First, we propose a method to fuse features to more naturally describe the gradient and color information of the tracked object, and introduce the fused feature into a background-aware correlation filter to obtain the response map. Second, we present a novel strategy to significantly reduce noise in the response map and therefore ease the problem of model drift. Systematic comparative evaluations performed over multiple tracking benchmarks demonstrate the efficacy of the proposed approach. The results show that the proposed RCT significantly improves the performance compared to the baseline tracker while still maintaining a real-time tracking speed of 26 fps in MATLAB implementation.},
  archive      = {J_JRTIP},
  author       = {Lin, Bin and Xue, Xizhe and Li, Ying and Shen, Qiang},
  doi          = {10.1007/s11554-022-01195-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {417-427},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Learning correlation filter with fused feature and reliable response for real-time tracking},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Efficient fast motion estimation algorithm
for real-time applications. <em>JRTIP</em>, <em>19</em>(2), 415. (<a
href="https://doi.org/10.1007/s11554-021-01192-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Agha, Shahrukh and Khan, Mansoor and Jan, Farmanullah},
  doi          = {10.1007/s11554-021-01192-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {415},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: Efficient fast motion estimation algorithm for real-time applications},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Efficient fast motion estimation algorithm for real-time
applications. <em>JRTIP</em>, <em>19</em>(2), 403–413. (<a
href="https://doi.org/10.1007/s11554-021-01188-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion estimation (ME) process is the most computational complex part of a video encoder. It exploits temporal redundancy present in a video sequence to achieve compression. To make the video encoding process in real time, alleviating the computational burden of motion estimation is mandatory. In this work, an efficient hierarchical diamond search (EHDS) ME algorithm is being presented. This algorithm consists of multiresolution hierarchies, modified diamond search and modified early jump-out mechanism. Multiresolution hierarchies and the modified diamond search algorithm help either locate the global minima or some location nearby it, as best match by first finding starting point of search near the global minima which improves quality and reduces complexity, while the modified early jump-out mechanism further reduces complexity. Results have shown that the proposed algorithm has less complexity and power consumption than the conventional diamond search (DS) and other existing DS algorithms, while the quality is comparable to full search (FS) ME algorithm.},
  archive      = {J_JRTIP},
  author       = {Agha, Shahrukh and Khan, Mansoor and Jan, Farmanullah},
  doi          = {10.1007/s11554-021-01188-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {403-413},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient fast motion estimation algorithm for real-time applications},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel fractal image compression using quadtree partition
with task and dynamic parallelism. <em>JRTIP</em>, <em>19</em>(2),
391–402. (<a href="https://doi.org/10.1007/s11554-021-01193-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractal image compression is a lossy compression technique based on the iterative function system, which can be used to reduce the storage space and increase the speed of data transmission. The main disadvantage of fractal image compression is the high computational cost of the encoding step, compared with the popular image compression based on discrete cosine transform. The aim of this paper is the development of parallel implementations of fractal image compression using quadtree partition. We develop two parallel implementations: the first one uses task parallelism over a multi-core system and the second uses dynamic parallelism over a GPU architecture. We show performance comparisons of the parallel implementations using standard images to compare the capabilities of these parallel architectures. The proposed parallel implementations achieve speedups over the serial implementation of approximately $$15 \times$$ using the multi-core CPU and $$25 \times$$ using the GPU.},
  archive      = {J_JRTIP},
  author       = {Hernandez-Lopez, Francisco J. and Muñiz-Pérez, Omar},
  doi          = {10.1007/s11554-021-01193-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {391-402},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel fractal image compression using quadtree partition with task and dynamic parallelism},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient coding unit classifier for HEVC screen content
coding based on machine learning. <em>JRTIP</em>, <em>19</em>(2),
375–390. (<a href="https://doi.org/10.1007/s11554-021-01189-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Video Coding Joint Collaboration team (JCT-VC) has been working on an emerging standard for screen content coding (SCC) as an extension of high efficiency video coding (HEVC) standard known as HEVC-SCC. The two powerful coding mechanisms used in HEVC-SCC are intra block copy (IBC) and palette coding (PLT). These techniques achieve the best coding efficiency at the expense of extremely high computational complexity. Therefore, we propose a new technique to minimize computational complexity by skipping undesired modes and retaining coding efficiency. A fast intra mode decision approach is suggested based on efficient CU classification. Our proposed solution depends on categorizing a CU as a natural content block (NCB) or a screen content block (SCB). Two classifiers are used for the classification process. The first one is a neural network (NN) classifier, and the other is an AdaBoost classifier, which depends on a boosted decision stump algorithm. The two classifiers predict the CU type individually and the final decision for CU classification depends on both of them. The experimental results reveal that the suggested technique significantly decreases encoding time without sacrificing coding efficiency. The suggested framework can achieve a 26.13% encoding time reduction on average with just a 0.81% increase in Bjontegaard Delta bit-rate (BD-Rate). Furthermore, the suggested framework saves encoding time by 51.5% on average for a set of NC sequences recommended for standard HEVC tests with minimal performance degradation. The proposed strategy has been merged with an existing methodology to accelerate the process even further.},
  archive      = {J_JRTIP},
  author       = {Elsawy, Nabila and Sayed, Mohammed S. and Farag, Fathi},
  doi          = {10.1007/s11554-021-01189-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {375-390},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient coding unit classifier for HEVC screen content coding based on machine learning},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event camera simulator design for modeling attention-based
inference architectures. <em>JRTIP</em>, <em>19</em>(2), 363–374. (<a
href="https://doi.org/10.1007/s11554-021-01191-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras ca be expensive, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator’s relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads},
  archive      = {J_JRTIP},
  author       = {Pantho, Md Jubaer Hossain and Mbongue, Joel Mandebi and Bhowmik, Pankaj and Bobda, Christophe},
  doi          = {10.1007/s11554-021-01191-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {363-374},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Event camera simulator design for modeling attention-based inference architectures},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAV-based autonomous detection and tracking of beyond visual
range (BVR) non-stationary targets using deep learning. <em>JRTIP</em>,
<em>19</em>(2), 345–361. (<a
href="https://doi.org/10.1007/s11554-021-01185-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial surveillance and tracking have gained significant traction in recent years for both civilian applications and military reconnaissance. Disaster analysis, emergency medical response, pandemic spread analysis, etc. have significantly improved with the availability of aerial data. The next big step is to push the system for autonomous detection and tracking of targets beyond visual range (BVR). Presently, this is done using GPS-based techniques in which the target information is assumed to be precisely known. In situations where such information is unavailable or if the target of interest is non-stationary, this method is not applicable and currently, no alternative exists. In this work, we aim to address this limitation and propose a deep learning-based algorithm for terminal guidance of aerial vehicle BVR with only bearing information about the target of interest. The algorithm operates in search and track modes. We describe both the modes and also discuss the challenges associated with this kind of deployment in real time. Since the weight and power requirements of the payload directly translate to the cost of deployment and endurance of aerial vehicles, we have configured a custom lightweight convolutional neural network (CNN) with minimal layers and successfully deployed the system on Jetson Nano, the smallest GPU available from NVIDIA as of this writing. We evaluated the performance of the proposed algorithm on proprietary and open-source datasets and achieved detection accuracy greater than 98.6% on custom datasets.},
  archive      = {J_JRTIP},
  author       = {Chandrakanth, V. and Murthy, V. S. N. and Channappayya, Sumohana S.},
  doi          = {10.1007/s11554-021-01185-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {345-361},
  shortjournal = {J. Real-Time Image Process.},
  title        = {UAV-based autonomous detection and tracking of beyond visual range (BVR) non-stationary targets using deep learning},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FL-MISR: Fast large-scale multi-image super-resolution for
computed tomography based on multi-GPU acceleration. <em>JRTIP</em>,
<em>19</em>(2), 331–344. (<a
href="https://doi.org/10.1007/s11554-021-01181-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-image super-resolution (MISR) usually outperforms single-image super-resolution (SISR) under a proper inter-image alignment by explicitly exploiting the inter-image correlation. However, the large computational demand encumbers the deployment of MISR in practice. In this work, we propose a distributed optimization framework based on data parallelism for fast large-scale MISR using multi-GPU acceleration named FL-MISR. The scaled conjugate gradient (SCG) algorithm is applied to the distributed subfunctions and the local SCG variables are communicated to synchronize the convergence rate over multi-GPU systems towards a consistent convergence. Furthermore, an inner-outer border exchange scheme is performed to obviate the border effect between neighboring GPUs. The proposed FL-MISR is applied to the computed tomography (CT) system by super-resolving the projections acquired by subpixel detector shift. The SR reconstruction is performed on the fly during the CT acquisition such that no additional computation time is introduced. FL-MISR is extensively evaluated from different aspects and experimental results demonstrate that FL-MISR effectively improves the spatial resolution of CT systems in modulation transfer function (MTF) and visual perception. Comparing to a multi-core CPU implementation, FL-MISR achieves a more than 50 $$\times$$ speedup on an off-the-shelf 4-GPU system.},
  archive      = {J_JRTIP},
  author       = {Sun, Kaicong and Tran, Trung-Hieu and Guhathakurta, Jajnabalkya and Simon, Sven},
  doi          = {10.1007/s11554-021-01181-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {331-344},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FL-MISR: Fast large-scale multi-image super-resolution for computed tomography based on multi-GPU acceleration},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time optical flow processing on embedded GPU: An
hardware-aware algorithm to implementation strategy. <em>JRTIP</em>,
<em>19</em>(2), 317–329. (<a
href="https://doi.org/10.1007/s11554-021-01187-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the optical flow of a video is a compute-intensive task essential for computer vision. For achieving this processing in real time, the whole algorithm deployment chain must be thought of for efficiency first. The development is usually divided into two parts: first, designing an algorithm that meets precision constraints, then, implementing and optimizing its execution on the targeted platform. We argue that unifying those operations enhances performance on the embedded processor. This paper is based on an industrial use case of computer vision. The objective is to determine dense optical flow in real time on an embedded GPU platform: the Nvidia AGX Xavier. The CLG (combined local–global) optical flow method, initially chosen, is analyzed to understand the convergence speed of its underlying optimization problem. The Jacobi solver is selected for implementation because of its parallel nature. The whole multi-level processing is then ported to the GPU, using several specific optimization strategies. In particular, we analyze the impact of fusing the solver’s iterations with the roofline model. As a result, with a 30 W power budget, our implementation runs at 60FPS, on $$640 \times 512$$ images, with a four-level processing. Hopefully, this example should provide feedback on the issues that arise when trying to port a method to a parallel platform and serve for further implementations of computer vision algorithms on specialized hardware.},
  archive      = {J_JRTIP},
  author       = {Seznec, Mickaël and Gac, Nicolas and Orieux, François and Naik, Alvin Sashala},
  doi          = {10.1007/s11554-021-01187-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {317-329},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time optical flow processing on embedded GPU: An hardware-aware algorithm to implementation strategy},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time adaptive skin detection using skin color model
updating unit in videos. <em>JRTIP</em>, <em>19</em>(2), 303–315. (<a
href="https://doi.org/10.1007/s11554-021-01186-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin color plays an important role in color image processing and human–computer interaction. However, factors such as rapidly changing illumination, various color styles, and camera characteristics also make skin detection a challenging task. In particular, the real-time requirement of practical applications is a challenging task in skin detection. In this paper, face detection and alignment are applied to select facial reference points for modeling the skin color distribution. Moreover, we propose the conception and detection approach of skin color model updating unit (SCMUU) according to the fact of skin color distribution remains consistent in a range of frames. The redundant operation of frame by frame updating is avoided using one model in frames of SCMUU. When no reliable faces are detected, two strategies are introduced to remedy and reduce the computational cost. It uses the corresponding model parameters if a similar previous SCMUU is found. Otherwise, we use fixed thresholds instead and increase the interval between two consecutive face detection. Besides, the time-consuming steps are accelerated using a graphic processing unit (GPU) with CUDA in this paper. Experimental results show that, compared with other existing methods, the proposed method has good real time and accuracy for skin detection of various resolution videos under different illumination conditions.},
  archive      = {J_JRTIP},
  author       = {Zhang, Kun and Wang, Yedong and Li, Wenyuan and Li, Changlu and Lei, Zhichun},
  doi          = {10.1007/s11554-021-01186-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {303-315},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time adaptive skin detection using skin color model updating unit in videos},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time high-precision pedestrian tracking: A
detection–tracking–correction strategy based on improved SSD and cascade
r-CNN. <em>JRTIP</em>, <em>19</em>(2), 287–302. (<a
href="https://doi.org/10.1007/s11554-021-01183-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing pedestrian tracking applications are challenging to balance real-time performance and accuracy. We propose a detection–tracking–correction strategy based on the improved single-shot multi-box detector (SSD), Deep-SORT, and the improved multi-stage object detection architecture (Cascade-R-CNN), which takes both real-time performance and accuracy into consideration. For the detection mechanism, the SSD network is fast and efficient, but the disadvantage of the SSD network is relatively low accuracy. Therefore, the tricks such as cross-entropy loss function, deconvolution, and non-maximum suppression are introduced to improve the SSD network. Then, the improved SSD network is used as the central pedestrian detector to ensure real-time performance. For the tracking mechanism, the Deep-SORT is used to improve the mismatch between tracking and detection. For the correction mechanism, the improved Cascade R-CNN (introducing deformable convolution and group normalization) is used as the reference network to correct the detection errors. The experiment on the data set OTB-100 shows that the proposed strategy has good stability and adaptability in various complex scenes, and the conditions of missed detection and false detection are significantly reduced.},
  archive      = {J_JRTIP},
  author       = {Yang, Shudi and Chen, Zhehan and Ma, Xiaoming and Zong, Xianhui and Feng, Zhipeng},
  doi          = {10.1007/s11554-021-01183-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {287-302},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time high-precision pedestrian tracking: A detection–tracking–correction strategy based on improved SSD and cascade R-CNN},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving performance of background subtraction on mobile
devices: A parallel approach. <em>JRTIP</em>, <em>19</em>(2), 275–286.
(<a href="https://doi.org/10.1007/s11554-021-01184-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of moving objects in a resource-constrained environment, such as a personal mobile device, is a challenging task. Nowadays, cameras of cell phones and other mobile devices produce high-resolution videos. In addition, possible camera motion which is inherent to mobile devices adds further complexity to the image processing. Real-time analysis of those videos can be performed using optimized versions of the background subtraction methods. The focus of this work is an efficient implementation of background subtraction using dual-mode single Gaussian model with age on Android platform. Several optimizations were applied: parallelization through RenderScript framework, block processing and grayscale transformation of pixels, and memory transfer and footprint reduction. Implemented algorithm was evaluated using multiple test scenarios on two different platforms. We observed speedups up to 6 times over the reference sequential implementation, real time performance of 50 FPS for 640 $$\times$$ 480 videos and 20.7 FPS for 1280 $$\times$$ 720 videos. Comparative analysis with state-of-the-art methods on CDNet 2014 PTZ category showed good F1-measure. The obtained results are carefully discussed.},
  archive      = {J_JRTIP},
  author       = {Mišić, Marko and Kovačev, Petar and Tomašević, Milo},
  doi          = {10.1007/s11554-021-01184-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {275-286},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improving performance of background subtraction on mobile devices: A parallel approach},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time human detection in thermal infrared imaging at
night using enhanced tiny-yolov3 network. <em>JRTIP</em>,
<em>19</em>(2), 261–274. (<a
href="https://doi.org/10.1007/s11554-021-01182-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human detection is a technology that detects human shapes in the image and ignores everything else. However, modern person detectors have some inefficiencies in detecting pedestrians during video surveillance at night, and the accuracy rate is still insufficient. Therefore, this paper aims to increase the accuracy rate for automatic human detection at night from thermal infrared (TIR) images and real-time video sequences. For this purpose, a new architecture is proposed to enhance the backbone of the Tiny-yolov3 network. The enhanced network used the YOLOv3 algorithm’s tasks with the K-means clustering method to extract more complex features of a person. This network was pre-trained on the MS. COCO dataset to obtain the initial weights. Through the comparison with other related methods showed that the experimental results have achieved the significantly improved performance of human detection from thermal imaging in terms of accuracy, speed, and detection time. The method has achieved a high accuracy rate (90%) compared with the TF-YOLOv3 (88%) trained on the DHU Night Dataset. Although the method has achieved an accuracy rate equal to the YOLOv3-Human (90%), the detection time (4.88 ms) is less, Furthermore, the method has a higher accuracy rate (49.8%) than the YOLO (29.36%) and TF-YOLOv3 (29.8%) with lower detection time (8 ms) on the FLIR Dataset. In addition, the model has achieved a good TP detection for multiple small size of person. By improving the performance of human detection in thermal imaging at night, the method will be able to detect intruders in the night surveillance system.},
  archive      = {J_JRTIP},
  author       = {Manssor, Samah A. F. and Sun, Shaoyuan and Abdalmajed, Mohammed and Ali, Shima},
  doi          = {10.1007/s11554-021-01182-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {261-274},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time human detection in thermal infrared imaging at night using enhanced tiny-yolov3 network},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time embedded system for valve detection in water
pipelines. <em>JRTIP</em>, <em>19</em>(2), 247–259. (<a
href="https://doi.org/10.1007/s11554-021-01178-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Condition assessment is an essential process to comprehend the condition of the water pipelines and facilitate the maintenance as well as the renewal plans. Nowadays, varied in-pipe inspection platforms equipped with closed-circuit cameras are employed to capture the internal condition of the water pipelines. However, the automated platform often faces the challenge to negotiate with the installed valves during the inspection. To ensure continuous inspection, the platform needs identify the valve automatically and activate the control mechanism to pass through it. Thus, the valves need to be detected to facilitate the negotiation and ensure that the control mechanism can take an action in time. This paper focuses on real-time valve detection using Jetson TX2™ and a lightweight algorithm, namely YOLOv3-tiny. The performance of the implementation is compared with state-of-the-art real-time detection models. The experimental results demonstrate that YOLOv3-tiny has a high detection speed in frame per second for valve detection and outperforms the state-of-the-art real-time algorithms. Hence, the deployment YOLOv3-tiny into the embedded system will aid the automated platform to accomplish the uninterrupted inspection and enhance the capability for the condition assessment of the water pipelines.},
  archive      = {J_JRTIP},
  author       = {Rayhana, Rakiba and Jiao, Yutong and Liu, Zheng and Wu, Angie and Kong, Xiangjie},
  doi          = {10.1007/s11554-021-01178-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {247-259},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time embedded system for valve detection in water pipelines},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic programming with adaptive and self-adjusting penalty
for real-time accurate stereo matching. <em>JRTIP</em>, <em>19</em>(2),
233–245. (<a href="https://doi.org/10.1007/s11554-021-01180-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense disparity map extraction is one of the most active research areas in computer vision. It tries to recover three-dimensional information from a stereo image pair. A large variety of algorithms has been developed to solve stereo matching problems. This paper proposes a new stereo matching algorithm, capable of generating the disparity map in real-time and with high accuracy. A novel stereo matching approach is based on per-pixel difference adjustment for the absolute differences, gradient matching and rank transform. The selected cost metrics are aggregated using guided filter. The disparity calculation is performed using dynamic programming with self-adjusting and adaptive penalties to improve disparity map accuracy. Our approach exploits mean-shift image segmentation and refinement technique to reach higher accuracy. In addition, a parallel high-performance graphics hardware based on Compute Unified Device Architecture is used to implement this method. Our algorithm runs at 36 frames per second on $$640 \times 480$$ video with 64 disparity levels. Over 707 million disparity evaluations per second (MDE/s) are achieved in our current implementation. In terms of accuracy and runtime, our algorithm ranks the third place on Middlebury stereo benchmark in quarter resolution up to the submitting.},
  archive      = {J_JRTIP},
  author       = {Hallek, Mohamed and Boukamcha, Hamdi and Mtibaa, Abdellatif and Atri, Mohamed},
  doi          = {10.1007/s11554-021-01180-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {233-245},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dynamic programming with adaptive and self-adjusting penalty for real-time accurate stereo matching},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Publisher’s note. <em>JRTIP</em>, <em>19</em>(1), 231. (<a
href="https://doi.org/10.1007/s11554-022-01196-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  doi          = {10.1007/s11554-022-01196-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {231},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Publisher&#39;s note},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling real-time object detection on low cost FPGAs.
<em>JRTIP</em>, <em>19</em>(1), 217–229. (<a
href="https://doi.org/10.1007/s11554-021-01177-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection using convolutional neural networks (CNNs) has garnered a lot of interest due to their high performance capability. Yet, the large number of operations and memory fetches to both on-chip and external memory needed for such CNNs result in high latency and power dissipation on resource constrained edge devices, hence impeding their real-time operation from a battery supply. In this paper, a resource and cost efficient hardware accelerator for CNN is implemented on an FPGA. Using an existing metric $$\mathrm{DSP}_\mathrm{efficiency}$$ and a new metric $$\mathrm{Cost}_\mathrm{efficiency}$$ as the primary optimization variables, exploration of algorithms and hardware using a design space exploration tool, called ZigZag, is undertaken. An optimized architecture is implemented on a Xilinx XC7Z035 FPGA and tiny-YOLOv2 is mapped to demonstrate the real-time object detection application. Compared to the state-of-the-art (SotA), the implementation results shows that the hardware achieves the best $$\mathrm{DSP}_\mathrm{efficiency}$$ at 90% and $$\mathrm{Cost}_\mathrm{efficiency}$$ at 0.146.},
  archive      = {J_JRTIP},
  author       = {Jain, Vikram and Jadhav, Ninad and Verhelst, Marian},
  doi          = {10.1007/s11554-021-01177-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {217-229},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enabling real-time object detection on low cost FPGAs},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time stereo semi-global matching for video processing
using previous incremental information. <em>JRTIP</em>, <em>19</em>(1),
205–216. (<a href="https://doi.org/10.1007/s11554-021-01175-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an incremental stereo algorithm designed to calculate a real-time disparity image. The algorithm is designed for stereo video sequences and uses previous information to reduce computation time and improve disparity image quality. It is based on the semi-global matching stereo algorithm but modified to reuse previous calculation information. Storing and reusing this information not only reduces computation time but improves accuracy in a cost filtering scheme. Some tests are presented to compare the computation time and results of the algorithm, which show that it can achieve better results in terms of quality and time than standard algorithms for some scenarios.},
  archive      = {J_JRTIP},
  author       = {Toledo, Jonay and Lauer, Martin and Stiller, Christoph},
  doi          = {10.1007/s11554-021-01175-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {205-216},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time stereo semi-global matching for video processing using previous incremental information},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A real-time image-centric transfer function design based on
incremental classification. <em>JRTIP</em>, <em>19</em>(1), 185–203. (<a
href="https://doi.org/10.1007/s11554-021-01176-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key issue in scientific visualization is the transfer function (TF) for direct volume rendering (DVR). The TF serves as a tool for translating data values into color and opacity, to visualize the relevant structures present in the volumetric data studied. An adequate transfer function should have a non-complicated interactive strategy for new users or even experts. Furthermore, it has to achieve high-quality and not time-consuming visualization. In this paper, we propose a novel image-centric method for the real-time generation of transfer functions. The method is based on incremental classification. This incremental classification-based approach is theoretically faster than that using batch classification. The method does not require users to manipulate complex widgets. We present a simple user interface adapted to the incremental learning process. Thus, this interface made it possible for the user to interact with a series of 2D images, precise the cluster, and identify some voxels. The whole volume is incrementally classified and the rendering result is shown to the user as selected voxels are added. The TF is generated by assigning the optical properties to clusters using harmonic colors. We further introduce a novel incremental classifier, namely incremental discriminant-based support vector machine( IDSVM), that can learn through time. The IDSVM was used in the classification stage of the proposed image-centric method. To evaluate the IDSVM, an extensive comparison of the model with other state-of-the-art incremental and batch classifiers on 12 real-world datasets and four other famous large datasets, namely MNIST-full, MNIST-test, USPS, and Fashion-MNIST, has been carried out. Using the area under curve, it has been found that the IDSVM outperforms the other classifiers. Furthermore, to evaluate the proposed image-centric method, we made use of several benchmark datasets. Qualitative results and a detailed user survey demonstrate the effectiveness of the proposed method and the positive effect of the incrementality in visual and interaction time performance.},
  archive      = {J_JRTIP},
  author       = {Salhi, Marwa and Ksantini, Riadh and Zouari, Belhassen},
  doi          = {10.1007/s11554-021-01176-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {185-203},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time image-centric transfer function design based on incremental classification},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-complexity rounded KLT approximation for image
compression. <em>JRTIP</em>, <em>19</em>(1), 173–183. (<a
href="https://doi.org/10.1007/s11554-021-01173-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Karhunen–Loève transform (KLT) is often used for data decorrelation and dimensionality reduction. Because its computation depends on the matrix of covariances of the input signal, the use of the KLT in real-time applications is severely constrained by the difficulty in developing fast algorithms to implement it. In this context, this paper proposes a new class of low-complexity transforms that are obtained through the application of the round function to the elements of the KLT matrix. The proposed transforms are evaluated considering figures of merit that measure the coding power and distance of the proposed approximations to the exact KLT and are also explored in image compression experiments. Fast algorithms are introduced for the proposed approximate transforms. It was shown that the proposed transforms perform well in image compression and require a low implementation cost.},
  archive      = {J_JRTIP},
  author       = {Radünz, Anabeth P. and Bayer, Fábio M. and Cintra, Renato J.},
  doi          = {10.1007/s11554-021-01173-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {173-183},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-complexity rounded KLT approximation for image compression},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient low-complexity block partition scheme for VVC
intra coding. <em>JRTIP</em>, <em>19</em>(1), 161–172. (<a
href="https://doi.org/10.1007/s11554-021-01174-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The newest versatile video coding (VVC) adopts a novel quadtree with a nested multi-type tree (QTMT) partition structure for intra-frame coding and splits the coding unit (CU) into not only square sub-blocks but also rectangular sub-blocks. The more flexible CU sizes improve the encoding efficiency while cause much heavier computational complexity than High-Efficiency Video Coding (HEVC). In this paper, an efficient low-complexity CU partition method based on the texture characteristic is proposed to achieve a good trade-off between the coding efficiency and the complexity reduction. Specifically, the texture complexities of vertical and horizontal directions are quantitatively measured in terms of the sum of the mean absolute deviation (SMAD) of the sub-blocks. Then, the vertical and horizontal texture complexities are compared to eliminate some unlikely partition modes. Moreover, the threshold of directional SMAD ratios for choosing the texture direction is adjusted according to the quantization parameter (QP) to improve the prediction accuracy. Experimental results show that the proposed method achieves the average complexity reduction efficiency by 54.60% than the VVC test model (VTM). The implementation of the proposed method is publicly available at http://sites.google.com/site/wangmiaohui/ and http://github.com/csust-sonie/fastVVC_RTIP_2107 .},
  archive      = {J_JRTIP},
  author       = {Song, Yun and Zeng, Biao and Wang, Miaohui and Deng, Zelin},
  doi          = {10.1007/s11554-021-01174-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {161-172},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient low-complexity block partition scheme for VVC intra coding},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fractional-order PDE-based contour detection model with
CeNN scheme for medical images. <em>JRTIP</em>, <em>19</em>(1), 147–160.
(<a href="https://doi.org/10.1007/s11554-021-01172-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a contour detection scheme to detect object contours in medical images. A new PDE model is designed by including a fractional-order regularization term, making it robust against noise and maintaining the regularity of level set function (LSF) during evolution. A cellular neural network (CeNN) model is used to solve the proposed contour detection PDE. The main advantages of using the CeNN-based approach are that it wipes out the requirement of a reinitialization of level set and can be implemented efficiently on parallel chips. Finally, an experimental study is carried out, which exhibits the feasibility of the proposed approach in contour detection from a set of medical images.},
  archive      = {J_JRTIP},
  author       = {Lakra, Mahima and Kumar, Sanjeev},
  doi          = {10.1007/s11554-021-01172-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {147-160},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fractional-order PDE-based contour detection model with CeNN scheme for medical images},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFSOD: A lightweight single-stage detector for real-time
embedded applications to detect small-size objects. <em>JRTIP</em>,
<em>19</em>(1), 133–146. (<a
href="https://doi.org/10.1007/s11554-021-01170-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-size object detection (SOD) is one of the challenging problems in computer vision applications. SOD is highly useful in defense, military, surveillance, medical, industrial and analysis in sports applications. Various algorithms were developed in the past to solve the problem of SOD. However, the algorithms developed are not suitable for real-time applications. In this work, a convolutional neural network architecture based on YOLO is proposed to enhance small objects&#39; detection performance. The proposed network is inspired by the ideas of Residual blocks, Densenet, Feature Pyramidal Network, Cross stage partial connections, and 1 × 1 convolutions. The Receptive field and the reuse of feature maps are the main factors in the design of the architecture and is hence referred to as RFSOD. It is developed as a lightweight network to suit real-time applications and can run smoothly on single-board computers such as Jetson Nano, Tx2, Raspberry Pi and the like. The proposed model is evaluated on various public datasets such as VHR10, BCCD dataset and few small-size objects from the MS COCO dataset. This work is motivated by the need to develop a vision system for a badminton-playing robot. Therefore, the proposed model is also tested on a custom-made shuttlecock dataset. The model&#39;s performance is compared with the state-of-the-art deep learning models that are suitable for real-time applications. The hardware implementation of the proposed model was carried out on Jetson Nano, Raspberry Pi4 and a Laptop with an i5 processor. Improved Detection accuracy was observed on small objects. More than 2 × detection speed was obtained on Raspberry Pi, and i5 processor while 30% improvement was observed on Jetson Nano with real-time videos.},
  archive      = {J_JRTIP},
  author       = {Amudhan, A. N. and Vrajesh, Shah Rutvik and Sudheer, A. P. and Lijiya, A.},
  doi          = {10.1007/s11554-021-01170-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {133-146},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RFSOD: A lightweight single-stage detector for real-time embedded applications to detect small-size objects},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic error-bounded lossy compression to reduce the
bandwidth requirement for real-time vision-based pedestrian safety
applications. <em>JRTIP</em>, <em>19</em>(1), 117–131. (<a
href="https://doi.org/10.1007/s11554-021-01165-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As camera quality improves and their deployment moves to areas with limited bandwidth, communication bottlenecks can impair real-time constraints of an intelligent transportation systems application, such as video-based real-time pedestrian detection. Video compression reduces the bandwidth requirement to transmit the video which degrades the video quality. As the quality level of the video decreases, it results in the corresponding decreases in the accuracy of the vision-based pedestrian detection model. Furthermore, environmental conditions, such as rain and night-time darkness impact the ability to leverage compression by making it more difficult to maintain high pedestrian detection accuracy. The objective of this study is to develop a real-time error-bounded lossy compression (EBLC) strategy to dynamically change the video compression level depending on different environmental conditions to maintain a high pedestrian detection accuracy. We conduct a case study to show the efficacy of our dynamic EBLC strategy for real-time vision-based pedestrian detection under adverse environmental conditions. Our strategy dynamically selects the lossy compression error tolerances that maintain a high detection accuracy across a representative set of environmental conditions. Analyses reveal that for adverse environmental conditions, our dynamic EBLC strategy increases pedestrian detection accuracy up to 14% and reduces the communication bandwidth up to 14 × compared to the state-of-the-practice. Moreover, we show our dynamic EBLC strategy is independent of pedestrian detection models and environmental conditions allowing other detection models and environmental conditions to be easily incorporated.},
  archive      = {J_JRTIP},
  author       = {Rahman, Mizanur and Islam, Mhafuzul and Holt, Cavender and Calhoun, Jon and Chowdhury, Mashrur},
  doi          = {10.1007/s11554-021-01165-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {117-131},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dynamic error-bounded lossy compression to reduce the bandwidth requirement for real-time vision-based pedestrian safety applications},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smartphone-based real-time object recognition architecture
for portable and constrained systems. <em>JRTIP</em>, <em>19</em>(1),
103–115. (<a href="https://doi.org/10.1007/s11554-021-01164-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms based on convolutional neural networks (CNNs) have recently been explored in a myriad of object detection applications. Nonetheless, many devices with limited computation resources and strict power consumption constraints are not suitable to run such algorithms designed for high-performance computers. Hence, a novel smartphone-based architecture intended for portable and constrained systems is designed and implemented to run CNN-based object recognition in real time and with high efficiency. The system is designed and optimised by leveraging the integration of the best of its kind from the state-of-the-art machine learning platforms including OpenCV, TensorFlow Lite, and Qualcomm Snapdragon informed by empirical testing and evaluation of each candidate framework in a comparable scenario with a high demanding neural network. The final system has been prototyped combining the strengths from these frameworks and led to a new machine learning-based object recognition execution environment embedded in a smartphone with advantageous performance compared with the previous frameworks.},
  archive      = {J_JRTIP},
  author       = {Martinez-Alpiste, Ignacio and Golcarenarenji, Gelayol and Wang, Qi and Alcaraz-Calero, Jose Maria},
  doi          = {10.1007/s11554-021-01164-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {103-115},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Smartphone-based real-time object recognition architecture for portable and constrained systems},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hardware implementation of PSO-based approximate DST
transform for VVC standard. <em>JRTIP</em>, <em>19</em>(1), 87–101. (<a
href="https://doi.org/10.1007/s11554-021-01160-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The H.266/Versatile Video Coding (VVC) standard, released in July 2020, has improved the encoder performance over the previous High Efficiency Video Coding (HEVC) with a significant increase in coding complexity. Enhancements on the transform module mainly involve the introduction of the Adaptive Multiple Transform (AMT) which has led to an additional computational complexity. This paper aims at reducing the transform module complexity by approximating the AMT core. The transform approximation has to reach a low MSE, a low total error energy, a low transform distortion and a high transform efficiency. The Particle Swarm Optimization (PSO) is used to solve the optimization problem modeled as a constrained one. The proposed approximate transforms preserve a good coding efficiency compared to the exact transforms and require a less arithmetic complexity as well. The hardware architectures of both the exact and the approximate versions of the 8, and 16-point DST VII transform are designed. The exact transforms are defined using multipliers and MCM-based designs. The approximate transforms are described using additions and bit-shifting operations. All the designs are implemented in the Arria 10 FPGA device. Synthesis results have shown that the proposed approximation saves more than 75% and 63% of logic utilization when compared to multipliers and MCM-based designs, respectively. The maximum operational frequency is of 180 MHz, supporting 2K and 4K videos at 231 and 58 fps, respectively.},
  archive      = {J_JRTIP},
  author       = {Ben Jdidia, Sonda and Belghith, Fatma and Sallem, Amin and Jridi, Maher and Masmoudi, Nouri},
  doi          = {10.1007/s11554-021-01160-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {87-101},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware implementation of PSO-based approximate DST transform for VVC standard},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A compact CNN approach for drone localisation in autonomous
drone racing. <em>JRTIP</em>, <em>19</em>(1), 73–86. (<a
href="https://doi.org/10.1007/s11554-021-01162-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous drone racing, a drone flies through a gate track at high speed. Some solutions involve the use of camera localisation to control the drone. However, effective localisation is very demanding in processing time, which may compromise the flight speed. To address the latter, we propose a deep learning-based method for camera localisation that processes a small sequence of images of the scene at high-frequency operation. Our solution is a compact convolutional neural network based on the Inception network that uses a sequence of grey-scale images rather than colour images as input; we have called this network ‘GreySeqNet’. Our approach aims at leveraging the localisation process using a small stack of consecutive images fed as input to the network. To save computational effort, we explore the use of grey images instead of colour images, thus saving convolutional layers. We have conducted experiments in a simulated environment to measure the performance of GreySeqNet in different race tracks with variations in gate’s height and position. According to the results obtained in several test runs, our method achieves a camera pose estimation at an average operation frequency of 83 Hz running on GPU and 26 Hz on CPU, with an average camera pose error of 31 cm.},
  archive      = {J_JRTIP},
  author       = {Cocoma-Ortega, J. Arturo and Martinez-Carranza, J.},
  doi          = {10.1007/s11554-021-01162-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {73-86},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A compact CNN approach for drone localisation in autonomous drone racing},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient binary 3D convolutional neural network and
hardware accelerator. <em>JRTIP</em>, <em>19</em>(1), 61–71. (<a
href="https://doi.org/10.1007/s11554-021-01161-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimensional convolutional neural networks have abundant parameters and computational costs. It is urgent to compress the three-dimensional convolutional neural network. In this paper, an efficient and simple binary three-dimensional convolutional neural network architecture is proposed, in which the weight and activation are constrained to 0 or 1 instead of the common + 1 or – 1. Binary weight and activation are first applied to the three-dimensional convolutional neural networks. The proposed binary three-dimensional convolutional neural network has less computational complexity and memory consumption than standard convolution, and it is more appropriate for digital hardware design. Furthermore, an optimized convolution operation is proposed, in which case one input pixel is only required to be read once. A distributed storage approach is proposed to support the proposed convolution operation. With the proposed methods, a hardware accelerator for the binary three-dimensional convolutional neural network on the field programmable gate array platform is designed. The experimental results show that the presented accelerator is excellent in terms of computational resources and power efficiency. By jointly optimizing the algorithm and hardware, the accelerator achieves 89.2% accuracy and 384 frames per second on the KTH dataset.},
  archive      = {J_JRTIP},
  author       = {Li, Guoqing and Zhang, Meng and Zhang, Qianru and Lin, Zhijian},
  doi          = {10.1007/s11554-021-01161-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {61-71},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient binary 3D convolutional neural network and hardware accelerator},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid graphics/video rate control method based on
graphical assets for cloud gaming. <em>JRTIP</em>, <em>19</em>(1),
41–59. (<a href="https://doi.org/10.1007/s11554-021-01159-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Hybrid Cloud Gaming (HCG), as long as the graphical assets are available at the client side, rendering can be performed locally. However, if the client is not able to render all the frames in time, this may result in the frame rate to drop under the target value. This paper presents an Asset-based frame-level hybrid graphics/video rate control method for HCG, referred to as AHCG, which aims at improving the Quality of Experience (QoE) of players by tapping into the available processing power at the client side, while keeping a steady frame rate for thin clients and taking full advantage of the user’s tolerable delay. In the proposed client/server model, graphics data are intercepted and streamed in an asset-based approach. This approach handles the rate control issue per asset which is defined to be 3D object model data, textures, and shader programs. Rendering a frame on the client side not only maintains original quality for that frame, but it also reduces bandwidth requirements of the entire service by reusing the same assets for different frames. In the proposed method, if the accumulated rendering delay at the client side violates the tolerable delay set by the user, video streaming is used to compensate for the client device’s lack of processing power. Furthermore, quality fluctuation between graphics and video frames is addressed to provide a seamless experience when switching between graphics and video streaming. Several objective and subjective tests are conducted and the experimental results show a 20-fps increase in frame rate while maintaining a minimum value of 58 fps, with a minimum of 0.25 units improvement in the pooled standard deviation of SSIM values, compared to existing HCGs. Also, the subjective tests suggest an average 5.62 percent improvement in MOS compared to best HCG methods.},
  archive      = {J_JRTIP},
  author       = {Mohammadi, Iman Soltani and Ghanbari, Mohammad and Hashemi, Mahmoud Reza},
  doi          = {10.1007/s11554-021-01159-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {41-59},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A hybrid graphics/video rate control method based on graphical assets for cloud gaming},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time low-power binocular stereo vision based on FPGA.
<em>JRTIP</em>, <em>19</em>(1), 29–39. (<a
href="https://doi.org/10.1007/s11554-021-01158-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binocular stereo vision is a commonly applied computer vision technique with a wide range of applications in 3D scene perception. However, binocular stereo matching algorithms are computationally intensive and complicated. In addition, some traditional platforms are unable to meet the real-time and energy efficient dual requirements. In this paper, we proposed a hardware/software co-design FPGA (Field Programmable Gate Array) approach to overcome these limitations. Based on the characteristics of binocular stereo vision, we modularize the system functions to achieve the hardware/software partitioning. This accelerates the data processing on the FPGA, while simultaneously performing data control on the ARM (Advanced RISC Machine) cores. The parallelism of the FPGA allows for a full-pipeline design that is synchronized with an identical system clock for the simultaneous running of multiple stereo processing components, thus improving the processing speed. Furthermore, to minimize hardware costs, the collected images and data are compressed prior to matching, while the precision is subsequently enhanced during post-processing. The proposed system was evaluated on the PYNQ-Z2 development board, with experimental results revealing its high real-time performance and low power consumption for a 100M clock frequency. Compared with existing designs, the simple yet flexible system demonstrated a higher image processing speed and less hardware resource overhead (thus lower power consumption). The average error rate of the BM matching algorithm was also improved, particularly with the limited PYNQ-Z2 hardware resource. The proposed system has been opened on GitHub.},
  archive      = {J_JRTIP},
  author       = {Wu, Gang and Yang, Jinglei and Yang, Hao},
  doi          = {10.1007/s11554-021-01158-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {29-39},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time low-power binocular stereo vision based on FPGA},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast computation of 3D tchebichef moments for higher orders.
<em>JRTIP</em>, <em>19</em>(1), 15–27. (<a
href="https://doi.org/10.1007/s11554-021-01152-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new method for the fast and efficient calculation of 3D Tchebichef moments,which are an essential tool for the characterization and analysis of 3D objects. This method integrates the Kronecker tensor product to the computation of 3D Tchebichef moments for higher orders with the advantage of being parallelizable. The experimental results clearly show the benefits and efficacy of the proposed method compared to existing methods.},
  archive      = {J_JRTIP},
  author       = {Rivera-Lopez, J. Saúl and Camacho-Bello, César and Vargas-Vargas, Horlando and Escamilla-Noriega, Alicia},
  doi          = {10.1007/s11554-021-01152-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {15-27},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast computation of 3D tchebichef moments for higher orders},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast-PLDN: Fast power line detection network.
<em>JRTIP</em>, <em>19</em>(1), 3–13. (<a
href="https://doi.org/10.1007/s11554-021-01154-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obstacle detection, especially real-time power line detection plays a vital role in the low-altitude flight safety of aircrafts. Most of previous power line detection methods fail to deal with curved power lines due to the small size and unapparent visual features in the complex scene. In the paper, we propose a novel fast power line detection network (Fast-PLDN), a real-time semantic segmentation model, for pixel-wise straight and curved power line detection. Besides, we construct our network with low-high pass block and edge attention fusion module, which extract spatial and semantic information effectively to improve the power line detection result along the boundary. Furthermore, we also build up a new dataset named AIR Power Line dataset based on pixel-wise annotations for power line detection task because public Power Line dataset based on pixel-wise annotations is so limited. Our model can run at 189.6 frames per second (fps) with 71.3% mean intersection over union (mIoU) on AIR Power Line dataset, which outperforms most of the previous power line detection methods and the existing real-time semantic segmentation models.},
  archive      = {J_JRTIP},
  author       = {Zhu, Kejian and Xu, Chenghua and Wei, Yucheng and Cai, Gang},
  doi          = {10.1007/s11554-021-01154-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {3-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast-PLDN: Fast power line detection network},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Journal of real-time image processing: First issue of
volume 19. <em>JRTIP</em>, <em>19</em>(1), 1. (<a
href="https://doi.org/10.1007/s11554-022-01197-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-022-01197-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: First issue of volume 19},
  volume       = {19},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
