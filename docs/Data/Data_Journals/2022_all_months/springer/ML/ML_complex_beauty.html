<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---147">ML - 147</h2>
<ul>
<li><details>
<summary>
(2022). Optimistic optimisation of composite objective with
exponentiated update. <em>ML</em>, <em>111</em>(12), 4719–4764. (<a
href="https://doi.org/10.1007/s10994-022-06229-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new family of algorithms for the online optimisation of composite objectives. The algorithms can be interpreted as the combination of the exponentiated gradient and p-norm algorithm. Combined with algorithmic ideas of adaptivity and optimism, the proposed algorithms achieve a sequence-dependent regret upper bound, matching the best-known bounds for sparse target decision variables. Furthermore, the algorithms have efficient implementations for popular composite objectives and constraints and can be converted to stochastic optimisation algorithms with the optimal accelerated rate for smooth objectives.},
  archive      = {J_ML},
  author       = {Shao, Weijia and Sivrikaya, Fikret and Albayrak, Sahin},
  doi          = {10.1007/s10994-022-06229-1},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4719-4764},
  shortjournal = {Mach. Learn.},
  title        = {Optimistic optimisation of composite objective with exponentiated update},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning with risks based on m-location. <em>ML</em>,
<em>111</em>(12), 4679–4718. (<a
href="https://doi.org/10.1007/s10994-022-06217-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a new class of risks defined in terms of the location and deviation of the loss distribution, generalizing far beyond classical mean-variance risk functions. The class is easily implemented as a wrapper around any smooth loss, it admits finite-sample stationarity guarantees for stochastic gradient methods, it is straightforward to interpret and adjust, with close links to M-estimators of the loss location, and has a salient effect on the test loss distribution, giving us control over symmetry and deviations that are not possible under naive ERM.},
  archive      = {J_ML},
  author       = {Holland, Matthew J.},
  doi          = {10.1007/s10994-022-06217-5},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4679-4718},
  shortjournal = {Mach. Learn.},
  title        = {Learning with risks based on M-location},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variance reduction on general adaptive stochastic mirror
descent. <em>ML</em>, <em>111</em>(12), 4639–4677. (<a
href="https://doi.org/10.1007/s10994-022-06227-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a simple and generalized algorithmic framework for applying variance reduction to adaptive mirror descent algorithms for faster convergence. We introduce the SVRAMD algorithm, and provide its general convergence analysis in both the nonsmooth nonconvex optimization problem and the generalized P–L conditioned nonconvex optimization problem. We prove that variance reduction can reduce the gradient complexity of all adaptive mirror descent algorithms that satisfy a mild assumption and thus accelerate their convergence. In particular, our general theory implies that variance reduction can be applied to different algorithms with their distinct choices of the proximal function, such as gradient descent with time-varying step sizes, mirror descent with $$L_1$$ mirror maps, and self-adaptive algorithms such as AdaGrad and RMSProp. Moreover, the proved convergence rates of SVRAMD recover the existing rates without complicated algorithmic components, which indicates their optimality. Extensive experiments validate our theoretical findings.},
  archive      = {J_ML},
  author       = {Li, Wenjie and Wang, Zhanyu and Zhang, Yichen and Cheng, Guang},
  doi          = {10.1007/s10994-022-06227-3},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4639-4677},
  shortjournal = {Mach. Learn.},
  title        = {Variance reduction on general adaptive stochastic mirror descent},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation learning for clustering via building
consensus. <em>ML</em>, <em>111</em>(12), 4601–4638. (<a
href="https://doi.org/10.1007/s10994-022-06194-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on unsupervised representation learning for clustering of images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be close in the representation space (exemplar consistency), and/or similar images must have similar cluster assignments (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learned to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a single clustering algorithm. We define a clustering loss by executing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, consensus clustering using unsupervised representation learning (ConCURL), improves upon the clustering performance of state-of-the-art methods on four out of five image datasets. Furthermore, we extend the evaluation procedure for clustering to reflect the challenges encountered in real-world clustering tasks, such as maintaining clustering performance in cases with distribution shifts. We also perform a detailed ablation study for a deeper understanding of the proposed algorithm. The code and the trained models are available at https://github.com/JayanthRR/ConCURL_NCE .},
  archive      = {J_ML},
  author       = {Deshmukh, Aniket Anand and Regatti, Jayanth Reddy and Manavoglu, Eren and Dogan, Urun},
  doi          = {10.1007/s10994-022-06194-9},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4601-4638},
  shortjournal = {Mach. Learn.},
  title        = {Representation learning for clustering via building consensus},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BT-unet: A self-supervised learning framework for biomedical
image segmentation using barlow twins with u-net models. <em>ML</em>,
<em>111</em>(12), 4585–4600. (<a
href="https://doi.org/10.1007/s10994-022-06219-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has brought the most profound contribution towards biomedical image segmentation to automate the process of delineation in medical imaging. To accomplish such task, the models are required to be trained using huge amount of annotated or labelled data that highlights the region of interest with a binary mask. However, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this problem, a self-supervised learning framework, BT-Unet is proposed that uses the Barlow Twins approach to pre-train the encoder of a U-Net model via redundancy reduction in an unsupervised manner to learn data representation. Later, complete network is fine-tuned to perform actual segmentation. The BT-Unet framework can be trained with a limited number of annotated samples while having high number of unannotated samples, which is mostly the case in real-world problems. This framework is validated over multiple U-Net models over diverse datasets by generating scenarios of a limited number of labelled samples using standard evaluation metrics. With exhaustive experiment trials, it is observed that the BT-Unet framework enhances the performance of the U-Net models with significant margin under such circumstances.},
  archive      = {J_ML},
  author       = {Punn, Narinder Singh and Agarwal, Sonali},
  doi          = {10.1007/s10994-022-06219-3},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4585-4600},
  shortjournal = {Mach. Learn.},
  title        = {BT-unet: A self-supervised learning framework for biomedical image segmentation using barlow twins with U-net models},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up neural network robustness verification via
algorithm configuration and an optimised mixed integer linear
programming solver portfolio. <em>ML</em>, <em>111</em>(12), 4565–4584.
(<a href="https://doi.org/10.1007/s10994-022-06212-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their great success in recent years, neural networks have been found to be vulnerable to adversarial attacks. These attacks are often based on slight perturbations of given inputs that cause them to be misclassified. Several methods have been proposed to formally prove robustness of a given network against such attacks. However, these methods typically give rise to high computational demands, which severely limit their scalability. Recent state-of-the-art approaches state the verification task as a minimisation problem, which is formulated and solved as a mixed-integer linear programming (MIP) problem. We extend this approach by leveraging automated algorithm configuration techniques and, more specifically, construct a portfolio of MIP solver configurations optimised for the neural network verification task. We test this approach on two recent, state-of-the-art MIP-based verification engines, $$\mathrm {MIPVerify}$$ and $$\mathrm {Venus}$$ , and achieve substantial improvements in CPU time by average factors of up to 4.7 and 10.3, respectively.},
  archive      = {J_ML},
  author       = {König, Matthias and Hoos, Holger H. and Rijn, Jan N. van},
  doi          = {10.1007/s10994-022-06212-w},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4565-4584},
  shortjournal = {Mach. Learn.},
  title        = {Speeding up neural network robustness verification via algorithm configuration and an optimised mixed integer linear programming solver portfolio},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial examples for extreme multilabel text
classification. <em>ML</em>, <em>111</em>(12), 4539–4563. (<a
href="https://doi.org/10.1007/s10994-022-06263-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Multilabel Text Classification (XMTC) is a text classification problem in which, (i) the output space is extremely large, (ii) each data point may have multiple positive labels, and (iii) the data follows a strongly imbalanced distribution. With applications in recommendation systems and automatic tagging of web-scale documents, the research on XMTC has been focused on improving prediction accuracy and dealing with imbalanced data. However, the robustness of deep learning based XMTC models against adversarial examples has been largely underexplored. In this paper, we investigate the behaviour of XMTC models under adversarial attacks. To this end, first, we define adversarial attacks in multilabel text classification problems. We categorize attacking multilabel text classifiers as (a) positive-to-negative, where the target positive label should fall out of top-k predicted labels, and (b) negative-to-positive, where the target negative label should be among the top-k predicted labels. Then, by experiments on APLC-XLNet and AttentionXML, we show that XMTC models are highly vulnerable to positive-to-negative attacks but more robust to negative-to-positive ones. Furthermore, our experiments show that the success rate of positive-to-negative adversarial attacks has an imbalanced distribution. More precisely, tail classes are highly vulnerable to adversarial attacks for which an attacker can generate adversarial samples with high similarity to the actual data-points. To overcome this problem, we explore the effect of rebalanced loss functions in XMTC where not only do they increase accuracy on tail classes, but they also improve the robustness of these classes against adversarial attacks. The code for our experiments is available at https://github.com/xmc-aalto/adv-xmtc .},
  archive      = {J_ML},
  author       = {Qaraei, Mohammadreza and Babbar, Rohit},
  doi          = {10.1007/s10994-022-06263-z},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4539-4563},
  shortjournal = {Mach. Learn.},
  title        = {Adversarial examples for extreme multilabel text classification},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lead–lag detection and network clustering for multivariate
time series with an application to the US equity market. <em>ML</em>,
<em>111</em>(12), 4497–4538. (<a
href="https://doi.org/10.1007/s10994-022-06250-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate time series systems, it has been observed that certain groups of variables partially lead the evolution of the system, while other variables follow this evolution with a time delay; the result is a lead–lag structure amongst the time series variables. In this paper, we propose a method for the detection of lead–lag clusters of time series in multivariate systems. We demonstrate that the web of pairwise lead–lag relationships between time series can be helpfully construed as a directed network, for which there exist suitable algorithms for the detection of pairs of lead–lag clusters with high pairwise imbalance. Within our framework, we consider a number of choices for the pairwise lead–lag metric and directed network clustering model components. Our framework is validated on both a synthetic generative model for multivariate lead–lag time series systems and daily real-world US equity prices data. We showcase that our method is able to detect statistically significant lead–lag clusters in the US equity market. We study the nature of these clusters in the context of the empirical finance literature on lead–lag relations, and demonstrate how these can be used for the construction of predictive financial signals.},
  archive      = {J_ML},
  author       = {Bennett, Stefanos and Cucuringu, Mihai and Reinert, Gesine},
  doi          = {10.1007/s10994-022-06250-4},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4497-4538},
  shortjournal = {Mach. Learn.},
  title        = {Lead–lag detection and network clustering for multivariate time series with an application to the US equity market},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Matrix-wise <span
class="math display"><em>ℓ</em><sub>0</sub></span> -constrained sparse
nonnegative least squares. <em>ML</em>, <em>111</em>(12), 4453–4495. (<a
href="https://doi.org/10.1007/s10994-022-06260-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative least squares problems with multiple right-hand sides (MNNLS) arise in models that rely on additive linear combinations. In particular, they are at the core of most nonnegative matrix factorization algorithms and have many applications. The nonnegativity constraint is known to naturally favor sparsity, that is, solutions with few non-zero entries. However, it is often useful to further enhance this sparsity, as it improves the interpretability of the results and helps reducing noise, which leads to the sparse MNNLS problem. In this paper, as opposed to most previous works that enforce sparsity column- or row-wise, we first introduce a novel formulation for sparse MNNLS, with a matrix-wise sparsity constraint. Then, we present a two-step algorithm to tackle this problem. The first step divides sparse MNNLS in subproblems, one per column of the original problem. It then uses different algorithms to produce, either exactly or approximately, a Pareto front for each subproblem, that is, to produce a set of solutions representing different tradeoffs between reconstruction error and sparsity. The second step selects solutions among these Pareto fronts in order to build a sparsity-constrained matrix that minimizes the reconstruction error. We perform experiments on facial and hyperspectral images, and we show that our proposed two-step approach provides more accurate results than state-of-the-art sparse coding heuristics applied both column-wise and globally.},
  archive      = {J_ML},
  author       = {Nadisic, Nicolas and Cohen, Jeremy E. and Vandaele, Arnaud and Gillis, Nicolas},
  doi          = {10.1007/s10994-022-06260-2},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4453-4495},
  shortjournal = {Mach. Learn.},
  title        = {Matrix-wise $$\ell _0$$ -constrained sparse nonnegative least squares},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A brain-inspired algorithm for training highly sparse neural
networks. <em>ML</em>, <em>111</em>(12), 4411–4452. (<a
href="https://doi.org/10.1007/s10994-022-06266-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse neural networks attract increasing interest as they exhibit comparable performance to their dense counterparts while being computationally efficient. Pruning the dense neural networks is among the most widely used methods to obtain a sparse neural network. Driven by the high training cost of such methods that can be unaffordable for a low-resource device, training sparse neural networks sparsely from scratch has recently gained attention. However, existing sparse training algorithms suffer from various issues, including poor performance in high sparsity scenarios, computing dense gradient information during training, or pure random topology search. In this paper, inspired by the evolution of the biological brain and the Hebbian learning theory, we present a new sparse training approach that evolves sparse neural networks according to the behavior of neurons in the network. Concretely, by exploiting the cosine similarity metric to measure the importance of the connections, our proposed method, “Cosine similarity-based and random topology exploration (CTRE)”, evolves the topology of sparse neural networks by adding the most important connections to the network without calculating dense gradient in the backward. We carried out different experiments on eight datasets, including tabular, image, and text datasets, and demonstrate that our proposed method outperforms several state-of-the-art sparse training algorithms in extremely sparse neural networks by a large gap. The implementation code is available on Github.},
  archive      = {J_ML},
  author       = {Atashgahi, Zahra and Pieterse, Joost and Liu, Shiwei and Mocanu, Decebal Constantin and Veldhuis, Raymond and Pechenizkiy, Mykola},
  doi          = {10.1007/s10994-022-06266-w},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4411-4452},
  shortjournal = {Mach. Learn.},
  title        = {A brain-inspired algorithm for training highly sparse neural networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SVRG meets AdaGrad: Painless variance reduction.
<em>ML</em>, <em>111</em>(12), 4359–4409. (<a
href="https://doi.org/10.1007/s10994-022-06265-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variance reduction (VR) methods for finite-sum minimization typically require the knowledge of problem-dependent constants that are often unknown and difficult to estimate. To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method. AdaSVRG uses AdaGrad, a common adaptive gradient method, in the inner loop of SVRG, making it robust to the choice of step-size. When minimizing a sum of n smooth convex functions, we prove that a variant of AdaSVRG requires $$\tilde{O}(n + 1/\epsilon )$$ gradient evaluations to achieve an $$O(\epsilon )$$ -suboptimality, matching the typical rate, but without needing to know problem-dependent constants. Next, we show that the dynamics of AdaGrad exhibit a two-phase behavior – the step-size remains approximately constant in the first phase, and then decreases at a $$O\left( {1}/{\sqrt{t}}\right)$$ rate. This result maybe of independent interest, and allows us to propose a heuristic that adaptively determines the length of each inner-loop in AdaSVRG. Via experiments on synthetic and real-world datasets, we validate the robustness and effectiveness of AdaSVRG, demonstrating its superior performance over standard and other “tune-free” VR methods.},
  archive      = {J_ML},
  author       = {Dubois-Taine, Benjamin and Vaswani, Sharan and Babanezhad, Reza and Schmidt, Mark and Lacoste-Julien, Simon},
  doi          = {10.1007/s10994-022-06265-x},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4359-4409},
  shortjournal = {Mach. Learn.},
  title        = {SVRG meets AdaGrad: Painless variance reduction},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian mixture variational autoencoders for multi-modal
learning. <em>ML</em>, <em>111</em>(12), 4329–4357. (<a
href="https://doi.org/10.1007/s10994-022-06272-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an in-depth analysis on how to effectively acquire and generalize cross-modal knowledge for multi-modal learning. Mixture-of-Expert (MoE) and Product-of-Expert (PoE) are two popular directions in generalizing multi-modal information. Existing works based on MoE or PoE have shown notable improvement on data generation, while new challenges such as high training cost, overconfident experts, and encoding modal-specific features also emerge. In this work, we propose Bayesian mixture variational autoencoder (BMVAE) which learns to select or combine experts via Bayesian inference. We show that the proposed idea can naturally encourage models to learn modal-specific knowledge and avoid overconfident experts. Also, we show that the idea is compatible with both MoE and PoE frameworks. When being a MoE model, BMVAE can be optimized by a tight lower bound and is efficient to train. The PoE BMVAE has the same advantages and a theoretical connection to existing works. In the experiments, we show that BMVAE achieves state-of-the-art performance.},
  archive      = {J_ML},
  author       = {Liao, Keng-Te and Huang, Bo-Wei and Yang, Chih-Chun and Lin, Shou-De},
  doi          = {10.1007/s10994-022-06272-y},
  journal      = {Machine Learning},
  number       = {12},
  pages        = {4329-4357},
  shortjournal = {Mach. Learn.},
  title        = {Bayesian mixture variational autoencoders for multi-modal learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet-packets for deepfake image analysis and detection.
<em>ML</em>, <em>111</em>(11), 4295–4327. (<a
href="https://doi.org/10.1007/s10994-022-06225-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As neural networks become able to generate realistic artificial images, they have the potential to improve movies, music, video games and make the internet an even more creative and inspiring place. Yet, the latest technology potentially enables new digital ways to lie. In response, the need for a diverse and reliable method toolbox arises to identify artificial images and other content. Previous work primarily relies on pixel-space convolutional neural networks or the Fourier transform. To the best of our knowledge, synthesized fake image analysis and detection methods based on a multi-scale wavelet-packet representation, localized in both space and frequency, have been absent thus far. The wavelet transform conserves spatial information to a degree, allowing us to present a new analysis. Comparing the wavelet coefficients of real and fake images allows interpretation. Significant differences are identified. Additionally, this paper proposes to learn a model for the detection of synthetic images based on the wavelet-packet representation of natural and generated images. Our forensic classifiers exhibit competitive or improved performance at small network sizes, as we demonstrate on the Flickr Faces High Quality, Large-scale Celeb Faces Attributes and Large-scale Scene UNderstanding source identification problems. Furthermore, we study the binary Face Forensics++ (ff++) fake-detection problem.},
  archive      = {J_ML},
  author       = {Wolter, Moritz and Blanke, Felix and Heese, Raoul and Garcke, Jochen},
  doi          = {10.1007/s10994-022-06225-5},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4295-4327},
  shortjournal = {Mach. Learn.},
  title        = {Wavelet-packets for deepfake image analysis and detection},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An accurate, scalable and verifiable protocol for federated
differentially private averaging. <em>ML</em>, <em>111</em>(11),
4249–4293. (<a
href="https://doi.org/10.1007/s10994-022-06267-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from data owned by several parties, as in federated learning, raises challenges regarding the privacy guarantees provided to participants and the correctness of the computation in the presence of malicious parties. We tackle these challenges in the context of distributed averaging, an essential building block of federated learning algorithms. Our first contribution is a scalable protocol in which participants exchange correlated Gaussian noise along the edges of a graph, complemented by independent noise added by each party. We analyze the differential privacy guarantees of our protocol and the impact of the graph topology under colluding malicious parties, showing that we can nearly match the utility of the trusted curator model even when each honest party communicates with only a logarithmic number of other parties chosen at random. This is in contrast with protocols in the local model of privacy (with lower utility) or based on secure aggregation (where all pairs of users need to exchange messages). Our second contribution enables users to prove the correctness of their computations without compromising the efficiency and privacy guarantees of the protocol. Our construction relies on standard cryptographic primitives like commitment schemes and zero knowledge proofs.},
  archive      = {J_ML},
  author       = {Sabater, César and Bellet, Aurélien and Ramon, Jan},
  doi          = {10.1007/s10994-022-06267-9},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4249-4293},
  shortjournal = {Mach. Learn.},
  title        = {An accurate, scalable and verifiable protocol for federated differentially private averaging},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A taxonomy for similarity metrics between markov decision
processes. <em>ML</em>, <em>111</em>(11), 4217–4247. (<a
href="https://doi.org/10.1007/s10994-022-06242-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the notion of task similarity is potentially interesting in a wide range of areas such as curriculum learning or automated planning, it has mostly been tied to transfer learning. Transfer is based on the idea of reusing the knowledge acquired in the learning of a set of source tasks to a new learning process in a target task, assuming that the target and source tasks are close enough. In recent years, transfer learning has succeeded in making reinforcement learning (RL) algorithms more efficient (e.g., by reducing the number of samples needed to achieve (near-)optimal performance). Transfer in RL is based on the core concept of similarity: whenever the tasks are similar, the transferred knowledge can be reused to solve the target task and significantly improve the learning performance. Therefore, the selection of good metrics to measure these similarities is a critical aspect when building transfer RL algorithms, especially when this knowledge is transferred from simulation to the real world. In the literature, there are many metrics to measure the similarity between MDPs, hence, many definitions of similarity or its complement distance have been considered. In this paper, we propose a categorization of these metrics and analyze the definitions of similarity proposed so far, taking into account such categorization. We also follow this taxonomy to survey the existing literature, as well as suggesting future directions for the construction of new metrics.},
  archive      = {J_ML},
  author       = {García, Javier and Visús, Álvaro and Fernández, Fernando},
  doi          = {10.1007/s10994-022-06242-4},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4217-4247},
  shortjournal = {Mach. Learn.},
  title        = {A taxonomy for similarity metrics between markov decision processes},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning in corporate credit rating assessment using
the expanded audit report. <em>ML</em>, <em>111</em>(11), 4183–4215. (<a
href="https://doi.org/10.1007/s10994-022-06226-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate whether key audit matter (KAM) paragraphs disclosed in extended audit reports—paragraphs in which the auditor highlights significant risks and critical judgments of the company—contribute to assess corporate credit ratings. This assessment is a complicated and expensive process to grade the reliability of a company, and it is relevant for many stakeholders, such as issuers, investors, and creditors. Although credit rating evaluations have attracted the interest of many researchers, previous studies have mainly focused only on financial ratios. We are the first to use KAMs for credit rating modelling purposes. Applying four machine learning techniques to answer this real-world problem—C4.5 decision tree, two different rule induction classifiers (PART algorithm and Rough Set) and the logistic regression methodology—, our evidence suggests that by simply identifying the KAM topics disclosed in the report, any decision-maker can assess credit scores with 74\% accuracy using the rules provided by the PART algorithm. These rules specifically indicate that KAMs on both external (such as going concern) and internal (such as company debt) aspects may contribute to explaining a company’s credit rating. The rule induction classifiers have similar predictive power. Interestingly, if we combine audit data with accounting ratios, the predictive power of our model increases to 84\%, outperforming the accuracy in the existing literature.},
  archive      = {J_ML},
  author       = {Muñoz-Izquierdo, Nora and Segovia-Vargas, María Jesús and Camacho-Miñano, María-del-Mar and Pérez-Pérez, Yolanda},
  doi          = {10.1007/s10994-022-06226-4},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4183-4215},
  shortjournal = {Mach. Learn.},
  title        = {Machine learning in corporate credit rating assessment using the expanded audit report},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical optimal transport for unsupervised domain
adaptation. <em>ML</em>, <em>111</em>(11), 4159–4182. (<a
href="https://doi.org/10.1007/s10994-022-06231-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach for unsupervised domain adaptation that relates notions of optimal transport, learning probability measures, and unsupervised learning. The proposed approach, HOT-DA, is based on a hierarchical formulation of optimal transport that leverages beyond the geometrical information captured by the ground metric, richer structural information in the source and target domains. The additional information in the labeled source domain is formed instinctively by grouping samples into structures according to their class labels. While exploring hidden structures in the unlabeled target domain is reduced to the problem of learning probability measures through Wasserstein barycenter, which we prove to be equivalent to spectral clustering. Experiments show the superiority of the proposed approach over state-of-the-art across a range of domain adaptation problems including inter-twinning moons dataset, Digits, Office-Caltech, and Office-Home. Experiments also show the robustness of our model against structure imbalance. We make our code publicly available.},
  archive      = {J_ML},
  author       = {El Hamri, Mourad and Bennani, Younès and Falih, Issam},
  doi          = {10.1007/s10994-022-06231-7},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4159-4182},
  shortjournal = {Mach. Learn.},
  title        = {Hierarchical optimal transport for unsupervised domain adaptation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting survival outcomes in the presence of unlabeled
data. <em>ML</em>, <em>111</em>(11), 4139–4157. (<a
href="https://doi.org/10.1007/s10994-022-06257-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical studies require the follow-up of patients over time. This is challenging: apart from frequently observed drop-out, there are often also organizational and financial challenges, which can lead to reduced data collection and, in turn, can complicate subsequent analyses. In contrast, there is often plenty of baseline data available of patients with similar characteristics and background information, e.g., from patients that fall outside the study time window. In this article, we investigate whether we can benefit from the inclusion of such unlabeled data instances to predict accurate survival times. In other words, we introduce a third level of supervision in the context of survival analysis, apart from fully observed and censored instances, we also include unlabeled instances. We propose three approaches to deal with this novel setting and provide an empirical comparison over fifteen real-life clinical and gene expression survival datasets. Our results demonstrate that all approaches are able to increase the predictive performance over independent test data. We also show that integrating the partial supervision provided by censored data in a semi-supervised wrapper approach generally provides the best results, often achieving high improvements, compared to not using unlabeled data.},
  archive      = {J_ML},
  author       = {Nateghi Haredasht, Fateme and Vens, Celine},
  doi          = {10.1007/s10994-022-06257-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4139-4157},
  shortjournal = {Mach. Learn.},
  title        = {Predicting survival outcomes in the presence of unlabeled data},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothing policies and safe policy gradients. <em>ML</em>,
<em>111</em>(11), 4081–4137. (<a
href="https://doi.org/10.1007/s10994-022-06232-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy gradient (PG) algorithms are among the best candidates for the much-anticipated applications of reinforcement learning to real-world control tasks, such as robotics. However, the trial-and-error nature of these methods poses safety issues whenever the learning process itself must be performed on a physical system or involves any form of human-computer interaction. In this paper, we address a specific safety formulation, where both goals and dangers are encoded in a scalar reward signal and the learning agent is constrained to never worsen its performance, measured as the expected sum of rewards. By studying actor-only PG from a stochastic optimization perspective, we establish improvement guarantees for a wide class of parametric policies, generalizing existing results on Gaussian policies. This, together with novel upper bounds on the variance of PG estimators, allows us to identify meta-parameter schedules that guarantee monotonic improvement with high probability. The two key meta-parameters are the step size of the parameter updates and the batch size of the gradient estimates. Through a joint, adaptive selection of these meta-parameters, we obtain a PG algorithm with monotonic improvement guarantees.},
  archive      = {J_ML},
  author       = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
  doi          = {10.1007/s10994-022-06232-6},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4081-4137},
  shortjournal = {Mach. Learn.},
  title        = {Smoothing policies and safe policy gradients},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaling up stochastic gradient descent for non-convex
optimisation. <em>ML</em>, <em>111</em>(11), 4039–4079. (<a
href="https://doi.org/10.1007/s10994-022-06243-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is a widely adopted iterative method for optimizing differentiable objective functions. In this paper, we propose and discuss a novel approach to scale up SGD in applications involving non-convex functions and large datasets. We address the bottleneck problem arising when using both shared and distributed memory. Typically, the former is bounded by limited computation resources and bandwidth whereas the latter suffers from communication overheads. We propose a unified distributed and parallel implementation of SGD (named DPSGD) that relies on both asynchronous distribution and lock-free parallelism. By combining two strategies into a unified framework, DPSGD is able to strike a better trade-off between local computation and communication. The convergence properties of DPSGD are studied for non-convex problems such as those arising in statistical modelling and machine learning. Our theoretical analysis shows that DPSGD leads to speed-up with respect to the number of cores and number of workers while guaranteeing an asymptotic convergence rate of $$O(1/\sqrt{T})$$ given that the number of cores is bounded by $$T^{1/4}$$ and the number of workers is bounded by $$T^{1/2}$$ where T is the number of iterations. The potential gains that can be achieved by DPSGD are demonstrated empirically on a stochastic variational inference problem (Latent Dirichlet Allocation) and on a deep reinforcement learning (DRL) problem (advantage actor critic - A2C) resulting in two algorithms: DPSVI and HSA2C. Empirical results validate our theoretical findings. Comparative studies are conducted to show the performance of the proposed DPSGD against the state-of-the-art DRL algorithms.},
  archive      = {J_ML},
  author       = {Mohamad, Saad and Alamri, Hamad and Bouchachia, Abdelhamid},
  doi          = {10.1007/s10994-022-06243-3},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4039-4079},
  shortjournal = {Mach. Learn.},
  title        = {Scaling up stochastic gradient descent for non-convex optimisation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GENs: Generative encoding networks. <em>ML</em>,
<em>111</em>(11), 4003–4038. (<a
href="https://doi.org/10.1007/s10994-022-06220-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping data from and/or onto a known family of distributions has become an important topic in machine learning and data analysis. Deep generative models (e.g., generative adversarial networks) have been used effectively to match known and unknown distributions. Nonetheless, when the form of the target distribution is known, analytical methods are advantageous in providing robust results with provable properties. In this paper, we propose and analyze the use of nonparametric density methods to estimate the Jensen-Shannon divergence for matching unknown data distributions to known target distributions, such Gaussian or mixtures of Gaussians, in latent spaces. This analytical method has several advantages: better behavior when training sample quantity is low, provable convergence properties, and relatively few parameters, which can be derived analytically. Using the proposed method, we enforce the latent representation of an autoencoder to match a target distribution in a learning framework that we call a generative encoding network. Here, we present the numerical methods for bandwidth estimation; derive the expected distribution of the data in the latent space; show the advantages over the adversarial counterpart; study the properties of the latent space such as entropy, sample generation, interpolation; and demonstrate the application of the method in the real world.},
  archive      = {J_ML},
  author       = {Saha, Surojit and Elhabian, Shireen and Whitaker, Ross},
  doi          = {10.1007/s10994-022-06220-w},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {4003-4038},
  shortjournal = {Mach. Learn.},
  title        = {GENs: Generative encoding networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attacking neural machine translations via hybrid attention
learning. <em>ML</em>, <em>111</em>(11), 3977–4002. (<a
href="https://doi.org/10.1007/s10994-022-06249-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning based natural language processing (NLP) models are proven vulnerable to adversarial attacks. However, there is currently insufficient research that studies attacks to neural machine translations (NMTs) and examines the robustness of deep-learning based NMTs. In this paper, we aim to fill this critical research gap. When generating word-level adversarial examples in NLP attacks, there is a conventional trade-off in existing methods between the attacking performance and the amount of perturbations. Although some literature has studied such a trade-off and successfully generated adversarial examples with a reasonable amount of perturbations, it is still challenging to generate highly successful translation attacks while concealing the changes to the texts. To this end, we propose a novel Hybrid Attentive Attack method to locate language-specific and sequence-focused words, and make semantic-aware substitutions to attack NMTs. We evaluate the effectiveness of our attack strategy by attacking three high-performing translation models. The experimental results show that our method achieves the highest attacking performance compared with other existing attacking strategies.},
  archive      = {J_ML},
  author       = {Ni, Mingze and Wang, Ce and Zhu, Tianqing and Yu, Shui and Liu, Wei},
  doi          = {10.1007/s10994-022-06249-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3977-4002},
  shortjournal = {Mach. Learn.},
  title        = {Attacking neural machine translations via hybrid attention learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding-up one-versus-all training for extreme
classification via mean-separating initialization. <em>ML</em>,
<em>111</em>(11), 3953–3976. (<a
href="https://doi.org/10.1007/s10994-022-06228-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show that a simple, data dependent way of setting the initial vector can be used to substantially speed up the training of linear one-versus-all classifiers in extreme multi-label classification (XMC). We discuss the problem of choosing the initial weights from the perspective of three goals. We want to start in a region of weight space (a) with low loss value, (b) that is favourable for second-order optimization, and (c) where the conjugate-gradient (CG) calculations can be performed quickly. For margin losses, such an initialization is achieved by selecting the initial vector such that it separates the mean of all positive (relevant for a label) instances from the mean of all negatives – two quantities that can be calculated quickly for the highly imbalanced binary problems occurring in XMC. We demonstrate a training speedup of up to $$5\times$$ on Amazon-670K dataset with 670,000 labels. This comes in part from the reduced number of iterations that need to be performed due to starting closer to the solution, and in part from an implicit negative-mining effect that allows to ignore easy negatives in the CG step. Because of the convex nature of the optimization problem, the speedup is achieved without any degradation in classification accuracy. The implementation can be found at https://github.com/xmc-aalto/dismecpp .},
  archive      = {J_ML},
  author       = {Schultheis, Erik and Babbar, Rohit},
  doi          = {10.1007/s10994-022-06228-2},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3953-3976},
  shortjournal = {Mach. Learn.},
  title        = {Speeding-up one-versus-all training for extreme classification via mean-separating initialization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aliasing and adversarial robust generalization of CNNs.
<em>ML</em>, <em>111</em>(11), 3925–3951. (<a
href="https://doi.org/10.1007/s10994-022-06222-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many commonly well-performing convolutional neural network models have shown to be susceptible to input data perturbations, indicating a low model robustness. To reveal model weaknesses, adversarial attacks are specifically optimized to generate small, barely perceivable image perturbations that flip the model prediction. Robustness against attacks can be gained by using adversarial examples during training, which in most cases reduces the measurable model attackability. Unfortunately, this technique can lead to robust overfitting, which results in non-robust models. In this paper, we analyze adversarially trained, robust models in the context of a specific network operation, the downsampling layer, and provide evidence that robust models have learned to downsample more accurately and suffer significantly less from downsampling artifacts, aka. aliasing, than baseline models. In the case of robust overfitting, we observe a strong increase in aliasing and propose a novel early stopping approach based on the measurement of aliasing.},
  archive      = {J_ML},
  author       = {Grabinski, Julia and Keuper, Janis and Keuper, Margret},
  doi          = {10.1007/s10994-022-06222-8},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3925-3951},
  shortjournal = {Mach. Learn.},
  title        = {Aliasing and adversarial robust generalization of CNNs},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A user-guided bayesian framework for ensemble feature
selection in life science applications (UBayFS). <em>ML</em>,
<em>111</em>(10), 3897–3923. (<a
href="https://doi.org/10.1007/s10994-022-06221-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection reduces the complexity of high-dimensional datasets and helps to gain insights into systematic variation in the data. These aspects are essential in domains that rely on model interpretability, such as life sciences. We propose a (U)ser-Guided (Bay)esian Framework for (F)eature (S)election, UBayFS, an ensemble feature selection technique embedded in a Bayesian statistical framework. Our generic approach considers two sources of information: data and domain knowledge. From data, we build an ensemble of feature selectors, described by a multinomial likelihood model. Using domain knowledge, the user guides UBayFS by weighting features and penalizing feature blocks or combinations, implemented via a Dirichlet-type prior distribution. Hence, the framework combines three main aspects: ensemble feature selection, expert knowledge, and side constraints. Our experiments demonstrate that UBayFS (a) allows for a balanced trade-off between user knowledge and data observations and (b) achieves accurate and robust results.},
  archive      = {J_ML},
  author       = {Jenul, Anna and Schrunner, Stefan and Pilz, Jürgen and Tomic, Oliver},
  doi          = {10.1007/s10994-022-06221-9},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3897-3923},
  shortjournal = {Mach. Learn.},
  title        = {A user-guided bayesian framework for ensemble feature selection in life science applications (UBayFS)},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Greedy structure learning from data that contain systematic
missing values. <em>ML</em>, <em>111</em>(10), 3867–3896. (<a
href="https://doi.org/10.1007/s10994-022-06195-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from data that contain missing values represents a common phenomenon in many domains. Relatively few Bayesian Network structure learning algorithms account for missing data, and those that do tend to rely on standard approaches that assume missing data are missing at random, such as the Expectation-Maximisation algorithm. Because missing data are often systematic, there is a need for more pragmatic methods that can effectively deal with data sets containing missing values not missing at random. The absence of approaches that deal with systematic missing data impedes the application of BN structure learning methods to real-world problems where missingness are not random. This paper describes three variants of greedy search structure learning that utilise pairwise deletion and inverse probability weighting to maximally leverage the observed data and to limit potential bias caused by missing values. The first two of the variants can be viewed as sub-versions of the third and best performing variant, but are important in their own in illustrating the successive improvements in learning accuracy. The empirical investigations show that the proposed approach outperforms the commonly used and state-of-the-art Structural EM algorithm, both in terms of learning accuracy and efficiency, as well as both when data are missing at random and not at random.},
  archive      = {J_ML},
  author       = {Liu, Yang and Constantinou, Anthony C.},
  doi          = {10.1007/s10994-022-06195-8},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3867-3896},
  shortjournal = {Mach. Learn.},
  title        = {Greedy structure learning from data that contain systematic missing values},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified framework for online trip destination prediction.
<em>ML</em>, <em>111</em>(10), 3839–3865. (<a
href="https://doi.org/10.1007/s10994-022-06175-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trip destination prediction is an area of increasing importance in many applications such as trip planning, autonomous driving and electric vehicles. Even though this problem could be naturally addressed in an online learning paradigm where data is arriving in a sequential fashion, the majority of research has rather considered the offline setting. In this paper, we present a unified framework for trip destination prediction in an online setting, which is suitable for both online training and online prediction. For this purpose, we develop two clustering algorithms and integrate them within two online prediction models for this problem. We investigate the different configurations of clustering algorithms and prediction models on a real-world dataset. We demonstrate that both the clustering and the entire framework yield consistent results compared to the offline setting. Finally, we propose a novel regret metric for evaluating the entire online framework in comparison to its offline counterpart. This metric makes it possible to relate the source of erroneous predictions to either the clustering or the prediction model. Using this metric, we show that the proposed methods converge to a probability distribution resembling the true underlying distribution with a lower regret than all of the baselines.},
  archive      = {J_ML},
  author       = {Eberstein, Victor and Sjöblom, Jonas and Murgovski, Nikolce and Haghir Chehreghani, Morteza},
  doi          = {10.1007/s10994-022-06175-y},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3839-3865},
  shortjournal = {Mach. Learn.},
  title        = {A unified framework for online trip destination prediction},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifted model checking for relational MDPs. <em>ML</em>,
<em>111</em>(10), 3797–3838. (<a
href="https://doi.org/10.1007/s10994-021-06102-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic model checking has been developed for verifying systems that have stochastic and nondeterministic behavior. Given a probabilistic system, a probabilistic model checker takes a property and checks whether or not the property holds in that system. For this reason, probabilistic model checking provide rigorous guarantees. So far, however, probabilistic model checking has focused on propositional models where a state is represented by a symbol. On the other hand, it is commonly required to make relational abstractions in planning and reinforcement learning. Various frameworks handle relational domains, for instance, STRIPS planning and relational Markov Decision Processes. Using propositional model checking in relational settings requires one to ground the model, which leads to the well known state explosion problem and intractability. We present pCTL-REBEL, a lifted model checking approach for verifying pCTL properties of relational MDPs. It extends REBEL, a relational model-based reinforcement learning technique, toward relational pCTL model checking. PCTL-REBEL is lifted, which means that rather than grounding, the model exploits symmetries to reason about a group of objects as a whole at the relational level. Theoretically, we show that pCTL model checking is decidable for relational MDPs that have a possibly infinite domain, provided that the states have a bounded size. Practically, we contribute algorithms and an implementation of lifted relational model checking, and we show that the lifted approach improves the scalability of the model checking approach.},
  archive      = {J_ML},
  author       = {Yang, Wen-Chi and Raskin, Jean-François and De Raedt, Luc},
  doi          = {10.1007/s10994-021-06102-7},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3797-3838},
  shortjournal = {Mach. Learn.},
  title        = {Lifted model checking for relational MDPs},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust reputation independence in ranking systems for
multiple sensitive attributes. <em>ML</em>, <em>111</em>(10), 3769–3796.
(<a href="https://doi.org/10.1007/s10994-022-06173-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking systems have an unprecedented influence on how and what information people access, and their impact on our society is being analyzed from different perspectives, such as users’ discrimination. A notable example is represented by reputation-based ranking systems, a class of systems that rely on users’ reputation to generate a non-personalized item-ranking, proved to be biased against certain demographic classes. To safeguard that a given sensitive user’s attribute does not systematically affect the reputation of that user, prior work has operationalized a reputation independence constraint on this class of systems. In this paper, we uncover that guaranteeing reputation independence for a single sensitive attribute is not enough. When mitigating biases based on one sensitive attribute (e.g., gender), the final ranking might still be biased against certain demographic groups formed based on another attribute (e.g., age). Hence, we propose a novel approach to introduce reputation independence for multiple sensitive attributes simultaneously. We then analyze the extent to which our approach impacts on discrimination and other important properties of the ranking system, such as its quality and robustness against attacks. Experiments on two real-world datasets show that our approach leads to less biased rankings with respect to multiple users’ sensitive attributes, without affecting the system’s quality and robustness.},
  archive      = {J_ML},
  author       = {Ramos, Guilherme and Boratto, Ludovico and Marras, Mirko},
  doi          = {10.1007/s10994-022-06173-0},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3769-3796},
  shortjournal = {Mach. Learn.},
  title        = {Robust reputation independence in ranking systems for multiple sensitive attributes},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-parametric bayes regression with network-valued
covariates. <em>ML</em>, <em>111</em>(10), 3733–3767. (<a
href="https://doi.org/10.1007/s10994-022-06174-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there has been an explosive rise in network data in a variety of disciplines, there is very limited development of regression modeling approaches based on high-dimensional networks. The scarce literature in this area typically assume linear relationships between the outcome and the high-dimensional network edges that results in an inflated model plagued by the curse of dimensionality and these models are unable to accommodate non-linear relationships or higher order interactions. In order to overcome these limitations, we develop a novel two-stage Bayesian non-parametric regression modeling framework using high-dimensional networks as covariates, which first finds a lower dimensional node-specific representation for the networks, and then embeds these representations in a flexible Gaussian process regression framework along with supplemental covariates for modeling the continuous outcome variable. Moving from edge-level analysis to node-level model allows us to scale up to high-dimensional networks, and enables node selection via an extension of the Gaussian process framework that involves spike-and-slab priors on the lengthscale parameters. Extensive simulations show a distinct advantage of the proposed approach in terms of prediction, coverage, and node selection. The proposed model achieves considerable gains when predicting posttraumatic stress disorder (PTSD) resilience based on brain networks in our motivating neuroimaging applications, and also identifies important brain regions associated with PTSD. In contrast, existing non-linear approaches that employ the full-edge set or those that use other dimension reduction techniques on the network are not equipped for node selection and results in poor prediction and characterization of predictive uncertainty, while linear approaches using the edge-level features are overly inflated and typically result in poor performance.},
  archive      = {J_ML},
  author       = {Ma, Xin and Kundu, Suprateek and Stevens, Jennifer},
  doi          = {10.1007/s10994-022-06174-z},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3733-3767},
  shortjournal = {Mach. Learn.},
  title        = {Semi-parametric bayes regression with network-valued covariates},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Meta-interpretive learning as metarule specialisation.
<em>ML</em>, <em>111</em>(10), 3703–3731. (<a
href="https://doi.org/10.1007/s10994-022-06156-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Meta-interpretive learning (MIL) the metarules, second-order datalog clauses acting as inductive bias, are manually defined by the user. In this work we show that second-order metarules for MIL can be learned by MIL. We define a generality ordering of metarules by $$\theta$$ -subsumption and show that user-defined sort metarules are derivable by specialisation of the most-general matrix metarules in a language class; and that these matrix metarules are in turn derivable by specialisation of third-order punch metarules with variables quantified over the set of atoms and for which only an upper bound on their number of literals need be user-defined. We show that the cardinality of a metarule language is polynomial in the number of literals in punch metarules. We re-frame MIL as metarule specialisation by resolution. We modify the MIL metarule specialisation operator to return new metarules rather than first-order clauses and prove the correctness of the new operator. We implement the new operator as TOIL, a sub-system of the MIL system Louise. Our experiments show that as user-defined sort metarules are progressively replaced by sort metarules learned by TOIL, Louise’s predictive accuracy and training times are maintained. We conclude that automatically derived metarules can replace user-defined metarules.},
  archive      = {J_ML},
  author       = {Patsantzis, S. and Muggleton, S. H.},
  doi          = {10.1007/s10994-022-06156-1},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3703-3731},
  shortjournal = {Mach. Learn.},
  title        = {Meta-interpretive learning as metarule specialisation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient fair principal component analysis. <em>ML</em>,
<em>111</em>(10), 3671–3702. (<a
href="https://doi.org/10.1007/s10994-021-06100-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that dimension reduction methods such as Principal Component Analysis (PCA) may be inherently prone to unfairness and treat data from different sensitive groups such as race, color, sex, etc., unfairly. In pursuit of fairness-enhancing dimensionality reduction, using the notion of Pareto optimality, we propose an adaptive first-order algorithm to learn a subspace that preserves fairness, while slightly compromising the reconstruction loss. Theoretically, we provide sufficient conditions that the solution of the proposed algorithm belongs to the Pareto frontier for all sensitive groups; thereby, the optimal trade-off between overall reconstruction loss and fairness constraints is guaranteed. We also provide the convergence analysis of our algorithm and show its efficacy through empirical studies on different datasets, which demonstrates superior performance in comparison with state-of-the-art algorithms. The proposed fairness-aware PCA algorithm can be efficiently generalized to multiple group sensitive features and effectively reduce the unfairness decisions in downstream tasks such as classification.},
  archive      = {J_ML},
  author       = {Kamani, Mohammad Mahdi and Haddadpour, Farzin and Forsati, Rana and Mahdavi, Mehrdad},
  doi          = {10.1007/s10994-021-06100-9},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3671-3702},
  shortjournal = {Mach. Learn.},
  title        = {Efficient fair principal component analysis},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning any memory-less discrete semantics for dynamical
systems represented by logic programs. <em>ML</em>, <em>111</em>(10),
3593–3670. (<a
href="https://doi.org/10.1007/s10994-021-06105-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from interpretation transition (LFIT) automatically constructs a model of the dynamics of a system from the observation of its state transitions. So far the systems that LFIT handled were mainly restricted to synchronous deterministic dynamics. However, other dynamics exist in the field of logical modeling, in particular the asynchronous semantics which is widely used to model biological systems. In this paper, we propose a modeling of discrete memory-less multi-valued dynamic systems as logic programs in which a rule represents what can occur rather than what will occur. This modeling allows us to represent non-determinism and to propose an extension of LFIT to learn regardless of the update schemes, allowing to capture a large range of semantics. We also propose a second algorithm which is able to learn a whole system dynamics, including its semantics, in the form of a single propositional logic program with constraints. We show through theoretical results the correctness of our approaches. Practical evaluation is performed on benchmarks from biological literature.},
  archive      = {J_ML},
  author       = {Ribeiro, Tony and Folschette, Maxime and Magnin, Morgan and Inoue, Katsumi},
  doi          = {10.1007/s10994-021-06105-4},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3593-3670},
  shortjournal = {Mach. Learn.},
  title        = {Learning any memory-less discrete semantics for dynamical systems represented by logic programs},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A network-based positive and unlabeled learning approach for
fake news detection. <em>ML</em>, <em>111</em>(10), 3549–3592. (<a
href="https://doi.org/10.1007/s10994-021-06111-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news can rapidly spread through internet users and can deceive a large audience. Due to those characteristics, they can have a direct impact on political and economic events. Machine Learning approaches have been used to assist fake news identification. However, since the spectrum of real news is broad, hard to characterize, and expensive to label data due to the high update frequency, One-Class Learning (OCL) and Positive and Unlabeled Learning (PUL) emerge as an interesting approach for content-based fake news detection using a smaller set of labeled data than traditional machine learning techniques. In particular, network-based approaches are adequate for fake news detection since they allow incorporating information from different aspects of a publication to the problem modeling. In this paper, we propose a network-based approach based on Positive and Unlabeled Learning by Label Propagation (PU-LP), a one-class and transductive semi-supervised learning algorithm that performs classification by first identifying potential interest and non-interest documents into unlabeled data and then propagating labels to classify the remaining unlabeled documents. A label propagation approach is then employed to classify the remaining unlabeled documents. We assessed the performance of our proposal considering homogeneous (only documents) and heterogeneous (documents and terms) networks. Our comparative analysis considered four OCL algorithms extensively employed in One-Class text classification (k-Means, k-Nearest Neighbors Density-based, One-Class Support Vector Machine, and Dense Autoencoder), and another traditional PUL algorithm (Rocchio Support Vector Machine). The algorithms were evaluated in three news collections, considering balanced and extremely unbalanced scenarios. We used Bag-of-Words and Doc2Vec models to transform news into structured data. Results indicated that PU-LP approaches are more stable and achieve better results than other PUL and OCL approaches in most scenarios, performing similarly to semi-supervised binary algorithms. Also, the inclusion of terms in the news network activate better results, especially when news are distributed in the feature space considering veracity and subject. News representation using the Doc2Vec achieved better results than the Bag-of-Words model for both algorithms based on vector-space model and document similarity network.},
  archive      = {J_ML},
  author       = {de Souza, Mariana Caravanti and Nogueira, Bruno Magalhães and Rossi, Rafael Geraldeli and Marcacini, Ricardo Marcondes and dos Santos, Brucce Neves and Rezende, Solange Oliveira},
  doi          = {10.1007/s10994-021-06111-6},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3549-3592},
  shortjournal = {Mach. Learn.},
  title        = {A network-based positive and unlabeled learning approach for fake news detection},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale pinball twin support vector machines.
<em>ML</em>, <em>111</em>(10), 3525–3548. (<a
href="https://doi.org/10.1007/s10994-021-06061-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machines (TWSVMs) have been shown to be effective classifiers for a range of┬Āpattern classification tasks. However, the TWSVM formulation suffers from a range of shortcomings: (i) TWSVM uses hinge loss function which renders it sensitive to dataset outliers (noise sensitivity). (ii) It requires a matrix inversion calculation in the Wolfe-dual formulation which is intractable for datasets with large numbers of features/samples. (iii) TWSVM minimizes the empirical risk instead of the structural risk in its formulation with the consequent risk of overfitting. This paper proposes a novel large scale pinball twin support vector machines (LPTWSVM) to address these shortcomings. The proposed LPTWSVM model firstly utilizes the pinball loss function to achieve a high level of noise insensitivity, especially in relation to data with substantial feature noise. Secondly, and most significantly, the proposed LPTWSVM formulation eliminates the need to calculate inverse matrices in the dual problem (which apart from being very computationally demanding may not be possible due to matrix singularity). Further, LPTWSVM does not employ kernel-generated surfaces for the non-linear case, instead using the kernel trick directly; this ensures that the proposed LPTWSVM is a fully modular kernel approach in contrast to the original TWSVM. Lastly, structural risk is explicitly minimized in LPTWSVM with consequent improvement in classification accuracy (we explicitly analyze the properties of classification accuracy and noise insensitivity of the proposed LPTWSVM). Experiments on benchmark datasets show that the proposed LPTWSVM model may be effectively deployed on large datasets and that it exhibits similar or better performance on most datasets in comparison to relevant baseline methods.},
  archive      = {J_ML},
  author       = {Tanveer, M. and Tiwari, A. and Choudhary, R. and Ganaie, M. A.},
  doi          = {10.1007/s10994-021-06061-z},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3525-3548},
  shortjournal = {Mach. Learn.},
  title        = {Large-scale pinball twin support vector machines},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing and repairing concept drift adaptation in data
stream classification. <em>ML</em>, <em>111</em>(10), 3489–3523. (<a
href="https://doi.org/10.1007/s10994-021-05993-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected over time often exhibit changes in distribution, or concept drift, caused by changes in factors relevant to the classification task, e.g. weather conditions. Incorporating all relevant factors into the model may be able to capture these changes, however, this is usually not practical. Data stream based methods, which instead explicitly detect concept drift, have been shown to retain performance under unknown changing conditions. These methods adapt to concept drift by training a model to classify each distinct data distribution. However, we hypothesize that existing methods do not robustly handle real-world tasks, leading to adaptation errors where context is misidentified. Adaptation errors may cause a system to use a model which does not fit the current data, reducing performance. We propose a novel repair algorithm to identify and correct errors in concept drift adaptation. Evaluation on synthetic data shows that our proposed AiRStream system has higher performance than baseline methods, while is also better at capturing the dynamics of the stream. Evaluation on an air quality inference task shows AiRStream provides increased real-world performance compared to eight baseline methods. A case study shows that AiRStream is able to build a robust model of environmental conditions over this task, allowing the adaptions made to concept drift to be analysed and related to changes in weather. We discovered a strong predictive link between the adaptions made by AiRStream and changes in meteorological conditions.},
  archive      = {J_ML},
  author       = {Halstead, Ben and Koh, Yun Sing and Riddle, Patricia and Pears, Russel and Pechenizkiy, Mykola and Bifet, Albert and Olivares, Gustavo and Coulson, Guy},
  doi          = {10.1007/s10994-021-05993-w},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {3489-3523},
  shortjournal = {Mach. Learn.},
  title        = {Analyzing and repairing concept drift adaptation in data stream classification},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainable online ensemble of deep neural network pruning
for time series forecasting. <em>ML</em>, <em>111</em>(9), 3459–3487.
(<a href="https://doi.org/10.1007/s10994-022-06218-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both the complex and evolving nature of time series data make forecasting among one of the most challenging tasks in machine learning. Typical methods for forecasting are designed to model time-evolving dependencies between data observations. However, it is generally accepted that none of them are universally valid for every application. Therefore, methods for learning heterogeneous ensembles by combining a diverse set of forecasters together appears as a promising solution to tackle this task. While several approaches in the context of time series forecasting have focused on how to combine individual models in an ensemble, ranging from simple and enhanced averaging tactics to applying meta-learning methods, few works have tackled the task of ensemble pruning, i.e. individual model selection to take part in the ensemble. In addition, in classical ML literature, ensemble pruning techniques are mostly restricted to operate in a static manner. To deal with changes in the relative performance of models as well as changes in the data distribution, we employ gradient-based saliency maps for online ensemble pruning of deep neural networks. This method consists of generating individual models’ performance saliency maps that are subsequently used to prune the ensemble by taking into account both aspects of accuracy and diversity. In addition, the saliency maps can be exploited to provide suitable explanations for the reason behind selecting specific models to construct an ensemble that plays the role of a forecaster at a certain time interval or instant. An extensive empirical study on many real-world datasets demonstrates that our method achieves excellent or on par results in comparison to the state-of-the-art approaches as well as several baselines. Our code is available on Github ( https://github.com/MatthiasJakobs/os-pgsm/tree/ecml_journal_2022 ).},
  archive      = {J_ML},
  author       = {Saadallah, Amal and Jakobs, Matthias and Morik, Katharina},
  doi          = {10.1007/s10994-022-06218-4},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3459-3487},
  shortjournal = {Mach. Learn.},
  title        = {Explainable online ensemble of deep neural network pruning for time series forecasting},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the robustness of randomized classifiers to adversarial
examples. <em>ML</em>, <em>111</em>(9), 3425–3457. (<a
href="https://doi.org/10.1007/s10994-022-06216-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the theory of robustness against adversarial attacks. We focus on randomized classifiers (i.e. classifiers that output random variables) and provide a thorough analysis of their behavior through the lens of statistical learning theory and information theory. To this aim, we introduce a new notion of robustness for randomized classifiers, enforcing local Lipschitzness using probability metrics. Equipped with this definition, we make two new contributions. The first one consists in devising a new upper bound on the adversarial generalization gap of randomized classifiers. More precisely, we devise bounds on the generalization gap and the adversarial gap i.e. the gap between the risk and the worst-case risk under attack) of randomized classifiers. The second contribution presents a yet simple but efficient noise injection method to design robust randomized classifiers. We show that our results are applicable to a wide range of machine learning models under mild hypotheses. We further corroborate our findings with experimental results using deep neural networks on standard image datasets, namely CIFAR-10 and CIFAR-100. On these tasks, we manage to design robust models that simultaneously achieve state-of-the-art accuracy (over 0.82 clean accuracy on CIFAR-10) and enjoy guaranteed robust accuracy bounds (0.45 against $$\ell _{2}$$ adversaries with magnitude 0.5 on CIFAR-10).},
  archive      = {J_ML},
  author       = {Pinot, Rafael and Meunier, Laurent and Yger, Florian and Gouy-Pailler, Cédric and Chevaleyre, Yann and Atif, Jamal},
  doi          = {10.1007/s10994-022-06216-6},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3425-3457},
  shortjournal = {Mach. Learn.},
  title        = {On the robustness of randomized classifiers to adversarial examples},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recursive tree grammar autoencoders. <em>ML</em>,
<em>111</em>(9), 3393–3423. (<a
href="https://doi.org/10.1007/s10994-022-06223-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning on trees has been mostly focused on trees as input. Much less research has investigated trees as output, which has many applications, such as molecule optimization for drug discovery, or hint generation for intelligent tutoring systems. In this work, we propose a novel autoencoder approach, called recursive tree grammar autoencoder (RTG-AE), which encodes trees via a bottom-up parser and decodes trees via a tree grammar, both learned via recursive neural networks that minimize the variational autoencoder loss. The resulting encoder and decoder can then be utilized in subsequent tasks, such as optimization and time series prediction. RTG-AEs are the first model to combine three features: recursive processing, grammatical knowledge, and deep learning. Our key message is that this unique combination of all three features outperforms models which combine any two of the three. Experimentally, we show that RTG-AE improves the autoencoding error, training time, and optimization score on synthetic as well as real datasets compared to four baselines. We further prove that RTG-AEs parse and generate trees in linear time and are expressive enough to handle all regular tree grammars.},
  archive      = {J_ML},
  author       = {Paaßen, Benjamin and Koprinska, Irena and Yacef, Kalina},
  doi          = {10.1007/s10994-022-06223-7},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3393-3423},
  shortjournal = {Mach. Learn.},
  title        = {Recursive tree grammar autoencoders},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stabilize deep ResNet with a sharp scaling factor <span
class="math display"><em>τ</em></span>. <em>ML</em>, <em>111</em>(9),
3359–3392. (<a
href="https://doi.org/10.1007/s10994-022-06192-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the stability and convergence of training deep ResNets with gradient descent. Specifically, we show that the parametric branch in the residual block should be scaled down by a factor $$\tau =O(1/\sqrt{L})$$ to guarantee stable forward/backward process, where L is the number of residual blocks. Moreover, we establish a converse result that the forward process is unbounded when $$\tau &gt;L^{-\frac{1}{2}+c}$$ , for any positive constant c. The above two results together establish a sharp value of the scaling factor in determining the stability of deep ResNet. Based on the stability result, we further show that gradient descent finds the global minima if the ResNet is properly over-parameterized, which significantly improves over the previous work with a much larger range of $$\tau$$ that admits global convergence. Moreover, we show that the convergence rate is independent of the depth, theoretically justifying the advantage of ResNet over vanilla feedforward network. Empirically, with such a factor $$\tau$$ , one can train deep ResNet without normalization layer. Moreover for ResNets with normalization layer, adding such a factor $$\tau$$ also stabilizes the training and obtains significant performance gain for deep ResNet.},
  archive      = {J_ML},
  author       = {Zhang, Huishuai and Yu, Da and Yi, Mingyang and Chen, Wei and Liu, Tie-Yan},
  doi          = {10.1007/s10994-022-06192-x},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3359-3392},
  shortjournal = {Mach. Learn.},
  title        = {Stabilize deep ResNet with a sharp scaling factor $$\tau$$},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein-based fairness interpretability framework for
machine learning models. <em>ML</em>, <em>111</em>(9), 3307–3357. (<a
href="https://doi.org/10.1007/s10994-022-06213-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this article is to introduce a fairness interpretability framework for measuring and explaining the bias in classification and regression models at the level of a distribution. In our work, we measure the model bias across sub-population distributions in the model output using the Wasserstein metric. To properly quantify the contributions of predictors, we take into account favorability of both the model and predictors with respect to the non-protected class. The quantification is accomplished by the use of transport theory, which gives rise to the decomposition of the model bias and bias explanations to positive and negative contributions. To gain more insight into the role of favorability and allow for additivity of bias explanations, we adapt techniques from cooperative game theory.},
  archive      = {J_ML},
  author       = {Miroshnikov, Alexey and Kotsiopoulos, Konstandinos and Franks, Ryan and Ravi Kannan, Arjun},
  doi          = {10.1007/s10994-022-06213-9},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3307-3357},
  shortjournal = {Mach. Learn.},
  title        = {Wasserstein-based fairness interpretability framework for machine learning models},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The pure exploration problem with general reward functions
depending on full distributions. <em>ML</em>, <em>111</em>(9),
3279–3306. (<a
href="https://doi.org/10.1007/s10994-022-06214-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the pure exploration model with general distribution functions, which means that the reward function of each arm depends on the whole distribution, not only its mean. We adapt the elimination framework and the LUCB framework to solve this problem, and design algorithms for estimating the value of the reward functions with different types of distributions. Then we show that our estimation methods have the correctness guarantee with proper parameters, and obtain sample complexity upper bounds for them. The order of the complexity upper bounds matches the complexity lower bound when the error constraint $$\delta$$ converges to 0, which implies that our algorithms are asymptotically optimal. Finally, we discuss about some important applications and their corresponding solutions under our learning framework, and use some experiments to demonstrate the effectiveness of our algorithms. The experimental results show that our algorithms outperform existing solutions.},
  archive      = {J_ML},
  author       = {Wang, Siwei and Chen, Wei},
  doi          = {10.1007/s10994-022-06214-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3279-3306},
  shortjournal = {Mach. Learn.},
  title        = {The pure exploration problem with general reward functions depending on full distributions},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive polyak heavy-ball method. <em>ML</em>,
<em>111</em>(9), 3245–3277. (<a
href="https://doi.org/10.1007/s10994-022-06215-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heavy-ball (HB) method has become a well-known practice for large-scale machine learning problems, and it can achieve the fastest local convergence rate when objective functions are smooth and strongly convex using Polyak’s optimal hyper-parameters. However, such convergence rates are based on specific uncertain and time-invariant hyper-parameters that limit its potential. In this paper, we propose an adaptive HB that estimates the Polyak’s optimal hyper-parameters at each iteration. Our adaptive approach employs the absolute differences of current and previous model parameters and their gradients. Such representation allows for a computationally efficient optimizer. We show that our method guarantees a global linear convergence rate for smooth and strongly convex objective functions. Whereas in the stochastic setting, we show that proposed stochastic algorithm converges almost surely for non-convex smooth functions with bounded gradient. We validate the effectiveness of our method on image classification datasets with no empirical tuning, and its superiority on quadratic and non-convex functions while comparing its performance to the state-of-the-art optimizers.},
  archive      = {J_ML},
  author       = {Saab, Samer and Phoha, Shashi and Zhu, Minghui and Ray, Asok},
  doi          = {10.1007/s10994-022-06215-7},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3245-3277},
  shortjournal = {Mach. Learn.},
  title        = {An adaptive polyak heavy-ball method},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stateless neural meta-learning using second-order gradients.
<em>ML</em>, <em>111</em>(9), 3227–3244. (<a
href="https://doi.org/10.1007/s10994-022-06210-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning can be used to learn a good prior that facilitates quick learning; two popular approaches are MAML and the meta-learner LSTM. These two methods represent important and different approaches in meta-learning. In this work, we study the two and formally show that the meta-learner LSTM subsumes MAML, although MAML, which is in this sense less general, outperforms the other. We suggest the reason for this surprising performance gap is related to second-order gradients. We construct a new algorithm (named TURTLE) to gain more insight into the importance of second-order gradients. TURTLE is simpler than the meta-learner LSTM yet more expressive than MAML and outperforms both techniques at few-shot sine wave regression and 50\% of the tested image classification settings (without any additional hyperparameter tuning) and is competitive otherwise, at a computational cost that is comparable to second-order MAML. We find that second-order gradients also significantly increase the accuracy of the meta-learner LSTM. When MAML was introduced, one of its remarkable features was the use of second-order gradients. Subsequent work focused on cheaper first-order approximations. On the basis of our findings, we argue for more attention for second-order gradients.},
  archive      = {J_ML},
  author       = {Huisman, Mike and Plaat, Aske and van Rijn, Jan N.},
  doi          = {10.1007/s10994-022-06210-y},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3227-3244},
  shortjournal = {Mach. Learn.},
  title        = {Stateless neural meta-learning using second-order gradients},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine unlearning: Linear filtration for logit-based
classifiers. <em>ML</em>, <em>111</em>(9), 3203–3226. (<a
href="https://doi.org/10.1007/s10994-022-06178-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently enacted legislation grants individuals certain rights to decide in what fashion their personal data may be used and in particular a “right to be forgotten”. This poses a challenge to machine learning: how to proceed when an individual retracts permission to use data which has been part of the training process of a model? From this question emerges the field of machine unlearning, which could be broadly described as the investigation of how to “delete training data from models”. Our work complements this direction of research for the specific setting of class-wide deletion requests for classification models (e.g. deep neural networks). As a first step, we propose linear filtration as an intuitive, computationally efficient sanitization method. Our experiments demonstrate benefits in an adversarial setting over naive deletion schemes.},
  archive      = {J_ML},
  author       = {Baumhauer, Thomas and Schöttle, Pascal and Zeppelzauer, Matthias},
  doi          = {10.1007/s10994-022-06178-9},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3203-3226},
  shortjournal = {Mach. Learn.},
  title        = {Machine unlearning: Linear filtration for logit-based classifiers},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards harnessing feature embedding for robust learning
with noisy labels. <em>ML</em>, <em>111</em>(9), 3181–3201. (<a
href="https://doi.org/10.1007/s10994-022-06197-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The memorization effect of deep neural networks (DNNs) plays a pivotal role in recent label noise learning methods. To exploit this effect, the model prediction-based methods have been widely adopted, which aim to exploit the outputs of DNNs in the early stage of learning to correct noisy labels. However, we observe that the model will make mistakes during label prediction, resulting in unsatisfactory performance. By contrast, the produced features in the early stage of learning show better robustness. Inspired by this observation, in this paper, we propose a novel feature embedding-based method for deep learning with label noise, termed Lab El Noise Dilution (LEND). To be specific, we first compute a similarity matrix based on current embedded features to capture the local structure of training data. Then, the noisy supervision signals carried by mislabeled data are overwhelmed by nearby correctly labeled ones (i.e., label noise dilution), of which the effectiveness is guaranteed by the inherent robustness of feature embedding. Finally, the training data with diluted labels are further used to train a robust classifier. Empirically, we conduct extensive experiments on both synthetic and real-world noisy datasets by comparing our LEND with several representative robust learning approaches. The results verify the effectiveness of our LEND.},
  archive      = {J_ML},
  author       = {Zhang, Chuang and Shen, Li and Yang, Jian and Gong, Chen},
  doi          = {10.1007/s10994-022-06197-6},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3181-3201},
  shortjournal = {Mach. Learn.},
  title        = {Towards harnessing feature embedding for robust learning with noisy labels},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pruning convolutional neural networks via filter similarity
analysis. <em>ML</em>, <em>111</em>(9), 3161–3180. (<a
href="https://doi.org/10.1007/s10994-022-06193-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown excellent performance in many fields, especially image recognition and retrieval in recent years. The performance of convolutional neural networks (CNNs) is particularly outstanding. CNNs, however, are usually computationally intensive, which hinders the deployment of CNNs in resource-limited devices. Methods of network compression, pruning methods in particularly, removing redundant structures of CNNs, can significantly reduce the computational complexity of CNNs. Most of the state-of-the-art pruning methods for CNNs, however, have two defects. (1) Filters, also called convolutional kernels that are matrices used to extract features in an image, are pruned by ranking their weight without considering the effects of their actual output, which results in the deletion of important filters and the difficulty in determining the pruning threshold on weight. (2) Filters are pruned either in the forward direction or in isolation, which are difficult to control the loss of accuracy. This paper proposes a novel pruning method called filter similarity analysis with backward pruning (FSABP). FSABP calculates the similarity coefficients of filters in each layer, and deletes the filters associated with small similarity coefficients. The smaller the coefficient the more similar the filters. Filters are pruned layer by layer in the backward direction starting from the last convolution layer, which can effectively control the loss of accuracy by avoiding early removal of the shallow convolution filters. Experiments on LENET, VGG-16 and ResNet-50 show that FSABP can reduce parameter redundancy at the cost of negligible loss of accuracy and even improve accuracy in some cases. The results on LENET also suggest that FSABP is applicable to both deep and shallow CNNs.},
  archive      = {J_ML},
  author       = {Geng, Lili and Niu, Baoning},
  doi          = {10.1007/s10994-022-06193-w},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3161-3180},
  shortjournal = {Mach. Learn.},
  title        = {Pruning convolutional neural networks via filter similarity analysis},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised semantic segmentation in earth observation:
The MiniFrance suite, dataset analysis and multi-task network study.
<em>ML</em>, <em>111</em>(9), 3125–3160. (<a
href="https://doi.org/10.1007/s10994-020-05943-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of semi-supervised learning techniques is essential to enhance the generalization capacities of machine learning algorithms. Indeed, raw image data are abundant while labels are scarce, therefore it is crucial to leverage unlabeled inputs to build better models. The availability of large databases have been key for the development of learning algorithms with high level performance. Despite the major role of machine learning in Earth Observation to derive products such as land cover maps, datasets in the field are still limited, either because of modest surface coverage, lack of variety of scenes or restricted classes to identify. We introduce a novel large-scale dataset for semi-supervised semantic segmentation in Earth Observation, the MiniFrance suite. MiniFrance has several unprecedented properties: it is large-scale, containing over 2000 very high resolution aerial images, accounting for more than 200 billions samples (pixels); it is varied, covering 16 conurbations in France, with various climates, different landscapes, and urban as well as countryside scenes; and it is challenging, considering land use classes with high-level semantics. Nevertheless, the most distinctive quality of MiniFrance is being the only dataset in the field especially designed for semi-supervised learning: it contains labeled and unlabeled images in its training partition, which reproduces a life-like scenario. Along with this dataset, we present tools for data representativeness analysis in terms of appearance similarity and a thorough study of MiniFrance data, demonstrating that it is suitable for learning and generalizes well in a semi-supervised setting. Finally, we present semi-supervised deep architectures based on multi-task learning and the first experiments on MiniFrance. These results will serve as baselines for future work on semi-supervised learning over the MiniFrance dataset. The Minifrance suite and related semi-supervised networks will be publicly available to promote semi-supervised works in Earth Observation.},
  archive      = {J_ML},
  author       = {Castillo-Navarro, Javiera and Le Saux, Bertrand and Boulch, Alexandre and Audebert, Nicolas and Lefèvre, Sébastien},
  doi          = {10.1007/s10994-020-05943-y},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {3125-3160},
  shortjournal = {Mach. Learn.},
  title        = {Semi-supervised semantic segmentation in earth observation: The MiniFrance suite, dataset analysis and multi-task network study},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relating instance hardness to classification performance in
a dataset: A visual approach. <em>ML</em>, <em>111</em>(8), 3085–3123.
(<a href="https://doi.org/10.1007/s10994-022-06205-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning studies often involve a series of computational experiments in which the predictive performance of multiple models are compared across one or more datasets. The results obtained are usually summarized through average statistics, either in numeric tables or simple plots. Such approaches fail to reveal interesting subtleties about algorithmic performance, including which observations an algorithm may find easy or hard to classify, and also which observations within a dataset may present unique challenges. Recently, a methodology known as Instance Space Analysis was proposed for visualizing algorithm performance across different datasets. This methodology relates predictive performance to estimated instance hardness measures extracted from the datasets. However, the analysis considered an instance as being an entire classification dataset and the algorithm performance was reported for each dataset as an average error across all observations in the dataset. In this paper, we developed a more fine-grained analysis by adapting the ISA methodology. The adapted version of ISA allows the analysis of an individual classification dataset by a 2-D hardness embedding, which provides a visualization of the data according to the difficulty level of its individual observations. This allows deeper analyses of the relationships between instance hardness and predictive performance of classifiers. We also provide an open-access Python package named PyHard, which encapsulates the adapted ISA and provides an interactive visualization interface. We illustrate through case studies how our tool can provide insights about data quality and algorithm performance in the presence of challenges such as noisy and biased data.},
  archive      = {J_ML},
  author       = {Paiva, Pedro Yuri Arbs and Moreno, Camila Castro and Smith-Miles, Kate and Valeriano, Maria Gabriela and Lorena, Ana Carolina},
  doi          = {10.1007/s10994-022-06205-9},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3085-3123},
  shortjournal = {Mach. Learn.},
  title        = {Relating instance hardness to classification performance in a dataset: A visual approach},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). World-class interpretable poker. <em>ML</em>,
<em>111</em>(8), 3063–3083. (<a
href="https://doi.org/10.1007/s10994-022-06179-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of interpretability in iterative game solving for imperfect-information games such as poker. This lack of interpretability has two main sources: first, the use of an uninterpretable feature representation, and second, the use of black box methods such as neural networks, for the fitting procedure. In this paper, we present advances on both fronts. Namely, first we propose a novel, compact, and easy-to-understand game-state feature representation for Heads-up No-limit (HUNL) Poker. Second, we make use of globally optimal decision trees, paired with a counterfactual regret minimization (CFR) self-play algorithm, to train our poker bot which produces an entirely interpretable agent. Through experiments against Slumbot, the winner of the most recent Annual Computer Poker Competition, we demonstrate that our approach yields a HUNL Poker agent that is capable of beating the Slumbot. Most exciting of all, the resulting poker bot is highly interpretable, allowing humans to learn from the novel strategies it discovers.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Paskov, Alex},
  doi          = {10.1007/s10994-022-06179-8},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3063-3083},
  shortjournal = {Mach. Learn.},
  title        = {World-class interpretable poker},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Correction to: Meta-interpretive learning as metarule
specialisation. <em>ML</em>, <em>111</em>(8), 3061. (<a
href="https://doi.org/10.1007/s10994-022-06180-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Patsantzis, S. and Muggleton, S. H.},
  doi          = {10.1007/s10994-022-06180-1},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3061},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Meta-interpretive learning as metarule specialisation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive infinite dropout for noisy and sparse data streams.
<em>ML</em>, <em>111</em>(8), 3025–3060. (<a
href="https://doi.org/10.1007/s10994-022-06169-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to analyze data streams, which arrive sequentially and possibly infinitely, is increasingly vital in various online applications. However, data streams pose various challenges, including sparse and noisy data as well as concept drifts, which easily mislead a learning method. This paper proposes a simple yet robust framework, called Adaptive Infinite Dropout (aiDropout), to effectively tackle these problems. Our framework uses a dropout technique in a recursive Bayesian approach in order to create a flexible mechanism for balancing between old and new information. In detail, the recursive Bayesian approach imposes a constraint on the model parameters to make a regularization term between the current and previous mini-batches. Then, dropout whose drop rate is autonomously learned can adjust the constraint to new data. Thanks to the ability to reduce overfitting and the ensemble property of Dropout, our framework obtains better generalization, thus it effectively handles undesirable effects of noise and sparsity. In particular, theoretical analyses show that aiDropout imposes a data-dependent regularization, therefore, it can adapt quickly to sudden changes from data streams. Extensive experiments show that aiDropout significantly outperforms the state-of-the-art baselines on a variety of tasks such as supervised and unsupervised learning.},
  archive      = {J_ML},
  author       = {Nguyen, Ha and Pham, Hoang and Nguyen, Son and Van Linh, Ngo and Than, Khoat},
  doi          = {10.1007/s10994-022-06169-w},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {3025-3060},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive infinite dropout for noisy and sparse data streams},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal survival trees. <em>ML</em>, <em>111</em>(8),
2951–3023. (<a
href="https://doi.org/10.1007/s10994-021-06117-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-based models are increasingly popular due to their ability to identify complex relationships that are beyond the scope of parametric models. Survival tree methods adapt these models to allow for the analysis of censored outcomes, which often appear in medical data. We present a new Optimal Survival Trees algorithm that leverages mixed-integer optimization (MIO) and local search techniques to generate globally optimized survival tree models. We demonstrate that the OST algorithm improves on the accuracy of existing survival tree methods, particularly in large datasets.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Dunn, Jack and Gibson, Emma and Orfanoudaki, Agni},
  doi          = {10.1007/s10994-021-06117-0},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2951-3023},
  shortjournal = {Mach. Learn.},
  title        = {Optimal survival trees},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware spatio-temporal event prediction via
convolutional hawkes processes. <em>ML</em>, <em>111</em>(8), 2929–2950.
(<a href="https://doi.org/10.1007/s10994-022-06136-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive spatio-temporal event data sets are now available that cover events such as disease outbreaks, armed conflicts and crimes. Predicting such events and revealing the underlying triggering patterns are a crucial task for many applications, ranging from disease control to global politics. Traditional event prediction models based on Hawkes processes capture the spatio-temporal relationships between events, but cannot incorporate complex and heterogeneous external features, including population distribution, weather and terrain. This paper proposes an event prediction method that effectively utilizes the rich external information present in sets of unstructured data (e.g., map images, satellite images and weather map). Specifically, we extend a convolutional neural network (CNN) by combining it with continuous kernel convolution; and design the conditional intensity of Hawkes process based on the extended neural network model that accepts images as its input. Our approach of using the continuous convolution kernel provides a flexible way to discover the complex effect of external factors on the triggering process, as well as yielding tractable optimization algorithms. We use real-world event data from different domains (i.e., disease outbreaks, armed conflicts and protests) to demonstrate that the proposed method has better prediction performance than existing methods.},
  archive      = {J_ML},
  author       = {Okawa, Maya and Iwata, Tomoharu and Tanaka, Yusuke and Kurashima, Takeshi and Toda, Hiroyuki and Kashima, Hisashi},
  doi          = {10.1007/s10994-022-06136-5},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2929-2950},
  shortjournal = {Mach. Learn.},
  title        = {Context-aware spatio-temporal event prediction via convolutional hawkes processes},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional correlation matrix estimation for general
continuous data with bagging technique. <em>ML</em>, <em>111</em>(8),
2905–2927. (<a
href="https://doi.org/10.1007/s10994-022-06138-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional covariance matrix estimation plays a central role in multivariate statistical analysis. It is well-known that the sample covariance matrix is singular when the sample size is smaller than the dimension of the variable, but the covariance estimate must be positive-definite. This motivates some modifications of the sample covariance matrix to preserve its efficient estimation of pairwise covariance. In this paper, we modify the sample correlation matrix using the Bagging technique. The proposed Bagging estimator is flexible for general continuous data. Under some mild conditions, we show theoretically that the Bagging estimator can ensure positive-definiteness with probability one in finite samples. We also prove the consistency of the bootstrap estimator of Pearson correlation and the consistency of our Bagging estimator when the dimension p is fixed. Simulation results and a real application are provided to demonstrate that our method strikes a better balance between RMSE and likelihood, and is more robust, than other existing estimators.},
  archive      = {J_ML},
  author       = {Wang, Chaojie and Du, Jin and Fan, Xiaodan},
  doi          = {10.1007/s10994-022-06138-3},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2905-2927},
  shortjournal = {Mach. Learn.},
  title        = {High-dimensional correlation matrix estimation for general continuous data with bagging technique},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustered and deep echo state networks for signal noise
reduction. <em>ML</em>, <em>111</em>(8), 2885–2904. (<a
href="https://doi.org/10.1007/s10994-022-06135-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) are Recurrent Neural Networks with fixed input and internal (hidden) weights, and adaptable output weights. The hidden part of an ESN can be considered as a discrete-time dynamical system, called reservoir. In classical ESNs, the internal connections are obtained from an Erdős-Rényi graph. A recent study proposed ESNs with clustered adjacency matrices (CESNs), where the clusters are either Erdős-Rényi graphs or Barabási-Albert-like graphs. In this work, we investigate the effectiveness of CESNs and apply them for signal denoising. In addition, we introduce and study deep CESNs with multiple clustered layers. We found that CESNs and deep CESNs can compete with deep ESNs for all tasks that we considered.},
  archive      = {J_ML},
  author       = {de Oliveira Junior, Laercio and Stelzer, Florian and Zhao, Liang},
  doi          = {10.1007/s10994-022-06135-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2885-2904},
  shortjournal = {Mach. Learn.},
  title        = {Clustered and deep echo state networks for signal noise reduction},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimised one-class classification performance. <em>ML</em>,
<em>111</em>(8), 2863–2883. (<a
href="https://doi.org/10.1007/s10994-022-06147-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a thorough treatment of one-class classification with hyperparameter optimisation for five data descriptors: Support Vector Machine (SVM), Nearest Neighbour Distance (NND), Localised Nearest Neighbour Distance (LNND), Local Outlier Factor (LOF) and Average Localised Proximity (ALP). The hyperparameters of SVM and LOF have to be optimised through cross-validation, while NND, LNND and ALP allow an efficient form of leave-one-out validation and the reuse of a single nearest-neighbour query. We experimentally evaluate the effect of hyperparameter optimisation with 246 classification problems drawn from 50 datasets. From a selection of optimisation algorithms, the recent Malherbe–Powell proposal optimises the hyperparameters of all data descriptors most efficiently. We calculate the increase in test AUROC and the amount of overfitting as a function of the number of hyperparameter evaluations. After 50 evaluations, ALP and SVM significantly outperform LOF, NND and LNND, and LOF and NND outperform LNND. The performance of ALP and SVM is comparable, but ALP can be optimised more efficiently so constitutes a good default choice. Alternatively, using validation AUROC as a selection criterion between ALP or SVM gives the best overall result, and NND is the least computationally demanding option. We thus end up with a clear trade-off between three choices, allowing practitioners to make an informed decision.},
  archive      = {J_ML},
  author       = {Lenz, Oliver Urs and Peralta, Daniel and Cornelis, Chris},
  doi          = {10.1007/s10994-022-06147-2},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2863-2883},
  shortjournal = {Mach. Learn.},
  title        = {Optimised one-class classification performance},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maintaining AUC and h-measure over time. <em>ML</em>,
<em>111</em>(8), 2839–2862. (<a
href="https://doi.org/10.1007/s10994-021-06084-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the performance of a classifier is a vital task in machine learning. The running time of an algorithm that computes the measure plays a very small role in an offline setting, for example, when the classifier is being developed by a researcher. However, the running time becomes more crucial if our goal is to monitor the performance of a classifier over time. In this paper we study three algorithms for maintaining two measures. The first algorithm maintains area under the ROC curve (AUC) under addition and deletion of data points in $$\mathcal {O} \mathopen {}\left( \log n\right)$$ time. This is done by maintaining the data points sorted in a self-balanced search tree. In addition, we augment the search tree that allows us to query the ROC coordinates of a data point in $$\mathcal {O} \mathopen {}\left( \log n\right)$$ time. In doing so we are able to maintain AUC in $$\mathcal {O} \mathopen {}\left( \log n\right)$$ time. Our next two algorithms involve in maintaining H-measure, an alternative measure based on the ROC curve. Computing the measure is a two-step process: first we need to compute a convex hull of the ROC curve, followed by a sum over the convex hull. We demonstrate that we can maintain the convex hull using a minor modification of the classic convex hull maintenance algorithm. We then show that under certain conditions, we can compute the H-measure exactly in $$\mathcal {O} \mathopen {}\left( \log ^2 n\right)$$ time, and if the conditions are not met, then we can estimate the H-measure in $$\mathcal {O} \mathopen {}\left( (\log n + \epsilon ^{-1})\log n\right)$$ time. We show empirically that our methods are significantly faster than the baselines.},
  archive      = {J_ML},
  author       = {Tatti, Nikolaj},
  doi          = {10.1007/s10994-021-06084-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2839-2862},
  shortjournal = {Mach. Learn.},
  title        = {Maintaining AUC and H-measure over time},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A taxonomy of weight learning methods for statistical
relational learning. <em>ML</em>, <em>111</em>(8), 2799–2838. (<a
href="https://doi.org/10.1007/s10994-021-06069-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical relational learning (SRL) frameworks are effective at defining probabilistic models over complex relational data. They often use weighted first-order logical rules where the weights of the rules govern probabilistic interactions and are usually learned from data. Existing weight learning approaches typically attempt to learn a set of weights that maximizes some function of data likelihood; however, this does not always translate to optimal performance on a desired domain metric, such as accuracy or F1 score. In this paper, we introduce a taxonomy of search-based weight learning approaches for SRL frameworks that directly optimize weights on a chosen domain performance metric. To effectively apply these search-based approaches, we introduce a novel projection, referred to as scaled space (SS), that is an accurate representation of the true weight space. We show that SS removes redundancies in the weight space and captures the semantic distance between the possible weight configurations. In order to improve the efficiency of search, we also introduce an approximation of SS which simplifies the process of sampling weight configurations. We demonstrate these approaches on two state-of-the-art SRL frameworks: Markov logic networks and probabilistic soft logic. We perform empirical evaluation on five real-world datasets and evaluate them each on two different metrics. We also compare them against four other weight learning approaches. Our experimental results show that our proposed search-based approaches outperform likelihood-based approaches and yield up to a 10\% improvement across a variety of performance metrics. Further, we perform an extensive evaluation to measure the robustness of our approach to different initializations and hyperparameters. The results indicate that our approach is both accurate and robust.},
  archive      = {J_ML},
  author       = {Srinivasan, Sriram and Dickens, Charles and Augustine, Eriq and Farnadi, Golnoosh and Getoor, Lise},
  doi          = {10.1007/s10994-021-06069-5},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2799-2838},
  shortjournal = {Mach. Learn.},
  title        = {A taxonomy of weight learning methods for statistical relational learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Receiver operating characteristic (ROC) movies, universal
ROC (UROC) curves, and coefficient of predictive ability (CPA).
<em>ML</em>, <em>111</em>(8), 2769–2797. (<a
href="https://doi.org/10.1007/s10994-021-06114-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Throughout science and technology, receiver operating characteristic (ROC) curves and associated area under the curve ( $$\mathrm{AUC}$$ ) measures constitute powerful tools for assessing the predictive abilities of features, markers and tests in binary classification problems. Despite its immense popularity, ROC analysis has been subject to a fundamental restriction, in that it applies to dichotomous (yes or no) outcomes only. Here we introduce ROC movies and universal ROC (UROC) curves that apply to just any linearly ordered outcome, along with an associated coefficient of predictive ability ( $${\mathrm{CPA}}$$ ) measure. $${\mathrm{CPA}}$$ equals the area under the UROC curve, and admits appealing interpretations in terms of probabilities and rank based covariances. For binary outcomes $${\mathrm{CPA}}$$ equals $$\mathrm{AUC}$$ , and for pairwise distinct outcomes $${\mathrm{CPA}}$$ relates linearly to Spearman’s coefficient, in the same way that the C index relates linearly to Kendall’s coefficient. ROC movies, UROC curves, and $${\mathrm{CPA}}$$ nest and generalize the tools of classical ROC analysis, and are bound to supersede them in a wealth of applications. Their usage is illustrated in data examples from biomedicine and meteorology, where rank based measures yield new insights in the WeatherBench comparison of the predictive performance of convolutional neural networks and physical-numerical models for weather prediction.},
  archive      = {J_ML},
  author       = {Gneiting, Tilmann and Walz, Eva-Maria},
  doi          = {10.1007/s10994-021-06114-3},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2769-2797},
  shortjournal = {Mach. Learn.},
  title        = {Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA)},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal policy trees. <em>ML</em>, <em>111</em>(7),
2741–2768. (<a
href="https://doi.org/10.1007/s10994-022-06128-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for learning optimal tree-based prescription policies directly from data, combining methods for counterfactual estimation from the causal inference literature with recent advances in training globally-optimal decision trees. The resulting method, Optimal Policy Trees, yields interpretable prescription policies, is highly scalable, and handles both discrete and continuous treatments. We conduct extensive experiments on both synthetic and real-world datasets and demonstrate that these trees offer best-in-class performance across a wide variety of problems.},
  archive      = {J_ML},
  author       = {Amram, Maxime and Dunn, Jack and Zhuo, Ying Daisy},
  doi          = {10.1007/s10994-022-06128-5},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2741-2768},
  shortjournal = {Mach. Learn.},
  title        = {Optimal policy trees},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial dependence between training and test sets: Another
pitfall of classification accuracy assessment in remote sensing.
<em>ML</em>, <em>111</em>(7), 2715–2740. (<a
href="https://doi.org/10.1007/s10994-021-05972-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial autocorrelation is inherent to remotely sensed data. Nearby pixels are more similar than distant ones. This property can help to improve the classification performance, by adding spatial or contextual features into the model. However, it can also lead to overestimation of generalisation capabilities, if the spatial dependence between training and test sets is ignored. In this paper, we review existing approaches that deal with spatial autocorrelation for image classification in remote sensing and demonstrate the importance of bias in accuracy metrics when spatial independence between the training and test sets is not respected. We compare three spatial and non-spatial cross-validation strategies at pixel and object levels and study how performances vary at different sample sizes. Experiments based on Sentinel-2 data for mapping two simple forest classes show that spatial leave-one-out cross-validation is the better strategy to provide unbiased estimates of predictive error. Its performance metrics are consistent with the real quality of the resulting map contrary to traditional non-spatial cross-validation that overestimates accuracy. This highlight the need to change practices in classification accuracy assessment. To encourage it we developped Museo ToolBox, an open-source python library that makes spatial cross-validation possible.},
  archive      = {J_ML},
  author       = {Karasiak, N. and Dejoux, J.-F. and Monteil, C. and Sheeren, D.},
  doi          = {10.1007/s10994-021-05972-1},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2715-2740},
  shortjournal = {Mach. Learn.},
  title        = {Spatial dependence between training and test sets: Another pitfall of classification accuracy assessment in remote sensing},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large scale tensor regression using kernels and variational
inference. <em>ML</em>, <em>111</em>(7), 2663–2713. (<a
href="https://doi.org/10.1007/s10994-021-06067-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We outline an inherent flaw of tensor factorization models when latent factors are expressed as a function of side information and propose a novel method to mitigate this. We coin our methodology kernel fried tensor (KFT) and present it as a large-scale prediction and forecasting tool for high dimensional data. Our results show superior performance against LightGBM and Field aware factorization machines (FFM), two algorithms with proven track records, widely used in large-scale prediction. We also develop a variational inference framework for KFT which enables associating the predictions and forecasts with calibrated uncertainty estimates on several datasets.},
  archive      = {J_ML},
  author       = {Hu, Robert and Nicholls, Geoff K. and Sejdinovic, Dino},
  doi          = {10.1007/s10994-021-06067-7},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2663-2713},
  shortjournal = {Mach. Learn.},
  title        = {Large scale tensor regression using kernels and variational inference},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variance reduction in feature hashing using MLE and control
variate method. <em>ML</em>, <em>111</em>(7), 2631–2662. (<a
href="https://doi.org/10.1007/s10994-022-06166-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feature hashing algorithm introduced by Weinberger et al. (2009) is a popular dimensionality reduction algorithm that compresses high dimensional data points into low dimensional data points that closely approximate the pairwise inner product. This algorithm has been used in many fundamental machine learning applications such as model compression (Chen et al. 2015), spam classification (Weinberger et al. 2009), compressing text classifiers (Joulin et al. 2016), large scale image classification (Mensink et al. 2012). However, a limitation of this approach is that the variance of its estimator for the inner product tends to be large for small values of the reduced dimensions, making the estimate less reliable. We address this challenge and suggest two simple and practical solutions in this work. Our approach relies on control variate (CV) and maximum likelihood estimator (MLE), which are popular variance reduction techniques used in statistics. We show that these methods lead to significant variance reduction in the inner product similarity estimation. We give theoretical bounds on the same and complement it via extensive experiments on synthetic and real-world datasets. Given the simplicity and effectiveness of our approach, we hope that it can be adapted in practice.},
  archive      = {J_ML},
  author       = {Verma, Bhisham Dev and Pratap, Rameshwar and Thakur, Manoj},
  doi          = {10.1007/s10994-022-06166-z},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2631-2662},
  shortjournal = {Mach. Learn.},
  title        = {Variance reduction in feature hashing using MLE and control variate method},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized weisfeiler-lehman graph kernel. <em>ML</em>,
<em>111</em>(7), 2601–2629. (<a
href="https://doi.org/10.1007/s10994-022-06131-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After more than one decade, Weisfeiler-Lehman graph kernels are still among the most prevalent graph kernels due to their remarkable predictive performance and time complexity. They are based on a fast iterative partitioning of vertices, originally designed for deciding graph isomorphism with one-sided error. The Weisfeiler-Lehman graph kernels retain this idea and compare such labels with respect to equality. This binary valued comparison is, however, arguably too rigid for defining suitable graph kernels for certain graph classes. To overcome this limitation, we propose a generalization of Weisfeiler-Lehman graph kernels which takes into account a more natural and finer grade of similarity between Weisfeiler-Lehman labels than equality. We show that the proposed similarity can be calculated efficiently by means of the Wasserstein distance between certain vectors representing Weisfeiler-Lehman labels. This and other facts give rise to the natural choice of partitioning the vertices with the Wasserstein k-means algorithm. We empirically demonstrate on the Weisfeiler-Lehman subtree kernel, which is one of the most prominent Weisfeiler-Lehman graph kernels, that our generalization significantly outperforms this and other state-of-the-art graph kernels in terms of predictive performance on datasets which contain structurally more complex graphs beyond the typically considered molecular graphs.},
  archive      = {J_ML},
  author       = {Schulz, Till Hendrik and Horváth, Tamás and Welke, Pascal and Wrobel, Stefan},
  doi          = {10.1007/s10994-022-06131-w},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2601-2629},
  shortjournal = {Mach. Learn.},
  title        = {A generalized weisfeiler-lehman graph kernel},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROSE: Robust online self-adjusting ensemble for continual
learning on imbalanced drifting data streams. <em>ML</em>,
<em>111</em>(7), 2561–2599. (<a
href="https://doi.org/10.1007/s10994-022-06168-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data streams are potentially unbounded sequences of instances arriving over time to a classifier. Designing algorithms that are capable of dealing with massive, rapidly arriving information is one of the most dynamically developing areas of machine learning. Such learners must be able to deal with a phenomenon known as concept drift, where the data stream may be subject to various changes in its characteristics over time. Furthermore, distributions of classes may evolve over time, leading to a highly difficult non-stationary class imbalance. In this work we introduce Robust Online Self-Adjusting Ensemble (ROSE), a novel online ensemble classifier capable of dealing with all of the mentioned challenges. The main features of ROSE are: (1) online training of base classifiers on variable size random subsets of features; (2) online detection of concept drift and creation of a background ensemble for faster adaptation to changes; (3) sliding window per class to create skew-insensitive classifiers regardless of the current imbalance ratio; and (4) self-adjusting bagging to enhance the exposure of difficult instances from minority classes. The interplay among these features leads to an improved performance in various data stream mining benchmarks. An extensive experimental study comparing with 30 ensemble classifiers shows that ROSE is a robust and well-rounded classifier for drifting imbalanced data streams, especially under the presence of noise and class imbalance drift, while maintaining competitive time complexity and memory consumption. Results are supported by a thorough non-parametric statistical analysis.},
  archive      = {J_ML},
  author       = {Cano, Alberto and Krawczyk, Bartosz},
  doi          = {10.1007/s10994-022-06168-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2561-2599},
  shortjournal = {Mach. Learn.},
  title        = {ROSE: Robust online self-adjusting ensemble for continual learning on imbalanced drifting data streams},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arbitrary conditional inference in variational autoencoders
via fast prior network training. <em>ML</em>, <em>111</em>(7),
2537–2559. (<a
href="https://doi.org/10.1007/s10994-022-06171-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditionally trained VAEs provide an attractive solution. However, to efficiently support arbitrary queries over pre-trained VAEs when the query and evidence are not known in advance, one is generally reduced to MCMC sampling methods that can suffer from long mixing times. In this paper, we propose an idea of efficiently training small conditional prior networks to approximate the latent distribution of the VAE after conditioning on an evidence assignment; this permits generating query samples without retraining the full VAE. We experimentally evaluate three variations of conditional prior networks showing that (i) they can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform existing state-of-the-art methods for conditional inference in pre-trained VAEs.},
  archive      = {J_ML},
  author       = {Wu, Ga and Domke, Justin and Sanner, Scott},
  doi          = {10.1007/s10994-022-06171-2},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2537-2559},
  shortjournal = {Mach. Learn.},
  title        = {Arbitrary conditional inference in variational autoencoders via fast prior network training},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traditional and context-specific spam detection in low
resource settings. <em>ML</em>, <em>111</em>(7), 2515–2536. (<a
href="https://doi.org/10.1007/s10994-022-06176-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media data has a mix of high and low-quality content. One form of commonly studied low-quality content is spam. Most studies assume that spam is context-neutral. We show on different Twitter data sets that context-specific spam exists and is identifiable. We then compare multiple traditional machine learning models and a neural network model that uses a pre-trained BERT language model to capture contextual features for identifying spam, both traditional and context-specific, using only content-based features. The neural network model outperforms the traditional models with an F1 score of 0.91. Because spam training data sets are notoriously imbalanced, we also investigate the impact of this imbalance and show that simple Bag-of-Words models are best with extreme imbalance, but a neural model that fine-tunes using language models from other domains significantly improves the F1 score, but not to the levels of domain-specific neural models. This suggests that the strategy employed may vary depending upon the level of imbalance in the data set, the amount of data available in a low resource setting, and the prevalence of context-specific spam vs. traditional spam. Finally, we make our data sets available for use by the research community.},
  archive      = {J_ML},
  author       = {Kawintiranon, Kornraphop and Singh, Lisa and Budak, Ceren},
  doi          = {10.1007/s10994-022-06176-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2515-2536},
  shortjournal = {Mach. Learn.},
  title        = {Traditional and context-specific spam detection in low resource settings},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re-thinking model robustness from stability: A new insight
to defend adversarial examples. <em>ML</em>, <em>111</em>(7), 2489–2513.
(<a href="https://doi.org/10.1007/s10994-022-06186-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the model robustness against adversarial examples, referred to as small perturbed input data that may however fool many state-of-the-art deep learning models. Unlike previous research, we establish a novel theory addressing the robustness issue from the perspective of stability of the loss function in the small neighborhood of natural examples. We propose to exploit an energy function to describe the total variation in a small neighborhood and prove that reducing such energy guarantees the robustness against adversarial examples. We also show that the traditional training methods including adversarial training and virtual adversarial training tend to minimize the lower bound of our proposed energy function. Importantly, we prove that minimizing the energy function can obtain a better generalization bound than traditional adversarial training approaches. Through a series of experiments, we demonstrate the superiority of our model on different datasets for defending adversarial attacks. In particular, our proposed adversarial framework achieves the best performance compared with previous adversarial training methods on benchmark datasets CIFAR-10, CIFAR-100 and SVHN and they demonstrate much better robustness against adversarial examples than all the other comparison methods.},
  archive      = {J_ML},
  author       = {Zhang, Shufei and Huang, Kaizhu and Xu, Zenglin},
  doi          = {10.1007/s10994-022-06186-9},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2489-2513},
  shortjournal = {Mach. Learn.},
  title        = {Re-thinking model robustness from stability: A new insight to defend adversarial examples},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning explanations for biological feedback with delays
using an event calculus. <em>ML</em>, <em>111</em>(7), 2435–2487. (<a
href="https://doi.org/10.1007/s10994-021-06038-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the identification of feedback mechanisms in biological systems by learning logical rules in R. Thomas’ Kinetic Logic (Thomas and D’Ari in Biological feedback. CRC Press, 1990). The principal advantages claimed for Kinetic Logic are that it captures an important class of regulatory networks at an appropriate level of precision, and that the representation is close to that used routinely by biologists, with a well-understood relationship to a differential description. In this paper we present a formalisation of Kinetic Logic as a labelled transition system and provide a provably correct implementation in a modified form of the Event Calculus. The behaviour of a system is then a logical consequence of the core-axioms of a (modified) Event Calculus C, the axioms K implementing Kinetic Logic and the axioms H describing the system. This formulation allows us to specify system identification in the manner adopted in Inductive Logic Programming (ILP), namely, given C, K, system behaviour S and possibly some additional domain-knowledge B, find H s.t. $$B \wedge C \wedge K \wedge H \models S$$ . Identifying a suitable Kinetic Logic hypothesis requires the simultaneous identification of definite clauses for: (a) logical definitions relating the occurrence of events to values of fluents; (b) delays in changes of the values of fluents arising from the occurrence of events; and possibly (c) exceptions to changes in fluent values, arising from asynchronous behaviour inherent to the system. We use a standard ILP engine for (a), and special-purpose abduction procedures for (b) and (c). We demonstrate this combination of induction and abduction on several canonical feedback patterns described by Thomas, and to identify the regulatory mechanism in two well-known biological problems (immune-response and phage-infection).},
  archive      = {J_ML},
  author       = {Srinivasan, Ashwin and Bain, Michael and Baskar, A.},
  doi          = {10.1007/s10994-021-06038-y},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2435-2487},
  shortjournal = {Mach. Learn.},
  title        = {Learning explanations for biological feedback with delays using an event calculus},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robustness verification of ReLU networks via quadratic
programming. <em>ML</em>, <em>111</em>(7), 2407–2433. (<a
href="https://doi.org/10.1007/s10994-022-06132-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are known to be sensitive to adversarial perturbations. To investigate this undesired behavior we consider the problem of computing the distance to the decision boundary (DtDB) from a given sample for a deep neural net classifier. In this work we present a procedure where we solve a convex quadratic programming (QP) task to obtain a lower bound on the DtDB. This bound is used as a robustness certificate of the classifier around a given sample. We show that our approach provides better or competitive results in comparison with a wide range of existing techniques.},
  archive      = {J_ML},
  author       = {Kuvshinov, Aleksei and Günnemann, Stephan},
  doi          = {10.1007/s10994-022-06132-9},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2407-2433},
  shortjournal = {Mach. Learn.},
  title        = {Robustness verification of ReLU networks via quadratic programming},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-domain graph convolutional networks for skeleton-based
action recognition. <em>ML</em>, <em>111</em>(7), 2381–2406. (<a
href="https://doi.org/10.1007/s10994-022-06141-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is attracting more and more attention owing to the general representation ability of skeleton data. The Graph Convolutional Networks (GCNs) methods extended from Convolutional Neural Networks (CNNs) are proposed to directly extract spatial–temporal information from the graphs. Previous GCNs usually aggregate the skeleton information locally in the vertex domain. However, the focus on the local information brought about the limited representation ability in some actions containing overall dynamics in both spatial and temporal, which pulled down the overall accuracy of the model. Therefore, this paper proposes a more comprehensive two-stream GCN architecture containing the vertex-domain graph convolution and the spectral graph convolution based on Graph Fourier Transform (GFT). One stream utilizes an efficient vertex-domain graph convolution to obtain effective spatial–temporal information via Graph Shift Blocks (GSB), while the other brings the global spectral information from our improved Residual Spectral Blocks (RSB). According to the analysis of the experimental results, the action misalignment for certain actions is reduced. Moreover, along with other GCN methods that only focus on spatial–temporal information, our RSB strategies help improve their performance. DD-GCN is evaluated on three large skeleton-based datasets, NTU-RGBD 60, NTU-RGBD 120, and Kinetics-Skeleton. The experiment results demonstrate a comparable ability to the state-of-the-art.},
  archive      = {J_ML},
  author       = {Chen, Shuo and Xu, Ke and Mi, Zhongjie and Jiang, Xinghao and Sun, Tanfeng},
  doi          = {10.1007/s10994-022-06141-8},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {2381-2406},
  shortjournal = {Mach. Learn.},
  title        = {Dual-domain graph convolutional networks for skeleton-based action recognition},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling spatiotemporal dynamics from earth observation
data with neural differential equations. <em>ML</em>, <em>111</em>(6),
2349–2380. (<a
href="https://doi.org/10.1007/s10994-022-06139-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting complex spatiotemporal dynamics is central in Earth science for modeling a variety of phenomena ranging from atmospheric dynamics to the evolution of vegetation. Those phenomena are often observed from remote sensing measurements that only provide partial information on the underlying physical equations. In this context, we consider the problem of automatically learning the dynamics of physical spatiotemporal processes from incomplete observations. We propose a new data-driven framework where the dynamics is modeled by an unknown differential equation and where the state representation and evolution is learned only from partial observations. The dynamical model is parametrized by a deep neural network. Since the problem is underconstrained, the model may learn high quality forecasts of the observations while being physically inconsistent. We introduce two settings that help analyze and interpret the learned model states. We evaluate the proposed model on two benchmarks: (1) the incompressible Navier–Stokes equations which underlie transport phenomena in the atmosphere and in the ocean, (2) a challenging problem of sea surface temperature prediction where the underlying dynamics corresponds to a sophisticated ocean dynamics model. The proposed model is able to provide long term forecasts for these complex dynamics and large dimensional observation spaces.},
  archive      = {J_ML},
  author       = {Ayed, Ibrahim and de Bézenac, Emmanuel and Pajot, Arthur and Gallinari, Patrick},
  doi          = {10.1007/s10994-022-06139-2},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2349-2380},
  shortjournal = {Mach. Learn.},
  title        = {Modelling spatiotemporal dynamics from earth observation data with neural differential equations},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online active classification via margin-based and
feature-based label queries. <em>ML</em>, <em>111</em>(6), 2323–2348.
(<a href="https://doi.org/10.1007/s10994-022-06133-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paradigm of online active classification, the learner not only has to predict the label of each incoming instance, but also must decide whether the true label of that instance should be supplied, or not. The overall goal is to minimize the number of prediction mistakes with few label queries. In this paper, we focus on a novel framework for online active learning, with the aim of handling high dimensional classification problems. The key component of our framework is to exploit both the margin-based predictive uncertainty and the feature-based discriminative information of the current instance, in order to determine whether it should be labeled. Based on this labeling strategy, we propose several online active learning algorithms, for both binary classification tasks and multiclass ones. For these algorithms, which use adaptive subgradient methods for updating their linear model, expected mistake bounds are provided. Experiments on high-dimensional (binary and multiclass) classification datasets reveal the benefit of our label query strategy, and show the superiority of our algorithms over the existing ones.},
  archive      = {J_ML},
  author       = {Zhai, Tingting and Koriche, Frédéric and Gao, Yang and Zhu, Junwu and Li, Bin},
  doi          = {10.1007/s10994-022-06133-8},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2323-2348},
  shortjournal = {Mach. Learn.},
  title        = {Online active classification via margin-based and feature-based label queries},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast spectral analysis for approximate nearest neighbor
search. <em>ML</em>, <em>111</em>(6), 2297–2322. (<a
href="https://doi.org/10.1007/s10994-021-06124-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale machine learning, of central interest is the problem of approximate nearest neighbor (ANN) search, where the goal is to query particular points that are close to a given object under certain metric. In this paper, we develop a novel data-driven ANN search algorithm where the data structure is learned by fast spectral technique based on s landmarks selected by approximate ridge leverage scores. We show that with overwhelming probability, our algorithm returns the $$(1+\epsilon /4)$$ -ANN for any approximation parameter $$\epsilon \in (0,1)$$ . A remarkable feature of our algorithm is that it is computationally efficient. Specifically, learning k-length hash codes requires $$O((s^3+ns^2)\log n)$$ running time and $$O(d^2)$$ extra space, and returning the $$(1+\epsilon /4)$$ -ANN of the query needs $$O(k\log n)$$ running time. The experimental results on computer vision and natural language understanding tasks demonstrate the significant advantage of our algorithm compared to state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Wang, Jing and Shen, Jie},
  doi          = {10.1007/s10994-021-06124-1},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2297-2322},
  shortjournal = {Mach. Learn.},
  title        = {Fast spectral analysis for approximate nearest neighbor search},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DEFT: Distilling entangled factors by preventing information
diffusion. <em>ML</em>, <em>111</em>(6), 2275–2295. (<a
href="https://doi.org/10.1007/s10994-022-06134-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentanglement is a highly desirable property of representation owing to its similarity to human understanding and reasoning. Many works achieve disentanglement upon information bottlenecks. Despite their elegant mathematical foundations, the IB branch usually exhibits lower performance. In order to provide an insight into the problem, we develop an annealing test to calculate the information freezing point (IFP), which is a transition state to freeze information into the latent variables. We also explore this clue or inductive bias for separating the entangled factors according to the differences in the IFP distributions. We found the existing approaches suffer from the information diffusion problem, according to which the increased information diffuses in all latent variables. Based on this insight, we propose a novel disentanglement framework, termed the distilling entangled factor (DEFT), to address the information diffusion problem by scaling backward information. DEFT applies a multistage training strategy, including multigroup encoders with different learning rates and piecewise pressure, to disentangle the factors stage by stage. We evaluate DEFT on three variants of dSprites and SmallNORB, which shows low-variance and high-level disentanglement scores. Furthermore, the experiment under the correlative factors demonstrates incapable of TC-based approaches. DEFT also exhibits a competitive performance in the unsupervised setting.},
  archive      = {J_ML},
  author       = {Jiantao and Wang, Lin and Yang, Bo and Li, Fanqi and Liu, Chunxiuzi and Zhou, Jin},
  doi          = {10.1007/s10994-022-06134-7},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2275-2295},
  shortjournal = {Mach. Learn.},
  title        = {DEFT: Distilling entangled factors by preventing information diffusion},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning for potential: Efficient safe reinforcement
learning. <em>ML</em>, <em>111</em>(6), 2255–2274. (<a
href="https://doi.org/10.1007/s10994-022-06143-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has shown remarkable success in artificial domains and in some real-world applications. However, substantial challenges remain such as learning efficiently under safety constraints. Adherence to safety constraints is a hard requirement in many high-impact application domains such as healthcare and finance. These constraints are preferably represented symbolically to ensure clear semantics at a suitable level of abstraction. Existing approaches to safe DRL assume that being unsafe leads to low rewards. We show that this is a special case of symbolically constrained RL and analyze a generic setting in which total reward and being safe may or may not be correlated. We analyze the impact of symbolic constraints and identify a connection between expected future reward and distance towards a goal in an automaton representation of the constraints. We use this connection in an algorithm for learning complex behaviors safely and efficiently. This algorithm relies on symbolic reasoning over safety constraints to improve the efficiency of a subsymbolic learner with a symbolically obtained measure of progress. We measure sample efficiency on a grid world and a conversational product recommender with real-world constraints. The so-called Planning for Potential algorithm converges quickly and significantly outperforms all baselines. Specifically, we find that symbolic reasoning is necessary for safety during and after learning and can be effectively used to guide a neural learner towards promising areas of the solution space. We conclude that RL can be applied both safely and efficiently when combined with symbolic reasoning.},
  archive      = {J_ML},
  author       = {den Hengst, Floris and François-Lavet, Vincent and Hoogendoorn, Mark and van Harmelen, Frank},
  doi          = {10.1007/s10994-022-06143-6},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2255-2274},
  shortjournal = {Mach. Learn.},
  title        = {Planning for potential: Efficient safe reinforcement learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nyström landmark sampling and regularized christoffel
functions. <em>ML</em>, <em>111</em>(6), 2213–2254. (<a
href="https://doi.org/10.1007/s10994-022-06165-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting diverse and important items, called landmarks, from a large set is a problem of interest in machine learning. As a specific example, in order to deal with large training sets, kernel methods often rely on low rank matrix Nyström approximations based on the selection or sampling of landmarks. In this context, we propose a deterministic and a randomized adaptive algorithm for selecting landmark points within a training data set. These landmarks are related to the minima of a sequence of kernelized Christoffel functions. Beyond the known connection between Christoffel functions and leverage scores, a connection of our method with finite determinantal point processes (DPPs) is also explained. Namely, our construction promotes diversity among important landmark points in a way similar to DPPs. Also, we explain how our randomized adaptive algorithm can influence the accuracy of Kernel Ridge Regression.},
  archive      = {J_ML},
  author       = {Fanuel, Michaël and Schreurs, Joachim and Suykens, Johan A. K.},
  doi          = {10.1007/s10994-022-06165-0},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2213-2254},
  shortjournal = {Mach. Learn.},
  title        = {Nyström landmark sampling and regularized christoffel functions},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The backbone method for ultra-high dimensional sparse
machine learning. <em>ML</em>, <em>111</em>(6), 2161–2212. (<a
href="https://doi.org/10.1007/s10994-021-06123-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the backbone method, a general framework that enables sparse and interpretable supervised machine learning methods to scale to ultra-high dimensional problems. We solve sparse regression problems with $$10^7$$ features in minutes and $$10^8$$ features in hours, as well as decision tree problems with $$10^5$$ features in minutes. The proposed method operates in two phases: we first determine the backbone set, consisting of potentially relevant features, by solving a number of tractable subproblems; then, we solve a reduced problem, considering only the backbone features. For the sparse regression problem, our theoretical analysis shows that, under certain assumptions and with high probability, the backbone set consists of the truly relevant features. Numerical experiments on both synthetic and real-world datasets demonstrate that our method outperforms or competes with state-of-the-art methods in ultra-high dimensional problems, and competes with optimal solutions in problems where exact methods scale, both in terms of recovering the truly relevant features and in its out-of-sample predictive performance.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Digalakis, Vassilis},
  doi          = {10.1007/s10994-021-06123-2},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2161-2212},
  shortjournal = {Mach. Learn.},
  title        = {The backbone method for ultra-high dimensional sparse machine learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Receiver operating characteristic (ROC) curves:
Equivalences, beta model, and minimum distance estimation. <em>ML</em>,
<em>111</em>(6), 2147–2159. (<a
href="https://doi.org/10.1007/s10994-021-06115-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic (ROC) curves are used ubiquitously to evaluate scores, features, covariates or markers as potential predictors in binary problems. We characterize ROC curves from a probabilistic perspective and establish an equivalence between ROC curves and cumulative distribution functions (CDFs). These results support a subtle shift of paradigms in the statistical modelling of ROC curves, which we view as curve fitting. We propose the flexible two-parameter beta family for fitting CDFs to empirical ROC curves and derive the large sample distribution of minimum distance estimators in general parametric settings. In a range of empirical examples the beta family fits better than the classical binormal model, particularly under the vital constraint of the fitted curve being concave.},
  archive      = {J_ML},
  author       = {Gneiting, Tilmann and Vogel, Peter},
  doi          = {10.1007/s10994-021-06115-2},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2147-2159},
  shortjournal = {Mach. Learn.},
  title        = {Receiver operating characteristic (ROC) curves: Equivalences, beta model, and minimum distance estimation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Policy space identification in configurable environments.
<em>ML</em>, <em>111</em>(6), 2093–2145. (<a
href="https://doi.org/10.1007/s10994-021-06033-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of identifying the policy space available to an agent in a learning process, having access to a set of demonstrations generated by the agent playing the optimal policy in the considered space. We introduce an approach based on frequentist statistical testing to identify the set of policy parameters that the agent can control, within a larger parametric policy space. After presenting two identification rules (combinatorial and simplified), applicable under different assumptions on the policy space, we provide a probabilistic analysis of the simplified one in the case of linear policies belonging to the exponential family. To improve the performance of our identification rules, we make use of the recently introduced framework of the Configurable Markov Decision Processes, exploiting the opportunity of configuring the environment to induce the agent to reveal which parameters it can control. Finally, we provide an empirical evaluation, on both discrete and continuous domains, to prove the effectiveness of our identification rules.},
  archive      = {J_ML},
  author       = {Metelli, Alberto Maria and Manneschi, Guglielmo and Restelli, Marcello},
  doi          = {10.1007/s10994-021-06033-3},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2093-2145},
  shortjournal = {Mach. Learn.},
  title        = {Policy space identification in configurable environments},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Randomized approximate class-specific kernel spectral
regression analysis for large-scale face verification. <em>ML</em>,
<em>111</em>(6), 2037–2091. (<a
href="https://doi.org/10.1007/s10994-022-06140-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space. The approximate class-specific kernel spectral regression (ACS-KSR) method is a powerful tool for face verification. This method consists of two steps: an eigenanalysis step and a kernel regression step, however, it may suffer from heavily computational overhead in practice, especially for large-sample data sets. In this paper, we propose two randomized algorithms based on the ACS-KSR method. The main contribution of our work is four-fold. First, we point out that the formula utilized in the eigenanalysis step of the ACS-KSR method is mathematically incomplete, and we give a correction to it. Moreover, we consider how to efficiently solve the ratio-trace problem and the trace-ratio problem involved in this method. Second, it is well known that kernel matrix is approximately low-rank, however, to the best of our knowledge, there are few theoretical results that can provide simple and feasible strategies to determine the numerical rank of a kernel matrix without forming it explicitly. To fill-in this gap, we focus on the commonly used Gaussian kernel and provide a practical strategy for determining numerical rank of the kernel matrix. Third, based on numerically low-rank property of the kernel matrix, we propose a modified Nyström method with fixed-rank for the kernel regression step, and establish a probabilistic error bound on the approximation. Fourth, although the proposed Nyström method can reduce the computational cost of the original method, it is required to form and store the reduced kernel matrix explicitly. This is unfavorable to extremely large-sample data sets. To settle this problem, we propose a randomized block Kaczmarz method for kernel regression problem with multiple right-hand sides, in which there is no need to compute and store the reduced kernel matrix explicitly. The convergence of this method is established. Comprehensive numerical experiments on real-world data sets are performed to show the effectiveness of our theoretical results and the efficiency of the proposed methods.},
  archive      = {J_ML},
  author       = {Li, Ke and Wu, Gang},
  doi          = {10.1007/s10994-022-06140-9},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2037-2091},
  shortjournal = {Mach. Learn.},
  title        = {Randomized approximate class-specific kernel spectral regression analysis for large-scale face verification},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stream-based active learning for sliding windows under the
influence of verification latency. <em>ML</em>, <em>111</em>(6),
2011–2036. (<a
href="https://doi.org/10.1007/s10994-021-06099-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream-based active learning (AL) strategies minimize the labeling effort by querying labels that improve the classifier’s performance the most. So far, these strategies neglect the fact that an oracle or expert requires time to provide a queried label. We show that existing AL methods deteriorate or even fail under the influence of such verification latency. The problem with these methods is that they estimate a label’s utility on the currently available labeled data. However, when this label would arrive, some of the current data may have gotten outdated and new labels have arrived. In this article, we propose to simulate the available data at the time when the label would arrive. Therefore, our method Forgetting and Simulating (FS) forgets outdated information and simulates the delayed labels to get more realistic utility estimates. We assume to know the label’s arrival date a priori and the classifier’s training data to be bounded by a sliding window. Our extensive experiments show that FS improves stream-based AL strategies in settings with both, constant and variable verification latency.},
  archive      = {J_ML},
  author       = {Pham, Tuan and Kottke, Daniel and Krempl, Georg and Sick, Bernhard},
  doi          = {10.1007/s10994-021-06099-z},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {2011-2036},
  shortjournal = {Mach. Learn.},
  title        = {Stream-based active learning for sliding windows under the influence of verification latency},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JGPR: A computationally efficient multi-target gaussian
process regression algorithm. <em>ML</em>, <em>111</em>(6), 1987–2010.
(<a href="https://doi.org/10.1007/s10994-022-06170-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target regression algorithms are designed to predict multiple outputs at the same time, and allow us to take all output variables into account during the training phase. Despite the recent advances, this context of machine learning is still an open challenge for developing a low-cost and high accurate algorithm. The main challenge in multi-target regression algorithms is how to use different targets’ information in the training and/or test phases. In this paper, we introduce a low-cost multi-target Gaussian process regression (GPR) algorithm, called joint GPR (JGPR) that employs a shared covariance matrix among the targets during the training phase and solves a sub-optimal cost function for optimization of hyperparameters. The proposed strategy reduces the computational complexity considerably during the training and test phases and simultaneously avoids overfitting of the multi-target regression algorithm upon the targets. We have performed extensive experiments on both simulated data and 18 benchmark datasets to assess the proposed method compared with other multi-target regression algorithms. Experimental results show that the proposed JGPR outperforms the state-of-the-art approaches on most of the given benchmark datasets.},
  archive      = {J_ML},
  author       = {Nabati, Mohammad and Ghorashi, Seyed Ali and Shahbazian, Reza},
  doi          = {10.1007/s10994-022-06170-3},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1987-2010},
  shortjournal = {Mach. Learn.},
  title        = {JGPR: A computationally efficient multi-target gaussian process regression algorithm},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One-stage tree: End-to-end tree builder and pruner.
<em>ML</em>, <em>111</em>(5), 1959–1985. (<a
href="https://doi.org/10.1007/s10994-021-06094-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees have favorable properties, including interpretability, high computational efficiency, and the ability to learn from little training data. Learning a decision tree is known to be NP-complete. The researchers have proposed many greedy algorithms such as CART to learn approximate solutions. Inspired by the current popular neural networks, soft trees that support end-to-end training with back-propagation have attracted more and more attention. However, existing soft trees either lose the interpretability due to the continuous relaxation or employ the two-stage method of end-to-end building and then pruning. In this paper, we propose One-Stage Tree to build and prune the decision tree jointly through a bilevel optimization problem. Moreover, we leverage the reparameterization trick and proximal iterations to keep the tree discrete during end-to-end training. As a result, One-Stage Tree reduces the performance gap between training and testing and maintains the advantage of interpretability. Extensive experiments demonstrate that the proposed One-Stage Tree outperforms CART and the existing soft trees on classification and regression tasks.},
  archive      = {J_ML},
  author       = {Xu, Zhuoer and Zhu, Guanghui and Yuan, Chunfeng and Huang, Yihua},
  doi          = {10.1007/s10994-021-06094-4},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1959-1985},
  shortjournal = {Mach. Learn.},
  title        = {One-stage tree: End-to-end tree builder and pruner},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embedding and extraction of knowledge in tree ensemble
classifiers. <em>ML</em>, <em>111</em>(5), 1925–1958. (<a
href="https://doi.org/10.1007/s10994-021-06068-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The embedding and extraction of knowledge is a recent trend in machine learning applications, e.g., to supplement training datasets that are small. Whilst, as the increasing use of machine learning models in security-critical applications, the embedding and extraction of malicious knowledge are equivalent to the notorious backdoor attack and defence, respectively. This paper studies the embedding and extraction of knowledge in tree ensemble classifiers, and focuses on knowledge expressible with a generic form of Boolean formulas, e.g., point-wise robustness and backdoor attacks. For the embedding, it is required to be preservative (the original performance of the classifier is preserved), verifiable (the knowledge can be attested), and stealthy (the embedding cannot be easily detected). To facilitate this, we propose two novel, and effective embedding algorithms, one of which is for black-box settings and the other for white-box settings. The embedding can be done in PTIME. Beyond the embedding, we develop an algorithm to extract the embedded knowledge, by reducing the problem to be solvable with an SMT (satisfiability modulo theories) solver. While this novel algorithm can successfully extract knowledge, the reduction leads to an NP computation. Therefore, if applying embedding as backdoor attacks and extraction as defence, our results suggest a complexity gap (P vs. NP) between the attack and defence when working with tree ensemble classifiers. We apply our algorithms to a diverse set of datasets to validate our conclusion extensively.},
  archive      = {J_ML},
  author       = {Huang, Wei and Zhao, Xingyu and Huang, Xiaowei},
  doi          = {10.1007/s10994-021-06068-6},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1925-1958},
  shortjournal = {Mach. Learn.},
  title        = {Embedding and extraction of knowledge in tree ensemble classifiers},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scrutinizing XAI using linear ground-truth data with
suppressor variables. <em>ML</em>, <em>111</em>(5), 1903–1923. (<a
href="https://doi.org/10.1007/s10994-022-06167-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of ‘explainable AI’ (XAI). Saliency methods rank input features according to some measure of ‘importance’. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, serving as a benchmark to study the problem of suppressor variables. We evaluate common explanation methods including LRP, DTD, PatternNet, PatternAttribution, LIME, Anchors, SHAP, and permutation-based methods with respect to our objective definition. We show that most of these methods are unable to distinguish important features from suppressors in this setting.},
  archive      = {J_ML},
  author       = {Wilming, Rick and Budding, Céline and Müller, Klaus-Robert and Haufe, Stefan},
  doi          = {10.1007/s10994-022-06167-y},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1903-1923},
  shortjournal = {Mach. Learn.},
  title        = {Scrutinizing XAI using linear ground-truth data with suppressor variables},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Order preserving hierarchical agglomerative clustering.
<em>ML</em>, <em>111</em>(5), 1851–1901. (<a
href="https://doi.org/10.1007/s10994-021-06125-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial orders and directed acyclic graphs are commonly recurring data structures that arise naturally in numerous domains and applications and are used to represent ordered relations between entities in the domains. Examples are task dependencies in a project plan, transaction order in distributed ledgers and execution sequences of tasks in computer programs, just to mention a few. We study the problem of order preserving hierarchical clustering of this kind of ordered data. That is, if we have $$a&lt;b$$ in the original data and denote their respective clusters by [a] and [b], then we shall have $$[a]&lt;[b]$$ in the produced clustering. The clustering is similarity based and uses standard linkage functions, such as single- and complete linkage, and is an extension of classical hierarchical clustering. To achieve this, we develop a novel theory that extends classical hierarchical clustering to strictly partially ordered sets. We define the output from running classical hierarchical clustering on strictly ordered data to be partial dendrograms; sub-trees of classical dendrograms with several connected components. We then construct an embedding of partial dendrograms over a set into the family of ultrametrics over the same set. An optimal hierarchical clustering is defined as the partial dendrogram corresponding to the ultrametric closest to the original dissimilarity measure, measured in the p-norm. Thus, the method is a combination of classical hierarchical clustering and ultrametric fitting. A reference implementation is employed for experiments on both synthetic random data and real world data from a database of machine parts. When compared to existing methods, the experiments show that our method excels both in cluster quality and order preservation.},
  archive      = {J_ML},
  author       = {Bakkelund, Daniel},
  doi          = {10.1007/s10994-021-06125-0},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1851-1901},
  shortjournal = {Mach. Learn.},
  title        = {Order preserving hierarchical agglomerative clustering},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAGMA: Inference and prediction using multi-task gaussian
processes with common mean. <em>ML</em>, <em>111</em>(5), 1821–1849. (<a
href="https://doi.org/10.1007/s10994-022-06172-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel multi-task Gaussian process (GP) framework is proposed, by using a common mean process for sharing information across tasks. In particular, we investigate the problem of time series forecasting, with the objective to improve multiple-step-ahead predictions. The common mean process is defined as a GP for which the hyper-posterior distribution is tractable. Therefore an EM algorithm is derived for handling both hyper-parameters optimisation and hyper-posterior computation. Unlike previous approaches in the literature, the model fully accounts for uncertainty and can handle irregular grids of observations while maintaining explicit formulations, by modelling the mean process in a unified GP framework. Predictive analytical equations are provided, integrating information shared across tasks through a relevant prior mean. This approach greatly improves the predictive performances, even far from observations, and may reduce significantly the computational complexity compared to traditional multi-task GP models. Our overall algorithm is called Magma (standing for Multi tAsk GPs with common MeAn). The quality of the mean process estimation, predictive performances, and comparisons to alternatives are assessed in various simulated scenarios and on real datasets.},
  archive      = {J_ML},
  author       = {Leroy, Arthur and Latouche, Pierre and Guedj, Benjamin and Gey, Servane},
  doi          = {10.1007/s10994-022-06172-1},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1821-1849},
  shortjournal = {Mach. Learn.},
  title        = {MAGMA: Inference and prediction using multi-task gaussian processes with common mean},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating contrastive explanations for inductive logic
programming based on a near miss approach. <em>ML</em>, <em>111</em>(5),
1799–1820. (<a
href="https://doi.org/10.1007/s10994-021-06048-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent research, human-understandable explanations of machine learning models have received a lot of attention. Often explanations are given in form of model simplifications or visualizations. However, as shown in cognitive science as well as in early AI research, concept understanding can also be improved by the alignment of a given instance for a concept with a similar counterexample. Contrasting a given instance with a structurally similar example which does not belong to the concept highlights what characteristics are necessary for concept membership. Such near misses have been proposed by Winston (Learning structural descriptions from examples, 1970) as efficient guidance for learning in relational domains. We introduce an explanation generation algorithm for relational concepts learned with Inductive Logic Programming (GeNME). The algorithm identifies near miss examples from a given set of instances and ranks these examples by their degree of closeness to a specific positive instance. A modified rule which covers the near miss but not the original instance is given as an explanation. We illustrate GeNME with the well-known family domain consisting of kinship relations, the visual relational Winston arches domain, and a real-world domain dealing with file management. We also present a psychological experiment comparing human preferences of rule-based, example-based, and near miss explanations in the family and the arches domains.},
  archive      = {J_ML},
  author       = {Rabold, Johannes and Siebers, Michael and Schmid, Ute},
  doi          = {10.1007/s10994-021-06048-w},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1799-1820},
  shortjournal = {Mach. Learn.},
  title        = {Generating contrastive explanations for inductive logic programming based on a near miss approach},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-lipschitz functions and machine learning for discrete
dynamical systems on graphs. <em>ML</em>, <em>111</em>(5), 1765–1797.
(<a href="https://doi.org/10.1007/s10994-022-06130-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a directed tree $${\mathcal {U}}$$ and the space of all finite walks on it endowed with a quasi-pseudo-metric—the space of the strategies $${\mathcal {S}}$$ on the graph,—which represent the possible changes in the evolution of a dynamical system over time. Consider a reward function acting in a subset $${\mathcal {S}}_0 \subset {\mathcal {S}}$$ which measures the success. Using well-known facts of the theory of semi-Lipschitz functions in quasi-pseudo-metric spaces, we extend the reward function to the whole space $${\mathcal {S}}.$$ We obtain in this way an oracle function, which gives a forecast of the reward function for the elements of $${\mathcal {S}}$$ , that is, an estimate of the degree of success for any given strategy. After explaining the fundamental properties of a specific quasi-pseudo-metric that we define for the (graph) trees (the bifurcation quasi-pseudo-metric), we focus our attention on analyzing how this structure can be used to represent dynamical systems on graphs. We begin the explanation of the method with a simple example, which is proposed as a reference point for which some variants and successive generalizations are consecutively shown. The main objective is to explain the role of the lack of symmetry of quasi-metrics in our proposal: the irreversibility of dynamical processes is reflected in the asymmetry of their definition.},
  archive      = {J_ML},
  author       = {Falciani, H. and Sánchez-Pérez, E. A.},
  doi          = {10.1007/s10994-022-06130-x},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1765-1797},
  shortjournal = {Mach. Learn.},
  title        = {Semi-lipschitz functions and machine learning for discrete dynamical systems on graphs},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised latent block model with pairwise
constraints. <em>ML</em>, <em>111</em>(5), 1739–1764. (<a
href="https://doi.org/10.1007/s10994-022-06137-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering aims at simultaneously partitioning both dimensions of a data matrix. It has demonstrated better performances than one-sided clustering for high-dimensional data. The Latent Block Model (LBM) is a probabilistic model for co-clustering based on mixture models that has proven useful for a broad class of data. In this paper, we propose to leverage prior knowledge in the form of pairwise semi-supervision in both row and column space to improve the clustering performances of the algorithms derived from this model. We present a general probabilistic framework for incorporating must link and cannot link relationships in the LBM based on Hidden Markov Random Fields. We instantiate this framework on a model for count data and present two inference algorithms based on Variational and Classification EM. Extensive experiments on simulated data and on real-world attributed networks confirm the interest of our approach and demonstrate the effectiveness of our algorithms.},
  archive      = {J_ML},
  author       = {Riverain, Paul and Fossier, Simon and Nadif, Mohamed},
  doi          = {10.1007/s10994-022-06137-4},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1739-1764},
  shortjournal = {Mach. Learn.},
  title        = {Semi-supervised latent block model with pairwise constraints},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partitioned hybrid learning of bayesian network structures.
<em>ML</em>, <em>111</em>(5), 1695–1738. (<a
href="https://doi.org/10.1007/s10994-022-06145-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel hybrid method for Bayesian network structure learning called partitioned hybrid greedy search (pHGS), composed of three distinct yet compatible new algorithms: Partitioned PC (pPC) accelerates skeleton learning via a divide-and-conquer strategy, p-value adjacency thresholding (PATH) effectively accomplishes parameter tuning with a single execution, and hybrid greedy initialization (HGI) maximally utilizes constraint-based information to obtain a high-scoring and well-performing initial graph for greedy search. We establish structure learning consistency of our algorithms in the large-sample limit, and empirically validate our methods individually and collectively through extensive numerical comparisons. The combined merits of pPC and PATH achieve significant computational reductions compared to the PC algorithm without sacrificing the accuracy of estimated structures, and our generally applicable HGI strategy reliably improves the estimation structural accuracy of popular hybrid algorithms with negligible additional computational expense. Our empirical results demonstrate the competitive empirical performance of pHGS against many state-of-the-art structure learning algorithms.},
  archive      = {J_ML},
  author       = {Huang, Jireh and Zhou, Qing},
  doi          = {10.1007/s10994-022-06145-4},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1695-1738},
  shortjournal = {Mach. Learn.},
  title        = {Partitioned hybrid learning of bayesian network structures},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nested aggregation of experts using inducing points for
approximated gaussian process regression. <em>ML</em>, <em>111</em>(5),
1671–1694. (<a
href="https://doi.org/10.1007/s10994-021-06101-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process regression is a flexible regression scheme but suffers from its high computational complexity regarding the inversion of a matrix with the same size as the training dataset. Aggregation method is one of the approximation techniques for reducing the complexity. In this paper, we propose a novel aggregation method, Nested Aggregation of Experts using Inducing Points (NAE-IP), which is an extension of a conventional method and enables dimensionality reduction by making use of the idea of linear sketching. There are some options for selecting inducing points in the proposed method. The options can introduce test points of interest as inducing points, albeit at the cost of slightly higher computational complexity. The other options exploiting less informative inducing points can yield a significant reduction of the computational complexity. The proposed NAE-IP is theoretically guaranteed to have consistency under certain conditions. Results of our computational experiments using synthetic and real data show that the proposed method achieves lower prediction error and even lower computing time than conventional methods.},
  archive      = {J_ML},
  author       = {Nakai-Kasai, Ayano and Tanaka, Toshiyuki},
  doi          = {10.1007/s10994-021-06101-8},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1671-1694},
  shortjournal = {Mach. Learn.},
  title        = {Nested aggregation of experts using inducing points for approximated gaussian process regression},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal transport for conditional domain matching and label
shift. <em>ML</em>, <em>111</em>(5), 1651–1670. (<a
href="https://doi.org/10.1007/s10994-021-06088-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of unsupervised domain adaptation under the setting of generalized target shift (joint class-conditional and label shifts). For this framework, we theoretically show that, for good generalization, it is necessary to learn a latent representation in which both marginals and class-conditional distributions are aligned across domains. For this sake, we propose a learning problem that minimizes importance weighted loss in the source domain and a Wasserstein distance between weighted marginals. For a proper weighting, we provide an estimator of target label proportion by blending mixture estimation and optimal matching by optimal transport. This estimation comes with theoretical guarantees of correctness under mild assumptions. Our experimental results show that our method performs better on average than competitors across a range domain adaptation problems including digits,VisDA and Office. Code for this paper is available at https://github.com/arakotom/mars_domain_adaptation .},
  archive      = {J_ML},
  author       = {Rakotomamonjy, A. and Flamary, R. and Gasso, G. and Alaya, M. El and Berar, M. and Courty, N.},
  doi          = {10.1007/s10994-021-06088-2},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1651-1670},
  shortjournal = {Mach. Learn.},
  title        = {Optimal transport for conditional domain matching and label shift},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust linear classification from limited training data.
<em>ML</em>, <em>111</em>(5), 1621–1649. (<a
href="https://doi.org/10.1007/s10994-021-06093-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of linear classification under general loss functions in the limited-data setting. Overfitting is a common problem here. The standard approaches to prevent overfitting are dimensionality reduction and regularization. But dimensionality reduction loses information, while regularization requires the user to choose a norm, or a prior, or a distance metric. We propose an algorithm called RoLin that needs no user choice and applies to a large class of loss functions. RoLin combines “reliable” information from the top principal components with a robust optimization to extract any useful information from “unreliable” subspaces. It also includes a new robust cross-validation that is better than existing cross-validation methods in the limited-data setting. Experiments on 25 real-world datasets and three standard loss functions show that RoLin broadly outperforms both dimensionality reduction and regularization. Dimensionality reduction has $$14\%-40\%$$ worse test loss on average as compared to RoLin. Against $$L_1$$ and $$L_2$$ regularization, RoLin can be up to 3x better for logistic loss and 12x better for squared hinge loss. The differences are greatest for small sample sizes, where RoLin achieves the best loss on 2x to 3x more datasets than any competing method. For some datasets, RoLin with 15 training samples is better than the best norm-based regularization with 1500 samples.},
  archive      = {J_ML},
  author       = {Chakrabarti, Deepayan},
  doi          = {10.1007/s10994-021-06093-5},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1621-1649},
  shortjournal = {Mach. Learn.},
  title        = {Robust linear classification from limited training data},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving sequential latent variable models with
autoregressive flows. <em>ML</em>, <em>111</em>(4), 1597–1620. (<a
href="https://doi.org/10.1007/s10994-021-06092-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for improving sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving frame of reference, removing temporal correlations and simplifying the modeling of higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone flow-based models and as a component within sequential latent variable models. Results are presented on three benchmark video datasets and three other time series datasets, where autoregressive flow-based dynamics improve log-likelihood performance over baseline models. Finally, we illustrate the decorrelation and improved generalization properties of using flow-based dynamics.},
  archive      = {J_ML},
  author       = {Marino, Joseph and Chen, Lei and He, Jiawei and Mandt, Stephan},
  doi          = {10.1007/s10994-021-06092-6},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1597-1620},
  shortjournal = {Mach. Learn.},
  title        = {Improving sequential latent variable models with autoregressive flows},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stochastic approach to handle resource constraints as
knapsack problems in ensemble pruning. <em>ML</em>, <em>111</em>(4),
1551–1595. (<a
href="https://doi.org/10.1007/s10994-021-06109-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble-based methods are highly popular approaches that increase the accuracy of a decision by aggregating the opinions of individual voters. The common point is to maximize accuracy; however, a natural limitation occurs if incremental costs are also assigned to the individual voters. Consequently, we investigate creating ensembles under an additional constraint on the total cost of the members. This task can be formulated as a knapsack problem, where the energy is the ensemble accuracy formed by some aggregation rules. However, the generally applied aggregation rules lead to a nonseparable energy function, which takes the common solution tools—such as dynamic programming—out of action. We introduce a novel stochastic approach that considers the energy as the joint probability function of the member accuracies. This type of knowledge can be efficiently incorporated in a stochastic search process as a stopping rule, since we have the information on the expected accuracy or, alternatively, the probability of finding more accurate ensembles. Experimental analyses of the created ensembles of pattern classifiers and object detectors confirm the efficiency of our approach over other pruning ones. Moreover, we propose a novel stochastic search method that better fits the energy, which can be incorporated in other stochastic strategies as well.},
  archive      = {J_ML},
  author       = {Hajdu, András and Terdik, György and Tiba, Attila and Tomán, Henrietta},
  doi          = {10.1007/s10994-021-06109-0},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1551-1595},
  shortjournal = {Mach. Learn.},
  title        = {A stochastic approach to handle resource constraints as knapsack problems in ensemble pruning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detect, understand, act: A neuro-symbolic hierarchical
reinforcement learning framework. <em>ML</em>, <em>111</em>(4),
1523–1549. (<a
href="https://doi.org/10.1007/s10994-022-06142-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce Detect, Understand, Act (DUA), a neuro-symbolic reinforcement learning framework. The Detect component is composed of a traditional computer vision object detector and tracker. The Act component houses a set of options, high-level actions enacted by pre-trained deep reinforcement learning (DRL) policies. The Understand component provides a novel answer set programming (ASP) paradigm for symbolically implementing a meta-policy over options and effectively learning it using inductive logic programming (ILP). We evaluate our framework on the Animal-AI (AAI) competition testbed, a set of physical cognitive reasoning problems. Given a set of pre-trained DRL policies, DUA requires only a few examples to learn a meta-policy that allows it to improve the state-of-the-art on multiple of the most challenging categories from the testbed. DUA constitutes the first holistic hybrid integration of computer vision, ILP and DRL applied to an AAI-like environment and sets the foundations for further use of ILP in complex DRL challenges.},
  archive      = {J_ML},
  author       = {Mitchener, Ludovico and Tuckey, David and Crosby, Matthew and Russo, Alessandra},
  doi          = {10.1007/s10994-022-06142-7},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1523-1549},
  shortjournal = {Mach. Learn.},
  title        = {Detect, understand, act: A neuro-symbolic hierarchical reinforcement learning framework},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lipschitzness is all you need to tame off-policy generative
adversarial imitation learning. <em>ML</em>, <em>111</em>(4), 1431–1521.
(<a href="https://doi.org/10.1007/s10994-022-06144-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward shaping methods, which makes the base method it is plugged into provably more robust, as shown in several additional theoretical guarantees. We then discuss these through a fine-grained lens and share our insights. Crucially, the guarantees derived and reported in this work are valid for any reward satisfying the Lipschitzness condition, nothing is specific to imitation. As such, these may be of independent interest.},
  archive      = {J_ML},
  author       = {Blondé, Lionel and Strasser, Pablo and Kalousis, Alexandros},
  doi          = {10.1007/s10994-022-06144-5},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1431-1521},
  shortjournal = {Mach. Learn.},
  title        = {Lipschitzness is all you need to tame off-policy generative adversarial imitation learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-aware tensor decomposition for sparse tensors.
<em>ML</em>, <em>111</em>(4), 1409–1430. (<a
href="https://doi.org/10.1007/s10994-021-06059-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a sparse time-evolving tensor, how can we effectively factorize it to accurately discover latent patterns? Tensor decomposition has been extensively utilized for analyzing various multi-dimensional real-world data. However, existing tensor decomposition models have disregarded the temporal property for tensor decomposition while most real-world data are closely related to time. Moreover, they do not address accuracy degradation due to the sparsity of time slices. The essential problems of how to exploit the temporal property for tensor decomposition and consider the sparsity of time slices remain unresolved. In this paper, we propose time-aware tensor decomposition (tatd), an accurate tensor decomposition method for sparse temporal tensors. tatd is designed to exploit time dependency and time-varying sparsity of real-world temporal tensors. We propose a new smoothing regularization with Gaussian kernel for modeling time dependency. Moreover, we improve the performance of tatd by considering time-varying sparsity. We design an alternating optimization scheme suitable for temporal tensor decomposition with our smoothing regularization. Extensive experiments show that tatd provides the state-of-the-art accuracy for decomposing temporal tensors.},
  archive      = {J_ML},
  author       = {Ahn, Dawon and Jang, Jun-Gi and Kang, U},
  doi          = {10.1007/s10994-021-06059-7},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1409-1430},
  shortjournal = {Mach. Learn.},
  title        = {Time-aware tensor decomposition for sparse tensors},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised anomaly detection in multivariate time series
with online evolving spiking neural networks. <em>ML</em>,
<em>111</em>(4), 1377–1408. (<a
href="https://doi.org/10.1007/s10994-022-06129-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for digital products, processes and services the research area of automatic detection of signal outliers in streaming data has gained a lot of attention. The range of possible applications for this kind of algorithms is versatile and ranges from the monitoring of digital machinery and predictive maintenance up to applications in analyzing big data healthcare sensor data. In this paper we present a method for detecting anomalies in streaming multivariate times series by using an adapted evolving Spiking Neural Network. As the main components of this work we contribute (1) an alternative rank-order-based learning algorithm which uses the precise times of the incoming spikes for adjusting the synaptic weights, (2) an adapted, realtime-capable and efficient encoding technique for multivariate data based on multi-dimensional Gaussian Receptive Fields and (3) a continuous outlier scoring function for an improved interpretability of the classifications. Spiking neural networks are extremely efficient when it comes to process time dependent information. We demonstrate the effectiveness of our model on a synthetic dataset based on the Numenta Anomaly Benchmark with various anomaly types. We compare our algorithm to other streaming anomaly detecting algorithms and can prove that our algorithm performs better in detecting anomalies while demanding less computational resources for processing high dimensional data.},
  archive      = {J_ML},
  author       = {Bäßler, Dennis and Kortus, Tobias and Gühring, Gabriele},
  doi          = {10.1007/s10994-022-06129-4},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1377-1408},
  shortjournal = {Mach. Learn.},
  title        = {Unsupervised anomaly detection in multivariate time series with online evolving spiking neural networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient SVDD sampling with approximation guarantees for
the decision boundary. <em>ML</em>, <em>111</em>(4), 1349–1375. (<a
href="https://doi.org/10.1007/s10994-022-06149-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Data Description (SVDD) is a popular one-class classifier for anomaly and novelty detection. But despite its effectiveness, SVDD does not scale well with data size. To avoid prohibitive training times, sampling methods select small subsets of the training data on which SVDD trains a decision boundary hopefully equivalent to the one obtained on the full data set. According to the literature, a good sample should therefore contain so-called boundary observations that SVDD would select as support vectors on the full data set. However, non-boundary observations also are essential to not fragment contiguous inlier regions and avoid poor classification accuracy. Other aspects, such as selecting a sufficiently representative sample, are important as well. But existing sampling methods largely overlook them, resulting in poor classification accuracy. In this article, we study how to select a sample considering these points. Our approach is to frame SVDD sampling as an optimization problem, where constraints guarantee that sampling indeed approximates the original decision boundary. We then propose RAPID, an efficient algorithm to solve this optimization problem. RAPID does not require any tuning of parameters, is easy to implement and scales well to large data sets. We evaluate our approach on real-world and synthetic data. Our evaluation is the most comprehensive one for SVDD sampling so far. Our results show that RAPID outperforms its competitors in classification accuracy, in sample size, and in runtime.},
  archive      = {J_ML},
  author       = {Englhardt, Adrian and Trittenbach, Holger and Kottke, Daniel and Sick, Bernhard and Böhm, Klemens},
  doi          = {10.1007/s10994-022-06149-0},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1349-1375},
  shortjournal = {Mach. Learn.},
  title        = {Efficient SVDD sampling with approximation guarantees for the decision boundary},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDANet: Spatial deep attention-based for point cloud
classification and segmentation. <em>ML</em>, <em>111</em>(4),
1327–1348. (<a
href="https://doi.org/10.1007/s10994-022-06148-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using deep learning to learn point cloud features directly have become one of the research hotspots in the field of 3D point cloud processing. The existing methods usually construct local regions, extract features from local regions, and then aggregate global features through multi-layer perceptron and maximum pooling layer. However, most of these processes do not consider the contribution of point cloud local features to the final decision and the spatial relationship between neighbor points, which limits the accuracy of 3D point cloud classification and segmentation. In this article, a novel network model called spatial depth attention network is designed to improve the accuracy of point cloud classification and segmentation, which embeds local depth attention mechanism into MLP layer to learn local neighborhood geometric representation. The local deep attention of the point cloud is obtained through the SDA module, and then combined with feature learning and local deep attention to effectively capture the local geometric structure. In order to achieve the best feature extraction ability, local depth attention features are combined with global features. Experiments show that SDANet achieves the same or better performance as the most advanced methods on several challenging benchmark datasets and tasks.},
  archive      = {J_ML},
  author       = {Gao, Jiangjiang and Lan, Jinhui and Wang, Bingxu and Li, Feifan},
  doi          = {10.1007/s10994-022-06148-1},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1327-1348},
  shortjournal = {Mach. Learn.},
  title        = {SDANet: Spatial deep attention-based for point cloud classification and segmentation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifting symmetry breaking constraints with inductive logic
programming. <em>ML</em>, <em>111</em>(4), 1303–1326. (<a
href="https://doi.org/10.1007/s10994-022-06146-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient omission of symmetric solution candidates is essential for combinatorial problem-solving. Most of the existing approaches are instance-specific and focus on the automatic computation of Symmetry Breaking Constraints (SBCs) for each given problem instance. However, the application of such approaches to large-scale instances or advanced problem encodings might be problematic since the computed SBCs are propositional and, therefore, can neither be meaningfully interpreted nor transferred to other instances. As a result, a time-consuming recomputation of SBCs must be done before every invocation of a solver. To overcome these limitations, we introduce a new model-oriented approach for Answer Set Programming that lifts the SBCs of small problem instances into a set of interpretable first-order constraints using the Inductive Logic Programming paradigm. Experiments demonstrate the ability of our framework to learn general constraints from instance-specific SBCs for a collection of combinatorial problems. The obtained results indicate that our approach significantly outperforms a state-of-the-art instance-specific method as well as the direct application of a solver.},
  archive      = {J_ML},
  author       = {Tarzariol, Alice and Gebser, Martin and Schekotihin, Konstantin},
  doi          = {10.1007/s10994-022-06146-3},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1303-1326},
  shortjournal = {Mach. Learn.},
  title        = {Lifting symmetry breaking constraints with inductive logic programming},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handling epistemic and aleatory uncertainties in
probabilistic circuits. <em>ML</em>, <em>111</em>(4), 1259–1301. (<a
href="https://doi.org/10.1007/s10994-021-06086-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When collaborating with an AI system, we need to assess when to trust its recommendations. If we mistakenly trust it in regions where it is likely to err, catastrophic failures may occur, hence the need for Bayesian approaches for probabilistic reasoning in order to determine the confidence (or epistemic uncertainty) in the probabilities in light of the training data. We propose an approach to Bayesian inference of posterior distributions that overcomes the independence assumption behind most of the approaches dealing with a large class of probabilistic reasoning that includes Bayesian networks as well as several instances of probabilistic logic. We provide an algorithm for Bayesian inference of posterior distributions from sparse, albeit complete, observations, and for deriving inferences and their confidences keeping track of the dependencies between variables when they are manipulated within the unifying computational formalism provided by probabilistic circuits. Each leaf of such circuits is labelled with a beta-distributed random variable that provides us with an elegant framework for representing uncertain probabilities. We achieve better estimation of epistemic uncertainty than state-of-the-art approaches, including highly engineered ones, while being able to handle general circuits and with just a modest increase in the computational effort compared to using point probabilities.},
  archive      = {J_ML},
  author       = {Cerutti, Federico and Kaplan, Lance M. and Kimmig, Angelika and Şensoy, Murat},
  doi          = {10.1007/s10994-021-06086-4},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1259-1301},
  shortjournal = {Mach. Learn.},
  title        = {Handling epistemic and aleatory uncertainties in probabilistic circuits},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot learning for spatial regression via neural
embedding-based gaussian processes. <em>ML</em>, <em>111</em>(4),
1239–1257. (<a
href="https://doi.org/10.1007/s10994-021-06118-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a few-shot learning method for spatial regression. Although Gaussian processes (GPs), or kriging, have been successfully used for spatial regression, they require many observations in the target task to achieve a high predictive performance. Our model is trained using spatial datasets on various attributes in various regions, and predicts values on unseen attributes in unseen regions given a few observed data. With our model, a task representation is inferred from given small data using a neural network. Then, spatial values are predicted by neural networks with a GP framework, in which task-specific properties are controlled by the task representations. The GP framework allows us to analytically obtain predictions that are adapted to small data. By using the adapted predictions in the objective function, we can train our model efficiently and effectively so that the test predictive performance improves when adapted to newly given small data. In our experiments, we demonstrate that the proposed method achieves better predictive performance than existing meta-learning methods using spatial datasets.},
  archive      = {J_ML},
  author       = {Iwata, Tomoharu and Tanaka, Yusuke},
  doi          = {10.1007/s10994-021-06118-z},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1239-1257},
  shortjournal = {Mach. Learn.},
  title        = {Few-shot learning for spatial regression via neural embedding-based gaussian processes},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polynomial-based graph convolutional neural networks for
graph classification. <em>ML</em>, <em>111</em>(4), 1205–1237. (<a
href="https://doi.org/10.1007/s10994-021-06098-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple graph convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper, we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then processed by subsequent readout layers. We implement this strategy introducing the polynomial graph convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a graph neural network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.},
  archive      = {J_ML},
  author       = {Pasa, Luca and Navarin, Nicolò and Sperduti, Alessandro},
  doi          = {10.1007/s10994-021-06098-0},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {1205-1237},
  shortjournal = {Mach. Learn.},
  title        = {Polynomial-based graph convolutional neural networks for graph classification},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end entity-aware neural machine translation.
<em>ML</em>, <em>111</em>(3), 1181–1203. (<a
href="https://doi.org/10.1007/s10994-021-06073-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate translation of entities (e.g., person names, organizations, geography) is important in neural machine translation (briefly, NMT), as they are usually more difficult to translate than other words, and an incorrect translation of them will greatly hurt user experiences. In previous works, entities are either treated in the same way as other words, which leads to inaccurate translation, or handled by multiple steps (including named entity recognition, translation, and replacing entities back), which significantly increase the inference latency. In this work, we propose an end-to-end algorithm that carefully handles the translation of entities. There are mainly two novel parts compared to conventional NMT model: (1) The encoder and the decoder are attached with entity classifiers, which are used to verify whether the input token is a named entity. In this way, the encoder and decoder are capable to treat named entities differently; (2) The translation loss of each target token is adaptively increased by the probability that the target token is a named entity, which results in more accurate translation of entities. During inference time, these two parts will be removed so that the translation model maintains the same inference speed as conventional NMT models. Empirical results on six translation tasks demonstrate the effectiveness of our methods of improving the translation quality. Specifically, we improve 1.7 BLEU scores on Japanese to English translation and 4.6 entity $$F_{1}$$ scores on English to Chinese translation, without additional inference cost.},
  archive      = {J_ML},
  author       = {Xie, Shufang and Xia, Yingce and Wu, Lijun and Huang, Yiqing and Fan, Yang and Qin, Tao},
  doi          = {10.1007/s10994-021-06073-9},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1181-1203},
  shortjournal = {Mach. Learn.},
  title        = {End-to-end entity-aware neural machine translation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards interpreting deep neural networks via layer behavior
understanding. <em>ML</em>, <em>111</em>(3), 1159–1179. (<a
href="https://doi.org/10.1007/s10994-021-06074-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved success in many machine learning tasks. However, how to interpret DNNs is still an open problem. In particular, how do hidden layers behave is not clearly understood. In this paper, relying on a teacher-student paradigm, we seek to understand the layer behaviors of DNNs by “monitoring” the distribution evolution for both across-layer and single-layer along the depth and training epochs, respectively. Relying on the optimal transport theory, we employ the Wasserstein distance (W-distance) to measure the divergence between the layer distribution and the target distribution. Theoretically, we prove that (i) the W-distance between the distribution of any layer and the target distribution tends to decrease along the depth; (ii) for a specific layer, the W-distance between the distribution in an iteration and the target distribution tends to decrease along training epochs; (iii) a deeper layer, however, is not always better than a shallower layer. Relying on these properties, we are able to propose an early-exit inference method to improve the performance of the multi-label classification. Moreover, our results help to analyze the stability of layer distributions and explain why auxiliary losses are helpful in training DNNs. Extensive experiments justify our theoretical findings.},
  archive      = {J_ML},
  author       = {Cao, Jiezhang and Li, Jincheng and Hu, Xiping and Wu, Xiangmiao and Tan, Mingkui},
  doi          = {10.1007/s10994-021-06074-8},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1159-1179},
  shortjournal = {Mach. Learn.},
  title        = {Towards interpreting deep neural networks via layer behavior understanding},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the common principal subspace of deep features in
neural networks. <em>ML</em>, <em>111</em>(3), 1125–1157. (<a
href="https://doi.org/10.1007/s10994-021-06076-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We find that different Deep Neural Networks (DNNs) trained with the same dataset share a common principal subspace in latent spaces, no matter in which architectures (e.g., Convolutional Neural Networks (CNNs), Multi-Layer Preceptors (MLPs) and Autoencoders (AEs)) the DNNs were built or even whether labels have been used in training (e.g., supervised, unsupervised, and self-supervised learning). Specifically, we design a new metric $${\mathcal {P}}$$ -vector to represent the principal subspace of deep features learned in a DNN, and propose to measure angles between the principal subspaces using $${\mathcal {P}}$$ -vectors. Small angles (with cosine close to 1.0) have been found in the comparisons between any two DNNs trained with different algorithms/architectures. Furthermore, during the training procedure from random scratch, the angle decrease from a larger one (70°–80° usually) to the small one, which coincides the progress of feature space learning from scratch to convergence. Then, we carry out case studies to measure the angle between the $${\mathcal {P}}$$ -vector and the principal subspace of training dataset, and connect such angle with generalization performance. Extensive experiments with practically-used Multi-Layer Perceptron (MLPs), AEs and CNNs for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.},
  archive      = {J_ML},
  author       = {Liu, Haoran and Xiong, Haoyi and Wang, Yaqing and An, Haozhe and Dou, Dejing and Wu, Dongrui},
  doi          = {10.1007/s10994-021-06076-6},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1125-1157},
  shortjournal = {Mach. Learn.},
  title        = {Exploring the common principal subspace of deep features in neural networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving deep label noise learning with dual active label
correction. <em>ML</em>, <em>111</em>(3), 1103–1124. (<a
href="https://doi.org/10.1007/s10994-021-06081-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise is now a common problem in many applications, which may lead to significant learning performance degeneration. To deal with the label noise, Active Label Correction (ALC) was proposed to query the true labels for a small subset of instances. As the true labels costs can be high, the focus of ALC is to maximally improve the learning performance with minimal query costs. Existing ALC methods mainly proceed by querying the most likely mislabeled instances, or using criteria derived from standard active learning. In this paper, we focus on deep neural network models and show that due to their intrinsic memorization effect, the true labels of a large proportion of mislabeled instances can be correctly predicted with early stopped training, even under severe noise. Inspired by this, we propose to train deep label noise learning models robustly with dual ALC (DALC): on one hand, we select the most useful instances for classifier improvement and query their true labels from external experts; on the other hand, due to the active data sampling bias, the label noise model estimation can be highly biased, which may in turn hurt the classifier learning. To alleviate this issue, we propose to identify the instances that are most likely predicted with true labels by the classifier, and take the predictions as their true labels. By integrating the two sources of true labels, we experiment on multiple benchmark datasets with various label noise rate and show the effectiveness of the proposed DALC on both the classification accuracy and the label noise model estimation. The code is available at https://github.com/lilylisy/mlj21DALC .},
  archive      = {J_ML},
  author       = {Li, Shao-Yuan and Shi, Ye and Huang, Sheng-Jun and Chen, Songcan},
  doi          = {10.1007/s10994-021-06081-9},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1103-1124},
  shortjournal = {Mach. Learn.},
  title        = {Improving deep label noise learning with dual active label correction},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Switching: Understanding the class-reversed sampling in tail
sample memorization. <em>ML</em>, <em>111</em>(3), 1073–1101. (<a
href="https://doi.org/10.1007/s10994-021-06087-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed visual recognition poses significant challenges to traditional machine learning and emerging deep networks due to its inherent class imbalance. Existing reweighting and re-sampling methods, although effective, lack a fundamental theory while leaving the paradoxical effects of long tail unsolved, where network failing with head classes under-represented and tail classes overfitted. In this paper, we investigate long-tailed recognition from a memorization-generalization point of view, which not only unravels the whys of previous methods, but also derives a new principled solution. Specifically, we first empirically identify the regularity of classes under long-tailed distributions, finding that long-tailed challenge is essentially a trade-off between the representation of high-regularity head classes and generalization to low-regularity tail classes. To memorize tail samples without seriously damaging the representation of head samples, we propose a simple yet effective sampling strategy for ordinary mini-batch SGD optimization process, Switching, which switches from instance-balanced sampling to class-reversed sampling for only once at small learning rate. By theoretical analysis, we show that the upper bound on the generalization error of the proposed sampling strategy is lower than instance-balanced sampling conditionally. In our experiments, the proposed method can reach feasible performance more efficiently than current methods. Further experiments validate the superiority of the proposed Switching strategy, implying that the long-tailed learning trade-off could be parsimoniously tackled only in the memorization stage with a small learning rate and over-exposure of tail samples.},
  archive      = {J_ML},
  author       = {Zhang, Chi and Hu, Benyi and Liuzhang, Yuhang and Wang, Le and Liu, Li and Liu, Yuehu},
  doi          = {10.1007/s10994-021-06087-3},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1073-1101},
  shortjournal = {Mach. Learn.},
  title        = {Switching: Understanding the class-reversed sampling in tail sample memorization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple partitions alignment via spectral rotation.
<em>ML</em>, <em>111</em>(3), 1049–1072. (<a
href="https://doi.org/10.1007/s10994-021-06071-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering has drawn much attention due to the effectiveness of exploiting the similarity relationships among data points. These methods typically reveal the intrinsic structure using a predefined graph for each view. The predefined graphs are fused to a consensus one, on which the final clustering results are obtained. However, such common strategies may lead to information loss because of the inconsistency or noise among multiple views. In this paper, we propose to merge multi-view information in partition level instead of the raw feature space where the data points lie. The partition of each view is treated as a perturbation of the consensus clustering, and the multiple partitions are integrated by estimating a distinct rotation for each partition. The proposed model is formulated as a joint learning framework, i.e., with the input data matrix, our model directly outputs the final discrete clustering result. Hence it is an end-to-end single-stage learning model. An iterative updating algorithm is proposed to solve the learning problem, in which the involved variables can be optimized in a mutual reinforcement manner. Experimental results on real-world data sets illustrate the effectiveness of our model.},
  archive      = {J_ML},
  author       = {Huang, Shudong and Tsang, Ivor W. and Xu, Zenglin and Lv, Jiancheng},
  doi          = {10.1007/s10994-021-06071-x},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1049-1072},
  shortjournal = {Mach. Learn.},
  title        = {Multiple partitions alignment via spectral rotation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian optimization with partially specified queries.
<em>ML</em>, <em>111</em>(3), 1019–1048. (<a
href="https://doi.org/10.1007/s10994-021-06079-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization (BO) is an approach to optimizing an expensive-to-evaluate black-box function and sequentially determines the values of input variables to evaluate the function. However, it is expensive and in some cases becomes difficult to specify values for all input variables, for example, in outsourcing scenarios where production of input queries with many input variables involves significant cost. In this paper, we propose a novel Gaussian process bandit problem, BO with partially specified queries (BOPSQ). In BOPSQ, unlike the standard BO setting, a learner specifies only the values of some input variables, and the values of the unspecified input variables are randomly determined according to a known or unknown distribution. We propose two algorithms based on posterior sampling for cases of known and unknown input distributions. We further derive their regret bounds that are sublinear for popular kernels. We demonstrate the effectiveness of the proposed algorithms using test functions and real-world datasets.},
  archive      = {J_ML},
  author       = {Hayashi, Shogo and Honda, Junya and Kashima, Hisashi},
  doi          = {10.1007/s10994-021-06079-3},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {1019-1048},
  shortjournal = {Mach. Learn.},
  title        = {Bayesian optimization with partially specified queries},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving kernel online learning with a snapshot memory.
<em>ML</em>, <em>111</em>(3), 997–1018. (<a
href="https://doi.org/10.1007/s10994-021-06075-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose in this paper the Stochastic Variance-reduced Gradient Descent for Kernel Online Learning (DualSVRG), which obtains the $$\varepsilon$$ -approximate linear convergence rate and is not vulnerable to the curse of kernelization. Our approach uses a variance reduction technique to reduce the variance when estimating full gradient, and further exploits recent work in dual space gradient descent for online learning to achieve model optimality. This is achieved by introducing the concept of an instant memory, which is a snapshot storing the most recent incoming data instances and proposing three transformer oracles, namely budget, coverage, and always-move oracles. We further develop rigorous theoretical analysis to demonstrate that our proposed approach can obtain the $$\varepsilon$$ -approximate linear convergence rate, while maintaining model sparsity, hence encourages fast training. We conduct extensive experiments on several benchmark datasets to compare our DualSVRG with state-of-the-art baselines in both batch and online settings. The experimental results show that our DualSVRG yields superior predictive performance, while spending comparable training time with baselines.},
  archive      = {J_ML},
  author       = {Le, Trung and Nguyen, Khanh and Phung, Dinh},
  doi          = {10.1007/s10994-021-06075-7},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {997-1018},
  shortjournal = {Mach. Learn.},
  title        = {Improving kernel online learning with a snapshot memory},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improve generated adversarial imitation learning with reward
variance regularization. <em>ML</em>, <em>111</em>(3), 977–995. (<a
href="https://doi.org/10.1007/s10994-021-06083-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning aims at recovering expert policies from limited demonstration data. Generative Adversarial Imitation Learning (GAIL) employs the generative adversarial learning framework for imitation learning and has shown great potentials. GAIL and its variants, however, are found highly sensitive to hyperparameters and hard to converge well in practice. One key issue is that the supervised learning discriminator has a much faster learning speed than the reinforcement learning generator, making the generator gradient vanishing. Although GAIL is formulated as a zero-sum adversarial game, the ultimate goal of GAIL is to learn the generator, thus the discriminator should play the role more like a teacher rather than a real opponent. Therefore, the learning of the discriminator should consider how the generator could learn. In this paper, we disclose that enhancing the gradient of the generator training is equivalent to increase the variance of the fake reward provided by the discriminator output. We thus propose an improved version of GAIL, GAIL-VR, in which the discriminator also learns to avoid generator gradient vanishing through regularization of the fake rewards variance. Experiments in various tasks, including locomotion tasks and Atari games, indicate that GAIL-VR can improve the training stability and imitation scores.},
  archive      = {J_ML},
  author       = {Zhang, Yi-Feng and Luo, Fan-Ming and Yu, Yang},
  doi          = {10.1007/s10994-021-06083-7},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {977-995},
  shortjournal = {Mach. Learn.},
  title        = {Improve generated adversarial imitation learning with reward variance regularization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Worst-case regret analysis of computationally budgeted
online kernel selection. <em>ML</em>, <em>111</em>(3), 937–976. (<a
href="https://doi.org/10.1007/s10994-021-06082-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of online kernel selection under computational constraints, where the memory or time of kernel selection and online prediction procedures is restricted to a fixed budget. In this paper, we analyze the worst-case lower bounds on the regret of online kernel selection algorithm with a subset of the observed examples, and design algorithms enjoying corresponding upper bounds. We also identify the condition under which online kernel selection with time constraints is different from that with memory constraints. To design algorithms, we reduce the problems to two sequential decision problems, that is, the problem of prediction with expert advice and the multi-armed bandit problem with an additional observation. Our algorithms invent some new techniques, such as memory sharing, hypothesis space discretization and decoupled exploration-exploitation scheme. Numerical experiments on online regression and classification are conducted to verify our theoretical results.},
  archive      = {J_ML},
  author       = {Li, Junfan and Liao, Shizhong},
  doi          = {10.1007/s10994-021-06082-8},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {937-976},
  shortjournal = {Mach. Learn.},
  title        = {Worst-case regret analysis of computationally budgeted online kernel selection},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of BERT for context-aware neural machine
translation. <em>ML</em>, <em>111</em>(3), 917–935. (<a
href="https://doi.org/10.1007/s10994-021-06070-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context-aware neural machine translation (NMT), which targets at translating sentences with contextual information, has attracted much attention recently. A key problem for context-aware NMT is to effectively encode and aggregate the contextual information. BERT (Devlin et al., in: NAACL, 2019) has been proven to be an effective feature extractor in natural language understanding tasks, but it has not been well studied in context-aware NMT. In this work, we conduct a study about leveraging BERT to encode the contextual information for NMT, and explore three commonly used methods to aggregate the contextual features. We conduct experiments on five translation tasks and find that concatenating all contextual sequences as a longer one and then encoding it by BERT obtains the best translation results. Specifically, we achieved state-of-the-art BLEU scores on several widely investigated tasks, including IWSLT’14 German $$\rightarrow$$ English, News Commentary v11 English $$\rightarrow$$ German translation and OpenSubtitle English $$\rightarrow$$ Russian translation.},
  archive      = {J_ML},
  author       = {Wu, Xueqing and Xia, Yingce and Zhu, Jinhua and Wu, Lijun and Xie, Shufang and Qin, Tao},
  doi          = {10.1007/s10994-021-06070-y},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {917-935},
  shortjournal = {Mach. Learn.},
  title        = {A study of BERT for context-aware neural machine translation},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the benefits of representation regularization in
invariance based domain generalization. <em>ML</em>, <em>111</em>(3),
895–915. (<a href="https://doi.org/10.1007/s10994-021-06080-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial aspect of reliable machine learning is to design a deployable system for generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning the invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning the invariant representation is vulnerable to the related unseen environment. To this end, we derive a novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. The proposed regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms that ensure invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various datasets and invariance criteria.},
  archive      = {J_ML},
  author       = {Shui, Changjian and Wang, Boyu and Gagné, Christian},
  doi          = {10.1007/s10994-021-06080-w},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {895-915},
  shortjournal = {Mach. Learn.},
  title        = {On the benefits of representation regularization in invariance based domain generalization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online strongly convex optimization with unknown delays.
<em>ML</em>, <em>111</em>(3), 871–893. (<a
href="https://doi.org/10.1007/s10994-021-06072-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of online convex optimization with unknown delays, in which the feedback of a decision arrives with an arbitrary delay. Previous studies have presented delayed online gradient descent (DOGD), and achieved the regret bound of $$O(\sqrt{D})$$ by only utilizing the convexity condition, where $$D\ge T$$ is the sum of delays over T rounds. In this paper, we further exploit the strong convexity to improve the regret bound. Specifically, we first propose a variant of DOGD for strongly convex functions, and establish a better regret bound of $$O(d\log T)$$ , where d is the maximum delay. The essential idea is to let the learning rate decay with the total number of received feedback linearly. Furthermore, we extend the strongly convex variant of DOGD and its theoretical guarantee to the more challenging bandit setting by combining with the classical $$(n+1)$$ -point and two-point gradient estimators, where n is the dimensionality. To the best of our knowledge, this is the first work that solves online strongly convex optimization under the general delayed setting.},
  archive      = {J_ML},
  author       = {Wan, Yuanyu and Tu, Wei-Wei and Zhang, Lijun},
  doi          = {10.1007/s10994-021-06072-w},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {871-893},
  shortjournal = {Mach. Learn.},
  title        = {Online strongly convex optimization with unknown delays},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The flowing nature matters: Feature learning from the
control flow graph of source code for bug localization. <em>ML</em>,
<em>111</em>(3), 853–870. (<a
href="https://doi.org/10.1007/s10994-021-06078-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug localization plays an important role in software maintenance. Traditional works treat the source code from the lexical perspective, while some recent researches indicate that exploiting the program structure is beneficial for improving bug localization. Control flow graph (CFG) is a widely used graph representation, which essentially represents the program structure. Although using graph neural network for feature learning is a straightforward way and has been proven effective in various software mining problems, this approach is inappropriate since adjacent nodes in the CFG could be totally unrelated in semantics. On the other hand, previous statements may affect the semantics of subsequent statements along the execution path, which we call the flowing nature of control flow graph. In this paper, we claim that the flowing nature should be explicitly considered and propose a novel model named cFlow for bug localization, which employs a particular designed flow-based GRU for feature learning from the CFG. The flow-based GRU exploits the program structure represented by the CFG to transmit the semantics of statements along the execution path, which reflects the flowing nature. Experimental results on widely-used real-world software projects show that cFlow significantly outperforms the state-of-the-art bug localization methods, indicating that exploiting the program structure from the CFG with respect to the flowing nature is beneficial for improving bug localization.},
  archive      = {J_ML},
  author       = {Ma, Yi-Fan and Li, Ming},
  doi          = {10.1007/s10994-021-06078-4},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {853-870},
  shortjournal = {Mach. Learn.},
  title        = {The flowing nature matters: Feature learning from the control flow graph of source code for bug localization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CMD: Controllable matrix decomposition with global
optimization for deep neural network compression. <em>ML</em>,
<em>111</em>(3), 831–851. (<a
href="https://doi.org/10.1007/s10994-021-06077-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The compression and acceleration of Deep neural networks (DNNs) are necessary steps to deploy sophisticated networks into resource-constrained hardware systems. Due to the weight matrix tends to be low-rank and sparse, several low-rank and sparse compression schemes are leveraged to reduce the overwhelmed weight parameters of DNNs. In these previous schemes, how to make the most of the low-rank and sparse components of weight matrices and how to globally decompose the weight matrix of different layers for efficient compression need to be further investigated. In this paper, in order to effectively utilize the low-rank and sparse characteristics of the weight matrix, we first introduce a sparse coefficient to dynamically control the allocation between the low-rank and sparse components, and an efficient reconstructed network is designed to reduce the inference time. Secondly, since the results of low-rank decomposition can affect the compression ratio and accuracy of DNNs, we establish an optimization problem to automatically select the optimal hyperparameters of the compressed network and achieve global compression for all the layers of network synchronously. Finally, to solve the optimization problem, we present a decomposition-searching algorithm to search the optimal solution. The algorithm can dynamically keep the balance between the compression ratio and accuracy. Extensive experiments of AlexNet, VGG-16 and ResNet-18 on CIFAR-10 and ImageNet are employed to evaluate the effectiveness of the proposed approach. After slight fine-tuning, compressed networks have gained $$1.2\times$$ to $$11.3\times$$ speedup and our method reduces the size of different networks by $$1.4\times$$ to $$14.6\times$$ .},
  archive      = {J_ML},
  author       = {Zhang, Haonan and Liu, Longjun and Zhou, Hengyi and Sun, Hongbin and Zheng, Nanning},
  doi          = {10.1007/s10994-021-06077-5},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {831-851},
  shortjournal = {Mach. Learn.},
  title        = {CMD: Controllable matrix decomposition with global optimization for deep neural network compression},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiway p-spectral graph cuts on grassmann manifolds.
<em>ML</em>, <em>111</em>(2), 791–829. (<a
href="https://doi.org/10.1007/s10994-021-06108-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear reformulations of the spectral clustering method have gained a lot of recent attention due to their increased numerical benefits and their solid mathematical background. We present a novel direct multiway spectral clustering algorithm in the p-norm, for $$p\in (1,2]$$ . The problem of computing multiple eigenvectors of the graph p-Laplacian, a nonlinear generalization of the standard graph Laplacian, is recasted as an unconstrained minimization problem on a Grassmann manifold. The value of p is reduced in a pseudocontinuous manner, promoting sparser solution vectors that correspond to optimal graph cuts as p approaches one. Monitoring the monotonic decrease of the balanced graph cuts guarantees that we obtain the best available solution from the p-levels considered. We demonstrate the effectiveness and accuracy of our algorithm in various artificial test-cases. Our numerical examples and comparative results with various state-of-the-art clustering methods indicate that the proposed method obtains high quality clusters both in terms of balanced graph cut metrics and in terms of the accuracy of the labelling assignment. Furthermore, we conduct studies for the classification of facial images and handwritten characters to demonstrate the applicability in real-world datasets.},
  archive      = {J_ML},
  author       = {Pasadakis, Dimosthenis and Alappat, Christie Louis and Schenk, Olaf and Wellein, Gerhard},
  doi          = {10.1007/s10994-021-06108-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {791-829},
  shortjournal = {Mach. Learn.},
  title        = {Multiway p-spectral graph cuts on grassmann manifolds},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model selection in reconciling hierarchical time series.
<em>ML</em>, <em>111</em>(2), 739–789. (<a
href="https://doi.org/10.1007/s10994-021-06126-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection has been proven an effective strategy for improving accuracy in time series forecasting applications. However, when dealing with hierarchical time series, apart from selecting the most appropriate forecasting model, forecasters have also to select a suitable method for reconciling the base forecasts produced for each series to make sure they are coherent. Although some hierarchical forecasting methods like minimum trace are strongly supported both theoretically and empirically for reconciling the base forecasts, there are still circumstances under which they might not produce the most accurate results, being outperformed by other methods. In this paper we propose an approach for dynamically selecting the most appropriate hierarchical forecasting reconciliation method and leading to more accurate coherent forecasts. The approach, which we call conditional hierarchical forecasting, is based on machine learning classification methods that use time series features to select the reconciliation method for each hierarchy. Moreover, it allows the selection to be tailored according to the accuracy measure of preference and the hierarchical level(s) of interest. Our results suggest that conditional hierarchical forecasting can lead to significantly more accurate forecasts than standard approaches, especially at lower hierarchical levels.},
  archive      = {J_ML},
  author       = {Abolghasemi, Mahdi and Hyndman, Rob J. and Spiliotis, Evangelos and Bergmeir, Christoph},
  doi          = {10.1007/s10994-021-06126-z},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {739-789},
  shortjournal = {Mach. Learn.},
  title        = {Model selection in reconciling hierarchical time series},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible class of dependence-aware multi-label loss
functions. <em>ML</em>, <em>111</em>(2), 713–737. (<a
href="https://doi.org/10.1007/s10994-021-06107-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea to exploit label dependencies for better prediction is at the core of methods for multi-label classification (MLC), and performance improvements are normally explained in this way. Surprisingly, however, there is no established methodology that allows to analyze the dependence-awareness of MLC algorithms. With that goal in mind, we introduce a class of loss functions that are able to capture the important aspect of label dependence. To this end, we leverage the mathematical framework of non-additive measures and integrals. Roughly speaking, a non-additive measure allows for modeling the importance of correct predictions of label subsets (instead of single labels), and thereby their impact on the overall evaluation, in a flexible way. The well-known Hamming and subset 0/1 losses are rather extreme special cases of this function class, which give full importance to single label sets or the entire label set, respectively. We present concrete instantiations of this class, which appear to be especially appealing from a modeling perspective. The assessment of multi-label classifiers in terms of these losses is illustrated in an empirical study, clearly showing their aptness at capturing label dependencies. Finally, while not being the main goal of this study, we also show some preliminary results on the minimization of this parametrized family of losses.},
  archive      = {J_ML},
  author       = {Hüllermeier, Eyke and Wever, Marcel and Loza Mencia, Eneldo and Fürnkranz, Johannes and Rapp, Michael},
  doi          = {10.1007/s10994-021-06107-2},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {713-737},
  shortjournal = {Mach. Learn.},
  title        = {A flexible class of dependence-aware multi-label loss functions},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Achieving adversarial robustness via sparsity. <em>ML</em>,
<em>111</em>(2), 685–711. (<a
href="https://doi.org/10.1007/s10994-021-06049-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning has been known to produce compact models without much accuracy degradation. However, how the pruning process affects a network’s robustness and the working mechanism behind remain unresolved. In this work, we theoretically prove that the sparsity of network weights is closely associated with model robustness. Through experiments on a variety of adversarial pruning methods, image-classification models and datasets, we find that weights sparsity will not hurt but improve robustness, where both weights inheritance from the lottery ticket and adversarial training improve model robustness in network pruning. Based on these findings, we propose a novel adversarial training method called inverse weights inheritance, which imposes sparse weights distribution on a large network by inheriting weights from a small network, thereby improving the robustness of the large network.},
  archive      = {J_ML},
  author       = {Liao, Ningyi and Wang, Shufan and Xiang, Liyao and Ye, Nanyang and Shao, Shuo and Chu, Pengzhi},
  doi          = {10.1007/s10994-021-06049-9},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {685-711},
  shortjournal = {Mach. Learn.},
  title        = {Achieving adversarial robustness via sparsity},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-target prediction for dummies using two-branch neural
networks. <em>ML</em>, <em>111</em>(2), 651–684. (<a
href="https://doi.org/10.1007/s10994-021-06104-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target prediction (MTP) serves as an umbrella term for machine learning tasks that concern the simultaneous prediction of multiple target variables. Classical instantiations are multi-label classification, multivariate regression, multi-task learning, dyadic prediction, zero-shot learning, network inference, and matrix completion. Despite the significant similarities, all these domains have evolved separately into distinct research areas over the last two decades. This led to the development of a plethora of highly-engineered methods, and created a substantially-high entrance barrier for machine learning practitioners that are not experts in the field. In this work we present a generic deep learning methodology that can be used for a wide range of multi-target prediction problems. We introduce a flexible multi-branch neural network architecture, partially configured via a questionnaire that helps end users to select a suitable MTP problem setting for their needs. Experimental results for a wide range of domains illustrate that the proposed methodology manifests a competitive performance compared to methods from specific MTP domains.},
  archive      = {J_ML},
  author       = {Iliadis, Dimitrios and De Baets, Bernard and Waegeman, Willem},
  doi          = {10.1007/s10994-021-06104-5},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {651-684},
  shortjournal = {Mach. Learn.},
  title        = {Multi-target prediction for dummies using two-branch neural networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symbolic DNN-tuner. <em>ML</em>, <em>111</em>(2), 625–650.
(<a href="https://doi.org/10.1007/s10994-021-06097-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-Parameter Optimization (HPO) occupies a fundamental role in Deep Learning systems due to the number of hyper-parameters (HPs) to be set. The state-of-the-art of HPO methods are Grid Search, Random Search and Bayesian Optimization. The first two methods try all possible combinations and random combination of the HPs values, respectively. This is performed in a blind manner, without any information for choosing the new set of HPs values. Bayesian Optimization (BO), instead, keeps track of past results and uses them to build a probabilistic model mapping HPs into a probability density of the objective function. Bayesian Optimization builds a surrogate probabilistic model of the objective function, finds the HPs values that perform best on the surrogate model and updates it with new results. In this paper, we improve BO applied to Deep Neural Network (DNN) by adding an analysis of the results of the network on training and validation sets. This analysis is performed by exploiting rule-based programming, and in particular by using Probabilistic Logic Programming. The resulting system, called Symbolic DNN-Tuner, logically evaluates the results obtained from the training and the validation phase and, by applying symbolic tuning rules, fixes the network architecture, and its HPs, therefore improving performance. We also show the effectiveness of the proposed approach, by an experimental evaluation on literature and real-life datasets.},
  archive      = {J_ML},
  author       = {Fraccaroli, Michele and Lamma, Evelina and Riguzzi, Fabrizio},
  doi          = {10.1007/s10994-021-06097-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {625-650},
  shortjournal = {Mach. Learn.},
  title        = {Symbolic DNN-tuner},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inclusion of domain-knowledge into GNNs using mode-directed
inverse entailment. <em>ML</em>, <em>111</em>(2), 575–623. (<a
href="https://doi.org/10.1007/s10994-021-06090-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general technique for constructing Graph Neural Networks (GNNs) capable of using multi-relational domain knowledge. The technique is based on mode-directed inverse entailment (MDIE) developed in Inductive Logic Programming (ILP). Given a data instance e and background knowledge B, MDIE identifies a most-specific logical formula $$\bot _B(e)$$ that contains all the relational information in B that is related to e. We represent $$\bot _B(e)$$ by a “bottom-graph” that can be converted into a form suitable for GNN implementations. This transformation allows a principled way of incorporating generic background knowledge into GNNs: we use the term ‘BotGNN’ for this form of graph neural networks. For several GNN variants, using real-world datasets with substantial background knowledge, we show that BotGNNs perform significantly better than both GNNs without background knowledge and a recently proposed simplified technique for including domain knowledge into GNNs. We also provide experimental evidence comparing BotGNNs favourably to multi-layer perceptrons that use features representing a “propositionalised” form of the background knowledge; and BotGNNs to a standard ILP based on the use of most-specific clauses. Taken together, these results point to BotGNNs as capable of combining the computational efficacy of GNNs with the representational versatility of ILP.},
  archive      = {J_ML},
  author       = {Dash, Tirtharaj and Srinivasan, Ashwin and Baskar, A.},
  doi          = {10.1007/s10994-021-06090-8},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {575-623},
  shortjournal = {Mach. Learn.},
  title        = {Inclusion of domain-knowledge into GNNs using mode-directed inverse entailment},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized vec trick for fast learning of pairwise kernel
models. <em>ML</em>, <em>111</em>(2), 543–573. (<a
href="https://doi.org/10.1007/s10994-021-06127-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise learning corresponds to the supervised learning setting where the goal is to make predictions for pairs of objects. Prominent applications include predicting drug-target or protein-protein interactions, or customer-product preferences. In this work, we present a comprehensive review of pairwise kernels, that have been proposed for incorporating prior knowledge about the relationship between the objects. Specifically, we consider the standard, symmetric and anti-symmetric Kronecker product kernels, metric-learning, Cartesian, ranking, as well as linear, polynomial and Gaussian kernels. Recently, a $$O(nm+nq)$$ time generalized vec trick algorithm, where $$n$$ , $$m$$ , and $$q$$ denote the number of pairs, drugs and targets, was introduced for training kernel methods with the Kronecker product kernel. This was a significant improvement over previous $$O(n^2)$$ training methods, since in most real-world applications $$m,q&lt;&lt; n$$ . In this work we show how all the reviewed kernels can be expressed as sums of Kronecker products, allowing the use of generalized vec trick for speeding up their computation. In the experiments, we demonstrate how the introduced approach allows scaling pairwise kernels to much larger data sets than previously feasible, and provide an extensive comparison of the kernels on a number of biological interaction prediction tasks.},
  archive      = {J_ML},
  author       = {Viljanen, Markus and Airola, Antti and Pahikkala, Tapio},
  doi          = {10.1007/s10994-021-06127-y},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {543-573},
  shortjournal = {Mach. Learn.},
  title        = {Generalized vec trick for fast learning of pairwise kernel models},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ResGCN: Attention-based deep residual modeling for anomaly
detection on attributed networks. <em>ML</em>, <em>111</em>(2), 519–541.
(<a href="https://doi.org/10.1007/s10994-021-06044-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively detecting anomalous nodes in attributed networks is crucial for the success of many real-world applications such as fraud and intrusion detection. Existing approaches have difficulties with three major issues: sparsity and nonlinearity capturing, residual modeling, and network smoothing. We propose Residual Graph Convolutional Network (ResGCN), an attention-based deep residual modeling approach that can tackle these issues: modeling the attributed networks with GCN allows to capture the sparsity and nonlinearity, utilizing a deep neural network allows direct residual ing from the input, and a residual-based attention mechanism reduces the adverse effect from anomalous nodes and prevents over-smoothing. Extensive experiments on several real-world attributed networks demonstrate the effectiveness of ResGCN in detecting anomalies.},
  archive      = {J_ML},
  author       = {Pei, Yulong and Huang, Tianjin and van Ipenburg, Werner and Pechenizkiy, Mykola},
  doi          = {10.1007/s10994-021-06044-0},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {519-541},
  shortjournal = {Mach. Learn.},
  title        = {ResGCN: Attention-based deep residual modeling for anomaly detection on attributed networks},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-technical losses detection in energy consumption
focusing on energy recovery and explainability. <em>ML</em>,
<em>111</em>(2), 487–517. (<a
href="https://doi.org/10.1007/s10994-021-06051-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-technical losses (NTL) is a problem that many utility companies try to solve, often using black-box supervised classification algorithms. In general, this approach achieves good results. However, in practice, NTL detection faces technical, economic, and transparency challenges that cannot be easily solved and which compromise the quality and fairness of the predictions. In this work, we contextualise these problems in an NTL detection system built for an international utility company. We explain how we have mitigated them by moving from classification into a regression system and introducing explanatory techniques to improve its accuracy and understanding. As we show in this work, the regression approach can be a good option to mitigate these technical problems, and can be adjusted in order to capture the most striking NTL cases. Moreover, explainable AI (through Shapley Values) allows us to both validate the correctness of the regression approach in this context beyond benchmarking, and improve the transparency of our system drastically.},
  archive      = {J_ML},
  author       = {Coma-Puig, Bernat and Carmona, Josep},
  doi          = {10.1007/s10994-021-06051-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {487-517},
  shortjournal = {Mach. Learn.},
  title        = {Non-technical losses detection in energy consumption focusing on energy recovery and explainability},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning for robotic manipulation using
simulated locomotion demonstrations. <em>ML</em>, <em>111</em>(2),
465–486. (<a href="https://doi.org/10.1007/s10994-021-06116-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mastering robotic manipulation skills through reinforcement learning (RL) typically requires the design of shaped reward functions. Recent developments in this area have demonstrated that using sparse rewards, i.e. rewarding the agent only when the task has been successfully completed, can lead to better policies. However, state-action space exploration is more difficult in this case. Recent RL approaches to learning with sparse rewards have leveraged high-quality human demonstrations for the task, but these can be costly, time consuming or even impossible to obtain. In this paper, we propose a novel and effective approach that does not require human demonstrations. We observe that every robotic manipulation task could be seen as involving a locomotion task from the perspective of the object being manipulated, i.e. the object could learn how to reach a target state on its own. In order to exploit this idea, we introduce a framework whereby an object locomotion policy is initially obtained using a realistic physics simulator. This policy is then used to generate auxiliary rewards, called simulated locomotion demonstration rewards (SLDRs), which enable us to learn the robot manipulation policy. The proposed approach has been evaluated on 13 tasks of increasing complexity, and can achieve higher success rate and faster learning rates compared to alternative algorithms. SLDRs are especially beneficial for tasks like multi-object stacking and non-rigid object manipulation.},
  archive      = {J_ML},
  author       = {Kilinc, Ozsel and Montana, Giovanni},
  doi          = {10.1007/s10994-021-06116-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {465-486},
  shortjournal = {Mach. Learn.},
  title        = {Reinforcement learning for robotic manipulation using simulated locomotion demonstrations},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on instance ranking problems in statistical
learning. <em>ML</em>, <em>111</em>(2), 415–463. (<a
href="https://doi.org/10.1007/s10994-021-06122-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking problems, also known as preference learning problems, define a widely spread class of statistical learning problems with many applications, including fraud detection, document ranking, medicine, chemistry, credit risk screening, image ranking or media memorability. While there already exist reviews concentrating on specific types of ranking problems like label and object ranking problems, there does not yet seem to exist an overview concentrating on instance ranking problems that both includes developments in distinguishing between different types of instance ranking problems as well as careful discussions about their differences and the applicability of the existing ranking algorithms to them. In instance ranking, one explicitly takes the responses into account with the goal to infer a scoring function which directly maps feature vectors to real-valued ranking scores, in contrast to object ranking problems where the ranks are given as preference information with the goal to learn a permutation. In this article, we systematically review different types of instance ranking problems and the corresponding loss functions resp. goodness criteria. We discuss the difficulties when trying to optimize those criteria. As for a detailed and comprehensive overview of existing machine learning techniques to solve such ranking problems, we systematize existing techniques and recapitulate the corresponding optimization problems in a unified notation. We also discuss to which of the instance ranking problems the respective algorithms are tailored and identify their strengths and limitations. Computational aspects and open research problems are also considered.},
  archive      = {J_ML},
  author       = {Werner, Tino},
  doi          = {10.1007/s10994-021-06122-3},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {415-463},
  shortjournal = {Mach. Learn.},
  title        = {A review on instance ranking problems in statistical learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quick and robust feature selection: The strength of
energy-efficient sparse training for autoencoders. <em>ML</em>,
<em>111</em>(1), 377–414. (<a
href="https://doi.org/10.1007/s10994-021-06063-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major complications arise from the recent increase in the amount of high-dimensional data, including high computational costs and memory requirements. Feature selection, which identifies the most relevant and informative attributes of a dataset, has been introduced as a solution to this problem. Most of the existing feature selection methods are computationally inefficient; inefficient algorithms lead to high energy consumption, which is not desirable for devices with limited computational and energy resources. In this paper, a novel and flexible method for unsupervised feature selection is proposed. This method, named QuickSelection (The code is available at: https://github.com/zahraatashgahi/QuickSelection), introduces the strength of the neuron in sparse neural networks as a criterion to measure the feature importance. This criterion, blended with sparsely connected denoising autoencoders trained with the sparse evolutionary training procedure, derives the importance of all input features simultaneously. We implement QuickSelection in a purely sparse manner as opposed to the typical approach of using a binary mask over connections to simulate sparsity. It results in a considerable speed increase and memory reduction. When tested on several benchmark datasets, including five low-dimensional and three high-dimensional datasets, the proposed method is able to achieve the best trade-off of classification and clustering accuracy, running time, and maximum memory usage, among widely used approaches for feature selection. Besides, our proposed method requires the least amount of energy among the state-of-the-art autoencoder-based feature selection methods.},
  archive      = {J_ML},
  author       = {Atashgahi, Zahra and Sokar, Ghada and van der Lee, Tim and Mocanu, Elena and Mocanu, Decebal Constantin and Veldhuis, Raymond and Pechenizkiy, Mykola},
  doi          = {10.1007/s10994-021-06063-x},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {377-414},
  shortjournal = {Mach. Learn.},
  title        = {Quick and robust feature selection: The strength of energy-efficient sparse training for autoencoders},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding generalization error of SGD in nonconvex
optimization. <em>ML</em>, <em>111</em>(1), 345–375. (<a
href="https://doi.org/10.1007/s10994-021-06056-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning has led to a rising interest in the generalization property of the stochastic gradient descent (SGD) method, and stability is one popular approach to study it. Existing generalization bounds based on stability do not incorporate the interplay between the optimization of SGD and the underlying data distribution, and hence cannot even capture the effect of randomized labels on the generalization performance. In this paper, we establish generalization error bounds for SGD by characterizing the corresponding stability in terms of the on-average variance of the stochastic gradients. Such characterizations lead to improved bounds on the generalization error of SGD and experimentally explain the effect of the random labels on the generalization performance. We also study the regularized risk minimization problem with strongly convex regularizers, and obtain improved generalization error bounds for the proximal SGD.},
  archive      = {J_ML},
  author       = {Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  doi          = {10.1007/s10994-021-06056-w},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {345-375},
  shortjournal = {Mach. Learn.},
  title        = {Understanding generalization error of SGD in nonconvex optimization},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothing graphons for modelling exchangeable relational
data. <em>ML</em>, <em>111</em>(1), 319–344. (<a
href="https://doi.org/10.1007/s10994-021-06046-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling exchangeable relational data can be described appropriately in graphon theory. Most Bayesian methods for modelling exchangeable relational data can be attributed to this framework by exploiting different forms of graphons. However, the graphons adopted by existing Bayesian methods are either piecewise-constant functions, which are insufficiently flexible for accurate modelling of the relational data, or are complicated continuous functions, which incur heavy computational costs for inference. In this work, we overcome these two shortcomings by smoothing piecewise-constant graphons, which permits continuous intensity values for describing relations, without impractically increasing computational costs. In particular, we focus on the Bayesian Stochastic Block Model (SBM) and demonstrate how to adapt the piecewise-constant SBM graphon to the smoothed version. We first propose the Integrated Smoothing Graphon (ISG) which introduces one smoothing parameter to the SBM graphon to generate continuous relational intensity values. Then, we further develop the Latent Feature Smoothing Graphon (LFSG), which improves the ISG, by introducing auxiliary hidden labels to decompose the calculation of the ISG intensity and enable efficient inference. Experimental results on real-world data sets validate the advantages of applying smoothing strategies to the Stochastic Block Model, demonstrating that smoothing graphons can greatly improve AUC and precision for link prediction without increasing computational complexity.},
  archive      = {J_ML},
  author       = {Li, Yaqiong and Fan, Xuhui and Chen, Ling and Li, Bin and Sisson, Scott A.},
  doi          = {10.1007/s10994-021-06046-y},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {319-344},
  shortjournal = {Mach. Learn.},
  title        = {Smoothing graphons for modelling exchangeable relational data},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReliefE: Feature ranking in high-dimensional spaces via
manifold embeddings. <em>ML</em>, <em>111</em>(1), 273–317. (<a
href="https://doi.org/10.1007/s10994-021-05998-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature ranking has been widely adopted in machine learning applications such as high-throughput biology and social sciences. The approaches of the popular Relief family of algorithms assign importances to features by iteratively accounting for nearest relevant and irrelevant instances. Despite their high utility, these algorithms can be computationally expensive and not-well suited for high-dimensional sparse input spaces. In contrast, recent embedding-based methods learn compact, low-dimensional representations, potentially facilitating down-stream learning capabilities of conventional learners. This paper explores how the Relief branch of algorithms can be adapted to benefit from (Riemannian) manifold-based embeddings of instance and target spaces, where a given embedding’s dimensionality is intrinsic to the dimensionality of the considered data set. The developed ReliefE algorithm is faster and can result in better feature rankings, as shown by our evaluation on 20 real-life data sets for multi-class and multi-label classification tasks. The utility of ReliefE for high-dimensional data sets is ensured by its implementation that utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE to other ranking algorithms is studied via the Fuzzy Jaccard Index.},
  archive      = {J_ML},
  author       = {Škrlj, Blaž and Džeroski, Sašo and Lavrač, Nada and Petković, Matej},
  doi          = {10.1007/s10994-021-05998-5},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {273-317},
  shortjournal = {Mach. Learn.},
  title        = {ReliefE: Feature ranking in high-dimensional spaces via manifold embeddings},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting poisson regression models with telematics car
driving data. <em>ML</em>, <em>111</em>(1), 243–272. (<a
href="https://doi.org/10.1007/s10994-021-05957-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of telematics car driving data, insurance companies have started to boost classical actuarial regression models for claim frequency prediction with telematics car driving information. In this paper, we propose two data-driven neural network approaches that process telematics car driving data to complement classical actuarial pricing with a driving behavior risk factor from telematics data. Our neural networks simultaneously accommodate feature engineering and regression modeling which allows us to integrate telematics car driving data in a one-step approach into the claim frequency regression models. We conclude from our numerical analysis that both classical actuarial risk factors and telematics car driving data are necessary to receive the best predictive models. This emphasizes that these two sources of information interact and complement each other.},
  archive      = {J_ML},
  author       = {Gao, Guangyuan and Wang, He and Wüthrich, Mario V.},
  doi          = {10.1007/s10994-021-05957-0},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {243-272},
  shortjournal = {Mach. Learn.},
  title        = {Boosting poisson regression models with telematics car driving data},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InfoGram and admissible machine learning. <em>ML</em>,
<em>111</em>(1), 205–242. (<a
href="https://doi.org/10.1007/s10994-021-06121-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have entered a new era of machine learning (ML), where the most accurate algorithm with superior predictive power may not even be deployable, unless it is admissible under the regulatory constraints. This has led to great interest in developing fair, transparent and trustworthy ML methods. The purpose of this article is to introduce a new information-theoretic learning framework (admissible machine learning) and algorithmic risk-management tools (InfoGram, L-features, ALFA-testing) that can guide an analyst to redesign off-the-shelf ML methods to be regulatory compliant, while maintaining good prediction accuracy. We have illustrated our approach using several real-data examples from financial sectors, biomedical research, marketing campaigns, and the criminal justice system.},
  archive      = {J_ML},
  author       = {Mukhopadhyay, Subhadeep},
  doi          = {10.1007/s10994-021-06121-4},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {205-242},
  shortjournal = {Mach. Learn.},
  title        = {InfoGram and admissible machine learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAMBA: Safe model-based &amp; active reinforcement learning.
<em>ML</em>, <em>111</em>(1), 173–203. (<a
href="https://doi.org/10.1007/s10994-021-06103-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.},
  archive      = {J_ML},
  author       = {Cowen-Rivers, Alexander I. and Palenicek, Daniel and Moens, Vincent and Abdullah, Mohammed Amin and Sootla, Aivar and Wang, Jun and Bou-Ammar, Haitham},
  doi          = {10.1007/s10994-021-06103-6},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {173-203},
  shortjournal = {Mach. Learn.},
  title        = {SAMBA: Safe model-based &amp; active reinforcement learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inductive logic programming at 30. <em>ML</em>,
<em>111</em>(1), 147–172. (<a
href="https://doi.org/10.1007/s10994-021-06089-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inductive logic programming (ILP) is a form of logic-based machine learning. The goal is to induce a hypothesis (a logic program) that generalises given training examples and background knowledge. As ILP turns 30, we review the last decade of research. We focus on (i) new meta-level search methods, (ii) techniques for learning recursive programs, (iii) new approaches for predicate invention, and (iv) the use of different technologies. We conclude by discussing current limitations of ILP and directions for future research.},
  archive      = {J_ML},
  author       = {Cropper, Andrew and Dumančić, Sebastijan and Evans, Richard and Muggleton, Stephen H.},
  doi          = {10.1007/s10994-021-06089-1},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {147-172},
  shortjournal = {Mach. Learn.},
  title        = {Inductive logic programming at 30},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from interpretation transition using differentiable
logic programming semantics. <em>ML</em>, <em>111</em>(1), 123–145. (<a
href="https://doi.org/10.1007/s10994-021-06058-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of learning and reasoning is an essential and challenging topic in neuro-symbolic research. Differentiable inductive logic programming is a technique for learning a symbolic knowledge representation from either complete, mislabeled, or incomplete observed facts using neural networks. In this paper, we propose a novel differentiable inductive logic programming system called differentiable learning from interpretation transition (D-LFIT) for learning logic programs through the proposed embeddings of logic programs, neural networks, optimization algorithms, and an adapted algebraic method to compute the logic program semantics. The proposed model has several characteristics, including a small number of parameters, the ability to generate logic programs in a curriculum-learning setting, and linear time complexity for the extraction of trained neural networks. The well-known bottom clause positionalization algorithm is incorporated when the proposed system learns from relational datasets. We compare our model with NN-LFIT, which extracts propositional logic rules from retuned connected networks, the highly accurate rule learner RIPPER, the purely symbolic LFIT system LF1T, and CILP++, which integrates neural networks and the propositionalization method to handle first-order logic knowledge. From the experimental results, we conclude that D-LFIT yields comparable accuracy with respect to the baselines when given complete, incomplete, and mislabeled data. Our experimental results indicate that D-LFIT not only learns symbolic logic programs quickly and precisely but also performs robustly when processing mislabeled and incomplete datasets.},
  archive      = {J_ML},
  author       = {Gao, Kun and Wang, Hanpin and Cao, Yongzhi and Inoue, Katsumi},
  doi          = {10.1007/s10994-021-06058-8},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {123-145},
  shortjournal = {Mach. Learn.},
  title        = {Learning from interpretation transition using differentiable logic programming semantics},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to measure uncertainty in uncertainty sampling for
active learning. <em>ML</em>, <em>111</em>(1), 89–122. (<a
href="https://doi.org/10.1007/s10994-021-06003-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various strategies for active learning have been proposed in the machine learning literature. In uncertainty sampling, which is among the most popular approaches, the active learner sequentially queries the label of those instances for which its current prediction is maximally uncertain. The predictions as well as the measures used to quantify the degree of uncertainty, such as entropy, are traditionally of a probabilistic nature. Yet, alternative approaches to capturing uncertainty in machine learning, alongside with corresponding uncertainty measures, have been proposed in recent years. In particular, some of these measures seek to distinguish different sources and to separate different types of uncertainty, such as the reducible (epistemic) and the irreducible (aleatoric) part of the total uncertainty in a prediction. The goal of this paper is to elaborate on the usefulness of such measures for uncertainty sampling, and to compare their performance in active learning. To this end, we instantiate uncertainty sampling with different measures, analyze the properties of the sampling strategies thus obtained, and compare them in an experimental study.},
  archive      = {J_ML},
  author       = {Nguyen, Vu-Linh and Shaker, Mohammad Hossein and Hüllermeier, Eyke},
  doi          = {10.1007/s10994-021-06003-9},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {89-122},
  shortjournal = {Mach. Learn.},
  title        = {How to measure uncertainty in uncertainty sampling for active learning},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embed2Detect: Temporally clustered embedded words for event
detection in social media. <em>ML</em>, <em>111</em>(1), 49–87. (<a
href="https://doi.org/10.1007/s10994-021-05988-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media is becoming a primary medium to discuss what is happening around the world. Therefore, the data generated by social media platforms contain rich information which describes the ongoing events. Further, the timeliness associated with these data is capable of facilitating immediate insights. However, considering the dynamic nature and high volume of data production in social media data streams, it is impractical to filter the events manually and therefore, automated event detection mechanisms are invaluable to the community. Apart from a few notable exceptions, most previous research on automated event detection have focused only on statistical and syntactical features in data and lacked the involvement of underlying semantics which are important for effective information retrieval from text since they represent the connections between words and their meanings. In this paper, we propose a novel method termed Embed2Detect for event detection in social media by combining the characteristics in word embeddings and hierarchical agglomerative clustering. The adoption of word embeddings gives Embed2Detect the capability to incorporate powerful semantical features into event detection and overcome a major limitation inherent in previous approaches. We experimented our method on two recent real social media data sets which represent the sports and political domain and also compared the results to several state-of-the-art methods. The obtained results show that Embed2Detect is capable of effective and efficient event detection and it outperforms the recent event detection methods. For the sports data set, Embed2Detect achieved 27\% higher F-measure than the best-performed baseline and for the political data set, it was an increase of 29\%.},
  archive      = {J_ML},
  author       = {Hettiarachchi, Hansi and Adedoyin-Olowe, Mariam and Bhogal, Jagdev and Gaber, Mohamed Medhat},
  doi          = {10.1007/s10994-021-05988-7},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {49-87},
  shortjournal = {Mach. Learn.},
  title        = {Embed2Detect: Temporally clustered embedded words for event detection in social media},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stronger data poisoning attacks break data sanitization
defenses. <em>ML</em>, <em>111</em>(1), 1–47. (<a
href="https://doi.org/10.1007/s10994-021-06119-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models’ training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. In this paper, we develop three attacks that can bypass a broad range of common data sanitization defenses, including anomaly detectors based on nearest neighbors, training loss, and singular-value decomposition. By adding just 3\% poisoned data, our attacks successfully increase test error on the Enron spam detection dataset from 3 to 24\% and on the IMDB sentiment classification dataset from 12 to 29\%. In contrast, existing attacks which do not explicitly account for these data sanitization defenses are defeated by them. Our attacks are based on two ideas: (i) we coordinate our attacks to place poisoned points near one another, and (ii) we formulate each attack as a constrained optimization problem, with constraints designed to ensure that the poisoned points evade detection. As this optimization involves solving an expensive bilevel problem, our three attacks correspond to different ways of approximating this problem, based on influence functions; minimax duality; and the Karush–Kuhn–Tucker (KKT) conditions. Our results underscore the need to develop more robust defenses against data poisoning attacks.},
  archive      = {J_ML},
  author       = {Koh, Pang Wei and Steinhardt, Jacob and Liang, Percy},
  doi          = {10.1007/s10994-021-06119-y},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {Stronger data poisoning attacks break data sanitization defenses},
  volume       = {111},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
