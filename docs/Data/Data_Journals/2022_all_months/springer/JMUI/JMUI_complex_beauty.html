<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui---27">JMUI - 27</h2>
<ul>
<li><details>
<summary>
(2022). Grouping and determining perceived severity of cyber-attack
consequences: Gaining information needed to sonify cyber-attacks.
<em>JMUI</em>, <em>16</em>(4), 399–412. (<a
href="https://doi.org/10.1007/s12193-022-00397-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-attacks are a continuing problem. These attacks are problematic for users who are visually impaired and cannot rely on visual cues to indicate a potential cyber-attack. Sonification is an alternative way to help users who are visually impaired detect potential cyber-attacks. Sonification provides information to users using non-speech sounds. Sonification could provide users who are visually impaired with information on potential cyber-attack consequences that could stem from their actions. However, there are two challenges with sonifying cyber-attack consequences. First, there are many potential cyber-attack consequences to sonify, and humans have a limited ability to remember associations between sonifications and their meanings. Second, cyber-attack warning messages are better trusted when they align the severity of the consequences with the user’s perceived severity. However, we do not know the perceived severity of individual consequences. Therefore, we need to reduce the number of consequences to sonify and to determine the perceived severity of these consequences. We had non-expert participants group cyber-attack consequences based on perceived similarity. Analyses revealed that participants’ groupings formed seven clusters. We then had non-expert participants rate the perceived severity of each cyber-attack consequence. Those ratings were used to determine the perceived severity of each cluster. These efforts resulted in a set of cyber-attack consequence clusters that (a) is small enough that users should be able to remember associations between sonifications and their meanings, and (b) can be sonified in a way that reflects users’ perceptions regarding the severity of the clustered cyber-attack consequences. As such, the results of these studies are critical steps towards creating effective sonifications that serve as cyber-security warning messages.},
  archive      = {J_JMUI},
  author       = {Jones, Keith S. and Lodinger, Natalie R. and Widlus, Benjamin P. and Namin, Akbar Siami and Maw, Emily and Armstrong, Miriam},
  doi          = {10.1007/s12193-022-00397-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {399-412},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Grouping and determining perceived severity of cyber-attack consequences: Gaining information needed to sonify cyber-attacks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TapCAPTCHA: Non-visual CAPTCHA on touchscreens for visually
impaired people. <em>JMUI</em>, <em>16</em>(4), 385–398. (<a
href="https://doi.org/10.1007/s12193-022-00394-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most websites use CAPTCHAs for privacy and security reasons. Most existing CAPTCHA methods require users to solve them by typing on a keyboard. With improved smartphones allowing users to browse the web using a smartphone, accessible CAPTCHA suited to smartphone features should be developed, keeping the abilities of visually impaired people in mind because typing text on soft keyboards is time-consuming, frustrating, and error-prone. A novel CAPTCHA has been proposed, TapCAPTCHA, which presents the CAPTCHA challenge using an audio clip and an image simultaneously. Instead of typing the text on a soft keyboard, the user responds by tapping on the phone screen. The user hears a CAPTCHA question regarding, for example, how many times the user will listen to a letter or a number in the ensuing clip, which is a distorted audio clip in which six digits are readout. After that, the user must tap the screen according to the number of times the user has heard the specified letter or number. For example, the user must tap on the screen two times after hearing the letter D twice. The novel method aims to remove the cognitive burden, eliminate access obstacles, and enable users to browse websites independently. The paper also discusses the initial study with 16 visually impaired users to examine the method’s feasibility. The user study reveals that TapCAPTCHA is easy to learn, more accurate, and faster than traditional audio CAPTCHA. The success rate for TapCAPTCHA was 82.5%, while the audio CAPTCHA was 47.5%. The solving time for TapCAPTCHA was 11 and 119 s for the audio CAPTCHA. Ultimately, this research underscores the importance of designing a CAPTCHA method for visually impaired users that involves accessible gestures to reduce the error rate, effort, and cognitive load.},
  archive      = {J_JMUI},
  author       = {Alnfiai, Mrim and Alassery, Fawaz},
  doi          = {10.1007/s12193-022-00394-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {385-398},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {TapCAPTCHA: Non-visual CAPTCHA on touchscreens for visually impaired people},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gesture-based guidance for navigation in virtual
environments. <em>JMUI</em>, <em>16</em>(4), 371–383. (<a
href="https://doi.org/10.1007/s12193-022-00395-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different navigation aids (text, audio, maps, arrows, etc.) are used in complex virtual environments (VEs) to assist users in task operation and performance enhancement. Most current studies use cognitive cues to assist users during navigation and path selection in VEs. However, novices need to know which gesture to execute to carry out specific navigation. In this paper, a new concept of navigation aid is proposed that uses visual aids with gestural interaction during navigation tasks. The proposed aids provide two-fold guidance in the VE: they assist users in selecting the correct path and posing the correct gestures. In addition, it proposes fingertip pointing-based gestures for realistic navigation inside the VE to achieve high performance and usability using simple and lightweight gestures. Furthermore, the proposed aids (gesture guides) are compared with existing aids such as audio, textual, arrow-casting + textual, and 3D map + textual aids in terms of performance and usability. A VE is designed and implemented using OpenGL for experimental purposes. The Leap Motion controller is used for hand gesture recognition and interaction with VE. The System Usability Scale (SUS) is used for assessing system usability. Experimental results show comparatively improved performance and high usability for the proposed aids as compared to audio, textual, arrow-casting + textual, and 3D map + textual aids.},
  archive      = {J_JMUI},
  author       = {Rehman, Inam Ur and Ullah, Sehat and Ali, Numan and Rabbi, Ihsan and Khan, Riaz Ullah},
  doi          = {10.1007/s12193-022-00395-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {371-383},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Gesture-based guidance for navigation in virtual environments},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commanding a drone through body poses, improving the user
experience. <em>JMUI</em>, <em>16</em>(4), 357–369. (<a
href="https://doi.org/10.1007/s12193-022-00396-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose the use of Multimodal Human-Computer Interfaces (MHCI) through body poses to command a drone in an easy and intuitive way. First, the human-user pose is recovered from a video stream, with the help of the open source library Open Pose. Then, a Support Vector Classifier (SVC), trained to distinguish between different body poses, is used to interpret eleven different human poses as the most important high level commands to the drone. The proposed strategy was successfully implemented to remotely control a drone through a web interface, where the user can interact with a drone in a remote location using only a web interface. Real-time experiments were carried out with fourteen different volunteers, selected to represent different segments of the population, with different ages, gender, experience with technology, socioeconomic class, etc., in order to evaluate the user experience with the help of a User Experience Questionnaire (UEQ), demonstrating satisfactory results. The study suggests that the use of the proposed MHCI received good acceptance between the participants, even in users without previous experience with drones, and received excellent scores in terms of attractiveness, stimulation and novelty from most of the volunteers.},
  archive      = {J_JMUI},
  author       = {Yam-Viramontes, Brandon and Cardona-Reyes, Héctor and González-Trejo, Javier and Trujillo-Espinoza, Cristian and Mercado-Ravell, Diego},
  doi          = {10.1007/s12193-022-00396-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {357-369},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Commanding a drone through body poses, improving the user experience},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring visual stimuli as a support for novices’ creative
engagement with digital musical interfaces. <em>JMUI</em>,
<em>16</em>(3), 343–356. (<a
href="https://doi.org/10.1007/s12193-022-00393-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual materials are a widely used tool for stimulating creativity. This paper explores the potential for visual stimuli to support novices’ creative engagement with multimodal digital musical interfaces. An empirical study of 24 participants was conducted to compare the effect of abstract and literal forms of graphical scores on novices’ creative engagement, and whether being informed or uninformed about meanings of symbols in the score had any impact on creative engagement. The results suggest that abstract visual stimuli can provide an effective scaffold for creative engagement when participants are not informed about their design. It was found that providing information about visual stimuli has both advantages and disadvantages, depending largely on the stimuli’s visual style. Being informed about the meaning of a literal visual stimuli helped participants in making interpretations and gaining inspiration, whereas having information about abstract stimuli led to frustration. Qualitative data indicates that both forms of visual stimuli support creative engagement but at different stages of a creative process, and a descriptive model is presented to explain this. The findings highlight the benefits of visual stimuli in supporting creative engagement in the process of music making – a multimodal interaction domain typically involving few or no visual activities.},
  archive      = {J_JMUI},
  author       = {Wu, Yongmeng and Bryan-Kinns, Nick and Zhi, Jinyi},
  doi          = {10.1007/s12193-022-00393-3},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {343-356},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring visual stimuli as a support for novices’ creative engagement with digital musical interfaces},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing multi-purpose devices to enhance users’ perception
of haptics. <em>JMUI</em>, <em>16</em>(3), 335–342. (<a
href="https://doi.org/10.1007/s12193-022-00391-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Virtual Reality has slowly become a common sight, haptics is still struggling to appeal to the general public. We argue that one of the possible reasons is that while VR is designed to be as easily adaptable as possible to many different contexts, haptics is often designed to fulfil a specific purpose and fails to present itself as a tool that can be exploited by designers. To test our hypothesis, we created a VR game where a wrist exoskeleton was used to interact with the environment. The game was composed of multiple levels, some of which also featured a metaphorical interaction through the same haptic device, and was tested by expert haptics scholars during a conference. Preliminary results suggest that by showing multiple potential usages of an exoskeleton, it was possible to enhance users’ interest towards the haptic device and the game.},
  archive      = {J_JMUI},
  author       = {Galdieri, Riccardo and Camardella, Cristian and Carrozzino, Marcello and Frisoli, Antonio},
  doi          = {10.1007/s12193-022-00391-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {335-342},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Designing multi-purpose devices to enhance users’ perception of haptics},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A SLAM-based augmented reality app for the assessment of
spatial short-term memory using visual and auditory stimuli.
<em>JMUI</em>, <em>16</em>(3), 319–333. (<a
href="https://doi.org/10.1007/s12193-022-00392-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A SLAM-based Augmented Reality (AR) app has been designed, developed, and validated to assess spatial short-term memory. Our app can be used with visual and auditory stimuli and can run on mobile devices. It can be used in any indoor environment. The anchors and data of the app are persistently stored in the cloud. As an authoring tool, the type of stimulus, its number, and specific positions in the real environment can be customized for each session. A study involving 48 participants was carried out to analyze the performance outcomes comparing the location and remembering of stimuli in a real environment using visual versus auditory stimuli. The number of objects placed correctly was similar for the two different stimuli used. However, the group that used the auditory stimulus spent significantly more time completing the task and required significantly more attempts. The performance outcomes were independent of age and gender. For the auditory stimuli, correlations among all of the variables of the AR app and the variables of two other tasks (object-recall and map-pointing) were found. We also found that the greater the number of correctly placed auditory stimuli, the greater the perceived competence and the less mental effort required. The greater the number of errors, the less the perceived competence. Finally, the auditory stimuli are valid stimuli that may benefit the assessment of the memorization of spatial-auditory associations, but the memorization of spatial-visual associations is dominant, as our results suggest.},
  archive      = {J_JMUI},
  author       = {Juan, M.-Carmen and Mendez-Lopez, Magdalena and Fidalgo, Camino and Molla, Ramon and Vivo, Roberto and Paramo, David},
  doi          = {10.1007/s12193-022-00392-4},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {319-333},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A SLAM-based augmented reality app for the assessment of spatial short-term memory using visual and auditory stimuli},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ipsilateral and contralateral warnings: Effects on
decision-making and eye movements in near-collision scenarios.
<em>JMUI</em>, <em>16</em>(3), 303–317. (<a
href="https://doi.org/10.1007/s12193-022-00390-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cars are increasingly capable of providing drivers with warnings and advice. However, whether drivers should be provided with ipsilateral warnings (signaling the direction to steer towards) or contralateral warnings (signaling the direction to avoid) is inconclusive. Furthermore, how auditory warnings and visual information from the driving environment together contribute to drivers’ responses is relatively unexplored. In this study, 34 participants were presented with animated video clips of traffic situations on a three-lane road, while their eye movements were recorded with an eye-tracker. The videos ended with a near collision in front after 1, 3, or 6 s, while either the left or the right lane was safe to swerve into. Participants were instructed to make safe lane-change decisions by pressing the left or right arrow key. Upon the start of each video, participants heard a warning: Go Left/Right (ipsilateral), Danger Left/Right (contralateral), and nondirectional beeps (Baseline), emitted from the spatially corresponding left and right speakers. The results showed no significant differences in response times and accuracy between ipsilateral and contralateral warnings, although participants rated ipsilateral warnings as more satisfactory. Ipsilateral and contralateral warnings both improved response times in situations in which the left/right hazard was not yet manifest or was poorly visible. Participants fixated on salient and relevant vehicles as quickly as 220 ms after the trial started, with no significant differences between the audio types. In conclusion, directional warnings can aid in making a correct left/right evasive decision while not affecting the visual attention distribution.},
  archive      = {J_JMUI},
  author       = {de Winter, Joost and Hu, Jimmy and Petermeijer, Bastiaan},
  doi          = {10.1007/s12193-022-00390-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {303-317},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Ipsilateral and contralateral warnings: Effects on decision-making and eye movements in near-collision scenarios},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The multimodal EchoBorg: Not as smart as it looks.
<em>JMUI</em>, <em>16</em>(3), 293–302. (<a
href="https://doi.org/10.1007/s12193-022-00389-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a Multimodal Echoborg interface to explore the effect of different embodiments of an Embodied Conversational Agent (ECA) in an interaction. We compared an interaction where the ECA was embodied as a virtual human (VH) with one where it was embodied as an Echoborg, i.e, a person whose actions are covertly controlled by a dialogue system. The Echoborg in our study not only shadowed the speech output of the dialogue system but also its non-verbal actions. The interactions were structured as a debate between three participants on an ethical dilemma. First, we collected a corpus of debate sessions with three humans debaters. This we used as baseline to design and implement our ECAs. For the experiment, we designed two debate conditions. In one the participant interacted with two ECAs both embodied by virtual humans). In the other the participant interacted with one ECA embodied by a VH and the other by an Echoborg. Our results show that a human embodiment of the ECA overall scores better on perceived social attributes of the ECA. In many other respects the Echoborg scores as poorly as the VH except copresence.},
  archive      = {J_JMUI},
  author       = {Falcone, Sara and Kolkmeier, Jan and Bruijnes, Merijn and Heylen, Dirk},
  doi          = {10.1007/s12193-022-00389-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {293-302},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The multimodal EchoBorg: Not as smart as it looks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of challenges and methods for quality of experience
assessment of interactive VR applications. <em>JMUI</em>,
<em>16</em>(3), 257–291. (<a
href="https://doi.org/10.1007/s12193-022-00388-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User acceptance of virtual reality (VR) applications is dependent on multiple aspects, such as usability, enjoyment, and cybersickness. To fully realize the disruptive potential of VR technology in light of recent technological advancements (e.g., advanced headsets, immersive graphics), gaining a deeper understanding of underlying factors and dimensions impacting and contributing to the overall end-user experience is of great benefit to hardware manufacturers, software and content developers, and service providers. To provide insight into user behaviour and preferences, researchers conduct user studies exploring the influence of various user-, system-, and context-related factors on the overall Quality of Experience (QoE) and its dimensions. When planning and executing such studies, researchers are faced with numerous methodological challenges related to study design aspects, such as specification of dependant and independent variables, subjective and objective assessment methods, preparation of test materials, test environment, and participant recruitment. Approaching these challenges from a multidisciplinary perspective, this paper reviews different aspects of performing perception-based QoE assessment for interactive VR applications and presents options and recommendations for research methodology design. We provide an overview of different influence factors and dimensions that may affect the overall QoE, with a focus on presence, immersion, and discomfort. Furthermore, we address ethical and practical issues regarding participant choice and test material, present different assessment methods and measures commonly used in VR research, and discuss approaches to choosing study duration and location. Lastly, we provide a concise analysis of key challenges that need to be addressed in future studies centered around VR QoE.},
  archive      = {J_JMUI},
  author       = {Vlahovic, Sara and Suznjevic, Mirko and Skorin-Kapov, Lea},
  doi          = {10.1007/s12193-022-00388-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {257-291},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A survey of challenges and methods for quality of experience assessment of interactive VR applications},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on communication cues for augmented reality based
remote guidance. <em>JMUI</em>, <em>16</em>(2), 239–256. (<a
href="https://doi.org/10.1007/s12193-022-00387-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote guidance on physical tasks is a type of collaboration in which a local worker is guided by a remote helper to operate on a set of physical objects. It has many applications in industrial sections such as remote maintenance and how to support this type of remote collaboration has been researched for almost three decades. Although a range of different modern computing tools and systems have been proposed, developed and used to support remote guidance in different application scenarios, it is essential to provide communication cues in a shared visual space to achieve common ground for effective communication and collaboration. In this paper, we conduct a selective review to summarize communication cues, approaches that implement the cues and their effects on augmented reality based remote guidance. We also discuss challenges and propose possible future research and development directions.},
  archive      = {J_JMUI},
  author       = {Huang, Weidong and Wakefield, Mathew and Rasmussen, Troels Ammitsbøl and Kim, Seungwon and Billinghurst, Mark},
  doi          = {10.1007/s12193-022-00387-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {239-256},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A review on communication cues for augmented reality based remote guidance},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining haptics and inertial motion capture to enhance
remote control of a dual-arm robot. <em>JMUI</em>, <em>16</em>(2),
219–238. (<a href="https://doi.org/10.1007/s12193-021-00386-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dexterity is required in tasks in which there is contact between objects, such as surface conditioning (wiping, polishing, scuffing, sanding, etc.), specially when the location of the objects involved is unknown or highly inaccurate because they are moving, like a car body in automotive industry lines. These applications require the human adaptability and the robot accuracy. However, sharing the same workspace is not possible in most cases due to safety issues. Hence, a multi-modal teleoperation system combining haptics and an inertial motion capture system is introduced in this work. The human operator gets the sense of touch thanks to haptic feedback, whereas using the motion capture device allows more naturalistic movements. Visual feedback assistance is also introduced to enhance immersion. A Baxter dual-arm robot is used to offer more flexibility and manoeuvrability, allowing to perform two independent operations simultaneously. Several tests have been carried out to assess the proposed system. As it is shown by the experimental results, the task duration is reduced and the overall performance improves thanks to the proposed teleoperation method.},
  archive      = {J_JMUI},
  author       = {Girbés-Juan, Vicent and Schettino, Vinicius and Gracia, Luis and Solanes, J. Ernesto and Demiris, Yiannis and Tornero, Josep},
  doi          = {10.1007/s12193-021-00386-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {219-238},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Combining haptics and inertial motion capture to enhance remote control of a dual-arm robot},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The audio-corsi: An acoustic virtual reality-based
technological solution for evaluating audio-spatial memory abilities.
<em>JMUI</em>, <em>16</em>(2), 207–218. (<a
href="https://doi.org/10.1007/s12193-021-00383-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial memory is a cognitive skill that allows the recall of information about the space, its layout, and items’ locations. We present a novel application built around 3D spatial audio technology to evaluate audio-spatial memory abilities. The sound sources have been spatially distributed employing the 3D Tune-In Toolkit, a virtual acoustic simulator. The participants are presented with sequences of sounds of increasing length emitted from virtual auditory sources around their heads. To identify stimuli positions and register the test responses, we designed a custom-made interface with buttons arranged according to sound locations. We took inspiration from the Corsi-Block test for the experimental procedure, a validated clinical approach for assessing visuo-spatial memory abilities. In two different experimental sessions, the participants were tested with the classical Corsi-Block and, blindfolded, with the proposed task, named Audio-Corsi for brevity. Our results show comparable performance across the two tests in terms of the estimated memory parameter precision. Furthermore, in the Audio-Corsi we observe a lower span compared to the Corsi-Block test. We discuss these results in the context of the theoretical relationship between the auditory and visual sensory modalities and potential applications of this system in multiple scientific and clinical contexts.},
  archive      = {J_JMUI},
  author       = {Setti, Walter and Engel, Isaac Alonso-Martinez and Cuturi, Luigi F. and Gori, Monica and Picinali, Lorenzo},
  doi          = {10.1007/s12193-021-00383-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {207-218},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The audio-corsi: An acoustic virtual reality-based technological solution for evaluating audio-spatial memory abilities},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preliminary assessment of a multimodal electric-powered
wheelchair simulator for training of activities of daily living.
<em>JMUI</em>, <em>16</em>(2), 193–205. (<a
href="https://doi.org/10.1007/s12193-021-00385-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving an Electric-powered wheelchair requires a specific set of motor, visual and cognitive skills. One of the options is the use of wheelchair simulators to train the driving of the wheelchair in a controlled and completely safe environment. However, existing simulators do not have simultaneous characteristics of being multimodal and having training scenarios of activities of daily living. These two features would be very useful for training users with different disabilities in the activities they will use in daily living, as it could be adapted to a large number of users, avoiding specific customizations for each disability. This research proposes a multimodal Electric-powered wheelchair simulator with simultaneous focus in the training of activities of daily living and adaptability. The simulator was developed using the Unity 3D tool, with three scenarios (obstacles, accessibility ramp and elevators) and three input controls: joystick, electromyography and eye tracking. A pilot test was performed with four participants with different disabilities and experience in driving electric powered wheelchairs. Participants improved both their skills in operating the wheelchair and in relation to the control used, taking less time and effort at later stages of the experiment. And they demonstrated more confidence and ability in the use of the simulator as the experiment progressed. The multimodal simulator has the potential to help individuals train and develop the motor, cognitive and visual skills necessary to drive the wheelchair correctly while providing a high degree of adaptability to the user and his or her pathologies.},
  archive      = {J_JMUI},
  author       = {Martins, Felipe R. and Naves, Eduardo L. M. and Morère, Yann and de Sá, Angela A. R.},
  doi          = {10.1007/s12193-021-00385-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {193-205},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Preliminary assessment of a multimodal electric-powered wheelchair simulator for training of activities of daily living},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Importance of force feedback for following uneven virtual
paths with a stylus. <em>JMUI</em>, <em>16</em>(2), 183–191. (<a
href="https://doi.org/10.1007/s12193-021-00384-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is commonly known that a physical textured path can be followed by indirect touch through a probe also in absence of vision if sufficiently informative cues are delivered by the other sensory channels, but prior research indicates that the level of performance while following a virtual path on a touchscreen depends on the type and channel such cues belong to. The re-enactment of oriented forces, as they are induced by localized obstacles in probe-based exploration, may be important to equalize the performance between physical and virtual path following. Using a stylus attached to a force-feedback arm, an uneven path marked by virtual bars was traversed while time and positions were measured under normal sensory conditions, as well as in absence of vision or hearing. Alternatively, participants followed the same path on a wooden tablet provided with physical bars in relief (i.e., without receiving synthetic force) under the same conditions. The visual conditions were found to be significantly faster than the non-visual conditions. However, there was no significant advantage of traversing either path. In contrast to previous experiments in which the virtual bars were rendered using vibrotactile and/or auditory cues, comparable times to traverse the physical and virtual path were found also when vision was disabled. Our results hence suggest that users who are deprived of vision follow textured virtual paths as efficiently as physical paths, if unevenness is rendered using restorative force cues through a stylus.},
  archive      = {J_JMUI},
  author       = {Fontana, Federico and Muzzolini, Francesco and Rocchesso, Davide},
  doi          = {10.1007/s12193-021-00384-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {183-191},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Importance of force feedback for following uneven virtual paths with a stylus},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effect of eye movement sonification on visual search
patterns and anticipation in novices. <em>JMUI</em>, <em>16</em>(2),
173–182. (<a href="https://doi.org/10.1007/s12193-021-00381-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual information is essential to successfully anticipate the direction of the shot in ball sports whereas using another sense in motor learning has received less attention. The present study aimed to examine whether the multisensory learning with the orienting visual attention through the sound would influence anticipatory judgments with respect to the visual system alone. Forty novice students were randomly divided into visual and audio-visual groups. The experimental procedure comprised two phases; 1 training and 3 testing phases, respectively. During the training sessions, 200 video clips were employed to anticipate the direction of the shot, interspersed by 5-min of rest every 25 trials. A sound was used to orient the attention of the audio-visual group toward key points meanwhile the visual group watched the videos without sounds. Then, during the testing phases, they watched 20 video clips in the pretest, immediate retention, and delay retention test. The film was occluded at the racket-ball contact and then they quickly and carefully decided the direction of the shot. The audio-visual group showed higher response accuracy and shorter decision time than the visual group in the immediate and delayed retention. The audio-visual group exhibited longer fixation duration to the key areas than the visual group. In conclusion, using multisensory learning may not only reallocate perceptual and cognitive workload but also could reduce distraction, since, unlike visual perception, auditory perception requires neither specific athlete orientation nor a focus of attention. In general, the use of the multisensory learning is likely to be effective in learning complex motor tasks, facilitating the discovery of the new task needs and helping to perceive the exercise structure.},
  archive      = {J_JMUI},
  author       = {Khalaji, Maryam and Aghdaei, Mahin and Farsi, Alireza and Piras, Alessandro},
  doi          = {10.1007/s12193-021-00381-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {173-182},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The effect of eye movement sonification on visual search patterns and anticipation in novices},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informing the design of a multisensory learning environment
for elementary mathematics learning. <em>JMUI</em>, <em>16</em>(2),
155–171. (<a href="https://doi.org/10.1007/s12193-021-00382-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that primary school children may face difficulties in acquiring mathematical competence, possibly because teaching is generally based on formal lessons with little opportunity to exploit more multisensory-based activities within the classroom. To overcome such difficulties, we report here the exemplary design of a novel multisensory learning environment for teaching mathematical concepts based on meaningful inputs from elementary school teachers. First, we developed and administered a questionnaire to 101 teachers asking them to rate based on their experience the learning difficulty for specific arithmetical and geometrical concepts encountered by elementary school children. Additionally, the questionnaire investigated the feasibility to use multisensory information to teach mathematical concepts. Results show that challenging concepts differ depending on children school level, thus providing a guidance to improve teaching strategies and the design of new and emerging learning technologies accordingly. Second, we obtained specific and practical design inputs with workshops involving elementary school teachers and children. Altogether, these findings are used to inform the design of emerging multimodal technological applications, that take advantage not only of vision but also of other sensory modalities. In the present work, we describe in detail one exemplary multisensory environment design based on the questionnaire results and design ideas from the workshops: the Space Shapes game, which exploits visual and haptic/proprioceptive sensory information to support mental rotation, 2D–3D transformation and percentages. Corroborating research evidence in neuroscience and pedagogy, our work presents a functional approach to develop novel multimodal user interfaces to improve education in the classroom.},
  archive      = {J_JMUI},
  author       = {Cuturi, Luigi F. and Cappagli, Giulia and Yiannoutsou, Nikoleta and Price, Sara and Gori, Monica},
  doi          = {10.1007/s12193-021-00382-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {155-171},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Informing the design of a multisensory learning environment for elementary mathematics learning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TapCalculator: Nonvisual touchscreen calculator for visually
impaired people preliminary user study. <em>JMUI</em>, <em>16</em>(2),
143–154. (<a href="https://doi.org/10.1007/s12193-021-00379-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TapCalculator is a novel, non-visual touchscreen calculator that uses simple gestures to represent digits and operations with the support of audio feedback and is designed specifically for visually impaired users. It enables users to do basic calculations without visually locating buttons on a screen or having to know braille or learning complex coding for digits. It uses a series of tapping and swiping to perform arithmetic operations. This paper lays the groundwork for devising, improving, and implementing intuitive, user-friendly application to meet the needs of individuals with visual impairments. This paper is supported by primary-source research conducted by the author, including a review of several touchscreen calculators, using a small sample of visually impaired users. Seven participants were asked to compare the TapCalculator to two other virtual calculators, BrailleTap and TalkBack. From this preliminary usability study, quantitative findings indicate that blind users enter digits and arithmetic operations on the TapCalculator faster and less power consumption than the others due to TapCalcultator’s emphasis on directional gestures and allows users to tap with a maximum of 5 fingers. BrailleTap was shown to be more accurate than both TapCalculator and TalkBack, suggesting areas for improvement and further study. Through interviews with participants, qualitative results suggest that, for blind users attuned to braille, directional gestures are more accessible than locating the position of a touchscreen button. This paper suggests improvements to enhance the user experience of the TapCalculator, which is still in its infancy, and other virtual calculators for the visually impaired users.},
  archive      = {J_JMUI},
  author       = {Alnfiai, Mrim},
  doi          = {10.1007/s12193-021-00379-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {143-154},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {TapCalculator: Nonvisual touchscreen calculator for visually impaired people preliminary user study},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining audio and visual displays to highlight temporal
and spatial seismic patterns. <em>JMUI</em>, <em>16</em>(1), 125–142.
(<a href="https://doi.org/10.1007/s12193-021-00378-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization, and to a lesser extent data sonification, are classic tools to the scientific community. However, these two approaches are very rarely combined, although they are highly complementary: our visual system is good at recognizing spatial patterns, whereas our auditory system is better tuned for temporal patterns. In this article, data representation methods are proposed that combine visualization, sonification, and spatial audio techniques, in order to optimize the user’s perception of spatial and temporal patterns in a single display, to increase the feeling of immersion, and to take advantage of multimodal integration mechanisms. Three seismic data sets are used to illustrate the methods, covering different physical phenomena, time scales, spatial distributions, and spatio-temporal dynamics. The methods are adapted to the specificities of each data set, and to the amount of information that the designer wants to display. This leads to further developments, namely the use of audification with two time scales, the switch from pure audification to time-modulated noise, and the switch from pure audification to sonic icons. First user feedback from live demonstrations indicates that the methods presented in this article seem to enhance the perception of spatio-temporal patterns, which is a key parameter to the understanding of seismically active systems, and a step towards apprehending the processes that drive this activity.},
  archive      = {J_JMUI},
  author       = {Paté, Arthur and Farge, Gaspard and Holtzman, Benjamin K. and Barth, Anna C. and Poli, Piero and Boschi, Lapo and Karlstrom, Leif},
  doi          = {10.1007/s12193-021-00378-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {125-142},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Combining audio and visual displays to highlight temporal and spatial seismic patterns},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SoundSight: A mobile sensory substitution device that
sonifies colour, distance, and temperature. <em>JMUI</em>,
<em>16</em>(1), 107–123. (<a
href="https://doi.org/10.1007/s12193-021-00376-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth, colour, and thermal images contain practical and actionable information for the blind. Conveying this information through alternative modalities such as audition creates new interaction possibilities for users as well as opportunities to study neuroplasticity. The ‘SoundSight’ App ( www.SoundSight.co.uk ) is a smartphone platform that allows 3D position, colour, and thermal information to directly control thousands of high-quality sounds in real-time to create completely unique and responsive soundscapes for the user. Users can select the specific sensor input and style of auditory output, which can be based on anything—tones, rainfall, speech, instruments, or even full musical tracks. Appropriate default settings for image-sonification are given by designers, but users still have a fine degree of control over the timing and selection of these sounds. Through utilising smartphone technology with a novel approach to sonification, the SoundSight App provides a cheap, widely accessible, scalable, and flexible sensory tool. In this paper we discuss common problems encountered with assistive sensory tools reaching long-term adoption, how our device seeks to address these problems, its theoretical background, its technical implementation, and finally we showcase both initial user experiences and a range of use case scenarios for scientists, artists, and the blind community.},
  archive      = {J_JMUI},
  author       = {Hamilton-Fletcher, Giles and Alvarez, James and Obrist, Marianna and Ward, Jamie},
  doi          = {10.1007/s12193-021-00376-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {107-123},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {SoundSight: A mobile sensory substitution device that sonifies colour, distance, and temperature},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wearable virtual touch system for IVIS in cars.
<em>JMUI</em>, <em>16</em>(1), 87–106. (<a
href="https://doi.org/10.1007/s12193-021-00377-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In automotive domain, operation of secondary tasks like accessing infotainment system, adjusting air conditioning vents, and side mirrors distract drivers from driving. Though existing modalities like gesture and speech recognition systems facilitate undertaking secondary tasks by reducing duration of eyes off the road, those often require remembering a set of gestures or screen sequences. In this paper, we have proposed two different modalities for drivers to virtually touch the dashboard display using a laser tracker with a mechanical switch and an eye gaze switch. We compared performances of our proposed modalities against conventional touch modality in automotive environment by comparing pointing and selection times of representative secondary task and also analysed effect on driving performance in terms of deviation from lane, average speed, variation in perceived workload and system usability. We did not find significant difference in driving and pointing performance between laser tracking system and existing touchscreen system. Our result also showed that the driving and pointing performance of the virtual touch system with eye gaze switch was significantly better than the same with mechanical switch. We evaluated the efficacy of the proposed virtual touch system with eye gaze switch inside a real car and investigated acceptance of the system by professional drivers using qualitative research. The quantitative and qualitative studies indicated importance of using multimodal system inside car and highlighted several criteria for acceptance of new automotive user interface.},
  archive      = {J_JMUI},
  author       = {Prabhakar, Gowdham and Rajkhowa, Priyam and Harsha, Dharmesh and Biswas, Pradipta},
  doi          = {10.1007/s12193-021-00377-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {87-106},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A wearable virtual touch system for IVIS in cars},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive exploration of a hierarchical spider web
structure with sound. <em>JMUI</em>, <em>16</em>(1), 71–85. (<a
href="https://doi.org/10.1007/s12193-021-00375-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D spider webs exhibit highly intricate fiber architectures and owe their outstanding performance to a hierarchical organization that spans orders of magnitude in length scale from the molecular silk protein, to micrometer-sized fibers, and up to cm-scale web. Similarly, but in a completely different physical manifestation, music has a hierarchical structure composed of elementary sine wave building blocks that can be combined with other waveforms to create complex timbres, which are then arranged within larger-scale musical compositions. Although apparently different, spider webs and music have many similarities, as we point out in this work. Here, we propose an intuitive and interactive way to explore and visualize a 3D Cyrtophora citricola spider web geometry that has been digitally modeled with micron-scale details from full-scale laboratory experiments. We use model-based sonification to translate the web architecture into sound, allowing for aural perception and interpretation of its essential topological features. We implement this sonification using Unity3D and Max/MSP to create an interactive spider web environment in which a user travels through a virtual spider web. Each silk fiber in their field of view is sonified using different sine waves. Together, the sonified fibers create new and more complex timbres that reflects the architecture of 3D spider webs. These concepts are implemented into a spider web-based instrument for live performances, art installations and data exploration. It provides an unprecedented and creative way to immerse the composer, audience and user in an immersive multimedia experience generated by the complexity of a 3D spider web.},
  archive      = {J_JMUI},
  author       = {Su, Isabelle and Hattwick, Ian and Southworth, Christine and Ziporyn, Evan and Bisshop, Ally and Mühlethaler, Roland and Saraceno, Tomás and Buehler, Markus J.},
  doi          = {10.1007/s12193-021-00375-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {71-85},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Interactive exploration of a hierarchical spider web structure with sound},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Correction to: A gaze-based interactive system to explore
artwork imagery. <em>JMUI</em>, <em>16</em>(1), 69. (<a
href="https://doi.org/10.1007/s12193-021-00374-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Correction to this paper has been published: https://doi.org/10.1007/s12193-021-00373-z},
  archive      = {J_JMUI},
  author       = {Dondi, Piercarlo and Porta, Marco and Donvito, Angelo and Volpe, Giovanni},
  doi          = {10.1007/s12193-021-00374-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {69},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Correction to: A gaze-based interactive system to explore artwork imagery},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A gaze-based interactive system to explore artwork imagery.
<em>JMUI</em>, <em>16</em>(1), 55–67. (<a
href="https://doi.org/10.1007/s12193-021-00373-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive and immersive technologies can significantly enhance the fruition of museums and exhibits. Several studies have proved that multimedia installations can attract visitors, presenting cultural and scientific information in an appealing way. In this article, we present our workflow for achieving a gaze-based interaction with artwork imagery. We designed both a tool for creating interactive “gaze-aware” images and an eye tracking application conceived to interact with those images with the gaze. Users can display different pictures, perform pan and zoom operations, and search for regions of interest with associated multimedia content (text, image, audio, or video). Besides being an assistive technology for motor impaired people (like most gaze-based interaction applications), our solution can also be a valid alternative to the common touch screen panels present in museums, in accordance with the new safety guidelines imposed by the COVID-19 pandemic. Experiments carried out with a panel of volunteer testers have shown that the tool is usable, effective, and easy to learn.},
  archive      = {J_JMUI},
  author       = {Dondi, Piercarlo and Porta, Marco and Donvito, Angelo and Volpe, Giovanni},
  doi          = {10.1007/s12193-021-00373-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {55-67},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A gaze-based interactive system to explore artwork imagery},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFID-based tangible and touch tabletop for dual reality in
crisis management context. <em>JMUI</em>, <em>16</em>(1), 31–53. (<a
href="https://doi.org/10.1007/s12193-021-00370-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots are becoming more and more present in many domains of our daily lives. Their usage encompasses industry, home automation, space exploration, and military operations. Robots can also be used in crisis management situations, where it is impossible to access or dangerous to send humans into the intervention area. The present work compares users’ performances on tangible and on touch user interfaces, for a crisis management application on tabletop. The studied task consists of remotely controlling robots in a simulated disaster/intervention area using a tabletop equipped with a layer of RFID antennas, by displacing mini-robots on its surface matching the situation of the real robots on the ground. Dual reality enforces an accurate and up-to-date mapping between the real robots and the mini robots on the tabletop surface. Our findings show that tangible interaction outperforms touch interaction in effectiveness, efficiency and usability, in a task of remote control of one and two robots; only when the user manipulates a single robot remains the efficiency dimension unchanged between tangible and touch interaction. Results also show that tangible interaction technique does not significantly lower the users’ workload. We finally expose a post-experiment interview and questionnaire results, assessing the participants’ overall satisfaction and agreement on using tangible objects on a tabletop.},
  archive      = {J_JMUI},
  author       = {Merrad, Walid and Héloir, Alexis and Kolski, Christophe and Krüger, Antonio},
  doi          = {10.1007/s12193-021-00370-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {31-53},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {RFID-based tangible and touch tabletop for dual reality in crisis management context},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training public speaking with virtual social interactions:
Effectiveness of real-time feedback and delayed feedback. <em>JMUI</em>,
<em>16</em>(1), 17–29. (<a
href="https://doi.org/10.1007/s12193-021-00371-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social signal processing and virtual social interaction technologies have allowed the creation of social skills training applications, and initial studies have shown that such solutions can lead to positive training outcomes and could complement traditional teaching methods by providing cheap, accessible, safe tools for training social skills. However, these studies evaluated social skills training systems as a whole and it is unclear to what extent which components contributed to positive outcomes. In this paper, we describe an experimental study where we compared the relative efficacy of real-time interactive feedback and after-action feedback in the context of a public speaking training application. We observed that both components provide benefits to the overall training: the real-time interactive feedback made the experience more immersive and improved participants’ motivation in using the system, while the after-action feedback led to positive training outcomes when it contained personalized feedback elements. Taken in combination, these results confirm that both social signal processing technologies and virtual social interactions are both contributing to social skills training systems’ efficiency. Additionally, we observed that several individual factors, here the subjects’ initial level of public speaking anxiety, personality and tendency to immersion significantly influenced the training experience. This finding suggests that social skills training systems could benefit from being tailored to participants’ particular individual circumstances.},
  archive      = {J_JMUI},
  author       = {Chollet, Mathieu and Marsella, Stacy and Scherer, Stefan},
  doi          = {10.1007/s12193-021-00371-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {17-29},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Training public speaking with virtual social interactions: Effectiveness of real-time feedback and delayed feedback},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting multimodal presentation skills based on instance
weighting domain adaptation. <em>JMUI</em>, <em>16</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12193-021-00367-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presentation skills assessment is one of the central challenges of multimodal modeling. Presentation skills are composed of verbal and nonverbal skill components, but because people demonstrate their presentation skills in a variety of manners, the observed multimodal features vary widely. Due to the differences in features, when test data samples are generated on different training data sample distributions, in many cases, the prediction accuracy of the skills degrades. In machine learning theory, this problem in which training (source) data are biased is known as instance selection bias or covariate shift. To solve this problem, this paper presents an instance weighting adaptation method that is applied to estimate the presentation skills of each participant from multimodal (verbal and nonverbal) features. For this purpose, we collect a novel multimodal presentation dataset that includes audio signal data, body motion sensor data, and text data of the speech content for participants observed in 58 presentation sessions. The dataset also includes both verbal and nonverbal presentation skills, which are assessed by two external experts from a human resources department. We extract multimodal features, such as spoken utterances, acoustic features, and the amount of body motion, to estimate the presentation skills. We propose two approaches, early fusing and late fusing, for the regression models based on multimodal instance weighting adaptation. The experimental results show that the early fusing regression model with instance weighting adaptation achieved $$\rho =0.39$$ for the Pearson correlation, which presents the regression accuracy for the clarity of presentation goal elements. In the maximum case, the accuracy (correlation coefficient) is improved from $$-0.34$$ to +0.35 by instance weighting adaptation.},
  archive      = {J_JMUI},
  author       = {Yagi, Yutaro and Okada, Shogo and Shiobara, Shota and Sugimura, Sota},
  doi          = {10.1007/s12193-021-00367-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Predicting multimodal presentation skills based on instance weighting domain adaptation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
