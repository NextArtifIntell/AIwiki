<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SW_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sw---55">SW - 55</h2>
<ul>
<li><details>
<summary>
(2022). Linking women editors of periodicals to the wikidata
knowledge graph. <em>SW</em>, <em>14</em>(2), 443–455. (<a
href="https://doi.org/10.3233/SW-222845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stories are important tools for recounting and sharing the past. To tell a story one has to put together diverse information about people, places, time periods, and things. We detail here how a machine, through the power of Semantic Web, can compile scattered and diverse materials and information to construct stories. Through the example of the WeChangEd research project on women editors of periodicals in Europe from 1710–1920 we detail how to move from archive, to a structured data model and relational database, to Wikidata, to the use of the Stories Services API to generate multimedia stories related to people, organizations and periodicals. As more humanists, social scientists and other researchers choose to contribute their data to Wikidata we will all benefit. As researchers add data, the breadth and complexity of the questions we can ask about the data we have contributed will increase. Building applications that syndicate data from Wikidata allows us to leverage a general purpose knowledge graph with a growing number of references back to scholarly literature. Using frameworks developed by the Wikidata community allows us to rapidly provision interactive sites that will help us engage new audiences. This process that we detail here may be of interest to other researchers and cultural heritage institutions seeking web-based presentation options for telling stories from their data.},
  archive      = {J_SW},
  author       = {Thornton, Katherine and Seals-Nutt, Kenneth and Van Remoortel, Marianne and Birkholz, Julie M. and De Potter, Pieterjan},
  doi          = {10.3233/SW-222845},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {443-455},
  shortjournal = {Semantic Web},
  title        = {Linking women editors of periodicals to the wikidata knowledge graph},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Typed properties and negative typed properties: Dealing with
type observations and negative statements in the CIDOC CRM. <em>SW</em>,
<em>14</em>(2), 421–441. (<a
href="https://doi.org/10.3233/SW-223159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical case of producing records within the domain of conservation of cultural heritage is considered. During condition and collection surveys in memory organisations, surveyors observe types of multiple components of an object but without creating a record for each one. They also observe the absence of components. Such observations are significant to researchers and are documented in registration forms but they are not easy to implement using popular ontologies, such as the CIDOC CRM which primarily consider individuals. In this paper techniques for expressing such observations within the context of the CIDOC CRM in both OWL and RDFS are explored. OWL cardinality restrictions are considered and new special properties deriving from the CIDOC CRM are proposed, namely ‘typed properties’ and ‘negative typed properties’ which allow stating the types of multiple individuals and the absence of individuals. The nature of these properties is then explored in relation to their correspondence to longer property paths, their hierarchical arrangement and relevance to thesauri. An example from bookbinding history is used alongside a demonstration of the proposed solution with a dataset from the library collection of the Saint Catherine Monastery in Sinai, Egypt.},
  archive      = {J_SW},
  author       = {Velios, Athanasios and Meghini, Carlo and Doerr, Martin and Stead, Stephen},
  doi          = {10.3233/SW-223159},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {421-441},
  shortjournal = {Semantic Web},
  title        = {Typed properties and negative typed properties: Dealing with type observations and negative statements in the CIDOC CRM},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing biography collections historiographically as
linked data: Case national biography of finland. <em>SW</em>,
<em>14</em>(2), 385–419. (<a
href="https://doi.org/10.3233/SW-222887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biographical collections are available on the Web for close reading. However, the underlying texts can also be used for data analysis and distant reading, if the documents are available as data. Such data is usable for creating intelligent user interfaces to biographical data, including Digital Humanities tooling for visualizations, data analysis, and knowledge discovery in biographical and prosopographical research. In this paper, we re-use biographical collection data from a historiographical perspective for analyzing the underlying collection. For example: What kind of people have been included in the collection? Does the language used for describing female biographees differ from that for men? As a case study, the Finnish National Biography, available as part of the Linked Open Data service and semantic portal BiographySampo – Finnish Biographies on the Semantic Web is used. The analyses show interesting results related to, e.g., how specific prosopographical groups, such as women or professional groups are represented and portrayed. Various novel statistics and network analyses of the biographees are presented. Our analyses give new insights to the editors of the National Biography as well as to researchers in biography, prosopography, and historiography. The presented approach can be applied also to similar biography collections in other countries.},
  archive      = {J_SW},
  author       = {Tamper, Minna and Leskinen, Petri and Hyvönen, Eero and Valjus, Risto and Keravuori, Kirsi},
  doi          = {10.3233/SW-222887},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {385-419},
  shortjournal = {Semantic Web},
  title        = {Analyzing biography collections historiographically as linked data: Case national biography of finland},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transdisciplinary approach to archaeological investigations
in a semantic web perspective. <em>SW</em>, <em>14</em>(2), 361–383. (<a
href="https://doi.org/10.3233/SW-223016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the transdisciplinarity of archaeological studies has greatly increased because of the mature interactions between archaeologists and scientists from different disciplines (called “archaeometers”). A number of diverse scientific disciplines collaborate to get an objective account of the archaeological records. A large amount of digital data support the whole process, and there is a great value in keeping the coherence of information and knowledge, as contributed by each intervening discipline. During the years, a number of representation models have been developed to account for the recording of the archaeological process in data bases. Lately, some semantic models, compliant with the CRMarchaeo reference model, have been developed to account for linking the institutional forms with the formal knowledge concerning the archaeological excavations and the related findings. On the contrary, the archaeometric processes have not been addressed yet in the Semantic Web community and only an upper reference model, called CRMsci, accounts for the representation of the scientific investigations in general. This paper presents a modular computational ontology for the interlinked representation of all the facts related to the archaeological and archaeometric analyses and interpretations, also connected to the recording catalogues. The computational ontology is compliant with CIDOC-CRM reference models CRMarchaeo and CRMsci and introduces a number of novel classes and properties to merge the two worlds in a joint representation. The ontology is in use in “Beyond Archaeology”, a methodological project for the establishing of a transdisciplinary approach to archaeology and archaeometry, interlinked through a semantic model of processes and objects.},
  archive      = {J_SW},
  author       = {Lombardo, Vincenzo and Karatas, Tugce and Gulmini, Monica and Guidorzi, Laura and Angelici, Debora},
  doi          = {10.3233/SW-223016},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {361-383},
  shortjournal = {Semantic Web},
  title        = {Transdisciplinary approach to archaeological investigations in a semantic web perspective},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Move cultural heritage knowledge graphs in everyone’s
pocket. <em>SW</em>, <em>14</em>(2), 323–359. (<a
href="https://doi.org/10.3233/SW-223117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Last years witnessed a shift from the potential utility in digitisation to a crucial need to enjoy activities virtually. In fact, before 2019, data curators recognised the utility of performing data digitisation, while during the lockdown caused by the COVID-19, investing in virtual and remote activities to make culture survive became crucial as no one could enjoy Cultural Heritage in person. The Cultural Heritage community heavily invested in digitisation campaigns, mainly modelling data as Knowledge Graphs by becoming one of the most successful Semantic Web technologies application domains. Despite the vast investment in Cultural Heritage Knowledge Graphs, the syntactic complexity of RDF query languages, e.g., SPARQL, negatively affects and threatens data exploitation, risking leaving this enormous potential untapped. Thus, we aim to support the Cultural Heritage community (and everyone interested in Cultural Heritage) in querying Knowledge Graphs without requiring technical competencies in Semantic Web technologies. We propose an engaging exploitation tool accessible to all without losing sight of developers’ technological challenges. Engagement is achieved by letting the Cultural Heritage community leave the passive position of the visitor and actively create their Virtual Assistant extensions to exploit proprietary or public Knowledge Graphs in question-answering. By accessible to all, we mean that the proposed software framework is freely available on GitHub and Zenodo with an open-source license. We do not lose sight of developers’ technical challenges, which are carefully considered in the design and evaluation phases. This article first analyses the effort invested in publishing Cultural Heritage Knowledge Graphs to quantify data developers can rely on in designing and implementing data exploitation tools in this domain. Moreover, we point out challenges developers may face in exploiting them in automatic approaches. Second, it presents a domain-agnostic Knowledge Graph exploitation approach based on virtual assistants as they naturally enable question-answering features where users formulate questions in natural language directly by their smartphones. Then, we discuss the design and implementation of this approach within an automatic community-shared software framework (a.k.a. generator) of virtual assistant extensions and its evaluation in terms of performance and perceived utility according to end-users. Finally, according to a taxonomy of the Cultural Heritage field, we present a use case for each category to show the applicability of the proposed approach in the Cultural Heritage domain. In overviewing our analysis and the proposed approach, we point out challenges that a developer may face in designing virtual assistant extensions to query Knowledge Graphs, and we show the effect of these challenges in practice.},
  archive      = {J_SW},
  author       = {Pellegrino, Maria Angela and Scarano, Vittorio and Spagnuolo, Carmine},
  doi          = {10.3233/SW-223117},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {323-359},
  shortjournal = {Semantic Web},
  title        = {Move cultural heritage knowledge graphs in everyone’s pocket},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RelTopic: A graph-based semantic relatedness measure in
topic ontologies and its applicability for topic labeling of old press
articles. <em>SW</em>, <em>14</em>(2), 293–321. (<a
href="https://doi.org/10.3233/SW-222919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semantic measures have been used to solve problems in several domains. They tend to compare semantic entities in order to estimate their similarity or relatedness. While semantic similarity is applicable to hierarchies or taxonomies, semantic relatedness is adapted to ontologies. In this work, we propose a novel semantic relatedness measure, named Rel Topic , within topic ontologies for topic labeling purposes. In contrast to traditional measures, which are dependent on textual resources, Rel Topic considers semantic properties of entities in ontologies. Thus, correlations of nodes and weights of nodes and edges are assessed. The pertinence of Rel Topic is evaluated for topic labeling of old press articles. For this purpose, a topic ontology representing the articles, named Topic-OPA, is derived from open knowledge graphs by applying a SPARQL-based automatic approach. A use-case is presented in the context of the old French newspaper Le Matin . The generated topics are evaluated using a dual evaluation approach with the help of human annotators. Our approach shows an agreement quite close to that shown by humans. The entire approach’s reuse is demonstrated for labeling a different context of articles, recent (modern) newspapers.},
  archive      = {J_SW},
  author       = {El Ghosh, Mirna and Delestre, Nicolas and Kotowicz, Jean-Philippe and Zanni-Merk, Cecilia and Abdulrab, Habib},
  doi          = {10.3233/SW-222919},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {293-321},
  shortjournal = {Semantic Web},
  title        = {RelTopic: A graph-based semantic relatedness measure in topic ontologies and its applicability for topic labeling of old press articles},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic models and services for conservation and
restoration of cultural heritage: A comprehensive survey. <em>SW</em>,
<em>14</em>(2), 261–291. (<a
href="https://doi.org/10.3233/SW-223105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, the Cultural Heritage (CH) domain has gradually adopted Semantic Web (SW) technologies for organizing information and for tackling interoperability issues. Several semantic models have been proposed which accommodate essential aspects of information management: retrieval, integration, reuse and sharing. In this context, the CH subdomain of Conservation and Restoration (CnR) exhibits an increasing interest in SW technologies, in an attempt to effectively handle the highly heterogeneous and often secluded CnR information. This paper investigates semantic models relevant to the CnR knowledge domain. The scope, development methodology and coverage of CnR aspects are described and discussed. Furthermore, the evaluation, deployment and current exploitation of each model are examined, with focus on the types and variety of services provided to support the CnR professional. Through this study, the following research questions are investigated: To what extent the various aspects of CnR are covered by existing CnR models? To what extent existing CnR models incorporate models of the broader CH domain and of relevant disciplines (e.g., Chemistry)? In what ways and to what extent services built upon the reviewed models facilitate CnR professionals in their various tasks? Finally, based on the findings, fields of interest that merit further investigation are suggested.},
  archive      = {J_SW},
  author       = {Moraitou, Efthymia and Christodoulou, Yannis and Caridakis, George},
  doi          = {10.3233/SW-223105},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {261-291},
  shortjournal = {Semantic Web},
  title        = {Semantic models and services for conservation and restoration of cultural heritage: A comprehensive survey},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generation of training data for named entity recognition of
artworks. <em>SW</em>, <em>14</em>(2), 239–260. (<a
href="https://doi.org/10.3233/SW-223177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning techniques are being increasingly employed for text processing tasks, the need for training data has become a major bottleneck for their application. Manual generation of large scale training datasets tailored to each task is a time consuming and expensive process, which necessitates their automated generation. In this work, we turn our attention towards creation of training datasets for named entity recognition (NER) in the context of the cultural heritage domain. NER plays an important role in many natural language processing systems. Most NER systems are typically limited to a few common named entity types, such as person, location, and organization. However, for cultural heritage resources, such as digitized art archives, the recognition of fine-grained entity types such as titles of artworks is of high importance. Current state of the art tools are unable to adequately identify artwork titles due to unavailability of relevant training datasets. We analyse the particular difficulties presented by this domain and motivate the need for quality annotations to train machine learning models for identification of artwork titles. We present a framework with heuristic based approach to create high-quality training data by leveraging existing cultural heritage resources from knowledge bases such as Wikidata. Experimental evaluation shows significant improvement over the baseline for NER performance for artwork titles when models are trained on the dataset generated using our framework.},
  archive      = {J_SW},
  author       = {Jain, Nitisha and Sierra-Múnera, Alejandro and Ehmueller, Jan and Krestel, Ralf},
  doi          = {10.3233/SW-223177},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {239-260},
  shortjournal = {Semantic Web},
  title        = {Generation of training data for named entity recognition of artworks},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Question answering with deep neural networks for
semi-structured heterogeneous genealogical knowledge graphs.
<em>SW</em>, <em>14</em>(2), 209–237. (<a
href="https://doi.org/10.3233/SW-222925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: (1) representing genealogical data as knowledge graphs, (2) converting them to texts, (3) combining them with unstructured texts, and (4) training a transformer-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-answering models was performed. The findings indicate that there are significant differences between answering genealogical questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts as well as the general public.},
  archive      = {J_SW},
  author       = {Suissa, Omri and Zhitomirsky-Geffet, Maayan and Elmalech, Avshalom},
  doi          = {10.3233/SW-222925},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {209-237},
  shortjournal = {Semantic Web},
  title        = {Question answering with deep neural networks for semi-structured heterogeneous genealogical knowledge graphs},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linked open images: Visual similarity for the semantic web.
<em>SW</em>, <em>14</em>(2), 197–208. (<a
href="https://doi.org/10.3233/SW-212893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents ArtVision, a Semantic Web application that integrates computer vision APIs with the ResearchSpace platform, allowing for the matching of similar artworks and photographs across cultural heritage image collections. The field of Digital Art History stands to benefit a great deal from computer vision, as numerous projects have already made good progress in tackling issues of visual similarity, artwork classification, style detection, gesture analysis, among others. Pharos, the International Consortium of Photo Archives, is building its platform using the ResearchSpace knowledge system, an open-source semantic web platform that allows heritage institutions to publish and enrich collections as Linked Open Data through the CIDOC-CRM, and other ontologies. Using the images and artwork data of Pharos collections, this paper outlines the methodologies used to integrate visual similarity data from a number of computer vision APIs, allowing users to discover similar artworks and generate canonical URIs for each artwork.},
  archive      = {J_SW},
  author       = {Klic, Lukas},
  doi          = {10.3233/SW-212893},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {197-208},
  shortjournal = {Semantic Web},
  title        = {Linked open images: Visual similarity for the semantic web},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Casual learn: A linked data-based mobile application for
learning about local cultural heritage. <em>SW</em>, <em>14</em>(2),
181–195. (<a href="https://doi.org/10.3233/SW-212907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Casual Learn, an application that proposes ubiquitous learning tasks about Cultural Heritage. Casual Learn exploits a dataset of 10,000 contextualized learning tasks that were semiautomatically generated out of open data from the Web. Casual Learn offers these tasks to learners according to their physical location. For example, it may suggest describing the characteristics of the Gothic style when passing by a Gothic Cathedral. Additionally, Casual Learn has an interactive mode where learners can geo-search the tasks available. Casual Learn has been successfully used to support three pilot studies in two secondary-school institutions. It has also been awarded by the regional government and an international research conference. This made Casual Learn to appear in several regional newspapers, radios, and TV channels.},
  archive      = {J_SW},
  author       = {Ruiz-Calleja, Adolfo and García-Zarza, Pablo and Vega-Gorgojo, Guillermo and Bote-Lorenzo, Miguel L. and Gómez-Sánchez, Eduardo and Asensio-Pérez, Juan I. and Serrano-Iglesias, Sergio and Martínez-Monés, Alejandra},
  doi          = {10.3233/SW-212907},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {181-195},
  shortjournal = {Semantic Web},
  title        = {Casual learn: A&amp;nbsp;linked data-based mobile application for learning about local cultural heritage},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A shape expression approach for assessing the quality of
linked open data in libraries. <em>SW</em>, <em>14</em>(2), 159–179. (<a
href="https://doi.org/10.3233/SW-210441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.},
  archive      = {J_SW},
  author       = {Candela, Gustavo and Escobar, Pilar and Sáez, María Dolores and Marco-Such, Manuel},
  doi          = {10.3233/SW-210441},
  journal      = {Semantic Web},
  month        = {12},
  number       = {2},
  pages        = {159-179},
  shortjournal = {Semantic Web},
  title        = {A&amp;nbsp;Shape expression approach for assessing the quality of linked open data in libraries},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Components.js: Semantic dependency injection. <em>SW</em>,
<em>14</em>(1), 135–153. (<a
href="https://doi.org/10.3233/SW-222945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common practice within object-oriented software is using composition to realize complex object behavior in a reusable way. Such compositions can be managed by Dependency Injection ( DI ), a popular technique in which components only depend on minimal interfaces and have their concrete dependencies passed into them. Instead of requiring program code, this separation enables describing the desired instantiations in declarative configuration files, such that objects can be wired together automatically at runtime. Configurations for existing DI frameworks typically only have local semantics, which limits their usage in other contexts. Yet some cases require configurations outside of their local scope, such as for the reproducibility of experiments, static program analysis, and semantic workflows. As such, there is a need for globally interoperable, addressable, and discoverable configurations, which can be achieved by leveraging Linked Data. We created Components.js as an open-source semantic DI framework for TypeScript and JavaScript applications, providing global semantics via Linked Data-based configuration files. In this article, we report on the Components.js framework by explaining its architecture and configuration, and discuss its impact by mentioning where and how applications use it. We show that Components.js is a stable framework that has seen significant uptake during the last couple of years. We recommend it for software projects that require high flexibility, configuration without code changes, sharing configurations with others, or applying these configurations in other contexts such as experimentation or static program analysis. We anticipate that Components.js will continue driving concrete research and development projects that require high degrees of customization to facilitate experimentation and testing, including the Comunica query engine and the Community Solid Server for decentralized data publication.},
  archive      = {J_SW},
  author       = {Taelman, Ruben and Van Herwegen, Joachim and Vander Sande, Miel and Verborgh, Ruben},
  doi          = {10.3233/SW-222945},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {135-153},
  shortjournal = {Semantic Web},
  title        = {Components.js: Semantic dependency injection},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The OneGraph vision: Challenges of breaking the graph model
lock-in 1. <em>SW</em>, <em>14</em>(1), 125–134. (<a
href="https://doi.org/10.3233/SW-223273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amazon Neptune is a graph database service that supports two graph models: W3C’s Resource Description Framework (RDF) and Labeled Property Graphs (LPG). Customers choose one or the other model. This choice determines which data modeling features can be used and – perhaps more importantly – which query languages are available. The choice between the two technology stacks is difficult and time consuming. It requires consideration of data modeling aspects, query language features, their adequacy for current and future use cases, as well as developer knowledge. Even in cases where customers evaluate the pros and cons and make a conscious choice that fits their use case, over time we often see requirements from new use cases emerge that could be addressed more easily with a different data model or query language. It is therefore highly desirable that the choice of the query language can be made without consideration of what graph model is chosen and can be easily revised or complemented at a later point. To this end, we advocate and explore the idea of OneGraph (“1G” for short), a single, unified graph data model that embraces both RDF and LPGs. The goal of 1G is to achieve interoperability at both data level, by supporting the co-existence of RDF and LPG in the same database, as well as query level, by enabling queries and updates over the unified data model with a query language of choice. In this paper, we sketch our vision and investigate technical challenges towards a unification of the two graph data models.},
  archive      = {J_SW},
  author       = {Lassila, Ora and Schmidt, Michael and Hartig, Olaf and Bebee, Brad and Bechberger, Dave and Broekema, Willem and Khandelwal, Ankesh and Lawrence, Kelvin and Lopez Enriquez, Carlos Manuel and Sharda, Ronak and Thompson, Bryan},
  doi          = {10.3233/SW-223273},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {125-134},
  shortjournal = {Semantic Web},
  title        = {The OneGraph vision: Challenges of breaking the graph model lock-in 1},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning SHACL shapes from knowledge graphs. <em>SW</em>,
<em>14</em>(1), 101–121. (<a
href="https://doi.org/10.3233/SW-223063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) have proliferated on the Web since the introduction of knowledge panels to Google search in 2012. KGs are large data-first graph databases with weak inference rules and weakly-constraining data schemes. SHACL, the Shapes Constraint Language, is a W3C recommendation for expressing constraints on graph data as shapes. SHACL shapes serve to validate a KG, to underpin manual KG editing tasks, and to offer insight into KG structure. Often in practice, large KGs have no available shape constraints and so cannot obtain these benefits for ongoing maintenance and extension. We introduce Inverse Open Path (IOP) rules, a predicate logic formalism which presents specific shapes in the form of paths over connected entities that are present in a KG. IOP rules express simple shape patterns that can be augmented with minimum cardinality constraints and also used as a building block for more complex shapes, such as trees and other rule patterns. We define formal quality measures for IOP rules and propose a novel method to learn high-quality rules from KGs. We show how to build high-quality tree shapes from the IOP rules. Our learning method, SHACLearner , is adapted from a state-of-the-art embedding-based open path rule learner ( Oprl ). We evaluate SHACLearner on some real-world massive KGs, including YAGO2s (4M facts), DBpedia 3.8 (11M facts), and Wikidata (8M facts). The experiments show that our SHACLearner can effectively learn informative and intuitive shapes from massive KGs. The shapes are diverse in structural features such as depth and width, and also in quality measures that indicate confidence and generality.},
  archive      = {J_SW},
  author       = {Ghiasnezhad Omran, Pouya and Taylor, Kerry and Rodríguez Méndez, Sergio and Haller, Armin},
  doi          = {10.3233/SW-223063},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {101-121},
  shortjournal = {Semantic Web},
  title        = {Learning SHACL shapes from knowledge graphs},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance level analysis on linked open data connectivity for
cultural heritage entity linking and data integration. <em>SW</em>,
<em>14</em>(1), 55–100. (<a
href="https://doi.org/10.3233/SW-223026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cultural heritage, many projects execute Named Entity Linking (NEL) through global Linked Open Data (LOD) references in order to identify and disambiguate entities in their local datasets. It allows users to obtain extra information and contextualise the data with it. Thus, the aggregation and integration of heterogeneous LOD are expected. However, such development is still limited partly due to data quality issues. In addition, analysis on the LOD quality has not sufficiently been conducted for cultural heritage. Moreover, most research on data quality concentrates on ontology and corpus level observations. This paper examines the quality of the eleven major LOD sources used for NEL in cultural heritage with an emphasis on instance-level connectivity and graph traversals. Standardised linking properties are inspected for 100 instances/entities in order to create traversal route maps. Other properties are also assessed for quantity and quality. The outcomes suggest that the LOD is not fully interconnected and centrally condensed; the quantity and quality are unbalanced. Therefore, they cast doubt on the possibility of automatically identifying, accessing, and integrating known and unknown datasets. This implies the need for LOD improvement, as well as the NEL strategies to maximise the data integration.},
  archive      = {J_SW},
  author       = {Sugimoto, Go},
  doi          = {10.3233/SW-223026},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {55-100},
  shortjournal = {Semantic Web},
  title        = {Instance level analysis on linked open data connectivity for cultural heritage entity linking and data integration},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An assertion and alignment correction framework for large
scale knowledge bases. <em>SW</em>, <em>14</em>(1), 29–53. (<a
href="https://doi.org/10.3233/SW-210448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various knowledge bases (KBs) have been constructed via information extraction from encyclopedias, text and tables, as well as alignment of multiple sources. Their usefulness and usability is often limited by quality issues. One common issue is the presence of erroneous assertions and alignments, often caused by lexical or semantic confusion. We study the problem of correcting such assertions and alignments, and present a general correction framework which combines lexical matching, context-aware sub-KB extraction, semantic embedding, soft constraint mining and semantic consistency checking. The framework is evaluated with one set of literal assertions from DBpedia, one set of entity assertions from an enterprise medical KB, and one set of mapping assertions from a music KB constructed by integrating Wikidata, Discogs and MusicBrainz. It has achieved promising results, with a correction rate (i.e., the ratio of the target assertions/alignments that are corrected with right substitutes) of 70.1 % , 60.9 % and 71.8 % , respectively.},
  archive      = {J_SW},
  author       = {Chen, Jiaoyan and Jiménez-Ruiz, Ernesto and Horrocks, Ian and Chen, Xi and Myklebust, Erik Bryhn},
  doi          = {10.3233/SW-210448},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {29-53},
  shortjournal = {Semantic Web},
  title        = {An assertion and alignment correction framework for large scale knowledge bases},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using the W3C generating RDF from tabular data on the web
recommendation to manage small wikidata datasets. <em>SW</em>,
<em>14</em>(1), 5–27. (<a
href="https://doi.org/10.3233/SW-210443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The W3C Generating RDF from Tabular Data on the Web Recommendation provides a mechanism for mapping CSV-formatted data to any RDF graph model. Since the Wikibase data model used by Wikidata can be expressed as RDF, this Recommendation can be used to document tabular snapshots of parts of the Wikidata knowledge graph in a simple form that is easy for humans and applications to read. Those snapshots can be used to document how subgraphs of Wikidata have changed over time and can be compared with the current state of Wikidata using its Query Service to detect vandalism and value added through community contributions.},
  archive      = {J_SW},
  author       = {Baskauf, Steven J. and Baskauf, Jessica K.},
  doi          = {10.3233/SW-210443},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {5-27},
  shortjournal = {Semantic Web},
  title        = {Using the W3C generating RDF from tabular data on the web recommendation to manage small wikidata datasets},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial of knowledge graphs validation and quality.
<em>SW</em>, <em>14</em>(1), 3–4. (<a
href="https://doi.org/10.3233/SW-223261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SW},
  author       = {Labra Gayo, Jose Emilio and Dimou, Anastasia and Thornton, Katherine and Rula, Anisa},
  doi          = {10.3233/SW-223261},
  journal      = {Semantic Web},
  month        = {11},
  number       = {1},
  pages        = {3-4},
  shortjournal = {Semantic Web},
  title        = {Editorial of knowledge graphs validation and quality},
  volume       = {14},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Paving the way for enriched metadata of linguistic linked
data. <em>SW</em>, <em>13</em>(6), 1133–1157. (<a
href="https://doi.org/10.3233/SW-222994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for reusable, interoperable, and interlinked linguistic resources in Natural Language Processing downstream tasks has been proved by the increasing efforts to develop standards and metadata suitable to represent several layers of information. Nevertheless, despite these efforts, the achievement of full compatibility for metadata in linguistic resource production is still far from being reached. Access to resources observing these standards is hindered either by (i) lack of or incomplete information, (ii) inconsistent ways of coding their metadata, and (iii) lack of maintenance. In this paper, we offer a quantitative and qualitative analysis of descriptive metadata and resources availability of two main metadata repositories: LOD Cloud and Annohub. Furthermore, we introduce a metadata enrichment, which aims at improving resource information, and a metadata alignment to META-SHARE ontology, suitable for easing the accessibility and interoperability of such resources.},
  archive      = {J_SW},
  author       = {di Buono, Maria Pia and Gonçalo Oliveira, Hugo and Barbu Mititelu, Verginica and Spahiu, Blerina and Nolano, Gennaro},
  doi          = {10.3233/SW-222994},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {1133-1157},
  shortjournal = {Semantic Web},
  title        = {Paving the way for enriched metadata of linguistic linked data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bilingual dictionary generation and enrichment via graph
exploration. <em>SW</em>, <em>13</em>(6), 1103–1132. (<a
href="https://doi.org/10.3233/SW-222899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have witnessed a steady growth of linguistic information represented and exposed as linked data on the Web. Such linguistic linked data have stimulated the development and use of openly available linguistic knowledge graphs, as is the case with the Apertium RDF, a collection of interconnected bilingual dictionaries represented and accessible through Semantic Web standards. In this work, we explore techniques that exploit the graph nature of bilingual dictionaries to automatically infer new links (translations). We build upon a cycle density based method: partitioning the graph into biconnected components for a speed-up, and simplifying the pipeline through a careful structural analysis that reduces hyperparameter tuning requirements. We also analyse the shortcomings of traditional evaluation metrics used for translation inference and propose to complement them with new ones, both-word precision (BWP) and both-word recall (BWR), aimed at being more informative of algorithmic improvements. Over twenty-seven language pairs, our algorithm produces dictionaries about 70% the size of existing Apertium RDF dictionaries at a high BWP of 85% from scratch within a minute. Human evaluation shows that 78% of the additional translations generated for dictionary enrichment are correct as well. We further describe an interesting use-case: inferring synonyms within a single language, on which our initial human-based evaluation shows an average accuracy of 84%. We release our tool as free/open-source software which can not only be applied to RDF data and Apertium dictionaries, but is also easily usable for other formats and communities.},
  archive      = {J_SW},
  author       = {Goel, Shashwat and Gracia, Jorge and Forcada, Mikel L.},
  doi          = {10.3233/SW-222899},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {1103-1132},
  shortjournal = {Semantic Web},
  title        = {Bilingual dictionary generation and enrichment via graph exploration},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linking discourse-level information and the induction of
bilingual discourse connective lexicons. <em>SW</em>, <em>13</em>(6),
1081–1102. (<a href="https://doi.org/10.3233/SW-223011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single biggest obstacle in performing comprehensive cross-lingual discourse analysis is the scarcity of multilingual resources. The existing resources are overwhelmingly monolingual, compelling researchers to infer the discourse-level information in the target languages through error-prone automatic means. The current paper aims to provide a more direct insight into the cross-lingual variations in discourse structures by linking the annotated relations of the TED-Multilingual Discourse Bank, which consists of independently annotated six TED talks in seven different languages. It is shown that the linguistic labels over the relations annotated in the texts of these languages can be automatically linked with English with high accuracy, as verified against the relations of three diverse languages semi-automatically linked with relations over English texts. The resulting corpus has a great potential to reveal the divergences in local discourse relations, as well as leading to new resources, as exemplified by the induction of bilingual discourse connective lexicons.},
  archive      = {J_SW},
  author       = {Özer, Sibel and Kurfalı, Murathan and Zeyrek, Deniz and Mendes, Amália and Valūnaitė Oleškevičienė, Giedrė},
  doi          = {10.3233/SW-223011},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {1081-1102},
  shortjournal = {Semantic Web},
  title        = {Linking discourse-level information and the induction of bilingual discourse connective lexicons},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LL(o)d and NLP perspectives on semantic change for
humanities research. <em>SW</em>, <em>13</em>(6), 1051–1080. (<a
href="https://doi.org/10.3233/SW-222848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with its main application in humanities research. The paper’s aim is to provide the starting point for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action Nexus Linguarum, European network for Web-centred linguistic data science , CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.},
  archive      = {J_SW},
  author       = {Armaselu, Florentina and Apostol, Elena-Simona and Khan, Anas Fahad and Liebeskind, Chaya and McGillivray, Barbara and Truică, Ciprian-Octavian and Utka, Andrius and Valūnaitė Oleškevičienė, Giedrė and van Erp, Marieke},
  doi          = {10.3233/SW-222848},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {1051-1080},
  shortjournal = {Semantic Web},
  title        = {LL(O)D and NLP perspectives on semantic change for humanities research},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When linguistics meets web technologies. Recent advances in
modelling linguistic linked data. <em>SW</em>, <em>13</em>(6), 987–1050.
(<a href="https://doi.org/10.3233/SW-222859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a comprehensive and up-to-date survey of models and vocabularies for creating linguistic linked data (LLD) focusing on the latest developments in the area and both building upon and complementing previous works covering similar territory. The article begins with an overview of some recent trends which have had a significant impact on linked data models and vocabularies. Next, we give a general overview of existing vocabularies and models for different categories of LLD resource. After which we look at some of the latest developments in community standards and initiatives including descriptions of recent work on the OntoLex-Lemon model, a survey of recent initiatives in linguistic annotation and LLD, and a discussion of the LLD metadata vocabularies META-SHARE and lime . In the next part of the paper, we focus on the influence of projects on LLD models and vocabularies, starting with a general survey of relevant projects, before dedicating individual sections to a number of recent projects and their impact on LLD vocabularies and models. Finally, in the conclusion, we look ahead at some future challenges for LLD models and vocabularies. The appendix to the paper consists of a brief introduction to the OntoLex-Lemon model.},
  archive      = {J_SW},
  author       = {Khan, Anas Fahad and Chiarcos, Christian and Declerck, Thierry and Gifu, Daniela and García, Elena González-Blanco and Gracia, Jorge and Ionov, Maxim and Labropoulou, Penny and Mambrini, Francesco and McCrae, John P. and Pagé-Perron, Émilie and Passarotti, Marco and Muñoz, Salvador Ros and Truică, Ciprian-Octavian},
  doi          = {10.3233/SW-222859},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {987-1050},
  shortjournal = {Semantic Web},
  title        = {When linguistics meets web technologies. recent advances in modelling linguistic linked data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TermitUp: Generation and enrichment of linked terminologies.
<em>SW</em>, <em>13</em>(6), 967–986. (<a
href="https://doi.org/10.3233/SW-222885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific terminologies play a central role in many language technology solutions. Substantial manual effort is still involved in the creation of such resources, and many of them are published in proprietary formats that cannot be easily reused in other applications. Automatic term extraction tools help alleviate this cumbersome task. However, their results are usually in the form of plain lists of terms or as unstructured data with limited linguistic information. Initiatives such as the Linguistic Linked Open Data cloud (LLOD) foster the publication of language resources in open structured formats, specifically RDF, and their linking to other resources on the Web of Data. In order to leverage the wealth of linguistic data in the LLOD and speed up the creation of linked terminological resources, we propose TermitUp, a service that generates enriched domain specific terminologies directly from corpora, and publishes them in open and structured formats. TermitUp is composed of five modules performing terminology extraction, terminology post-processing, terminology enrichment, term relation validation and RDF publication. As part of the pipeline implemented by this service, existing resources in the LLOD are linked with the resulting terminologies, contributing in this way to the population of the LLOD cloud. TermitUp has been used in the framework of European projects tackling different fields, such as the legal domain, with promising results. Different alternatives on how to model enriched terminologies are considered and good practices illustrated with examples are proposed.},
  archive      = {J_SW},
  author       = {Martín-Chozas, Patricia and Vázquez-Flores, Karen and Calleja, Pablo and Montiel-Ponsoda, Elena and Rodríguez-Doncel, Víctor},
  doi          = {10.3233/SW-222885},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {967-986},
  shortjournal = {Semantic Web},
  title        = {TermitUp: Generation and enrichment of linked terminologies},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survey on english entity linking on wikidata: Datasets and
approaches. <em>SW</em>, <em>13</em>(6), 925–966. (<a
href="https://doi.org/10.3233/SW-212865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikidata is a frequently updated, community-driven, and multilingual knowledge graph. Hence, Wikidata is an attractive basis for Entity Linking, which is evident by the recent increase in published papers. This survey focuses on four subjects: (1) Which Wikidata Entity Linking datasets exist, how widely used are they and how are they constructed? (2) Do the characteristics of Wikidata matter for the design of Entity Linking datasets and if so, how? (3) How do current Entity Linking approaches exploit the specific characteristics of Wikidata? (4) Which Wikidata characteristics are unexploited by existing Entity Linking approaches? This survey reveals that current Wikidata-specific Entity Linking datasets do not differ in their annotation scheme from schemes for other knowledge graphs like DBpedia. Thus, the potential for multilingual and time-dependent datasets, naturally suited for Wikidata, is not lifted. Furthermore, we show that most Entity Linking approaches use Wikidata in the same way as any other knowledge graph missing the chance to leverage Wikidata-specific characteristics to increase quality. Almost all approaches employ specific properties like labels and sometimes descriptions but ignore characteristics such as the hyper-relational structure. Hence, there is still room for improvement, for example, by including hyper-relational graph embeddings or type information. Many approaches also include information from Wikipedia, which is easily combinable with Wikidata and provides valuable textual information, which Wikidata lacks.},
  archive      = {J_SW},
  author       = {Möller, Cedric and Lehmann, Jens and Usbeck, Ricardo},
  doi          = {10.3233/SW-212865},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {925-966},
  shortjournal = {Semantic Web},
  title        = {Survey on english entity linking on wikidata: Datasets and approaches},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Glottocodes: Identifiers linking families, languages and
dialects to comprehensive reference information. <em>SW</em>,
<em>13</em>(6), 917–924. (<a
href="https://doi.org/10.3233/SW-212843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glottocodes constitute the backbone identification system for the language, dialect and family inventory Glottolog ( https://glottolog.org ). In this paper, we summarize the motivation and history behind the system of glottocodes and describe the principles and practices of data curation, technical infrastructure and update/version-tracking systematics. Since our understanding of the target domain – the dialects, languages and language families of the entire world – is continually evolving, changes and updates are relatively common. The resulting data is assessed in terms of the FAIR (Findable, Accessible, Interoperable, Reusable) Guiding Principles for scientific data management and stewardship. As such the glottocode-system responds to an important challenge in the realm of Linguistic Linked Data with numerous NLP applications.},
  archive      = {J_SW},
  author       = {Forkel, Robert and Hammarström, Harald},
  doi          = {10.3233/SW-212843},
  journal      = {Semantic Web},
  month        = {9},
  number       = {6},
  pages        = {917-924},
  shortjournal = {Semantic Web},
  title        = {Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep understanding of everyday activity commands for
household robots. <em>SW</em>, <em>13</em>(5), 895–909. (<a
href="https://doi.org/10.3233/SW-222973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Going from natural language directions to fully specified executable plans for household robots involves a challenging variety of reasoning steps. In this paper, a processing pipeline to tackle these steps for natural language directions is proposed and implemented. It uses the ontological Socio-physical Model of Activities (SOMA) as a common interface between its components. The pipeline includes a natural language parser and a module for natural language grounding. Several reasoning steps formulate simulation plans, in which robot actions are guided by data gathered using human computation. As a last step, the pipeline simulates the given natural language direction inside a virtual environment. The major advantage of employing an overarching ontological framework is that its asserted facts can be stored alongside the semantics of directions, contextual knowledge, and annotated activity models in one central knowledge base. This allows for a unified and efficient knowledge retrieval across all pipeline components, providing flexibility and reasoning capabilities as symbolic knowledge is combined with annotated sub-symbolic models.},
  archive      = {J_SW},
  author       = {Höffner, Sebastian and Porzel, Robert and Hedblom, Maria M. and Pomarlan, Mihai and Cangalovic, Vanja Sophie and Pfau, Johannes and Bateman, John A. and Malaka, Rainer},
  doi          = {10.3233/SW-222973},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {895-909},
  shortjournal = {Semantic Web},
  title        = {Deep understanding of everyday activity commands for household robots},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantics and canonicalisation of SPARQL 1.1. <em>SW</em>,
<em>13</em>(5), 829–893. (<a
href="https://doi.org/10.3233/SW-212871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries ), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.},
  archive      = {J_SW},
  author       = {Salas, Jaime and Hogan, Aidan},
  doi          = {10.3233/SW-212871},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {829-893},
  shortjournal = {Semantic Web},
  title        = {Semantics and canonicalisation of SPARQL&amp;nbsp;1.1},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond facts – a survey and conceptualisation of claims in
online discourse analysis. <em>SW</em>, <em>13</em>(5), 793–827. (<a
href="https://doi.org/10.3233/SW-212838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing statements of facts and claims in online discourse is subject of a multitude of research areas. Methods from natural language processing and computational linguistics help investigate issues such as the spread of biased narratives and falsehoods on the Web. Related tasks include fact-checking, stance detection and argumentation mining. Knowledge-based approaches, in particular works in knowledge base construction and augmentation, are concerned with mining, verifying and representing factual knowledge. While all these fields are concerned with strongly related notions, such as claims, facts and evidence, terminology and conceptualisations used across and within communities vary heavily, making it hard to assess commonalities and relations of related works and how research in one field may contribute to address problems in another. We survey the state-of-the-art from a range of fields in this interdisciplinary area across a range of research tasks. We assess varying definitions and propose a conceptual model – Open Claims – for claims and related notions that takes into consideration their inherent complexity, distinguishing between their meaning, linguistic representation and context. We also introduce an implementation of this model by using established vocabularies and discuss applications across various tasks related to online discourse analysis.},
  archive      = {J_SW},
  author       = {Boland, Katarina and Fafalios, Pavlos and Tchechmedjiev, Andon and Dietze, Stefan and Todorov, Konstantin},
  doi          = {10.3233/SW-212838},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {793-827},
  shortjournal = {Semantic Web},
  title        = {Beyond facts – a survey and conceptualisation of claims in online discourse analysis},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual notations for viewing RDF constraints with UnSHACLed.
<em>SW</em>, <em>13</em>(5), 757–792. (<a
href="https://doi.org/10.3233/SW-210450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of knowledge graphs can be assessed by a validation against specified constraints, typically use-case specific and modeled by human users in a manual fashion. Visualizations can improve the modeling process as they are specifically designed for human information processing, possibly leading to more accurate constraints, and in turn higher quality knowledge graphs. However, it is currently unknown how such visualizations support users when viewing RDF constraints as no scientific evidence for the visualizations’ effectiveness is provided. Furthermore, some of the existing tools are likely suboptimal, as they lack support for edit operations or common constraints types. To establish a baseline, we have defined visual notations to represent RDF constraints and implemented them in UnSHACLed , a tool that is independent of a concrete RDF constraint language. In this paper, we (i) present two visual notations that support all SHACL core constraints, built upon the commonly used visualizations VOWL and UML, (ii) analyze both notations based on cognitive effective design principles, (iii) perform a comparative user study between both visual notations, and (iv) present our open source tool UnSHACLed incorporating our efforts. Users were presented RDF constraints in both visual notations and had to answer questions based on visualization task taxonomies. Although no statistical significant difference in mean error rates was observed, all study participants preferred ShapeVOWL in a self assessment to answer RDF constraint-related questions. Furthermore, ShapeVOWL adheres to more cognitive effective design principles according to our performed comparison. Study participants argued that the increased visual features of ShapeVOWL made it easier to spot constraints, but a list of constraints – as in ShapeUML – is easier to read. However, also that more deviations from the strict UML specification and introduction of more visual features can improve ShapeUML . From these findings we conclude that ShapeVOWL has a higher potential to represent RDF constraints more effective compared to ShapeUML . But also that the clear and efficient text encoding of ShapeUML can be improved with visual features. A one-size-fits-all approach to RDF constraint visualization and editing will be insufficient. Therefore, to support different audiences and use cases, user interfaces of RDF constraint editors need to support different visual notations.},
  archive      = {J_SW},
  author       = {Lieber, Sven and De Meester, Ben and Heyvaert, Pieter and Brückmann, Femke and Wambacq, Ruben and Mannens, Erik and Verborgh, Ruben and Dimou, Anastasia},
  doi          = {10.3233/SW-210450},
  journal      = {Semantic Web},
  month        = {8},
  number       = {5},
  pages        = {757-792},
  shortjournal = {Semantic Web},
  title        = {Visual notations for viewing RDF constraints with UnSHACLed},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online approximative SPARQL query processing for
COUNT-DISTINCT queries with web preemption. <em>SW</em>, <em>13</em>(4),
735–755. (<a href="https://doi.org/10.3233/SW-222842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Getting complete results when processing aggregate queries on public SPARQL endpoints is challenging, mainly due to the application of quotas. Although Web preemption supports processing of aggregate queries online, on preemptable SPARQL servers, data transfer is still very large when processing count-distinct aggregate queries. In this paper, it is shown that count-distinct aggregate queries can be approximated with low data transfer by extending the partial aggregation operator with HyperLogLog++ sketches. Experimental results demonstrate that the proposed approach outperforms existing approaches by orders of magnitude in terms of the amount of data transferred.},
  archive      = {J_SW},
  author       = {Aimonier-Davat, Julien and Skaf-Molli, Hala and Molli, Pascal and Grall, Arnaud and Minier, Thomas},
  doi          = {10.3233/SW-222842},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {735-755},
  shortjournal = {Semantic Web},
  title        = {Online approximative SPARQL query processing for COUNT-DISTINCT queries with web preemption},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing storage of RDF archives using bidirectional delta
chains. <em>SW</em>, <em>13</em>(4), 705–734. (<a
href="https://doi.org/10.3233/SW-210449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked Open Datasets on the Web that are published as RDF can evolve over time. There is a need to be able to store such evolving RDF datasets, and query across their versions. Different storage strategies are available for managing such versioned datasets, each being efficient for specific types of versioned queries. In recent work, a hybrid storage strategy has been introduced that combines these different strategies to lead to more efficient query execution for all versioned query types at the cost of increased ingestion time. While this trade-off is beneficial in the context of Web querying, it suffers from exponential ingestion times in terms of the number of versions, which becomes problematic for RDF datasets with many versions. As such, there is a need for an improved storage strategy that scales better in terms of ingestion time for many versions. We have designed, implemented, and evaluated a change to the hybrid storage strategy where we make use of a bidirectional delta chain instead of the default unidirectional delta chain . In this article, we introduce a concrete architecture for this change, together with accompanying ingestion and querying algorithms. Experimental results from our implementation show that the ingestion time is significantly reduced. As an additional benefit, this change also leads to lower total storage size and even improved query execution performance in some cases. This work shows that modifying the structure of delta chains within the hybrid storage strategy can be highly beneficial for RDF archives. In future work, other modifications to this delta chain structure deserve to be investigated, to further improve the scalability of ingestion and querying of datasets with many versions.},
  archive      = {J_SW},
  author       = {Taelman, Ruben and Mahieu, Thibault and Vanbrabant, Martin and Verborgh, Ruben},
  doi          = {10.3233/SW-210449},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {705-734},
  shortjournal = {Semantic Web},
  title        = {Optimizing storage of RDF archives using bidirectional delta chains},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foundational ontologies meet ontology matching: A survey.
<em>SW</em>, <em>13</em>(4), 685–704. (<a
href="https://doi.org/10.3233/SW-210447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching is a research area aimed at finding ways to make different ontologies interoperable. Solutions to the problem have been proposed from different disciplines, including databases, natural language processing, and machine learning. The role of foundational ontologies for ontology matching is an important one, as they provide a well-founded reference model that can be shared across domains. It is multifaceted and with room for development. This paper presents an overview of the different tasks involved in ontology matching that consider foundational ontologies. We discuss the strengths and weaknesses of existing proposals and highlight the challenges to be addressed in the future.},
  archive      = {J_SW},
  author       = {Trojahn, Cassia and Vieira, Renata and Schmidt, Daniela and Pease, Adam and Guizzardi, Giancarlo},
  doi          = {10.3233/SW-210447},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {685-704},
  shortjournal = {Semantic Web},
  title        = {Foundational ontologies meet ontology matching: A survey},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handling qualitative preferences in SPARQL over virtual
ontology-based data access. <em>SW</em>, <em>13</em>(4), 659–682. (<a
href="https://doi.org/10.3233/SW-212895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of data volume in heterogeneous datasets that are being published following Open Data initiatives, new operators are necessary to help users to find the subset of data that best satisfies their preference criteria. Quantitative approaches such as top-k queries may not be the most appropriate approaches as they require the user to assign weights that may not be known beforehand to a scoring function. Unlike the quantitative approach, under the qualitative approach, which includes the well-known skyline, preference criteria are more intuitive in certain cases and can be expressed more naturally. In this paper, we address the problem of evaluating SPARQL qualitative preference queries over an Ontology-Based Data Access (OBDA) approach, which provides uniform access over multiple and heterogeneous data sources. Our main contribution is Morph-Skyline++, a framework for processing SPARQL qualitative preferences by directly querying relational databases. Our framework implements a technique that translates SPARQL qualitative preference queries directly into queries that can be evaluated by a relational database management system. We evaluate our approach over different scenarios, reporting the effects of data distribution, data size, and query complexity on the performance of our proposed technique in comparison with state-of-the-art techniques. Obtained results suggest that the execution time can be reduced by up to two orders of magnitude in comparison to current techniques scaling up to larger datasets while identifying precisely the result set.},
  archive      = {J_SW},
  author       = {Goncalves, Marlene and Chaves-Fraga, David and Corcho, Oscar},
  doi          = {10.3233/SW-212895},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {659-682},
  shortjournal = {Semantic Web},
  title        = {Handling qualitative preferences in SPARQL over virtual ontology-based data access},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust query processing for linked data fragments.
<em>SW</em>, <em>13</em>(4), 623–657. (<a
href="https://doi.org/10.3233/SW-212888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked Data Fragments (LDFs) refer to interfaces that allow for publishing and querying Knowledge Graphs on the Web. These interfaces primarily differ in their expressivity and allow for exploring different trade-offs when balancing the workload between clients and servers in decentralized SPARQL query processing. To devise efficient query plans, clients typically rely on heuristics that leverage the metadata provided by the LDF interface, since obtaining fine-grained statistics from remote sources is a challenging task. However, these heuristics are prone to potential estimation errors based on the metadata which can lead to inefficient query executions with a high number of requests, large amounts of data transferred, and, consequently, excessive execution times. In this work, we investigate robust query processing techniques for Linked Data Fragment clients to address these challenges. We first focus on robust plan selection by proposing CROP, a query plan optimizer that explores the cost and robustness of alternative query plans. Then, we address robust query execution by proposing a new class of adaptive operators: Polymorphic Join Operators. These operators adapt their join strategy in response to possible cardinality estimation errors. The results of our first experimental study show that CROP outperforms state-of-the-art clients by exploring alternative plans based on their cost and robustness. In our second experimental study, we investigate how different planning approaches can benefit from polymorphic join operators and find that they enable more efficient query execution in the majority of cases.},
  archive      = {J_SW},
  author       = {Heling, Lars and Acosta, Maribel},
  doi          = {10.3233/SW-212888},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {623-657},
  shortjournal = {Semantic Web},
  title        = {Robust query processing for linked data fragments},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous multi-query optimization for subgraph matching
over dynamic graphs. <em>SW</em>, <em>13</em>(4), 601–622. (<a
href="https://doi.org/10.3233/SW-212864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing need to perform real-time analytics on dynamic graphs in order to deliver the values of big data to users. An important problem from such applications is continuously identifying and monitoring critical patterns when fine-grained updates at a high velocity occur on the graphs. A lot of efforts have been made to develop practical solutions for these problems. Despite the efforts, existing algorithms showed limited running time and scalability in dealing with large and/or many graphs. In this paper, we study the problem of continuous multi-query optimization for subgraph matching over dynamic graph data. (1) We propose annotated query graph , which is obtained by merging the multi-queries into one. (2) Based on the annotated query, we employ a concise auxiliary data structure to represent partial solutions in a compact form. (3) In addition, we propose an efficient maintenance strategy to detect the affected queries for each update and report corresponding matches in one pass. (4) Extensive experiments over real-life and synthetic datasets verify the effectiveness and efficiency of our approach and confirm a two orders of magnitude improvement of the proposed solution.},
  archive      = {J_SW},
  author       = {Wang, Xi and Zhang, Qianzhen and Guo, Deke and Zhao, Xiang},
  doi          = {10.3233/SW-212864},
  journal      = {Semantic Web},
  month        = {5},
  number       = {4},
  pages        = {601-622},
  shortjournal = {Semantic Web},
  title        = {Continuous multi-query optimization for subgraph matching over dynamic graphs},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tab2KG: Semantic table interpretation with lightweight
semantic profiles. <em>SW</em>, <em>13</em>(3), 571–597. (<a
href="https://doi.org/10.3233/SW-222993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular data plays an essential role in many data analytics and machine learning tasks. Typically, tabular data does not possess any machine-readable semantics. In this context, semantic table interpretation is crucial for making data analytics workflows more robust and explainable. This article proposes Tab2KG – a novel method that targets at the interpretation of tables with previously unseen data and automatically infers their semantics to transform them into semantic data graphs. We introduce original lightweight semantic profiles that enrich a domain ontology’s concepts and relations and represent domain and table characteristics. We propose a one-shot learning approach that relies on these profiles to map a tabular dataset containing previously unseen instances to a domain ontology. In contrast to the existing semantic table interpretation approaches, Tab2KG relies on the semantic profiles only and does not require any instance lookup. This property makes Tab2KG particularly suitable in the data analytics context, in which data tables typically contain new instances. Our experimental evaluation on several real-world datasets from different application domains demonstrates that Tab2KG outperforms state-of-the-art semantic table interpretation baselines.},
  archive      = {J_SW},
  author       = {Gottschalk, Simon and Demidova, Elena},
  doi          = {10.3233/SW-222993},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {571-597},
  shortjournal = {Semantic Web},
  title        = {Tab2KG: Semantic table interpretation with lightweight semantic profiles},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural entity linking: A survey of models based on deep
learning. <em>SW</em>, <em>13</em>(3), 527–570. (<a
href="https://doi.org/10.3233/SW-222986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey presents a comprehensive description of recent neural entity linking (EL) systems developed since 2015 as a result of the “deep learning revolution” in natural language processing. Its goal is to systemize design features of neural entity linking systems and compare their performance to the remarkable classic methods on common benchmarks. This work distills a generic architecture of a neural EL system and discusses its components, such as candidate generation, mention-context encoding, and entity ranking, summarizing prominent methods for each of them. The vast variety of modifications of this general architecture are grouped by several common themes: joint entity mention detection and disambiguation, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of entity and mention/context embeddings to represent their meaning, this work also overviews prominent entity embedding techniques. Finally, the survey touches on applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models based on the Transformer architecture.},
  archive      = {J_SW},
  author       = {Sevgili, Özge and Shelmanov, Artem and Arkhipov, Mikhail and Panchenko, Alexander and Biemann, Chris},
  doi          = {10.3233/SW-222986},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {527-570},
  shortjournal = {Semantic Web},
  title        = {Neural entity linking: A&amp;nbsp;survey of models based on deep learning},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network representation learning method embedding linear and
nonlinear network structures. <em>SW</em>, <em>13</em>(3), 511–526. (<a
href="https://doi.org/10.3233/SW-212968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of neural networks, much attention has been focused on network embedding for complex network data, which aims to learn low-dimensional embedding of nodes in the network and how to effectively apply learned network representations to various graph-based analytical tasks. Two typical models exist namely the shallow random walk network representation method and deep learning models such as graph convolution networks (GCNs). The former one can be used to capture the linear structure of the network using depth-first search (DFS) and width-first search (BFS), whereas Hierarchical GCN (HGCN) is an unsupervised graph embedding that can be used to describe the global nonlinear structure of the network via aggregating node information. However, the two existing kinds of models cannot simultaneously capture the nonlinear and linear structure information of nodes. Thus, the nodal characteristics of nonlinear and linear structures are explored in this paper, and an unsupervised representation method based on HGCN that joins learning of shallow and deep models is proposed. Experiments on node classification and dimension reduction visualization are carried out on citation, language, and traffic networks. The results show that, compared with the existing shallow network representation model and deep network model, the proposed model achieves better performances in terms of micro-F1, macro-F1 and accuracy scores.},
  archive      = {J_SW},
  author       = {Zhang, Hu and Zhou, Jingjing and Li, Ru and Fan, Yue},
  doi          = {10.3233/SW-212968},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {511-526},
  shortjournal = {Semantic Web},
  title        = {Network representation learning method embedding linear and nonlinear network structures},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on visual transfer learning using knowledge graphs.
<em>SW</em>, <em>13</em>(3), 477–510. (<a
href="https://doi.org/10.3233/SW-212959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information perceived via visual observations of real-world phenomena is unstructured and complex. Computer vision (CV) is the field of research that attempts to make use of that information. Recent approaches of CV utilize deep learning (DL) methods as they perform quite well if training and testing domains follow the same underlying data distribution. However, it has been shown that minor variations in the images that occur when these methods are used in the real world can lead to unpredictable and catastrophic errors. Transfer learning is the area of machine learning that tries to prevent these errors. Especially, approaches that augment image data using auxiliary knowledge encoded in language embeddings or knowledge graphs (KGs) have achieved promising results in recent years. This survey focuses on visual transfer learning approaches using KGs, as we believe that KGs are well suited to store and represent any kind of auxiliary knowledge. KGs can represent auxiliary knowledge either in an underlying graph-structured schema or in a vector-based knowledge graph embedding . Intending to enable the reader to solve visual transfer learning problems with the help of specific KG-DL configurations we start with a description of relevant modeling structures of a KG of various expressions, such as directed labeled graphs, hypergraphs, and hyper-relational graphs. We explain the notion of feature extractor, while specifically referring to visual and semantic features. We provide a broad overview of knowledge graph embedding methods and describe several joint training objectives suitable to combine them with high dimensional visual embeddings. The main section introduces four different categories on how a KG can be combined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge Graph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as a Peer. To help researchers find meaningful evaluation benchmarks, we provide an overview of generic KGs and a set of image processing datasets and benchmarks that include various types of auxiliary knowledge. Last, we summarize related surveys and give an outlook about challenges and open issues for future research.},
  archive      = {J_SW},
  author       = {Monka, Sebastian and Halilaj, Lavdim and Rettinger, Achim},
  doi          = {10.3233/SW-212959},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {477-510},
  shortjournal = {Semantic Web},
  title        = {A survey on visual transfer learning using knowledge graphs},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Taxonomy enrichment with text and graph vector
representations. <em>SW</em>, <em>13</em>(3), 441–475. (<a
href="https://doi.org/10.3233/SW-212955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs such as DBpedia, Freebase or Wikidata always contain a taxonomic backbone that allows the arrangement and structuring of various concepts in accordance with hypo-hypernym (“class-subclass”) relationship. With the rapid growth of lexical resources for specific domains, the problem of automatic extension of the existing knowledge bases with new words is becoming more and more widespread. In this paper, we address the problem of taxonomy enrichment which aims at adding new words to the existing taxonomy. We present a new method which allows achieving high results on this task with little effort. It uses the resources which exist for the majority of languages, making the method universal. We extend our method by incorporating deep representations of graph structures like node2vec, Poincaré embeddings, GCN etc. that have recently demonstrated promising results on various NLP tasks. Furthermore, combining these representations with word embeddings allows us to beat the state of the art. We conduct a comprehensive study of the existing approaches to taxonomy enrichment based on word and graph vector representations and their fusion approaches. We also explore the ways of using deep learning architectures to extend taxonomic backbones of knowledge graphs. We create a number of datasets for taxonomy extension for English and Russian. We achieve state-of-the-art results across different datasets and provide an in-depth error analysis of mistakes.},
  archive      = {J_SW},
  author       = {Nikishina, Irina and Tikhomirov, Mikhail and Logacheva, Varvara and Nazarov, Yuriy and Panchenko, Alexander and Loukachevitch, Natalia},
  doi          = {10.3233/SW-212955},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {441-475},
  shortjournal = {Semantic Web},
  title        = {Taxonomy enrichment with text and graph vector representations},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing the generalizability of the network-based topic
emergence identification method. <em>SW</em>, <em>13</em>(3), 423–439.
(<a href="https://doi.org/10.3233/SW-212951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic evolution helps the understanding of current research topics and their histories by automatically modeling and detecting the set of shared research fields in academic publications as topics. This paper provides a generalized analysis of the topic evolution method for predicting the emergence of new topics, which can operate on any dataset where the topics are defined as the relationships of their neighborhoods in the past by extrapolating to the future topics. Twenty sample topic networks were built with various fields-of-study keywords as seeds, covering domains such as business, materials, diseases, and computer science from the Microsoft Academic Graph dataset. The binary classifier was trained for each topic network using 15 structural features of emerging and existing topics and consistently resulted in accuracy and F1 over 0.91 for all twenty datasets over the periods of 2000 to 2019. Feature selection showed that the models retained most of the performance with only one-third of the tested features. Incremental learning was tested within the same topic over time and between different topics, which resulted in slight performance improvements in both cases. This indicates there is an underlying pattern to the neighbors of new topics common to research domains, likely beyond the sample topics used in the experiment. The result showed that network-based new topic prediction can be applied to various research domains with different research patterns.},
  archive      = {J_SW},
  author       = {Jung, Sukhwan and Segev, Aviv},
  doi          = {10.3233/SW-212951},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {423-439},
  shortjournal = {Semantic Web},
  title        = {Analyzing the generalizability of the network-based topic emergence identification method},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge graph embedding for data mining vs. Knowledge
graph embedding for link prediction – two sides of the same coin?
<em>SW</em>, <em>13</em>(3), 399–422. (<a
href="https://doi.org/10.3233/SW-212892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1) providing an encoding for data mining tasks, and (2) predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.},
  archive      = {J_SW},
  author       = {Portisch, Jan and Heist, Nicolas and Paulheim, Heiko},
  doi          = {10.3233/SW-212892},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {399-422},
  shortjournal = {Semantic Web},
  title        = {Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction&amp;nbsp;– two sides of the same coin?},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering alignment relations with graph convolutional
networks: A biomedical case study. <em>SW</em>, <em>13</em>(3), 379–398.
(<a href="https://doi.org/10.3233/SW-210452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs are freely aggregated, published, and edited in the Web of data, and thus may overlap. Hence, a key task resides in aligning (or matching) their content. This task encompasses the identification, within an aggregated knowledge graph, of nodes that are equivalent, more specific, or weakly related. In this article, we propose to match nodes within a knowledge graph by (i) learning node embeddings with Graph Convolutional Networks such that similar nodes have low distances in the embedding space, and (ii) clustering nodes based on their embeddings, in order to suggest alignment relations between nodes of a same cluster. We conducted experiments with this approach on the real world application of aligning knowledge in the field of pharmacogenomics, which motivated our study. We particularly investigated the interplay between domain knowledge and GCN models with the two following focuses. First, we applied inference rules associated with domain knowledge, independently or combined, before learning node embeddings, and we measured the improvements in matching results. Second, while our GCN model is agnostic to the exact alignment relations ( e.g. , equivalence, weak similarity), we observed that distances in the embedding space are coherent with the “strength” of these different relations ( e.g. , smaller distances for equivalences), letting us considering clustering and distances in the embedding space as a means to suggest alignment relations in our case study.},
  archive      = {J_SW},
  author       = {Monnin, Pierre and Raïssi, Chedy and Napoli, Amedeo and Coulet, Adrien},
  doi          = {10.3233/SW-210452},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {379-398},
  shortjournal = {Semantic Web},
  title        = {Discovering alignment relations with graph convolutional networks: A biomedical case study},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIDI2vec: Learning MIDI embeddings for reliable prediction
of symbolic music metadata. <em>SW</em>, <em>13</em>(3), 357–377. (<a
href="https://doi.org/10.3233/SW-210446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale. Recently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good comparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music, avoiding feature engineering and producing highly scalable and reusable models with low dimensionality. Our proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.},
  archive      = {J_SW},
  author       = {Lisena, Pasquale and Meroño-Peñuela, Albert and Troncy, Raphaël},
  doi          = {10.3233/SW-210446},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {357-377},
  shortjournal = {Semantic Web},
  title        = {MIDI2vec: Learning MIDI embeddings for reliable prediction of symbolic music metadata},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Answer selection in community question answering exploiting
knowledge graph and context information. <em>SW</em>, <em>13</em>(3),
339–356. (<a href="https://doi.org/10.3233/SW-222970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of knowledge graph (KG), many applications such as sentiment analysis, trend prediction, and question answering use KG for better performance. Despite the obvious usefulness of commonsense and factual information in the KGs, to the best of our knowledge, KGs have been rarely integrated into the task of answer selection in community question answering (CQA). In this paper, we propose a novel answer selection method in CQA by using the knowledge embedded in KGs. We also learn a latent-variable model for learning the representations of the question and answer, jointly optimizing generative and discriminative objectives. It also uses the question category for producing context-aware representations for questions and answers. Moreover, the model uses variational autoencoders (VAE) in a multi-task learning process with a classifier to produce class-specific representations for answers. The experimental results on three widely used datasets demonstrate that our proposed method is effective and outperforms the existing baselines significantly.},
  archive      = {J_SW},
  author       = {Boroujeni, Golshan Afzali and Faili, Heshaam and Yaghoobzadeh, Yadollah},
  doi          = {10.3233/SW-222970},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {339-356},
  shortjournal = {Semantic Web},
  title        = {Answer selection in community question answering exploiting knowledge graph and context information},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of adverse biological effects of chemicals using
knowledge graph embeddings. <em>SW</em>, <em>13</em>(3), 299–338. (<a
href="https://doi.org/10.3233/SW-222804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have created a knowledge graph based on major data sources used in ecotoxicological risk assessment. We have applied this knowledge graph to an important task in risk assessment, namely chemical effect prediction. We have evaluated nine knowledge graph embedding models from a selection of geometric, decomposition, and convolutional models on this prediction task. We show that using knowledge graph embeddings can increase the accuracy of effect prediction with neural networks. Furthermore, we have implemented a fine-tuning architecture which adapts the knowledge graph embeddings to the effect prediction task and leads to a better performance. Finally, we evaluate certain characteristics of the knowledge graph embedding models to shed light on the individual model performance.},
  archive      = {J_SW},
  author       = {Myklebust, Erik B. and Jiménez-Ruiz, Ernesto and Chen, Jiaoyan and Wolf, Raoul and Tollefsen, Knut Erik},
  doi          = {10.3233/SW-222804},
  journal      = {Semantic Web},
  month        = {4},
  number       = {3},
  pages        = {299-338},
  shortjournal = {Semantic Web},
  title        = {Prediction of adverse biological effects of chemicals using knowledge graph embeddings},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TheyBuyForYou platform and knowledge graph: Expanding
horizons in public procurement with open linked data. <em>SW</em>,
<em>13</em>(2), 265–291. (<a
href="https://doi.org/10.3233/SW-210442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public procurement is a large market affecting almost every organisation and individual; therefore, governments need to ensure its efficiency, transparency, and accountability, while creating healthy, competitive, and vibrant economies. In this context, open data initiatives and integration of data from multiple sources across national borders could transform the procurement market by such as lowering the barriers of entry for smaller suppliers and encouraging healthier competition, in particular by enabling cross-border bids. Increasingly more open data is published in the public sector; however, these are created and maintained in siloes and are not straightforward to reuse or maintain because of technical heterogeneity, lack of quality, insufficient metadata, or missing links to related domains. To this end, we developed an open linked data platform, called TheyBuyForYou, consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border, and cross-lingual procurement knowledge graph. We developed advanced tools and services on top of the knowledge graph for anomaly detection, cross-lingual document search, and data storytelling. This article describes the TheyBuyForYou platform and knowledge graph, reports their adoption by different stakeholders and challenges and experiences we went through while creating them, and demonstrates the usefulness of Semantic Web and Linked Data technologies for enhancing public procurement.},
  archive      = {J_SW},
  author       = {Soylu, Ahmet and Corcho, Oscar and Elvesæter, Brian and Badenes-Olmedo, Carlos and Blount, Tom and Yedro Martínez, Francisco and Kovacic, Matej and Posinkovic, Matej and Makgill, Ian and Taggart, Chris and Simperl, Elena and Lech, Till C. and Roman, Dumitru},
  doi          = {10.3233/SW-210442},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {265-291},
  shortjournal = {Semantic Web},
  title        = {TheyBuyForYou platform and knowledge graph: Expanding horizons in public procurement with open linked data},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representing COVID-19 information in collaborative knowledge
graphs: The case of wikidata. <em>SW</em>, <em>13</em>(2), 233–264. (<a
href="https://doi.org/10.3233/SW-210444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information related to the COVID-19 pandemic ranges from biological to bibliographic, from geographical to genetic and beyond. The structure of the raw data is highly complex, so converting it to meaningful insight requires data curation, integration, extraction and visualization, the global crowdsourcing of which provides both additional challenges and opportunities. Wikidata is an interdisciplinary, multilingual, open collaborative knowledge base of more than 90 million entities connected by well over a billion relationships. It acts as a web-scale platform for broader computer-supported cooperative work and linked open data, since it can be written to and queried in multiple ways in near real time by specialists, automated tools and the public. The main query language, SPARQL, is a semantic language used to retrieve and process information from databases saved in Resource Description Framework (RDF) format. Here, we introduce four aspects of Wikidata that enable it to serve as a knowledge base for general information on the COVID-19 pandemic: its flexible data model, its multilingual features, its alignment to multiple external databases, and its multidisciplinary organization. The rich knowledge graph created for COVID-19 in Wikidata can be visualized, explored, and analyzed for purposes like decision support as well as educational and scholarly research.},
  archive      = {J_SW},
  author       = {Turki, Houcemeddine and Hadj Taieb, Mohamed Ali and Shafee, Thomas and Lubiana, Tiago and Jemielniak, Dariusz and Aouicha, Mohamed Ben and Labra Gayo, Jose Emilio and Youngstrom, Eric A. and Banat, Mus’ab and Das, Diptanshu and Mietchen, Daniel and on behalf of WikiProject COVID-19,},
  doi          = {10.3233/SW-210444},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {233-264},
  shortjournal = {Semantic Web},
  title        = {Representing COVID-19 information in collaborative knowledge graphs: The case of wikidata},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MQALD: Evaluating the impact of modifiers in question
answering over knowledge graphs. <em>SW</em>, <em>13</em>(2), 215–231.
(<a href="https://doi.org/10.3233/SW-210440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question Answering (QA) over Knowledge Graphs (KG) aims to develop a system that is capable of answering users’ questions using the information coming from one or multiple Knowledge Graphs, like DBpedia, Wikidata, and so on. Question Answering systems need to translate the user’s question, written using natural language, into a query formulated through a specific data query language that is compliant with the underlying KG. This translation process is already non-trivial when trying to answer simple questions that involve a single triple pattern. It becomes even more troublesome when trying to cope with questions that require modifiers in the final query, i.e., aggregate functions, query forms, and so on. The attention over this last aspect is growing but has never been thoroughly addressed by the existing literature. Starting from the latest advances in this field, we want to further step in this direction. This work aims to provide a publicly available dataset designed for evaluating the performance of a QA system in translating articulated questions into a specific data query language. This dataset has also been used to evaluate three QA systems available at the state of the art.},
  archive      = {J_SW},
  author       = {Siciliani, Lucia and Basile, Pierpaolo and Lops, Pasquale and Semeraro, Giovanni},
  doi          = {10.3233/SW-210440},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {215-231},
  shortjournal = {Semantic Web},
  title        = {MQALD: Evaluating the impact of modifiers in question answering over knowledge graphs},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Creating RESTful APIs over SPARQL endpoints using RAMOSE.
<em>SW</em>, <em>13</em>(2), 195–213. (<a
href="https://doi.org/10.3233/SW-210439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic Web technologies are widely used for storing RDF data and making them available on the Web through SPARQL endpoints, queryable using the SPARQL query language. While the use of SPARQL endpoints is strongly supported by Semantic Web experts, it hinders broader use of RDF data by common Web users, engineers and developers unfamiliar with Semantic Web technologies, who normally rely on Web RESTful APIs for querying Web-available data and creating applications over them. To solve this problem, we have developed RAMOSE, a generic tool developed in Python to create REST APIs over SPARQL endpoints. Through the creation of source-specific textual configuration files, RAMOSE enables the querying of SPARQL endpoints via simple Web RESTful API calls that return either JSON or CSV-formatted data, thus hiding all the intrinsic complexities of SPARQL and RDF from common Web users. We provide evidence that the use of RAMOSE to provide REST API access to RDF data within OpenCitations triplestores is beneficial in terms of the number of queries made by external users of such RDF data using the RAMOSE API, compared with the direct access via the SPARQL endpoint. Our findings show the importance for suppliers of RDF data of having an alternative API access service, which enables its use by those with no (or little) experience in Semantic Web technologies and the SPARQL query language. RAMOSE can be used both to query any SPARQL endpoint and to query any other Web API, and thus it represents an easy generic technical solution for service providers who wish to create an API service to access Linked Data stored as RDF in a triplestore.},
  archive      = {J_SW},
  author       = {Daquino, Marilena and Heibi, Ivan and Peroni, Silvio and Shotton, David},
  doi          = {10.3233/SW-210439},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {195-213},
  shortjournal = {Semantic Web},
  title        = {Creating RESTful APIs over SPARQL endpoints using RAMOSE},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using natural language generation to bootstrap missing
wikipedia articles: A human-centric perspective. <em>SW</em>,
<em>13</em>(2), 163–194. (<a
href="https://doi.org/10.3233/SW-210431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays natural language generation (NLG) is used in everything from news reporting and chatbots to social media management. Recent advances in machine learning have made it possible to train NLG systems that seek to achieve human-level performance in text writing and summarisation. In this paper, we propose such a system in the context of Wikipedia and evaluate it with Wikipedia readers and editors. Our solution builds upon the ArticlePlaceholder, a tool used in 14 under-resourced Wikipedia language versions, which displays structured data from the Wikidata knowledge base on empty Wikipedia pages. We train a neural network to generate an introductory sentence from the Wikidata triples shown by the ArticlePlaceholder, and explore how Wikipedia users engage with it. The evaluation, which includes an automatic, a judgement-based, and a task-based component, shows that the summary sentences score well in terms of perceived fluency and appropriateness for Wikipedia, and can help editors bootstrap new articles. It also hints at several potential implications of using NLG solutions in Wikipedia at large, including content quality, trust in technology, and algorithmic transparency.},
  archive      = {J_SW},
  author       = {Kaffee, Lucie-Aimée and Vougiouklis, Pavlos and Simperl, Elena},
  doi          = {10.3233/SW-210431},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {163-194},
  shortjournal = {Semantic Web},
  title        = {Using natural language generation to bootstrap missing wikipedia articles: A&amp;nbsp;human-centric perspective},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pioneering easy-to-use forestry data with forest explorer.
<em>SW</em>, <em>13</em>(2), 147–162. (<a
href="https://doi.org/10.3233/SW-210430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forest Explorer is a web tool that can be used to easily browse the contents of the Cross-Forest dataset, a Linked Open Data resource containing the forestry inventory and land cover map from Spain. The tool is purposed for domain experts and lay users to facilitate the exploration of forestry data. Since these two groups are not knowledgable on Semantic Web, the user interface is designed to hide the complexity of RDF, OWL or SPARQL. An interactive map is provided for this purpose, allowing users to navigate to the area of interest and presenting forestry data with different levels of detail according to the zoom level. Forest Explorer offers different filter controls and is localized to English and Spanish. All the data is retrieved from the Cross-Forest and DBpedia endpoints through the Data manager . This component feeds the different Feature managers with the data needed to be displayed in the map. The Data manager uses a reduced set of SPARQL templates to accommodate any data request of the Feature managers . Caching and smart geographic querying are employed to limit data exchanges with the endpoint. A live version of the tool is freely available for everybody that wants to try it – any device with a modern browser should be sufficient to test it. Since December 2019, more than 3,200 users have employed Forest Explorer and it has appeared 12 times in the Spanish media. Results from a user study with 28 participants (mainly domain experts) show that Forest Explorer can be used to easily navigate the contents of the Cross-Forest dataset. No important limitations were found, only feature requests such as the integration of new datasets from other countries that are part of our future work.},
  archive      = {J_SW},
  author       = {Vega-Gorgojo, Guillermo and Giménez-García, José M. and Ordóñez, Cristóbal and Bravo, Felipe},
  doi          = {10.3233/SW-210430},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {147-162},
  shortjournal = {Semantic Web},
  title        = {Pioneering easy-to-use forestry data with forest explorer},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observational/hydrographic data of the south atlantic ocean
published as LOD. <em>SW</em>, <em>13</em>(2), 133–145. (<a
href="https://doi.org/10.3233/SW-210426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes the publication of occurrences of Southern Elephant Seals Mirounga leonina (Linnaeus, 1758) as Linked Open Data in two environments (marine and coastal). The data constitutes hydrographic measurements of instrumented animals and observation data collected during censuses between 1990 and 2017. The data scheme is based on the previously developed ontology BiGe-Onto and the new version of the Semantic Sensor Network ontology (SSN) . We introduce the network of ontologies used to organize the data and the transformation process to publish the dataset. In the use case, we develop an application to access and analyze the dataset. The linked open dataset and the related visualization tool turned data into a resource that can be located by the international community and thus increase the commitment to its sustainability. The data, coming from Península Valdés (UNESCO World Heritage), is available for interdisciplinary studies of management and conservation of marine and coastal protected areas which demand reliable and updated data.},
  archive      = {J_SW},
  author       = {Zárate, Marcos and Braun, Germán and Lewis, Mirtha and Fillottrani, Pablo},
  doi          = {10.3233/SW-210426},
  journal      = {Semantic Web},
  month        = {2},
  number       = {2},
  pages        = {133-145},
  shortjournal = {Semantic Web},
  title        = {Observational/hydrographic data of the south atlantic ocean published as LOD},
  volume       = {13},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
