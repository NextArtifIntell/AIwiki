<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda---167">CSDA - 167</h2>
<ul>
<li><details>
<summary>
(2022). High-dimensional robust regression with lq-loss functions.
<em>CSDA</em>, <em>176</em>, 107567. (<a
href="https://doi.org/10.1016/j.csda.2022.107567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust procedures in high-dimensional regression are important because outliers are often present in data. For data with heavy-tailed errors, quantile regression and least absolute deviation regression methods have been widely used with great success. Some interesting Huber-loss-based and robust M-type regularized estimators have also been developed. However, high-dimensional regression estimation under L q Lq -loss functions ( 1 ≤ q (1≤q&amp;lt;2) has not been fully studied in the literature. A lack of smoothness of these loss functions near the origin makes the regularized optimization problems computationally challenging. Robust sparse regression estimation under the L q Lq -loss functions ( 1 ≤ q (1≤q&amp;lt;2) is investigated. A regularized estimator under the L q Lq -loss combined with a weighted penalty function is proposed and its properties, such as the model-selection oracle property and asymptotic normality , are studied. The l 1 l1 and l 2 l2 estimation error bounds of the proposed estimator are also obtained. A novel computational algorithm is also proposed. Monte Carlo studies are conducted to compare the finite-sample and robustness properties of the proposed procedure with some existing regularized robust methods. The methods are also compared using two real data examples. The numerical studies show the satisfactory finite-sample performance of our procedure.},
  archive      = {J_CSDA},
  author       = {Yibo Wang and Rohana J. Karunamuni},
  doi          = {10.1016/j.csda.2022.107567},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107567},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional robust regression with lq-loss functions},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agglomerative and divisive hierarchical bayesian clustering.
<em>CSDA</em>, <em>176</em>, 107566. (<a
href="https://doi.org/10.1016/j.csda.2022.107566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis methods are designed to discover groups of subjects or objects in datasets by uncovering latent patterns in data. Two model-based Bayesian hierarchical clustering algorithms are presented—divisive and agglomerative—that return nested clustering configurations and provide guidance on the plausible number of clusters in a principled way. These algorithms outperform many existing clustering methods on benchmark data. The methods are applied to identify subpopulations among Parkinson&#39;s disease subjects using only baseline data, and differing patterns of progression in the few years following diagnosis are demonstrated in the identified clusters.},
  archive      = {J_CSDA},
  author       = {Elliot Burghardt and Daniel Sewell and Joseph Cavanaugh},
  doi          = {10.1016/j.csda.2022.107566},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107566},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Agglomerative and divisive hierarchical bayesian clustering},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional non-parametric latent block model: A multivariate
time series clustering approach for autonomous driving validation.
<em>CSDA</em>, <em>176</em>, 107565. (<a
href="https://doi.org/10.1016/j.csda.2022.107565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced driving-assistance systems validation remains one of the biggest challenges car manufacturers must tackle to provide safe driverless cars. The reliable validation of these systems requires to assess their reaction&#39;s quality and consistency to a broad spectrum of driving scenarios. In this context, large-scale simulation systems bypass the physical “on-tracks” limitations and produce important quantities of high-dimensional time series data . The challenge is to find valuable information in these multivariate unlabeled datasets that may contain noisy, sometimes correlated or non-informative variables. A new model-based tool is proposed for multivariate time series clustering based on a Bayesian co-clustering approach. The tool discriminates groups of correlated temporal variables, while modeling noise and providing probabilistic confidence interval for outlier detection . The proposed Functional Non-Parametric Latent Block Model (FunNPLBM) simultaneously creates a partition of observations and a partition of variables, using latent multivariate Gaussian block distributions. The model parameters follow a bi-dimensional Dirichlet Process as a prior for the block distribution parameters and for block proportions, and natively provides model selection. The method&#39;s capacities are illustrated with experiments and benchmarks on a simulated dataset and on an advanced driver-assistance system validation use-case.},
  archive      = {J_CSDA},
  author       = {Etienne Goffinet and Mustapha Lebbah and Hanane Azzag and Giraldi Loïc and Anthony Coutant},
  doi          = {10.1016/j.csda.2022.107565},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107565},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional non-parametric latent block model: A multivariate time series clustering approach for autonomous driving validation},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust polytomous logistic regression. <em>CSDA</em>,
<em>176</em>, 107564. (<a
href="https://doi.org/10.1016/j.csda.2022.107564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of polytomous regression, as with any generalized linear model , robustness issues are well documented. Existing robust estimators are designed to protect against misclassification , but do not protect against outlying covariates . It is shown that this can have a much bigger impact on estimation and testing than misclassification alone. To address this problem, two new estimators are introduced: a robust generalized linear model-type estimator and an optimal B-robust estimator, together with the corresponding Wald-type and score-type tests. Asymptotic distributions and variances of these estimators are provided as well as the asymptotic distributions of the test statistics under the null hypothesis. A complete comparison of the proposed new estimators and existing alternatives is presented. This is performed theoretically by studying the influence functions of the estimators, and empirically through simulations and applications to a medical dataset.},
  archive      = {J_CSDA},
  author       = {Julien Miron and Benjamin Poilane and Eva Cantoni},
  doi          = {10.1016/j.csda.2022.107564},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107564},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust polytomous logistic regression},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extremal quantile autoregression for heavy-tailed time
series. <em>CSDA</em>, <em>176</em>, 107563. (<a
href="https://doi.org/10.1016/j.csda.2022.107563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional quantile estimator of the extreme conditional quantiles under quantile autoregression models is relatively unstable due to data sparsity in the tails. Extreme value theory provides a mathematical foundation of extrapolation to estimate extreme quantiles. However, the asymptotic distributions of existing estimators are often complicated making it inconvenient to apply for statistical inference . We develop a new adaptive estimation procedure based on a generalized Hill estimator of the extreme value index to estimate extreme conditional quantiles in autoregression models. We establish the asymptotic normality of the proposed estimators under some regularity conditions by applying the martingale central limit theorem . The asymptotic variances have closed expressions and can be directly estimated. Based on these results, we propose a procedure to construct confidence intervals for the extreme conditional quantiles. We also provide a diagnostic tool to check the heavy-tailedness of the distribution. Simulation studies and a real data analysis are carried out to demonstrate the advantages of the proposed methods over existing approaches.},
  archive      = {J_CSDA},
  author       = {Fengyang He and Huixia Judy Wang and Yuejin Zhou},
  doi          = {10.1016/j.csda.2022.107563},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107563},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Extremal quantile autoregression for heavy-tailed time series},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On cross-distance selection algorithm for hybrid sufficient
dimension reduction. <em>CSDA</em>, <em>176</em>, 107562. (<a
href="https://doi.org/10.1016/j.csda.2022.107562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the extensive development of a variety of sufficient dimension reduction (SDR) methodologies, Ye and Weiss (2003) proposed a hybrid SDR method combining two pre-existing SDR methods. In particular, they used a bootstrap approach to select a proper weight. Since bootstrapping is computationally intensive and time-consuming, the hybrid reduction approach has not been widely used, although it is more accurate than conventional single SDR methods. To overcome these deficits, we propose a novel cross-distance selection algorithm . Similar to the bootstrapping method, the proposed selection algorithm is data-driven and has a strong rationale for its performance. The numerical studies demonstrate that the chosen hybrid method from our proposed algorithm offers a good estimation quality and reduces the computing time dramatically at the same time. Furthermore, our real data analysis confirms that the proposed selection algorithm has potential advantages with its practical usefulness over the existing bootstrapping method.},
  archive      = {J_CSDA},
  author       = {Yujin Park and Kyongwon Kim and Jae Keun Yoo},
  doi          = {10.1016/j.csda.2022.107562},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107562},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On cross-distance selection algorithm for hybrid sufficient dimension reduction},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vine copula statistical disclosure control for mixed-type
data. <em>CSDA</em>, <em>176</em>, 107561. (<a
href="https://doi.org/10.1016/j.csda.2022.107561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new statistical disclosure control (SDC) method for mixed-type data based on vine copulas . The use of Gaussian and skew-t copulas has been demonstrated to be capable of incorporating information from the marginal distributions of mixed-type variables, whether they are discrete or continuous. In particular, our proposed SDC method using vine copulas generalizes a data perturbation method using an extended skew-t copula. Our vine-SDC method improves the SDC method using the extended skew-t copula by allowing the bivariate copulas in the vine decomposition to take various forms, thus offering a better fit for the joint distribution of the data and more flexibility in data perturbation. An additional advantage of our vine-SDC method is the significant improvement in computational efficiency compared with that using the extended skew-t copula. We discuss some statistical properties of vine copulas and the methodology of vine-SDC. A simulation and a study of real healthcare survey data are provided to explore the performance and strength of vine-SDC and compare it with a common copula-based SDC method.},
  archive      = {J_CSDA},
  author       = {Amanda M.Y. Chu and Chun Yin Ip and Benson S.Y. Lam and Mike K.P. So},
  doi          = {10.1016/j.csda.2022.107561},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107561},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Vine copula statistical disclosure control for mixed-type data},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparison of allocation strategies for optimising
clinical trial designs under variance heterogeneity. <em>CSDA</em>,
<em>176</em>, 107559. (<a
href="https://doi.org/10.1016/j.csda.2022.107559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balanced allocation of patients across treatments is a widely adopted and practical strategy in clinical trials. Under the assumption of variance homogeneity , a balanced allocation is known to possess desirable properties in terms of commonly considered operating characteristics such as power, type I error rate, and estimation accuracy. When this assumption is violated, the balanced allocation strategy can perform suboptimally in terms of these (and other) metrics of interest, compared to alternative allocation rules. Such allocation rules are examined under the assumption of response variance heterogeneity across treatments and within an adaptive framework. A blocked design is proposed in order to account for additional sources of variability which are incorporated into the proposed design through a mixed effects model . For this setting, two allocation strategies are derived: an efficiency-oriented and an outcome-oriented strategy. These target two different and potentially conflicting objectives (estimation accuracy and within-trial patient response, respectively) that may be optimised in the presence of variance heterogeneity. A comparison of the resulting allocation strategies is provided in the context of a clinical trial studying the effect of different treatment protocols on the level of inflammation caused by Rheumatoid Arthritis. The interrelation of common clinical trial objectives under the examined allocation strategies is explored, demonstrating the benefits but also the costs in terms of objectives that are not formally incorporated into the optimisation criterion in each case.},
  archive      = {J_CSDA},
  author       = {Lida Mavrogonatou and Yuxuan Sun and David S. Robertson and Sofía S. Villar},
  doi          = {10.1016/j.csda.2022.107559},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107559},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A comparison of allocation strategies for optimising clinical trial designs under variance heterogeneity},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative importance sampling with markov chain monte carlo
sampling in robust bayesian analysis. <em>CSDA</em>, <em>176</em>,
107558. (<a href="https://doi.org/10.1016/j.csda.2022.107558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference under a set of priors, called robust Bayesian analysis , allows for estimation of parameters within a model and quantification of epistemic uncertainty in quantities of interest by bounded (or imprecise) probability . Iterative importance sampling can be used to estimate bounds on the quantity of interest by optimizing over the set of priors. A method for iterative importance sampling when the robust Bayesian inference relies on Markov chain Monte Carlo (MCMC) sampling is proposed. To accommodate the MCMC sampling in iterative importance sampling, a new expression for the effective sample size of the importance sampling is derived, which accounts for the correlation in the MCMC samples. To illustrate the proposed method for robust Bayesian analysis, iterative importance sampling with MCMC sampling is applied to estimate the lower bound of the overall effect in a previously published meta-analysis with a random effects model. The performance of the method compared to a grid search method and under different degrees of prior-data conflict is also explored.},
  archive      = {J_CSDA},
  author       = {Ivette Raices Cruz and Johan Lindström and Matthias C.M. Troffaes and Ullrika Sahlin},
  doi          = {10.1016/j.csda.2022.107558},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107558},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Iterative importance sampling with markov chain monte carlo sampling in robust bayesian analysis},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series graphical lasso and sparse VAR estimation.
<em>CSDA</em>, <em>176</em>, 107557. (<a
href="https://doi.org/10.1016/j.csda.2022.107557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-stage sparse vector autoregression method is proposed. It relies on the more recent and powerful technique of time series graphical lasso to estimate sparse inverse spectral density matrices in the first stage, and its second stage refines non-zero entries of the AR coefficient matrices using a false discovery rate (FDR) procedure. Compared to a recent approach, the method has the advantage of avoiding the inversion of the spectral density matrix, but has to deal with optimization over Hermitian matrices with complex-valued entries. Such modifications significantly improve the computational time with a little loss in forecasting performance. The algorithmic and computational properties of the method have been studied and the performance of the two methods is compared using simulated and a real macro-economic dataset. The simulation results show that the proposed modification is preferred over the existing method when the goal is to learn the structure of the AR coefficient matrices while the latter outperforms the former when forecasting is the ultimate task.},
  archive      = {J_CSDA},
  author       = {Aramayis Dallakyan and Rakheon Kim and Mohsen Pourahmadi},
  doi          = {10.1016/j.csda.2022.107557},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107557},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Time series graphical lasso and sparse VAR estimation},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smooth LASSO estimator for the function-on-function linear
regression model. <em>CSDA</em>, <em>176</em>, 107556. (<a
href="https://doi.org/10.1016/j.csda.2022.107556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new estimator, named S-LASSO, is proposed for the coefficient function of the Function-on-Function linear regression model. The S-LASSO estimator is shown to be able to increase the interpretability of the model, by better locating regions where the coefficient function is zero, and to smoothly estimate non-zero values of the coefficient function. The sparsity of the estimator is ensured by a functional LASSO penalty , which pointwise shrinks toward zero the coefficient function, while the smoothness is provided by two roughness penalties that penalize the curvature of the final estimator. The resulting estimator is proved to be estimation and pointwise sign consistent. Via an extensive Monte Carlo simulation study, the estimation and predictive performance of the S-LASSO estimator are shown to be better than (or at worst comparable with) competing estimators already presented in the literature before. Practical advantages of the S-LASSO estimator are illustrated through the analysis of the Canadian weather , Swedish mortality and ship CO 2 emission data . The S-LASSO method is implemented in the R package slasso , openly available online on CRAN.},
  archive      = {J_CSDA},
  author       = {Fabio Centofanti and Matteo Fontana and Antonio Lepore and Simone Vantini},
  doi          = {10.1016/j.csda.2022.107556},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107556},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smooth LASSO estimator for the function-on-function linear regression model},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric bayesian modelling of longitudinally
integrated covariance functions on spheres. <em>CSDA</em>, <em>176</em>,
107555. (<a href="https://doi.org/10.1016/j.csda.2022.107555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking into account axial symmetry in the covariance function of a Gaussian random field is essential when the purpose is modelling data defined over a large portion of the sphere representing our planet. Axially symmetric covariance functions admit a convoluted spectral representation that makes modelling and inference difficult. This motivates the interest in devising alternative strategies to attain axial symmetry , an appealing option being longitudinal integration of isotropic random fields on the sphere. This paper provides a comprehensive theoretical framework to model longitudinal integration on spheres through a nonparametric Bayesian approach. Longitudinally integrated covariances are treated as random objects, where the randomness is implied by the randomised spectrum associated with the covariance function . After investigating the topological support induced by our construction, we give the posterior distribution a thorough inspection. A Bayesian nonparametric model for the analysis of data defined on the sphere is described and implemented, its performance investigated by means of the analysis of both simulated and real data sets .},
  archive      = {J_CSDA},
  author       = {Pier Giovanni Bissiri and Galatia Cleanthous and Xavier Emery and Bernardo Nipoti and Emilio Porcu},
  doi          = {10.1016/j.csda.2022.107555},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107555},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric bayesian modelling of longitudinally integrated covariance functions on spheres},
  volume       = {176},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A high dimensional dissimilarity measure. <em>CSDA</em>,
<em>175</em>, 107560. (<a
href="https://doi.org/10.1016/j.csda.2022.107560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new dissimilarity measure for high-dimensional, low sample size settings to compare high dimensional probability distributions is proposed. The asymptotic behavior of the new dissimilarity index is studied theoretically. Numerical experiments from high dimensional distributions exhibit the usefulness of the method. The eigenvalues of the matrix of dissimilarities for comparing two high dimensional samples are determined and shown to be related to the asymptotic value of the dissimilarity index . A dissimilarity visualization plot that is useful for detection of outliers and change points is proposed and utilized to find the change points in S&amp;P500 stock return data.},
  archive      = {J_CSDA},
  author       = {Reza Modarres},
  doi          = {10.1016/j.csda.2022.107560},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107560},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A high dimensional dissimilarity measure},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference of heterogeneous treatment effect
based on single-index model. <em>CSDA</em>, <em>175</em>, 107554. (<a
href="https://doi.org/10.1016/j.csda.2022.107554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous treatment effect (HTE) is estimated by using the semiparametric regression method . Firstly, a flexible semiparametric single-index model is considered by assuming the nonparametric link function and the interaction between treatment and covariates , and the index parameter vector and the unknown link function are estimated by using the rMAVE method. Then a HTE estimator can be obtained based on the estimators of index parameter vector and the link function. The consistency and asymptotic normality of the HTE estimator are established under some regularity conditions . Secondly, a hypothesis test is developed for the existence of HTE, and the bootstrap procedure is utilized to evaluate the null distribution of test statistic. Finally, simulation studies and a real data analysis are conducted to assess the performance of our proposed method.},
  archive      = {J_CSDA},
  author       = {Sanying Feng and Kaidi Kong and Yinfei Kong and Gaorong Li and Zhaoliang Wang},
  doi          = {10.1016/j.csda.2022.107554},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107554},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical inference of heterogeneous treatment effect based on single-index model},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical likelihood inference for longitudinal data with
covariate measurement errors: An application to the LEAN study.
<em>CSDA</em>, <em>175</em>, 107553. (<a
href="https://doi.org/10.1016/j.csda.2022.107553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement errors usually arise during the longitudinal data collection process. Ignoring the effects of measurement errors will lead to invalid estimates. The Lifestyle Education for Activity and Nutrition (LEAN) study was designed to assess the effectiveness of intervention for enhancing weight loss over nine months. The covariates systolic blood pressure (SBP) and diastolic blood pressure (DBP) were measured at baseline, month 4, and month 9. At each assessment time, there were two replicate measurements for SBP and DBP. The replicate measurement errors of SBP follow different distributions, as does DBP. To account for the distributional difference of replicate measurement errors, a new method for analyzing longitudinal data with replicate covariate measurement errors is developed based on the empirical likelihood method . The asymptotic properties of the proposed estimator are established under some regularity conditions . The confidence region for the parameters of interest can be constructed based on the chi-squared approximation without estimating the covariance matrix . Additionally, the proposed empirical likelihood estimator is asymptotically more efficient than the estimator of Lin et al. ( 2018 ). Extensive simulations demonstrate that the proposed method can eliminate the effects of measurement errors in the covariate and has a high estimation efficiency. The proposed method indicates the significant effect of the intervention on BMI in the LEAN study.},
  archive      = {J_CSDA},
  author       = {Yuexia Zhang and Guoyou Qin and Zhongyi Zhu and Jiajia Zhang},
  doi          = {10.1016/j.csda.2022.107553},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107553},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical likelihood inference for longitudinal data with covariate measurement errors: An application to the LEAN study},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bootstrap confidence intervals for multiple change points
based on moving sum procedures. <em>CSDA</em>, <em>175</em>, 107552. (<a
href="https://doi.org/10.1016/j.csda.2022.107552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of quantifying uncertainty about the locations of multiple change points by means of confidence intervals is addressed. The asymptotic distribution of the change point estimators obtained as the local maximisers of moving sum statistics is derived, where the limit distributions differ depending on whether the corresponding size of changes is local, i.e. tends to zero as the sample size increases, or fixed. A bootstrap procedure for confidence interval generation is proposed which adapts to the unknown magnitude of changes and guarantees asymptotic validity both for local and fixed changes. Simulation studies show good performance of the proposed bootstrap procedure, and some discussions about how it can be extended to serially dependent errors are provided.},
  archive      = {J_CSDA},
  author       = {Haeran Cho and Claudia Kirch},
  doi          = {10.1016/j.csda.2022.107552},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107552},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bootstrap confidence intervals for multiple change points based on moving sum procedures},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Copula link-based additive models for bivariate
time-to-event outcomes with general censoring scheme. <em>CSDA</em>,
<em>175</em>, 107550. (<a
href="https://doi.org/10.1016/j.csda.2022.107550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate survival outcomes arise frequently in applied studies where the occurrence of two events of interest are associated. Often the exact event times are unknown due to censoring which can manifest in various forms. A general and flexible copula regression model that can handle bivariate survival data subject to various censoring mechanisms, which include a mixture of uncensored, left-, right-, and interval-censored data, is proposed. The proposal permits to specify all model parameters as flexible functions of covariate effects, flexibly model the baseline survival functions by means of monotonic P-splines, characterise the marginals via transformations of the survival functions which yield, e.g., the proportional hazards and odds models as special cases, and model the dependence between events using a wide variety of copulae. The algorithm is based on a computationally efficient and stable penalised maximum likelihood estimation approach with integrated automatic multiple smoothing parameter selection. The proposed model is evaluated in a simulation study and illustrated using data from the Age-Related Eye Disease Study. The modelling framework has been incorporated in the newly-revised R package GJRM , hence allowing any user to fit the desired model(s) and produce easy-to-interpret numerical and visual summaries.},
  archive      = {J_CSDA},
  author       = {Danilo Petti and Alessia Eletti and Giampiero Marra and Rosalba Radice},
  doi          = {10.1016/j.csda.2022.107550},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107550},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Copula link-based additive models for bivariate time-to-event outcomes with general censoring scheme},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic normality of residual density estimator in
stationary and explosive autoregressive models. <em>CSDA</em>,
<em>175</em>, 107549. (<a
href="https://doi.org/10.1016/j.csda.2022.107549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The error density estimator in the first-order autoregressive model is considered based on α -mixing errors. Since the errors are not observed, the residual kernel density estimator is provided. The asymptotic normality of the residual estimator is obtained when the autoregressive model is a stationary process or an explosive process. Moreover, some simulations such as the fitted curves, mean integrated square errors and histograms are illustrated to the residual kernel estimator and residual histogram estimator. It is shown that the residual kernel estimator with smooth kernel is smoother than the residual histogram estimator.},
  archive      = {J_CSDA},
  author       = {Min Gao and Wenzhi Yang and Shipeng Wu and Wei Yu},
  doi          = {10.1016/j.csda.2022.107549},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107549},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Asymptotic normality of residual density estimator in stationary and explosive autoregressive models},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general monte carlo method for multivariate
goodness–of–fit testing applied to elliptical families. <em>CSDA</em>,
<em>175</em>, 107548. (<a
href="https://doi.org/10.1016/j.csda.2022.107548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general and relatively simple method for construction of multivariate goodness–of–fit tests is introduced. The proposed test is applied to elliptical distributions . The method is based on a characterization of probability distributions via their characteristic function. The consistency and other limit properties of the new test statistics are studied. Also in a simulation study the proposed tests are compared with earlier as well as more recent competitors.},
  archive      = {J_CSDA},
  author       = {Feifei Chen and M. Dolores Jiménez–Gamero and Simos Meintanis and Lixing Zhu},
  doi          = {10.1016/j.csda.2022.107548},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107548},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A general monte carlo method for multivariate goodness–of–fit testing applied to elliptical families},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SJIVE: Supervised joint and individual variation explained.
<em>CSDA</em>, <em>175</em>, 107547. (<a
href="https://doi.org/10.1016/j.csda.2022.107547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing multi-source data, which are multiple views of data on the same subjects, has become increasingly common in molecular biomedical research. Recent methods have sought to uncover underlying structure and relationships within and/or between the data sources, and other methods have sought to build a predictive model for an outcome using all sources. However, existing methods that do both are presently limited because they either (1) only consider data structure shared by all datasets while ignoring structures unique to each source, or (2) they extract underlying structures first without consideration to the outcome. The proposed method, supervised joint and individual variation explained (sJIVE), can simultaneously (1) identify shared (joint) and source-specific (individual) underlying structure and (2) build a linear prediction model for an outcome using these structures. These two components are weighted to compromise between explaining variation in the multi-source data and in the outcome. Simulations show sJIVE to outperform existing methods when large amounts of noise are present in the multi-source data. An application to data from the COPDGene study explores gene expression and proteomic patterns associated with lung function.},
  archive      = {J_CSDA},
  author       = {Elise F. Palzer and Christine H. Wendt and Russell P. Bowler and Craig P. Hersh and Sandra E. Safo and Eric F. Lock},
  doi          = {10.1016/j.csda.2022.107547},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107547},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {SJIVE: Supervised joint and individual variation explained},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting times to event based on vine copula models.
<em>CSDA</em>, <em>175</em>, 107546. (<a
href="https://doi.org/10.1016/j.csda.2022.107546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistics , time-to-event analysis methods traditionally focus on the estimation of hazards. In recent years, machine learning methods have been proposed to directly predict the event times. A method based on vine copula models is proposed to make point and interval predictions for a right-censored response variable given mixed discrete-continuous explanatory variables . Extensive experiments on simulated and real datasets show that the proposed vine copula approach provides a decent approximation to other time-to-event analysis models including proportional hazards and Weibull Accelerate Failure Time models. When the proportional hazards or Weibull Accelerate Failure Time assumptions do not hold, predictions based on vine copulas can significantly outperform other models, depending on the shape of the conditional quantile functions. This shows the flexibility of the proposed vine copula approach for general time-to-event datasets.},
  archive      = {J_CSDA},
  author       = {Shenyi Pan and Harry Joe},
  doi          = {10.1016/j.csda.2022.107546},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107546},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Predicting times to event based on vine copula models},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified framework on defining depth for point process
using function smoothing. <em>CSDA</em>, <em>175</em>, 107545. (<a
href="https://doi.org/10.1016/j.csda.2022.107545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of statistical depth has been extensively studied in multivariate and functional data over the past few decades. In contrast, the depth on temporal point process is still under-explored. The problem is challenging because a point process has two types of randomness: 1) the number of events in a process, and 2) the distribution of these events. Recent studies proposed depths in a weighted product of two terms, describing the above two types of randomness, respectively. Under a new framework through a smoothing procedure, these two randomnesses can be unified. Basically, the point process observations are transformed into functions using conventional kernel smoothing methods, and then the well-known functional h -depth and its modified, center-based version are adopted to describe the center-outward rank in the original data. To do so, a proper metric is defined on the point processes with smoothed functions. Then an efficient algorithm is provided to estimate the defined “center”. The mathematical properties of the newly defined depths are explored and the asymptotic theories are studied. Simulation results show that the proposed depths can properly rank point process observations. Finally, the new methods are demonstrated in a classification task using a real neuronal spike train dataset.},
  archive      = {J_CSDA},
  author       = {Zishen Xu and Chenran Wang and Wei Wu},
  doi          = {10.1016/j.csda.2022.107545},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107545},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A unified framework on defining depth for point process using function smoothing},
  volume       = {175},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale local surrogate modeling of stochastic
simulation experiments. <em>CSDA</em>, <em>174</em>, 107537. (<a
href="https://doi.org/10.1016/j.csda.2022.107537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) surrogate modeling for large computer experiments is limited by cubic runtimes, especially with data from stochastic simulations with input-dependent noise. A popular workaround to reduce computational complexity involves local approximation (e.g., LAGP). However, LAGP has only been vetted in deterministic settings. A recent variation utilizing inducing points (LIGP) for additional sparsity improves upon LAGP on the speed-vs-accuracy frontier. The authors show that another benefit of LIGP over LAGP is that (local) nugget estimation for stochastic responses is more natural, especially when designs contain substantial replication as is common when attempting to separate signal from noise. Woodbury identities, extended in LIGP from inducing points to replicates, afford efficient computation in terms of unique design locations only. This increases the amount of local data (i.e., the neighborhood size) that may be incorporated without additional flops, thereby enhancing statistical efficiency. Performance of the authors&#39; LIGP upgrades is illustrated on benchmark data and real-world stochastic simulation experiments, including an options pricing control framework. Results indicate that LIGP provides more accurate prediction and uncertainty quantification for varying data dimension and replication strategies versus modern alternatives.},
  archive      = {J_CSDA},
  author       = {D. Austin Cole and Robert B. Gramacy and Mike Ludkovski},
  doi          = {10.1016/j.csda.2022.107537},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107537},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Large-scale local surrogate modeling of stochastic simulation experiments},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Independence index sufficient variable screening for
categorical responses. <em>CSDA</em>, <em>174</em>, 107530. (<a
href="https://doi.org/10.1016/j.csda.2022.107530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable screening is very popular in modern data analysis and is particularly useful for large p small n data. In this paper, a novel two-stage sufficient variable screening procedure, especially when the response is categorical is proposed. This procedure is very general and model-free, thus is robust against model mis-specification. In addition, the proposed procedure always improves existing screening approach in literature which only uses marginal relation. Asymptotic results and sure screening properties of the proposed methods are proved. Numerical studies and real data analysis are provided to demonstrate the advantages of the proposed method.},
  archive      = {J_CSDA},
  author       = {Qingcong Yuan and Xianyan Chen and Chenlu Ke and Xiangrong Yin},
  doi          = {10.1016/j.csda.2022.107530},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107530},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Independence index sufficient variable screening for categorical responses},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood-free inference with deep gaussian processes.
<em>CSDA</em>, <em>174</em>, 107529. (<a
href="https://doi.org/10.1016/j.csda.2022.107529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models have been successfully used in likelihood-free inference to decrease the number of simulator evaluations. The current state-of-the-art performance for this task has been achieved by Bayesian Optimization with Gaussian Processes (GPs). While this combination works well for unimodal target distributions , it is restricting the flexibility and applicability of Bayesian Optimization for accelerating likelihood-free inference more generally. This problem is addressed by proposing a Deep Gaussian Process (DGP) surrogate model that can handle more irregularly behaved target distributions. The experiments show how DGPs can outperform GPs on objective functions with multimodal distributions and maintain a comparable performance in unimodal cases. At the same time, DGPs generally require much fewer data to achieve the same level of performance as neural density and kernel mean embedding alternatives. This confirms that DGPs as surrogate models can extend the applicability of Bayesian Optimization for likelihood-free inference (BOLFI), while only adding computational overhead that remains negligible for computationally intensive simulators.},
  archive      = {J_CSDA},
  author       = {Alexander Aushev and Henri Pesonen and Markus Heinonen and Jukka Corander and Samuel Kaski},
  doi          = {10.1016/j.csda.2022.107529},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107529},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Likelihood-free inference with deep gaussian processes},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPU accelerated estimation of a shared random effect joint
model for dynamic prediction. <em>CSDA</em>, <em>174</em>, 107528. (<a
href="https://doi.org/10.1016/j.csda.2022.107528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal cohort studies, it is often of interest to predict the risk of a terminal clinical event using longitudinal predictor data among subjects at risk by the time of the prediction. The at-risk population changes over time; so does the association between predictors and the outcome, as well as the accumulating longitudinal predictor history. The dynamic nature of this prediction problem has received increasing interest in the literature, but computation often poses a challenge. The widely used joint model of longitudinal and survival data often comes with intensive computation and excessive model fitting time, due to numerical optimization and the analytically intractable high-dimensional integral in the likelihood function. This problem is exacerbated when the model is fit to a large dataset or the model involves multiple longitudinal predictors with nonlinear trajectories. This challenge can be addressed from an algorithmic perspective, by a novel two-stage estimation procedure, and from a computing perspective, by Graphics Processing Unit (GPU) programming. The latter is implemented through PyTorch , an emerging deep learning framework. The numerical studies demonstrate that the proposed algorithm and software can substantially speed up the estimation of the joint model, particularly with large datasets. The numerical studies also concluded that accounting for nonlinearity in longitudinal predictor trajectories can improve the prediction accuracy in comparison to joint modeling that ignore nonlinearity.},
  archive      = {J_CSDA},
  author       = {Shikun Wang and Zhao Li and Lan Lan and Jieyi Zhao and W. Jim Zheng and Liang Li},
  doi          = {10.1016/j.csda.2022.107528},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107528},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {GPU accelerated estimation of a shared random effect joint model for dynamic prediction},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A roughness penalty approach to estimate densities over
two-dimensional manifolds. <em>CSDA</em>, <em>174</em>, 107527. (<a
href="https://doi.org/10.1016/j.csda.2022.107527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovative nonparametric method for density estimation over general two-dimensional Riemannian manifolds is proposed. The method follows a functional data analysis approach, combining maximum likelihood estimation with a roughness penalty that involves a differential operator appropriately defined over the manifold domain, thus controlling the smoothness of the estimate. The proposed method can accurately handle point pattern data over complicated curved domains. Moreover, it is able to capture complex multimodal signals, with strongly localized and highly skewed modes, with varying directions and intensity of anisotropy. The estimation procedure exploits a discretization in finite element bases, enabling great flexibility on the spatial domain. The method is tested through simulation studies, showing the strengths of the proposed approach. Finally, the density estimation method is illustrated with an application to the distribution of earthquakes in the world.},
  archive      = {J_CSDA},
  author       = {Eleonora Arnone and Federico Ferraccioli and Clara Pigolotti and Laura M. Sangalli},
  doi          = {10.1016/j.csda.2022.107527},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107527},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A roughness penalty approach to estimate densities over two-dimensional manifolds},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust fitting of mixture models using weighted complete
estimating equations. <em>CSDA</em>, <em>174</em>, 107526. (<a
href="https://doi.org/10.1016/j.csda.2022.107526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture modeling, which considers the potential heterogeneity in data, is widely adopted for classification and clustering problems . Mixture models can be estimated using the Expectation-Maximization algorithm, which works with the complete estimating equations conditioned by the latent membership variables of the cluster assignment based on the hierarchical expression of mixture models. However, when the mixture components have light tails such as a normal distribution , the mixture model can be sensitive to outliers. This study proposes a method of weighted complete estimating equations (WCE) for the robust fitting of mixture models. Our WCE introduces weights to complete estimating equations such that the weights can automatically downweight the outliers. The weights are constructed similarly to the density power divergence for mixture models, but in our WCE, they depend only on the component distributions and not on the whole mixture. A novel expectation-estimating-equation (EEE) algorithm is also developed to solve the WCE. For illustrative purposes, a multivariate Gaussian mixture , a mixture of experts, and a multivariate skew normal mixture are considered, and how our EEE algorithm can be implemented for these specific models is described. The numerical performance of the proposed robust estimation method was examined using simulated and real datasets.},
  archive      = {J_CSDA},
  author       = {Shonosuke Sugasawa and Genya Kobayashi},
  doi          = {10.1016/j.csda.2022.107526},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107526},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust fitting of mixture models using weighted complete estimating equations},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov-switching state-space models with applications to
neuroimaging. <em>CSDA</em>, <em>174</em>, 107525. (<a
href="https://doi.org/10.1016/j.csda.2022.107525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-space models (SSM) with Markov switching offer a powerful framework for detecting multiple regimes in time series, analyzing mutual dependence and dynamics within regimes, and assessing transitions between regimes. These models however present considerable computational challenges due to the exponential number of possible regime sequences to account for. In addition, high dimensionality of time series can hinder likelihood-based inference. To address these challenges, novel statistical methods for Markov-switching SSMs are proposed using maximum likelihood estimation, Expectation-Maximization (EM), and parametric bootstrap . Solutions are developed for initializing the EM algorithm , accelerating convergence, and conducting inference. These methods, which are ideally suited to massive spatio-temporal data such as brain signals, are evaluated in simulations and applications to EEG studies of epilepsy and of motor imagery are presented.},
  archive      = {J_CSDA},
  author       = {David Degras and Chee-Ming Ting and Hernando Ombao},
  doi          = {10.1016/j.csda.2022.107525},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107525},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Markov-switching state-space models with applications to neuroimaging},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data-driven line search rule for support recovery in
high-dimensional data analysis. <em>CSDA</em>, <em>174</em>, 107524. (<a
href="https://doi.org/10.1016/j.csda.2022.107524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For ℓ 0 ℓ0 penalized (nonlinear) regression problems , most existing algorithms carried out theoretical analysis and numerical calculation with a fixed step size. However, the selection of an appropriate step size and the guarantee of good performance depend heavily on the parameters of the restricted strong convexity and smoothness of the loss function, which are difficult to calculate in practice. To overcome this problem, a novel and efficient data-driven line search rule is proposed to adaptively determine the appropriate step size based on the idea of support detection and root finding. For the step size by the line search, the ℓ 2 ℓ2 error bound of iteration sequence and the target regression coefficient has be analyzed without any restrictions on the parameters of the loss function. A lot of numerical comparisons with state-of-the-art algorithms in linear and logistic regression problems show the stability, effectiveness and superiority of the proposed algorithms.},
  archive      = {J_CSDA},
  author       = {Peili Li and Yuling Jiao and Xiliang Lu and Lican Kang},
  doi          = {10.1016/j.csda.2022.107524},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107524},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A data-driven line search rule for support recovery in high-dimensional data analysis},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Support vector regression with penalized likelihood.
<em>CSDA</em>, <em>174</em>, 107522. (<a
href="https://doi.org/10.1016/j.csda.2022.107522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the method of support vector regression (SVR) with penalized likelihood. The ε -insensitive loss function utilized in SVR is naturally incorporated into the likelihood and is combined with the penalty for the vector of regression coefficients . We include all parameters necessary to implement SVR in the proposed penalized likelihood. An efficient algorithm to obtain estimators of parameters is provided and asymptotic results for the estimators are developed. We perform Monte Carlo simulations to confirm the effectiveness of the proposed method and report the results of applying the method to real data sets .},
  archive      = {J_CSDA},
  author       = {Takumi Uemoto and Kanta Naito},
  doi          = {10.1016/j.csda.2022.107522},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107522},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Support vector regression with penalized likelihood},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical testing procedure for three arm
non-inferiority trials. <em>CSDA</em>, <em>174</em>, 107521. (<a
href="https://doi.org/10.1016/j.csda.2022.107521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-inferiority trials are becoming very popular for comparative effectiveness research. Non-inferiority trials establish that the effect of an experimental treatment is not worse than that of a reference treatment by more than a specified margin. A three-arm non-inferiority trial that includes the placebo, experimental treatment, and a reference treatment is considered. It has been criticized that the conventional approach for three-arm non-inferiority trials loses power for the non-inferiority hypothesis test unless the power of the assay sensitivity test is close to one. In order to overcome this situation, a novel hierarchical testing procedure with two stages for three-arm non-inferiority trials is developed. The family-wise error rate (FWER) is investigated analytically and numerically of the proposed test procedure. Numerical studies indicate that the suggested method controls FWER and has more power than the traditional approach particularly when the power of that assay sensitivity test is not close to one. Through these empirical studies, it is shown that the proposed method can be successfully applied in practice.},
  archive      = {J_CSDA},
  author       = {Santu Ghosh and Wenge Guo and Samiran Ghosh},
  doi          = {10.1016/j.csda.2022.107521},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107521},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A hierarchical testing procedure for three arm non-inferiority trials},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional causal mediation analysis based on partial
linear structural equation models. <em>CSDA</em>, <em>174</em>, 107501.
(<a href="https://doi.org/10.1016/j.csda.2022.107501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis has become popular in recent years. The goal of mediation analyses is to learn the direct effects of exposure on outcome as well as mediated effects on the pathway from exposure to outcome. A set of generalized structural equations to estimate the direct and indirect effects for mediation analysis is proposed when the number of mediators is of high-dimensionality. Specifically, a two-step procedure is considered where the penalization framework can be adopted to perform variable selection. A partial linear model is used to account for a nonlinear relationship among pre-treatment confounders and the response variable in each model. Procedures for estimating the coefficients for the treatment and the mediators in the structural models are developed. The obtained estimators can be interpreted as causal effects without imposing a linear assumption on the model structure. The performance of Sobel&#39;s method in obtaining the standard error and confidence interval for the estimated joint indirect effect is also evaluated in simulation studies. Simulation results show a superior performance of the proposed method. It is applied to an epidemiologic study in which the goal is to understand how DNA methylation mediates the effect of childhood trauma on regulation of human stress reactivity.},
  archive      = {J_CSDA},
  author       = {Xizhen Cai and Yeying Zhu and Yuan Huang and Debashis Ghosh},
  doi          = {10.1016/j.csda.2022.107501},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107501},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional causal mediation analysis based on partial linear structural equation models},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved linear regression prediction by transfer learning.
<em>CSDA</em>, <em>174</em>, 107499. (<a
href="https://doi.org/10.1016/j.csda.2022.107499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning , also referred as knowledge transfer, aims at reusing knowledge from a source dataset to a similar target one. While several studies address the problem of what to transfer, the very important question of when to answer remains mostly unanswered, especially from a theoretical point-of-view for regression problems . A new theoretical framework for the problem of parameter transfer for the linear model is proposed. It is shown that the quality of transfer for a new input vector depends on its representation in an eigenbasis involving the parameters of the problem. Furthermore, a statistical test is constructed to predict whether a fine-tuned model has a lower prediction quadratic risk than the base target model for an unobserved sample. Efficiency of the test is illustrated on synthetic data as well as real electricity consumption data.},
  archive      = {J_CSDA},
  author       = {David Obst and Badih Ghattas and Sandra Claudel and Jairo Cugliari and Yannig Goude and Georges Oppenheim},
  doi          = {10.1016/j.csda.2022.107499},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107499},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Improved linear regression prediction by transfer learning},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Outlier detection in multivariate functional data through a
contaminated mixture model. <em>CSDA</em>, <em>174</em>, 107496. (<a
href="https://doi.org/10.1016/j.csda.2022.107496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an industrial context, the activity of sensors is recorded at a high frequency. A challenge is to automatically detect abnormal measurement behavior. Considering the sensor measures as functional data, the problem can be formulated as the detection of outliers in a multivariate functional data set. Due to the heterogeneity of this data set, the proposed contaminated mixture model both clusters the multivariate functional data into homogeneous groups and detects outliers . The main advantage of this procedure over its competitors is that it does not require to specify the proportion of outliers. Model inference is performed through an Expectation-Conditional Maximization algorithm, and the BIC is used to select the number of clusters. Numerical experiments on simulated data demonstrate the high performance achieved by the inference algorithm. In particular, the proposed model outperforms the competitors. Its application on the real data which motivated this study allows to correctly detect abnormal behaviors.},
  archive      = {J_CSDA},
  author       = {Martial Amovin-Assagba and Irène Gannaz and Julien Jacques},
  doi          = {10.1016/j.csda.2022.107496},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107496},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Outlier detection in multivariate functional data through a contaminated mixture model},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying spectral matrix estimation via intrinsic
wavelet regression for surfaces of hermitian positive definite matrices.
<em>CSDA</em>, <em>174</em>, 107477. (<a
href="https://doi.org/10.1016/j.csda.2022.107477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic wavelet transforms and denoising methods are introduced for the purpose of time-varying Fourier spectral matrix estimation. A non-degenerate time-varying spectral matrix constitutes a surface of Hermitian positive definite matrices across time and frequency and any spectral matrix estimator ideally adheres to these geometric constraints . Spectral matrix estimation of a locally stationary time series by means of linear or nonlinear wavelet shrinkage naturally respects positive definiteness at each time-frequency point, without any postprocessing. Moreover, the spectral matrix estimator enjoys equivariance in the sense that it does not nontrivially depend on the chosen basis or coordinate system of the multivariate time series . The algorithmic construction is based on a second-generation average-interpolating wavelet transform in the space of Hermitian positive definite matrices equipped with an affine-invariant metric. The wavelet coefficient decay and linear wavelet thresholding convergence rates of intrinsically smooth surfaces of Hermitian positive definite matrices are derived. Furthermore, practical nonlinear thresholding based on the trace of the matrix-valued wavelet coefficients is investigated. Finally, the time-varying spectral matrix of a nonstationary multivariate electroencephalography (EEG) time series recorded during an epileptic brain seizure is estimated.},
  archive      = {J_CSDA},
  author       = {Joris Chau and Rainer von Sachs},
  doi          = {10.1016/j.csda.2022.107477},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107477},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Time-varying spectral matrix estimation via intrinsic wavelet regression for surfaces of hermitian positive definite matrices},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear predictive directions in clinical trials.
<em>CSDA</em>, <em>174</em>, 107476. (<a
href="https://doi.org/10.1016/j.csda.2022.107476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical trials, individuals in different subgroups may experience differential treatment effects. This leads to the need to consider individualized differences in treatment benefit. The general concept of predictive directions, which are risk scores motivated by potential outcomes considerations, is introduced. These techniques borrow heavily from the literature from sufficient dimension reduction (SDR) and causal inference. Initially directions assuming an idealized complete data structure are formulated. Then a new connection between SDR and kernel machine methodology for detection of treatment-covariate interactions is developed. Simulation studies and a real data analysis from AIDS Clinical Trials Group (ACTG) 175 data show the utility of the proposed approach.},
  archive      = {J_CSDA},
  author       = {Youngjoo Cho and Xiang Zhan and Debashis Ghosh},
  doi          = {10.1016/j.csda.2022.107476},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107476},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonlinear predictive directions in clinical trials},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet testing for a replicate-effect within an ordered
multiple-trial experiment. <em>CSDA</em>, <em>174</em>, 107456. (<a
href="https://doi.org/10.1016/j.csda.2022.107456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimental time series data collected across a sequence of ordered trials (replicates) often crop up in many fields, from neuroscience to circadian biology. In order to decide when to appropriately evade the simplifying assumption that all replicates stem from the same process, an assumption often untrue even when identical stimuli are applied, two novel tests are proposed that assess whether a significant trial-effect is manifest along the experiment. The modelling framework uses wavelet multiscale constructions that mitigate against the potential nonstationarities often present in experimental data , both across times and across replicates. The proposed tests are evaluated in thorough simulation studies and illustrated on neuroscience data, proving to be flexible tools with great promise in dealing with complex multiple-trials time series data and allowing the analyst to accordingly tune their subsequent analysis.},
  archive      = {J_CSDA},
  author       = {Jonathan Embleton and Marina I. Knight and Hernando Ombao},
  doi          = {10.1016/j.csda.2022.107456},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107456},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Wavelet testing for a replicate-effect within an ordered multiple-trial experiment},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Horseshoe shrinkage methods for bayesian fusion estimation.
<em>CSDA</em>, <em>174</em>, 107450. (<a
href="https://doi.org/10.1016/j.csda.2022.107450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation and structure learning of high-dimensional signals via a normal sequence model are considered, where the underlying parameter vector is piecewise constant, or has a block structure. A Bayesian fusion estimation method is developed by using the Horseshoe prior to induce a strong shrinkage effect on successive differences in the mean parameters, simultaneously imposing sufficient prior concentration for non-zero values of the same. Fast and efficient computational procedures are presented via Markov Chain Monte Carlo methods exploring the full posterior distributions of the underlying parameters, and theoretical justifications of the approach are also provided by deriving posterior convergence rates and establishing selection consistency under suitable assumptions. The proposed method is extended to signal de-noising over arbitrary graphs and efficient computational methods are developed along with theoretical guarantees. The superior performance of the Horseshoe based Bayesian fusion estimation method is demonstrated through extensive simulations and two real-life examples on signal de-noising in biological and geophysical applications. The estimation performance of the method is also demonstrated on a real-world large network for the graph signal de-noising problem.},
  archive      = {J_CSDA},
  author       = {Sayantan Banerjee},
  doi          = {10.1016/j.csda.2022.107450},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107450},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Horseshoe shrinkage methods for bayesian fusion estimation},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing the asymptotic distribution of second-order u- and
v-statistics. <em>CSDA</em>, <em>174</em>, 107437. (<a
href="https://doi.org/10.1016/j.csda.2022.107437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under general conditions, the asymptotic distribution of degenerate second-order U - and V -statistics is an (infinite) weighted sum of χ 2 χ2 random variables whose weights are the eigenvalues of an integral operator associated with the kernel of the statistic. Also the behavior of the statistic in terms of power can be characterized through the eigenvalues and the eigenfunctions of the same integral operator. No general algorithm seems to be available to compute these quantities starting from the kernel of the statistic. An algorithm is proposed to approximate (as precisely as needed) the asymptotic distribution and to build several measures of performance for tests based on U - and V -statistics. The algorithm uses the Wielandt–Nyström method of approximation of an integral operator based on quadrature, and can be used with several methods of numerical integration. An extensive numerical study shows that the Wielandt–Nyström method based on Clenshaw–Curtis quadrature performs very well both for the eigenvalues and the eigenfunctions .},
  archive      = {J_CSDA},
  author       = {Raffaello Seri},
  doi          = {10.1016/j.csda.2022.107437},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107437},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Computing the asymptotic distribution of second-order U- and V-statistics},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of the volume under a ROC surface in presence of
covariates. <em>CSDA</em>, <em>174</em>, 107434. (<a
href="https://doi.org/10.1016/j.csda.2022.107434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method to adjust for covariate effects in the estimation of volume under a ROC surface (VUS) is presented. The method is based on the induced-regression methodology, which uses location-scale regression models to explain the relation between the test results and the covariate(s). For the estimation of the models, it is proposed to use a semiparametric generalized estimating equations (GEE) approach if the parametric forms of the mean and variance functions are specified. Alternatively, a nonparametric method is proposed, based on local linear regression (LL). In order to estimate the covariate-specific VUS, a covariate-specific Mann-Whitney representation of VUS is used, and working samples constructed after fitting the location-scale models by the GEE or LL approach. This leads to new MW-GEE and MW-LL covariate-specific VUS estimators. The asymptotic behavior of the new estimators is investigated. More precisely, their mean squared consistency is proved. Moreover, the performance of the estimators in finite samples is explored through several simulation experiments, and an illustration, based on data from the Alzheimer&#39;s Disease Neuroimaging Initiative, is provided.},
  archive      = {J_CSDA},
  author       = {Duc-Khanh To and Gianfranco Adimari and Monica Chiogna},
  doi          = {10.1016/j.csda.2022.107434},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107434},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of the volume under a ROC surface in presence of covariates},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian graphical modeling for spectrometric data analysis.
<em>CSDA</em>, <em>174</em>, 107416. (<a
href="https://doi.org/10.1016/j.csda.2021.107416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of spectrometric data, a Gaussian graphical model for learning the dependence structure among frequency bands of the infrared absorbance spectrum is introduced. The spectra are modeled as continuous functional data through a B-spline basis expansion and a Gaussian graphical model is assumed as a prior specification for the smoothing coefficients to induce sparsity in their precision matrix . Bayesian inference is carried out to simultaneously smooth the curves and to estimate the conditional independence structure between portions of the functional domain. The proposed model is applied to the analysis of infrared absorbance spectra of strawberry purees.},
  archive      = {J_CSDA},
  author       = {Laura Codazzi and Alessandro Colombi and Matteo Gianella and Raffaele Argiento and Lucia Paci and Alessia Pini},
  doi          = {10.1016/j.csda.2021.107416},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107416},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Gaussian graphical modeling for spectrometric data analysis},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain waves analysis via a non-parametric bayesian mixture
of autoregressive kernels. <em>CSDA</em>, <em>174</em>, 107409. (<a
href="https://doi.org/10.1016/j.csda.2021.107409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard approach to analyzing brain electrical activity is to examine the spectral density function (SDF) and identify frequency bands, defined a priori, that have the most substantial relative contributions to the overall variance of the signal. However, a limitation of this approach is that the precise frequency and bandwidth of oscillations are not uniform across different cognitive demands. Thus, these bands should not be arbitrarily set in any analysis. To overcome this limitation, the Bayesian mixture auto-regressive decomposition (BMARD) method is proposed, as a data-driven approach that identifies (i) the number of prominent spectral peaks , (ii) the frequency peak locations, and (iii) their corresponding bandwidths (or spread of power around the peaks). Using the BMARD method, the standardized SDF is represented as a Dirichlet process mixture based on a kernel derived from second-order auto-regressive processes which completely characterize the location (peak) and scale (bandwidth) parameters. A Metropolis-Hastings within the Gibbs algorithm is developed for sampling the posterior distribution of the mixture parameters. Simulations demonstrate the robust performance of the proposed method. Finally, the BMARD method is applied to analyze local field potential (LFP) activity from the hippocampus of laboratory rats across different conditions in a non-spatial sequence memory experiment, to identify the most prominent frequency bands and examine the link between specific patterns of brain oscillatory activity and trial-specific cognitive demands.},
  archive      = {J_CSDA},
  author       = {Guilllermo Granados-Garcia and Mark Fiecas and Shahbaba Babak and Norbert J. Fortin and Hernando Ombao},
  doi          = {10.1016/j.csda.2021.107409},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107409},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Brain waves analysis via a non-parametric bayesian mixture of autoregressive kernels},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalisations of a bayesian decision-theoretic
randomisation procedure and the impact of delayed responses.
<em>CSDA</em>, <em>174</em>, 107407. (<a
href="https://doi.org/10.1016/j.csda.2021.107407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of sequential experiments and, in particular, randomised controlled trials involves a trade-off between operational characteristics such as statistical power, estimation bias and patient benefit. The family of randomisation procedures referred to as Constrained Randomised Dynamic Programming (CRDP), which is set in the Bayesian decision-theoretic framework, can be used to balance these competing objectives. A generalisation and novel interpretation of CRDP is proposed to highlight its inherent flexibility to adapt to a variety of practicalities and align with individual trial objectives. CRDP, as with most response-adaptive randomisation procedures, hinges on the limiting assumption of patient responses being available before allocation of the next patient. This forms one of the greatest barriers to their implementation in practice which, despite being an important research question, has not received a thorough treatment. Therefore, motivated by the existing gap between the theory of response-adaptive randomisation (which is abundant with proposed methods in the immediate response setting) and clinical practice (in which responses are typically delayed), the performance of CRDP in the presence of fixed and random delays is evaluated. Simulation results show that CRDP continues to offer patient benefit gains over alternative procedures and is relatively robust to delayed responses. To compensate for a fixed delay, a method which adjusts the time horizon used in the optimisation objective is proposed and its performance illustrated.},
  archive      = {J_CSDA},
  author       = {S. Faye Williamson and Peter Jacko and Thomas Jaki},
  doi          = {10.1016/j.csda.2021.107407},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107407},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalisations of a bayesian decision-theoretic randomisation procedure and the impact of delayed responses},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Airflow recovery from thoracic and abdominal movements using
synchrosqueezing transform and locally stationary gaussian process
regression. <em>CSDA</em>, <em>174</em>, 107384. (<a
href="https://doi.org/10.1016/j.csda.2021.107384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wealth of information about respiratory system is encoded in the airflow signal. While direct measurement of airflow via spirometer with an occlusive seal is the gold standard, this may not be practical for ambulatory monitoring of patients. Advances in sensor technology have made measurement of motion of the thorax and abdomen feasible with small inexpensive devices, but estimating airflow from these time series is challenging due to the presence of complicated nonstationary oscillatory signals. To properly extract the relevant oscillatory features from thoracic and abdominal movement, a nonlinear-type time-frequency analysis tool, the synchrosqueezing transform, is employed; these features are then used to estimate the airflow by a locally stationary Gaussian process regression. It is shown that, using a dataset that contains respiratory signals under normal sleep conditions, accurate airflow out-of-sample predictions, and hence the precise estimation of an important physiological quantity, inspiration respiration ratio, can be achieved by fitting the proposed model both in the intra- and inter-subject setups. The method is also applied to a more challenging case, where subjects under general anesthesia underwent transitions from pressure support to unassisted ventilation to further demonstrate the utility of the proposed method.},
  archive      = {J_CSDA},
  author       = {Whitney K. Huang and Yu-Min Chung and Yu-Bo Wang and Jeff E. Mandel and Hau-Tieng Wu},
  doi          = {10.1016/j.csda.2021.107384},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107384},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Airflow recovery from thoracic and abdominal movements using synchrosqueezing transform and locally stationary gaussian process regression},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The wasserstein impact measure (WIM): A practical tool for
quantifying prior impact in bayesian statistics. <em>CSDA</em>,
<em>174</em>, 107352. (<a
href="https://doi.org/10.1016/j.csda.2021.107352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prior distribution is a crucial building block in Bayesian analysis , and its choice will impact the subsequent inference. It is therefore important to have a convenient way to quantify this impact, as such a measure of prior impact will help to choose between two or more priors in a given situation. To this end a new approach, the Wasserstein Impact Measure (WIM), is introduced. In three simulated scenarios, the WIM is compared to two competitor prior impact measures from the literature, and its versatility is illustrated via two real datasets.},
  archive      = {J_CSDA},
  author       = {Fatemeh Ghaderinezhad and Christophe Ley and Ben Serrien},
  doi          = {10.1016/j.csda.2021.107352},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107352},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The wasserstein impact measure (WIM): A practical tool for quantifying prior impact in bayesian statistics},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient estimation in a partially specified nonignorable
propensity score model. <em>CSDA</em>, <em>174</em>, 107322. (<a
href="https://doi.org/10.1016/j.csda.2021.107322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the regression setting where the response variable is subject to missing data and the covariates are fully observed. A nonignorable propensity score model, i.e., the probability that the response is observed conditional on all variables depends on the missing values themselves, is assumed throughout the paper. In such problems, model misspecification and model identifiability are two critical issues. A fully parametric approach can produce results that are sensitive to the model assumptions, while a fully nonparametric approach may not be sufficient for model identification. A new flexible semiparametric propensity score model is proposed where the relationship between the missingness indicator and the partially observed response is totally unspecified and estimated nonparametrically, while the relationship between the missingness indicator and the fully observed covariates is modeled parametrically. The proposed estimator is constructed via a semiparametric treatment and is proved to be semiparametrically efficient. Comprehensive simulation studies are conducted to examine the finite-sample performance of the estimators. While the naive parametric method leads to heavily biased estimator and poor coverage results, the proposed method produces estimator with negligible finite-sample biases and also correct inference results. The proposed method is further illustrated via an electronic health records (EHR) data application for the albumin level in the blood sample. The empirical analyses demonstrated that the proposed semiparametric propensity score model is more sensible than a purely parametric model . The proposed method could be very useful to uncover the unknown and possibly nonlinear dependence of the propensity score model to the albumin level, and is recommended for practical use.},
  archive      = {J_CSDA},
  author       = {Mengyan Li and Yanyuan Ma and Jiwei Zhao},
  doi          = {10.1016/j.csda.2021.107322},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107322},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient estimation in a partially specified nonignorable propensity score model},
  volume       = {174},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering, multicollinearity, and singular vectors.
<em>CSDA</em>, <em>173</em>, 107523. (<a
href="https://doi.org/10.1016/j.csda.2022.107523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let A be a matrix with its Moore-Penrose pseudo-inverse A † A† . It is proved that, after re-ordering the columns of A , the projector P = I − A † A P=I−A†A has a block-diagonal form, that is there is a permutation matrix Π such that Π P Π T = diag ( S 1 , S 2 , … , S k ) ΠPΠT=diag(S1,S2,…,Sk) . It is further proved that each block S i Si corresponds to a cluster of columns of A that are linearly dependent with each other. A clustering algorithm is provided that allows to partition the columns of A into clusters where columns in a cluster correlate only with columns within the same cluster. Some applications in supervised and unsupervised learning , specially feature selection, clustering, and sensitivity of solutions of least squares solutions are discussed.},
  archive      = {J_CSDA},
  author       = {Hamid Usefi},
  doi          = {10.1016/j.csda.2022.107523},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107523},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clustering, multicollinearity, and singular vectors},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complexity reduction and approximation of multidomain
systems of partially ordered data. <em>CSDA</em>, <em>173</em>, 107520.
(<a href="https://doi.org/10.1016/j.csda.2022.107520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two greedy algorithms for the synthesis and approximation of multidomain systems of partially ordered data are proposed. Given k input partially ordered sets (posets) on the same elements, the algorithms search for the optimally approximating partial orders, minimizing the dissimilarity between the generated and input posets , based on their matrices of mutual ranking probabilities. A general approximation algorithm is developed, together with a specific procedure for approximation over bucket orders, which are the natural choice when the goal is to “condense” the inputs into rankings, possibly with ties. Different loss functions are also employed, and their outputs are compared. A real example pertaining to regional well-being in Italy motivates the algorithms and shows them in action.},
  archive      = {J_CSDA},
  author       = {Alberto Arcagni and Alessandro Avellone and Marco Fattore},
  doi          = {10.1016/j.csda.2022.107520},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107520},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Complexity reduction and approximation of multidomain systems of partially ordered data},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint non-parametric estimation of mean and auto-covariances
for gaussian processes. <em>CSDA</em>, <em>173</em>, 107519. (<a
href="https://doi.org/10.1016/j.csda.2022.107519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes that can be decomposed into a smooth mean function and a stationary autocorrelated noise process are considered and a fully automatic nonparametric method to simultaneous estimation of mean and auto-covariance functions of such processes is developed. The proposed empirical Bayes approach is data-driven, numerically efficient, and allows for the construction of confidence sets for the mean function. Performance is demonstrated in simulations and real data analysis. The method is implemented in the R package eBsc . 1},
  archive      = {J_CSDA},
  author       = {Tatyana Krivobokova and Paulo Serra and Francisco Rosales and Karolina Klockmann},
  doi          = {10.1016/j.csda.2022.107519},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107519},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint non-parametric estimation of mean and auto-covariances for gaussian processes},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local and global topics in text modeling of web pages nested
in web sites. <em>CSDA</em>, <em>173</em>, 107518. (<a
href="https://doi.org/10.1016/j.csda.2022.107518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models assert that documents are distributions over latent topics and latent topics are distributions over words. A nested document collection has documents nested inside a higher order structure such as articles nested in journals, podcasts within authors, or web pages nested in web sites. In a single collection of documents, topics are global or shared across all documents. For web pages nested in web sites, topic frequencies likely vary across web sites and within a web site, topic frequencies almost certainly vary from web page to web page. A hierarchical prior for topic frequencies models this hierarchical structure with a global topic distribution, web site topic distributions varying around the global topic distribution, and web page topic distributions varying around the web site topic distribution. Web pages in one United States local health department web site often contain local geographic and news topics not found on web pages of other local health department web sites. For web pages nested in web sites, some topics are likely local topics and unique to an individual web site. Regular topic models ignore the nesting structure and may identify local topics but cannot label those topics as local nor identify the corresponding web site owner. Explicitly modeling local topics identifies the owning web site and identifies the topic as local. In US health web site data, topic coverage is defined at the web site level after removing local topic words from pages. Hierarchical local topic models can be used to study how well health topics are covered.},
  archive      = {J_CSDA},
  author       = {Jason Wang and Robert E. Weiss},
  doi          = {10.1016/j.csda.2022.107518},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107518},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Local and global topics in text modeling of web pages nested in web sites},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Log-regularly varying scale mixture of normals for robust
regression. <em>CSDA</em>, <em>173</em>, 107517. (<a
href="https://doi.org/10.1016/j.csda.2022.107517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression that employs the assumption of normality for the error distribution may lead to an undesirable posterior inference of regression coefficients due to potential outliers. A finite mixture of two components, one with thin and one with heavy tails, is considered as the error distribution in this study. For the heavily-tailed component, the novel class of distributions is introduced; their densities are log-regularly varying and have heavier tails than the Cauchy distribution . Yet, they are expressed as a scale mixture of normals which enables the efficient posterior inference when using a Gibbs sampler . The robustness of the posterior distributions is proved under the proposed models using a minimal set of assumptions, which justifies the use of shrinkage priors with unbounded densities for the coefficient vector in the presence of outliers. An extensive comparison with the existing methods via simulation study shows the improved performance of the proposed model in point and interval estimation , as well as its computational efficiency. Further, the posterior robustness of the proposed method is confirmed in an empirical study with shrinkage priors for regression coefficients .},
  archive      = {J_CSDA},
  author       = {Yasuyuki Hamura and Kaoru Irie and Shonosuke Sugasawa},
  doi          = {10.1016/j.csda.2022.107517},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107517},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Log-regularly varying scale mixture of normals for robust regression},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe sample screening rules for multicategory angle-based
support vector machines. <em>CSDA</em>, <em>173</em>, 107508. (<a
href="https://doi.org/10.1016/j.csda.2022.107508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machines are popular techniques for classification problems, where the optimal separating hyperplane only depends on a subset of training data. To reduce computational costs, safe sample screening rules are proposed in the literature, which enable us to remove redundant samples prior to the training phase. However, existing works on safe sample screening rules mainly focus on binary classification . The multicategory angle-based support vector machine (MASVM) is a computationally efficient method for multicategory classification problems, which constructs a decision function without the sum-to-zero constraint. To further reduce computational costs in linear MASVM, two safe sample screening methods are proposed: the gap safe rule (MAGSR) and the dual screening with variational inequalities (MADVI). A two-stage screening framework combining MAGSR and MADVI together is then developed. Extensive simulations and real applications show the great advantage of the proposed methods in computation, compared with existing approaches.},
  archive      = {J_CSDA},
  author       = {Yiwei Fan and Junlong Zhao},
  doi          = {10.1016/j.csda.2022.107508},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107508},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Safe sample screening rules for multicategory angle-based support vector machines},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thresholding tests based on affine LASSO to achieve
non-asymptotic nominal level and high power under sparse and dense
alternatives in high dimension. <em>CSDA</em>, <em>173</em>, 107507. (<a
href="https://doi.org/10.1016/j.csda.2022.107507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thresholding estimators such as the existing square-root and LAD LASSO, and the new affine and GLM LASSO with new link functions, have the ability to set coefficients to zero. They will yield new pivotal statistics which enjoy high power under sparse or dense alternative hypotheses. Under a general formalism, thresholding tests not only recover existing tests such as Rao score test and Fisher nonparametric sign test, but also unveil new tests, for the global/omnibus hypothesis in high dimension in particular. Although pivotal, the new statistics do not have a known distribution, so the critical value of the test is calculated by straightforward Monte Carlo, which yields exact level and high power as illustrated on simulated data.},
  archive      = {J_CSDA},
  author       = {Sylvain Sardy and Jairo Diaz-Rodriguez and Caroline Giacobino},
  doi          = {10.1016/j.csda.2022.107507},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107507},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Thresholding tests based on affine LASSO to achieve non-asymptotic nominal level and high power under sparse and dense alternatives in high dimension},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic representation of FGM copulas using multivariate
bernoulli random variables. <em>CSDA</em>, <em>173</em>, 107506. (<a
href="https://doi.org/10.1016/j.csda.2022.107506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A one-to-one correspondence between Fréchet&#39;s class of multivariate Bernoulli distribution with symmetric marginals and the well-known family of Farlie-Gumbel-Morgenstern (FGM) copulas is established. A new stochastic representation of the family of d -variate FGM copulas is introduced. The representation is bijective: from any d -variate Bernoulli distribution , one may define a corresponding d -variate FGM copula; and for any d -variate FGM copula, one finds the corresponding d -variate Bernoulli distribution. The proposed stochastic representation has many advantages, notably establishing stochastic orders , constructing subclasses of FGM copulas and sampling. In particular, one may use the stochastic representation to develop computational methods to perform sampling from subclasses of FGM copulas, which scale well to large dimensions.},
  archive      = {J_CSDA},
  author       = {Christopher Blier-Wong and Hélène Cossette and Etienne Marceau},
  doi          = {10.1016/j.csda.2022.107506},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107506},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Stochastic representation of FGM copulas using multivariate bernoulli random variables},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stage optimal subsampling estimation for missing data
problems with large-scale data. <em>CSDA</em>, <em>173</em>, 107505. (<a
href="https://doi.org/10.1016/j.csda.2022.107505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is useful to downsize data volumes and speed up calculations for large-scale data and is well studied with completely observed data. In the presence of missing data, computation is more challenging and subsampling becomes more crucial and complex. However, there is still a lack of study on subsampling for missing data problems. This paper fills the gap by studying the subsampling method for a widely used missing data estimator, the augmented inverse probability weighting (AIPW) estimator. The response mean estimation problem with missing responses is discussed for illustration. A two-stage subsampling method is proposed via Poisson sampling framework. A small subsample of expected size n 1 n1 is used in the first stage to estimate the parameters in the propensity score and the outcome regression models, while a larger subsample of expected size n 2 n2 is used in the computationally simple second stage to calculate the final estimator. An attractive property of the resulting estimator is that its convergence rate is n 2 − 1 / 2 n2−1/2 rather than n 1 − 1 / 2 n1−1/2 when both the propensity score and the outcome regression functions are correctly specified. The rate n 2 − 1 / 2 n2−1/2 is still attainable for some important cases if only one of the two functions is correctly specified. This indicates that using a small subsample in the computationally complex first stage can reduce the computational burden with little impact on the statistical accuracy. Asymptotic normality of the resulting estimator is established and the optimal subsampling probability is derived by minimizing the asymptotic variance of the resulting estimator. Simulations and a real data analysis were conducted to demonstrate the empirical performance of the resulting estimator.},
  archive      = {J_CSDA},
  author       = {Miaomiao Su and Ruoyu Wang and Qihua Wang},
  doi          = {10.1016/j.csda.2022.107505},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107505},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A two-stage optimal subsampling estimation for missing data problems with large-scale data},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature screening and FDR control with knockoff features for
ultrahigh-dimensional right-censored data. <em>CSDA</em>, <em>173</em>,
107504. (<a href="https://doi.org/10.1016/j.csda.2022.107504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-free feature screening method for ultrahigh-dimensional right-censored data is advocated. A two-step approach, with the help of knockoff features, is proposed to specify the threshold for feature screening such that the false discovery rate (FDR) is controlled under a prespecified level. The proposed two-step approach enjoys both a sure screening property with high probability and FDR control simultaneously if the prespecified FDR level is greater than or equal to 1 / s 1/s , where s is the number of active features. The finite sample properties of the newly suggested method are assessed through simulation studies. An application to the mantle cell lymphoma (MCL) study demonstrates the utility of the proposed method in practice.},
  archive      = {J_CSDA},
  author       = {Yingli Pan},
  doi          = {10.1016/j.csda.2022.107504},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107504},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Feature screening and FDR control with knockoff features for ultrahigh-dimensional right-censored data},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian multiresolution modeling of georeferenced data: An
extension of “LatticeKrig.” <em>CSDA</em>, <em>173</em>, 107503. (<a
href="https://doi.org/10.1016/j.csda.2022.107503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘LatticeKrig’ (LK) is a spatial model that is often used for modeling multiresolution spatial data with flexible covariance structures . An extension to LK under a Bayesian framework is proposed that uses integrated nested Laplace approximations (INLA). The extension enables the spatial analysis of non-Gaussian responses in latent Gaussian models , joint spatial modeling with structured and unstructured random effects, and native support for multithreaded parallel likelihood computation. The proposed extended LatticeKrig (ELK) model uses a reparameterization of LK so that the parameters and prior selection are intuitive and interpretable. Priors can be used to make inference robust by penalizing more complex models, and integration over model parameters allows for posterior uncertainty estimates that account for uncertainty in covariance parameters . ELK&#39;s ability to reliably resolve multiresolution spatial structure for pointwise and areal predictions is demonstrated in both simulation study and two applications with non-Gaussian observations: a set of 188,717 LiDAR forest canopy height observations in Bonanza Creek Experimental Forest in Alaska, and a set of 1,612 clusters containing counts of secondary education completion from the 2014 Kenya demographic health survey. ELK has improved central predictions as well as uncertainty characterization according to the considered scoring rules when compared against a number of other models, particularly in the forest canopy height application, and performed faster than LK in our tests in part due to its support for and use of parallelization .},
  archive      = {J_CSDA},
  author       = {John Paige and Geir-Arne Fuglstad and Andrea Riebler and Jon Wakefield},
  doi          = {10.1016/j.csda.2022.107503},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107503},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian multiresolution modeling of georeferenced data: An extension of ‘LatticeKrig’},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entropy-based test for generalised gaussian distributions.
<em>CSDA</em>, <em>173</em>, 107502. (<a
href="https://doi.org/10.1016/j.csda.2022.107502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proof of L 2 L2 consistency for the k th nearest neighbour distance estimator of the Shannon entropy for an arbitrary fixed k ≥ 1 k≥1 is provided. It is constructed the non-parametric test of goodness-of-fit for a class of introduced generalised multivariate Gaussian distributions based on a maximum entropy principle. The theoretical results are followed by numerical studies on simulated samples. It is shown that increasing of k improves the power of the introduced goodness of fit tests. The asymptotic normality of the test statistics is experimentally proven.},
  archive      = {J_CSDA},
  author       = {Mehmet Siddik Cadirci and Dafydd Evans and Nikolai Leonenko and Vitalii Makogin},
  doi          = {10.1016/j.csda.2022.107502},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107502},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Entropy-based test for generalised gaussian distributions},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Marginal m-quantile regression for multivariate dependent
data. <em>CSDA</em>, <em>173</em>, 107500. (<a
href="https://doi.org/10.1016/j.csda.2022.107500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An M-quantile regression model is developed for the analysis of multiple dependent outcomes by introducing the notion of directional M-quantiles for multivariate responses. In order to incorporate the correlation structure of the data into the estimation framework, a robust marginal M-quantile model is proposed extending the well-known generalized estimating equations approach to the case of regression M-quantiles with Huber&#39;s loss function. The estimation of the model and the asymptotic properties of estimators are discussed. In addition, the idea of M-quantile contours is introduced to describe the dependence between the response variables and to investigate the effect of covariates on the location, spread and shape of the distribution of the responses. To examine their variability, confidence envelopes via nonparametric bootstrap are constructed. The validity of the proposed methodology is explored both by means of simulation studies and through an application to educational data.},
  archive      = {J_CSDA},
  author       = {Luca Merlo and Lea Petrella and Nicola Salvati and Nikos Tzavidis},
  doi          = {10.1016/j.csda.2022.107500},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107500},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Marginal M-quantile regression for multivariate dependent data},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mallows model averaging with effective model size in
fragmentary data prediction. <em>CSDA</em>, <em>173</em>, 107497. (<a
href="https://doi.org/10.1016/j.csda.2022.107497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing model averaging methods consider fully observed data while fragmentary data, in which not all the covariate data are available for many subjects, becomes more and more popular nowadays with the increasing data sources in many areas such as economics, social sciences and medical studies. The main challenge of model averaging in fragmentary data is that the samples to fit candidate models are different to the sample used for weight selection, which introduces bias to the Mallows criterion in the classical Mallows Model Averaging (MMA). A novel Mallows model averaging method that utilizes the “effective model size” taking different samples into consideration is proposed and its asymptotic optimality is established. Empirical evidences from a simulation study and a real data analysis are presented. The proposed Effective Mallows Model Averaging (EMMA) method not only provides a novel solution to the fragmentary data prediction, but also sheds light on model selection when candidate models have different sample sizes, which has rarely been discussed in the literature.},
  archive      = {J_CSDA},
  author       = {Chaoxia Yuan and Fang Fang and Lyu Ni},
  doi          = {10.1016/j.csda.2022.107497},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107497},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mallows model averaging with effective model size in fragmentary data prediction},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A rank-based high-dimensional test for equality of mean
vectors. <em>CSDA</em>, <em>173</em>, 107495. (<a
href="https://doi.org/10.1016/j.csda.2022.107495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Wilcoxon signed-rank test and the Wilcoxon-Mann-Whitney test are two commonly used rank-based methods for one- and two-sample tests when the one-dimensional data are not normally distributed. The new rank-based nonparametric tests for equality of mean vectors are proposed in the high-dimensional settings. To overcome the technical challenges in data sorting, the new statistics are constructed by taking the sum of the Wilcoxon signed-rank or Wilcoxon-Mann-Whitney test statistics from each dimension of the data. The asymptotic properties of the proposed test statistics are investigated under the null and local alternative hypotheses. Simulation studies show that the new tests perform as well as the state-of-the-art methods when the high-dimensional data are normally distributed, but they turn out to be more powerful when the normality assumption is violated. Finally, the new testing methods are also applied to a human peripheral blood mononuclear cells gene expression data set for demonstrating their usefulness in practice.},
  archive      = {J_CSDA},
  author       = {Yanyan Ouyang and Jiamin Liu and Tiejun Tong and Wangli Xu},
  doi          = {10.1016/j.csda.2022.107495},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107495},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A rank-based high-dimensional test for equality of mean vectors},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamical modeling for non-gaussian data with
high-dimensional sparse ordinary differential equations. <em>CSDA</em>,
<em>173</em>, 107483. (<a
href="https://doi.org/10.1016/j.csda.2022.107483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinary differential equations (ODE) have been widely used for modeling dynamical complex systems . For high-dimensional ODE models where the number of differential equations is large, it remains challenging to estimate the ODE parameters and to identify the sparse structure of the ODE models. Most existing methods exploit the least-square based approach and are only applicable to Gaussian observations. However, as discrete data are ubiquitous in applications, it is of practical importance to develop dynamic modeling for non-Gaussian observations. New methods and algorithms are developed for both parameter estimation and sparse structure identification in high-dimensional linear ODE systems. First, the high-dimensional generalized profiling method is proposed as a likelihood-based approach with ODE fidelity and sparsity-inducing regularization , along with efficient computation based on parameter cascading. Second, two versions of the two-step collocation methods are extended to the non-Gaussian set-up by incorporating the iteratively reweighted least squares technique. Simulations show that the profiling procedure has excellent performance in latent process and derivative fitting and ODE parameter estimation, while the two-step collocation approach excels in identifying the sparse structure of the ODE system. The usefulness of the proposed methods is also demonstrated by analyzing three real datasets from Google trends, stock market sectors, and yeast cell cycle studies.},
  archive      = {J_CSDA},
  author       = {Muye Nanshan and Nan Zhang and Xiaolei Xun and Jiguo Cao},
  doi          = {10.1016/j.csda.2022.107483},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107483},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dynamical modeling for non-gaussian data with high-dimensional sparse ordinary differential equations},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Varying-coefficient hidden markov models with zero-effect
regions. <em>CSDA</em>, <em>173</em>, 107482. (<a
href="https://doi.org/10.1016/j.csda.2022.107482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In psychological, social, behavioral, and medical studies, hidden Markov models (HMMs) have been extensively applied to the simultaneous modeling of longitudinal observations and the underlying dynamic transition process. However, the existing HMMs mainly focus on constant-coefficient HMMs. This study considers a varying-coefficient HMM, which enables simultaneous investigation of the dynamic covariate effects and between-state transitions. Moreover, a soft-thresholding operator is introduced to detect zero-effect regions of the coefficient functions . A full Bayesian approach with a hybird Markov chain Monte Carlo algorithm that combines B-spline approximation and penalization technique is developed for statistical inference . The empirical performance of the propose method is evaluated through simulation studies. An application to a study on the Alzheimer&#39;s Disease Neuroimaging Initiative dataset is presented.},
  archive      = {J_CSDA},
  author       = {Hefei Liu and Xinyuan Song and Baoxue Zhang},
  doi          = {10.1016/j.csda.2022.107482},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107482},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Varying-coefficient hidden markov models with zero-effect regions},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the semi-varying coefficient dynamic panel data model
with autocorrelated errors. <em>CSDA</em>, <em>173</em>, 107458. (<a
href="https://doi.org/10.1016/j.csda.2022.107458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nonlinear time series modeling , autocorrelation of the random errors may cause critical problems in estimation and inference. The situation becomes even worse for panel data with dynamic structure. However, most of the existing literature has not taken into account this problem. The challenge comes from the fact that the expectation of random errors conditional on lag variables is hardly to be zero. Based on the extension of Whittle likelihood, a semi-parametric dynamic model with ARMA errors for panel data is proposed. Asymptotic normality for the estimators of finite parameters and varying coefficients have been established respectively. Statistical simulations show that the proposed method can efficiently remove the bias of estimation. In real data analysis, it demonstrates that the proposed method can improve prediction when errors are autocorrelated.},
  archive      = {J_CSDA},
  author       = {Honglei Wei and Hongfan Zhang and Hui Jiang and Lei Huang},
  doi          = {10.1016/j.csda.2022.107458},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107458},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On the semi-varying coefficient dynamic panel data model with autocorrelated errors},
  volume       = {173},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable screening for varying coefficient models with
ultrahigh-dimensional survival data. <em>CSDA</em>, <em>172</em>,
107498. (<a href="https://doi.org/10.1016/j.csda.2022.107498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a variable screening method for varying coefficient hazards models of single-index form. The proposed method can be viewed as a natural survival extension of conditional correlation screening. An appealing feature of the proposed method is that it is applicable to many popularly used survival models, including the varying coefficient additive hazards model and the varying coefficient Cox model . The proposed method enjoys the sure screening property, and the number of the selected covariates can be bounded by a moderate order. Simulation studies demonstrate that our method performs well, and an empirical example is also presented.},
  archive      = {J_CSDA},
  author       = {Lianqiang Qu and Xiaoyu Wang and Liuquan Sun},
  doi          = {10.1016/j.csda.2022.107498},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107498},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable screening for varying coefficient models with ultrahigh-dimensional survival data},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient algorithm to assess multivariate surrogate
endpoints in a causal inference framework. <em>CSDA</em>, <em>172</em>,
107494. (<a href="https://doi.org/10.1016/j.csda.2022.107494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate surrogate endpoints can improve the efficiency of the drug development process, but their evaluation raises many challenges. Recently, the so-called individual causal association (ICA) has been introduced for validation purposes in the causal-inference paradigm. The ICA is a function of a partially identifiable correlation matrix ( R ) (R) and, hence, it cannot be estimated without making untestable assumptions. This issue has been addressed via a simulation-based analysis. Essentially, the ICA is assessed across a set of values for the non-identifiable entries in R that lead to a valid correlation matrix and this has been implemented using a fast algorithm based on partial correlations (PC). Using theoretical arguments and simulations, it is shown that, in spite of its computational efficiency, the PC algorithm may lead to the spurious effect that adding non-informative surrogates, i.e., surrogates that convey no information on the treatment effect on the true endpoint, seemingly reduces the ICA range. To address this, a modified PC algorithm (MPC) is proposed. Based on simulations, it is shown that the MPC algorithm removes this nuisance effect and increases computational efficiency.},
  archive      = {J_CSDA},
  author       = {Alvaro J. Flórez and Geert Molenberghs and Wim Van der Elst and Ariel Alonso Abad},
  doi          = {10.1016/j.csda.2022.107494},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107494},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An efficient algorithm to assess multivariate surrogate endpoints in a causal inference framework},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable selection for case-cohort studies with
informatively interval-censored outcomes. <em>CSDA</em>, <em>172</em>,
107484. (<a href="https://doi.org/10.1016/j.csda.2022.107484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection has recently attracted a great deal of attention and in particular, a couple of methods have been proposed for general interval-censored failure time data or the interval-censored data arising from case-cohort studies. However, all of them have some limitations or apply only to limited situations. Corresponding to these, a new, more general variable selection approach is proposed under a class of flexible semiparametric transformation models that allows for dependent interval censoring. In particular, the oracle property of the method under the broken adaptive ridge penalty function is established and for its implementation, a novel EM algorithm is developed. Also a simulation study is performed and suggests that the proposed approach works well in practical situations. Finally the method is applied to a HIV trial that motivated this study.},
  archive      = {J_CSDA},
  author       = {Mingyue Du and Xingqiu Zhao and Jianguo Sun},
  doi          = {10.1016/j.csda.2022.107484},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107484},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection for case-cohort studies with informatively interval-censored outcomes},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian linear models for cardinal paired comparison data.
<em>CSDA</em>, <em>172</em>, 107481. (<a
href="https://doi.org/10.1016/j.csda.2022.107481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a methodology for Bayesian updating in normal linear models in situations where the parameter of interest is restricted to a linear subspace. The methodology is motivated by and applied to the calculation of posterior distributions for the merit parameters and ranks arising in paired comparison data. The Bayesian paradigm is found to be ideal for assessing and quantifying the uncertainty in ranking procedures. The methodology is illustrated using simulated data and applied to two data sets: a network meta–analysis example and to the ranking of teams in the National Basketball Association (NBA).},
  archive      = {J_CSDA},
  author       = {Prince P. Osei and Ori Davidov},
  doi          = {10.1016/j.csda.2022.107481},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107481},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian linear models for cardinal paired comparison data},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate ranks based on randomized lift-interdirections.
<em>CSDA</em>, <em>172</em>, 107480. (<a
href="https://doi.org/10.1016/j.csda.2022.107480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every multivariate sign and rank test needs a workable concept of ranks for multivariate data . Unfortunately, multidimensional spaces lack natural ordering and, consequently, there are no universally accepted ways how to rank vector observations . Existing proposals usable beyond small dimensions are very few in number, and each of them has its own advantages and drawbacks. Therefore, new multivariate ranks based on randomized lift-interdirections are presented, discussed and investigated. These naturally robust and invariant hyperplane-based ranks can be computed quickly and easily even in relatively high-dimensional spaces, and they can be used for nonparametric statistical inference in some existing optimal statistical procedures without altering their asymptotic behavior under null hypotheses or changing their performance under local alternatives. This is not only proved theoretically in case of the canonical sign and rank one-sample test for elliptically distributed observations, but also illustrated empirically in a small simulation study.},
  archive      = {J_CSDA},
  author       = {Šárka Hudecová and Miroslav Šiman},
  doi          = {10.1016/j.csda.2022.107480},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107480},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multivariate ranks based on randomized lift-interdirections},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible estimation of the state dwell-time distribution in
hidden semi-markov models. <em>CSDA</em>, <em>172</em>, 107479. (<a
href="https://doi.org/10.1016/j.csda.2022.107479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden semi-Markov models generalise hidden Markov models by explicitly modelling the time spent in a given state, the so-called dwell time, using some distribution defined on the natural numbers. While the (shifted) Poisson and negative binomial distribution provide natural choices for such distributions, in practice, parametric distributions can lack the flexibility to adequately model the dwell times. To overcome this problem, a penalised maximum likelihood approach is proposed that allows for a flexible and data-driven estimation of the dwell-time distributions without the need to make any distributional assumption. This approach is suitable for direct modelling purposes or as an exploratory tool to investigate the latent state dynamics. The feasibility and potential of the suggested approach is illustrated in a simulation study and by modelling muskox movements in northeast Greenland using GPS tracking data. The proposed method is implemented in the R -package PHSMM which is available on CRAN.},
  archive      = {J_CSDA},
  author       = {Jennifer Pohle and Timo Adam and Larissa T. Beumer},
  doi          = {10.1016/j.csda.2022.107479},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107479},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Flexible estimation of the state dwell-time distribution in hidden semi-markov models},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extrapolation estimation in parametric regression models
with measurement error. <em>CSDA</em>, <em>172</em>, 107478. (<a
href="https://doi.org/10.1016/j.csda.2022.107478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the general parametric regression models with covariates contaminated with normal measurement errors, an alternative method to the traditional simulation extrapolation algorithm is proposed to estimate the unknown parameters in the regression function . By applying the conditional expectation directly to the target function, the proposed algorithm successfully removes the simulation step, by generating an estimation equation either for immediate use or for extrapolating, thus providing a possibility of reducing the computational time or the Monte Carlo simulation error. Large sample properties of the resulting estimator, including the consistency and the asymptotic normality , are thoroughly discussed. Potential wide applications of the proposed estimation procedure are illustrated by examples, simulation studies, as well as a real data analysis.},
  archive      = {J_CSDA},
  author       = {Kanwal Ayub and Weixing Song and Jianhong Shi},
  doi          = {10.1016/j.csda.2022.107478},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107478},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Extrapolation estimation in parametric regression models with measurement error},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust deterministic affine-equivariant algorithm for
multivariate location and scatter. <em>CSDA</em>, <em>172</em>, 107475.
(<a href="https://doi.org/10.1016/j.csda.2022.107475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new computationally feasible algorithm for the Minimum Covariance Determinant (MCD) estimator is proposed. The resulting estimator is deterministic, affine equivariant and permutation invariant unlike prominent alternatives. The new procedure, referred to as Projection Pursuit MCD, combines a single preliminary estimator obtained with a type of non-linear principal component analysis and the so-called concentration step (C-step). Fixed points of the C-step are proved to be local minimizers of the covariance determinant objective. Extensive comparisons for simulated datasets, multivariate Swiss banknote and image segmentation examples show the new algorithm is competitive with and mostly superior to such state-of-the-art procedures as FastMCD and DetMCD for both Gaussian and heavy-tailed real-world data. Outlier detection for Swiss banknote and image segmentation data is presented. A corresponding R package is provided.},
  archive      = {J_CSDA},
  author       = {Michael Pokojovy and J. Marcus Jobe},
  doi          = {10.1016/j.csda.2022.107475},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107475},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A robust deterministic affine-equivariant algorithm for multivariate location and scatter},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A prior for record linkage based on allelic partitions.
<em>CSDA</em>, <em>172</em>, 107474. (<a
href="https://doi.org/10.1016/j.csda.2022.107474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In database management, record linkage aims to identify multiple records that correspond to the same individual. Record linkage can be treated as a clustering problem in which one or more noisy database records are associated with a unique latent entity. In contrast to traditional clustering applications, a large number of clusters with a few observations per cluster is expected in this context. Hence, a new class of prior distributions based on allelic partitions is proposed for the small cluster setting of record linkage. The proposed prior facilitates the introduction of information about the cluster size distribution at different scales, and naturally enforces sublinear growth of the maximum cluster size – known as the microclustering property . In addition, a set of novel microclustering conditions are introduced in order to impose further constraints on the cluster sizes a priori. The performance of the proposed class of priors is evaluated using simulated data and three official statistics data sets. Moreover, different loss functions for optimal point estimation of the partitions are compared using decision-theoretical based approaches recently proposed in the literature.},
  archive      = {J_CSDA},
  author       = {Brenda Betancourt and Juan Sosa and Abel Rodríguez},
  doi          = {10.1016/j.csda.2022.107474},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107474},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A prior for record linkage based on allelic partitions},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regular vines with strongly chordal pattern of (conditional)
independence. <em>CSDA</em>, <em>172</em>, 107461. (<a
href="https://doi.org/10.1016/j.csda.2022.107461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate statistical models can be simplified by assuming that a pattern of conditional independence is presented in the given data. A popular way of capturing the (conditional) independence is to use probabilistic graphical models . The relationship between strongly chordal graphs and m-saturated vines is proved. Moreover, an algorithm to construct an m-saturated vine structure corresponding to strongly chordal graph is provided. This allows the reduction of regular vine copula models complexity. When the underlying data is sparse our approach leads to model estimation improvement when compared with current heuristic methods . Furthermore, due to reduction of model complexity it is possible to evaluate all vine structures as well as to fit non-simplified vines. These advantages have been shown in the simulated and real data examples. 1},
  archive      = {J_CSDA},
  author       = {Kailun Zhu and Dorota Kurowicka},
  doi          = {10.1016/j.csda.2022.107461},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107461},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regular vines with strongly chordal pattern of (conditional) independence},
  volume       = {172},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized correlated cp criterion for derivative
estimation with dependent errors. <em>CSDA</em>, <em>171</em>, 107473.
(<a href="https://doi.org/10.1016/j.csda.2022.107473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, it is common that errors are correlated for the nonparametric regression model. Although many methods have been developed for addressing correlated errors for tuning parameter selection to recover the mean response function, few studies have been proposed to select tuning parameters for derivative estimation. In this paper, a generalized correlated C p Cp ( G C C p GCCp ) criterion is proposed to choose a tuning parameter for derivative estimation in the presence of correlated errors . It can be applied for any nonparametric estimation linear in responses, including kernel regression, local regression, smoothing spline, etc. The G C C p GCCp criterion is justified both theoretically and empirically via simulation studies. Finally, an air quality index data example in Changsha city is provided to illustrate the application of the proposed criterion.},
  archive      = {J_CSDA},
  author       = {Sisheng Liu and Xiaoli Kong},
  doi          = {10.1016/j.csda.2022.107473},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107473},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A generalized correlated cp criterion for derivative estimation with dependent errors},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized ordinal patterns allowing for ties and their
applications in hydrology. <em>CSDA</em>, <em>171</em>, 107472. (<a
href="https://doi.org/10.1016/j.csda.2022.107472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using ordinal patterns, which describe the ordinal structure within a data vector, the problem of ties appears permanently. So far, model classes were used which do not allow for ties; randomization has been another attempt to overcome this problem. Often, time periods with constant values even have been counted as times of monotone increase. However, ties can contain valuable information which is disregarded by all of these approaches. To overcome this, a new approach is proposed: it explicitly allows for ties and, hence, considers more patterns than before. Ties are no longer seen as nuisance, but the information they carry is taken into account explicitly. Limit theorems in the new framework are provided, both, for a single time series and for the dependence between two time series. The methods are applied to hydrological data sets. In hydrology, it is common to distinguish five flood classes (plus ‘absence of flood’). Considering data vectors of these classes at a certain gauge in a river basin, one will usually encounter several ties. Co-monotonic behavior between the data sets of two gauges (increasing, constant, decreasing) can be detected by the method as well as spatial patterns. Thus, it helps to analyze the strength of dependence between different gauges in an intuitive way. This knowledge can be used to assess risk and to plan future construction projects.},
  archive      = {J_CSDA},
  author       = {Alexander Schnurr and Svenja Fischer},
  doi          = {10.1016/j.csda.2022.107472},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107472},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized ordinal patterns allowing for ties and their applications in hydrology},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust estimation and regression with parametric quantile
functions. <em>CSDA</em>, <em>171</em>, 107471. (<a
href="https://doi.org/10.1016/j.csda.2022.107471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new, broad family of quantile-based estimators is described, and theoretical and empirical evidence is provided for their robustness to outliers in the response. The proposed method can be used to estimate all types of parameters, including location, scale, rate and shape parameters, extremes, regression coefficients and hazard ratios, and can be extended to censored and truncated data . The described estimator can be utilized to construct robust versions of common parametric and semiparametric methods, such as linear (Normal) regression, generalized linear models , and proportional hazards models . A variety of significant results and applications is presented to show the flexibility of the proposed approach. The R package Qest implements the estimator and provides the necessary functions for model building, prediction, and inference.},
  archive      = {J_CSDA},
  author       = {Gianluca Sottile and Paolo Frumento},
  doi          = {10.1016/j.csda.2022.107471},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107471},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust estimation and regression with parametric quantile functions},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network inference from temporally dependent grouped
observations. <em>CSDA</em>, <em>171</em>, 107470. (<a
href="https://doi.org/10.1016/j.csda.2022.107470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social network analysis , the observed data usually reflect certain social behaviors, such as the formation of groups, rather than an explicit network structure. Zhao and Weko proposed a model-based approach called the hub model to infer implicit networks from grouped observations ( Zhao and Weko, 2019 ). The hub model assumes independence between groups, which sometimes is not valid in practice. The hub model is generalized into the case of grouped observations with temporal dependence . As in the hub model, the group at each time point is gathered under one leader in the new model. Unlike in the hub model, the group leaders are not sampled independently but follow a Markov chain , and other members in adjacent groups can also be correlated. An expectation-maximization (EM) algorithm is developed for this model and a polynomial-time algorithm is proposed for the E-step. The performance of the new model is evaluated under different simulation settings. The proposed model is applied to a data set of the Kibale Chimpanzee Project.},
  archive      = {J_CSDA},
  author       = {Yunpeng Zhao},
  doi          = {10.1016/j.csda.2022.107470},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107470},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Network inference from temporally dependent grouped observations},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interaction forests: Identifying and exploiting
interpretable quantitative and qualitative interaction effects.
<em>CSDA</em>, <em>171</em>, 107460. (<a
href="https://doi.org/10.1016/j.csda.2022.107460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although interaction effects can be exploited to improve predictions and allow for valuable insights into covariate interplay , they are given limited attention in analysis. Interaction forests are a variant of random forests for categorical, continuous, and survival outcomes that explicitly models quantitative and qualitative interaction effects in bivariable splits performed by the trees constituting the forests. The new effect importance measure (EIM) associated with interaction forests allows for ranking of covariate pairs with respect to their interaction effects&#39; importance to prediction. Using EIM, separate importance value lists for univariable effects, quantitative interaction effects, and qualitative interaction effects are obtained. In the spirit of interpretable machine learning , the bivariable split types of interaction forests target easily interpretable and communicable interaction effects. To learn about the nature of the interplay between covariates identified as interacting it is convenient to visualise their estimated bivariable influence. Functions that perform this task are provided in the R package diversityForest , which implements interaction forests. In a large-scale empirical study using 220 data sets, interaction forests tended to deliver better predictions than conventional random forests and competing random forest variants that use multivariable splitting. In a simulation study, EIM delivered considerably better rankings for the relevant quantitative and qualitative interaction effects than competing approaches. These results indicate that interaction forests are suitable tools for the challenging task of identifying and making use of easily interpretable and communicable interaction effects in predictive modelling .},
  archive      = {J_CSDA},
  author       = {Roman Hornung and Anne-Laure Boulesteix},
  doi          = {10.1016/j.csda.2022.107460},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107460},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Interaction forests: Identifying and exploiting interpretable quantitative and qualitative interaction effects},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic covariance estimation by gaussian random
perturbation. <em>CSDA</em>, <em>171</em>, 107459. (<a
href="https://doi.org/10.1016/j.csda.2022.107459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most cases, the asymptotic covariance matrix of an M -estimator is in a sandwich form. This sandwich form involves calculations of the first and second order derivatives of the loss function, which is intractable if the loss function is complex. To alleviate this problem, we propose in this article a novel method called Gaussian random perturbation . This method can be used to estimate the asymptotic covariance matrix of a general M -estimator without derivative calculations. The idea can be summarized as follows. We first generate a small random perturbation around the M -estimator. Then, we re-evaluate the loss function at the randomly perturbed M -estimator and obtain the estimators of the first and second order derivatives of the loss function via Taylor series expansion . This leads to a novel estimator for the asymptotic covariance matrix. We then rigorously show that the resulting covariance estimator is statistically consistent with two elegant characteristics. First, it involves no computation of derivatives. This makes it easier to estimate the covariance matrix of an M -estimator with a complex loss function. Second, it is convenient for parallel computing and thus attractive for massive data analysis. The consistency of the proposed asymptotic covariance estimator is demonstrated under appropriate regularity conditions . The practical usefulness of the method is further demonstrated with both simulation studies and real data analysis.},
  archive      = {J_CSDA},
  author       = {Jing Zhou and Wei Lan and Hansheng Wang},
  doi          = {10.1016/j.csda.2022.107459},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107459},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Asymptotic covariance estimation by gaussian random perturbation},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter estimation and model-based clustering with
spherical normal distribution on the unit hypersphere. <em>CSDA</em>,
<em>171</em>, 107457. (<a
href="https://doi.org/10.1016/j.csda.2022.107457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In directional statistics, the von Mises-Fisher (vMF) distribution is one of the most basic and popular probability distributions for data on the unit hypersphere . Recently, the spherical normal (SN) distribution was proposed as an intrinsic counterpart to the vMF distribution by replacing the standard Euclidean norm with the great-circle distance, which is length of the shortest path joining two points on the unit sphere. Focusing on an isotropic version of SN distribution, it is shown that maximum likelihood estimators uniquely exist under mild support conditions. Since no analytic formula are available for the estimation, efficient numerical routines are proposed for parameter estimation. The estimation is considered in a general setting where non-negative weights are assigned to observations. This leads to a more interesting contribution for model-based clustering on the unit hypersphere by finite mixture model with SN distributions. Efficiency of optimization-based estimation procedures and effectiveness of SN mixture model are validated using simulated and real data examples.},
  archive      = {J_CSDA},
  author       = {Kisung You and Changhee Suh},
  doi          = {10.1016/j.csda.2022.107457},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107457},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parameter estimation and model-based clustering with spherical normal distribution on the unit hypersphere},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group linear non-gaussian component analysis with
applications to neuroimaging. <em>CSDA</em>, <em>171</em>, 107454. (<a
href="https://doi.org/10.1016/j.csda.2022.107454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent component analysis (ICA) is an unsupervised learning method popular in functional magnetic resonance imaging (fMRI). Group ICA has been used to search for biomarkers in neurological disorders including autism spectrum disorder and dementia. However, current methods use a principal component analysis (PCA) step that may remove low-variance features. Linear non-Gaussian component analysis (LNGCA) enables simultaneous dimension reduction and feature estimation including low-variance features in single-subject fMRI. A group LNGCA model is proposed to extract group components shared by more than one subject. Unlike group ICA methods, this novel approach also estimates individual (subject-specific) components orthogonal to the group components. To determine the total number of components in each subject, a parametric resampling test is proposed that samples spatially correlated Gaussian noise to match the spatial dependence observed in data. In simulations, estimated group components achieve higher accuracy compared to group ICA. The method is applied to a resting-state fMRI study on autism spectrum disorder in 342 children (252 typically developing, 90 with autism), where the group signals include resting-state networks. The discovered group components appear to exhibit different levels of temporal engagement in autism versus typically developing children, as revealed using group LNGCA. This novel approach to matrix decomposition is a promising direction for feature detection in neuroimaging.},
  archive      = {J_CSDA},
  author       = {Yuxuan Zhao and David S. Matteson and Stewart H. Mostofsky and Mary Beth Nebel and Benjamin B. Risk},
  doi          = {10.1016/j.csda.2022.107454},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107454},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Group linear non-gaussian component analysis with applications to neuroimaging},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2-d rayleigh autoregressive moving average model for SAR
image modeling. <em>CSDA</em>, <em>171</em>, 107453. (<a
href="https://doi.org/10.1016/j.csda.2022.107453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-dimensional (2-D) autoregressive moving average (ARMA) models are commonly applied to describe real-world image data, usually assuming Gaussian or symmetric noise. However, real-world data often present non-Gaussian signals, with asymmetrical distributions and strictly positive values. In particular, SAR images are known to be well characterized by the Rayleigh distribution. In this context, the ARMA model tailored for 2-D Rayleigh-distributed data is introduced—the 2-D RARMA model. The 2-D RARMA model is derived and conditional likelihood inferences are discussed. The proposed model was submitted to extensive Monte Carlo simulations to evaluate the performance of the conditional maximum likelihood estimators . Moreover, in the context of SAR image processing, two comprehensive numerical experiments were performed comparing anomaly detection and image modeling results of the proposed model with traditional 2-D ARMA models and competing methods in the literature.},
  archive      = {J_CSDA},
  author       = {Bruna G. Palm and Fábio M. Bayer and Renato J. Cintra},
  doi          = {10.1016/j.csda.2022.107453},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107453},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {2-D rayleigh autoregressive moving average model for SAR image modeling},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing minimum-volume enclosing ellipsoids for large
datasets. <em>CSDA</em>, <em>171</em>, 107452. (<a
href="https://doi.org/10.1016/j.csda.2022.107452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm is provided for calculating the minimum-volume enclosing ellipsoid (MVEE) for a large dataset stored in a separate database, for which the existing algorithms run out of memory or become prohibitively slow. The focus is on tall datasets, i.e., those consisting of huge numbers of data points of moderate dimensionality. The proposed Big Index Batching algorithm works in an optimization-deletion-adaptation cycle that consists of: using an existing algorithm by applying it on a smaller batch of data; pruning the vector of the indices of the data points by removing the points that are guaranteed to not lie on the boundary of the MVEE; and efficiently adapting the choice of the batch. The algorithm is provably convergent, and simple to describe and implement. The reading of tall data from the database is very time consuming, therefore the amount of reading during an MVEE computation should be as small as possible. It is shown on examples that Big Index Batching tends to find the MVEE after reading all data points just two or three times. As a consequence, the proposed algorithm usually converges to the MVEE reasonably fast. Its usefulness in robust statistics and anomaly detection is demonstrated by finding the potential outliers in a large dataset by using so-called ellipsoidal trimming.},
  archive      = {J_CSDA},
  author       = {Samuel Rosa and Radoslav Harman},
  doi          = {10.1016/j.csda.2022.107452},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107452},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Computing minimum-volume enclosing ellipsoids for large datasets},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate cluster-weighted models based on seemingly
unrelated linear regression. <em>CSDA</em>, <em>171</em>, 107451. (<a
href="https://doi.org/10.1016/j.csda.2022.107451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class of cluster-weighted models for a vector of continuous random variables is proposed. This class provides an extension to cluster-weighted modelling of multivariate and correlated responses that let the researcher free to use a different vector of covariates for each response. The class also includes parsimonious models obtained by imposing suitable constraints on the component-covariance matrices of either the responses or the covariates . Conditions for model identifiability are illustrated and discussed. Maximum likelihood estimation is carried out by means of an expectation-conditional maximisation algorithm. The effectiveness and usefulness of the proposed models are shown through the analysis of simulated and real datasets.},
  archive      = {J_CSDA},
  author       = {Cecilia Diani and Giuliano Galimberti and Gabriele Soffritti},
  doi          = {10.1016/j.csda.2022.107451},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107451},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multivariate cluster-weighted models based on seemingly unrelated linear regression},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid maximum likelihood inference for stochastic block
models. <em>CSDA</em>, <em>171</em>, 107449. (<a
href="https://doi.org/10.1016/j.csda.2022.107449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic block models have known a flowering interest in the social network literature. They provide a tool for discovering communities and identifying clusters of individuals characterized by similar social behaviors. In this framework, full maximum likelihood estimates are not achievable due to the intractability of the likelihood function. For this reason, several approximate solutions are available in the literature. In this respect, a new and more efficient approximate method for estimating model parameters is introduced. This has a hybrid nature, in the sense that it exploits different features of existing methods. The proposal is illustrated by an intensive Monte Carlo simulation study and an application to a real-world network.},
  archive      = {J_CSDA},
  author       = {Maria Francesca Marino and Silvia Pandolfi},
  doi          = {10.1016/j.csda.2022.107449},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107449},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hybrid maximum likelihood inference for stochastic block models},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Construction of symmetric orthogonal designs with deep
q-network and orthogonal complementary design. <em>CSDA</em>,
<em>171</em>, 107448. (<a
href="https://doi.org/10.1016/j.csda.2022.107448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction of orthogonal designs (ODs) has received much attention over the past decades, where previous work was originated from either mathematical theory or algorithmic search. A new algorithm is proposed to construct symmetric ODs. It is established on a well-designed framework of sequential construction, combining the deep Q-network (DQN) and orthogonal complementary design (OCD). The DQN-OCD algorithm shows its superiority by constructing various non-isomorphic ODs in an efficient manner. In particular, the constructions of symmetric ODs, including the saturated ODs L 27 ( 3 13 ) L27(313) , L 28 ( 2 27 ) L28(227) and non-saturated ODs L 18 ( 3 7 ) L18(37) , L 36 ( 3 13 ) L36(313) are presented, where the performance of DQN-OCD algorithm surpasses the others. Furthermore, a series of previously unknown ODs in non-isomorphic subclasses of L 28 ( 2 27 ) L28(227) and L 36 ( 3 13 ) L36(313) are constructed as new collections of ODs.},
  archive      = {J_CSDA},
  author       = {Jianfa Lai and Lin-Chen Weng and Xiaoling Peng and Kai-Tai Fang},
  doi          = {10.1016/j.csda.2022.107448},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107448},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Construction of symmetric orthogonal designs with deep Q-network and orthogonal complementary design},
  volume       = {171},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dealing with overdispersion in multivariate count data.
<em>CSDA</em>, <em>170</em>, 107447. (<a
href="https://doi.org/10.1016/j.csda.2022.107447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of overdispersion in multivariate count data is a challenging issue. It covers a central role mainly due to the relevance of modern technology-based data, such as Next Generation Sequencing and textual data from the web or digital collections. A comprehensive analysis of the likelihood-based models for extra-variation data is presented. Particular attention is paid to the models feasible for high-dimensional data. A new approach together with its parametric-estimation procedure is proposed. It can be viewed as a deeper version of the Dirichlet-Multinomial distribution and it leads to important results allowing to get a better approximation of the observed variability. A significative comparison of the proposed model and existing strategies is made through two different simulation studies and an empirical data set, that confirm a better capability to describe overdispersion .},
  archive      = {J_CSDA},
  author       = {Noemi Corsini and Cinzia Viroli},
  doi          = {10.1016/j.csda.2022.107447},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107447},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dealing with overdispersion in multivariate count data},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian spatio-temporal models for stream networks.
<em>CSDA</em>, <em>170</em>, 107446. (<a
href="https://doi.org/10.1016/j.csda.2022.107446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal models are widely used in many research areas including ecology. The recent proliferation of the use of in-situ sensors in streams and rivers supports space-time water quality modelling and monitoring in near real-time. A new family of spatio-temporal models is introduced. These models incorporate spatial dependence using stream distance while temporal autocorrelation is captured using vector autoregression approaches. Several variations of these novel models are proposed using a Bayesian framework. The results show that our proposed models perform well using spatio-temporal data collected from real stream networks, particularly in terms of out-of-sample RMSPE . This is illustrated considering a case study of water temperature data in the northwestern United States.},
  archive      = {J_CSDA},
  author       = {Edgar Santos-Fernandez and Jay M. Ver Hoef and Erin E. Peterson and James McGree and Daniel J. Isaak and Kerrie Mengersen},
  doi          = {10.1016/j.csda.2022.107446},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107446},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian spatio-temporal models for stream networks},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust modeling of multivariate longitudinal data using
modified cholesky and hypersphere decompositions. <em>CSDA</em>,
<em>170</em>, 107439. (<a
href="https://doi.org/10.1016/j.csda.2022.107439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the convenience of the statistical interpretation and parameter estimation, a normal distribution is typically assumed for multivariate longitudinal data analysis. However, this assumption may be questionable in practice, because it is possible that outliers exist or that the underlying data will show heavy tails. In addition, since the covariance matrix should explain complex correlation structures , it must be positive-definite, and as it is also high-dimensional, the modeling of the covariance matrix is not easy. To solve these problems, we propose the robust modeling of multivariate longitudinal data by considering multivariate t distribution, and we exploit modified Cholesky and hypersphere decompositions to model the covariance matrix. The estimation of the models is shown to be robust when the data include outliers and exhibit heavy tails. The performance of our proposed model and algorithm is illustrated using a nonalcoholic fatty liver disease data set and several simulation studies.},
  archive      = {J_CSDA},
  author       = {Anbin Rhee and Min-Sun Kwak and Keunbaik Lee},
  doi          = {10.1016/j.csda.2022.107439},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107439},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust modeling of multivariate longitudinal data using modified cholesky and hypersphere decompositions},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast approximate EM algorithm for joint models of survival
and multivariate longitudinal data. <em>CSDA</em>, <em>170</em>, 107438.
(<a href="https://doi.org/10.1016/j.csda.2022.107438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models are an increasingly popular way to characterise the relationship between one or more longitudinal responses and an event of interest. However, for multivariate joint models the increased dimensionality and complexity of random effects present in the model specification are commensurate with increased computing time, hampering the implementation of many classic approaches. An approximate EM algorithm which ameliorates the so-called ‘curse of dimensionality’ is developed. The scaleability and accuracy of the proposed method are demonstrated via two simulation studies and applied to data arising from two clinical trials in the disease areas of cirrhosis and Alzheimer&#39;s disease, each with three biomarkers.},
  archive      = {J_CSDA},
  author       = {James Murray and Pete Philipson},
  doi          = {10.1016/j.csda.2022.107438},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107438},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A fast approximate EM algorithm for joint models of survival and multivariate longitudinal data},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric feature selection by random forests and deep
neural networks. <em>CSDA</em>, <em>170</em>, 107436. (<a
href="https://doi.org/10.1016/j.csda.2022.107436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forests are a widely used machine learning algorithm, but their computational efficiency is undermined when applied to large-scale datasets with numerous instances and useless features. Herein, we propose a nonparametric feature selection algorithm that incorporates random forests and deep neural networks, and its theoretical properties are also investigated under regularity conditions . Using different synthetic models and a real-world example, we demonstrate the advantage of the proposed algorithm over other alternatives in terms of identifying useful features, avoiding useless ones, and the computation efficiency. Although the algorithm is proposed using standard random forests, it can be widely adapted to other machine learning algorithms, as long as features can be sorted accordingly.},
  archive      = {J_CSDA},
  author       = {Xiaojun Mao and Liuhua Peng and Zhonglei Wang},
  doi          = {10.1016/j.csda.2022.107436},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107436},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric feature selection by random forests and deep neural networks},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the use of random forest for two-sample testing.
<em>CSDA</em>, <em>170</em>, 107435. (<a
href="https://doi.org/10.1016/j.csda.2022.107435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the line of classification-based two-sample testing, tests based on the Random Forest classifier are proposed. The developed tests are easy to use, require almost no tuning, and are applicable for any distribution on R d Rd . Furthermore, the built-in variable importance measure of the Random Forest gives potential insights into which variables make out the difference in distribution. An asymptotic power analysis for the proposed tests is conducted. Finally, two real-world applications illustrate the usefulness of the introduced methodology. To simplify the use of the method, the R-package “hypoRF” is provided.},
  archive      = {J_CSDA},
  author       = {Simon Hediger and Loris Michel and Jeffrey Näf},
  doi          = {10.1016/j.csda.2022.107435},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107435},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On the use of random forest for two-sample testing},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparison of single and multiple changepoint techniques
for time series data. <em>CSDA</em>, <em>170</em>, 107433. (<a
href="https://doi.org/10.1016/j.csda.2022.107433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated time series data arise in many applications. This paper describes and compares several prominent single and multiple changepoint techniques for correlated time series. In the single changepoint problem, various cumulative sum (CUSUM) and likelihood ratio statistics , along with boundary cropping scenarios and scaling methods (e.g., scaling to an extreme value or Brownian Bridge limit) are compared. A recently developed test based on summing squared CUSUM statistics over all time indices is shown to have controlled Type I error and superior detection power. In the multiple changepoint setting, penalized likelihoods drive the discourse, with AIC, BIC, mBIC , and MDL penalties being considered. Binary and wild binary segmentation techniques are also compared. A new distance metric is introduced that measures differences between two multiple changepoint segmentations. Algorithmic and computational concerns are discussed and simulations are given to support all conclusions. In the end, the multiple changepoint setting admits no clear methodological winner, performance depending on the particular scenario. Nonetheless, some practical guidance emerges.},
  archive      = {J_CSDA},
  author       = {Xuesheng Shi and Colin Gallagher and Robert Lund and Rebecca Killick},
  doi          = {10.1016/j.csda.2022.107433},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107433},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A comparison of single and multiple changepoint techniques for time series data},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ℓ0-regularized high-dimensional accelerated failure time
model. <em>CSDA</em>, <em>170</em>, 107430. (<a
href="https://doi.org/10.1016/j.csda.2022.107430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a constructive approach for ℓ 0 ℓ0 -penalized estimation in the sparse accelerated failure time (AFT) model with high-dimensional covariates . The proposed approach is based on Stute&#39;s weighted least squares criterion combined with ℓ 0 ℓ0 -penalization. This method is a computational algorithm that generates a sequence of solutions iteratively, based on active sets derived from primal and dual information and root finding according to the Karush-Kuhn-Tucker (KKT) conditions. We refer to the proposed method as AFT-SDAR (for support detection and root finding). An important aspect of our theoretical results is that we directly concern the sequence of solutions generated based on the AFT-SDAR algorithm. We prove that the estimation errors of the solution sequence decay exponentially to the optimal error bound with high probability , as long as the covariate matrix satisfies a mild regularity condition which is necessary and sufficient for model identification even in the setting of high-dimensional linear regression. An adaptive version of AFT-SDAR is also proposed, i.e., AFT-ASDAR, which determines the support size of the estimated coefficient in a data-driven fashion. Simulation studies demonstrate the superior performance of the proposed method over the lasso and MCP in terms of accuracy and speed. The application of the proposed method is also illustrated by analyzing a real data set .},
  archive      = {J_CSDA},
  author       = {Chao Cheng and Xingdong Feng and Jian Huang and Yuling Jiao and Shuang Zhang},
  doi          = {10.1016/j.csda.2022.107430},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107430},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {ℓ0-regularized high-dimensional accelerated failure time model},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A latent space model for multilayer network data.
<em>CSDA</em>, <em>169</em>, 107432. (<a
href="https://doi.org/10.1016/j.csda.2022.107432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian statistical model to simultaneously characterize two or more social networks defined over a common set of actors is proposed. The key feature of the model is a hierarchical prior distribution that allows the user to represent the entire system jointly, achieving a compromise between dependent and independent networks. Among others things, such a specification provides an easy way to visualize multilayer network data in a low-dimensional Euclidean space, generate a weighted network that reflects the consensus affinity between actors, establish a measure of correlation between networks, assess cognitive judgments that subjects form about the relationships among actors, and perform clustering tasks at different social instances. The model&#39;s capabilities are illustrated using real-world and synthetic datasets , taking into account different types of actors, sizes, and relations.},
  archive      = {J_CSDA},
  author       = {Juan Sosa and Brenda Betancourt},
  doi          = {10.1016/j.csda.2022.107432},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107432},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A latent space model for multilayer network data},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel-based hidden markov conditional densities.
<em>CSDA</em>, <em>169</em>, 107431. (<a
href="https://doi.org/10.1016/j.csda.2022.107431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A natural way to obtain conditional density estimates for time series processes is to adopt a kernel-based (nonparametric) conditional density estimation (KCDE) method. To this end, the data generating process is commonly assumed to be Markovian of finite order. Markov processes , however, have limited memory range so that only the most recent observations are informative for estimating future observations, assuming the underlying model is known. Hidden Markov models (HMMs), on the other hand, can integrate information over arbitrary lengths of time and thus describe a wider variety of data generating processes. The KCDE and HMMs are combined into one method. The resulting KCDE-HMM method is described in detail, and an iterative algorithm is presented for estimating its transition probabilities , weights and bandwidths. Consistency and asymptotic normality of the resulting conditional density estimator are proved. The conditional forecast ability of the proposed conditional density method is examined and compared via a rolling forecasting window with three benchmark methods: HMM, autoregressive HMM, and KCDE-MM. Large-sample performance of the above conditional estimation methods as a function of training data size is explored. Finally, the methods are applied to the U.S. Industrial Production series and the S&amp;P 500 index. The results indicate that KCDE-HMM outperforms the benchmark methods for moderate-to-large sample sizes, irrespective of the number of hidden states considered.},
  archive      = {J_CSDA},
  author       = {Jan G. De Gooijer and Gustav Eje Henter and Ao Yuan},
  doi          = {10.1016/j.csda.2022.107431},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107431},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Kernel-based hidden markov conditional densities},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank matrix denoising for count data using unbiased
kullback-leibler risk estimation. <em>CSDA</em>, <em>169</em>, 107423.
(<a href="https://doi.org/10.1016/j.csda.2022.107423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical studies are concerned with the analysis of observations organized in a matrix form whose elements are count data. When these observations are assumed to follow a Poisson or a multinomial distribution , it is of interest to focus on the estimation of either the intensity matrix (Poisson case) or the compositional matrix (multinomial case) when it is assumed to have a low rank structure. In this setting, it is proposed to construct an estimator minimizing the regularized negative log-likelihood by a nuclear norm penalty. Such an approach easily yields a low-rank matrix-valued estimator with positive entries which belongs to the set of row-stochastic matrices in the multinomial case. Then, as a main contribution, a data-driven procedure is constructed to select the regularization parameter in the construction of such estimators by minimizing (approximately) unbiased estimates of the Kullback-Leibler (KL) risk in such models, which generalize Stein&#39;s unbiased risk estimation originally proposed for Gaussian data. The evaluation of these quantities is a delicate problem, and novel methods are introduced to obtain accurate numerical approximation of such unbiased estimates. Simulated data are used to validate this way of selecting regularizing parameters for low-rank matrix estimation from count data. For data following a multinomial distribution , the performances of this approach are also compared to K -fold cross-validation. Examples from a survey study and metagenomics also illustrate the benefits of this methodology for real data analysis.},
  archive      = {J_CSDA},
  author       = {Jérémie Bigot and Charles Deledalle},
  doi          = {10.1016/j.csda.2022.107423},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107423},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Low-rank matrix denoising for count data using unbiased kullback-leibler risk estimation},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computation of quantile sets for bivariate ordered data.
<em>CSDA</em>, <em>169</em>, 107422. (<a
href="https://doi.org/10.1016/j.csda.2022.107422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms are proposed for the computation of set-valued quantiles and the values of the lower cone distribution function for bivariate data sets. These new objects make data analysis possible involving an order relation for the data points in form of a vector order in two dimensions . The bivariate case deserves special attention since two-dimensional vector orders are much simpler to handle than such orders in higher dimensions . Several examples illustrate how the algorithms work and what kind of conclusions can be drawn with the proposed approach. As a new feature, it is observed that the computational effort depends on how much the original data points are aligned with respect to the vector order.},
  archive      = {J_CSDA},
  author       = {Andreas H. Hamel and Daniel Kostner},
  doi          = {10.1016/j.csda.2022.107422},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107422},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Computation of quantile sets for bivariate ordered data},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Truncated estimation in functional generalized linear
regression models. <em>CSDA</em>, <em>169</em>, 107421. (<a
href="https://doi.org/10.1016/j.csda.2022.107421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional generalized linear models investigate the effect of functional predictors on a scalar response. An interesting case is when the functional predictor is thought to exert an influence on the conditional mean of the response only through its values up to a certain point in the domain. In the literature, models with this type of restriction on the functional effect have been termed truncated or historical regression models. A penalized likelihood estimator is formulated by combining a structured variable selection method with a localized B-spline expansion of the regression coefficient function . In addition to a smoothing penalty that is typical for functional regression, a nested group lasso penalty is also included which guarantees the sequential entering of B-splines and thus induces the desired truncation on the estimator. An optimization scheme is developed to compute the solution path efficiently when varying the truncation tuning parameter. The convergence rate of the coefficient function estimator and consistency of the truncation point estimator are given under suitable smoothness assumptions . The proposed method is demonstrated through simulations and an application involving the effects of blood pressure values in patients who suffered a spontaneous intracerebral hemorrhage.},
  archive      = {J_CSDA},
  author       = {Xi Liu and Afshin A. Divani and Alexander Petersen},
  doi          = {10.1016/j.csda.2022.107421},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107421},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Truncated estimation in functional generalized linear regression models},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial two-stage designs for phase II clinical trials.
<em>CSDA</em>, <em>169</em>, 107420. (<a
href="https://doi.org/10.1016/j.csda.2021.107420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common endpoint in a single-arm phase II study is tumor response as a binary variable. Two widely used designs for such a study are Simon&#39;s two-stage minimax and optimal designs. The minimax design minimizes the maximal sample size and the optimal design minimizes the expected sample size under the null hypothesis. The optimal design generally has the larger total sample size than the minimax design, but its first stage&#39;s sample size is smaller than that of the minimax design. The difference in the total sample size between two types of designs can be large and so both designs can be unappealing to investigators. We develop novel designs that compromise on the two optimality criteria and avoid such occurrences using the spatial information on the first stage&#39;s required sample size and the total required sample size. We study properties of these spatial designs and show our proposed designs have advantages over Simon&#39;s designs and one of its extensions by Lin and Shih. As applications, we construct spatial designs for real-life studies on patients with Hodgkin disease and another study on effect of head and neck cancer on apnea.},
  archive      = {J_CSDA},
  author       = {Seongho Kim and Weng Kee Wong},
  doi          = {10.1016/j.csda.2021.107420},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107420},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Spatial two-stage designs for phase II clinical trials},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed adaptive huber regression. <em>CSDA</em>,
<em>169</em>, 107419. (<a
href="https://doi.org/10.1016/j.csda.2021.107419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed data naturally arise in scenarios involving multiple sources of observations, each stored at a different location. Directly pooling all the data together is often prohibited due to limited bandwidth and storage, or due to privacy protocols. A new robust distributed algorithm is introduced for fitting linear regressions when data are subject to heavy-tailed and/or asymmetric errors with finite second moments. The algorithm only communicates gradient information at each iteration, and therefore is communication-efficient. To achieve the bias-robustness tradeoff, the key is a novel double-robustification approach that applies on both the local and global objective functions. Statistically, the resulting estimator achieves the centralized nonasymptotic error bound as if all the data were pooled together and came from a distribution with sub-Gaussian tails. Under a finite ( 2 + δ ) (2+δ) -th moment condition, a Berry-Esseen bound for the distributed estimator is established, based on which robust confidence intervals are constructed. In high dimensions , the proposed doubly-robustified loss function is complemented with ℓ 1 ℓ1 -penalization for fitting sparse linear models with distributed data. Numerical studies further confirm that compared with extant distributed methods, the proposed methods achieve near-optimal accuracy with low variability and better coverage with tighter confidence width.},
  archive      = {J_CSDA},
  author       = {Jiyu Luo and Qiang Sun and Wen-Xin Zhou},
  doi          = {10.1016/j.csda.2021.107419},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107419},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Distributed adaptive huber regression},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference for high-dimensional pathway analysis
with multiple responses. <em>CSDA</em>, <em>169</em>, 107418. (<a
href="https://doi.org/10.1016/j.csda.2021.107418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathway analysis, i.e., grouping analysis, has important applications in genomic studies. Existing pathway analysis approaches are mostly focused on a single response and are not suitable for analyzing complex diseases that are often related with multiple response variables. Although a handful of approaches have been developed for multiple responses, these methods are mainly designed for pathways with a moderate number of features. A multi-response pathway analysis approach that is able to conduct statistical inference when the dimension is potentially higher than sample size is introduced. Asymptotical properties of the test statistic are established and theoretical investigation of the statistical power is conducted. Simulation studies and real data analysis show that the proposed approach performs well in identifying important pathways that influence multiple expression quantitative trait loci (eQTL).},
  archive      = {J_CSDA},
  author       = {Yang Liu and Wei Sun and Li Hsu and Qianchuan He},
  doi          = {10.1016/j.csda.2021.107418},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107418},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical inference for high-dimensional pathway analysis with multiple responses},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate bayesian conditional copulas. <em>CSDA</em>,
<em>169</em>, 107417. (<a
href="https://doi.org/10.1016/j.csda.2021.107417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copula models are flexible tools to represent complex structures of dependence for multivariate random variables . According to Sklar&#39;s theorem, any multidimensional absolutely continuous distribution function can be uniquely represented as a copula, i.e. a joint cumulative distribution function on the unit hypercube with uniform marginals, which captures the dependence structure among the vector components. In real data applications, the interest of the analyses often lies on specific functionals of the dependence, which quantify aspects of it in a few numerical values. A broad literature exists on such functionals, however extensions to include covariates are still limited. This is mainly due to the lack of unbiased estimators of the conditional copula, especially when one does not have enough information to select the copula model. Several Bayesian methods to approximate the posterior distribution of functionals of the dependence varying according covariates are presented and compared; the main advantage of the investigated methods is that they use nonparametric models , avoiding the selection of the copula, which is usually a delicate aspect of copula modelling. These methods are compared in simulation studies and in two realistic applications, from civil engineering and astrophysics.},
  archive      = {J_CSDA},
  author       = {Clara Grazian and Luciana Dalla Valle and Brunero Liseo},
  doi          = {10.1016/j.csda.2021.107417},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107417},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Approximate bayesian conditional copulas},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust subset selection. <em>CSDA</em>, <em>169</em>,
107415. (<a href="https://doi.org/10.1016/j.csda.2021.107415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The best subset selection (or “best subsets”) estimator is a classic tool for sparse regression, and developments in mathematical optimization over the past decade have made it more computationally tractable than ever. Notwithstanding its desirable statistical properties, the best subsets estimator is susceptible to outliers and can break down in the presence of a single contaminated data point. To address this issue, a robust adaption of best subsets is proposed that is highly resistant to contamination in both the response and the predictors. The adapted estimator generalizes the notion of subset selection to both predictors and observations, thereby achieving robustness in addition to sparsity . This procedure, referred to as “robust subset selection” (or “robust subsets”), is defined by a combinatorial optimization problem for which modern discrete optimization methods are applied. The robustness of the estimator in terms of the finite-sample breakdown point of its objective value is formally established. In support of this result, experiments on synthetic and real data are reported that demonstrate the superiority of robust subsets over best subsets in the presence of contamination. Importantly, robust subsets fares competitively across several metrics compared with popular robust adaptions of continuous shrinkage estimators .},
  archive      = {J_CSDA},
  author       = {Ryan Thompson},
  doi          = {10.1016/j.csda.2021.107415},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107415},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust subset selection},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiclass-penalized logistic regression. <em>CSDA</em>,
<em>169</em>, 107414. (<a
href="https://doi.org/10.1016/j.csda.2021.107414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multinomial logistic regression model that penalizes the number of class-specific parameters is proposed. The number of parameters in a standard multinomial regression model increases linearly with the number of classes and number of explanatory variables . The multiclass-penalized regression model clusters parameters together by penalizing the differences between class-specific parameter vectors , instead of penalizing the number of explanatory variables . The model provides interpretable parameter estimates, even in settings with many classes. An algorithm for maximum likelihood estimation in the multiclass-penalized regression model is discussed. Applications to simulated and real data show in- and out-of-sample improvements in performance relative to a standard multinomial regression model.},
  archive      = {J_CSDA},
  author       = {Didier Nibbering and Trevor J. Hastie},
  doi          = {10.1016/j.csda.2021.107414},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107414},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multiclass-penalized logistic regression},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Test-inversion confidence intervals for estimands in
contingency tables subject to equality constraints. <em>CSDA</em>,
<em>169</em>, 107413. (<a
href="https://doi.org/10.1016/j.csda.2021.107413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of test-inversion approximate confidence intervals is explored for estimands in contingency tables subject to equality constraints. Recommended test statistics include the difference in G 2 G2 statistic and nested versions of a family of power-divergence statistics. Efficient and robust computational algorithms are proposed. The computational approach herein is applicable for a broadened class of estimands and constraints: (1) Compared with existing standard methods, which are applicable only for likelihood-explicit estimands, our algorithms can also handle likelihood-implicit estimands, where the log-likelihood cannot be reparameterized in terms of the estimand of interest and a collection of nuisance parameters ; (2) Only mild conditions on equality constraints are required, and it is unnecessary to re-express the constraints as a generalized linear model . A simulation study highlights the advantages of using likelihood-ratio intervals rather than bootstrap and Wald intervals, especially when cell counts are small and/or the true estimand is close to the boundary. In addition, appropriate loss functions are proposed to investigate efficiency gain upon imposing constraints. Examples are presented to illustrate the appropriateness of imposing constraints and the utility of test-inversion intervals.},
  archive      = {J_CSDA},
  author       = {Qiansheng Zhu and Joseph B. Lang},
  doi          = {10.1016/j.csda.2021.107413},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107413},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Test-inversion confidence intervals for estimands in contingency tables subject to equality constraints},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On MCMC sampling in self-exciting integer-valued threshold
time series models. <em>CSDA</em>, <em>169</em>, 107410. (<a
href="https://doi.org/10.1016/j.csda.2021.107410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov Chain Monte Carlo (MCMC) methods have been shown to be a useful tool in many branches in statistics . However, due to the complex structure of the models, this method remains an open problem for threshold integer-valued time series models . This study develops Bayesian inference for a class of self-exciting integer-valued threshold autoregressive models , which is implemented by means of a new MCMC algorithm. By introducing the latent variables series, a complete data likelihood is obtained. Based on which, the full conditional distributions are easily obtained with familiar forms. Furthermore, by maximizing the complete data likelihood , the threshold parameter is also accurately estimated. Finally, the performance of the MCMC algorithm is evaluated via some simulations and a real data example.},
  archive      = {J_CSDA},
  author       = {Kai Yang and Xinyang Yu and Qingqing Zhang and Xiaogang Dong},
  doi          = {10.1016/j.csda.2021.107410},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107410},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On MCMC sampling in self-exciting integer-valued threshold time series models},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational bayesian inference for network autoregression
models. <em>CSDA</em>, <em>169</em>, 107406. (<a
href="https://doi.org/10.1016/j.csda.2021.107406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a variational Bayesian (VB) approach for estimating large-scale dynamic network models in the network autoregression framework. The VB approach allows for the automatic identification of the dynamic structure of such a model and obtains a direct approximation of the posterior density. Compared to the Markov chain Monte Carlo (MCMC)-based sampling approaches, the VB approach achieves enhanced computational efficiency without sacrificing estimation accuracy. In a real data analysis scenario of day-ahead natural gas flow prediction in the German gas transmission network with 51 nodes between October 2013 and September 2015, the VB approach delivers promising forecasting accuracy along with clearly detected structures in terms of dynamic dependence.},
  archive      = {J_CSDA},
  author       = {Wei-Ting Lai and Ray-Bing Chen and Ying Chen and Thorsten Koch},
  doi          = {10.1016/j.csda.2021.107406},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107406},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variational bayesian inference for network autoregression models},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Missing link survival analysis with applications to
available pandemic data. <em>CSDA</em>, <em>169</em>, 107405. (<a
href="https://doi.org/10.1016/j.csda.2021.107405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is shown how to overcome a new missing data problem in survival analysis . Iterative nonparametric techniques are utilized and the missing data information is both estimated and used for further estimation in each iterative step. Theory is developed and a good finite sample performance is illustrated by simulations. The main motivation is an application to French data on the temporal development of the number of hospitalized Covid-19 patients.},
  archive      = {J_CSDA},
  author       = {María Luz Gámiz and Enno Mammen and María Dolores Martínez-Miranda and Jens Perch Nielsen},
  doi          = {10.1016/j.csda.2021.107405},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107405},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Missing link survival analysis with applications to available pandemic data},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of semi-varying coefficient models for
longitudinal data with irregular error structure. <em>CSDA</em>,
<em>169</em>, 107389. (<a
href="https://doi.org/10.1016/j.csda.2021.107389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric models are often considered for modeling longitudinal data for a good balance between flexibility and parsimony. In this paper, we focus on the estimation for a longitudinal semi-varying coefficient model which is of irregular errors. A semiparametric profile least-squares method is developed to estimate parameters in the mean function and error structure simultaneously. Then, a two-stage local linear estimator is investigated for the nonparametric part. Further, we establish the asymptotic properties of the resulting estimators under some mild conditions. The practical problems of implementation are also addressed. Finally, three numerical experiments are conducted to verify the finite sample performance of the proposed methods, and an application to the CD4 cell data is provided for illustration.},
  archive      = {J_CSDA},
  author       = {Yan-Yong Zhao and Jin-Guan Lin and Jian-Qiang Zhao and Zhang-Xiao Miao},
  doi          = {10.1016/j.csda.2021.107389},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107389},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of semi-varying coefficient models for longitudinal data with irregular error structure},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A likelihood-based boosting algorithm for factor analysis
models with binary data. <em>CSDA</em>, <em>168</em>, 107412. (<a
href="https://doi.org/10.1016/j.csda.2021.107412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical boosting represents a very effective method for fitting complex models, while performing variable selection and preventing overfitting at the same time. However, the available methods are not directly applicable to factor analysis models for binary data, since any gradient descent method is not able to move from the starting point with zero loadings. The proposed algorithm, exploiting the directions of negative curvature of the log-likelihood function, is able to escape from the regions of local non-convexity. The component-wise approach followed leads to a sparse solution , which has the advantage of facilitating the interpretation without requiring a posterior rotation of the loadings. The method also performs regularization of the estimates, hence reducing their mean square error . To lighten the computational burden of the inferential procedure, a suitable pseudolikelihood, called pairwise likelihood, is exploited. In addition, a group lasso penalty is considered in order to automatically select the number of latent variables included in the model. The good performance of the proposal is illustrated through a simulation study and a real-data example.},
  archive      = {J_CSDA},
  author       = {Michela Battauz and Paolo Vidoni},
  doi          = {10.1016/j.csda.2021.107412},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107412},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A likelihood-based boosting algorithm for factor analysis models with binary data},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TA algorithms for d-optimal OofA mixture designs.
<em>CSDA</em>, <em>168</em>, 107411. (<a
href="https://doi.org/10.1016/j.csda.2021.107411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a mixture experiment, m components are mixed to produce a response. The total amount of the mixture is a constant. This classical experiment has been studied for a long time, but little attention has been given to the addition order of the components. In an Order-of-Addition (OofA) Mixture experiment, the response depends on both the mixture proportions of components and their order of addition. The overall goal of the OofA Mixture experiment is to identify the addition order and mixture proportions that produce an optimal response. Methodology for constructing full OofA Mixture designs is discussed, but the size of these full designs increases rapidly as m increases. A Threshold Accepting (TA) algorithm is used to find a subset of n rows of the full OofA Mixture design that maximize the D-optimality criterion, reducing the number of required runs. Neighborhood structures are proposed for OofA simplex lattice and general mixture designs. The TA algorithm is compared with the well-known Fedorov algorithm, and recommendations for the use of this algorithm are provided.},
  archive      = {J_CSDA},
  author       = {Nicholas Rios and Peter Winker and Dennis K.J. Lin},
  doi          = {10.1016/j.csda.2021.107411},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107411},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {TA algorithms for D-optimal OofA mixture designs},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum likelihood estimation of diffusions by continuous
time markov chain. <em>CSDA</em>, <em>168</em>, 107408. (<a
href="https://doi.org/10.1016/j.csda.2021.107408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method is presented for estimating the parameters of a parametric diffusion process . The approach is based on a closed-form Maximum Likelihood estimator for an approximating Continuous Time Markov Chain (CTMC) of the diffusion process . Unlike typical time discretization approaches, such as pseudo-likelihood approximations with Shoji-Ozaki or Kessler&#39;s method, the CTMC approximation introduces no time-discretization error during parameter estimation, and is thus well-suited for typical econometric situations with infrequently sampled data. Due to the structure of the CTMC, closed-form approximations are obtained for the sample likelihood which hold for general univariate diffusions. Comparisons of the state-discretization approach with approximate MLE (time-discretization) and Exact MLE (when applicable) demonstrate favorable performance of the CTMC estimator. Simulated examples are provided in addition to real data experiments with FX rates and constant maturity interest rates.},
  archive      = {J_CSDA},
  author       = {J.L. Kirkby and Dang H. Nguyen and Duy Nguyen and Nhu N. Nguyen},
  doi          = {10.1016/j.csda.2021.107408},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107408},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Maximum likelihood estimation of diffusions by continuous time markov chain},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An objective bayes factor with improper priors.
<em>CSDA</em>, <em>168</em>, 107404. (<a
href="https://doi.org/10.1016/j.csda.2021.107404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new look at the use of improper priors in Bayes factors for model comparison is presented. As is well known, in such a case, the Bayes factor is only defined up to an arbitrary constant. Most current methods overcome the problem by using part of the sample to train the Bayes factor (Fractional Bayes Factor) or to transform the improper prior in to a proper distribution (Intrinsic Bayes Factors) and use the remainder of the sample for the model comparison. It is provided an alternative approach which relies on matching divergences between density functions so as to establish a value for the constant appearing in the Bayes factor. These are the Kullback–Leibler divergence and the Fisher information divergence; the latter being crucial as it does not depend on an unknown normalizing constant. Demonstrations of the performance of the proposed method are provided through numerous illustrations and comparisons, showing that the main advantage over existing ones is that it does not require any input from the experimenter; it is fully automated.},
  archive      = {J_CSDA},
  author       = {Cristiano Villa and Stephen G. Walker},
  doi          = {10.1016/j.csda.2021.107404},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107404},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An objective bayes factor with improper priors},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a family of two–piece circular distributions.
<em>CSDA</em>, <em>168</em>, 107403. (<a
href="https://doi.org/10.1016/j.csda.2021.107403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new way of constructing flexible and unimodal circular models, focusing on the modal direction, is proposed. Starting from a base symmetric density and a weight function, a two–piece four parameters density is introduced. The proposed density provides an extension of the base density to allow for sharply peaked and flat–topped unimodal distributions as well as a wide range of skewness. In particular, it generalizes some well–known peakedness–free models such as the Batschelet and Papakonstantinou densities. The four parameters of the model have a clear interpretation: modal direction, concentration, peakedness at the left and at the right of the modal direction. Symmetric submodels are obtained when the peakedness parameters are equal. The main properties related to the shape of the new density are presented and asymptotic results for maximum likelihood estimators are derived. An illustrative application concerning the flight orientation of migrating raptors is investigated.},
  archive      = {J_CSDA},
  author       = {Jose Ameijeiras-Alonso and Irène Gijbels and Anneleen Verhasselt},
  doi          = {10.1016/j.csda.2021.107403},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107403},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On a family of two–piece circular distributions},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional independence test of failure and truncation
times: Essential tool for method selection. <em>CSDA</em>, <em>168</em>,
107402. (<a href="https://doi.org/10.1016/j.csda.2021.107402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional independence assumption of truncation and failure times conditioning on covariates is a fundamental and common assumption in the regression analysis of left-truncated and right-censored data. Testing for this assumption is essential to ensure the correct inference on the failure time, but this has often been overlooked in the literature. With consideration of challenges caused by left truncation and right censoring, tests for this conditional independence assumption are developed in which the generalized odds ratio derived from a Cox proportional hazards model on the failure time and the concept of Kendall&#39;s tau are combined. Except for the Cox proportional hazards model , no additional model assumptions are imposed, and the distributions of the truncation time and conditioning variables are unspecified. The asymptotic properties of the test statistic are established and an easy implementation for obtaining its distribution is developed. The performance of the proposed test has been evaluated through simulation studies and two real studies.},
  archive      = {J_CSDA},
  author       = {Jing Ning and Daewoo Pak and Hong Zhu and Jing Qin},
  doi          = {10.1016/j.csda.2021.107402},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107402},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional independence test of failure and truncation times: Essential tool for method selection},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical test for anomalous diffusion based on empirical
anomaly measure for gaussian processes. <em>CSDA</em>, <em>168</em>,
107401. (<a href="https://doi.org/10.1016/j.csda.2021.107401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes with anomalous diffusion behavior are considered. A new statistical test for the model identification that is based on the empirical anomaly measure (EAM) is introduced. This measure is considered as the distance between the anomalous and normal diffusion. In particular, the main properties of the EAM based on the quadratic form representation of Gaussian processes are investigated. The effectiveness of the test is evaluated for the fractional Brownian motion . Theoretical results and simulation studies are supported by the analysis of experimental data describing the sub-diffusive motion of microspheres in agarose hydrogels.},
  archive      = {J_CSDA},
  author       = {Dawid Szarek and Katarzyna Maraj-Zygmąt and Grzegorz Sikora and Diego Krapf and Agnieszka Wyłomańska},
  doi          = {10.1016/j.csda.2021.107401},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107401},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical test for anomalous diffusion based on empirical anomaly measure for gaussian processes},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible quantile contour estimation for multivariate
functional data: Beyond convexity. <em>CSDA</em>, <em>168</em>, 107400.
(<a href="https://doi.org/10.1016/j.csda.2021.107400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, multivariate functional data are frequently observed in many scientific fields, and the estimation of quantiles of these data is essential in data analysis. Unlike in the univariate setting, quantiles are more challenging to estimate for multivariate data , let alone multivariate functional data. This article proposes a new method to estimate the quantiles for multivariate functional data with application to air pollution data. The proposed multivariate functional quantile model is a nonparametric, time-varying coefficient model , and basis functions are used for the estimation and prediction. The estimated quantile contours can account for non-Gaussian and even nonconvex features of the multivariate distributions marginally, and the estimated multivariate quantile function is a continuous function of time for a fixed quantile level. Computationally, the proposed method is shown to be efficient for both bivariate and trivariate functional data. The monotonicity, uniqueness, and consistency of the estimated multivariate quantile function have been established. The proposed method was demonstrated on bivariate and trivariate functional data in the simulation studies, and was applied to study the joint distribution of PM 2.5 PM2.5 and geopotential height over time in the Northeastern United States; the estimated contours highlight the nonconvex features of the joint distribution, and the functional quantile curves capture the dynamic change across time.},
  archive      = {J_CSDA},
  author       = {Gaurav Agarwal and Wei Tu and Ying Sun and Linglong Kong},
  doi          = {10.1016/j.csda.2021.107400},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107400},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Flexible quantile contour estimation for multivariate functional data: Beyond convexity},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doubly robust estimation in causal inference with missing
outcomes: With an application to the aerobics center longitudinal study.
<em>CSDA</em>, <em>168</em>, 107399. (<a
href="https://doi.org/10.1016/j.csda.2021.107399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the average treatment effect (ATE) and the average treatment effect on the treated (ATT) are two important topics of causal inference. However, when using the observational data for causal inference, two main problems including unbalanced covariates and missing outcomes should be tackled. In order to handle these two challenges and provide protection against model misspecification , the doubly robust estimators are developed, which remain consistent when the propensity score model and the selection probability model are correctly specified concurrently, or the outcome regression model is correctly specified. Under regularity conditions , the asymptotic normality of the estimators is established. Simulation studies confirm the desirable finite-sample performance of the proposed methods. Based on the Aerobics Center Longitudinal Study, the significant positive causal effect of physical activity levels on health status is discovered.},
  archive      = {J_CSDA},
  author       = {Kecheng Wei and Guoyou Qin and Jiajia Zhang and Xuemei Sui},
  doi          = {10.1016/j.csda.2021.107399},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107399},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Doubly robust estimation in causal inference with missing outcomes: With an application to the aerobics center longitudinal study},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fusing sufficient dimension reduction with neural networks.
<em>CSDA</em>, <em>168</em>, 107390. (<a
href="https://doi.org/10.1016/j.csda.2021.107390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are combined with sufficient dimension reduction methodology in order to remove the limitation of small p and n of the latter. NN-SDR applies when the dependence of the response Y on a set of predictors X is fully captured by the regression function g ( B ′ X ) g(B′X) , for an unknown function g and low rank parameter B matrix. It is shown that the proposed estimator is on par with competing sufficient dimension reduction methods , such as minimum average variance estimation and conditional variance estimation , in small p and n settings in simulations. Its main advantage is its scalability in regressions with large data, for which the other methods are infeasible.},
  archive      = {J_CSDA},
  author       = {Daniel Kapla and Lukas Fertl and Efstathia Bura},
  doi          = {10.1016/j.csda.2021.107390},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107390},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fusing sufficient dimension reduction with neural networks},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse high-dimensional semi-nonparametric quantile
regression in a reproducing kernel hilbert space. <em>CSDA</em>,
<em>168</em>, 107388. (<a
href="https://doi.org/10.1016/j.csda.2021.107388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider partially linear quantile regression with a high-dimensional linear part, with the nonparametric function assumed to be in a reproducing kernel Hilbert space . We establish the overall learning rate in this setting, as well as the rate of the linear part separately. Our proof relies heavily on the empirical processes and the Rademacher complexity in the semi-nonparametric setting as analytic tools . Some simulation studies and a real data analysis are presented for illustration.},
  archive      = {J_CSDA},
  author       = {Yue Wang and Yan Zhou and Rui Li and Heng Lian},
  doi          = {10.1016/j.csda.2021.107388},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107388},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sparse high-dimensional semi-nonparametric quantile regression in a reproducing kernel hilbert space},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical file-matching of non-gaussian data: A game
theoretic approach. <em>CSDA</em>, <em>168</em>, 107387. (<a
href="https://doi.org/10.1016/j.csda.2021.107387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical file-matching problem is a data integration problem with structured missing data. The general form involves the analysis of multiple datasets that only have a strict subset of variables jointly observed across all datasets. Missing-data imputation is complicated by the fact that the joint distribution of the variables is nonidentifiable as there are no completely observed cases. Nonparametric imputation methods typically involve an implicit conditional independence assumption that is forced by the missing-data pattern. Parametric imputation does not require conditional independence assumptions, but can be challenging due to identifiability issues and the difficulty of parameter estimation. The identification problem can be studied using game theory, and it is possible to establish a general characterization of the minimax optimal strategy under negative log likelihood loss. For non-Gaussian models, imputation using the minimax optimal strategy can lead to different results compared to generic methods. Computationally feasible procedures for parameter estimation can be implemented using data augmentation schemes and the EM algorithm . Comparisons of the minimax optimal imputation scheme to standard algorithms on real data from flow cytometry show that minimax strategies can better preserve the joint distribution of the variables.},
  archive      = {J_CSDA},
  author       = {Daniel Ahfock and Saumyadipta Pyne and Geoffrey J. McLachlan},
  doi          = {10.1016/j.csda.2021.107387},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107387},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical file-matching of non-gaussian data: A game theoretic approach},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional generalized estimating equations of
mean-variance-correlation for clustered data. <em>CSDA</em>,
<em>168</em>, 107386. (<a
href="https://doi.org/10.1016/j.csda.2021.107386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modelling of mean-covariance structure is an important topic in clustered data analysis. Existing methods, such as those based on modified Cholesky decomposition (MCD), alternative Cholesky decomposition (ACD) and hyperspherical coordinates decomposition (HPC), have two main restrictions. First, they often assume that responses in the same cluster are naturally ordered, for example, by time in longitudinal studies. Second, the existing methods model transformed parameters, for instance, the generalized autoregressive parameters and innovation variances in MCD/ACD, and the hyperspherical coordinates in HPC, making the dependence of correlation coefficients or variances on covariates hardly understandable. As an alternative, a data-driven method that models directly the mean, variances and correlation coefficients for clustered data is proposed. Comparing to the existing methods, the proposed approach not only has no need of natural order in responses but also works on original correlation coefficients and variances. The proposed models are flexible and interpretable, and the parameter estimators in joint generalized estimating equations (GEE) are shown to be consistent and asymptotically normally distributed. Consistent model selection criteria in spirit of quasi-likelihood under independence model criterion (QIC) are considered. The use of the proposed approach is demonstrated by intensive simulation studies and real data analysis.},
  archive      = {J_CSDA},
  author       = {Renwen Luo and Jianxin Pan},
  doi          = {10.1016/j.csda.2021.107386},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107386},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional generalized estimating equations of mean-variance-correlation for clustered data},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new normal reference test for linear hypothesis testing in
high-dimensional heteroscedastic one-way MANOVA. <em>CSDA</em>,
<em>168</em>, 107385. (<a
href="https://doi.org/10.1016/j.csda.2021.107385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, with rapid development of data collecting technologies, high-dimensional data become increasingly prevalent, and much work has been done for hypotheses on mean vectors for high-dimensional data. However, only a few methods have been proposed and studied for the general linear hypothesis testing (GLHT) problem for high-dimensional data which includes many well-studied problems as special cases. A centralized L 2 L2 -norm based test statistic is proposed and studied for the high-dimensional GLHT problem. It is shown that under some mild conditions, the proposed test statistic and a chi-square-mixture have the same normal or non-normal limiting distributions. It is then justified that the null distribution of the test statistic can be approximated by using that of the chi-square-type mixture. The distribution of the chi-square-type mixture can be well approximated by a three-cumulant matched chi-square approximation with its approximation parameters consistently estimated from the data. Since the chi-square-type mixture is obtained from the test statistic under the null hypothesis and when the data are normally distributed, the resulting test is termed as a normal reference test. The asymptotic power of the proposed test under a local alternative is established. The impact of the data non-normality on the proposed test is also studied. Three simulation studies and a real data example demonstrate that in terms of size control, the proposed test performs well regardless of whether the data are nearly uncorrelated, moderately correlated, or highly correlated and it outperforms two existing competitors substantially.},
  archive      = {J_CSDA},
  author       = {Jin-Ting Zhang and Tianming Zhu},
  doi          = {10.1016/j.csda.2021.107385},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107385},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new normal reference test for linear hypothesis testing in high-dimensional heteroscedastic one-way MANOVA},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence intervals for parameters in high-dimensional
sparse vector autoregression. <em>CSDA</em>, <em>168</em>, 107383. (<a
href="https://doi.org/10.1016/j.csda.2021.107383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregression (VAR) models are widely used to analyze the interrelationship between multiple variables over time. Estimation and inference of the transition matrices of VAR models are crucial for practitioners to make decisions in fields such as economics and finance. However, when the number of variables is larger than the sample size, it remains a challenge to perform inference of the model parameters. The de-biased Lasso and two bootstrap de-biased Lasso methods are proposed to construct confidence intervals for the elements of the transition matrices of high-dimensional VAR models. The proposed methods are asymptotically valid under appropriate sparsity and other regularity conditions . Moreover, feasible and parallelizable algorithms are developed to implement the proposed methods, which save a large amount of computational cost required by the nodewise Lasso and bootstrap . Simulation studies illustrate that the proposed methods perform well in finite-samples. Finally, the proposed methods are applied to analyze the price data of stocks in the S&amp;P 500 index. Some stocks, such as the largest producer of gold in the world, Newmont Corporation, are found to have significant predictive power over most stocks.},
  archive      = {J_CSDA},
  author       = {Ke Zhu and Hanzhong Liu},
  doi          = {10.1016/j.csda.2021.107383},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107383},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Confidence intervals for parameters in high-dimensional sparse vector autoregression},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correcting for sample selection bias in bayesian
distributional regression models. <em>CSDA</em>, <em>168</em>, 107382.
(<a href="https://doi.org/10.1016/j.csda.2021.107382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the presence of non-randomly selected samples, many statistical models, including standard regression models, can fail. In particular, without accounting for the underlying selection process estimates might be biased. Sample selection models can correct this bias when an informative selection process governs the availability of the outcome of interest. A copula based approach is presented for correcting the sample selection bias in Bayesian structured additive distributional regression models. This framework relaxes the distributional assumption on the response of the linear or the generalized linear model and models all distributional parameters as functions of the covariates . Covariate effects are not limited to being purely linear and other effect types, such as smooth functional effects, are available. As a consequence, the approach presented provides increased flexibility with respect to the dependence structure , the available predictor specifications and the choice of the marginal distributions compared to Heckman&#39;s classic sample selection model. To facilitate estimation in such a complex model, a fully Bayesian approach based on Markov chain Monte Carlo simulations is developed and the presented methodology is empirically evaluated. Furthermore, the introduced approach is compared to a frequentist competitor and an application on a data set from psychological judge-advisor research is presented.},
  archive      = {J_CSDA},
  author       = {Paul F.V. Wiemann and Nadja Klein and Thomas Kneib},
  doi          = {10.1016/j.csda.2021.107382},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107382},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Correcting for sample selection bias in bayesian distributional regression models},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-parametric analysis of serial dependence in time series
using ordinal patterns. <em>CSDA</em>, <em>168</em>, 107381. (<a
href="https://doi.org/10.1016/j.csda.2021.107381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A list of new tests for serial dependence based on ordinal patterns is provided. These new methods rely exclusively on the order structure of the data sets. Hence, the novel tests are stable under monotone transformations of the time series and robust against small perturbations or measurement errors. The standard asymptotic distributions are given, and their finite sample behavior under linear and non-linear departures from the null of independence are studied. Moreover, it is proved that under mild conditions, any ordinal-pattern-based test is nuisance free, which is appealing for modeling, as these tests can eventually be used as misspecification tests. This property is also analyzed for finite samples and illustrated through an empirical application. Much of the discussion is based on a detailed combinatorial analysis of ordinal pattern probabilities .},
  archive      = {J_CSDA},
  author       = {Christian H. Weiß and Manuel Ruiz Marín and Karsten Keller and Mariano Matilla-García},
  doi          = {10.1016/j.csda.2021.107381},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107381},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Non-parametric analysis of serial dependence in time series using ordinal patterns},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fourier transform sparse inverse regression estimators for
sufficient variable selection. <em>CSDA</em>, <em>168</em>, 107380. (<a
href="https://doi.org/10.1016/j.csda.2021.107380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction aims to reduce the dimension of predictors while maintaining the regression information. Recently, researchers have studied an impressive range of sparse inverse regression estimators . Nonetheless, conspicuously less attention has been given to the multivariate response with high-dimensional covariates settings. To fill the gap, a Fourier transform inverse regression approach via regularized quadratic discrepancy functions is investigated. Theoretically, consistency and oracle properties for the proposed estimators are established. An iterated alternating direction method of multipliers (ADMM) algorithm is employed to estimate two target parameters simultaneously. Each step of the ADMM algorithm has an explicit solution yielding computational efficiency gain. Numerical studies and real data analysis confirm the theoretical properties and yield superior performance of our proposed methods. In specific, the proposal has higher support recovery rates compared to the state-of-the-art approach. An open-source Python package called ADMMFTIRE accompanying this paper is available online. 1},
  archive      = {J_CSDA},
  author       = {Jiaying Weng},
  doi          = {10.1016/j.csda.2021.107380},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107380},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fourier transform sparse inverse regression estimators for sufficient variable selection},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering multivariate functional data using unsupervised
binary trees. <em>CSDA</em>, <em>168</em>, 107376. (<a
href="https://doi.org/10.1016/j.csda.2021.107376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model-based clustering algorithm is proposed for a general class of functional data for which the components could be curves or images. The random functional data realizations could be measured with errors at discrete, and possibly random, points in the definition domain. The idea is to build a set of binary trees by recursive splitting of the observations. The number of groups are determined in a data-driven way. The new algorithm provides easily interpretable results and fast predictions for online data sets. Results on simulated datasets reveal good performance in various complex settings. The methodology is applied to the analysis of vehicle trajectories on a German roundabout.},
  archive      = {J_CSDA},
  author       = {Steven Golovkine and Nicolas Klutchnikoff and Valentin Patilea},
  doi          = {10.1016/j.csda.2021.107376},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107376},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clustering multivariate functional data using unsupervised binary trees},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surrogate space based dimension reduction for nonignorable
nonresponse. <em>CSDA</em>, <em>168</em>, 107374. (<a
href="https://doi.org/10.1016/j.csda.2021.107374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) for nonignorable nonresponse poses a challenge and the literature about this issue is very rare. In the nonignorable case, the SDR methods developed for ignorable missing data generally yield serious estimation bias and thus are invalid. A regression-calibration-based cumulative mean estimation (RC-CUME) procedure is proposed to recover the central subspace (CS) with the aid of a surrogate subspace. Asymptotic properties of the RC-CUME are investigated. A modified BIC-type criterion is used to determine the structural dimension of the CS. Some extensions to other SDR methods are presented. Simulation studies are conducted to access the finite-sample performance of the proposed RC-CUME approach, and a real data set is analyzed for illustration.},
  archive      = {J_CSDA},
  author       = {Jianqiu Deng and Xiaojie Yang and Qihua Wang},
  doi          = {10.1016/j.csda.2021.107374},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107374},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Surrogate space based dimension reduction for nonignorable nonresponse},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaboration mechanisms and community detection of
statisticians based on ERGMs and kNN-walktrap. <em>CSDA</em>,
<em>168</em>, 107372. (<a
href="https://doi.org/10.1016/j.csda.2021.107372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive information on coauthorship from 2014 to 2018 was gathered from four top statistical journals and subsequently cleaned to provide a review in the field from the perspective of a co-authorship network analysis . Data on productivity and trends, as well as a skew analysis of publications and collaborations, was provided by the analysis. The coauthorship network was analyzed for both global and individual properties. Exponential random graph models (ERGMs) were also used to explore the formation mechanisms of collaboration while simultaneously considering exogenous covariate effects and endogenous network structure processes. It was discovered that homophily (authors from the same universities and countries) and transitivity (the tendency to collaborate with a coauthor&#39;s coauthor) have a significant positive effect on the production of collaborative studies. Finally, the k NN-walktrap was proposed, which combines the structures of the network and the homophily features of authors to detect network communities. In this method, the cosine similarity calculated by the homophily features of the nodes is utilized to build a k NN ( k Nearest Neighbor) network and apply walktrap to detect communities. Thus, more detailed and comprehensive community structures can be detected than when using the walktrap method. These results have practical significance for researching collaboration models and guiding future collaboration.},
  archive      = {J_CSDA},
  author       = {Jie Liu and Huilin Ge},
  doi          = {10.1016/j.csda.2021.107372},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107372},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Collaboration mechanisms and community detection of statisticians based on ERGMs and kNN-walktrap},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A convex programming solution based debiased estimator for
quantile with missing response and high-dimensional covariables.
<em>CSDA</em>, <em>168</em>, 107371. (<a
href="https://doi.org/10.1016/j.csda.2021.107371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the estimating problem of response quantile with high dimensional covariates when the response is missing at random . Some existing methods define root- n consistent estimators for the response quantile . But these methods require correct specifications of both the conditional distribution of response given covariates and the selection probability function. In this paper, a debiased method is proposed by solving a convex program . The estimator obtained by the proposed method is asymptotically normal given a correctly specified parametric model for the conditional distribution function, without the requirement to specify the selection probability function. Moreover, the proposed estimator can be asymptotically more efficient than the existing estimators. The proposed method is evaluated by a simulation study and is illustrated by a real data example.},
  archive      = {J_CSDA},
  author       = {Miaomiao Su and Qihua Wang},
  doi          = {10.1016/j.csda.2021.107371},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107371},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A convex programming solution based debiased estimator for quantile with missing response and high-dimensional covariables},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving performances of MCMC for nearest neighbor gaussian
process models with full data augmentation. <em>CSDA</em>, <em>168</em>,
107368. (<a href="https://doi.org/10.1016/j.csda.2021.107368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though Nearest Neighbor Gaussian Processes (NNGP) alleviate MCMC implementation of Bayesian space-time models considerably, they do not solve the convergence problems caused by high model dimension. Frugal alternatives such as response or collapsed algorithms are one answer. An alternative approach is to keep full data augmentation , but to try and make it more efficient. Two strategies are presented. The first is to pay particular attention to the seemingly trivial fixed effects of the model. Empirical exploration shows that re-centering the latent field on the intercept critically improves chain behavior. Theoretical elements support those observations. Besides the intercept, other fixed effects may have trouble mixing. This problem is addressed by interweaving, a simple method that requires no tuning, while remaining affordable thanks to the sparsity of NNGPs. The second accelerates sampling of the random field using Chromatic samplers. This method boils long sequential simulation down to group-parallelized or group-vectorized sampling. The attractive possibility for parallelizing NNGP density can therefore be carried over to field sampling. A R implementation of the two methods for Gaussian fields is freely available 1 , an extensive vignette is provided. The presented implementation is run on two synthetic toy examples, along with the state of the art package spNNGP . Finally, the methods are applied to a real data set of lead contamination on the mainland of the United States of America .},
  archive      = {J_CSDA},
  author       = {Sébastien Coube-Sisqueille and Benoît Liquet},
  doi          = {10.1016/j.csda.2021.107368},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107368},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Improving performances of MCMC for nearest neighbor gaussian process models with full data augmentation},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The general goodness-of-fit tests for correlated data.
<em>CSDA</em>, <em>167</em>, 107379. (<a
href="https://doi.org/10.1016/j.csda.2021.107379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing correlated data by goodness-of-fit type tests is a critical statistical problem in many applications. A unified framework is provided through a general family of goodness-of-fit tests (GGOF) to address this problem. The GGOF family covers many classic and newly developed tests, such as the minimal p -value test, Simes test, the GATES, one-sided Kolmogorov-Smirnov type tests, one-sided phi-divergence tests, the generalized Higher Criticism, the generalized Berk-Jones, etc. It is shown that the omnibus test that automatically adapts among GGOF statistics for given data, i.e., the GGOF-O, is still contained in the GGOF family and is computationally efficient. For analytically controlling the type I error rate of any GGOF tests, exact calculation is deduced under the Gaussian model with positive equal correlations. Based on that, the effective correlation coefficient (ECC) algorithm is proposed to address arbitrary correlations. Simulations are used to explore how signal and correlation patterns jointly influence typical GGOF tests&#39; statistical power. The GGOF-O is shown robustly powerful across various signal and correlation patterns. As demonstrated by a study of bone mineral density, the GGOF framework has good potential for detecting novel disease genes in genetic summary data analysis. Computational tools are available in the R package SetTest on the CRAN.},
  archive      = {J_CSDA},
  author       = {Hong Zhang and Zheyang Wu},
  doi          = {10.1016/j.csda.2021.107379},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107379},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The general goodness-of-fit tests for correlated data},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wilcoxon-mann-whitney spatial scan statistic for
functional data. <em>CSDA</em>, <em>167</em>, 107378. (<a
href="https://doi.org/10.1016/j.csda.2021.107378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric scan method for functional data indexed in space is introduced. The associated scan statistic is derived from the Wilcoxon-Mann-Whitney test statistic defined for infinite dimensional data. It is completely nonparametric as it does not assume any distribution concerning the functional marks. Whatever the clustering scenario, this scan test seems to be efficient to detect and locate the cluster. This method is applied to a data set for extracting features in Spanish province population growth. A significant spatial cluster of low demographic evolution rates is found, exhibiting a specific phenomenon in the North-West of Spain.},
  archive      = {J_CSDA},
  author       = {Zaineb Smida and Lionel Cucala and Ali Gannoun and Ghislain Durif},
  doi          = {10.1016/j.csda.2021.107378},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107378},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A wilcoxon-mann-whitney spatial scan statistic for functional data},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty quantification for honest regression trees.
<em>CSDA</em>, <em>167</em>, 107377. (<a
href="https://doi.org/10.1016/j.csda.2021.107377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method is developed for quantifying the uncertainties of the estimates and predictions produced by honest random forests . This new method is based on the generalized fiducial methodology, and provides a fiducial density function that measures how likely each single honest tree is the true model. With such a density function, estimates and predictions, as well as their confidence/prediction intervals, can be obtained. The promising empirical properties of the proposed method are demonstrated by numerical comparisons with several state-of-the-art methods, and by applications to a few real data sets . Lastly, the proposed method is theoretically backed up by an asymptotic guarantee.},
  archive      = {J_CSDA},
  author       = {Suofei Wu and Jan Hannig and Thomas C.M. Lee},
  doi          = {10.1016/j.csda.2021.107377},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107377},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Uncertainty quantification for honest regression trees},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive quantile computation for brownian bridge in
change-point analysis. <em>CSDA</em>, <em>167</em>, 107375. (<a
href="https://doi.org/10.1016/j.csda.2021.107375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an example for the fast calculation of distributional parameters of Gaussian processes , a new Monte Carlo algorithm for the computation of quantiles of the supremum norm of weighted Brownian bridges is proposed. As it is known, the corresponding distributions arise asymptotically for weighted CUSUM statistics for change-point detection. The new algorithm employs an adaptive (sequential) time discretization for the trajectories of the Brownian bridge. A simulation study shows that the new algorithm by far outperforms the standard approach, which employs a uniform time discretization .},
  archive      = {J_CSDA},
  author       = {Jürgen Franke and Mario Hefter and André Herzwurm and Klaus Ritter and Stefanie Schwaar},
  doi          = {10.1016/j.csda.2021.107375},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107375},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Adaptive quantile computation for brownian bridge in change-point analysis},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast selection of nonlinear mixed effect models using
penalized likelihood. <em>CSDA</em>, <em>167</em>, 107373. (<a
href="https://doi.org/10.1016/j.csda.2021.107373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear Mixed effects models are hidden variables models that are widely used in many fields such as pharmacometrics. In such models, the distribution characteristics of hidden variables can be specified by including several parameters such as covariates or correlations which must be selected. Recent development of pharmacogenomics has brought averaged/high dimensional problems to the field of nonlinear mixed effects modeling for which standard covariates selection techniques like stepwise methods are not well suited. The selection of covariates and correlation parameters using a penalized likelihood approach is proposed. The penalized likelihood problem is solved using a stochastic proximal gradient algorithm to avoid inner-outer iterations. Speed of convergence of the proximal gradient algorithm is improved using component-wise adaptive gradient step sizes. The practical implementation and tuning of the proximal gradient algorithm are explored using simulations. Calibration of regularization parameters is performed by minimizing the Bayesian Information Criterion using particle swarm optimization , a zero-order optimization procedure . The use of warm restart and parallelization allowed computing time to be reduced significantly. The performance of the proposed method compared to the traditional grid search strategy is explored using simulated data. Finally, an application to real data from two pharmacokinetics studies is provided, one studying an antifibrinolytic and the other studying an antibiotic.},
  archive      = {J_CSDA},
  author       = {Edouard Ollier},
  doi          = {10.1016/j.csda.2021.107373},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107373},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast selection of nonlinear mixed effect models using penalized likelihood},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). K-bMOM: A robust lloyd-type clustering algorithm based on
bootstrap median-of-means. <em>CSDA</em>, <em>167</em>, 107370. (<a
href="https://doi.org/10.1016/j.csda.2021.107370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The median-of-means is an estimator of the mean of a random variable that has emerged as an efficient and flexible tool to design robust learning algorithms with optimal theoretical guarantees. However, its use for the clustering task suggests dividing the dataset into blocks, which may provoke the disappearance of some clusters in some blocks and lead to bad performances. To overcome this difficulty, a procedure termed “bootstrap median-of-means” is proposed, where the blocks are generated with a replacement in the dataset. Considering the estimation of the mean of a random variable , the bootstrap median-of-means has a better breakdown point than the median-of-means if enough blocks are generated. A clustering algorithm called K-bMOM is designed, by performing Lloyd-type iterations together with the use of the bootstrap median-of-means strategy. Good performances are obtained on simulated and real-world datasets for color quantization and an emphasis is put on the benefits of our robust intialization procedure. On the theoretical side, K-bMOM is also proven to have a non-trivial probabilistic breakdown point in well-clusterizable situations.},
  archive      = {J_CSDA},
  author       = {Camille Brunet-Saumard and Edouard Genetay and Adrien Saumard},
  doi          = {10.1016/j.csda.2021.107370},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107370},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {K-bMOM: A robust lloyd-type clustering algorithm based on bootstrap median-of-means},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projection quantile correlation and its use in
high-dimensional grouped variable screening. <em>CSDA</em>,
<em>167</em>, 107369. (<a
href="https://doi.org/10.1016/j.csda.2021.107369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new measure, called Projection Quantile Correlation (PQC), to detect quantile dependence between a response and multivariate predictors at a given quantile level. The PQC measure is free of tuning parameters, robust against heavy tailed distributions and outliers, and does not require moment conditions on the predictors. We obtain theoretical properties of the PQC and its empirical counterpart. We then use the measure to select the grouped predictors that contribute to the conditional quantile of the response for high-dimensional data with group structures. We also establish sure independent screening properties for the group screening method. We illustrate the finite sample performance of the proposed method through simulations and an application to a dataset.},
  archive      = {J_CSDA},
  author       = {Jicai Liu and Yuefeng Si and Yong Niu and Riquan Zhang},
  doi          = {10.1016/j.csda.2021.107369},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107369},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Projection quantile correlation and its use in high-dimensional grouped variable screening},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of longitudinal trends in time-frequency
transformations of EEG data during a learning experiment. <em>CSDA</em>,
<em>167</em>, 107367. (<a
href="https://doi.org/10.1016/j.csda.2021.107367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG experiments yield high-dimensional event-related potential (ERP) data in response to repeatedly presented stimuli throughout the experiment. Changes in the high-dimensional ERP signal throughout the duration of an experiment (longitudinally) is the main quantity of interest in learning paradigms, where they represent the learning dynamics. Typical analysis, which can be performed in the time or the frequency domain, average the ERP waveform across all trials, leading to the loss of the potentially valuable longitudinal information in the data. Longitudinal time-frequency transformation of ERP (LTFT-ERP) is proposed to retain information from both the time and frequency domains, offering distinct but complementary information on the underlying cognitive processes evoked, while still retaining the longitudinal dynamics in the ERP waveforms. LTFT-ERP begins by time-frequency transformations of the ERP data, collected across subjects, electrodes, conditions and trials throughout the duration of the experiment, followed by a data driven multidimensional principal components analysis (PCA) approach for dimension reduction. Following projection of the data onto leading directions of variation in the time and frequency domains, longitudinal learning dynamics are modeled within a mixed effects modeling framework. Applications to a learning paradigm in autism depict distinct learning patterns throughout the experiment among children diagnosed with Autism Spectrum Disorder and their typically developing peers. LTFT-ERP time-frequency joint transformations are shown to bring an additional level of specificity to interpretations of the longitudinal learning patterns related to underlying cognitive processes, which is lacking in single domain analysis (in the time or the frequency domain only). Simulation studies show the efficacy of the proposed methodology.},
  archive      = {J_CSDA},
  author       = {Joanna Boland and Donatello Telesca and Catherine Sugar and Shafali Jeste and Cameron Goldbeck and Damla Senturk},
  doi          = {10.1016/j.csda.2021.107367},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107367},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A study of longitudinal trends in time-frequency transformations of EEG data during a learning experiment},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient unequal probability resampling from finite
populations. <em>CSDA</em>, <em>167</em>, 107366. (<a
href="https://doi.org/10.1016/j.csda.2021.107366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A resampling technique for probability-proportional-to size sampling designs is proposed. It is essentially based on a special form of variable probability , without replacement sampling applied directly to the sample data, yet according to the pseudo-population approach. From a theoretical point of view, it is asymptotically correct: as both the sample size and the population size increase, under mild regularity conditions the proposed resampling design tends to coincide with the original sampling design under which sample data were collected. From a computational point of view, the proposed methodology is easy to be implemented and efficient, because it neither requires the actual construction of the pseudo-population nor any form of randomization to ensure integer weights and sizes. Empirical evidence based on a simulation study 1 indicates that the proposed resampling technique outperforms its two main competitors for confidence interval construction of various population parameters including quantiles .},
  archive      = {J_CSDA},
  author       = {Pier Luigi Conti and Fulvia Mecatti and Federica Nicolussi},
  doi          = {10.1016/j.csda.2021.107366},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107366},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient unequal probability resampling from finite populations},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixture additive hazards cure model with latent variables:
Application to corporate default data. <em>CSDA</em>, <em>167</em>,
107365. (<a href="https://doi.org/10.1016/j.csda.2021.107365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture additive hazards cure model with latent variables is proposed to investigate the risk factors of the corporate default issue with a sample of corporate bonds from the Chinese financial market. The proposed model combines confirmatory factor analysis, additive hazards, and cure models to characterize latent attributes, such as profitability, liquidity, and operating capacity, through multiple manifest variables and investigate the effects of observed covariates and latent factors on the hazards of corporate default and the probability of nonsusceptibility to default. An expectation-maximization algorithm is developed to conduct statistical inference . The satisfactory performance of the suggested method is demonstrated by simulation studies. Application to the corporate default data illustrates the utility of the proposed methodology and its superiority over conventional methods. The empirical results reveal that defaulted companies usually have low profitability, high debt level, and poor operating capacity. The findings also help differentiate between groups that are susceptible and nonsusceptible to default and provide new insights into the warning signs and effective strategies for preventing defaults.},
  archive      = {J_CSDA},
  author       = {Qi Yang and Haijin He and Bin Lu and Xinyuan Song},
  doi          = {10.1016/j.csda.2021.107365},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107365},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mixture additive hazards cure model with latent variables: Application to corporate default data},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polymatching algorithm in observational studies with
multiple treatment groups. <em>CSDA</em>, <em>167</em>, 107364. (<a
href="https://doi.org/10.1016/j.csda.2021.107364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matched designs are commonly used in non-randomized studies to evaluate causal effects for dichotomous treatment. Optimal matching algorithms have been devised to form matched pairs or sets between treatment and control groups in various designs, including 1- k matching and full matching. With multiple treatment arms, however, the optimal matching problem cannot be solved in polynomial-time. This is a major challenge for implementing matched designs with multiple arms, which are important for evaluating causal effects with different dose levels or constructing evidence factors with multiple control groups. A polymatching framework for generating matched sets among multiple groups is proposed. An iterative multi-way algorithm for implementation is developed, which takes advantage of the existing optimal two-group matching algorithm repeatedly. An upper bound for the total distance attained by our algorithm is provided to show that the distance result is close to the optimal solution. Simulation studies are conducted to compare the proposed algorithm with the nearest neighbor algorithm under different scenarios. The algorithm is also used to construct a difference-in-difference matched design among four groups, to examine the impact of Medicaid expansion on the health status of Ohioans.},
  archive      = {J_CSDA},
  author       = {Giovanni Nattino and Chi Song and Bo Lu},
  doi          = {10.1016/j.csda.2021.107364},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107364},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Polymatching algorithm in observational studies with multiple treatment groups},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Oracle-efficient estimation for functional data error
distribution with simultaneous confidence band. <em>CSDA</em>,
<em>167</em>, 107363. (<a
href="https://doi.org/10.1016/j.csda.2021.107363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kolmogorov-Smirnov (K-S) simultaneous confidence band (SCB) is constructed for the error distribution of dense functional data based on kernel distribution estimator (KDE). The KDE is computed from residuals of B spline trajectories over a smaller number of measurements, whereas the B spline trajectories are computed from the remaining larger set of measurements. Under mild and simple assumptions, it is shown that the KDE is a uniformly oracle-efficient estimator of the error distribution, and the SCB has the same asymptotic properties as the classic K-S SCB based on the infeasible empirical cumulative distribution function (EDF) of unobserved errors. Simulation examples corroborate with the theoretical findings. The proposed method is illustrated by examples of an EEG (Electroencephalogram) data and a stock data.},
  archive      = {J_CSDA},
  author       = {Jiangyan Wang and Lijie Gu and Lijian Yang},
  doi          = {10.1016/j.csda.2021.107363},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107363},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Oracle-efficient estimation for functional data error distribution with simultaneous confidence band},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter estimation and computation of the fisher
information matrix for functions of phase type random variables.
<em>CSDA</em>, <em>167</em>, 107362. (<a
href="https://doi.org/10.1016/j.csda.2021.107362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation and statistical inference for the phase type (PH) class of distributions are significantly important while considering the role of PH random variables in stochastic models appearing across various disciplines. Estimation of the parameters and computation of the Fisher Information Matrix (FIM) for some functions of PH random variables g ( X ) g(X) , which are random variables of the continuous type is the main objective here. The following two cases are considered: ( i ) the set g − 1 ( y ) g−1(y) has at most a finite number of elements for every real y and ( i i ) (ii) for every real number y , g − 1 ( y ) y,g−1(y) has either a countably infinite number of elements that form an arithmetic progression or no element. The parameter estimation and the FIM computation for some random variables which can be obtained as functions of the PH random variables are carried out numerically for illustrative purposes. Also, some standard distributions like Pareto and beta as well as some real life data are shown to be approximated by appropriate functions of phase type random variables.},
  archive      = {J_CSDA},
  author       = {Celeste R. Pavithra and T.G. Deepak},
  doi          = {10.1016/j.csda.2021.107362},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107362},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parameter estimation and computation of the fisher information matrix for functions of phase type random variables},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A group comparison in fMRI data using a semiparametric model
under shape invariance. <em>CSDA</em>, <em>167</em>, 107361. (<a
href="https://doi.org/10.1016/j.csda.2021.107361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of functional magnetic resonance imaging (fMRI) data, a common type of analysis is to compare differences across scanning sessions. A challenge to direct comparisons of this type is the low signal-to-noise ratio in fMRI data. By using the property that brain signals from a task-related experiment may exhibit a similar pattern in regions of interest across participants, a semiparametric approach under shape invariance to quantify and test the differences in sessions and groups is developed. The common function is estimated with local polynomial regression and the shape invariance model parameters are estimated using evolutionary optimization methods. The efficacy of the semi-parametric approach is demonstrated on a study of brain activation changes across two sessions associated with practice-related cognitive control. The objective of the study is to evaluate neural circuitry supporting a cognitive control task, and associated practice-related changes via acquisition of blood oxygenation level dependent (BOLD) signal collected using fMRI. By using the proposed approach, BOLD signals in multiple regions of interest for control participants and participants with schizophrenia are compared as they perform a cognitive control task (known as the antisaccade task) at two sessions, and the effects of task practice in these groups are quantified.},
  archive      = {J_CSDA},
  author       = {Arunava Samaddar and Brooke S. Jackson and Christopher J. Helms and Nicole A. Lazar and Jennifer E. McDowell and Cheolwoo Park},
  doi          = {10.1016/j.csda.2021.107361},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107361},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A group comparison in fMRI data using a semiparametric model under shape invariance},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model averaging for linear mixed models via augmented
lagrangian. <em>CSDA</em>, <em>167</em>, 107351. (<a
href="https://doi.org/10.1016/j.csda.2021.107351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection for linear mixed models has been a focus of recent research in statistics . Yet, the method of model averaging has been sparsely explored in this context. A weight finding criterion for model averaging of linear mixed models is introduced, as well as its implementation for the programming language R. Since the optimization of the underlying criterion is non-trivial, a fast and robust implementation of the augmented Lagrangian optimization technique is employed. Furthermore, the influence of the weight finding criterion on the resulting model averaging estimator is illustrated through simulation studies and two applications based on real data.},
  archive      = {J_CSDA},
  author       = {René-Marcel Kruse and Alexander Silbersdorff and Benjamin Säfken},
  doi          = {10.1016/j.csda.2021.107351},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107351},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model averaging for linear mixed models via augmented lagrangian},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective inference for additive and linear mixed models.
<em>CSDA</em>, <em>167</em>, 107350. (<a
href="https://doi.org/10.1016/j.csda.2021.107350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After model selection, subsequent inference in statistical models tends to be overconfident if selection is not accounted for. One possible solution to address this problem is selective inference, which constitutes a post-selection inference framework and yields valid inference statements by conditioning on the selection event. Existing work on selective inference is, however, not directly applicable to additive and linear mixed models . A novel extension to recent work on selective inference to the class of additive and linear mixed models is thus presented. The approach can be applied for any type of model selection mechanism that can be expressed as a function of the outcome variable (and potentially of covariates on which the model conditions). Properties of the method are validated in simulation studies and in an application to a data set in monetary economics. The approach is particularly useful in cases of non-standard selection procedures, as present in the motivating application.},
  archive      = {J_CSDA},
  author       = {David Rügamer and Philipp F.M. Baumann and Sonja Greven},
  doi          = {10.1016/j.csda.2021.107350},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107350},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Selective inference for additive and linear mixed models},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical bayesian modeling of spatio-temporal
area-interaction processes. <em>CSDA</em>, <em>167</em>, 107349. (<a
href="https://doi.org/10.1016/j.csda.2021.107349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To model spatial point patterns with discrete time stamps a flexible spatio-temporal area-interaction point process is proposed. In particular, this model is suitable for describing the dependency between point patterns over time, when the new point pattern arises from the previous point pattern. A hierarchical model is also implemented in order to incorporate the underlying evolution process of the model parameters. For parameter estimation, a double Metropolis-Hastings within Gibbs sampler is used. The performance of the estimation algorithm is evaluated through a simulation study. Finally, the point pattern forecasting procedure is demonstrated through a simulation study and an application to United States natural caused wildfire data from 2002 to 2019.},
  archive      = {J_CSDA},
  author       = {Jiaxun Chen and Athanasios C. Micheas and Scott H. Holan},
  doi          = {10.1016/j.csda.2021.107349},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107349},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hierarchical bayesian modeling of spatio-temporal area-interaction processes},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimension reduction for block-missing data based on sparse
sliced inverse regression. <em>CSDA</em>, <em>167</em>, 107348. (<a
href="https://doi.org/10.1016/j.csda.2021.107348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high demand of data compression in the big data era, efficient dimension reduction has become a hot statistical research topic recently. One particular challenge for dimension reduction is the block-missing problem, which is prevalent in multi-modality data. Sliced inverse regression as a classical method would not handle the data-missing issue generally. In this paper, we propose the convex sparse sliced inverse regression with elastic net , whose estimation of the central subspace and variable selection are performed simultaneously. This method can be directly applied to block-missing data without imputation. The algorithm called Adjusted Linearized Alternating Direction of Method of Multipliers (Adjusted L-ADMM) is addressed correspondingly. The asymptotic properties are investigated. Numerical results show that our method can effectively and robustly identify important covariates in the high-dimensional case.},
  archive      = {J_CSDA},
  author       = {Zhen Xiao and Qi Zhang},
  doi          = {10.1016/j.csda.2021.107348},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107348},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dimension reduction for block-missing data based on sparse sliced inverse regression},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for monotone single-index conditional means: A
lorenz regression approach. <em>CSDA</em>, <em>167</em>, 107347. (<a
href="https://doi.org/10.1016/j.csda.2021.107347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lorenz regression procedure quantifies the inequality of a response explained by a set of covariates . Formally, it gives a weight to each covariate to maximize the concentration index between the response and a weighted average of the covariates . The obtained index is called the explained Gini coefficient . Unlike methods based on decompositions of inequality measures , the procedure does not assume a linear relationship between the response and the covariates. Inference can be performed by noticing a similarity with the monotone rank estimator, introduced in the context of the single-index model. A continuity correction is presented in the presence of discrete covariates. The Lorenz- R 2 R2 is a goodness-of-fit measure evaluating the proportion of explained inequality and is used to build a test of joint significance of several covariates. Monte-Carlo simulations and a real-data example are presented.},
  archive      = {J_CSDA},
  author       = {Cédric Heuchenne and Alexandre Jacquemain},
  doi          = {10.1016/j.csda.2021.107347},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107347},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inference for monotone single-index conditional means: A lorenz regression approach},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian beta regression for bounded responses with unknown
supports. <em>CSDA</em>, <em>167</em>, 107345. (<a
href="https://doi.org/10.1016/j.csda.2021.107345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new Bayesian regression framework is presented for the analysis of continuous response data with support restricted to an unknown finite interval. A four-parameter beta distribution is assumed for the response conditioning on covariates , with the mean or mode depending linearly on covariates through a known link function. An informative g -prior is proposed to incorporate the prior distribution for the marginal mean or mode of the response. Byproducts of the Markov chain Monte Carlo sampling for implementing the proposed method lead to model criteria useful for model selection. Goodness-of-fit of the model is assessed using Cox-Snell residual plots . The methodology is illustrated in simulations and demonstrated in two real-life data applications. An R package, betaBayes , is developed for easy implementation of the proposed regression methodology.},
  archive      = {J_CSDA},
  author       = {Haiming Zhou and Xianzheng Huang},
  doi          = {10.1016/j.csda.2021.107345},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107345},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian beta regression for bounded responses with unknown supports},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local logarithm partial likelihood estimation of panel count
data model with an unknown link function. <em>CSDA</em>, <em>166</em>,
107346. (<a href="https://doi.org/10.1016/j.csda.2021.107346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel count data have been extensively discussed in the literature. In general, the existing approaches in modeling panel count data usually assume an exponential form for the dependence of the conditional mean function on covariate variables. However, this assumption may be violated in practice. A more flexible panel count data model with an unknown link function is proposed, and a local logarithm partial likelihood function is formed for the estimation. A two-step iterative algorithm is employed to estimate the unknown link function and covariate effects. Furthermore, the baseline function is obtained by Breslow estimation. Asymptotic properties are derived under some mild conditions. Some numerical simulations and an application of bladder cancer are carried out to confirm and assess the performance of the proposed model and approach.},
  archive      = {J_CSDA},
  author       = {Yijun Wang and Weiwei Wang and Xiaobing Zhao},
  doi          = {10.1016/j.csda.2021.107346},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107346},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Local logarithm partial likelihood estimation of panel count data model with an unknown link function},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On principal graphical models with application to gene
network. <em>CSDA</em>, <em>166</em>, 107344. (<a
href="https://doi.org/10.1016/j.csda.2021.107344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principal graphical model is introduced by incorporating the ideas from the linear sufficient dimension reduction (SDR) methods such as the sliced inverse regression and sliced average variance estimation to a nonparametric graphical model . A nonparametric graphical model is a widely used method to investigate undirected graphs . However, when the number of nodes is large, a nonparametric graphical model suffers from the ‘curse of dimensionality’ because they contain intrinsic high dimensional kernels . The parametric graphical models such as the Gaussian or copula Gaussian graphical models are also well known for their intuitive structure and interpretability . However, they hinge on strong parametric model assumptions. The principal graphical model applies well-known linear SDR techniques to the nonparametric graphical models to enhance performance in high dimensional networks, avoid model assumptions, and maintain interpretability . We use components of linear SDR as modules and implement them in the ( p 2 ) pairs of variables in the network to evaluate conditional independence . In the numerical experiment, our methods have competitive accuracy in both low and high-dimensional settings. Our methods are applied to the DREAM 4 challenge gene network dataset and they work well in high dimensional settings with a limited number of observations.},
  archive      = {J_CSDA},
  author       = {Kyongwon Kim},
  doi          = {10.1016/j.csda.2021.107344},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107344},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On principal graphical models with application to gene network},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint estimation of monotone curves via functional principal
component analysis. <em>CSDA</em>, <em>166</em>, 107343. (<a
href="https://doi.org/10.1016/j.csda.2021.107343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A functional data approach is developed to jointly estimate a collection of monotone curves that are irregularly and possibly sparsely observed with noise. In this approach, the unconstrained relative curvature curves instead of the monotone-constrained functions are directly modeled. Functional principal components are used to describe the major modes of variations of curves and allow borrowing strength across curves for improved estimation. A two-step approach and an integrated approach are considered for model fitting. The simulation study shows that the integrated approach is more efficient than separate curve estimation and the two-step approach. The integrated approach also provides more interpretable principle component functions in an application of estimating weekly wind power curves of a wind turbine .},
  archive      = {J_CSDA},
  author       = {Yei Eun Shin and Lan Zhou and Yu Ding},
  doi          = {10.1016/j.csda.2021.107343},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107343},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint estimation of monotone curves via functional principal component analysis},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based sparse coding beyond gaussian independent model.
<em>CSDA</em>, <em>166</em>, 107336. (<a
href="https://doi.org/10.1016/j.csda.2021.107336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse coding aims to model data vectors as sparse linear combinations of basis elements, but a majority of related studies are restricted to continuous data without spatial or temporal structure . A new model-based sparse coding (MSC) method is proposed to provide an effective and flexible framework for learning features from different data types : continuous, discrete, or categorical, and modeling different types of correlations: spatial or temporal. The specification of the sparsity level and how to adapt the estimation method to large-scale studies are also addressed. A fast EM algorithm is proposed for estimation, and its superior performance is demonstrated in simulation and multiple real applications such as image denoising , brain connectivity study, and spatial transcriptomic imaging.},
  archive      = {J_CSDA},
  author       = {Xin Xing and Rui Xie and Wenxuan Zhong},
  doi          = {10.1016/j.csda.2021.107336},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107336},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model-based sparse coding beyond gaussian independent model},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference for generalized linear model with linear
inequality constraints. <em>CSDA</em>, <em>166</em>, 107335. (<a
href="https://doi.org/10.1016/j.csda.2021.107335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian statistical inference for Generalized Linear Models (GLMs) with parameters lying on a constrained space is of general interest (e.g., in monotonic or convex regression), but often constructing valid prior distributions supported on a subspace spanned by a set of linear inequality constraints can be challenging, especially when some of the constraints might be binding leading to a lower dimensional subspace . For the general case with canonical link, it is shown that a generalized truncated multivariate normal supported on a desired subspace can be used. Moreover, it is shown that such prior distribution facilitates the construction of a general purpose product slice sampling method to obtain (approximate) samples from corresponding posterior distribution , making the inferential method computationally efficient for a wide class of GLMs with an arbitrary set of linear inequality constraints. The proposed product slice sampler is shown to be uniformly ergodic, having a geometric convergence rate under a set of mild regularity conditions satisfied by many popular GLMs (e.g., logistic and Poisson regressions with constrained coefficients). One of the primary advantages of the proposed Bayesian estimation method over classical methods is that uncertainty of parameter estimates is easily quantified by using the samples simulated from the path of the Markov Chain of the slice sampler. Numerical illustrations using simulated data sets are presented to illustrate the superiority of the proposed methods compared to some existing methods in terms of sampling bias and variances. In addition, real case studies are presented using data sets for fertilizer-crop production and estimating the SCRAM rate in nuclear power plants .},
  archive      = {J_CSDA},
  author       = {Rahul Ghosal and Sujit K. Ghosh},
  doi          = {10.1016/j.csda.2021.107335},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107335},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian inference for generalized linear model with linear inequality constraints},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power analysis and type i and type II error rates of
bayesian nonparametric two-sample tests for location-shifts based on the
bayes factor under cauchy priors. <em>CSDA</em>, <em>165</em>, 107326.
(<a href="https://doi.org/10.1016/j.csda.2021.107326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothesis testing is a central statistical method in the biomedical sciences. The ongoing debate about the concept of statistical significance and the reliability of null hypothesis significance tests (NHST) and p-values has brought the advent of various Bayesian hypothesis tests as possible alternatives, which often employ the Bayes factor. However, careful calibration of the prior parameters is necessary for the type I error rates or power of these alternatives to be any better. Also, the availability of various Bayesian tests for the same statistical problem leads to the question which test to choose based on which criteria. Recently proposed Bayesian nonparametric two-sample tests are analyzed with regard to their type I error rates and power to detect an effect. Results show that approaches vary substantially in their ability to control the type I and II errors, and it is shown how to select the prior parameters to balance power and type I error control. This allows for prior elicitation and power analyses based on objective criteria like type I and II error rates even when conducting a Bayesian nonparametric two-sample test. Also, it is shown that existing nonparametric Bayesian two-sample tests are adequate only to test for location-shifts. Together, the results provide guidance how to perform a nonparametric Bayesian two-sample test while simultaneously improving the reliability of research.},
  archive      = {J_CSDA},
  author       = {Riko Kelter},
  doi          = {10.1016/j.csda.2021.107326},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107326},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Power analysis and type i and type II error rates of bayesian nonparametric two-sample tests for location-shifts based on the bayes factor under cauchy priors},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian model selection for high-dimensional ising models,
with applications to educational data. <em>CSDA</em>, <em>165</em>,
107325. (<a href="https://doi.org/10.1016/j.csda.2021.107325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly-intractable posterior distributions arise in many applications of statistics concerned with discrete and dependent data, including physics, spatial statistics , machine learning , the social sciences, and other fields. A specific example is psychometrics, which has adapted high-dimensional Ising models from machine learning , with a view to studying the interactions among binary item responses in educational assessments. To estimate high-dimensional Ising models from educational assessment data, ℓ 1 ℓ1 -penalized nodewise logistic regressions have been used. Theoretical results in high-dimensional statistics show that ℓ 1 ℓ1 -penalized nodewise logistic regressions can recover the true interaction structure with high probability , provided that certain assumptions are satisfied. Those assumptions are hard to verify in practice and may be violated, and quantifying the uncertainty about the estimated interaction structure and parameter estimators is challenging. We propose a Bayesian approach that helps quantify the uncertainty about the interaction structure and parameters without requiring strong assumptions, and can be applied to Ising models with thousands of parameters. We demonstrate the advantages of the proposed Bayesian approach compared with ℓ 1 ℓ1 -penalized nodewise logistic regressions by simulation studies and applications to small and large educational data sets with up to 2,485 parameters. Among other things, the simulation studies suggest that the Bayesian approach is more robust against model misspecification due to omitted covariates than ℓ 1 ℓ1 -penalized nodewise logistic regressions.},
  archive      = {J_CSDA},
  author       = {Jaewoo Park and Ick Hoon Jin and Michael Schweinberger},
  doi          = {10.1016/j.csda.2021.107325},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107325},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian model selection for high-dimensional ising models, with applications to educational data},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new classification tree method with interaction detection
capability. <em>CSDA</em>, <em>165</em>, 107324. (<a
href="https://doi.org/10.1016/j.csda.2021.107324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new classification tree algorithm is presented. It has a novel variable selection algorithm that can effectively detect interactions. The algorithm uses a look-ahead approach that considers not only the significance at the current node, but also the significance at child nodes to detect the interaction. It is also different from other classification tree methods in that it finds the splitting point using the odds ratio. To evaluate the predictive performance of the newly proposed tree algorithm, an empirical study of 27 real or artificial data sets is performed. As a result of the experiment, the proposed algorithm shows at least similar or significantly better performance than the well-known and successful decision tree methods: Ctree, CART and CRUISE.},
  archive      = {J_CSDA},
  author       = {Ahhyoun Kim and Hyunjoong Kim},
  doi          = {10.1016/j.csda.2021.107324},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107324},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new classification tree method with interaction detection capability},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for quantile regression under right censoring:
DeepQuantreg. <em>CSDA</em>, <em>165</em>, 107323. (<a
href="https://doi.org/10.1016/j.csda.2021.107323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational prediction algorithm of neural network , or deep learning , has drawn much attention recently in statistics as well as in image recognition and natural language processing . Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox&#39;s model and its variations to utilize existing neural network library such as Keras , which was built upon the open source library of TensorFlow. As a novel contribution to the literature, an extension of the neural network to the quantile regression is proposed for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization , emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at https://github.com/yicjia/DeepQuantreg .},
  archive      = {J_CSDA},
  author       = {Yichen Jia and Jong-Hyeon Jeong},
  doi          = {10.1016/j.csda.2021.107323},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107323},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Deep learning for quantile regression under right censoring: DeepQuantreg},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bootstrapping multivariate portmanteau tests for vector
autoregressive models with weak assumptions on errors. <em>CSDA</em>,
<em>165</em>, 107321. (<a
href="https://doi.org/10.1016/j.csda.2021.107321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses diagnostic checking for vector autoregressive models with uncorrelated but not independent innovations. In this situation, the multivariate portmanteau tests are severely over-sized due to the misspecification of critical values obtained from the χ 2 χ2 distribution. To address this issue, a random weighting bootstrap procedure is proposed to approximate the null distribution when the error is assumed to be martingale difference sequence. When this assumption is violated, a blockwise random weighting is further applied to replicate the dependence structure of innovations. The first-order asymptotic validity of these bootstrap procedures is derived. Monte Carlo experiments under various scenarios suggest the effectiveness of the random weighting bootstrap approaches in comparison with existing approaches. Finally, the proposed testing procedure is illustrated in an application to analyze feedback dynamics between the real GNP growth and the unemployment rate in the US.},
  archive      = {J_CSDA},
  author       = {Muyi Li and Yanfen Zhang},
  doi          = {10.1016/j.csda.2021.107321},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107321},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bootstrapping multivariate portmanteau tests for vector autoregressive models with weak assumptions on errors},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal designs for order-of-addition experiments.
<em>CSDA</em>, <em>165</em>, 107320. (<a
href="https://doi.org/10.1016/j.csda.2021.107320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The order-of-addition (OofA) designs have received significant attention over recent years. It is of great interest to seek for efficient fractional OofA designs especially when the number of components is large. It has been recognized that constructing efficient fractional OofA designs is a challenging work. A systematic construction method for a class of efficient fractional OofA designs, called OofA orthogonal arrays (OofA-OAs), is proposed. It is shown that OofA-OAs are superior over any other type of fractional OofA designs for the predominant pair-wise ordering (PWO) model. The balance property of OofA-OAs is also developed. In addition, the capacity of OofA-OAs for estimating different models is investigated.},
  archive      = {J_CSDA},
  author       = {Yuna Zhao and Dennis K.J. Lin and Min-Qian Liu},
  doi          = {10.1016/j.csda.2021.107320},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107320},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Optimal designs for order-of-addition experiments},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust regression with compositional covariates.
<em>CSDA</em>, <em>165</em>, 107315. (<a
href="https://doi.org/10.1016/j.csda.2021.107315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biological high-throughput datasets, such as targeted amplicon-based and metagenomic sequencing data, are compositional. A common exploratory data analysis task is to infer robust statistical associations between high-dimensional microbial compositions and habitat- or host-related covariates. To address this, a general robust statistical regression framework RobRegCC (Robust Regression with Compositional Covariates) is proposed, which extends the linear log-contrast model by a mean shift formulation for capturing outliers. RobRegCC includes sparsity-promoting convex and non-convex penalties for parsimonious model estimation, a data-driven robust initialization procedure, and a novel robust cross-validation model selection scheme. The procedure is implemented in the R package robregcc . Extensive simulation studies show the RobRegCC &#39;s ability to perform simultaneous sparse log-contrast regression and outlier detection over a wide range of settings. To demonstrate the seamless applicability of the workflow to real data, the gut microbiome dataset from HIV patients are analyzed and robust associations between a sparse set of microbial species and host immune response from soluble CD14 measurements are inferred.},
  archive      = {J_CSDA},
  author       = {Aditya Mishra and Christian L. Müller},
  doi          = {10.1016/j.csda.2021.107315},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107315},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust regression with compositional covariates},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
