<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JPDC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jpdc---155">JPDC - 155</h2>
<ul>
<li><details>
<summary>
(2022). TERMS: Task management policies to achieve high performance
for mixed workloads using surplus resources. <em>JPDC</em>,
<em>170</em>, 74–85. (<a
href="https://doi.org/10.1016/j.jpdc.2022.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource contentions and performance interferences can lead to workload performance degradation in mixed-workload deployment clusters. Previous work guarantees the resource requirements of latency-sensitive tasks and reduces performance losses to batch jobs by reclaiming surplus resources from over-provisioned tasks. While the fragmentation of resources leads to a mismatch between provisioned resources and task requirements, resulting in high operation overheads and losses of task fairness. This paper proposes TERMS , the task management policies based on task relevance, resource distribution, and task fairness to achieve efficient and low-cost task management. TERMS mainly includes three types of management policies. The task scheduling policy can schedule new tasks according to task relevance. Task selection strategies select tasks for resource provisioning and task resumption based on resource requirements and task fairness. If necessary, the node selection strategy can be used to choose befitting target nodes based on task relevance and node resource information for task migration when eliminating straggler tasks. Evaluation results show that TERMS can further improve the performance of latency-sensitive services and batch jobs, reduce management overheads, and avoid operation failures.},
  archive      = {J_JPDC},
  author       = {Jinyu Yu and Wei Tong and Pengze Lv and Dan Feng},
  doi          = {10.1016/j.jpdc.2022.08.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {74-85},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {TERMS: Task management policies to achieve high performance for mixed workloads using surplus resources},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability of arrangement networks in terms of the
h-restricted edge connectivity. <em>JPDC</em>, <em>170</em>, 68–73. (<a
href="https://doi.org/10.1016/j.jpdc.2022.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The h -restricted edge connectivity λ h λh is an important parameter to measure the reliability of massive parallel computing system. The λ h λh of G is the minimum cardinality of a set of edges in G , if any, whose deletion disconnects G , and every remaining component has minimum degree at least h . The arrangement graph is a generalization of the star graph , but its order is more flexible than that of the star graph . This paper investigates the h -restricted edge connectivity of the ( n , k ) (n,k) -arrangement graph A n , k An,k , and determines that λ 1 ( A n , k ) = 2 k ( n − k ) − 2 λ1(An,k)=2k(n−k)−2 , λ 2 ( A n , k ) = 3 k ( n − k ) − 6 λ2(An,k)=3k(n−k)−6 with k ≤ n − 2 k≤n−2 .},
  archive      = {J_JPDC},
  author       = {Rui Zhu and Xue-Qian Zeng and Xiang-Jun Li and Meijie Ma},
  doi          = {10.1016/j.jpdc.2022.08.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {68-73},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliability of arrangement networks in terms of the h-restricted edge connectivity},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed-memory MPI parallelization scheme for
multi-domain incompressible SPH. <em>JPDC</em>, <em>170</em>, 53–67. (<a
href="https://doi.org/10.1016/j.jpdc.2022.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A parallel scheme for a multi-domain truly incompressible smoothed particle hydrodynamics (SPH) approach is presented. The proposed method is developed for distributed-memory architectures through the Message Passing Interface (MPI) paradigm as communication between partitions. The proposal aims to overcome one of the main drawbacks of the SPH method, which is the high computational cost with respect to mesh-based methods, by coupling a multi-resolution approach with parallel computing techniques. The multi-domain approach aims to employ different resolutions by subdividing the computational domain into non-overlapping blocks separated by block interfaces . The particles belonging to different blocks are efficiently distributed among processors ensuring well balanced loads . The parallelization procedure handles particle exchanges both throughout the blocks and the competence domains of the processors. The matching of the velocity values between neighbouring blocks is obtained solving a system of interpolation equations at each block interfaces through a parallelized BiCGSTAB algorithm. Otherwise, a whole pseudo-pressure system is solved in parallel considering the Pressure Poisson equations of the fluid particles of all the blocks and the interpolation equations of all the block interfaces . The employed test cases show the strong reduction of the computational efforts of the SPH method thanks to the interaction of the employed multi-resolution approach and the proposed parallel algorithms .},
  archive      = {J_JPDC},
  author       = {Alessandra Monteleone and Gaetano Burriesci and Enrico Napoli},
  doi          = {10.1016/j.jpdc.2022.08.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-67},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A distributed-memory MPI parallelization scheme for multi-domain incompressible SPH},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Process mapping on any topology with TopoMatch.
<em>JPDC</em>, <em>170</em>, 39–52. (<a
href="https://doi.org/10.1016/j.jpdc.2022.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mapping (or process placement) is a useful algorithmic technique to optimize the way applications are launched and executed onto a parallel machine. By taking into account the topology of the machine and the affinity between the processes, process mapping helps reducing the communication time of the whole parallel application. Here, we present TopoMatch , a generic and versatile library and algorithm to address the process placement problem. We describe its features and characteristics, and we report different use-cases that benefit from this tool. We also study the impact of different factors: sparsity of the input affinity matrix , trade-off between the speed and the quality of the mapping procedure as well as the impact of the uncertainty (noise) onto the input.},
  archive      = {J_JPDC},
  author       = {Emmanuel Jeannot},
  doi          = {10.1016/j.jpdc.2022.08.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {39-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Process mapping on any topology with TopoMatch},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utilization prediction-based VM consolidation approach.
<em>JPDC</em>, <em>170</em>, 24–38. (<a
href="https://doi.org/10.1016/j.jpdc.2022.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing energy consumption and optimizing resource usage in large cloud data centers is still an essential target for the current researchers and cloud providers . The state-of-the-art highlights the effectiveness of VM consolidation and live migrations in achieving reasonable solutions. However, most proposals consider only the real-time workload variations to decide whether a host is overloaded or underloaded, or to trigger migration actions. Such approaches may apply frequent and needless VM migrations leading to energy waste , performance degradation , and service-level agreement (SLA) violations. In this paper, we propose a consolidation approach based on the resource utilization prediction to determine the overloaded and underloaded hosts. The prediction method combines a Kalman filter and support vector regression (SVR) to forecast the host&#39;s future CPU utilization. Simulations are conducted on Cloudsim using real PlanetLab workloads to verify the performance of our proposal against existing benchmark algorithms. Experimental results demonstrate that our consolidation technique significantly reduces the SLA violation rate, number of VM migrations , and energy consumed in the datacenter.},
  archive      = {J_JPDC},
  author       = {Mirna Awad and Nadjia Kara and Aris Leivadeas},
  doi          = {10.1016/j.jpdc.2022.08.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-38},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Utilization prediction-based VM consolidation approach},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AMBLE: Adjusting mini-batch and local epoch for federated
learning with heterogeneous devices. <em>JPDC</em>, <em>170</em>, 13–23.
(<a href="https://doi.org/10.1016/j.jpdc.2022.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data privacy becomes increasingly important, federated learning applied to the training of deep learning models while ensuring the data privacy of devices is entering the spotlight. Federated learning makes it possible to process all data at once while processing data independently from various devices without collecting distributed local data in a central server. However, there are still challenges to overcome for the system of devices in federated learning such as communication overheads and the heterogeneity of the system. In this paper, we propose the Adjusting Mini-Batch and Local Epoch (AMBLE) approach, which adaptively adjusts the local mini-batch and local epoch size for heterogeneous devices in federated learning and updates the parameters synchronously. With AMBLE, we enhance the computational efficiency by removing stragglers and scaling the local learning rate to improve the model convergence rate and accuracy. We verify that federated learning with AMBLE is a stably trained model with a faster convergence speed and higher accuracy than FedAvg and adaptive batch size scheme for both identically and independently distributed (IID) and non-IID cases.},
  archive      = {J_JPDC},
  author       = {Juwon Park and Daegun Yoon and Sangho Yeo and Sangyoon Oh},
  doi          = {10.1016/j.jpdc.2022.07.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {AMBLE: Adjusting mini-batch and local epoch for federated learning with heterogeneous devices},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the impact on convolutional neural networks
with different model scales in AIoT domain. <em>JPDC</em>, <em>170</em>,
1–12. (<a href="https://doi.org/10.1016/j.jpdc.2022.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years many amazing deep learning models have been developed, but in the process of practical applications, people often find that these deep learning models have high requirements for hardware storage space and computing power. In Artificial Intelligent of Things (AIoT) scenario, the computing power of the edge or terminal side are relatively limited, therefore, most conventional deep learning models are difficult to be deployed into AIoT devices. It is significant to explore the different performance under different scales of deep learning models. In this paper, we mainly propose a method to analyze the impact of deep learning models with various sizes through various experiments. We employ slimmable network as a Neural Archtecture Search (NAS) tool to realize various model size freely, and evaluate them on the indicators of flops, robustness and accuracy. The experimental results show the variation of flops, robustness and accuracy with the various model sizes, which help understand the impact on performance of deep learning models with different scales in AIoT systems.},
  archive      = {J_JPDC},
  author       = {Longxin Lin and Zhenxiong Xu and Chien-Ming Chen and Ke Wang and Md. Rafiul Hassan and Md. Golam Rabiul Alam and Mohammad Mehedi Hassan and Giancarlo Fortino},
  doi          = {10.1016/j.jpdc.2022.07.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Understanding the impact on convolutional neural networks with different model scales in AIoT domain},
  volume       = {170},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stochastic conditional gradient algorithm for
decentralized online convex optimization. <em>JPDC</em>, <em>169</em>,
334–351. (<a href="https://doi.org/10.1016/j.jpdc.2022.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study in this paper several problems at the intersection of decentralized optimization and online learning . Decentralized optimization plays a vital role in machine learning and has recently garnered much attention due to its inherent advantage in handling edge computations. Many decentralized optimization algorithms , both projection and projection-free algorithms with theoretical guarantees, have been proposed in the literature, focusing mainly on offline settings. However, for most real-world machine learning problems, the data is often revealed online, for example, in the case of recommender systems , image/video processing, and stock portfolio management. Therefore, in this work, we study decentralized optimization within the framework of online settings with constraints imposed on the optimization solutions (e.g., sparsity or low rank of matrices). More specifically, we consider the problem of optimizing an aggregate of convex loss functions that arrive over time such that their components are distributed over a connected network. We present a consensus-based online decentralized Frank-Wolfe algorithm that uses stochastic gradient estimates, which achieves an asymptotically tight regret guarantee of O ( T ) O(T) where T is a given time horizon. Furthermore, we demonstrate the performance of this algorithm for optimizing the online multiclass logistic regression model on real-world standard image datasets ( M N I S T MNIST , C I F A R 10 CIFAR10 ) by comparing with centralized online algorithms . We achieve better regret bounds than the previously best-known decentralized constrained online algorithms .},
  archive      = {J_JPDC},
  author       = {Nguyễn Kim Thắng and Abhinav Srivastav and Denis Trystram and Paul Youssef},
  doi          = {10.1016/j.jpdc.2022.07.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {334-351},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A stochastic conditional gradient algorithm for decentralized online convex optimization},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strategic investments in distributed computing: A stochastic
game perspective. <em>JPDC</em>, <em>169</em>, 317–333. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a stochastic game with a dynamic set of players, for modeling and analyzing their computational investment strategies in distributed computing . Players obtain a certain reward for solving a problem, while incurring a certain cost based on the invested time and computational power. We present our framework while considering a contemporary application of blockchain mining, and show that the framework is applicable to certain other distributed computing settings as well. For an in-depth analysis, we consider a particular yet natural scenario where the rate of solving the problem is proportional to the total computational power invested by the players. We show that, in Markov perfect equilibrium, players with cost parameters exceeding a certain threshold, do not invest; while those with cost parameters less than this threshold, invest maximal power. We arrive at an interesting conclusion that the players need not have information about the system state as well as each others&#39; parameters, namely, cost parameters and arrival/departure rates. With extensive simulations and insights through mean field approximation, we study the effects of players&#39; arrival/departure rates and the system parameters on the players&#39; utilities.},
  archive      = {J_JPDC},
  author       = {Swapnil Dhamal and Walid Ben-Ameur and Tijani Chahed and Eitan Altman and Albert Sunny and Sudheer Poojary},
  doi          = {10.1016/j.jpdc.2022.07.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {317-333},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Strategic investments in distributed computing: A stochastic game perspective},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ZeroCross: A sidechain-based privacy-preserving cross-chain
solution for monero. <em>JPDC</em>, <em>169</em>, 301–316. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sidechain-based Cross-chain exchange protocols enable payers to exchange cryptocurrencies among different blockchains via a sidechain. Many efforts, such as P2DEX (ACNS&#39; 21), have been proposed to enhance cross-chain exchange privacy protection. However, existing sidechain-based cross-chain solutions for Monero on privacy concerns have limitations: requiring multiple pairs of parties paying simultaneously or fixed transaction amounts. This paper proposes ZeroCross, a novel privacy-preserving sidechain-based scheme that guarantees transaction unlinkability, exchanging fairness, and value confidentiality. ZeroCross designs: (i) a key exchange mechanism that guarantees exchanging fairness and (ii) a verification mechanism that utilizes CP-SNARK to ensure the transaction is confirmed without revealing the details of transactions. In addition, we discuss the influence of the remote side-channel attack in cross-chain exchange and the defence strategy. Finally, we prove the privacy and security of ZeroCross under the Universal Composability (UC) framework and evaluate the practical performance on computation and communication costs.},
  archive      = {J_JPDC},
  author       = {Yuxian Li and Jian Weng and Ming Li and Wei Wu and Jiasi Weng and Jia-Nan Liu and Shun Hu},
  doi          = {10.1016/j.jpdc.2022.07.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {301-316},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ZeroCross: A sidechain-based privacy-preserving cross-chain solution for monero},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent user-collaborative edge device APC-based MEC 5G
IoT for computational offloading and resource allocation. <em>JPDC</em>,
<em>169</em>, 286–300. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) supports delay-sensitive and excellent processing capacity services on the 5G Internet of Things (IoT) network. This research proposes an intelligent resource allocation policy to minimize average service latency and average energy consumption for an IoT system while maximizing the available processing capacity (APC). We express the APC as a function of communication and computation resources at the user and task edge devices. We demonstrated that reducing execution delay and energy usage could improve overall system service performance. We evaluate the savings in average latency and average energy consumption when we reduce execution delay by allocating resources to maximize APC. The 5G IoT network uses natural actor-critic deep reinforcement learning to tackle complicated resource allocation decisions, and the simulation shows that reducing execution time improves overall system performance. Our results improved execution time and energy usage compared to random search, greedy search, and deep Q-learning. In addition, our single centralized agent DRL outperforms Multi-agent DRL for the number of rewards and completed task achievable under different episodes.},
  archive      = {J_JPDC},
  author       = {Chidiebere Sunday Chidume and Christantus O. Nnamani},
  doi          = {10.1016/j.jpdc.2022.07.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {286-300},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intelligent user-collaborative edge device APC-based MEC 5G IoT for computational offloading and resource allocation},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed-memory tensor completion for generalized loss
functions in python using new sparse tensor kernels. <em>JPDC</em>,
<em>169</em>, 269–285. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor computations are increasingly prevalent numerical techniques in data science, but pose unique challenges for high-performance implementation. We provide novel algorithms and systems infrastructure which enable efficient parallel implementation of algorithms for tensor completion with generalized loss functions. Specifically, we consider alternating minimization, coordinate minimization, and a quasi-Newton (generalized Gauss-Newton) method. By extending the Cyclops library, we implement all of these methods in high-level Python syntax. To make possible tensor completion for very sparse tensors, we introduce new multi-tensor primitives, for which we provide specialized parallel implementations. We compare these routines to pairwise contraction of sparse tensors by reduction to hypersparse matrix formats, and find that the multi-tensor routines are more efficient in theoretical cost and execution time in experiments. We provide microbenchmarking results on the Stampede2 supercomputer to demonstrate the efficiency of the new primitives and Cyclops functionality. We then study the performance of the tensor completion methods for a synthetic tensor with 10 billion nonzeros and the Netflix dataset, considering both least squares and Poisson loss functions.},
  archive      = {J_JPDC},
  author       = {Navjot Singh and Zecheng Zhang and Xiaoxiao Wu and Naijing Zhang and Siyuan Zhang and Edgar Solomonik},
  doi          = {10.1016/j.jpdc.2022.07.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {269-285},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed-memory tensor completion for generalized loss functions in python using new sparse tensor kernels},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to achieve adaptive security for asynchronous BFT?
<em>JPDC</em>, <em>169</em>, 252–268. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider how to build practical asynchronous BFT protocols with adaptive security. In particular, we build two protocols in both the computational setting (where the adversary is limited to polynomial-time) and the stronger information-theoretic model (where the adversary is unbounded). In the computational model , we provide EPIC using adaptively secure key generation and common coin protocols. In the information-theoretical model, we provide HALE leveraging the classic local coin protocol of Bracha. HALE is more robust than EPIC and does not need distributed key generation. Via a five-continent deployment on Amazon EC2, we show EPIC is slightly slower for small and medium-sized networks than the most efficient asynchronous BFT protocols with static security. As the number of replicas is smaller than 46, EPIC&#39;s throughput is stable, achieving peak throughput of 8,000–12,500 tx/sec with a transaction size of 250 bytes. When the network size grows larger, EPIC is not as efficient as asynchronous BFT protocols with static security, with throughput of 4,000–6,300 tx/sec. We also show while HALE is in general less efficient than EPIC, HALE is reasonably fast, achieving 42,000 tx/sec and 3,400 tx/sec for the 4-server setting in the LAN/WAN environments, respectively. Remarkably, HALE outperforms EPIC in LANs when the number of replicas is smaller than 16.},
  archive      = {J_JPDC},
  author       = {Haibin Zhang and Chao Liu and Sisi Duan},
  doi          = {10.1016/j.jpdc.2022.07.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {252-268},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {How to achieve adaptive security for asynchronous BFT?},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous simulated annealing on the placement problem: A
beneficial race condition. <em>JPDC</em>, <em>169</em>, 242–251. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Race conditions , which occur when compute workers do not synchronise correctly, are considered undesirable in parallel computing , as they introduce often-unintended stochastic behaviour . This study presents an asynchronous parallel algorithm with a race condition, and demonstrates that it reaches a superior solution faster than the equivalent synchronous algorithm without the race condition. Specifically, a parallel simulated annealing algorithm that solves a graph mapping problem (placement) is used to explore this. This paper illustrates how problem size and degree of parallelism affects both the collision rate caused by the race condition, and convergence time. The asynchronous approach reaches a superior solution in half the time of the equivalent synchronous approach. The solver presented here can be applied to application deployment in distributed systems, and the concept can be applied to problems solvable by global optimisation methods, where fitness errors can be tolerated in exchange for faster execution.},
  archive      = {J_JPDC},
  author       = {Mark Vousden and Graeme M. Bragg and Andrew D. Brown},
  doi          = {10.1016/j.jpdc.2022.07.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {242-251},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Asynchronous simulated annealing on the placement problem: A beneficial race condition},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast parallel max-flow algorithm. <em>JPDC</em>,
<em>169</em>, 226–241. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new parallel algorithm for the max-flow problem on directed networks with single-source and single-sink is proposed. The algorithm is based on tree sub-networks and on efficient parallel algorithm to compute max-flows on the tree sub-networks. The latter algorithm is proved to be work-optimal and time-optimal. The parallel implementation of the complete algorithm is more efficient than the best known parallel algorithm for the max-flow problem in terms of time-complexity and the sequential implementation of the algorithm achieves the best known sequential time-complexity, without using any complex data-structures or complex manipulations on the network.},
  archive      = {J_JPDC},
  author       = {Yossi Peretz and Yigal Fischler},
  doi          = {10.1016/j.jpdc.2022.07.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {226-241},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A fast parallel max-flow algorithm},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal load balancing and assessment of existing load
balancing criteria. <em>JPDC</em>, <em>169</em>, 211–225. (<a
href="https://doi.org/10.1016/j.jpdc.2022.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel iterative applications often suffer from load imbalance, one of the most critical performance degradation factors. Hence, load balancing techniques are used to distribute the workload evenly to maximize performance. A key challenge is to know when to use load balancing techniques. In general, this is done through load balancing criteria, which trigger load balancing based on runtime application data and/or user-defined information. In the first part of this paper, we introduce a novel, automatic load balancing criterion derived from a simple mathematical model. In the second part, we propose a branch-and-bound algorithm to find the load balancing iterations that lead to the optimal application performance. This algorithm finds the optimal load balancing scenario in polynomial time while, to the best of our knowledge, it has never been addressed in less than an exponential time. Finally, we compare the performance of the scenarios produced by state-of-the-art load balancing criteria relative to the optimal load balancing scenario in synthetic benchmarks and parallel N-body simulations. In the synthetic benchmarks, we observe that the proposed criterion outperforms the other automatic criteria. In the numerical experiments, we show that our new criterion is, on average, 4.9\% faster than state-of-the-art load balancing criteria and can outperform them by up to 17.6\%. Moreover, we see in the numerical study that the state-of-the-art automatic criteria are at worst 26.43\% slower than the optimum and at best 10\% slower.},
  archive      = {J_JPDC},
  author       = {Anthony Boulmier and Nabil Abdennadher and Bastien Chopard},
  doi          = {10.1016/j.jpdc.2022.07.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {211-225},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimal load balancing and assessment of existing load balancing criteria},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Challenging the security of “a PUF-based hardware mutual
authentication protocol.” <em>JPDC</em>, <em>169</em>, 199–210. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, using Physical Unclonable Functions (PUF) to design lightweight authentication protocols for constrained environments such as the Internet of Things (IoT) has received much attention. In this direction, Barbareschi et al. recently proposed PHEMAP in Journal of Parallel and Distributed Computing , a PUF based mutual authentication protocol. Also, they extended it to the later designed Salted PHEMAP, for low-cost cloud-edge (CE) IoT devices. This paper presents the first third-party security analysis of PHEMAP and Salted PHEMAP to the best of our knowledge. Despite the designer&#39;s claim, we show that these protocols are vulnerable to impersonation, de-synchronization, and traceability attacks. The success probability of the proposed attacks is ‘1’, while the complexity is negligible. In addition, we introduce two enhanced lightweight authentication protocols based on PUF chains (called PBAP and Salted PBAP), using the same design principles as PHEMAP and Salted PHEMAP. With the performance evaluation and the security analysis, it is justified that the two proposed schemes are practically well suited for use in resource-constrained IoT environments.},
  archive      = {J_JPDC},
  author       = {Morteza Adeli and Nasour Bagheri and Honorio Martín and Pedro Peris-Lopez},
  doi          = {10.1016/j.jpdc.2022.06.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {199-210},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Challenging the security of “A PUF-based hardware mutual authentication protocol”},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel task offloading algorithm based on an integrated
trust mechanism in mobile edge computing. <em>JPDC</em>, <em>169</em>,
185–198. (<a href="https://doi.org/10.1016/j.jpdc.2022.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new computing model, mobile edge computing (MEC) is designed to better deal with various forms of service requests, such as computing intensity and delay sensitivity, in the era of big data and the Internet of Things (IoT). However, the development of MEC is still in its infancy, and many issues need to be further investigated. One of the key issues that needs to be addressed in MEC is rational task offloading . Due to the dynamic, real time and complex nature of the MEC environment, the security and reliability of edge data are becoming increasingly important. Based on the above problems, we construct a task offloading integrated trust evaluation mechanism and, combined with the double deep Q-network (DDQN) algorithm in deep reinforcement learning (DRL), propose a novel task offloading algorithm, named DDTMOA. Simulation results show that the DDTMOA algorithm can effectively reduce the average task response time and total system energy consumption while ensuring task offloading performance compared to other classical algorithms.},
  archive      = {J_JPDC},
  author       = {Zhao Tong and Feng Ye and Jing Mei and Bilan Liu and Keqin Li},
  doi          = {10.1016/j.jpdc.2022.07.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {185-198},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel task offloading algorithm based on an integrated trust mechanism in mobile edge computing},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implementation of bio-inspired hybrid algorithm with
mutation operator for robotic path planning. <em>JPDC</em>,
<em>169</em>, 171–184. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is an NP-hard problem that is aimed to satisfy multi-constraint optimization requirements. In autonomous robotics applications , path planning together with collision avoidance presents a challenging task. It necessitates the generation of possible search directions from a designated point to a fixed varying destination location satisfying spatial constraints. This paper presents a framework for the design of an intelligent multi-objective robotic path planning algorithm. The algorithm relies on the generation of way-points by hybridizing two meta-heuristics techniques, namely Grey Wolf Algorithm (GWO) and Particle Swarm Optimization (PSO). A frequency-based modification in GWO search operators is introduced to fasten the search process. An improvised search strategy is employed for collision detection and avoidance, which converts non-desired points into the desired point. Sensors are deployed around the robot vicinity for search optimization. Mutation operators are then introduced to improve path length by smoothing out the trajectory. The proposed algorithm&#39;s effectivity is then validated through extensive simulations, in which different condition environments are simulated. To validate the effectiveness of the proposed methodology, the results are compared with contemporary algorithms namely Minimum Angle Artificial Bee Colony (MAABC) algorithms, Hybrid Cuckoo Search-Bat Algorithm (BA-CSA), Bacterial Bolony (BC) and Genetic Algorithm (GA) algorithms. The results conclusively demonstrated that the proposed algorithm ensures effective performance in path smoothness and safety under a wide range of conditions.},
  archive      = {J_JPDC},
  author       = {Faiza Gul and Imran Mir and Deemah Alarabiat and Hamzeh Mohammad Alabool and Laith Abualigah and Suleman Mir},
  doi          = {10.1016/j.jpdc.2022.06.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {171-184},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Implementation of bio-inspired hybrid algorithm with mutation operator for robotic path planning},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architecture slack exploitation for phase classification and
performance estimation in server-class processors. <em>JPDC</em>,
<em>169</em>, 157–170. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a highly accurate performance estimation methodology that accounts for architecture slack in workloads. Our work leverages the advanced instrumentation available in POWER8 processor that monitors core pipeline activity in relation to off-core memory accesses to build metrics for architecture slack characterization for workloads. Using these metrics, we construct a workload classifier that classifies workloads as core-bound and memory-bound and propose a performance prediction model for change in processor frequency for each class of workload – cPerf and mPerf, respectively. We evaluated these models with SPECCPU and PARSEC benchmark suites on a POWER8 based OpenPOWER system. We observed that the predicted performance with our models has high accuracy (97\%) for both CPU and memory intensive benchmarks. We validated that the classifier is suitable to accurately classify phase of workloads during execution intervals. We developed an algorithm that uses classifier for phase classification and prediction models for performance estimation at runtime. We leveraged this algorithm and evaluated the execution time impacts of CPU and memory classified benchmarks.},
  archive      = {J_JPDC},
  author       = {Diyanesh Chinnakkonda and Karthick Rajamani and M.B. Srinivas},
  doi          = {10.1016/j.jpdc.2022.06.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {157-170},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Architecture slack exploitation for phase classification and performance estimation in server-class processors},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability of the weight vector generation method of the
multi-objective evolutionary algorithm and application. <em>JPDC</em>,
<em>169</em>, 130–156. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decomposition-based multi-objective evolutionary algorithm first generates a set of weight vectors in advance, and it is very important to select a set of appropriate weight vectors for the decomposition-based algorithm. A variety of weight vector generation methods have been proposed in the existing algorithms, but in most algorithms, a pre-defined weight vector generation method is still used, the pre-defined weight vector is too specialized for the simplex-like front surface, which results in poor performance on the front surface with irregularities. At the same time, most of the existing algorithms have proposed many new adaptive strategies for weight vectors, but if you generate a set of more suitable weight vectors at the beginning, and then use the update strategy, it can make the algorithm achieve a better balance between diversity and convergence. In order to select a suitable weight vector, this paper proposes a multi-stage MOEA to select a suitable weight vector. The algorithm is divided into multiple stages according to the evolution process, first of all, in the early stage of evolution, the reliability of multiple weight vector generation methods was evaluated according to the spearman correlation coefficient in statistics, choose the most suitable weight generation method; Secondly, this method can be applied to the search for high-quality solutions in the middle of evolution; Finally, a weight vector adaptive strategy is adopted in the overall evolution process. In the experiment, the proposed algorithm was analyzed in the benchmark test problem, mechanical bearing and light aircraft gear reducer. The experimental results show the effectiveness of the proposed algorithm.},
  archive      = {J_JPDC},
  author       = {Shuzhi Gao and Xuepeng Ren and Yimin Zhang and Haihong Tang},
  doi          = {10.1016/j.jpdc.2022.06.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {130-156},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliability of the weight vector generation method of the multi-objective evolutionary algorithm and application},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ad hoc systems management and specification with distributed
petri nets. <em>JPDC</em>, <em>169</em>, 117–129. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing mobile ad hoc systems is a difficult task due to the high volatility of the systems&#39; topology. Ad hoc systems are commonly defined by means of their constituent entities and the relationships between such entities, however, a formal specification and run-time execution model is missing. The benefit of a formal specification is that it can enable reasoning about local and global system properties, for example, determining whether the system can reach a given state. We propose a Petri net-based specification and execution model to manage ad hoc distributed systems. Our model enables spontaneous communication between previously unknown system components. The model is locally equivalent to standard Petri nets, and hence could be used for the verification of properties for system snapshots static with respect to connections and disconnection, in which it is possible to analyze liveness, reachability , or conflicts. We validate the usability of our distributed ad hoc Petri net model by modeling distributable systems as described by existing distributed Petri nets approaches. Additionally, we demonstrate the applicability and usability of the proposed model in distributed ad hoc networks by implementing the communication behavior of two prototypical ad hoc network applications, disaster and crisis management, and VANETs , successfully validating the appropriate behavior of the system in each case.},
  archive      = {J_JPDC},
  author       = {Juan Sebastian Sosa and Paul Leger and Hiroaki Fukuda and Nicolás Cardozo},
  doi          = {10.1016/j.jpdc.2022.06.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {117-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Ad hoc systems management and specification with distributed petri nets},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel-FST: A feature selection library for multicore
clusters. <em>JPDC</em>, <em>169</em>, 106–116. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a subfield of machine learning focused on reducing the dimensionality of datasets by performing a computationally intensive process. This work presents Parallel-FST, a publicly available parallel library for feature selection that includes seven methods which follow a hybrid MPI/multithreaded approach to reduce their runtime when executed on high performance computing systems. Performance tests were carried out on a 256-core cluster, where Parallel-FST obtained speedups of up to 229x for representative datasets and it was able to analyze a 512 GB dataset, which was not previously possible with a sequential counterpart library due to memory constraints.},
  archive      = {J_JPDC},
  author       = {Bieito Beceiro and Jorge González-Domínguez and Juan Touriño},
  doi          = {10.1016/j.jpdc.2022.06.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {106-116},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel-FST: A feature selection library for multicore clusters},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A secure three-factor authentication scheme for IoT
environments. <em>JPDC</em>, <em>169</em>, 87–105. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, many users&#39; extensive use of the Internet of Things (IoT) has made authentication an inevitable issue in the IoT. The currently existing authentication methods are subjected to many challenges by various factors such as the limited resources, the lack of authorization, and the need for a light-weighted authentication process . Therefore, it is essential to provide a security framework and protect the users&#39; privacy at the lowest cost. This paper proposes a three-factor-based authentication scheme , called defense-in-depth, for the IoT environments on the blockchain platform. The proposed protocol applies mutual authentication with user authorization using smart card registration on a private blockchain without the need for a trustable server. The use of Elliptic-Curve Cryptography (ECC) and the analysis of the security of the proposed protocol using AVISPA tool, formal/informal security analysis altogether indicate that the proposed scheme is more secure and efficient in terms of computational and communications costs.},
  archive      = {J_JPDC},
  author       = {AmirHossein Ghafouri Mirsaraei and Ali Barati and Hamid Barati},
  doi          = {10.1016/j.jpdc.2022.06.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {87-105},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A secure three-factor authentication scheme for IoT environments},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel flow-vector generation approach for malicious
traffic detection. <em>JPDC</em>, <em>169</em>, 72–86. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious traffic detection is one of the most important parts of cyber security . The approaches of using the flow as the detection object are recognized as effective. Benefiting from the development of deep learning techniques, raw traffic can be directly used as a feature to detect malicious traffic. Most existing work usually converts raw traffic into images or long sequences to express a flow and then uses deep learning technology to extract features and classify them, but the generated features contain much redundant or even useless information, especially for encrypted traffic. The packet header field contains most of the packet characteristics except the payload content, and it is also an important element of the flow. In this paper, we only use the fields of the packet header in the raw traffic to construct the characteristic representation of the traffic and propose a novel flow-vector generation approach for malicious traffic detection. The preprocessed header fields are embedded as field vectors, and then a two-layer attention network is used to progressively generate the packet vectors and the flow vector containing context information. The flow vector is regarded as the abstraction of the raw traffic and is used to classify. The experiment results illustrate that the accuracy rate can reach up to 99.48\% in the binary classification task and the average of AUC-ROC can reach 0.9988 in the multi-classification task.},
  archive      = {J_JPDC},
  author       = {Jian Hou and Fangai Liu and Hui Lu and Zhiyuan Tan and Xuqiang Zhuang and Zhihong Tian},
  doi          = {10.1016/j.jpdc.2022.06.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {72-86},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel flow-vector generation approach for malicious traffic detection},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel blockchain-based and proxy-oriented public audit
scheme for low performance terminal devices. <em>JPDC</em>,
<em>169</em>, 58–71. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing technology, more and more individuals/organizations are inclined to store their data in cloud server (CS). Cloud storage audit schemes can help data owner (DO) to confirm the integrity of their data, but the existing schemes still have some limitations. On one hand, the existing schemes assume that DO&#39;s terminal devices are computationally powerful enough to handle various operations in a timely manner. While in practice, the terminal devices probably are mobile phones, tablets and other devices with low computing power. On the other hand, the existing schemes rely on a fully trusted third-party auditor (TPA), but it is not in line with real application scenarios. Therefore, in this paper, we proposed a novel blockchain-based and proxy-oriented public audit (BBPO-PA) scheme for low performance terminal devices. Firstly, we introduced a trusted proxy authorized by DO, which can process and upload DO&#39;s encrypted files. Secondly, we applied blockchain in our scheme and utilized smart contracts instead of untrusted TPA to improve the reliability and stability of audit results. Thirdly, we took advantage of an index table to ensure that our scheme can support dynamic data operation. Finally, security analysis revealed that our scheme is provably secure in random oracle model . Meanwhile, performance analysis demonstrated that our scheme is efficient and especially suitable for low performance terminal devices.},
  archive      = {J_JPDC},
  author       = {Mande Xie and Qiting Zhao and Haibo Hong and Chen Chen and Jun Yu},
  doi          = {10.1016/j.jpdc.2022.06.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {58-71},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel blockchain-based and proxy-oriented public audit scheme for low performance terminal devices},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CP-SGD: Distributed stochastic gradient descent with
compression and periodic compensation. <em>JPDC</em>, <em>169</em>,
42–57. (<a href="https://doi.org/10.1016/j.jpdc.2022.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication overhead is the key challenge for distributed training. Gradient compression is a widely used approach to reduce communication traffic. When combined with a parallel communication mechanism method like pipeline, gradient compression technique can greatly alleviate the impact of communication overhead. However, there exist two problems of gradient compression technique to be solved. Firstly, gradient compression brings in extra computation cost, which will delay the next training iteration. Secondly, gradient compression usually leads to a decrease in convergence accuracy. In this paper, we combine parallel mechanism with gradient quantization and periodic full-gradient compensation, and propose a new distributed optimization method named CP-SGD, which can hide the overhead of gradient compression, overlap part of the communication and obtain high convergence accuracy. The local update operation in CP-SGD allows the next iteration to be launched quickly without waiting for the completion of gradient compression and the current communication process. Besides, the accuracy loss caused by gradient compression is solved by k-step correction method introduced in CP-SGD, which provides a gradient correction every k iterations. We prove that CP-SGD has a convergence guarantee and it achieves at least O ( 1 K + 1 K ) O(1K+1K) convergence rate, where K is the number of iterations. We conduct extensive experiments on MXNet to verify the convergence properties and scaling performance of CP-SGD. Experimental results on a 32-GPU cluster show that convergence accuracy of CP-SGD is close to or even slightly better than that of S-SGD, and its end-to-end time is 30\% less than 2-bit gradient compression under a 56Gbps bandwidth environment. In addition, we analyze the performance of CP-SGD when training on 8, 16 and 32 GPUs. It is found that CP-SGD is suitable for most compression-supported update algorithms, and its scalability is approximately linear.},
  archive      = {J_JPDC},
  author       = {Enda Yu and Dezun Dong and Yemao Xu and Shuo Ouyang and Xiangke Liao},
  doi          = {10.1016/j.jpdc.2022.05.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {42-57},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CP-SGD: Distributed stochastic gradient descent with compression and periodic compensation},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimized protocol for cost effective communication in a
multi-agent environment. <em>JPDC</em>, <em>169</em>, 24–41. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile agents can migrate and communicate through interaction messages in a multi-agent environment. A multi-agent environment should provide a mechanism where mobile agents can be tracked easily and communication among them can take place while consuming the least amount of resources. Among all the proposed schemes the home based and forward pointing based scheme seems to be the best in achieving the desired objectives. The analysis shows that besides offering some advantages the aforementioned approach suffers from using high amounts of memory, high transmission cost, and the single point of failure problems. In this research we propose managing forwarding pointers intelligently by removing them from hosts when they are not required, preventing unnecessary update messages when the agent&#39;s mobility goes high, calculating the shortest path instantly after the agent&#39;s migration, and acknowledging only high priority interaction messages. We have validated our proposed approach with the help of experiments on a simulation tool OCEMAgents. The results show that the proposed approach has achieved all the objectives that are claimed and it has reduced memory and network overhead by minimizing the usage of forwarding pointers and by reducing the transmission cost. Besides that the proposed approach can effectively handle the single point of failure problem. Based on the results, we can safely declare that the proposed approach can be cost-effective if it is implemented in a real multi-agent system.},
  archive      = {J_JPDC},
  author       = {Hafza Nida Tariq and Muhammad Bilal Bashir},
  doi          = {10.1016/j.jpdc.2022.06.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-41},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An optimized protocol for cost effective communication in a multi-agent environment},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cluster-based routing method with authentication
capability in vehicular ad hoc networks (VANETs). <em>JPDC</em>,
<em>169</em>, 1–23. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing is challenging in vehicular ad hoc networks due to their features, such as high mobility of nodes and unstable wireless communication links. Therefore, it is an interesting issue for researchers. In addition, it is very important to design an authentication mechanism between the source node and the destination node because these networks are exposed to many attacks due to their features mentioned above. In this paper, we present a fuzzy logic-based routing method with authentication capability in vehicular ad hoc networks. The proposed routing method has three phases: clustering phase, routing phase between cluster head nodes, and authentication phase. In the first phase, vehicles are clustered using an efficient scheme. In the proposed method, we define two types of data packets: immediate and ordinary. Various data packets have different route discovery processes that are described in Phase 2. Note that any data packet type is divided into two groups: simple and secure. Simple data packets have no authentication mechanism. On the other hand, secure data packets use an authentication mechanism based on message authentication code (MAC) and symmetric key cryptography. We simulate the proposed method using NS2. Simulation results are compared with three routing protocols including, AODV, R 2 SCDT R2SCDT , and 3VSR. Experiments show that the proposed method outperforms others in terms of end-to-end delay, packet collision, packet delivery rate (PDR), packet loss rate (PLR), and throughput. However, it increases the routing overhead slightly.},
  archive      = {J_JPDC},
  author       = {Mohammad Sadegh Azhdari and Ali Barati and Hamid Barati},
  doi          = {10.1016/j.jpdc.2022.06.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A cluster-based routing method with authentication capability in vehicular ad hoc networks (VANETs)},
  volume       = {169},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on computer architecture and high-performance
computing. <em>JPDC</em>, <em>168</em>, 137–138. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Jorge G. Barbosa and Lúcia M.A. Drummond and Laurent Lefèvre},
  doi          = {10.1016/j.jpdc.2022.06.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {137-138},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Special issue on computer architecture and high-performance computing},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning for cost-effective IoT service
caching at the edge. <em>JPDC</em>, <em>168</em>, 120–136. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the edge computing environment, Internet of Things (IoT) application service providers can rent resources from edge servers to cache their service items such as datasets and code libraries, and thus significantly reducing the service request latency and the core network traffic. Since IoT service providers need to pay for the rented edge computing resources, it is essential to find a dynamical service caching strategy to minimize the service cost while optimizing the performance objective such as service latency reduction . However, most of the existing studies either overlooked the problem of collaborative service caching or failed to consider the system&#39;s long-term service cost and latency. In this paper, to address such a problem, we coordinate multiple edge servers to cache service items and formulate the collaborative service caching problem using a multi-agent multi-armed bandit model. Furthermore, we propose a utility-aware collaborative service caching (UACSC) scheme based on a multi-agent reinforcement learning . The UACSC scheme can coordinate multiple edge servers to make a dynamic joint caching decision, aiming at maximizing the system&#39;s long-term utility. To evaluate the performance of our proposed scheme, we implement four representative baseline algorithms and compare them with six different performance metrics. In addition, a real-world case study is also presented to demonstrate the effectiveness of the UACSC scheme. Comprehensive experimental results show that the UACSC scheme can effectively coordinate multiple edge servers to cache service items, and achieve higher service latency reduction and lower service cost compared with other baseline algorithms.},
  archive      = {J_JPDC},
  author       = {Binbin Huang and Xiao Liu and Yuanyuan Xiang and Dongjin Yu and Shuiguang Deng and Shangguang Wang},
  doi          = {10.1016/j.jpdc.2022.06.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {120-136},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reinforcement learning for cost-effective IoT service caching at the edge},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards scalable and efficient deep-RL in edge computing: A
game-based partition approach. <em>JPDC</em>, <em>168</em>, 108–119. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most edge-based Deep Reinforcement Learning (Deep-RL) applications have been deployed in the edge network, however, their mainstream studies are still short of adequate considerations on its limited compute and bandwidth resources. In this paper, we investigate the near on-policy of actions taking in distributed Deep-RL architecture, and propose a “hybrid near on-policy” Deep-RL framework, called Coknight , by leveraging a game-theory based DNN partition approach. We first formulate the partition problem into a variant of knapsack problem in device-edge setting, and then transform it into a potential game with a formal proof. Finally, we show the problem is NP-complete whereby an efficient distributed algorithm based on the potential game theory is developed from device perspective to achieve fast and dynamic partitioning. Coknight not only significantly improves the resource efficiency of the Deep-RL but also allows the inference to enforce the scalability of the actor policy. We prototype the framework with extensive experiments to validate our findings. The experimental results show that with the premise of a rapid convergence guarantee, Coknight , compared with Seed-RL , can reduce GPU utilization by 30\% while providing large-scale scalability.},
  archive      = {J_JPDC},
  author       = {Hao Dai and Jiashu Wu and Yang Wang and Chengzhong Xu},
  doi          = {10.1016/j.jpdc.2022.06.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {108-119},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards scalable and efficient deep-RL in edge computing: A game-based partition approach},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic resource provisioning for service-based cloud
applications: A bayesian learning approach. <em>JPDC</em>, <em>168</em>,
90–107. (<a href="https://doi.org/10.1016/j.jpdc.2022.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciding the correct extent of resources needed to run the various cloud services is always a challenge. Often in such dynamic environments, there is a tremendous need for accurate predictions and timely decision making methodologies to estimate the future demands within a minimal cost. This brings in a need to elucidate the research divergence for optimal dynamic resource provisioning that predicts the future enumerated resources on the support of application&#39;s type. This paper proposes a framework to provision the resources in an optimal way, by combining the concepts of autonomic computing, linear regression and Bayesian learning . The use of Bayesian learning to the proposed model helps in a proactive decision making process and provide a solid theoretical framework to estimate the future predictions using the prior information available. The autonomic resource provisioning framework proposed here is developed using CloudSim toolkit inspired by a cloud layer model. The efficacy of the proposed technique is evaluated using real world workload traces from google followed by the traces from Clarknet. The model is evaluated for various parameters namely – response time, SLA violations, virtual machine usage hours and cost. It is found that the proposed model lowers the overall cost by 31\% with the increase in the usage of resources by 12\% when compared with the other existing approaches.},
  archive      = {J_JPDC},
  author       = {Reena Panwar and M. Supriya},
  doi          = {10.1016/j.jpdc.2022.06.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {90-107},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dynamic resource provisioning for service-based cloud applications: A bayesian learning approach},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning-based edge caching optimization method for
cost-driven planning process over IIoT. <em>JPDC</em>, <em>168</em>,
80–89. (<a href="https://doi.org/10.1016/j.jpdc.2022.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has been considered as a leading paradigm to satisfy the low latency demand for some computation-intensive or data-intensive applications, especially for IIoT applications such as automatic line scheduling of the Internet of Vehicles, time-sensitive supply-chain supervision, and smart control of complex industrial processes. In the edge computing environment, app vendors prefer to cache their app data on edge servers to ensure low latency service. However, it is frequently a challenge in practice, because cache spaces on edge servers are limited and expensive. In view of this challenge, a d eep l earning-based e dge c aching o ptimization method, named DLECO, is proposed to reduce the cost during the cache planning process. In this paper, the edge app data caching problem is formulated as a constrained optimization problem. Then, the specific design of DLECO with a deep learning model is shown, which aims to minimize the overall system cost with lower service latency. The performance of DLECO is analyzed theoretically and experimentally with a collection of data from the real world. The experimental results show its superior performance through comparison with three representative methods.},
  archive      = {J_JPDC},
  author       = {Bowen Liu and Xutong Jiang and Xin He and Lianyong Qi and Xiaolong Xu and Xiaokang Wang and Wanchun Dou},
  doi          = {10.1016/j.jpdc.2022.06.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-89},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A deep learning-based edge caching optimization method for cost-driven planning process over IIoT},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abaci-finder: Linux kernel crash classification through
stack trace similarity learning. <em>JPDC</em>, <em>168</em>, 70–79. (<a
href="https://doi.org/10.1016/j.jpdc.2022.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developers often classify crashes by stack traces to analyze, locate and fix kernel bugs. Existing stack-trace-based crash classification approaches rely on string matching and statistical features, which ignore crash semantic contexts and cannot explore high-order correlations. Deep-learning-based approaches use crash embeddings and output end-to-end features for classification. However, they ignore kernel-specific information, which limits classification performance. Regarding these issues, we propose abaci-finder, a deep-learning-based classification framework specific to Linux kernel crashes. We first model the kernel stack trace as a stack frames sequence and then perform stack trace preprocessing. Then, we propose a vectorization method specific to kernel stack traces, called kstack2vec, to extract features with consideration for function semantics and kernel-specific offsets information. Finally, we exploit an attention-based BiLSTM neural network for classification, with consideration for both frame context and key frames in traces. The experiments on the real Linux kernel crash dataset indicate that abaci-finder outperforms existing methods of crash classification.},
  archive      = {J_JPDC},
  author       = {Heyuan Shi and Guyu Wang and Ying Fu and Chao Hu and Houbing Song and Jian Dong and Kun Tang and Kai Liang},
  doi          = {10.1016/j.jpdc.2022.06.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Abaci-finder: Linux kernel crash classification through stack trace similarity learning},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HY-DBSCAN: A hybrid parallel DBSCAN clustering algorithm
scalable on distributed-memory computers. <em>JPDC</em>, <em>168</em>,
57–69. (<a href="https://doi.org/10.1016/j.jpdc.2022.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dbscan is a density-based clustering algorithm which is well known for its ability to discover clusters of arbitrary shape as well as to distinguish noise. As it is computationally expensive for large datasets, research studies on the parallelization of Dbscan have been received a considerable amount of attention. In this paper we present an exact, efficient and scalable parallel Dbscan algorithm which we call Hy-Dbscan . It employs three major techniques to enable scalable data clustering on distributed-memory computers i ) a modified kd-tree for domain decomposition, ii ) a spatial indexing approach based on grid and inference, and iii ) a cluster merging scheme based on distributed Rem&#39;s Union-Find algorithm. Moreover, Hy-Dbscan exploits process level and thread level parallelization. In experiments, we have demonstrated performance and scalability using two scientific datasets on up to 2048 cores of a distributed-memory computer. Through extensive evaluation, we show that Hy-Dbscan significantly outperforms previous state-of-the-art Dbscan implementations.},
  archive      = {J_JPDC},
  author       = {Guoqing Wu and Liqiang Cao and Hongyun Tian and Wei Wang},
  doi          = {10.1016/j.jpdc.2022.06.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {57-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HY-DBSCAN: A hybrid parallel DBSCAN clustering algorithm scalable on distributed-memory computers},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ALBERT: An automatic learning based execution and resource
management system for optimizing hadoop workload in clouds.
<em>JPDC</em>, <em>168</em>, 45–56. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hadoop is a popular computing framework designed to deliver timely and cost-effective data processing on a large cluster of commodity machines. It relieves the burden of the programmers dealing with distributed programming, and an ecosystem of Big Data solutions has developed around it. However, Hadoop&#39;s job execution time can greatly depend on its runtime configurations and resource selections. Given the more than 100 job configuration settings provided by Hadoop , and diverse resource instance options in a cloud or virtualized computing environment, running Hadoop jobs still requires a substantial amount of expertise and experience. To address this challenge, we apply a deep neural network to predict Hadoop&#39;s job time based on historical execution data, and propose optimization methods to reduce job execution time and cost. The results show that our prediction method achieves almost 90\% time prediction accuracy and clearly outperforms three other state-of-the-art regression-based prediction methods. Based on the time prediction, our proposed configuration search method and job scheduling algorithm successfully shorten the execution time of a single Hadoop job by more than a factor of 2 and reduce the time of processing a batch of Hadoop jobs by 40\%∼65\%.},
  archive      = {J_JPDC},
  author       = {Chen-Chun Chen and Kai-Siang Wang and Yu-Tung Hsiao and Jerry Chou},
  doi          = {10.1016/j.jpdc.2022.05.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {45-56},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ALBERT: An automatic learning based execution and resource management system for optimizing hadoop workload in clouds},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A GAN-based method for time-dependent cloud workload
generation. <em>JPDC</em>, <em>168</em>, 33–44. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To design repeatable and comparable resource management policies for data centers, researchers mainly conduct experiments in the simulation environment, which requires large-scale workload traces to simulate real scenes. However, issues related to data collection, security and privacy hinder the public availability of cloud workload datasets. Though workload generation is a promising solution, due to the unpredictable time dependency, cloud workloads are difficult to model. In light of this, we propose a novel end-to-end model for time-dependent cloud workload generation using Generative Adversarial Networks, which adopts improved Temporal Convolution Networks and Spectral Normalization to capture the time dependency and stabilize the adversarial training . Experimental results on real cloud datasets demonstrate that our model can efficiently generate realistic workloads that fulfill the diversity, fidelity and usefulness. Further, we also propose a conditional GAN which is trained with labeled data and can generate specific kind of workloads according to the input.},
  archive      = {J_JPDC},
  author       = {Weiwei Lin and Kun Yao and Lan Zeng and Fagui Liu and Chun Shan and Xiaobin Hong},
  doi          = {10.1016/j.jpdc.2022.05.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {33-44},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A GAN-based method for time-dependent cloud workload generation},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Workflow simulation and multi-threading aware task
scheduling for heterogeneous computing. <em>JPDC</em>, <em>168</em>,
17–32. (<a href="https://doi.org/10.1016/j.jpdc.2022.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing systems . This problem has proved to be NP-complete even for the homogeneous case , heading research efforts in obtaining low complexity heuristics that produce good quality schedules. Such an example is HEFT, one of the most efficient list scheduling heuristics in terms of makespan and robustness. In this paper, we propose two task scheduling methods for heterogeneous computing systems that can be integrated to several task scheduling algorithms. First, a method that improves the scheduling time (the time for obtaining the output schedule) of a family of task scheduling algorithms is delivered without sacrificing the schedule length, when the computation costs of the application tasks are unknown. Second, a method that improves the scheduling length (makespan) of several task scheduling algorithms is proposed, by identifying which tasks are going to be executed as single-threaded and which as multi-threaded implementations, as well as the number of the threads used. We showcase both methods by using HEFT popular algorithm, but they can be integrated to other algorithms too, such as HCPT, HPS, PETS and CPOP. The experimental results, which consider 14580 random synthetic graphs and five real world applications , show that by enhancing HEFT algorithm with the two proposed methods, significant makespan gains and high scheduling time gains, are achieved.},
  archive      = {J_JPDC},
  author       = {Vasilios Kelefouras and Karim Djemame},
  doi          = {10.1016/j.jpdc.2022.05.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-32},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Workflow simulation and multi-threading aware task scheduling for heterogeneous computing},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MB-MaaS: Mobile blockchain-based mining-as-a-service for
IIoT environments. <em>JPDC</em>, <em>168</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a mobile blockchain (MB) based mining-as-a-service (MaaS) scheme, MB-MaaS for resource-constrained industrial internet-of-things (IIoT) environments. The scheme addresses the research gaps of fixed static allocation for miners to perform computationally intensive mining tasks through a multi-hop computational offloading (CO) scheme and addresses an auction mechanism for a fair bidding process among the miner nodes. The scheme operates in three phases. In the first phase, a multi-hop CO scheme with a fair incentive policy is formulated for miners. The CO schemes offer guaranteed offloading services to mobile devices from far-edge systems through a chain of neighbor nodes. Then, in the second phase, MaaS is proposed to leverage expensive mining tasks through 5G-enabled pico/femtocells. Integration of 5G allows massive end-to-end device and service connectivity. As IIoT ecosystems have limited memory and compute requirements, MaaS assures that the proposed consensus has a responsive validation and mining time. To make the data exchange in the consensus process lightweight, and allow a large number of sensors to share the data in a lightweight manner, an effective consensus mechanism Lightweight Proof-of-Proximity (LPoP), is proposed that forms group validations instead of single block validation. The data is exchanged through javascript object notation (JSON) format, maintaining a steady transaction rate. MB-MaaS is compared against the existing scheme for parameters bid thresholds and request servicing times, and mining and consensus formation. For example, the request serving time at 12 requests is improved by 56.78\%, and a significant improvement of 26.47\% is observed for processed blocks; parsing time on average is improved by 7.89\%. The comparative analysis suggests that the scheme is more efficient than other competing approaches.},
  archive      = {J_JPDC},
  author       = {Pronaya Bhattacharya and Farnazbanu Patel and Sudeep Tanwar and Neeraj Kumar and Ravi Sharma},
  doi          = {10.1016/j.jpdc.2022.05.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MB-MaaS: Mobile blockchain-based mining-as-a-service for IIoT environments},
  volume       = {168},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and portable GEMM-based convolution operators for
deep neural network training on multicore processors. <em>JPDC</em>,
<em>167</em>, 240–254. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) play a crucial role in many image recognition and classification tasks , recommender systems , brain-computer interfaces, etc. As a consequence, there is a notable interest in developing high performance realizations of the convolution operators, which concentrate a significant portion of the computational cost of this type of neural networks . In a previous work, we introduced a portable, high performance convolution algorithm, based on the BLIS realization of matrix multiplication, which eliminates most of the runtime and memory overheads that impair the performance of the convolution operators appearing in the forward training pass, when performed via explicit im2col transform. In this paper, we extend our ideas to the full training process of CNNs on multicore processors , proposing new high performance strategies to tackle the convolution operators that are present in the more complex backward pass of the training process, while maintaining the portability of the realizations. In addition, we conduct a full integration of these algorithms into a framework for distributed training of CNNs on clusters of computers, providing a complete experimental evaluation of the actual benefits in terms of both performance and memory consumption. Compared with baseline implementation , the use of the new convolution operators using pre-allocated memory can accelerate the training by a factor of about 6\%–25\%, provided there is sufficient memory available. In comparison, the operator variants that do not rely on persistent memory can save up to 70\% of memory.},
  archive      = {J_JPDC},
  author       = {Sergio Barrachina and Manuel F. Dolz and Pablo San Juan and Enrique S. Quintana-Ortí},
  doi          = {10.1016/j.jpdc.2022.05.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {240-254},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient and portable GEMM-based convolution operators for deep neural network training on multicore processors},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of elliptic curve scalar multiplication using
constraint based scheduling. <em>JPDC</em>, <em>167</em>, 232–239. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptic Curve Cryptography is public key cryptography that features smaller keys, ciphertexts , and signatures and is faster than RSA at the same security level . Scalar multiplication is the main and the most compute-intensive operation in the generation of keys. Point Addition, Doubling and Inversion are the basic operations for scalar multiplication. Inversion is a very expensive operation as compared to multiplication, addition and squaring in the finite fields with an affine coordinate system. López-Dahab coordinates are the best alternative to reduce the inversion overhead in scalar computation. Area, Delay and Power trade-offs are the main constraints in hardware implementations of scalar multiplication. In this paper, optimization of elliptic curve scalar multiplication using constraint-based scheduling for the López-Dahab coordinate system is proposed. Data dependency graphs of point addition and doubling are modified for optimization of area and delay. The proposed architecture is implemented on Altera Stratix-II FPGA . The constraint is applied on the field multiplication operation and the considerable area is reduced. The proposed architecture computes scalar multiplication in 11.43 μs and takes 9856 ALMs. The performance comparison with state of the art shows that area is reduced by 41.21\%, delay is reduced by 2.4\% and Area-Delay-Product is improved.},
  archive      = {J_JPDC},
  author       = {Pravin Zode and Raghavendra Deshmukh},
  doi          = {10.1016/j.jpdc.2022.05.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {232-239},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimization of elliptic curve scalar multiplication using constraint based scheduling},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A secure data transmission scheme based on multi-protection
routing in datacenter networks. <em>JPDC</em>, <em>167</em>, 222–231.
(<a href="https://doi.org/10.1016/j.jpdc.2022.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of protection routing guarantees the existence of a loop-free alternate path for packet forwarding when a single link or node failure occurs. By Tapolcai&#39;s method, the presence of two completely independent spanning trees (CISTs for short) suffices to configure a protection routing. This article extends the idea of protection routing to involve more CISTs and attach a secure mechanism in the configuration, which we call the secure multi-protection routing scheme (SMPR-scheme). Then, we use the SMPR-scheme to deal with privacy-preserving data transmissions, such as downloading personal medical records, tax bills, or other private information. To evaluate the effectiveness of the SMPR-scheme, we develop a probabilistic model that allows some malicious nodes to collect information illegally through neighboring access. We experimented with SMPR-scheme on the BCube (i.e., the generalized hypercube) datacenter network. Simulation results show that data transmission using SMPR-scheme ensures confidentiality (i.e., no node other than the recipient can receive the complete message) and effectively resists privacy collection even under malicious infringement.},
  archive      = {J_JPDC},
  author       = {Xiao-Yan Li and Wanling Lin and Wenzhong Guo and Jou-Ming Chang},
  doi          = {10.1016/j.jpdc.2022.05.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {222-231},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A secure data transmission scheme based on multi-protection routing in datacenter networks},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue: Connected vehicles meet big data
technologies: Recent advances and future trends. <em>JPDC</em>,
<em>167</em>, 221. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Ahmed Mostefaoui and Geyong Min and Peter Müller},
  doi          = {10.1016/j.jpdc.2022.05.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {221},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Special issue: connected vehicles meet big data technologies: recent advances and future trends},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An intelligence energy consumption model based on BP neural
network in mobile edge computing. <em>JPDC</em>, <em>167</em>, 211–220.
(<a href="https://doi.org/10.1016/j.jpdc.2022.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing an accurate edge server power model is helpful for resource providers to predict and optimize power consumption within edge data centers . Considering the fact that the accuracy of the previous energy consumption model is easily affected by the workload types, this paper develops an edge server power model based on BP (back propagation) neural network and feature selection, which is denoted by DSBF. For different task types, DSBF leverages “principal component analysis (PCA)” to analyze the contribution of each energy consumption parameter and selects “representative parameter”, and then builds a power model based on BP neural network. In contrast to other power models, DSBF can effectively handle the variable workload. To measure the effectiveness of the DSBF model, a series of experiments were conducted. The results suggest that compared with other energy consumption models, DSBF can better adapt to the changing workload and has advantages in predicting the accuracy of the energy consumption model .},
  archive      = {J_JPDC},
  author       = {Zhou Zhou and Yangfan Li and Fangmin Li and Hongbing Cheng},
  doi          = {10.1016/j.jpdc.2022.05.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {211-220},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An intelligence energy consumption model based on BP neural network in mobile edge computing},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding node connection modes in multi-rail fat-tree.
<em>JPDC</em>, <em>167</em>, 199–210. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Multi-Rail networks has become a popular choice for many leading HPC systems to overcome bandwidth limitations. Little is known, theoretically or practically, how the connection modes between multi-port nodes and switching network influence the performance of system. This work provides a detailed analysis of different node connection modes in Multi-Rail Fat-tree, which involves Single-Plane Multi-Rail Fat-tree and Multi-Plane Multi-Rail Fat-tree. We evaluate different node connection modes by theoretical analysis and flit level simulation. We show that there are great differences across various node connection modes. Among the key differences are the following: cost, average shortest path length, fault-tolerance, path diversity, latency and throughput. In addition, we propose MR-tree and MP-tree respectively to gain a deep understanding of relevant issues and improve the network performance. Our findings leave open the possibility that optimization of node connection mode can yield better results for Multi-Rail Fat-tree.},
  archive      = {J_JPDC},
  author       = {Yuyang Wang and Dezun Dong and Fei Lei},
  doi          = {10.1016/j.jpdc.2022.04.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {199-210},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Understanding node connection modes in multi-rail fat-tree},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid multi-grid parallelisation of WAVEWATCH III model on
spherical multiple-cell grids. <em>JPDC</em>, <em>167</em>, 187–198. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical Multiple-Cell (SMC) grid is an unstructured grid , supporting flexible domain shapes and multi-resolutions. It retains the quadrilateral cells as in the latitude-longitude (lat-lon) grid so that simple finite-difference schemes can be used. Sub-timesteps are applied on refined cells and grid cells are merged at high latitudes to relax the CFL restriction. A fixed reference direction is used in polar regions to solve vector polar problems. The SMC grid was implemented in the WAVEWATCH III (WW3) wave model in 2012 as an alternative for the lat-lon grid and updated in the latest WW3 V6.07. The WW3 model is parallelised by wave spectral component decomposition (CD) in MPI mode, which has a limit on number of MPI ranks. Hybrid or combined MPI-OpenMP parallelisation may extend the node usage but the OpenMP scalability flattens out beyond a few threads. Another parallelisation method that combines CD with domain decomposition (DD) is enabled in WW3 model by a multi-grid framework for further extension of node usage. Those regular lat-lon grid parallelisation options are gradually added to the SMC grid and this article reports the recent extension of the SMC grid into the multi-grid framework with hybrid parallelisation. The flexible domain shape of the SMC grid allows optimised domain splitting and minimised boundary exchanges. The combined CD-DD method is tested on SMC sub-grids with various hybrid node-thread combinations. Results indicate that switching from MPI to hybrid MPI-OpenMP mode can halve the global model elapsed time and using hybrid CD-DD on 3 SMC sub-grids may reduce it further by 30\%. Elapsed time for one model day run is reduced from about 3 min on 12 nodes to less than 1 min on 90 or 180 nodes. Besides, the hybrid multi-grid method reduces memory demand on one computing node and allows future model updates for higher resolutions.},
  archive      = {J_JPDC},
  author       = {Jian-Guo Li},
  doi          = {10.1016/j.jpdc.2022.05.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {187-198},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hybrid multi-grid parallelisation of WAVEWATCH III model on spherical multiple-cell grids},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ESDU: An elastic stripe-based delta update method for
erasure-coded cross-data center storage systems. <em>JPDC</em>,
<em>167</em>, 173–186. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The erasure-coded cross-data center storage system can achieve high disaster tolerance and low redundancy. But as it has large cross-data center update traffic, its data update time is long. In erasure-coded storage systems, each data object is sequentially divided into several stripes, and each stripe consists of several data packets. When erasure-coded stripes do not undergo insertion or deletion, existing work can effectively reduce cross-data center update traffic by performing delta updates—delta update methods can update the old stripe without transferring matched packets that are new and old stripes&#39; duplicate packets with the same offset-within-stripe. However, because existing delta update methods&#39; stripe size is fixed, when a stripe undergoes insertion or deletion, its subsequent stripes&#39; duplicate packets&#39; offset-within-stripe will change. In this scenery, the matched packet number is small, resulting in large cross-data center update traffic. This paper proposes an elastic stripe-based delta update method for erasure-coded cross-data center storage systems (ESDU). Under insertion or deletion, ESDS tries to avoid duplicate packets&#39; offset-within-stripe changing (i.e., maximizing matched packets) by adjusting the stripes&#39; size flexibly according to the duplicate packet locating result. So, it can reduce cross-data center traffic. Moreover, ESDU can optimize stripes&#39; update topology based on the location information of storage nodes to reduce cross-data center update traffic further. In addition, we implement an erasure-coded cross-data center storage system adopting ESDU, called ECESD. Experiments with the workloads derived from EduCoder&#39;s real-world trace show that compared with the existing erasure-coded cross-data center storage system adopting the fixed stripe-based delta update method, ECESD reduces average update time by 89.6\%. Moreover, compared with a replication-based storage system with a delta update method (HadoopRsync), ECESD achieves an 8.3\% shorter update time and much smaller redundancy.},
  archive      = {J_JPDC},
  author       = {Han Bao and Yijie Wang},
  doi          = {10.1016/j.jpdc.2022.05.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {173-186},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ESDU: An elastic stripe-based delta update method for erasure-coded cross-data center storage systems},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). POCache: Toward robust and configurable straggler tolerance
with parity-only caching. <em>JPDC</em>, <em>167</em>, 157–172. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stragglers (i.e., nodes with slow performance) are prevalent and incur performance instability in large-scale storage systems, yet it is challenging to detect stragglers in practice. We make a case by showing how erasure-coded caching provides robust straggler tolerance without relying on timely and accurate straggler detection, while incurring limited redundancy overhead in caching. We first analytically motivate that caching only parity blocks can achieve effective straggler tolerance. To this end, we present POCache , a parity-only caching design that provides robust straggler tolerance. To limit the erasure coding overhead, POCache slices blocks into smaller subblocks and parallelizes the coding operations at the subblock level. It further adopts a configurable straggler-aware cache algorithm (CSAC) that takes into account both file access popularity and straggler estimation to decide which parity blocks should be cached. CSAC enables POCache to configure various cache admission and eviction algorithms with straggler awareness and supports cache prefetching . We implement a POCache prototype atop Hadoop 3.1 HDFS, while preserving the performance and functionalities of normal HDFS operations. Extensive experiments on both local and Amazon EC2 clusters show that in the presence of stragglers, POCache can reduce the read latency by up to 87.9\% compared to vanilla HDFS.},
  archive      = {J_JPDC},
  author       = {Mi Zhang and Qiuping Wang and Zhirong Shen and Patrick P.C. Lee},
  doi          = {10.1016/j.jpdc.2022.05.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {157-172},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {POCache: Toward robust and configurable straggler tolerance with parity-only caching},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault-tolerability of the hypercube and variants with faulty
subcubes. <em>JPDC</em>, <em>167</em>, 148–156. (<a
href="https://doi.org/10.1016/j.jpdc.2022.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A faulty vertex may probably affect its neighbors and further causes them being faulty, which makes a subnetwork (structure) fail. Therefore, looking into the effect caused by some structures becoming faulty is meaningful. The connectivity and diagnosability are two important parameters to evaluate the fault-tolerability of networks. The connectivity of a network is the minimum number of vertices whose removal will disconnect the network or trivial. We call the network to be t -diagnosable if the number of faulty vertices does not exceed t and all faulty vertices can be identified without a replacement. Let Q n Qn be the n -dimensional hypercube and E H ( s , t ) EH(s,t) be the exchanged hypercube , which is the variant of hypercube. In this paper, we study connectivity, diagnosability, and 1-good-neighbor conditional diagnosability based on structure faults, respectively. Specifically, we first determine the connectivity ( 1 ≤ k ≤ n − 1 1≤k≤n−1 and n ≥ 3 n≥3 ) and diagnosability ( 1 ≤ k ≤ n − 1 1≤k≤n−1 and n ≥ 4 n≥4 ) of Q n − Q k Qn−Qk under the PMC model. Then, we determine the connectivity ( 2 ≤ k ≤ min ⁡ { s , t } 2≤k≤min⁡{s,t} ) and diagnosability ( 2 ≤ k ≤ min ⁡ { s , t } 2≤k≤min⁡{s,t} and min ⁡ { s , t } ≥ 3 min⁡{s,t}≥3 ) of E H ( s , t ) − Q k EH(s,t)−Qk under the PMC model. Finally, we show that the 1-good-neighbor conditional diagnosability of Q n − Q k Qn−Qk is 2 n − 3 2n−3 for n ≥ 5 n≥5 and 1 ≤ k ≤ n − 1 1≤k≤n−1 under the PMC model, which is almost twice as the traditional diagnosability for a large n .},
  archive      = {J_JPDC},
  author       = {Yihong Wang and Jianxi Fan and Xueli Sun and Baolei Cheng and Yan Wang},
  doi          = {10.1016/j.jpdc.2022.05.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {148-156},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fault-tolerability of the hypercube and variants with faulty subcubes},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient hierarchical hash tree for OpenFlow packet
classification with fast updates on GPUs. <em>JPDC</em>, <em>167</em>,
136–147. (<a href="https://doi.org/10.1016/j.jpdc.2022.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packet classification is an important functionality of modern routers/switches, needed in packet forwarding, Quality of Service (QoS), firewall etc. In order to better utilize routers on the Internet, Software Defined Network (SDN) decouples control plane from data plane to fulfill centralized management. Based on OpenFlow standards, packet classification in SDN is designed for multi-field rules which are more complex than traditional 5-tuple rules. In the paper, we propose a novel packet classification algorithm, called hierarchical hash tree (H-HashTree), based on the two IP address fields and the 7 exact-match fields to partition rules into groups. An extended Bloom filter is also proposed to accelerate search process by skipping groups in the hash tree. To further improve the performance, H-HashTree is implemented on GPU . We tested on 100K rules including synthesized rules containing characteristics of ACL, FW, and IPC with different wildcard ratios in exact-match fields, and real OpenFlow rules from Open vSwitch. Compared with the existing state-of-the-art algorithms, CutTSS and TabTree [19] [18] , H-HashTree achieves the best performance on both search and update speeds. H-HashTree achieves 1.17-13.9 and 2.48-12.7 times faster in search speed and 2.03-6.0 and 1.87-4.53 times faster in rule updates from synthesized rulesets than CutTSS and TabTree, respectively. On the GPU platform, H-HashTree can achieve up to 114 MPPS in search speed and less than 0.04 usec/rule in rule updates.},
  archive      = {J_JPDC},
  author       = {Yu-Hsiang Lin and Wen-Chi Shih and Yeim-Kuan Chang},
  doi          = {10.1016/j.jpdc.2022.04.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {136-147},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient hierarchical hash tree for OpenFlow packet classification with fast updates on GPUs},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a component-based acceleration of convolutional
neural networks on FPGAs. <em>JPDC</em>, <em>167</em>, 123–135. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Convolution Neural Networks (CNN) have been extensively adopted in broad Artificial Intelligence (AI) applications and have demonstrated ability and effectiveness in solving learning problems. However, developing high-performance hardware accelerators on Field Programmable Gate Array (FPGA) for CNNs often demands skills in hardware design and verification, accurate distribution localization, and long development cycles. Besides, the depth of CNN architectures increases by reusing and replicating several layers. In this work, we take advantage of the replication of CNN layers to achieve improvement in design performance and productivity. We propose a programming flow for CNNs on FPGA to generate high-performance accelerators by assembling CNN pre-implemented components as a puzzle based on the graph topology. Using pre-implemented components allows us to use minimum of resources, predict the performance, and gain in productivity since there is no need to synthesize any Hardware Description Language (HDL) source code. Furthermore, the pre-implemented components are reused for different range of applications, reducing the engineering time. Through prototyping, we demonstrate the viability and relevance of our approach. Experiments show a productivity improvement of up to 69\% compared to a traditional FPGA implementation while achieving over 1.75× higher Fmax with lower resources and higher energy efficiency.},
  archive      = {J_JPDC},
  author       = {Danielle Tchuinkou Kwadjo and Erman Nghonda Tchinda and Joel Mandebi Mbongue and Christophe Bobda},
  doi          = {10.1016/j.jpdc.2022.04.025},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {123-135},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards a component-based acceleration of convolutional neural networks on FPGAs},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPU-accelerated scalable solver with bit permutated
cyclic-min algorithm for quadratic unconstrained binary optimization.
<em>JPDC</em>, <em>167</em>, 109–122. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide range of combinatorial optimization problems can be reduced to the Ising model, and equivalently the quadratic unconstrained binary optimization (QUBO) problem. Thus, in recent years, researchers have proposed to solve QUBO on FPGAs, GPUs, and special-purpose processors. The adaptive bulk search (ABS) is a previously-proposed framework for solving QUBO in parallel on multiple GPUs. In the ABS, a CPU host performs a GA-based global search while GPUs asynchronously perform many local searches in parallel. An original ABS adopts a simple local search algorithm called a cyclic-min algorithm, which does not use pseudo random numbers. However, the lack of randomness may cause a potential drawback of restricted bit-flipping operations in a local search. To avoid this drawback, this paper proposes a cyclic-min algorithm with randomly-generated multiple bit permutations, which enables a more effective local search with random number generation in CPUs (not in GPUs). Furthermore, this paper introduces a scalable implementation of the ABS with MPI and OpenMP. Our experimental results on TSUBAME3.0 show that the solution quality improves and the throughput linearly increases as the number of GPUs increases; with 256 GPUs, it evaluates 20.1 × 10 12 20.1×1012 solutions per second.},
  archive      = {J_JPDC},
  author       = {Ryota Yasudo and Koji Nakano and Yasuaki Ito and Ryota Katsuki and Yusuke Tabata and Takashi Yazane and Kenichiro Hamano},
  doi          = {10.1016/j.jpdc.2022.04.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {109-122},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GPU-accelerated scalable solver with bit permutated cyclic-min algorithm for quadratic unconstrained binary optimization},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goal-driven scheduling model in edge computing for smart
city applications. <em>JPDC</em>, <em>167</em>, 97–108. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A formidable challenge in scheduling user applications lies in collecting and representing the user&#39;s goals and requirements. We introduce a “science goal” as a mechanism for users to define scientific objectives and conditions of interest. To provide an abstraction to run applications on an ensemble of edge computing nodes, we implement a two-layered scheduler—cloud and edge scheduler. In this scheduling model, the users submit their goals to the cloud scheduler. These goals are conveyed to the appropriate nodes based on a variety of constraints including geographical area, resource availability, node capabilities, and applicability. The edge scheduler, with complete understanding of the current conditions, assumes the responsibility for executing the applications on the nodes so that the users&#39; science goals are met. This paper provides a framework for the two-layered scheduling model for goal-driven edge computing and motivates and informs its architecture through a case study.},
  archive      = {J_JPDC},
  author       = {Yongho Kim and Seongha Park and Sean Shahkarami and Rajesh Sankaran and Nicola Ferrier and Pete Beckman},
  doi          = {10.1016/j.jpdc.2022.04.024},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {97-108},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Goal-driven scheduling model in edge computing for smart city applications},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Targeting a light-weight and multi-channel approach for
distributed stream processing. <em>JPDC</em>, <em>167</em>, 77–96. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing high-throughput data-streams has become a major challenge in areas such as real-time event monitoring, complex dataflow processing, and big data analytics. While there has been tremendous progress in distributed stream processing systems in the past few years, the high-throughput and low-latency (a.k.a. high sustainable-throughput ) requirement of modern applications is pushing the limits of traditional data processing infrastructures. This paper introduces a new distributed stream processing engine (DSPE), called Asynchronous Iterative Routing (or simply “AIR”), which implements a light-weight, dynamic sharding protocol. AIR expedites direct and asynchronous communication among all the worker nodes via a channel-like communication protocol on top of the Message Passing Interface (MPI), thereby completely avoiding the need for a dedicated driver node. The system adopts a new progress-tracking protocol, called hew-meld , which has been experimentally observed to show a low processing latency on our asynchronous master-less architecture when compared to the conventional low-watermark technique. The current version of AIR is also equipped with two fault tolerance and recovery strategies namely checkpointing &amp; rollback and replication . With its unique design, AIR scales out particularly well to multi-core HPC architectures; specifically, we deployed it on clusters with up to 16 nodes and 448 cores (thus reaching a peak of 435.3 million events and 55.14 GB of data processed per second), which we found to significantly outperform existing DSPEs.},
  archive      = {J_JPDC},
  author       = {Vinu Ellampallil Venugopal and Martin Theobald and Damien Tassetti and Samira Chaychi and Amal Tawakuli},
  doi          = {10.1016/j.jpdc.2022.04.022},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {77-96},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Targeting a light-weight and multi-channel approach for distributed stream processing},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cohort-based federated learning services for industrial
collaboration on the edge. <em>JPDC</em>, <em>167</em>, 64–76. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) is increasingly applied in industrial manufacturing, but often performance is limited due to insufficient training data. While ML models can benefit from collaboration, due to privacy concerns, individual manufacturers often cannot share data directly. Federated Learning (FL) enables collaborative training of ML models without revealing raw data. However, current FL approaches fail to take the characteristics and requirements of industrial clients into account. In this work, we propose an FL system consisting of a process description and a software architecture to provide FL as a Service (FLaaS) to industrial clients deployed to edge devices. Our approach deals with skewed data by organizing clients into cohorts with similar data distributions. We evaluated the system on two industrial datasets. We show how the FLaaS approach provides FL to client processes by considering their requests submitted to the Industrial Federated Learning (IFL) Services API. Experiments on both industrial datasets and different FL algorithms show that the proposed cohort building can increase the ML model performance notably.},
  archive      = {J_JPDC},
  author       = {Thomas Hiessl and Safoura Rezapour Lakani and Jana Kemnitz and Daniel Schall and Stefan Schulte},
  doi          = {10.1016/j.jpdc.2022.04.021},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-76},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cohort-based federated learning services for industrial collaboration on the edge},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPGA fabric conscious architecture design and automation of
speed-area efficient margolus neighborhood based cellular automata with
variegated scan path insertion. <em>JPDC</em>, <em>167</em>, 50–63. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimized Field Programmable Gate Array (FPGA) implementation of Cellular Automata (CA) for high speed design requires knowledge of the platform specific logic cell architecture. In this paper, we have proposed architectures and design automation of a particular class of CA, essentially a Finite State Machine (FSM), which obey rules governed by principles of Margolus neighborhood. Under this proposition, the inputs to the next state function of the FSM for every CA cell alternates between two sets of data in every successive clock cycle. Careful choice of logic elements and their compact placement was ensured for speed-area efficient implementation. Variants of scan insertion were carried out for fault localization by properly utilizing the logic cells realizing the original Margolus CA, so that area-delay overhead is minimized. We outperform behavioral or register transfer level (RTL) based descriptions for CA implementations, expressed through conventional higher levels of abstraction, with respect to delay and occupancy count of logic slices.},
  archive      = {J_JPDC},
  author       = {Ayan Palchaudhuri and Digvijay Anand and Anindya Sundar Dhar},
  doi          = {10.1016/j.jpdc.2022.04.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {50-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FPGA fabric conscious architecture design and automation of speed-area efficient margolus neighborhood based cellular automata with variegated scan path insertion},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using skip graphs for increased NUMA locality.
<em>JPDC</em>, <em>167</em>, 31–49. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a NUMA-aware concurrent data structure design based on a data-partitioned, concurrent skip graph indexed by thread-local sequential maps. Our design brings significant quantitative and qualitative improvements on NUMA locality, as well as reduced contention for synchronized memory accesses. Maps show up to 6x higher compare-and-swap (CAS) locality, up to a 68.6\% reduction on the number of remote CAS operations, and an increase from 88.3\% to 99\% on the CAS success rate compared to a control implementation. Remote memory accesses are not only reduced in number, but the larger the NUMA distance between threads, the larger the reduction is. Relaxed priority queues implemented using our technique show similar scalability improvements, with provable reduction in contention and decrease in relaxation in one of our proposed implementations.},
  archive      = {J_JPDC},
  author       = {Samuel Thomas and Roxana Hayne and Jonad Pulaj and Hammurabi Mendes},
  doi          = {10.1016/j.jpdc.2022.04.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {31-49},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Using skip graphs for increased NUMA locality},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing response latency of composite
functions-as-a-service through scheduling. <em>JPDC</em>, <em>167</em>,
18–30. (<a href="https://doi.org/10.1016/j.jpdc.2022.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Function-as-a-Service (FaaS) clouds, customers deploy to cloud individual functions, in contrast to complete virtual machines (IaaS) or Linux containers (PaaS). FaaS offerings are available in the largest public clouds (Amazon Lambda, Google Cloud Functions, Azure Serverless); there are also popular open-source implementations (Apache OpenWhisk) with commercial offerings (Adobe I/O Runtime, IBM Cloud Functions). A recent addition to FaaS is the ability to compose functions: a function may call another functions, which, in turn, may call yet another function — forming a directed acyclic graph (DAG) of invocations. From the perspective of the infrastructure, a composed function is less opaque than a virtual machine or a container. We show that this additional information about the internal structure of the function enables the infrastructure provider to reduce the response latency. In particular, knowing the successors of a function in a DAG, the infrastructure can schedule these future invocations along with necessary preparation of environments. We model resource management in FaaS as a scheduling problem combining (1) sequencing of invocations; (2) deploying execution environments on machines; and (3) allocating invocations to deployed environments. For each aspect, we propose heuristics that employ FaaS-specific features. We explore their performance by simulation on a range of synthetic workloads and on workloads inspired by trace from existing system. Our results show that if the setup times are long compared to invocation times, algorithms that use information about the composition of functions consistently outperform greedy, myopic algorithms, leading to significant decrease in response latency.},
  archive      = {J_JPDC},
  author       = {Pawel Zuk and Krzysztof Rzadca},
  doi          = {10.1016/j.jpdc.2022.04.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {18-30},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reducing response latency of composite functions-as-a-service through scheduling},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIDP: An MDP-based intelligent big data processing scheme
for vehicular edge computing. <em>JPDC</em>, <em>167</em>, 1–17. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of Vehicle Equipment (VE) connected to the Internet is increasing, and these VEs generate tasks that contain large amounts of data. Processing these tasks requires a lot of computing resources. Therefore, it is a promising issue that offloading compute-intensive tasks from resource-limited vehicles to Vehicular Edge Computing (VEC) servers, which involves big data transmission, processing and computation. In a network, multiple providers provide VEC servers. When a vehicle generates a task, our goal is to make an intelligent decision on whether and when to offload this task to VEC servers to minimize the task completion time and total big data processing time. When each vehicle passes VEC servers, the vehicle can decide to offload its task to the VEC server in the current communication range, or continue to drive until it reaches the next server&#39;s communication range. This issue can be considered as an asset selling problem. It is a challenging issue to make a smart decision for the vehicle with a location view because the vehicle is not sure when the next VEC server will be available and how much about the available computing capacity of the next VEC server. Firstly, this paper formulates the problem as a Markov Decision Process (MDP), defines and analyzes the state set, action set, reward model , and state transition probability distribution. Then it uses Asynchronous Advantage Actor-Critic (A3C) algorithm to solve this MDP problem, builds the various elements of the A3C algorithm, uses Actor (the strategy function) to generate two actions of the vehicle: offloading and moving without offloading. Thirdly, it uses Critic (the value function) to evaluate Actor&#39;s behavior, and guide Actor&#39;s actions in subsequent stages. The Actor starts from the initial state in the state space until it enters the termination state, forming a complete decision-making process. It minimizes the completion time of task offloading through learning thereby reducing the delay of big data processing . Compared to the Immediately Offload (IO) scheme and Expect Offload (EO) scheme, the MIDP scheme proposed in this paper reduces the average task offloading delay to 29.93\% and 29.99\%, close to the EO scheme in terms of task completion rate and up to 66.6\% improvement compared to the IO scheme.},
  archive      = {J_JPDC},
  author       = {Shun Liu and Qiang Yang and Shaobo Zhang and Tian Wang and Neal N. Xiong},
  doi          = {10.1016/j.jpdc.2022.04.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-17},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MIDP: An MDP-based intelligent big data processing scheme for vehicular edge computing},
  volume       = {167},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application-aware adaptive parameter control for LoRaWAN.
<em>JPDC</em>, <em>166</em>, 166–177. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced wireless communication technologies are leading towards large-scale, geographically distributed systems, which consist of thousands of co-existing devices with potentially conflicting application requirements, such as high data delivery ratio and low power consumption. At the same time, devices are challenged by varying environmental conditions, which can also lead to degradation in network performance. To ensure the overall network performance and service stability offered by individual devices, the devices must co-exist and adapt to changes in their surrounding environment. In this paper, an application-aware adaptive method is proposed to allow individual devices to dynamically and automatically decide and configure its communication parameters according to its application requirements and environmental conditions. The algorithm aims to enhance and maintain the network performance over time, while allowing every device to satisfy its application requirements. The algorithm is realised on the modern LoRaWAN network protocol. Simulated experiments confirm the proposed algorithm&#39;s ability to adapt to changing application requirements at runtime while maintaining network performance. They demonstrate an improvement in packet delivery ratio, energy consumption, and the novel ability to respond to an application&#39;s individual performance requirements.},
  archive      = {J_JPDC},
  author       = {Ameer Ivoghlian and Kevin I-Kai Wang and Zoran Salcic},
  doi          = {10.1016/j.jpdc.2022.04.023},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {166-177},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Application-aware adaptive parameter control for LoRaWAN},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Error-sensitive proof-labeling schemes. <em>JPDC</em>,
<em>166</em>, 149–165. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proof-labeling schemes are known mechanisms providing nodes of networks with certificates that can be verified locally by distributed algorithms. Given a boolean predicate on network states, such schemes enable to check whether the predicate is satisfied by the actual state of the network, by having nodes interacting with their neighbors only. Proof-labeling schemes are typically designed for enforcing fault-tolerance, by making sure that if the current state of the network is illegal with respect to some given predicate, then at least one node will detect it. Such a node can raise an alarm, or launch a recovery procedure enabling the system to return to a legal state. In this paper, we introduce error-sensitive proof-labeling schemes. These are proof-labeling schemes which guarantee that the number of nodes detecting illegal states is linearly proportional to the Hamming distance between the current state and the set of legal states. By using error-sensitive proof-labeling schemes, states which are far from satisfying the predicate will be detected by many nodes. We provide a structural characterization of the set of boolean predicates on network states for which there exist error-sensitive proof-labeling schemes. This characterization allows us to show that classical predicates such as, e.g., cycle-freeness, and leader admit error-sensitive proof-labeling schemes, while others like regular subgraphs do not. We also focus on compact error-sensitive proof-labeling schemes. In particular, we show that the known proof-labeling schemes for spanning tree and minimum spanning tree , using certificates on O ( log ⁡ n ) O(log⁡n) bits, and on O ( log 2 ⁡ n ) O(log2⁡n) bits, respectively, are error-sensitive, as long as the trees are locally represented by adjacency lists , and not just by parent pointers.},
  archive      = {J_JPDC},
  author       = {Laurent Feuilloley and Pierre Fraigniaud},
  doi          = {10.1016/j.jpdc.2022.04.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {149-165},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Error-sensitive proof-labeling schemes},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial: Smart mobility management and 5G/beyond 5G
(B5G) wireless access. <em>JPDC</em>, <em>166</em>, 147–148. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Rodolfo W.L. Coutinho and Frank Y. Li},
  doi          = {10.1016/j.jpdc.2022.04.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {147-148},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Guest editorial: Smart mobility management and 5G/beyond 5G (B5G) wireless access},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The g-extra connectivity of folded crossed cubes.
<em>JPDC</em>, <em>166</em>, 139–146. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are various ways to measure the reliability and fault tolerance of diverse networks. The g -extra connectivity κ g ( G ) κg(G) of a connected graph G is the minimal cardinality of vertex set F , if any, whose deletion disconnects G and every remaining component of G − F G−F has at least g + 1 g+1 vertices. Folded crossed cubes F C Q n FCQn , a kind of interconnection network , has more reliable properties. In this paper, we explore the g -extra connectivity of n -dimensional folded crossed cubes. It is shown that when 0 ≤ g ≤ ⌊ n 2 ⌋ 0≤g≤⌊n2⌋ , κ g ( F C Q n ) = ( g + 1 ) n − g − ( g 2 ) + 1 κg(FCQn)=(g+1)n−g−(g2)+1 for n ≥ 8 n≥8 . As a byproduct, we get g -extra conditional fault-diagnosability t g p ˜ ( F C Q n ) = ( g + 1 ) n − ( g 2 ) + 1 tgp˜(FCQn)=(g+1)n−(g2)+1 of F C Q n FCQn under PMC model.},
  archive      = {J_JPDC},
  author       = {Huimei Guo and Eminjan Sabir and Aygul Mamut},
  doi          = {10.1016/j.jpdc.2022.04.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {139-146},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {The g-extra connectivity of folded crossed cubes},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zoro: A robotic middleware combining high performance and
high reliability. <em>JPDC</em>, <em>166</em>, 126–138. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant advances of AI technology, robotic systems have achieved remarkable development and profound effects. The robotic middleware plays a critical role in robotic systems to provide message definition, data transmission, and service discovery functions. As the modern robotic systems are usually operated in safety critical environments, such as various autonomous driving scenarios, it requires the design of robotic middleware to combine both high performance and high reliability in order to achieve system reliability and product safety. However, conventional robotic middleware used in the majority of robotic systems is based on an inefficient socket communication mechanism and relies on a hasty service discovery design, which leads to system instability and high resource usage. In this work, we propose a sophisticated robotic middleware, Zoro, to fulfill both high performance and high reliability. For communication, we employ shared memory for performance improvement and propose a socket-based communication control algorithm to improve reliability during data transmission. Also, a hierarchical memory protection mechanism is proposed to address safety problems caused by shared memory. Furthermore, we design a light-weight service discovery, to achieve high performance and a weak centralized mechanism for high reliability. Experiments show the communication latency of Zoro significantly outperforms state-of-the-art robotic middleware such as ROS2 and CyberRT by up to 41\%. In terms of service discovery, Zoro reduces CPU usage by up to 44\% compared to ROS. Zoro achieves reliability with respect to communication and service discovery.},
  archive      = {J_JPDC},
  author       = {Wei Liu and Jiangming Jin and Hao Wu and Yifan Gong and Ziyue Jiang and Jidong Zhai},
  doi          = {10.1016/j.jpdc.2022.04.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {126-138},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Zoro: A robotic middleware combining high performance and high reliability},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation-based optimization and sensibility analysis of
MPI applications: Variability matters. <em>JPDC</em>, <em>166</em>,
111–125. (<a href="https://doi.org/10.1016/j.jpdc.2022.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finely tuning MPI applications and understanding the influence of key parameters (number of processes, granularity , collective operation algorithms, virtual topology , and process placement) is critical to obtain good performance on supercomputers . With the high consumption of running applications at scale, doing so solely to optimize their performance is particularly costly. Having inexpensive but faithful predictions of expected performance could be a great help for researchers and system administrators. The methodology we propose decouples the complexity of the platform, which is captured through statistical models of the performance of its main components (MPI communications, BLAS operations), from the complexity of adaptive applications by emulating the application and skipping regular non-MPI parts of the code. We demonstrate the capability of our method with High-Performance Linpack (HPL), the benchmark used to rank supercomputers in the TOP500, which requires careful tuning. We briefly present (1) how the open-source version of HPL can be slightly modified to allow a fast emulation on a single commodity server at the scale of a supercomputer. Then we present (2) an extensive (in)validation study that compares simulation with real experiments and demonstrates our ability to predict the performance of HPL within a few percent consistently. This study allows us to identify the main modeling pitfalls (e.g., spatial and temporal node variability or network heterogeneity and irregular behavior) that need to be considered. Last, we show (3) how our “surrogate” allows studying several subtle HPL parameter optimization problems while accounting for uncertainty on the platform.},
  archive      = {J_JPDC},
  author       = {Tom Cornebize and Arnaud Legrand},
  doi          = {10.1016/j.jpdc.2022.04.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {111-125},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Simulation-based optimization and sensibility analysis of MPI applications: Variability matters},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ULSED: An ultra-lightweight SED model for IoT devices.
<em>JPDC</em>, <em>166</em>, 104–110. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound event detection (SED) technology has been widely used in applications such as audio surveillance systems and smart home. Compared to the traditional machine learning methods, the neural networks (NN) based methods have been proposed in recent years to significantly improve the detection accuracy. However, a major issue of the NN-based SED models is that they often involve a large number of parameters and floating point operations (FLOPs), resulting in significant processing time, power consumption and memory storage. This poses a challenge to SED on IoT devices with constrained computational resources and power budget. To address this issue, in this work, an ultra-lightweight SED model (ULSED) with a selective separable convolution scheme and a coordinate attention scheme is proposed to significantly reduce the computational complexity while achieving high detection accuracy. The proposed ULSED model is evaluated on the ESC-10, ESC-50 and UrbanSound8K(US8K) datasets. Compared with several state-of-the-art models, the number of parameters and the number of FLOPs is significantly reduced by up to 388 times and 1140 times while achieving high detection accuracy of 97.0\%, 88.3\% and 83.5\% on the ESC-10, ESC-50 and US8K respectively. The proposed ULSED model is suitable for power- and hardware-constrained IoT devices.},
  archive      = {J_JPDC},
  author       = {Lujie Peng and Junyu Yang and Jianbiao Xiao and Mingxue Yang and Yujiang Wang and Haojie Qin and Xiaorong Li and Jun Zhou},
  doi          = {10.1016/j.jpdc.2022.04.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104-110},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ULSED: An ultra-lightweight SED model for IoT devices},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APT: The master-copy-free training method for quantised
neural network on edge devices. <em>JPDC</em>, <em>166</em>, 95–103. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantisation is a commonly applied technique to improve the efficiency of a neural network on edge devices. Many applications also require that a machine learning model extend its learning cycle into the field. Training a quantised neural network on edge devices is a non-trivial task since the resource available on the edge is limited. Most of the quantisation-aware training methods maintain a quantised model, as well as an extra full-precision model, which is used to prevent large accuracy losses. The keep of the full precision model is based on the assumption that there are sufficient memory and energy supply to the machine for training. The assumption is contradictory to the situation of edge AI . In this paper, we propose the Adaptive Precision Training method (APT), which only keeps a quantised model. The challenge is that a quantised model has difficulty in learning, due to the quantisation underflow issue. APT employs a metric called G a v g Gavg to quantify the learning ability of each layer and dynamically adjusts per-layer bitwidth to ensure the model can learn effectively. Experiments on image classification and text classification tasks suggest that APT trains quantised models effectively with limited accuracy loss. Compared with the 8-bit traditional QAT method, APT saves 60-72.5\% memory space for model parameters. We investigate the bitwidth necessary for effective training and gain preliminary insights into the relationship between architecture and its learning ability.},
  archive      = {J_JPDC},
  author       = {Tian Huang and Tao Luo and Joey Tianyi Zhou},
  doi          = {10.1016/j.jpdc.2022.04.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {95-103},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {APT: The master-copy-free training method for quantised neural network on edge devices},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed intelligence on the edge-to-cloud continuum: A
systematic literature review. <em>JPDC</em>, <em>166</em>, 71–94. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum , the Digital Continuum , or the Transcontinuum . Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds . This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum.},
  archive      = {J_JPDC},
  author       = {Daniel Rosendo and Alexandru Costan and Patrick Valduriez and Gabriel Antoniu},
  doi          = {10.1016/j.jpdc.2022.04.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {71-94},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed intelligence on the edge-to-cloud continuum: A systematic literature review},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data stream clustering for low-cost machines. <em>JPDC</em>,
<em>166</em>, 57–70. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the operations performed by the Internet of Things (IoT) systems are no more trivial since they rely on more sophisticated devices than in the past. The IoT system is physically composed of connected computing, digital, mechanical devices such as sensors or actuators. Most of the time, each of them incorporates a logical arithmetic unit that can pre-compute or compute on the device. To extract value from the data produced at the edge, processing power offered by cloud computing is still utilized. However, streaming data to the cloud exposes some limitations related to the increased communication and data transfer, which introduces delays and consumes network bandwidth . Clustering data is one example of a treatment that can be executed in the cloud. In this paper, we propose a methodology for solving the data stream clustering problem at the edge. Data Stream clustering is defined as the clustering of data that arrive continuously, such as telephone records, multimedia data, sensors data, financial transactions, etc. Since we use low-cost and low-capacity devices, the objective is, given a sequence of points, to construct a good clustering of the stream using a small amount of memory and time. We propose a ‘windowing’ scheme, coupled with a sampling scheme to respect the objective. Under the experimental conditions, experiments show that the clustering solutions can be controlled, with difficulties for time-stamped data but not for random data or data with well-delimited clusters. The main advantage of our schema is that we are clustering data “on the fly” with no knowledge or assumption regarding the available data. We do not assume that all the data are known before a treatment batch by batch. Our schema also has the potential to be adapted to other classes of machine learning algorithms .},
  archive      = {J_JPDC},
  author       = {Christophe Cérin and Keiji Kimura and Mamadou Sow},
  doi          = {10.1016/j.jpdc.2022.04.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {57-70},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Data stream clustering for low-cost machines},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinate-based efficient indexing mechanism for
intelligent IoT systems in heterogeneous edge computing. <em>JPDC</em>,
<em>166</em>, 45–56. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Powered by edge servers (also called as edge nodes) which are close to the data source, distributed edge AI processes the huge amounts of data generated by Internet of Things (IoT) devices, extracting value for users. In edge computing , massive data are stored in several distributed edge nodes with heterogeneous capabilities. Intelligent applications running on one edge node may need data from other edge nodes. An efficient data indexing mechanism can rapidly locate the edge node where the data is kept, supporting latency-sensitive intelligent applications. The existing indexing methods in edge computing assume that all edge nodes are the same in capability and the number of edge nodes is constant. This paper proposes CREIM, a coordinate-based efficient indexing mechanism for intelligent IoT systems in heterogeneous edge computing. CREIM achieves fair load balancing on edge nodes with heterogeneous capabilities. The indexing mechanism deals well with the horizontal scaling of edge nodes. Besides, CREIM addresses a fast lookup with one overlay hop, providing low latency data retrieval for edge intelligent applications. In the experiments, CREIM is applied in a realistic network simulated by the mininet and the routing forwarding is supported by the P4 switch. The experiments are constructed by combining real location datasets of Shanghai Telecoms base stations with the real-collected requests of end-devices. The experimental results demonstrate that CREIM achieves a near-optimal latency of index-lookup, adapts the heterogeneous capabilities among edge nodes and reduces the cost of increasing/decreasing edge nodes by 56.36\% compared with the state-of-the-art method.},
  archive      = {J_JPDC},
  author       = {Songtao Tang and Xin Du and Zhihui Lu and Keke Gai and Jie Wu and Patrick C.K. Hung and Kim-Kwang Raymond Choo},
  doi          = {10.1016/j.jpdc.2022.04.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {45-56},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Coordinate-based efficient indexing mechanism for intelligent IoT systems in heterogeneous edge computing},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ProvNet: Networked bi-directional blockchain for data
sharing with verifiable provenance. <em>JPDC</em>, <em>166</em>, 32–44.
(<a href="https://doi.org/10.1016/j.jpdc.2022.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing is increasingly popular especially for scientific research and business fields where large volume of datasets need to be used, but it involves data security and privacy concerns. This paper mitigates such concerns by tracking and logging the history of shared data (i.e., provenance records) while preserving data privacy. This is a challenging problem in the data sharing scenario in this paper because the environment is decentralized and internal logs are not accessible publicly due to privacy concerns. We present ProvNet, a decentralized data sharing platform that can detect malicious users and provide secure provenance records using the newly proposed networked blockchain without disclosing raw data contents. Valid sharing records are collected and stored in the blocknet and misbehavior is detected with the stored provenance records according to our accountable protocols. We give a proof-of-concept implementation, and evaluation results show that the overhead is acceptable.},
  archive      = {J_JPDC},
  author       = {Changhao Chenli and Wenyi Tang and Frank Gomulka and Taeho Jung},
  doi          = {10.1016/j.jpdc.2022.04.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-44},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ProvNet: Networked bi-directional blockchain for data sharing with verifiable provenance},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-latency tradeoffs for edge caching and dynamic
service migration based on DQN in mobile edge computing. <em>JPDC</em>,
<em>166</em>, 15–31. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing sinks computing and storage capabilities to the edge of the network to provide reliable and low-latency services. However, the mobility of users and the limited coverage of edge servers can cause service interruptions and reduce service quality. A cooperative edge caching strategy based on energy-latency balance is proposed to solve high power consumption and latency caused by processing computationally intensive applications. In the cache selection phase, the request prediction method based on a deep neural network improves the cache hit rate. In the cache placement stage, the objective function is established by comprehensively considering power consumption and latency, and We use the branch-and-bound algorithm to get the optimal value. We propose an improved service migration method to solve the problem of service interruption caused by user movement. The service migration problem is modeled using a Markov decision process (MDP). The optimization goal is to reduce service latency and improve user experience under the premise of specified cost and computing resources. Finally, the optimal solution of the model is solved by the deep Q-Network (DQN) algorithm. Experiments show that our edge caching algorithm has lower latency and energy consumption than other algorithms in the same conditions. The service migration algorithm proposed in this paper is superior to different service migration algorithms in migration cost and success rate.},
  archive      = {J_JPDC},
  author       = {Chunlin Li and Yong Zhang and Xiang Gao and Youlong Luo},
  doi          = {10.1016/j.jpdc.2022.03.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {15-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-latency tradeoffs for edge caching and dynamic service migration based on DQN in mobile edge computing},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A forward and backward private oblivious RAM for storage
outsourcing on edge-cloud computing. <em>JPDC</em>, <em>166</em>, 1–14.
(<a href="https://doi.org/10.1016/j.jpdc.2022.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, edge-cloud computing is regarded as a promising solution to meet the requirements of mobile computing and Internet-of-Things (IoT). However, due to the limited storage resources of edge equipment, there is a security threat when users outsource their sensitive data to the cloud computing center. The users usually adopt a data-encryption approach, Oblivious RAM (ORAM), which enables a user to read/write her outsourced private data without access pattern leakage. Not all users like the fully functional ORAM all the time since the ORAM protocol is usually highly interactive or occupies large edge storage space. We show that forward-private/backward-private (FP/BP) ORAMs are good alternatives for secure storage outsourcing. We introduce the FP/BP-ORAM definitions and present LL-ORAM, the first FP/BP-ORAM that achieves near-zero edge storage, single-round-trip read/write, and worst-case sublinear access time. For any outsourced record, LL-ORAM provides both an oblivious-access interface and a nonoblivious-access interface. FP-ORAM concerns more data-write privacy than data-read privacy. BP-ORAM concerns more data-read privacy. The constructions involve a tree data structure named LL-tree, which supports fast computation in the cloud with an access-pattern-reduced leakage profile. The security analysis shows that LL-ORAM meets the proposed forward and backward security model. The experimental results demonstrate that LL-ORAM is round-efficient and can be deployed on edge-cloud computing systems.},
  archive      = {J_JPDC},
  author       = {Zhiqiang Wu and Zhubin Cai and Xiaoyong Tang and Yuming Xu and Tan Deng},
  doi          = {10.1016/j.jpdc.2022.04.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-14},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A forward and backward private oblivious RAM for storage outsourcing on edge-cloud computing},
  volume       = {166},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An energy and carbon-aware algorithm for renewable energy
usage maximization in distributed cloud data centers. <em>JPDC</em>,
<em>165</em>, 156–166. (<a
href="https://doi.org/10.1016/j.jpdc.2022.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vigorous development and the increasing popularity of cloud computing highlight the necessity of reducing data center energy consumption and the environmental impact of carbon dioxide emissions. For geographically distributed data centers, cloud servers are connected to the conventional power grid and in addition they are supported by an attached renewable energy source . Since the carbon footprint rate of energy consumption has dynamic differences in space, reducing energy consumption does not mean decrease carbon emission, which indicates that energy consumption and carbon footprint need to be synergistically optimized. In this paper, an energy and carbon-aware algorithm for virtual machine placement is proposed. The goal is to obtain a virtual machine allocation scheme that aims to achieve the trade-off between energy consumption and carbon emissions by improving renewable energy utilization. The experimental results show that the proposed approach is more energy-efficient and greener, which can also maximize the renewable energy utilization with 73.11\% while ensuring the SLA violation with 0.2\% in comparison to the baseline algorithms.},
  archive      = {J_JPDC},
  author       = {Daming Zhao and Jiantao Zhou},
  doi          = {10.1016/j.jpdc.2022.04.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {156-166},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An energy and carbon-aware algorithm for renewable energy usage maximization in distributed cloud data centers},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stochastic mobility model for traffic forecasting in urban
environments. <em>JPDC</em>, <em>165</em>, 142–155. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the steadily growing traffic demand, urban traffic congestion is becoming a critical issue threatening several factors, including public safety, emissions of greenhouse gas, and transport inefficiencies. Thus, intelligent transport systems (ITS) have emerged as a promising solution to easing the burden of congestion. ITS rely on different technologies such as VANET (Vehicular Adhoc Networks) which provide the transportation system with ubiquitous connectivity allowing the exchange of traffic information between vehicles and roadside terminals. This can support numerous smart mobility applications such as traffic signal control and real-time traffic management. Hence, mobility models were developed to emulate and forecast the distribution of traffic which will be helpful to the design and management of traffic control strategies. In this context, this study specifically concentrates on developing a mobility model that reflects vehicular activities in urban environments based on vehicular information collected using vehicular communications . The behavior of vehicles along multi-lane roads and intersections is modeled as a stochastic process using queuing theory . Particularly, the queue system is analyzed as a continuous-time Markov chain (CTMC) and by calculating the steady-state probabilities, different performance measures are derived and analyzed under various scenarios. To validate the model, the obtained forecasts are compared with a queue model and realistic traces. The results show that the model is capable of reproducing the realistic behavior of traffic in urban roads without incurring heavy costs and time-consuming computing. The obtained estimates were then used to design an actuated traffic light and a vehicle speed adaptor. From the simulation results, it is clear that using the proposed traffic forecasting model helps reduce vehicles idling and travel times.},
  archive      = {J_JPDC},
  author       = {El Joubari Oumaima and Ben Othman Jalel and Veque Veronique},
  doi          = {10.1016/j.jpdc.2022.03.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {142-155},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A stochastic mobility model for traffic forecasting in urban environments},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying dual accelerated gradient ascent: A fast
network optimization algorithm. <em>JPDC</em>, <em>165</em>, 130–141.
(<a href="https://doi.org/10.1016/j.jpdc.2022.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a time-varying dual accelerated gradient method for minimizing the average of n strongly convex and smooth functions over a time-varying network with n nodes. We prove that the time-varying dual accelerated gradient ascent method converges at an R-linear rate with the time to reach an ϵ -neighborhood of the solution being of O ( 1 ln ⁡ ( 1 / c ) ln ⁡ M ϵ ) O(1ln⁡(1/c)ln⁡Mϵ) , where c is a constant depending on the graph and objective function parameters and M is a constant depending on the initial values. We test the proposed method on two classes of problems: L 2 L2 -regularized least squares and logistic classification problems. For each class, we generate 1000 problems and use the Dolan-Moré performance profiles to compare our obtained results with the ones obtained by several state-of-the-art algorithms to illustrate the efficiency of our method.},
  archive      = {J_JPDC},
  author       = {Elham Monifi and Nezam Mahdavi-Amiri},
  doi          = {10.1016/j.jpdc.2022.03.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {130-141},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Time-varying dual accelerated gradient ascent: A fast network optimization algorithm},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of intel’s DPC++ compatibility tool in
heterogeneous computing. <em>JPDC</em>, <em>165</em>, 120–129. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel DPC++ Compatibility Tool is a component of the Intel oneAPI Base Toolkit. This tool automatically transforms CUDA code into Data Parallel C++ (DPC++), thus assisting in the migration process. DPC++ is an implementation of the programming standard for heterogeneous computing known as SYCL, which unifies the development of parallel applications on CPUs, GPUs or even FPGAs . This paper analyzes the DPC++ Compatibility Tool by considering the manual intervention required and the problems encountered while migrating the Rodinia benchmarks. For this suite, this tool achieves an impressive rate of almost 87\% for code successfully migrated. Moreover, a comparative study of the performance obtained by the migrated code was carried out, showing a moderate overhead in most of the migrated examples. Finally, a performance comparison on different devices was also performed.},
  archive      = {J_JPDC},
  author       = {Germán Castaño and Youssef Faqir-Rhazoui and Carlos García and Manuel Prieto-Matías},
  doi          = {10.1016/j.jpdc.2022.03.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {120-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evaluation of intel&#39;s DPC++ compatibility tool in heterogeneous computing},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method for reducing cloud service request peaks based on
game theory. <em>JPDC</em>, <em>165</em>, 107–119. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Internet of Things (IoT) technology and the popularity of Artificial Intelligence (Al) technology research have brought new opportunities for the development of cloud computing (CC). With the increasing number of mobile Internet access devices and IoT access devices, the number of task requests from CC customers for AI services in the network has also experienced an explosive growth. In this paper, the focus is on the possible overload of cloud providers during the peak period of cloud service requests. Thetime attributes of cloud task execution are classified to avoid overloading the cloud provider as much as possible. In a distributed cloud environment, it is necessary to consider the time flexible attributes of cloud tasks to reasonably compete for cloud resources. In this work, game theory (GT) is introduced to formulate a cloud service scheduling game, in which participants are cloud customers who participate in the purchase of cloud services. The players&#39; strategies are the time flexibility of each cloud task. The problem is formulated as minimizing the cost of scheduling cloud services and a noncooperative game among the customers (as players) is presented. Then the existence of the Nash equilibrium (NE) solution of the game has been proved and a new algorithm has been proposed in this paper to compute it. In addition, the analysis process of the convergence of the proposed PCA PCA algorithm and the proof of its convergence to NE are also included in this paper. At the end of the paper, simulations were performed to verify the theoretical analysis presented. The experimental results show that the proposed PCA PCA algorithm can converge to the Nash equilibrium very quickly, effectively reducing the peak value and increasing profit.},
  archive      = {J_JPDC},
  author       = {Zheng Xiao and Mengyuan Wang and Anthony Theodore Chronopoulos and Jiuchuan Jiang},
  doi          = {10.1016/j.jpdc.2022.03.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {107-119},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A method for reducing cloud service request peaks based on game theory},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An SMDP-based approach to thermal-aware task scheduling in
NoC-based MPSoC platforms. <em>JPDC</em>, <em>165</em>, 79–106. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the operation of a thermal-aware task scheduler, dispatching tasks from an arrival queue as well as setting the voltage and frequency of the processing cores to optimize the mean temperature margin of the entire chip (i.e., cores as well as the NoC routers). We model the decision process of the task scheduler as a semi-Markov decision problem (SMDP) to account for the most common uncertainties prevalent in MPSoC systems (including: the stochastic nature of the workload inter-arrival times, time-varying workload characteristics , the uncertain chip thermal profile as well as random inter-task communications). SMDP is among the fairly general variants of continuous-time optimization frameworks from the stochastic control theory and is a much more efficient choice compared to discrete-time formalisms (which would lead to an increase in task waiting times and degraded system performance). To solve the formulated SMDP, we propose two reinforcement learning (RL) algorithms that are capable of computing the optimal task assignment policy without requiring the statistical knowledge of the stochastic dynamics underlying the system states. The proposed algorithms also rely on function approximation techniques to handle the infinite length of the task queue as well as the continuous nature of temperature readings. Compared to previous work, the simulations demonstrate nearly 6 Kelvin reduction in system average peak temperature and 66 milliseconds decrease in mean task service time.},
  archive      = {J_JPDC},
  author       = {Farnaz Niknia and Vesal Hakami and Kiamehr Rezaee},
  doi          = {10.1016/j.jpdc.2022.03.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {79-106},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An SMDP-based approach to thermal-aware task scheduling in NoC-based MPSoC platforms},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An edge intelligence empowered flooding process prediction
using internet of things in smart city. <em>JPDC</em>, <em>165</em>,
66–78. (<a href="https://doi.org/10.1016/j.jpdc.2022.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floods result in substantial damage throughout the world every year. Accurate predictions of floods can significantly alleviate casualties and property losses. However, due to the complexity of hydrology process especially in a city with complicated pipe network, the accuracy of traditional flood forecasting models suffer from the performance degradation with the increasing of required prediction period. In the work, based on the collected historical data of Xixian City, Henan Province, China, using the Internet of Things system (IoT) in 2011-2018, a Bidirectional Gated Recurrent Unit (BiGRU) multi-step flood prediction model with attention mechanism is proposed. In our model, the attention mechanism is used to automatically adjust the matching degree between the input features and output. Besides, we use a bidirectional GRU model, which can process the input sequence from two directions of time series (chronologically and antichronologically), then merge their representations together. Compared with the prediction model using Long Short Term Memory (LSTM), our method can generate better prediction result, as can be seen from the arrival time error and peak error of floods during multi-step predictions.},
  archive      = {J_JPDC},
  author       = {Chen Chen and Jiange Jiang and Yang Zhou and Ning Lv and Xiaoxu Liang and Shaohua Wan},
  doi          = {10.1016/j.jpdc.2022.03.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {66-78},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An edge intelligence empowered flooding process prediction using internet of things in smart city},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Path planning mechanism for mobile anchor-assisted
localization in wireless sensor networks. <em>JPDC</em>, <em>165</em>,
52–65. (<a href="https://doi.org/10.1016/j.jpdc.2022.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of a deployed sensor node location plays a vital role in all domains of Wireless Sensor Networks since its location provides the source of the information. Many approaches have been proposed in recent years. One of these approaches uses a single location-aware node, known as the Anchor or Beacon, that exchanges information with other nodes to help in their position estimation. Although this approach provides good accuracy, it creates new challenges in the picture. The key issue is to design an optimal trajectory for the anchor node in order to achieve maximum localization accuracy in the shortest amount of time. In this paper, we propose a static trajectory for a mobile beacon node for localization in WSNs. This trajectory ensures that the localization success ratio can be improved for lower communication range of an anchor node with higher localization precision and minimum localization time as compared to other static models. The proposed trajectory also overcomes the collinearity problem and ensures that all the unknown sensor nodes receive good quality beacon positions for position estimation.},
  archive      = {J_JPDC},
  author       = {Ketan Sabale and S. Mini},
  doi          = {10.1016/j.jpdc.2022.03.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-65},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Path planning mechanism for mobile anchor-assisted localization in wireless sensor networks},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoupling GPGPU voltage-frequency scaling for deep-learning
applications. <em>JPDC</em>, <em>165</em>, 32–51. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of GPUs to accelerate DNN training and inference is already widely adopted, allowing for a significant performance increase. However, this performance is usually obtained at the cost of a consequent increase in energy consumption. While several solutions have been proposed to perform voltage-frequency scaling on GPUs, these are still one-dimensional, by simply adjusting the frequency while relying on default voltage settings. To overcome this limitation, this paper introduces a new methodology to fully characterize the impact of non-conventional DVFS on GPUs. The proposed approach was evaluated on two devices, an AMD Vega 10 Frontier Edition and an AMD Radeon 5700XT. When applying this non-conventional DVFS scheme to DNN training, the obtained results show that it is possible to safely decrease the GPU voltage, allowing for a significant reduction of the energy consumption (up to 38\%) and of the EDP (up to 41\%) on the training procedure of CNN models, with no degradation of the networks accuracy.},
  archive      = {J_JPDC},
  author       = {Francisco Mendes and Pedro Tomás and Nuno Roma},
  doi          = {10.1016/j.jpdc.2022.03.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Decoupling GPGPU voltage-frequency scaling for deep-learning applications},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FELIDS: Federated learning-based intrusion detection system
for agricultural internet of things. <em>JPDC</em>, <em>165</em>, 17–31.
(<a href="https://doi.org/10.1016/j.jpdc.2022.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a federated learning-based intrusion detection system, named FELIDS, for securing agricultural-IoT infrastructures. Specifically, the FELIDS system protects data privacy through local learning, where devices benefit from the knowledge of their peers by sharing only updates from their model with an aggregation server that produces an improved detection model. In order to prevent Agricultural IoTs attacks, the FELIDS system employs three deep learning classifiers, namely, deep neural networks, convolutional neural networks, and recurrent neural networks. We study the performance of the proposed IDS on three different sources, including, CSE-CIC-IDS2018, MQTTset, and InSDN. The results demonstrate that the FELIDS system outperforms the classic/centralized versions of machine learning (non-federated learning) in protecting the privacy of IoT devices data and achieves the highest accuracy in detecting attacks.},
  archive      = {J_JPDC},
  author       = {Othmane Friha and Mohamed Amine Ferrag and Lei Shu and Leandros Maglaras and Kim-Kwang Raymond Choo and Mehdi Nafaa},
  doi          = {10.1016/j.jpdc.2022.03.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FELIDS: Federated learning-based intrusion detection system for agricultural internet of things},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based selection of optimal MPI broadcast algorithms
for multi-core clusters. <em>JPDC</em>, <em>165</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of collective communication operations determines the overall performance of MPI applications. Different algorithms have been developed and implemented for each MPI collective operation, but none proved superior in all situations. Therefore, MPI implementations have to solve the problem of selecting the optimal algorithm for the collective operation depending on the platform, the number of processes involved, the message size(s), etc. The current solution method is purely empirical. Recently, an alternative solution method using analytical performance models of collective algorithms has been proposed and proved both accurate and efficient for one-process-per-CPU configurations. The method derives the analytical performance models of algorithms from their code implementation rather than from high-level mathematical definitions, and estimates the parameters of the models separately for each algorithm. The method is network and topology oblivious and uses the Hockney model for point-to-point communications. In this paper, we extend that selection method to the case of clusters of multi-core processors, where each core of the platform runs a process of the MPI application. We present the proposed approach using Open MPI broadcast algorithms, and experimentally validate it on three different clusters of multi-core processors, Grisou, Gros and MareNostrum4.},
  archive      = {J_JPDC},
  author       = {Emin Nuriyev and Juan-Antonio Rico-Gallego and Alexey Lastovetsky},
  doi          = {10.1016/j.jpdc.2022.03.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Model-based selection of optimal MPI broadcast algorithms for multi-core clusters},
  volume       = {165},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast quantum algorithm for protein structure prediction in
hydrophobic-hydrophilic model. <em>JPDC</em>, <em>164</em>, 178–190. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In its unfolded form, a protein is a linear sequence of amino acids. Protein structure prediction attempts to find the native conformation for a given protein, which has potential applications in drug and vaccine development. Classically, protein structure prediction is an NP-complete, unsolved computational problem. Quantum computing however promises to improve upon the performance of classical algorithms. Here we develop a quantum algorithm in hydrophobic-hydrophilic model on two-dimensional square lattice to solve the problem for any sequence of length N amino acids with a quadratic speedup over its classical counterpart. This speedup is achieved using Grover&#39;s quantum search algorithm. The algorithm can be used for amino acid sequences of arbitrary length. It consists of three stages: (1) preparation of a superposition state that encodes all possible 2 2 ( N − 1 ) 22(N−1) conformations, (2) calculation of coordinates and energy for each possible conformation in parallel, and (3) finding the conformation with the minimal energy. The asymptotic complexity with regard to space is O ( N 3 ) O(N3) , while the obtained speedup is quadratic compared to the classical counterpart. We have successfully simulated the algorithm on the IBM Quantum&#39;s qasm simulator using Qiskit SDK. Also, we have further confirmed the correctness of the results by calculating theoretical probability of finding the right conformation.},
  archive      = {J_JPDC},
  author       = {Renata Wong and Weng-Long Chang},
  doi          = {10.1016/j.jpdc.2022.03.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {178-190},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast quantum algorithm for protein structure prediction in hydrophobic-hydrophilic model},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic asynchronous iterations. <em>JPDC</em>,
<em>164</em>, 168–177. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems can be solved by iteration by multiple participants (processors, servers, routers etc.). Previous mathematical models for such asynchronous iterations assume a single function being iterated by a fixed set of participants. We will call such iterations static since the system&#39;s configuration does not change. However in several real-world examples, such as inter-domain routing, both the function being iterated and the set of participants change frequently while the system continues to function. In this paper we extend Üresin and Dubois&#39;s work on static iterations to develop a model for this class of dynamic or always on asynchronous iterations. We explore what it means for such an iteration to be implemented correctly, and then prove two different conditions on the set of iterated functions that guarantee the full asynchronous iteration satisfies this new definition of correctness. These results have been formalised in Agda and the resulting library is publicly available.},
  archive      = {J_JPDC},
  author       = {Matthew L. Daggitt and Timothy G. Griffin},
  doi          = {10.1016/j.jpdc.2022.03.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {168-177},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dynamic asynchronous iterations},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable blockchain model using off-chain IPFS storage for
healthcare data security and privacy. <em>JPDC</em>, <em>164</em>,
152–167. (<a href="https://doi.org/10.1016/j.jpdc.2022.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional healthcare systems in the present scenario follow centralized client-server architecture to store and process patient-health related information. Data stored in each of the healthcare institution remain in silos which cannot be easily shared with other institutions due to technical and infrastructural constraints. Hospitals do not have an effective and secure data sharing mechanism leading to monetary and resource loss in the case of a person visiting different hospitals. Blockchain, a disruptive technology with secure and reliable decentralized framework, and can be used to circumvent problems in traditional healthcare architecture for secure storage, sharing and retrieval of Electronic Health Records (EHR). A blockchain-based framework integrated with InterPlanetary File System (IPFS) for EHR in healthcare management has been proposed in this paper. This proposed framework will enable healthcare institutions to maintain fail-safe and tamper-proof healthcare ledgers in a decentralized manner. Hospitals and doctors act as lightweight nodes, whereas patient nodes can be full or lightweight nodes. The model proposes two-factor authentication and multi-factor authentication for preventing fake node attacks. Patient-centric access model allows the patients to act as digital stewards for their health data, allowing access to doctors and hospitals on demand and revoking it after stipulated time. Symmetric key encryption (AES-128) is used for encrypting data before storing into IPFS. Asymmetric encryption (RSA-4096) is used for generating digital envelopes to pass on symmetric key to authorized entities. Digital signatures (RSA-1024) make sure that the transactions are valid and from authorized nodes. Hashing of the encrypted data is done using SHA-256 algorithm. Multiple layers of security implemented in this model makes sure that adversaries cannot obtain data stored in IPFS; even if they retrieve the data, it will not be meaningful since it is encrypted. The proposed framework for off-chain storage of health data using IPFS saves blockchain structure from scalability issues. Further the proposal for blockchain integration with IPFS helps preserve privacy in the healthcare system, making it highly secure, scalable and robust.},
  archive      = {J_JPDC},
  author       = {Jayapriya Jayabalan and N. Jeyanthi},
  doi          = {10.1016/j.jpdc.2022.03.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {152-167},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scalable blockchain model using off-chain IPFS storage for healthcare data security and privacy},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Oppositional chaos game optimization based clustering with
trust based data transmission protocol for intelligent IoT edge systems.
<em>JPDC</em>, <em>164</em>, 142–151. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, the Internet of Things (IoT) becomes essential in consumer and industrial applications. The accessibility of high bandwidth Internet connection particularly with the arrival of robust 5G networks rises to innovative IoT solutions such as smart city, automobiles, industry 4.0, etc. IoT analytics represents edge computing as a term commonly employed for defining intelligent computational resources placed closer to the source of data generation. Despite the benefits of the IoT edge systems, security and energy efficiency remains major challenging issues. With this motivation, this paper presents an energy-efficient clustering based secure data transmission protocol (EEC-SDTP) for Intelligent IoT Edge systems. The goal of the EEC-SDTP technique is to select an appropriate set of cluster heads (CHs) and optimally secure routes for data transmission in the network. The proposed model involves oppositional chaos game optimization-based clustering (OCGOC) technique for proper CH selection and cluster construction. Besides, a trust-based model is designed to determine the trustworthiness of the node in the IoT edge systems. The proposed OCGOC technique derives a fitness function utilizing 3 input parameters like trust level, distance to neighbors, and energy. Finally, a trust-based secure routing protocol using the quantum sand piper optimization (SRP-QSPO) technique is employed to derive routes for secure data transmission. For examining the better efficiency of the proposed EEC-SDTP algorithm, an extensive group of experimentations were performed and the outcomes are investigated under several performance measures. The experimental outcomes highlighted the improved efficiency of the proposed method over the other related techniques.},
  archive      = {J_JPDC},
  author       = {M. Padmaa and T. Jayasankar and S. Venkatraman and Ashit Kumar Dutta and Deepak Gupta and Shahab Shamshirband and Joel J.P.C. Rodrigues},
  doi          = {10.1016/j.jpdc.2022.03.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {142-151},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Oppositional chaos game optimization based clustering with trust based data transmission protocol for intelligent IoT edge systems},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mobility-based deployment strategy for edge data centers.
<em>JPDC</em>, <em>164</em>, 133–141. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of Multi-access Edge Computing (MEC) is to bring computational capabilities at the edge of the network to better support low-latency applications. Such capabilities are typically offered by Edge Data Centers (EDC). The MEC paradigm is not tied to a single radio technology, rather it embraces both cellular and other radio access technologies such as WiFi. Distributed intelligence at the edge for AI purposes requires careful spatial planning of computing and storage resources. The problem of EDC deployment in urban environments is challenging and, to the best of our knowledge, it has been explored only for cellular connectivity so far. In this paper, we study the possibility of deploying EDC without analyzing the expected data traffic load of the cellular network, a kind of information rarely shared by network operators. To this purpose, we propose in this work CLUB, CLUstering-Based strategy tailored on the analysis of urban mobility. We analyze two experimental mobility data sets, and we analyze some mobility features in order to characterize their properties. Finally, we compare the performance of CLUB against state-of-the-art techniques in terms of the outage probability, namely the probability an EDC is not able to serve a request. Our results show that the CLUB strategy is always comparable with respect to our benchmarks, but without using any information related to network traffic.},
  archive      = {J_JPDC},
  author       = {Michele Girolami and Piergiorgio Vitello and Andrea Capponi and Claudio Fiandrino and Luca Foschini and Paolo Bellavista},
  doi          = {10.1016/j.jpdc.2022.03.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {133-141},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A mobility-based deployment strategy for edge data centers},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel neural network approach for airfoil mesh quality
evaluation. <em>JPDC</em>, <em>164</em>, 123–132. (<a
href="https://doi.org/10.1016/j.jpdc.2022.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating mesh quality before solving is crucially important for error control in the numerical simulation of airfoils. Traditional mesh quality metrics are used to identify distorted mesh elements by analyzing their geometric shape information like angles and edges. However, these metrics fail to recognize numerical errors stemming from quality attributes such as improper mesh density or distribution. This deficiency has imposed a burden on intensive manual re-evaluation, which heavily increases the meshing cost. In this paper, we introduce deep neural networks to airfoil mesh quality evaluation to improve its automation and efficiency. Specifically, we propose a neural network-based evaluation model, MQNet, accompanied by the first large-scale mesh benchmark dataset, AirfoilSet. MQNet is trained on the dataset to learn the quality-related attributes including mesh orthogonality, smoothness, and density. Thereafter, the trained network can be used as a black box to automatically evaluate and output the overall quality of the airfoil mesh. To demonstrate the effectiveness and generalization behaviors of the proposed network, we establish an MQNet-based mesh generation workflow. Experimental results show that MQNet is small and accurate. It outperforms widely-used neural networks and can be a useful tool to guide the high-quality mesh generation process with minimal user intervention.},
  archive      = {J_JPDC},
  author       = {Xinhai Chen and Chunye Gong and Jie Liu and Yufei Pang and Liang Deng and Lihua Chi and Kenli Li},
  doi          = {10.1016/j.jpdc.2022.03.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {123-132},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel neural network approach for airfoil mesh quality evaluation},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying challenges and opportunities of in-memory
computing on large HPC systems. <em>JPDC</em>, <em>164</em>, 106–122.
(<a href="https://doi.org/10.1016/j.jpdc.2022.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing fidelity and resolution enabled by high-performance computing systems, simulation-based scientific discovery is able to model and understand microscopic physical phenomena at a level that was not possible in the past. A grand challenge that the HPC community facing is how to maintain the large amounts of analysis data generated from simulations. In-memory computing, among others, is recognized to be a viable path forward and has experienced tremendous success in the past decade. Nevertheless, there has been a lack of a complete study and understanding of in-memory computing as a whole on HPC systems. Given the enlarging disparity between compute and HPC storage I/O, it is urgent for the HPC community to assess the state of in-memory computing and understand the challenges and opportunities. This paper presents a comprehensive study of in-memory computing with regard to its software evolution, performance, usability, robustness, and portability. In particular, we conduct an indepth analysis on the evolution of in-memory computing based upon more than 3,000 commits, and use realistic workflows for two scientific workloads, i.e., LAMMPS and Laplace to quantitatively assess state-of-the-art in-memory computing libraries, including DataSpaces, DIMES, Flexpath, Decaf and SENSEI on two leading supercomputers , Titan and Cori. Our studies not only illustrate the performance and scalability, but also reveal the key aspects that are of interest to library developers and users, including usability, robustness, portability, potential design defects, etc.},
  archive      = {J_JPDC},
  author       = {Dan Huang and Zhenlu Qin and Qing Liu and Norbert Podhorszki and Scott Klasky},
  doi          = {10.1016/j.jpdc.2022.02.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {106-122},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Identifying challenges and opportunities of in-memory computing on large HPC systems},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient heuristic switch migration scheme for
software-defined vehicular networks. <em>JPDC</em>, <em>164</em>,
96–105. (<a href="https://doi.org/10.1016/j.jpdc.2022.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Vehicular Networks (SDVNs) have been a vital addition to the design of intelligent vehicular networks. SDVNs elevate the constraints of static hardware network devices by using programmable units, providing a global view of the network status, and standardizing the interface between different wireless access technologies. However, the static deployment and assignment of switches to control units do not consider vehicular network&#39;s rapid mobility changes and diverse densities. In this article, we propose a mobility-based heuristic switch migration scheme for software-defined vehicular networks. The proposed approach utilizes vehicles&#39; mobility among switch-enabled roadside units to efficiently migrate selected switches between different controllers. The proposed scheme efficiently migrated switch-enabled roadside units between distributed control units while maintaining low vehicles&#39; migration delay and cost. We have evaluated the proposed method under different environments and scenarios and reported its migration cost and delay performance.},
  archive      = {J_JPDC},
  author       = {Noura Aljeri and Azzedine Boukerche},
  doi          = {10.1016/j.jpdc.2022.01.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {96-105},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient heuristic switch migration scheme for software-defined vehicular networks},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the performance of batch schedulers using online
job runtime classification. <em>JPDC</em>, <em>164</em>, 83–95. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job scheduling in high-performance computing platforms is a hard problem that involves uncertainties on both the job arrival process and their execution times. Users typically provide only loose upper bounds for job execution times, which are not so useful for scheduling heuristics based on processing times. Previous studies focused on applying regression techniques to obtain better execution time estimates, which worked reasonably well and improved scheduling metrics. However, these approaches require a long period of training data. In this work, we propose a simpler approach by classifying jobs as small or large and prioritizing the execution of small jobs over large ones. Indeed, small jobs are the most impacted by queuing delays, but they typically represent a light load and incur a small burden on the other jobs. The classifier operates online and learns by using data collected over the previous weeks, facilitating its deployment and enabling a fast adaptation to changes in the workload characteristics. We evaluate our approach using four scheduling policies on seven HPC platform workload traces. We show that: first, incorporating such classification reduces the average bounded slowdown of jobs in all scenarios, second, in most considered scenarios, the improvements are comparable to the ideal hypothetical situation where the scheduler would know in advance the exact running time of jobs.},
  archive      = {J_JPDC},
  author       = {Salah Zrigui and Raphael Y. de Camargo and Arnaud Legrand and Denis Trystram},
  doi          = {10.1016/j.jpdc.2022.01.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-95},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving the performance of batch schedulers using online job runtime classification},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mutation and dynamic objective-based farmland fertility
algorithm for workflow scheduling in the cloud. <em>JPDC</em>,
<em>164</em>, 69–82. (<a
href="https://doi.org/10.1016/j.jpdc.2022.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many scientific applications are deployed in the cloud to execute at a lower cost. However, the growing scale of workflows makes scheduling problems challenging. To minimize the workflow execution cost under deadline constraints, this article proposes a Mutation and Dynamic Objective-based Farmland Fertility (MDO-FF) algorithm for obtaining a near-optimal solution within a relatively shorter time. A Dynamic Objective Strategy (DOS) is introduced to accelerate the convergence speed, while a multi-swarm evolutionary approach and mutation strategies are incorporated to enhance the search diversity and help to escape from local optima. By seeking new potential solutions and searching in its corresponding neighborhoods, our proposed MDO-FF can make a good trade-off between exploration and exploitation. Extensive experiments are conducted on well-known scientific workflows with different types and sizes. The experimental results demonstrate that in most cases, our MDO-FF outperforms the existing algorithms in terms of constraint satisfiability and solution quality.},
  archive      = {J_JPDC},
  author       = {Huifang Li and Yizhu Wang and Jingwei Huang and Yushun Fan},
  doi          = {10.1016/j.jpdc.2022.02.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {69-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Mutation and dynamic objective-based farmland fertility algorithm for workflow scheduling in the cloud},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed intrusion detection system to detect DDoS
attacks in blockchain-enabled IoT network. <em>JPDC</em>, <em>164</em>,
55–68. (<a href="https://doi.org/10.1016/j.jpdc.2022.01.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is emerging as a new technology for the development of various critical applications. However, these applications are still working on centralized storage architecture and have various key challenges like privacy, security, and single point of failure. Recently, the blockchain technology has emerged as a backbone for the IoT-based application development. The blockchain can be leveraged to solve privacy, security, and single point of failure (third-part dependency) issues of IoT applications. The integration of blockchain with IoT can benefit both individual and society. However, 2017 Distributed Denial of Service (DDoS) attack on mining pool exposed the critical fault-lines among blockchain-enabled IoT network. Moreover, this application generates huge amount of data. Machine Learning (ML) gives complete autonomy in big data analysis, capabilities of decision making and therefore is used as an analytical tool. Thus, in order to address above challenges, this paper proposes a novel distributed Intrusion Detection System (IDS) using fog computing to detect DDoS attacks against mining pool in blockchain-enabled IoT Network. The performance is evaluated by training Random Forest (RF) and an optimized gradient tree boosting system (XGBoost) on distributed fog nodes. The proposed model effectiveness is assessed using an actual IoT-based dataset i.e., BoT-IoT, which includes most recent attacks found in blockchain-enabled IoT network. The results indicate, for binary attack-detection XGBoost outperforms whereas for multi-attack detection Random Forest outperforms. Overall on distributed fog nodes RF takes less time for training and testing compared to XGBoost.},
  archive      = {J_JPDC},
  author       = {Randhir Kumar and Prabhat Kumar and Rakesh Tripathi and Govind P. Gupta and Sahil Garg and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.jpdc.2022.01.030},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {55-68},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A distributed intrusion detection system to detect DDoS attacks in blockchain-enabled IoT network},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient microscopy image analysis on CPU-GPU systems with
cost-aware irregular data partitioning. <em>JPDC</em>, <em>164</em>,
40–54. (<a href="https://doi.org/10.1016/j.jpdc.2022.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of high resolution whole slide tissue images is a computationally expensive task, which adversely impacts effective use of pathology imaging data in research. We propose runtime solutions to enable efficient execution of pathology image analysis applications on modern distributed memory hybrid platforms equipped with both CPUs and GPUs . Hybrid systems offer significant computation capacity, but taking advantage of this computing power is complex. An application developer may have to implement multiple versions of data processing codes targeted for different computing devices. The developer also has to tackle the challenges of efficiently distributing computational load among the nodes of a distributed memory machine and among computing devices on a node. This is particularly difficult in analysis of high resolution images because of irregular computing costs of processing different image regions. In order to address these problems, we have leveraged a high-level image processing language (Halide) and integrated it into our runtime system called Region Templates (RT). The language simplifies the application development while generating code for multiple devices, such as CPU and GPU. The integration with RT allows for efficient multiple node hybrid execution. We also developed a novel cost-aware data partitioning (CADP) strategy that considers the workload irregularity to minimize load imbalance. Our experimental evaluation shows significant performance improvements on hybrid CPU-GPU machines, as compared with using a single processor (CPU or GPU), as well as on multi-GPU systems. CADP resulted in 1.7× better performance than other workload partitioning approaches (e.g., KD-Trees) on a hybrid machine and was up to 2.24× faster in multi-node settings.},
  archive      = {J_JPDC},
  author       = {Willian Barreiros Jr. and Alba C.M.A. Melo and Jun Kong and Renato Ferreira and Tahsin M. Kurc and Joel H. Saltz and George Teodoro},
  doi          = {10.1016/j.jpdc.2022.02.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {40-54},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient microscopy image analysis on CPU-GPU systems with cost-aware irregular data partitioning},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifespan-based garbage collection to improve SSD’s
reliability and performance. <em>JPDC</em>, <em>164</em>, 28–39. (<a
href="https://doi.org/10.1016/j.jpdc.2022.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the “out-of-place” nature of write mode, Solid State Drive (SSD) has to execute a garbage collection (GC) to reclaim the space of invalidated data and free flash blocks for new written data. However, the GC can incur the movements of valid data inside SSD, resulting in the problems such as write latency, write amplification, etc. The GC overheads of SSD depend not only on the current write pattern but also on how data have been already placed in SSD. If data can be classified by their lifetime when written to SSD, and the data with similar lifetime are written into the same flash block, we can select the block with the shortest expected lifespan as the victim block. Ideally, the victim block contains few valid data, implying there are few valid pages that need to be migrated during the GC process with greatly reduced overhead. In this paper, we propose and implement a GC method, called Lifespan-based GC , for SSDs based on the data lifetime, together with the I/O distribution and the dynamic flash memory allocation. The results show that for those applications with the data lifetime characteristics, the lifespan-based GC can effectively reduce the amount of valid pages migrated in GC process , thereby minimizing the write latency and write amplification caused by GC.},
  archive      = {J_JPDC},
  author       = {Wen Cheng and Mi Luo and Lingfang Zeng and Yang Wang and André Brinkmann},
  doi          = {10.1016/j.jpdc.2022.02.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {28-39},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Lifespan-based garbage collection to improve SSD&#39;s reliability and performance},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SimGQ+: Simultaneously evaluating iterative point-to-all and
point-to-point graph queries. <em>JPDC</em>, <em>164</em>, 12–27. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing frameworks are typically designed to optimize the evaluation of a single graph query. However, in practice, we often need to respond to multiple graph queries, either from different users or from a single user performing a complex analytics task. Therefore in this paper we develop SimGQ+ , a system that optimizes simultaneous evaluation of a group of vertex queries that originate at different source vertices (e.g., multiple shortest path queries originating at different source vertices) and delivers substantial speedups over a conventional framework that evaluates and responds to queries one by one. Our work considers both point-to-all and point-to-point queries. The performance benefits are achieved via batching and sharing . Batching fully utilizes system resources to evaluate a batch of queries and amortizes runtime overheads incurred due to fetching vertices and edge lists, synchronizing threads, and maintaining computation frontiers. Sharing dynamically identifies shared queries that substantially represent subcomputations in the evaluation of different queries in a batch, evaluates the shared queries, and then uses their results to accelerate the evaluation of all queries in the batch. With four input power-law graphs and four graph algorithms SimGQ+ achieves speedups of up to 45.67× with batch sizes of up to 512 queries over the baseline implementation that evaluates the queries one by one using the state of the art Ligra system. Moreover, both batching and sharing contribute substantially to the speedups.},
  archive      = {J_JPDC},
  author       = {Chengshuo Xu and Abbas Mazloumi and Xiaolin Jiang and Rajiv Gupta},
  doi          = {10.1016/j.jpdc.2022.01.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {12-27},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SimGQ+: Simultaneously evaluating iterative point-to-all and point-to-point graph queries},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SG-PBFT: A secure and highly efficient distributed
blockchain PBFT consensus algorithm for intelligent internet of
vehicles. <em>JPDC</em>, <em>164</em>, 1–11. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an application of Internet of Things (IoT) technology, the Internet of Vehicles (IoV) faces two main security issues: (1) the central server of the IoV may not be powerful enough to support the centralized authentication of the rapidly increasing connected vehicles, (2) the IoV itself may not be robust enough to single-node attacks. To address these issues, in this paper, we propose SG-PBFT (Score Grouping-PBFT), a secure and efficient distributed consensus algorithm for blockchain applications in the IoV. The distributed structure can reduce the pressure on the central server and decrease the risk of single-node attacks. The SG-PBFT consensus algorithm improves the traditional practical Byzantine fault tolerance (PBFT) consensus algorithm by optimizing the PBFT consensus process and using a score grouping mechanism to achieve a higher consensus efficiency. The experimental results show that the method can greatly improve the consistency efficiency and effectively prevent single-node attacks. Specifically, when the number of consensus nodes reaches 1000, the consensus time of our algorithm is only about 27\% of what is required for the state-of-the-art PBFT consensus algorithm. Our proposed SG-PBFT algorithm is versatile and can be used in other application scenarios which require high consensus efficiency.},
  archive      = {J_JPDC},
  author       = {Guangquan Xu and Hongpeng Bai and Jun Xing and Tao Luo and Neal N. Xiong and Xiaochun Cheng and Shaoying Liu and Xi Zheng},
  doi          = {10.1016/j.jpdc.2022.01.029},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-11},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SG-PBFT: A secure and highly efficient distributed blockchain PBFT consensus algorithm for intelligent internet of vehicles},
  volume       = {164},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optically connected memory for disaggregated data centers.
<em>JPDC</em>, <em>163</em>, 300–312. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in integrated photonics enable the implementation of reconfigurable, high-bandwidth, and low energy-per-bit interconnects in next-generation data centers . We propose and evaluate an Optically Connected Memory ( OCM ) architecture that disaggregates the main memory from the computation nodes in data centers. OCM is based on micro-ring resonators (MRRs), and it does not require any modification to the DRAM memory modules. We calculate energy consumption from real photonic devices and integrate them into a system simulator to evaluate performance. Our results show that (1) OCM is capable of interconnecting four DDR4 memory channels to a computing node using two fibers with 1.02 pJ energy-per-bit consumption and (2) OCM performs up to 5.5× faster than a disaggregated memory with 40G PCIe NIC connectors to computing nodes.},
  archive      = {J_JPDC},
  author       = {Jorge Gonzalez and Mauricio G. Palma and Maarten Hattink and Ruth Rubio-Noriega and Lois Orosa and Onur Mutlu and Keren Bergman and Rodolfo Azevedo},
  doi          = {10.1016/j.jpdc.2022.01.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {300-312},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optically connected memory for disaggregated data centers},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FGFL: A blockchain-based fair incentive governor for
federated learning. <em>JPDC</em>, <em>163</em>, 283–299. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning is a framework that coordinates a large amount of workers to train a shared model in a distributed manner, in which the training data are located on the workers&#39; sides in order to preserve data privacy. There are two challenges in the crowdsourcing of FL, the workers who participant in training need to consume computing and communication resources, so that they are reluctant to participate in the training process if they can not get reasonable rewards. Moreover, there may be attackers who send arbitrary updates to get undeserving compensation or even destroy the model, thus, effective prevention of malicious workers is also critical. An incentive mechanism is urgently required in order to encourage high-quality workers to participate in FL and to punish the attackers. In this paper, we propose FGFL, a blockchain-based incentive governor for Federated Learning . In FGFL, we assess the participants with reputation and contribution indicators. Then the task publisher rewards workers fairly to attract efficient ones while the malicious ones are punished and eliminated. In addition, we propose a blockchain-based incentive management system to manage the incentive mechanism. We evaluate the effectiveness and fairness of FGFL through theoretical analysis and comprehensive experiments. The evaluation results show that FGFL fairly rewards workers according to their corresponding behavior and quality. FGFL increases the system revenue by 0.2\% to 3.4\% in reliable federations compared with baselines. And in the unreliable scenario where contains attackers, the system revenue of FGFL outperforms the baselines by more than 46.7\%.},
  archive      = {J_JPDC},
  author       = {Liang Gao and Li Li and Yingwen Chen and ChengZhong Xu and Ming Xu},
  doi          = {10.1016/j.jpdc.2022.01.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {283-299},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FGFL: A blockchain-based fair incentive governor for federated learning},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Early scheduling on steroids: Boosting parallel state
machine replication. <em>JPDC</em>, <em>163</em>, 269–282. (<a
href="https://doi.org/10.1016/j.jpdc.2022.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State machine replication (SMR) is a standard approach to fault tolerance in which replicas execute requests deterministically and often serially. For performance, some techniques allow concurrent execution of requests in SMR while keeping determinism. Such techniques exploit the fact that independent requests can execute concurrently. A promising category of early scheduling solutions trades scheduling freedom for simplicity, allowing to expedite decisions during scheduling. This paper generalizes early scheduling and proposes a general method to schedule requests to threads, restricting scheduling overhead. Moreover, it explores improvements to the original early scheduling mechanism, namely the use of busy-wait synchronization and work-stealing techniques. We integrate early scheduling and its proposed improvements to a popular SMR framework. Performance results of the basic mechanism and its improvements are presented and compared to more classic approaches, where it is shown that early scheduling with our proposed enhancements can outperform the original early scheduling and other systems by a large margin in many scenarios.},
  archive      = {J_JPDC},
  author       = {Eliã Batista and Eduardo Alchieri and Fernando Dotti and Fernando Pedone},
  doi          = {10.1016/j.jpdc.2022.02.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {269-282},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Early scheduling on steroids: Boosting parallel state machine replication},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General-purpose GPU hashing data structures and their
application in accelerated genomics. <em>JPDC</em>, <em>163</em>,
256–268. (<a href="https://doi.org/10.1016/j.jpdc.2022.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A broad variety of applications relies on associative data structures that exclusively support insert, retrieve, and delete operations. Hash maps represent such a class of effective dictionary implementations. Properties such as amortized constant time complexity for these table operations as well as a compact memory layout make them versatile data structures with manifold applications in data analytics and artificial intelligence . The rapidly growing amount of data emerging in many scientific fields can often only be tackled with modern massively parallel accelerators such as GPUs . Numerous GPU hash table implementations have been proposed over the recent years. However, most of these implementations lack flexibility in order to be used in existing analytics pipelines or suffer from significant performance degradation for certain application scenarios. As a more recent approach, the WarpCore framework aims to alleviate these aforementioned restrictions by placing a focus on both versatility and performance. In this work we reflect the key concepts of the WarpCore library and provide an extensive performance evaluation against the state-of-the-art. We further explore how WarpCore can be used for accelerating two bioinformatics applications (metagenomic classification and k -mer counting) with significant speedups.},
  archive      = {J_JPDC},
  author       = {Daniel Jünger and Robin Kobus and André Müller and Christian Hundt and Kai Xu and Weiguo Liu and Bertil Schmidt},
  doi          = {10.1016/j.jpdc.2022.01.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {256-268},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {General-purpose GPU hashing data structures and their application in accelerated genomics},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative deep learning framework on IoT data with
bidirectional NLSTM neural networks for energy consumption forecasting.
<em>JPDC</em>, <em>163</em>, 248–255. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption forecasting based on IoT data and deep learning algorithm inheriting distributed and collaborative learning is a widely studied topic both in engineering and computer science fields. For different households with drastically different energy consumption patterns, the traditional centralized machine learning (ML) and deep learning (DL) methods suffer problems including inaccuracy, inefficiency and laggings of the prediction performance. In this study, we propose a sophisticated multi-channel bidirectional nested LSTM framework (MC-BiNLSTM) combined with discrete stationary wavelet transform (SWT) for highly accurate and efficient energy consumption forecasting. The main contributions of this study include the decomposition using SWT for accuracy improvement and the collaborative BiNLSTM structure for efficiency improvement. A real-world IoT energy consumption dataset, named UK-DALE, is adopted for the comparative study. The experimental results showed the outperformance of the proposed method from various perspectives over the cutting-edge methods existed in the literature.},
  archive      = {J_JPDC},
  author       = {Ke Yan and Xiaokang Zhou and Jinjun Chen},
  doi          = {10.1016/j.jpdc.2022.01.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {248-255},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Collaborative deep learning framework on IoT data with bidirectional NLSTM neural networks for energy consumption forecasting},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Workload-aware storage policies for cloud object storage.
<em>JPDC</em>, <em>163</em>, 232–247. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different applications have different access characteristics and various performance requirements. Thus, the shared cloud object store entails providing tenant-specific policies. However, the limited configurability of existing storage policies makes it difficult to provide efficient and flexible policies to meet tenants&#39; evolving needs. First, existing policies that only control request forwarding cannot provide sufficient optimizations for workload performance. Second, those policies lack the flexibility to adapt to the possible workload changes during runtime. In this paper, we propose Mass, a programmable framework to provide the enhanced storage policies for diverse workloads based on their access characteristics. We also design its enhancements, C-Mass, extending Mass&#39;s capabilities through container-based policy deployment to efficiently handle workload changes. Compared with existing storage policies, the latency and throughput of workloads under Mass are improved by up to 81.6\% and 231.5\%, respectively. Further, the workload performance under C-Mass is optimized by up to 40\%.},
  archive      = {J_JPDC},
  author       = {Yu Chen and Wei Tong and Dan Feng and Zike Wang},
  doi          = {10.1016/j.jpdc.2022.01.026},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {232-247},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Workload-aware storage policies for cloud object storage},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending an asynchronous runtime system for high throughput
applications: A case study. <em>JPDC</em>, <em>163</em>, 214–231. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current supercomputers are mostly composed of vast numbers of nodes enhanced with accelerators (usually in the form of GPUs). However, having these heterogeneous designs in the forefront have exposed the software toolchains and application designers to the underlying complexities of an ever evolving hardware substrate. The need for a more dynamic view from the system software (i.e., compilers and runtimes) has become more apparent in these environments. Due to this, adaptive, fine grain runtime systems have seen a rise in popularity in the past decades. With low overhead and small tasks, these runtimes help to hide long latency operations by exploiting the massive concurrency presented in different application workflows . Such features allow the reduction of idle time (a result from ever deeper and complex memory hierarchies and memory types) with the execution of unrelated work across the machine. Of these runtimes, the Asynchronous Many Task (AMT) Runtimes are excellent exemplars as they can efficiently map onto hardware substrates and exhibit a high degree of latency hiding. Because of their latency tolerant characteristics, applications such as Graph Analytics and Big Data applications (which are latency sensitive) can use these runtimes very efficiently. Thanks to these characteristics, we present how a careful design can help to exploit the properties of an AMT when running high latency applications such as the ones encountered in the Big Data domain. In addition, when combined with introspection / adaptive capabilities, the runtime can further exploit optimization opportunities during its execution based on the ever changing state of the underlying hardware substrate. As a vehicle for this exploration, we use the Performance Open Community Runtime (P-OCR) to test all these concepts with Big Data workloads.},
  archive      = {J_JPDC},
  author       = {Joshua Suetterlein and Joseph Manzano and Andres Marquez and Guang R Gao},
  doi          = {10.1016/j.jpdc.2022.01.027},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {214-231},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Extending an asynchronous runtime system for high throughput applications: A case study},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Customer-satisfaction-aware and deadline-constrained profit
maximization problem in cloud computing. <em>JPDC</em>, <em>163</em>,
198–213. (<a href="https://doi.org/10.1016/j.jpdc.2022.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new and modern service method that can meet the requirements of customers effectively, cloud computing is becoming more and more popular. For cloud service providers, they play an important role in constructing the cloud computing platform, which is convenient for customers to enjoy the services without paying attention to the execution process of the services. For cloud service providers and customers, they focus on profit and satisfaction respectively in service supply-demand relationship, and the pursuit of their objectives both have an influence on each other. On one hand, the configuration of the cloud computing platform is the main solution for cloud service providers to earn profit, which affects customer satisfaction based on specific constraints. On the other hand, customer satisfaction affects the arrival rate of service requests, which will affect the profit in turn. In this paper, we devote ourselves to analyzing a deadline constrained profit maximization problem in a cyclic cascade queuing system , in which the maximum tolerance of customers towards task waiting time is taken into consideration. On this basis, the definition of customer satisfaction is given. And then, a mathematical model is formulated in detail on how customer satisfaction affects the revenues of cloud service providers, and further affects their profits. However, the exact solutions of such model can hardly be calculated due to its complexity. Hence, we propose a heuristic algorithm to find the high-quality solution, then the optimal configuration of cloud computing platform can be obtained to achieve the maximization of the profit for cloud service providers on the basis of promoting customer satisfaction. At last, a series of numerical simulations are conducted to validate the performance of the proposed algorithm. The results show that the proposed algorithm can not only optimize profit and customer satisfaction simultaneously but also maintain them at a high level in the long run of the system.},
  archive      = {J_JPDC},
  author       = {Siyi Chen and Jin Liu and Fengchao Ma and Huixian Huang},
  doi          = {10.1016/j.jpdc.2022.02.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {198-213},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Customer-satisfaction-aware and deadline-constrained profit maximization problem in cloud computing},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Separating lock-freedom from wait-freedom at every level of
the consensus hierarchy. <em>JPDC</em>, <em>163</em>, 181–197. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long-standing open question has been whether lock-freedom and wait-freedom are fundamentally different progress conditions, namely, can the former be provided in situations where the latter cannot? This paper answers the question in the affirmative, by proving that there are objects with lock-free implementations, but without wait-free implementations—using the same set of objects of any finite coordination power. We precisely define an object called n-process long-lived approximate agreement ( n -LLAA), in which two sets of processes associated with two sides , 0 or 1, need to decide on a sequence of increasingly closer outputs. We prove that 2-LLAA has a lock-free linearizable implementation using read/write objects only, and that n -LLAA has a lock-free linearizable implementation using read/write objects and ( n − 1 ) (n−1) -process consensus objects. In contrast, we prove that there is no wait-free linearizable implementation of n -LLAA using read/write objects and specific ( n − 1 ) (n−1) -process consensus objects, called ( n − 1 ) (n−1) -window registers.},
  archive      = {J_JPDC},
  author       = {Hagit Attiya and Armando Castañeda and Danny Hendler and Matthieu Perrin},
  doi          = {10.1016/j.jpdc.2022.01.025},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {181-197},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Separating lock-freedom from wait-freedom at every level of the consensus hierarchy},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid heuristic-based key generation protocol for
intelligent privacy preservation in cloud sector. <em>JPDC</em>,
<em>163</em>, 166–180. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing is defined as a set of software and hardware that are used together for delivering different kinds of cloud services based on user&#39;s demand. Cloud computing secures a major role in the Information Technology (IT) industry for accessing the services at any place around the world. On the other hand, there are increasing vulnerabilities and threats in the cloud environment due to the rise in popularity and demands in cloud computing services . Data privacy and integrity are the major issues in cloud computing while storing data in various geographical locations. So, it is necessary to consider the data privacy and integrity factors in a cloud computing environment. It is difficult to construct a common platform to interact in the cloud environment. So, it is essential for implementing the security solutions, which need to provide confidentiality while exchanging the data. The main intention of this paper is to design and develop a novel artificial intelligence approach for handling the privacy preservation problem in the cloud sector. Through the process of data sanitization, the sensitive data is hidden, so that it cannot be accessed by unauthorized users. To perform the sanitization process, the heuristic-based key generations play a vital role, and here, it is solved by considering a multi-objective function with constraints like the “degree of modification, hiding ratio, and information preservation ratio”. This multi-objective problem is solved by the adoption of novel Probability Switch searched Butterfly-Moth Flame Optimization (PS-BMFO). Finally, the restoration is also performed by the same PS-BMFO-based key generation. The analysis results confirm that the suggested model preserves privacy and ensures the integrity of the user&#39;s data against unauthorized parties. From the experimental analysis, the proposed PS-BMFO gives 12.5\%, 10\%, 30.7\%, and 12.5\% enriched than GWO, JA, MFO, and BOA, respectively. Therefore, the statistical analysis shows that the developed data privacy preservation model with suggested PS-BMFO performs better than the other conventional algorithms.},
  archive      = {J_JPDC},
  author       = {Saleh Muhammad Rubai},
  doi          = {10.1016/j.jpdc.2022.01.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {166-180},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hybrid heuristic-based key generation protocol for intelligent privacy preservation in cloud sector},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed collaboration and anti-interference optimization
in edge computing for IoT. <em>JPDC</em>, <em>163</em>, 156–165. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge computing (EC) systems for Internet of Things (IoT) can bring out low latency, high reliability, distributed intelligence, and network bandwidth savings to industrial real-time applications. However, limited computing and processing capabilities of edge devices remains to be difficult to meet complex data processing and artificial intelligence (AI) analysis requirements for diverse services. Besides, the scarcity of wireless spectrum resources in harsh industrial environment makes the interference between devices more serious. To address these challenges, this paper proposes an adaptive distributed collaborative anti-interference optimization scheme for IoT-edge system. Firstly, the EC system model is established, and the model of link failure probability is derived theoretically. Then, an Occupy-Interference Mitigation (O-IM) algorithm based on full-frequency multiplexing is proposed. The algorithm combines adaptive full-frequency multiplexing and interference mitigation to reduce the dependence of the reliable collaboration on bandwidth and signal-to-noise ratio (SNR). In addition, an anti-interference algorithm based on Interference Mitigation and inter-cellfrequency multiplexing Interference Avoidance (IM-IA) is proposed to balance bandwidth and SNR. This algorithm adopts interference cancellation scheme in the broadcasting phase, and adopts orthogonal frequency division in the collaboration phase. Extensive simulation results on Mininet platform verify that the proposals can obtain lower failure probability, and are less than traditional solutions in transmitting power, bandwidth, and SNR requirements. Moreover, the O-IM is suitable for low power scenarios, while the IM-IA is more suitable for high power case.},
  archive      = {J_JPDC},
  author       = {Yuhuai Peng and Chenlu Wang and Qiming Li and Lei Liu and Keping Yu},
  doi          = {10.1016/j.jpdc.2022.01.028},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {156-165},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed collaboration and anti-interference optimization in edge computing for IoT},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pipelined preconditioned conjugate gradient methods for real
and complex linear systems for distributed memory architectures.
<em>JPDC</em>, <em>163</em>, 147–155. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preconditioned Conjugate Gradient (PCG) is a popular method for solving large and sparse linear systems of equations. The performance of PCG at scale is affected due to the costly global synchronization steps that arise in dot-products on distributed memory systems. Pipelined PCG (PIPECG) removes the costly global synchronization steps from PCG by only executing a single non-blocking allreduce per iteration and overlapping it with independent computations. In our previous work, we have developed a novel pipelined PCG algorithm called PIPECG-OATI (One Allreduce per Two Iterations) for real linear systems which executes a single non-blocking allreduce per two iterations and provides a large overlap of global communication with independent computations at higher number of cores. Our method achieves this overlap by using iteration combination and by introducing new recurrence and non-recurrence computations. We implement optimizations in the PIPECG-OATI method to use cache memory efficiently. In this work, we present PIPECG-OATI-c method for linear systems with complex Hermitian positive definite and complex symmetric matrices. We compare our method with various pipelined CG methods on a variety of problems and demonstrate that our method always gives the least run times. We performed experiments with our method using 20M and 30M unknowns on up to 16K cores and obtained up to 2.48X performance improvement over PCG and 2.14X performance improvement over PIPECG methods. We also experimented with up to 1-billion unknowns on 16K cores, the largest problem size explored for the CG problem, to our knowledge, and obtained about 25\% improvement over PCG.},
  archive      = {J_JPDC},
  author       = {Manasi Tiwari and Sathish Vadhiyar},
  doi          = {10.1016/j.jpdc.2022.01.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {147-155},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Pipelined preconditioned conjugate gradient methods for real and complex linear systems for distributed memory architectures},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic load balancing with over decomposition in plasma
plume simulations. <em>JPDC</em>, <em>163</em>, 136–146. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An electric propulsion plasma plume simulation employs the Particle-in-Cell (PIC) model for heavy species (i.e. ions and neutral atoms) and yields a highly non-uniform particle distribution at steady state. For parallel simulations, partitioning the computational domain uniformly and evenly assigning the subdomains to MPI processes would never lead to properly balanced loads across MPI processes. For this reason, a patch-based dynamic load balancing method with over-decomposition and Hilbert space-filling curve has been implemented into the Thermophysics Universal Research Framework (TURF). The data transfer is accomplished by utilizing TURF&#39;s serialization and deserialization feature, in which the tree hierarchical object structure is copied into a contiguous memory block of data, and the tree hierarchy is reconstructed after the data transfer. This generalized dynamic load balancing approach allows for the same routine to be used for any models besides the PIC model in TURF (e.g. fluid, continuum kinetic, hybrid models, etc). This paper first introduces individual pieces of TURF that enable dynamic load balancing when they are combined. Then, the dynamic load balancing method is demonstrated in three different particle simulations, showing performance gains when the number of subdomains is larger than that of MPI processes. The benefit is even more notable in simulations with highly non-uniform particle distributions at steady state, commonly seen in plasma plume simulations.},
  archive      = {J_JPDC},
  author       = {Samuel J. Araki and Robert S. Martin},
  doi          = {10.1016/j.jpdc.2022.01.023},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {136-146},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dynamic load balancing with over decomposition in plasma plume simulations},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intermediate data placement and cache replacement strategy
under spark platform. <em>JPDC</em>, <em>163</em>, 114–135. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spark is widely used due to its high performance caching mechanism and high scalability, which still causes uneven workloads and produces useless intermediate caching results when faced with data-intensive applications. A data placement strategy based on an improved reservoir sampling algorithm is proposed to solve the problem of intermediate data tilt in the shuffle stage of Spark. Compared with the traditional sampling algorithm, the amount of intermediate data is accumulated while sampling. The data skew measurement model is used to classify data into skewed data, and non-skewed and coarse-grained, and fine-grained placement algorithms are designed. To further improve Spark&#39;s system memory utilization and cache hit rate , an adaptive cache replacement algorithm is proposed to maximize cache gain. We analyze the operational dependencies and propose a cache gain model. Compared with the traditional method, the two known and unknown job arrival rates are considered separately to obtain an online adaptive cache replacement strategy that maximizes cache gain. Experimental results show that our data placement strategy effectively reduces Spark applications&#39; execution time and improves the load balance of reduce tasks. Meanwhile, the proposed adaptive cache replacement strategy effectively reduces Spark&#39;s average completion time and improves the memory utilization and cache hit rate.},
  archive      = {J_JPDC},
  author       = {Chunlin Li and Yong Zhang and Youlong Luo},
  doi          = {10.1016/j.jpdc.2022.01.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {114-135},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intermediate data placement and cache replacement strategy under spark platform},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implementing three exchange read operations for distributed
atomic storage. <em>JPDC</em>, <em>163</em>, 97–113. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication latency typically dominates the performance of message-passing systems, and consequently defines the efficiency of operations of algorithms implementing atomic read/write objects in asynchronous, crash-prone, message-passing systems. Here latency is measured in terms of the number of communication exchanges (or simply exchanges ) involved in each operation. We present four algorithms, two for the single-writer/multiple-reader (SWMR) and two for the multi-writer/multiple-reader (MWMR) settings, that allow reads to take two or three exchanges, advancing the state-of-the-art in this area. Writes take the same number of exchanges as in prior works (i.e., two for SWMR and four for MWMR settings). In contrast with existing efficient implementations, ours come with no constraints on reader participation in both settings, and on the number of writers in the MWMR setting. Correctness of algorithms is rigorously argued. We conclude with an empirical study demonstrating the practicality of the algorithms, and identifying settings in which their read performance, is clearly superior compared to relevant algorithms.},
  archive      = {J_JPDC},
  author       = {Chryssis Georgiou and Theophanis Hadjistasi and Nicolas Nicolaou and Alexander A. Schwarzmann},
  doi          = {10.1016/j.jpdc.2022.01.024},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {97-113},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Implementing three exchange read operations for distributed atomic storage},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QoS provision for vehicle big data by parallel transmission
based on heterogeneous network characteristics prediction.
<em>JPDC</em>, <em>163</em>, 83–96. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipath parallel transmission has become an important research direction to improve big data transmission efficiency of connected vehicles. However, due to the heterogeneity and time-varying characteristics of parallel transmission paths, packets transmitted in parallel are usually out-of-order delivered to the destination, which greatly limits the throughput. To Lift the restriction of out-of-order delivery on the efficiency of big data transmission, this paper proposes a packet-granular real-time shortest delay scheduling scheme for multipath transmission based on path characteristics prediction. The scheme first clusters and models the heterogeneous network , which greatly reduces the complexity of the network. Subsequently, a prediction algorithm that can quickly converge to real-time delay is proposed. Then the details of the scheduling scheme are introduced by modules, and the bandwidth aggregation efficiency close to the theoretical upper limit is proved through simulation. Finally, we summarize the applicable scenarios and future work of the scheme.},
  archive      = {J_JPDC},
  author       = {Wenxuan Qiao and Ping Dong and Xiaojiang Du and Yuyang Zhang and Hongke Zhang and Mohsen Guizani},
  doi          = {10.1016/j.jpdc.2022.01.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-96},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {QoS provision for vehicle big data by parallel transmission based on heterogeneous network characteristics prediction},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based automated and robust cyber security
management. <em>JPDC</em>, <em>163</em>, 62–82. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study on the problem of automated and robust Cyber Security Management (CSM). We exemplify the problem by investigating how CSM should respond to the discovery of cyber intelligence that identifies new attackers , victims , or defense capabilities . Given the complexity of CSM, we divide it into three classes, referred to as Network-centric (N-CSM), Tools-centric (T-CSM) and Application-centric (A-CSM). These lead to a range of functions for examining whether, and to what extent, a network has been compromised. Moreover, we propose to incorporate blockchain (via Hyperledger Fabric) to build a decentralized CSM system, dubbed B2CSM, that ensures the retrieval of valid invocation results for CSM purposes. We also integrate B2CSM with a decentralized storage network (DSN), instantiated by InterPlanetary File System (IPFS), to reduce on-chain storage costs without hindering its robustness. We present the design and implementation of the prototype B2CSM system. Experiments with real-world datasets show that the CSM solutions and system are effective and efficient.},
  archive      = {J_JPDC},
  author       = {Songlin He and Eric Ficke and Mir Mehedi Ahsan Pritom and Huashan Chen and Qiang Tang and Qian Chen and Marcus Pendleton and Laurent Njilla and Shouhuai Xu},
  doi          = {10.1016/j.jpdc.2022.01.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {62-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-based automated and robust cyber security management},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping series-parallel streaming applications on
hierarchical platforms with reliability and energy constraints.
<em>JPDC</em>, <em>163</em>, 45–61. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming applications come from various application fields such as physics, where data is continuously generated and must be processed on the fly. Typical streaming applications have a series-parallel dependence graph , and they are processed on a hierarchical failure-prone platform, as for instance in miniaturized satellites. The goal is to minimize the energy consumed when processing each data set, while ensuring real-time constraints in terms of processing time. Dynamic voltage and frequency scaling (DVFS) is used to reduce the energy consumption, and we ensure a reliable execution by either executing a task at maximum speed, or by triplicating it, so that the time to execute a data set without failure is bounded. We propose a structure rule to partition the series-parallel applications and map the application onto the platform, and we prove that the optimization problem is NP-complete. We design a dynamic-programming algorithm for the special case of linear chains, which is optimal for a special class of schedules. Furthermore, this algorithm provides an interesting heuristic and a building block for designing heuristics for the general case. The heuristics are compared to a baseline solution, where each task is executed at maximum speed. Simulations on realistic settings demonstrate the good performance of the proposed heuristics; in particular, significant energy savings can be obtained.},
  archive      = {J_JPDC},
  author       = {Changjiang Gou and Anne Benoit and Mingsong Chen and Loris Marchal and Tongquan Wei},
  doi          = {10.1016/j.jpdc.2022.01.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {45-61},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Mapping series-parallel streaming applications on hierarchical platforms with reliability and energy constraints},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive parallel and distributed simulation of complex
networks. <em>JPDC</em>, <em>163</em>, 30–44. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex networks are an important methodology to model several (if not all) aspects of the real world, in which multiple entities interact, in some way. While many aspects related to such interactions can be investigated by looking at the general mathematical metrics of the networks, an alternative approach lies in the simulation of some application protocol on top of (large scale) complex networks. In this paper, we present a study on this intricate problem. The complexity of the simulation is due to the need to model all the interactions among network nodes. We focus on discrete-event simulation, a simulation methodology that enables both sequential (i.e. monolithic) and Parallel And Distributed Simulation (i.e. PADS) approaches. We discuss the performance and scalability requirements that the simulator should have. We also introduce a case study based on the agent-based simulation of gossip dissemination on top of a complex network. To demonstrate the viability of this simulation technique, we focus on a tool we built to simulate complex networks. The tool exploits adaptive partitioning mechanisms, which are essential to reduce the communication overhead in the PADS. An experimental evaluation has been conducted using different network topologies and simulator setups. Results demonstrate the feasibility of the approach to simulate complex networks.},
  archive      = {J_JPDC},
  author       = {Gabriele D&#39;Angelo and Stefano Ferretti},
  doi          = {10.1016/j.jpdc.2022.01.022},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {30-44},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive parallel and distributed simulation of complex networks},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast and concise parallel implementation of the 8x8 2D
forward and inverse DCTs using halide. <em>JPDC</em>, <em>163</em>,
20–29. (<a href="https://doi.org/10.1016/j.jpdc.2022.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Discrete Cosine Transform (DCT) is commonly used for image and video coding and very efficient implementations of the forward and inverse transforms are of great importance. The popular libjpeg-turbo library contains handwritten, highly-optimised assembly language DCT implementations utilizing SIMD instruction sets for a variety of architectures. We present an alternative approach, implementing the 8x8 IDCT and FDCT written in the functional image processing language Halide. We show how less than 200 lines of Halide can replace over 20,000 lines of code the libjpeg-turbo library to perform JPEG encoding and decoding. The Halide implementation is compared for ARMv8 NEON and x86-64 SIMD extensions and shows a 5–25 percent performance improvement over the SIMD code in libjpeg-turbo for decoding and a 10–40 percent improvement for encoding. The Halide code is significantly easier to maintain and port to new architectures than the existing code.},
  archive      = {J_JPDC},
  author       = {Martin Johnson and Daniel Playne},
  doi          = {10.1016/j.jpdc.2022.01.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {20-29},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A fast and concise parallel implementation of the 8x8 2D forward and inverse DCTs using halide},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of the leaky integrate-and-fire neuron model for
GPU implementation. <em>JPDC</em>, <em>163</em>, 1–19. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how neurons perform, when they are organized in interacting networks, is a key to understanding how the brain performs complex functions. Different models that approximate the behavior of interconnected neurons have been proposed in the literature. Implementing these models to simulate neuron behavior at an appropriately detailed level to observe collective phenomena is computationally intensive. In this study we analyze the coupled Leaky Integrate-and-Fire model and report on the issues that affect performance when the model is implemented on a GPU . We conclude that the problem is heavily memory-bound . Advances in memory technology at the hardware level seem to be the deciding factor to achieve better performance on the GPU . Our results show that using an NVidia K40 GPU a modest 2x speedup can be achieved compared to a parallel implementation running on a modern multi-core CPU. However, a substantial speedup of 11.1x can be achieved using an NVidia V100 GPU, mainly due to the improvements in its memory subsystem .},
  archive      = {J_JPDC},
  author       = {Ioannis E. Venetis and Astero Provata},
  doi          = {10.1016/j.jpdc.2022.01.021},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-19},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Analysis of the leaky integrate-and-fire neuron model for GPU implementation},
  volume       = {163},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DriverRep: Driver identification through driving behavior
embeddings. <em>JPDC</em>, <em>162</em>, 105–117. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver identification has emerged as an active field of study to further personalize the integrated advanced driver-assistance systems into intelligent vehicles , provide security and safety for ride-hailing services, and prevent auto theft. Several studies, within recent years, have investigated non-intrusive identification approaches, focusing on driving behavior analysis to characterize drivers&#39; driving behavior, in which a considerable volume of labeled data is required. This paper develops a deep learning architecture, namely DriverRep, to extract the latent representations associated with each individual, called driver embeddings. These embeddings represent the unique driving characteristics of drivers. To this aim, we introduce a fully unsupervised triplet loss that selects triplet samples from data in an unsupervised manner and extracts the embeddings using our proposed stacked encoder architecture. Dilated causal convolutions are used to make residual blocks of the encoder. To perform the task of driver identification, we leverage the classification accuracy of SVM on top of the obtained driver embeddings. The evaluation results over two datasets, each of which contains ten drivers, reveal that the DriverRep can successfully capture the underlying features within the data and outperform benchmark driver identification schemes, obtaining an average accuracy of 94.7\% and 96\% for two-way and three-way identification, respectively. We also investigate the ability of the DriverRep in handling sparsely labeled data. The results represent substantial improvements in comparison with the supervised approaches when applied to highly sparsely labeled data.},
  archive      = {J_JPDC},
  author       = {Mozhgan Nasr Azadani and Azzedine Boukerche},
  doi          = {10.1016/j.jpdc.2022.01.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {105-117},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DriverRep: Driver identification through driving behavior embeddings},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning and the internet of things security:
Solutions and open challenges. <em>JPDC</em>, <em>162</em>, 89–104. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is a pervasively-used technology for the last few years. IoT technologies are also responsible for intensifying various everyday smart applications improving the standard of living. However, the inter-crossing of IoT systems and the multi-directional elements responsible for these systems&#39; placement have raised new safety concerns. They generate and share a massive amount of sensitive data. Unfortunately, both the data and the devices are susceptible to many privacy and security challenges. Much research has been done to secure these infrastructures; however, Machine Learning (ML), among others, provides higher accuracy. This survey covers the major security issues and open challenges encountered by IoT infrastructures. It also encompasses an in-depth study and analysis of ML-based state-of-the-art solutions used in securing such domains. The security challenges and requirements in IoT-based systems have been highlighted, along with a discussion on how ML supports security measures in the said domain. Furthermore, the challenges associated with ML-based security solutions have been identified concerning IoT. An analysis of prevailing ML security techniques&#39; constraints is also contemplated.},
  archive      = {J_JPDC},
  author       = {Umer Farooq and Noshina Tariq and Muhammad Asim and Thar Baker and Ahmed Al-Shamma&#39;a},
  doi          = {10.1016/j.jpdc.2022.01.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {89-104},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Machine learning and the internet of things security: Solutions and open challenges},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive scheduling of multiprogrammed
dynamic-multithreading applications. <em>JPDC</em>, <em>162</em>, 76–88.
(<a href="https://doi.org/10.1016/j.jpdc.2022.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern parallel platforms, such as clouds or servers, are often shared among many different jobs. However, existing parallel programming runtime systems are designed and optimized for running a single parallel job, so it is generally hard to directly use them to schedule multiple parallel jobs without incurring high overhead and inefficiency. In this work, we develop AMCilk (Adaptive Multiprogrammed Cilk), a novel runtime system framework, designed to support multiprogrammed parallel workloads. AMCilk has client-server architecture where users can dynamically submit parallel jobs to the system. AMCilk has a single runtime system that runs these jobs while dynamically reallocating cores, last-level cache, and memory bandwidth among these jobs according to the scheduling policy. AMCilk exposes the interface to the system designer, which allows the designer to easily build different scheduling policies meeting the requirements of various application scenarios and performance metrics, while AMCilk transparently (to designers) enforces the scheduling policy. AMCilk also enables its use in cloud environment where other processes may be sharing the system with AMCilk. In this scenario, an external scheduler can change the resource availability for AMCilk and AMCilk seamlessly adapts to these changes. The primary feature of AMCilk is the low-overhead and responsive preemption mechanism that allows fast reallocation of cores between jobs. Our empirical evaluation indicates that AMCilk incurs small overheads and provides significant benefits on application-specific criteria for a set of 4 practical applications due to its fast and low-overhead core reallocation mechanism.},
  archive      = {J_JPDC},
  author       = {Zhe Wang and Chen Xu and Kunal Agrawal and Jing Li},
  doi          = {10.1016/j.jpdc.2022.01.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {76-88},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive scheduling of multiprogrammed dynamic-multithreading applications},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sboing4Real: A real-time crowdsensing-based traffic
management system. <em>JPDC</em>, <em>162</em>, 59–75. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes the architecture of the back-end engine of a real-time traffic data processing and satellite navigation system. The role of the engine is to process real-time feedback, such as speed and travel time, provided by in-vehicle devices and derive real-time reports and traffic predictions through leveraging historical data as well. We present the main building blocks and the versatile set of data sources and processing platforms that need to be combined together to form a fully-functional and scalable solution. We also present performance results focusing on meeting system requirements while keeping the need for computing resources low. The lessons and results presented are of value to additional real-time applications that rely on both recent and historical data. Finally, we discuss the application of the aforementioned solution to a successful pilot study, where the full system was deployed and processed data from 800 taxis for a period of 3 months.},
  archive      = {J_JPDC},
  author       = {Theodoros Toliopoulos and Nikodimos Nikolaidis and Anna-Valentini Michailidou and Andreas Seitaridis and Theodoros Nestoridis and Chrysa Oikonomou and Anastasios Temperekidis and Fotios Gioulekas and Anastasios Gounaris and Nick Bassiliades and Panagiotis Katsaros and Apostolos Georgiadis and Fotis K. Liotopoulos},
  doi          = {10.1016/j.jpdc.2022.01.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {59-75},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Sboing4Real: A real-time crowdsensing-based traffic management system},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic search space design for energy-efficient static
scheduling of moldable tasks. <em>JPDC</em>, <em>162</em>, 44–58. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static scheduling of independent, moldable tasks on parallel machines with frequency scaling comprises decisions on core allocation, assignment, frequency scaling and ordering, to meet a deadline and minimize energy consumption. Constraining some of these decisions reduces the solution space, i.e. may increase energy consumption, but may also open the path to new, near-optimal approaches. We investigate how constraints of different steps influence energy consumption, starting with an unrestricted scheduler for moldable tasks. The constraints are partly derived from existing schedulers, but also generalized in a systematic way. We present integer linear programs for all scheduling variants. We compare energy consumption of schedules for a benchmark suite of synthetic task sets of different sizes and for task sets derived from real applications. In addition, we check how close the results are to the optimum results when the ILP solver meets a timeout . Our results indicate that constraints on task execution order, which avoid explicit representation of task order in ILPs, are mostly responsible for near-optimal energy consumption among large task sets. Furthermore, we find that for all steps except allocation, non-optimal fast heuristics can be used without sacrificing too much energy for the resulting schedule. Finally, we can show that an ILP for a new scheduler, for which also a heuristic version exists, is comparable in quality to more complicated schedulers.},
  archive      = {J_JPDC},
  author       = {Jörg Keller and Sebastian Litzinger},
  doi          = {10.1016/j.jpdc.2022.01.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {44-58},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Systematic search space design for energy-efficient static scheduling of moldable tasks},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Core-aware combining: Accelerating critical section
execution on heterogeneous multi-core systems via combining
synchronization. <em>JPDC</em>, <em>162</em>, 27–43. (<a
href="https://doi.org/10.1016/j.jpdc.2022.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous multi-core systems, performance differences of the cores can affect lock synchronization, where the high-performance cores have to wait for slower cores to complete critical section execution. To better utilize the high-performance cores, we can offload critical section execution to high-performance cores. Since combining synchronization has the potential to transfer critical section execution to the combiner, this paper presents a core-aware combining approach for heterogeneous multi-core processors to accelerate critical section execution. In combining synchronization, one competing thread will become the combiner to help complete pending requests. It typically provides better performance than conventional locks on multi-core systems. To enable transferring critical section executions to a more efficient core, we implement the ideas of core efficiency-based selective lock ownership transfer and the dynamic helping quota in four combining implementations. On an aarch64 heterogeneous machine and an x86 asymmetric machine, we ran several micro-benchmarks and workloads to evaluate the performance of our core-aware implementations. The results show that core-aware combining implementations accelerate critical section execution and achieve better throughput than the original combining implementations.},
  archive      = {J_JPDC},
  author       = {Xiangzhen Ouyang and Yian Zhu},
  doi          = {10.1016/j.jpdc.2022.01.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {27-43},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Core-aware combining: Accelerating critical section execution on heterogeneous multi-core systems via combining synchronization},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Component diagnosability in terms of component connectivity
of hypercube-based compound networks. <em>JPDC</em>, <em>162</em>,
17–26. (<a href="https://doi.org/10.1016/j.jpdc.2021.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the invulnerability of multiprocessor systems against malicious attacks has been regarded as one of the important issues in network science and big data era. Thus, in order to firmly characterize the robustness of systems, several variants of classic connectivity have been proposed so far. The component connectivity is a significant metric in evaluating the robustness and fault tolerability of interconnection network . For an interconnection network G and a positive integer h , the ( h + 1 ) (h+1) -component connectivity of G , denoted c κ h + 1 ( G ) cκh+1(G) , is the cardinality of a minimum vertex cut F such that G − F G−F has at least h + 1 h+1 connected components. Based on component connectivity, component diagnosability has been proposed to measure the self-diagnosis capability of multiprocessor systems . In this paper, we suggest some characterizations of the ( h + 1 ) (h+1) -component connectivity of a class of regular networks under some restrictions. Furthermore, we establish the relationship between component connectivity and component diagnosability of one class of networks. As by-products, we present the ( h + 1 ) (h+1) -component diagnosability of the state-of-the-art compound networks based on hypercube , such as bicube network, generalized exchanged hypercube , hierarchical hypercube, half-hypercube, and so on.},
  archive      = {J_JPDC},
  author       = {Jiafei Liu and Shuming Zhou and Dajin Wang and Hong Zhang},
  doi          = {10.1016/j.jpdc.2021.12.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-26},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Component diagnosability in terms of component connectivity of hypercube-based compound networks},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reachability in parallel programs is polynomial in the
number of threads. <em>JPDC</em>, <em>162</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reachability in parallel finite-state programs equipped with interleaving semantics is an inherently difficult, important problem. Its complexity in the number of threads n , while keeping the thread-local–memory size and the shared-memory size bounded by constants, has been explored only poorly. We significantly narrow this gap by measuring: (i) the diameter , i.e., the longest finite distance realizable in the transition graph of the program, (ii) the local diameter , i.e., the maximum finite distance from any program state to any thread-local state, and (iii) the computational complexity of finding bugs. We prove that all these are majorized by polynomials in n and, in certain cases, by linear, logarithmic, or even constant functions in n ; we make the bounds explicit whenever possible. Our results shed new light on the widely expressed claim that one of the major obstacles to analyzing parallel programs is the exponential state explosion in the number of threads.},
  archive      = {J_JPDC},
  author       = {Alexander Malkis},
  doi          = {10.1016/j.jpdc.2021.11.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reachability in parallel programs is polynomial in the number of threads},
  volume       = {162},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architecting a congestion pre-avoidance and load-balanced
wireless network-on-chip. <em>JPDC</em>, <em>161</em>, 143–154. (<a
href="https://doi.org/10.1016/j.jpdc.2021.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The communication performance over conventional long-distant routers cannot satisfy the requirements of future multi-core systems. Wireless Network-on-Chip (WiNoC) architecture with CMOS compatible transceivers is utilized to obtain significant improvement in on-chip data transfer for multi-core systems. The wireless routers (WRs) in WiNoC architecture provide wireless shortcuts to alleviate the latency of multi-hop communications. Despite the additional bandwidth of wireless shortcuts, the WRs are prone to congestion due to the limited number of wireless channels and the shared use of these channels under unbalanced load. On the other hand, network performance will be severely degraded when the presence of head-of-line (HOL) blocking. In this paper, we establish a congestion pre-avoidance and load-balanced wireless network-on-chip architecture. To implement such architecture in our network, firstly, we propose an dynamic XY-YX routing algorithm detour WR to pre-avoid the additional traffic flow of the wireless router; Secondly, the virtual output queue scheme is adopted to handle HOL blocking, and on this basis we design a wireless router micro-architecture; Finally, a threshold-based load-balanced mechanism is designed, which uses the number of buffered flit as a guideline to avoid unbalance load. Through system-level simulations, the results demonstrate that the proposed WiNoC architecture can mitigate negative effect of congestion in WR. And with appropriate hardware overhead, network transmission latency and network throughput are significantly improved.},
  archive      = {J_JPDC},
  author       = {Chenglong Sun and Yiming Ouyang and Huaguo Liang},
  doi          = {10.1016/j.jpdc.2021.12.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {143-154},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Architecting a congestion pre-avoidance and load-balanced wireless network-on-chip},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy aware fuzzy approach for placement and consolidation
in cloud data centers. <em>JPDC</em>, <em>161</em>, 130–142. (<a
href="https://doi.org/10.1016/j.jpdc.2021.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Network Function (VNF) is one of the pillars of a Cloud network that separates network functions and their dedicated hardware devices, such as routers, firewalls, and load balancers, to host their services on virtual machines. The VNF is responsible for network services that run on virtual machines and can connect each of them alone or organize themselves into a single enclosure to use all the resources available in that enclosure. This flexibility allows physical and virtual resources to be used in a way that ensures control over power consumption, balance in resource use, and minimizing costs and latency. In order to consolidate VNF groups into a minimum number of Virtual Machine (VM) with estimation of the association relation to a measure of confidence under the context of possibility theory, we propose a new Fuzzy-FCA approach for VNF placement based on Formal Concept Analysis (FCA) and fuzzy logic in mixed environment based on cloud data centers and Multiple access Edge Computing (MEC) architecture. Thus, the inclusion of this architecture in the cloud environment ensures the distribution of compute resources to the end user in order to reduce end-to-end latency. To confirm the effectiveness of our solution, we compared it to one of the best algorithms studied in the literature, namely the MultiSwarm algorithm. The results of the series of experiments carried out show the feasibility and efficiency of our algorithm. Indeed, the harvested results confirm the capability of maximizing and balancing the use of resources, of minimizing the latency and the cost of energy consumption. The performance of our solution in terms of average latency represents 16\%, a slight increase compared to MultiSwarm, and an average gain, in runtime, of 49\%, compared to the same algorithm.},
  archive      = {J_JPDC},
  author       = {Wided Khemili and Jalel Eddine Hajlaoui and Mohamed Nazih Omri},
  doi          = {10.1016/j.jpdc.2021.12.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {130-142},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy aware fuzzy approach for placement and consolidation in cloud data centers},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MiCS-p: Parallel mutual-information computation of big
categorical data on spark. <em>JPDC</em>, <em>161</em>, 118–129. (<a
href="https://doi.org/10.1016/j.jpdc.2021.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual information can effectively measure the correlation between categorical attributes. However, it is found to be quite computationally intensive and time consuming process for enormous size and different distribution data sets. It involves steps for computation of marginal entropies, probability distribution and so on. Spark is a fast, general-purpose parallel framework designed specifically for large-scale data processing. Main motivation of this paper is to provide an intelligent method for parallel mutual information calculation based on Spark computing environment with maintaining the synchronization between different computing nodes. Proposed method named MiCS-P has been able to execute with different number of computing nodes, and gives significant speedup working with different dimensions and sizes of data sets. The MiCS-P algorithm adopts column-wise transformation scheme, which is conducive to the calculation of mutual information between a large number of feature pairs. And to alleviate imbalanced load causing long execution times, we implement a two-phase virtual partitioning scheme running on Spark.},
  archive      = {J_JPDC},
  author       = {Junli Li and Chaowei Zhang and Jifu Zhang and Xiao Qin and Lihua Hu},
  doi          = {10.1016/j.jpdc.2021.12.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {118-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MiCS-P: Parallel mutual-information computation of big categorical data on spark},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dispersion of mobile robots using global communication.
<em>JPDC</em>, <em>161</em>, 100–117. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dispersion problem on graphs asks k ≤ n k≤n robots placed initially arbitrarily on the nodes of an n -node anonymous graph to reposition autonomously to reach a configuration in which each robot is on a distinct node of the graph. This problem is of significant interest due to its relationship to other fundamental robot coordination problems, such as exploration, scattering, load balancing, and relocation of self-driven electric cars (robots) to recharge stations (nodes). In this paper, we consider dispersion using the global communication model where a robot can communicate with any other robot in the graph (but the graph is unknown to robots). We provide two novel deterministic algorithms for arbitrary graphs in a synchronous setting where all robots perform their actions in every time step. Our first algorithm is based on a DFS traversal and guarantees (i) O ( k Δ ) O(kΔ) steps runtime using O ( log ⁡ ( k + Δ ) ) ) O(log⁡(k+Δ))) bits at each robot and (ii) O ( min ⁡ ( m , k Δ ) ) O(min⁡(m,kΔ)) steps runtime using O ( Δ + log ⁡ k ) O(Δ+log⁡k) bits at each robot, where m is the number of edges and Δ is the maximum degree of the graph. The second algorithm is based on a BFS traversal and guarantees O ( ( D + k ) Δ ( D + Δ ) ) O((D+k)Δ(D+Δ)) steps runtime using O ( log ⁡ D + Δ log ⁡ k ) ) O(log⁡D+Δlog⁡k)) bits at each robot, where D is the diameter of the graph. Our results complement the existing results established using the local communication model where a robot can communication only with other robots present at the same node.},
  archive      = {J_JPDC},
  author       = {Ajay D. Kshemkalyani and Anisur Rahaman Molla and Gokarna Sharma},
  doi          = {10.1016/j.jpdc.2021.11.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {100-117},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dispersion of mobile robots using global communication},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). XML2HBase: Storing and querying large collections of XML
documents using a NoSQL database system. <em>JPDC</em>, <em>161</em>,
83–99. (<a href="https://doi.org/10.1016/j.jpdc.2021.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many big data applications such as smart transportation, healthcare, and e-commerce need to store and query large collections of small XML documents, which has become a fundamental problem. However, existing solutions are inadequate to deliver satisfactory query performance in such circumstances. In this paper, we propose a framework named XML2HBase to address this problem using HBase, a widely deployed NoSQL database. Within this framework, we design a novel encoding scheme called Pathed-Dewey Order and a two-layer mapping method to store XML documents in HBase tables. XML queries, which are represented as XPath expressions, are evaluated through their translation into queries over HBase tables. Based on an in-depth analysis of the characteristics of the proposed approach, we design and integrate four optimization strategies to reduce storage space and query response time. Extensive experiments on two well-known XML benchmarks demonstrate the superior performance of XML2HBase over three state-of-the-art methods.},
  archive      = {J_JPDC},
  author       = {Liang Bao and Jin Yang and Chase Q. Wu and Haiyang Qi and Xin Zhang and Shunda Cai},
  doi          = {10.1016/j.jpdc.2021.11.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-99},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {XML2HBase: Storing and querying large collections of XML documents using a NoSQL database system},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FMapper: Scalable read mapper based on succinct hash index
on SunWay TaihuLight. <em>JPDC</em>, <em>161</em>, 72–82. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important application in bioinformatics is read mapping. With the rapidly increasing number of reads produced by next-generation sequencing (NGS) technology, there is a need for fast and efficient high-throughput read mappers. In this paper, we present FMapper – a highly scalable read mapper on the TaihuLight supercomputer optimized for its fourth-generation ShenWei many-core architecture (SW26010). In order to fully exploit the computational power of the SW26010, we employ dynamic scheduling of tasks, asynchronous I/O and data transfers and implement a vectorized version of the banded Myers algorithm tailored to the 256 bit vector registers of the SW26010. Our performance evaluation demonstrates that FMapper using all 4 compute groups of a single SW26010 processor outperforms S-Aligner on the same hardware as well as RazerS3, Hobbes3, Minimap2 and BWA running on a 4-core Xeon W-2123v3 CPU and achieves speedups of 4.7, 24.8, 2.4, 4.6 and 14.7 respectively. Using several optimizations, we achieve a speedup of 6 compared to the naïve implementation on one compute group of an SW26010 processor and a strong scaling efficiency of 65\% on 512 compute groups.},
  archive      = {J_JPDC},
  author       = {Kai Xu and Xiaohui Duan and André Müller and Robin Kobus and Bertil Schmidt and Weiguo Liu},
  doi          = {10.1016/j.jpdc.2021.11.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {72-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FMapper: Scalable read mapper based on succinct hash index on SunWay TaihuLight},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hardware/software co-design methodology for in-memory
processors. <em>JPDC</em>, <em>161</em>, 63–71. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bottleneck between the processor and memory is the most significant barrier to the ongoing development of efficient processing systems. Therefore, a research effort begun to shift from processor-centric architectures to memory-centric architectures. Various in-memory processor architectures have been proposed to break this barrier to pave the way for ever-demanding memory-bound applications. Associative in-memory processing is a successful candidate for truly in-memory computing, in which processor and memory are combined in the same location to eliminate the expensive data access costs. The architecture exhibits an unmatched advantage for data-intensive applications due to its memory-centric design principles. On the other hand, this advantage can be revealed fully by an efficient design methodology. This study puts further progressive effort by proposing a hardware/software design methodology for associative in-memory processors. The methodology aims to decrease energy consumption and area requirement of the processor architecture specifically programmed to perform a given task. According to the evaluation of nine different benchmarks, such as fast Fourier transform and multiply-accumulate, the proposed design flow accomplishes an average ∼7\% reduction in memory area and ∼18\% savings in total energy consumption .},
  archive      = {J_JPDC},
  author       = {Hasan Erdem Yantır and Ahmed M. Eltawil and Khaled N. Salama},
  doi          = {10.1016/j.jpdc.2021.10.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {63-71},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A hardware/software co-design methodology for in-memory processors},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative energy transactions in micro and utility grids
integrating energy storage systems. <em>JPDC</em>, <em>161</em>, 48–62.
(<a href="https://doi.org/10.1016/j.jpdc.2021.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, Micro Grids (MGs) have gained much popularity due to two-way communication in the power network with special emphasis on Distributed Energy Resources (DERs), which comprise of both Renewable Energy Sources (RESs) and Nonrenewable Energy Sources (NRESs). Currently, the main focus of researchers is to deal with the intermittent nature of RESs, which lead to the fluctuations in power production and dispatch. In this paper, direct energy trading among MGs is considered as an assuring solution for improving the grid stability, reducing power line losses and minimizing energy trading cost. The focus of this work is on energy transactions amongst multiple MGs within the same geographic region. In the proposed method, coalitions among MGs are made on the basis of the distance between them for energy transaction. Furthermore, an Energy Transaction Algorithm (ETA) is proposed for energy trading among MGs in the coalitions. Energy Storage System (ESS) is also integrated, which stores energy when an MG has surplus energy and utilizes the stored energy if it becomes energy deficient. Simulations are performed and the results demonstrate that energy transactions among MGs in the proposed method minimize the power line losses and energy trading cost up to 34.6\% and 14\%, respectively as compared to the existing method.},
  archive      = {J_JPDC},
  author       = {Muhammad Usman Khalid and Nadeem Javaid and Ahmad Almogren and Abrar Ahmed and Sardar Muhammad Gulfam and Ayman Radwan},
  doi          = {10.1016/j.jpdc.2021.11.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {48-62},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cooperative energy transactions in micro and utility grids integrating energy storage systems},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication lower-bounds for distributed-memory
computations for mass spectrometry based omics data. <em>JPDC</em>,
<em>161</em>, 37–47. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass spectrometry (MS) based omics data analysis require significant time and resources. To date, few parallel algorithms have been proposed for deducing peptides from mass spectrometry-based data. However, these parallel algorithms were designed, and developed when the amount of data that needed to be processed was smaller in scale. In this paper, we prove that the communication bound that is reached by the existing parallel algorithms is Ω ( m n + 2 r q p ) Ω(mn+2rqp) , where m and n are the dimensions of the theoretical database matrix, q and r are dimensions of spectra, and p is the number of processors. We further prove that communication-optimal strategy with fast-memory M = m n + 2 q r p M=mn+2qrp can achieve Ω ( 2 m n q p ) Ω(2mnqp) but is not achieved by any existing parallel proteomics algorithms till date. To validate our claim, we performed a meta-analysis of published parallel algorithms, and their performance results. We show that sub-optimal speedups with increasing number of processors is a direct consequence of not achieving the communication lower-bounds. We further validate our claim by performing experiments which demonstrate the communication bounds that are proved in this paper. Consequently, we assert that next-generation of provable , and demonstrated superior parallel algorithms are urgently needed for MS based large systems-biology studies especially for meta-proteomics, proteogenomic, microbiome, and proteomics for non-model organisms. Our hope is that this paper will excite the parallel computing community to further investigate parallel algorithms for highly influential MS based omics problems.},
  archive      = {J_JPDC},
  author       = {Fahad Saeed and Muhammad Haseeb and S.S. Iyengar},
  doi          = {10.1016/j.jpdc.2021.11.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {37-47},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Communication lower-bounds for distributed-memory computations for mass spectrometry based omics data},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective biogeography-based optimization and
reinforcement learning hybridization for network-on chip reliability
improvement. <em>JPDC</em>, <em>161</em>, 20–36. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability is increasingly a major concern in network-on-a-chip (NoC) design, alongside increased performance demands from new applications and the need for continued miniaturization of silicon technology. In this article, we look at the task migration mechanism, used to recover from permanent processing element (PE) failures in NoCs, by remapping tasks performed on faulty cores to spare ones. An innovative reliability-aware task mapping technique is presented, based on a hybridization between Multi-Objective Optimization (MOO) and Reinforcement Learning (RL). It takes place in two steps. In the first, a set of optimal remapping solutions for different failure scenarios is generated at design-time, using a Biogeography-Based Multi-Objective Optimization algorithm , while considering communication energy and migration costs. In the second step, an artificial neural network agent is trained to select the best remapping solution, from those generated at design-time, to recover from execution failures at run-time. Experiments were carried out to evaluate our technique for different sizes of networks and on different benchmarks. The results obtained show that the technique based on the hybridization MOO_RL brings a great improvement in the reliability of the NoC and achieves a good compromise between reliability and performance. It also guarantees a reduction of the overhead caused by the storage space of the remapping solutions, compared to the existing solutions.},
  archive      = {J_JPDC},
  author       = {Nassima Kadri and Mouloud Koudil},
  doi          = {10.1016/j.jpdc.2021.11.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {20-36},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multi-objective biogeography-based optimization and reinforcement learning hybridization for network-on chip reliability improvement},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quorums over codes. <em>JPDC</em>, <em>161</em>, 1–19. (<a
href="https://doi.org/10.1016/j.jpdc.2021.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the design and analysis of quorum systems over erasure coded warm data (with low frequency of writes and accesses in general) to guarantee sequential consistency under a fail-stop model while supporting atomic read-modify-write operations by multiple clients. We propose a definition of asymmetric quorum systems that suit the framework of coded data by explicitly exploiting the structural properties of code and instantiate it over distinct families of coding strategies: maximum distance separable (MDS) codes and codes with locality, and we indicate a mechanism for synchronizing stale nodes using differential updates, which again exploits the code structures. The proposed quorum system&#39;s behavior is analyzed theoretically, exploring several aspects: viability of quorums under node unavailability; contention of resources between read and write operations; and quorum load. We complement these theoretical exploration with simulation based experiments to quantify the behavior of the proposed mechanism. The overall study demonstrates the feasibility and practicality of quorums over codes under practicable assumptions for achieving a stringent form of consistency, specifically, sequential consistency, while the stored data is being mutated by potentially multiple processes that might read and then modify the existing data. We achieve this in-place, without having to resort to store multiple versions of the data.},
  archive      = {J_JPDC},
  author       = {Anwitaman Datta and Frédérique Oggier},
  doi          = {10.1016/j.jpdc.2021.11.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-19},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Quorums over codes},
  volume       = {161},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realizing dynamic resource orchestration on cloud systems in
the cloud-to-edge continuum. <em>JPDC</em>, <em>160</em>, 100–109. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has been widely utilized to handle the huge volume of data from many cutting-edge research areas such as Big Data and Internet of Things (IoT). The fast growing of edge devices makes it difficult for cloud systems to process all data and jobs originating from edge devices, which leads to the development of edge computing by completing jobs on edges instead of clouds. Unfortunately, edge devices generally possess only limited computing power. Therefore, jobs demanding heavy computation under strict time constraints could have more difficulties to successfully complete their work on edges than on clouds in the Cloud-to-Edge continuum. If cloud systems could dynamically orchestrate cloud resources to expedite the execution of those jobs, not only their timely execution could be assured, also the loading of edge devices could be reduced. The Apache Hadoop is considered one of the most popular cloud systems in industry and academia. However, it does not support dynamic resource allocation . Previously we proposed and implemented a new model which can dynamically adjust the computing resources assigned to given jobs in the Hadoop cloud system to speed up their execution. Like other computer software, cloud systems completely rely on their underlying operating systems to access hardware components such as CPUs and hard drives. In this paper, we report our efforts to improve our model to collaborate with the Linux operating system to accelerate the execution of jobs with high priority to a greater extent. Compared with what our original model achieved, experiments show that our ameliorated model could further quicken the execution of prioritized jobs in Hadoop by up to around 21\%. As a result, jobs from edges that require substantial computing resources promptly could have better chances to get accomplished on cloud systems.},
  archive      = {J_JPDC},
  author       = {Tsozen Yeh and Shengchieh Yu},
  doi          = {10.1016/j.jpdc.2021.10.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {100-109},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Realizing dynamic resource orchestration on cloud systems in the cloud-to-edge continuum},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards soft real-time fault diagnosis for edge devices in
industrial IoT using deep domain adaptation training strategy.
<em>JPDC</em>, <em>160</em>, 90–99. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and industrial internet of things (IIoT) have been rejuvenating the fault diagnosis systems in Industry 4.0 for avoiding major financial losses caused by faults in rotating machines. Meanwhile, the diagnostic systems are provided with a number of sensory inputs that introduce variations in input space which causes difficulty for the algorithms in edge devices. This issue is generally dealt with bi-view cross-domain learning approach. We propose a soft real-time fault diagnosis system for edge devices using domain adaptation training strategy. The investigation is carried out using deep learning models that can learn representations irrespective of input dimensions. A comparative analysis is performed on a publicly available dataset to evaluate the efficacy of the proposed approach which achieved accuracy of 88.08\%. The experimental results show that our method using long short-term memory network achieves the best results for the bearing fault detection in an IIoT environmental setting.},
  archive      = {J_JPDC},
  author       = {Dileep Kumar and Sanaullah Mehran Ujjan and Kapal Dev and Sunder Ali Khowaja and Naveed Anwar Bhatti and Tanweer Hussain},
  doi          = {10.1016/j.jpdc.2021.10.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {90-99},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards soft real-time fault diagnosis for edge devices in industrial IoT using deep domain adaptation training strategy},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-depth FPGA accelerator performance evaluation with single
node benchmarks from the HPC challenge benchmark suite for intel and
xilinx FPGAs using OpenCL. <em>JPDC</em>, <em>160</em>, 79–89. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging high-level tools lead to a reduced development time for applications on FPGA accelerators while still producing high-quality results. This is one reason for the increased adoption of FPGAs in data center applications which emphasizes the need for a benchmark suite to enable the comparison of FPGA architecture, programming tools, runtimes, and libraries. Because of the lack of such a benchmark suite, we have developed an OpenCL-based open-source implementation of the HPCC benchmark suite for Xilinx and Intel FPGAs. In an in-depth evaluation, we show that the benchmarks allow to quantify the impact of HBM2 memory in comparison to FPGAs with DDR and to analyze differences in the arithmetic units on current FPGA architectures. Power measurements indicate, that not all benchmark implementations can utilize the full potential of the FPGAs in terms of power efficiency. We are continuing to optimize and port the benchmark for new generations of FPGAs and design tools and we encourage active participation to create a valuable tool for the community.},
  archive      = {J_JPDC},
  author       = {Marius Meyer and Tobias Kenter and Christian Plessl},
  doi          = {10.1016/j.jpdc.2021.10.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {79-89},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {In-depth FPGA accelerator performance evaluation with single node benchmarks from the HPC challenge benchmark suite for intel and xilinx FPGAs using OpenCL},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithms for addressing line-of-sight issues in mmWave
WiFi networks using access point mobility. <em>JPDC</em>, <em>160</em>,
65–78. (<a href="https://doi.org/10.1016/j.jpdc.2021.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line-of-sight (LOS) is a critical requirement for mmWave wireless communications . In this work, we explore the use of access point (AP) infrastructure mobility to optimize indoor mmWave WiFi network performance based on the discovery of LOS connectivity to stations (STAs). We consider a ceiling-mounted mobile (CMM) AP as the infrastructure mobility framework. Within this framework, we propose two heuristic algorithms (basic and weighted) derived from Hamming distance computation and a machine learning (ML) solution fully exploiting available network state information to address the LOS discovery problem . Based on the ML solution, we then propose a systematic solution WiMove , which can decide if and where the AP should move to for optimizing network performance. Using both ns-3 based simulation and experimental prototype implementation, we show that the throughput and fairness performance of WiMove is up to 119\% and 15\% better compared with single static AP and brute force search.},
  archive      = {J_JPDC},
  author       = {Yubing Jian and Ching-Lun Tai and Shyam Krishnan Venkateswaran and Mohit Agarwal and Yuchen Liu and Douglas M. Blough and Raghupathy Sivakumar},
  doi          = {10.1016/j.jpdc.2021.10.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-78},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Algorithms for addressing line-of-sight issues in mmWave WiFi networks using access point mobility},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Amnis: Optimized stream processing for edge computing.
<em>JPDC</em>, <em>160</em>, 49–64. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet-of-Things (IoT) devices is rapidly increasing the demands for efficient processing of low latency stream data generated close to the edge of the network. Edge computing-based stream processing techniques that carefully consider the heterogeneity of the computational and network resources available in the infrastructure provide significant benefits in optimizing the throughput and end-to-end latency of the data streams. In this paper, we propose a novel stream query processing framework called Amnis that optimizes the performance of the stream processing applications through a careful allocation of computational and network resources available at the edge. The Amnis approach differentiates itself through its consideration of data locality and resource constraints during physical plan generation and operator placement for the stream queries. Additionally, Amnis considers the coflow dependencies to optimize the network resource allocation through an application-level rate control mechanism. We implement a prototype of Amnis in Apache Storm. Our performance evaluation carried out in a real testbed shows that the proposed techniques achieve as much as 200X improvement on the end-to-end latency and 10X improvement on the overall throughput compared to the default resource aware scheduler in Storm.},
  archive      = {J_JPDC},
  author       = {Jinlai Xu and Balaji Palanisamy and Qingyang Wang and Heiko Ludwig and Sandeep Gopisetty},
  doi          = {10.1016/j.jpdc.2021.10.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {49-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Amnis: Optimized stream processing for edge computing},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing the performance of general matrix multiplication
routine on heterogeneous computing systems. <em>JPDC</em>, <em>160</em>,
39–48. (<a href="https://doi.org/10.1016/j.jpdc.2021.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper contains the results of research on the general matrix multiplication routine performance on modern heterogeneous computing systems . In addition to the single-threaded and multi-threaded performance of the routine for matrices of double-precision real and complex numbers on the IBM POWER and Intel Xeon CPUs, the possibility of automatic offload calculation to NVIDIA GPUs , which is supported by certain BLAS library implementations, was studied. Special attention was paid to the impact on the performance of the bandwidth of the interconnects, which ensure CPU-to-GPU interaction. The obtained results show that IBM computing systems with a high-speed NVLink interconnect demonstrate the best performance doing matrix multiplication on GPUs . Accordingly, these systems can be used to accelerate the solution of tasks that utilize this routines without the need to significantly alter the existing software. It should be noted that CPUs of Intel computing system and the Intel MKL library show the best efficiency performing operations with small matrices. Research results can be used to develop approaches to improving the performance of software, which utilize the general matrix multiplication routine.},
  archive      = {J_JPDC},
  author       = {Aleksei Sorokin and Sergey Malkovsky and Georgiy Tsoy},
  doi          = {10.1016/j.jpdc.2021.10.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {39-48},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Comparing the performance of general matrix multiplication routine on heterogeneous computing systems},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Keeping up with technology: Teaching parallel, distributed,
and high-performance computing. <em>JPDC</em>, <em>160</em>, 36–38. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Sushil Prasad and Sheikh Ghafoor and Martina Barnas and Felix Wolf and Erik Saule and Noemi Rodriguez and Rizos Sakellariou},
  doi          = {10.1016/j.jpdc.2021.10.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {36-38},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Keeping up with technology: Teaching parallel, distributed, and high-performance computing},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using hardware performance counters to speed up autotuning
convergence on GPUs. <em>JPDC</em>, <em>160</em>, 16–35. (<a
href="https://doi.org/10.1016/j.jpdc.2021.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, GPU accelerators are commonly used to speed up general-purpose computing tasks on a variety of hardware. However, due to the diversity of GPU architectures and processed data, optimization of codes for a particular type of hardware and specific data characteristics can be extremely challenging. The autotuning of performance-relevant source-code parameters allows for automatic optimization of applications and keeps their performance portable. Although the autotuning process typically results in code speed-up, searching the tuning space can bring unacceptable overhead if (i) the tuning space is vast and full of poorly-performing implementations, or (ii) the autotuning process has to be repeated frequently because of changes in processed data or migration to different hardware. In this paper, we introduce a novel method for searching generic tuning spaces. The tuning spaces can contain tuning parameters changing any user-defined property of the source code. The method takes advantage of collecting hardware performance counters (also known as profiling counters) during empirical tuning. Those counters are used to navigate the searching process towards faster implementations. The method requires the tuning space to be sampled on any GPU. It builds a problem-specific model, which can be used during autotuning on various, even previously unseen inputs or GPUs. Using a set of five benchmarks, we experimentally demonstrate that our method can speed up autotuning when an application needs to be ported to different hardware or when it needs to process data with different characteristics. We also compared our method to state of the art and show that our method is superior in terms of the number of searching steps and typically outperforms other searches in terms of convergence time.},
  archive      = {J_JPDC},
  author       = {Jiří Filipovič and Jana Hozzová and Amin Nezarat and Jaroslav Ol&#39;ha and Filip Petrovič},
  doi          = {10.1016/j.jpdc.2021.10.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {16-35},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Using hardware performance counters to speed up autotuning convergence on GPUs},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure data outsourcing in presence of the inference
problem: A graph-based approach. <em>JPDC</em>, <em>160</em>, 1–15. (<a
href="https://doi.org/10.1016/j.jpdc.2021.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In light of the emergence of Database-as-a-Service paradigm, secure data outsourcing has become one of the crucial challenges which strongly imposes itself. In such a scenario, access control is considered as a major challenge. In fact, access control policies of the data owner must be preserved when data is moved to the cloud. Here, we address this problem by considering inference leakage that could be produced by exploiting functional dependencies. The proposed approach is based on vertical partitioning to produce a set of secure sub-schemas stored in separated partitions in the distributed system. Then, we extend this approach by presenting a secure query processing model to preserve access control policies when querying data from distributed partitions. The effectiveness of our algorithms is confirmed through observations from a variety of conducted experiments.},
  archive      = {J_JPDC},
  author       = {Adel Jebali and Salma Sassi and Abderrazak Jemai and Richard Chbeir},
  doi          = {10.1016/j.jpdc.2021.09.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-15},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Secure data outsourcing in presence of the inference problem: A graph-based approach},
  volume       = {160},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Momentum-driven adaptive synchronization model for
distributed DNN training on HPC clusters. <em>JPDC</em>, <em>159</em>,
65–84. (<a href="https://doi.org/10.1016/j.jpdc.2021.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building a distributed deep learning (DDL) system on HPC clusters that guarantees convergence speed and scalability for the training of DNNs is challenging. The HPC cluster, which consists of multiple high-density multi-GPU servers connected by the Infiniband network (HDGib), compresses the computing and communication time for distributed DNNs&#39; training but brings new challenges. The convergence time is far from linear scalability (with respect to the number of workers) for parallel DNNs training. We thus analyze the optimization process and identify three key issues that cause scalability degradation. First, the high-frequency update for parameters due to the compression of the computing and communication times exacerbates the stale gradient problem, which slows down the convergence. Second, the previous methods used to constrain the gradient noise (stochastic error) of the SGD are outdated, as HDGib clusters can support more strict constraints due to the Infiniband network connections, which can further constrain the stochastic error. Third, the same learning rate for all workers is inefficient due to the different training stages of each worker. We thus propose a momentum-driven adaptive synchronization model that focuses on solving the above issues and accelerating the training procedure on HDGib clusters. Our adaptive k -synchronization algorithm uses the momentum term to absorb the stale gradients and adaptively bind the stochastic error to provide an approximate optimal descent direction for the distributed SGD. Our model also includes an individual dynamic learning rate search method for each worker to further improve training performance. Compared with previous linear and exponent decay methods, it can provide a more precise descent distance for distributed SGD based on different training stages. Extensive experimental results indicate that the proposed model effectively improves the training performance of CNNs, which retains high accuracy with a speed-up of up to 57.76\% and 125.3\% on the CPU-based and GPU-based clusters, respectively.},
  archive      = {J_JPDC},
  author       = {Zhaorui Zhang and Zhuoran Ji and Choli Wang},
  doi          = {10.1016/j.jpdc.2021.09.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-84},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Momentum-driven adaptive synchronization model for distributed DNN training on HPC clusters},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online delay-guaranteed workload scheduling to minimize
power cost in cloud data centers using renewable energy. <em>JPDC</em>,
<em>159</em>, 51–64. (<a
href="https://doi.org/10.1016/j.jpdc.2021.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more cloud data centers are turning to leverage on-site renewable energy to reduce power cost for sustainable development. But how to effectively coordinate the intermittent renewable energy with workload remains to be a great challenge. This paper investigates the problem of workload scheduling for power cost minimization under the constraints of different Service Level Agreements (SLAs) of delay tolerant workload and delay sensitive workload for green data centers in a smart grid. Different from the existing studies, we take into consideration of the impact of zero price in the smart grid and the cost of on-site renewable energy. To handle the randomness of workload, electricity price and renewable energy availability, we first formulate the problem as a constrained stochastic problem. Then we propose an efficient online control algorithm named ODGWS (Online Delay-Guaranteed Workload Scheduling) which makes online scheduling decisions achieve a bounded guarantee from the worst scheduling delay for delay tolerant workload. Compared with the existing solutions, our ODGWS decomposes the problem into that of solving a simple optimization problem within each time slot in O ( 1 ) O(1) time without needing any future information. The rigorous theoretical analysis demonstrates that our algorithm achieves a [ O ( 1 V ) , O ( V ) ] [O(1V),O(V)] cost-delay tradeoff, where V is a balance parameter between the cost optimality and service quality. Extensive simulations based on real-world traces are done to evaluate the performance of our algorithm. The results show that ODGWS saves about 5\% average power cost compared with the baseline algorithms.},
  archive      = {J_JPDC},
  author       = {Huaiwen He and Hong Shen and Qing Hao and Hui Tian},
  doi          = {10.1016/j.jpdc.2021.09.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {51-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Online delay-guaranteed workload scheduling to minimize power cost in cloud data centers using renewable energy},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the power of randomization in distributed algorithms in
dynamic networks with adaptive adversaries. <em>JPDC</em>, <em>159</em>,
35–50. (<a href="https://doi.org/10.1016/j.jpdc.2021.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the power of randomization in general distributed algorithms in dynamic networks where the network&#39;s topology may evolve over time, as determined by some adaptive adversary . In such a context, randomization may help algorithms to better deal with i) “bad” inputs to the algorithm, and ii) evolving topologies generated by “bad” adaptive adversaries. We prove that randomness offers limited power to better deal with “bad” adaptive adversary. We define a simple notion of prophetic adversary for determining the evolving topologies. Such an adversary accurately predicts all randomness in the algorithm beforehand, and hence the randomness will be useless against “bad” prophetic adversaries. Given a randomized algorithm P whose time complexity satisfies some mild conditions, we prove that P can always be converted to a new algorithm Q with comparable time complexity, even when Q runs against prophetic adversaries. This implies that the benefit of P using randomness for dealing with the adaptive adversaries is limited.},
  archive      = {J_JPDC},
  author       = {Irvan Jahja and Haifeng Yu and Ruomu Hou},
  doi          = {10.1016/j.jpdc.2021.09.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {35-50},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the power of randomization in distributed algorithms in dynamic networks with adaptive adversaries},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed and individualized computation offloading
optimization in a fog computing environment. <em>JPDC</em>,
<em>159</em>, 24–34. (<a
href="https://doi.org/10.1016/j.jpdc.2021.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a newly emerged fog computing environment, various user equipments (UE) enhance their computing power and extend their battery lifetime by computation offloading to mobile edge cloud (MEC) servers. Such an environment is distributed and competitive in nature. In this paper, we take a game theoretical approach to computation offloading optimization in a fog computing environment. Such an approach captures and characterizes the nature of a competitive environment. The main contributions of the paper can be summarized as follows. First, we formulate a non-cooperative game with both UEs and MECs as players. Each UE attempts to minimize the execution time of its tasks with an energy constraint. Each MEC attempts to minimize the product of its power consumption for computation and execution time for allocated tasks. Second, we develop a heuristic algorithm for a UE to determine its “heuristically” best response to the current situation, an algorithm for an MEC to determine its best response to the current situation, and an iterative algorithm to find the Nash equilibrium. Third, we prove that our iterative algorithm converges to a Nash equilibrium. We demonstrate numerical examples of our non-cooperative games with and without MECs&#39; participation. We observe that our iterative algorithm always quickly converges to a Nash equilibrium. The uniqueness of our non-cooperative games is that the strategy set of a player can be discrete and the payoff function of a player can be obtained by a heuristic algorithm for combinatorial optimization. To the best of the author&#39;s knowledge, there has been no such investigation of non-cooperative games based on combinatorial optimization for computation offloading optimization in a fog computing environment.},
  archive      = {J_JPDC},
  author       = {Keqin Li},
  doi          = {10.1016/j.jpdc.2021.09.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-34},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed and individualized computation offloading optimization in a fog computing environment},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving scalability of parallel CNN training by adaptively
adjusting parameter update frequency. <em>JPDC</em>, <em>159</em>,
10–23. (<a href="https://doi.org/10.1016/j.jpdc.2021.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronous SGD with data parallelism , the most popular parallelization strategy for CNN training, suffers from the expensive communication cost of averaging gradients among all workers. The iterative parameter updates of SGD cause frequent communications and it becomes the performance bottleneck . In this paper, we propose a lazy parameter update algorithm that adaptively adjusts the parameter update frequency to address the expensive communication cost issue. Our algorithm accumulates the gradients if the difference of the accumulated gradients and the latest gradients is sufficiently small. The less frequent parameter updates reduce the per-iteration communication cost while maintaining the model accuracy. Our experimental results demonstrate that the lazy update method remarkably improves the scalability while maintaining the model accuracy. For ResNet50 training on ImageNet, the proposed algorithm achieves a significantly higher speedup (739.6 on 2048 Cori KNL nodes) as compared to the vanilla synchronous SGD (276.6) while the model accuracy is almost not affected (&lt;0.2\% difference).},
  archive      = {J_JPDC},
  author       = {Sunwoo Lee and Qiao Kang and Reda Al-Bahrani and Ankit Agrawal and Alok Choudhary and Wei-keng Liao},
  doi          = {10.1016/j.jpdc.2021.09.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {10-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving scalability of parallel CNN training by adaptively adjusting parameter update frequency},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of the layered algorithm in search of an
airborne contaminant source. <em>JPDC</em>, <em>159</em>, 1–9. (<a
href="https://doi.org/10.1016/j.jpdc.2021.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a new method of optimization by the Layered Algorithm (LA). The proposed algorithm reduces the initial area to the sub-area containing the optimum. The proposed technique is based on the classification of the optimized function values (data sampled by sensors). The classification method uses a two-dimensional three-state Cellular Automata (CA). The CA classifies all area points ascribed to the CA cells based on their values. Specification of the categorization layers to the data gives a possibility to identify the different levels areas. Consequently, after analysis, a sub-area containing the optimum can be designated. In this paper, the proposed algorithm is applied to find the location of the airborne contaminant source by analyzing the concentration of released substances reported by mobile sensors distributed over the domain of interest. The Gaussian dispersion model simulation of the contaminant dispersion in the urbanized area is applied to generate the data used to verify the efficiency of the proposed Layered Algorithm. The LA successfully estimates the sub-area of the considered domain where the contamination source is located, taking to account data from sensors solely.},
  archive      = {J_JPDC},
  author       = {Miroslaw Szaban and Anna Wawrzynczak},
  doi          = {10.1016/j.jpdc.2021.09.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-9},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Application of the layered algorithm in search of an airborne contaminant source},
  volume       = {159},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
