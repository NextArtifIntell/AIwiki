<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---108">CVIU - 108</h2>
<ul>
<li><details>
<summary>
(2022). Balanced softmax cross-entropy for incremental learning with
and without memory. <em>CVIU</em>, <em>225</em>, 103582. (<a
href="https://doi.org/10.1016/j.cviu.2022.103582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When incrementally trained on new classes, deep neural networks are subject to catastrophic forgetting which leads to an extreme deterioration of their performance on the old classes while learning the new ones. Using a small memory containing few samples from past classes has shown to be an effective method to mitigate catastrophic forgetting. However, due to the limited size of the replay memory, there is a large imbalance between the number of samples for the new and the old classes in the training dataset resulting in bias in the final model. To address this issue, we propose to use the Balanced Softmax Cross-Entropy and show that it can be seamlessly combined with state-of-the-art approaches for class-incremental learning in order to improve their accuracy while also potentially decreasing the computational cost of the training procedure. We further extend this approach to the more demanding class-incremental learning without memory setting and achieve competitive results with memory-based approaches. Experiments on the challenging ImageNet, ImageNet-Subset, and CIFAR100 benchmarks with various settings demonstrate the benefits of our approach.},
  archive      = {J_CVIU},
  author       = {Quentin Jodelet and Xin Liu and Tsuyoshi Murata},
  doi          = {10.1016/j.cviu.2022.103582},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103582},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Balanced softmax cross-entropy for incremental learning with and without memory},
  volume       = {225},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The MSR-video to text dataset with clean annotations.
<em>CVIU</em>, <em>225</em>, 103581. (<a
href="https://doi.org/10.1016/j.cviu.2022.103581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning automatically generates short descriptions of the video content, usually in form of a single sentence. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benchmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning underlying patterns. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model trained on the original and cleaned datasets. The human behavior experiment demonstrated that trained on the cleaned dataset, the model generated captions that were more coherent and more relevant to the contents of the video clips.},
  archive      = {J_CVIU},
  author       = {Haoran Chen and Jianmin Li and Simone Frintrop and Xiaolin Hu},
  doi          = {10.1016/j.cviu.2022.103581},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103581},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {The MSR-video to text dataset with clean annotations},
  volume       = {225},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster with GANs. <em>CVIU</em>, <em>225</em>, 103571. (<a
href="https://doi.org/10.1016/j.cviu.2022.103571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) and clustering algorithms are both very popular unsupervised methodologies in machine learning . In this work, we propose a novel strategy that uses GANs to improve clustering and vice verse. We start by providing a simple but yet powerful scheme for improving clustering methods that rely on the latent space of GANs. Then, we turn to demonstrate how the output of clustering techniques can be employed for significantly improving the output quality of GANs. We empirically demonstrate the improvement that is achieved by our proposed framework both for clustering and the generation quality of GANs measured by the inception score.},
  archive      = {J_CVIU},
  author       = {Yuri Feigin and Hedva Spitzer and Raja Giryes},
  doi          = {10.1016/j.cviu.2022.103571},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103571},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cluster with GANs},
  volume       = {225},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Certifiable algorithms for the two-view planar triangulation
problem. <em>CVIU</em>, <em>225</em>, 103570. (<a
href="https://doi.org/10.1016/j.cviu.2022.103570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planar scenes predominate in man-made environments, e.g .interior or facades of buildings and in ground images from aerial vehicles. Points lying on those surfaces can be reconstructed from their observations in two images. However, generic reconstruction algorithms output 3D points not lying on the plane, thus obtaining inaccurate reconstructions. The problem also turns to be non-convex with many local minima, hence hindering the performance of iterative method. Therefore, being able to obtain and certify the optimal solution to this problem is of special relevant for these applications. In this paper we first propose a fast and certifiable algorithm that both estimates and certifies the optimal solution to the triangulation problem. From this formulation, we also present an optimality certificate that tells us whether a given solution (obtained by any solver) is the global optimum. Last, from this certificate we derive a sufficient (but not necessary) optimality condition that allows us to certify optimality in less than one microsecond . We test the proposed algorithms on extensive experiments on both synthetic and real data. Code is made available at https://github.com/mergarsal .},
  archive      = {J_CVIU},
  author       = {Mercedes Garcia-Salguero and Javier Gonzalez-Jimenez},
  doi          = {10.1016/j.cviu.2022.103570},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103570},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Certifiable algorithms for the two-view planar triangulation problem},
  volume       = {225},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-object tracker using dynamic bayesian networks and a
residual neural network based similarity estimator. <em>CVIU</em>,
<em>225</em>, 103569. (<a
href="https://doi.org/10.1016/j.cviu.2022.103569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a novel multi-object tracker based on the tracking-by-detection paradigm. This tracker utilises a Dynamic Bayesian Network for predicting objects’ positions through filtering and updating in real-time. The algorithm is trained and then tested using the MOTChallenge 1 challenge benchmark of video sequences. After initial testing, a state-of-the-art residual neural network for extracting feature descriptors is used. This ResNet feature extractor is integrated into the tracking algorithm for object similarity estimation to further enhance tracker performance. Finally, we demonstrate the effects of object detection on tracker performance using a custom trained state of the art You Only Look Once (YOLO) V5 object detector. Results are analysed and evaluated using the MOTChallenge Evaluation Kit, followed by a comparison to state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Mohamad Saada and Christos Kouppas and Baihua Li and Qinggang Meng},
  doi          = {10.1016/j.cviu.2022.103569},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103569},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A multi-object tracker using dynamic bayesian networks and a residual neural network based similarity estimator},
  volume       = {225},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel fast combine-and-conquer object detector based on
only one-level feature map. <em>CVIU</em>, <em>224</em>, 103561. (<a
href="https://doi.org/10.1016/j.cviu.2022.103561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a conceptually simple, flexible, and efficient ”Combine-and-Conquer” detection framework. The proposed framework is composed of a very simple one-level detection pipeline . We modularized the proposed framework into four parts, Backbone, Neck, Feature Aggregation and Detection Head. To verify the performance of this framework, we design a simple yet strong detector, CC-Det. First CC-Det deploys a backbone network to encode the input image, and then uses a neck network to extract rich features and enlarge the receptive field. Next, a feature aggregation network is deployed to aggregate multi-scale features into one feature map. Finally, only one detection head is deployed on the one-level feature map to output a heatmap and bounding boxes . Compared with existing multi-level detectors such as RetinaNet and FCOS, CC-Det achieves excellent performance with much fewer parameters of model and much lower FLOPs. In addition, CC-Det also achieves better trade-off among speed, accuracy and model size without any elaborate special design, compared to other one-level detectors. Moreover, CC-Det is easy to generalize to other tasks with minor modifications and achieves state-of-the-art performance. Excellent results are presented on COCO, PASCAL VOC, WiderFace and CrowdHuman datasets.},
  archive      = {J_CVIU},
  author       = {Jianhua Yang and Ke Wang and Ruifeng Li and Zhonghao Qin and Petra Perner},
  doi          = {10.1016/j.cviu.2022.103561},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103561},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel fast combine-and-conquer object detector based on only one-level feature map},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quaternion-based dynamic mode decomposition for background
modeling in color videos. <em>CVIU</em>, <em>224</em>, 103560. (<a
href="https://doi.org/10.1016/j.cviu.2022.103560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Background Initialization (SBI) is one of the challenging problems in computer vision . Dynamic mode decomposition (DMD) is a recently proposed method to robustly decompose a video sequence into the background model and the corresponding foreground part. However, this method needs to convert the color image into a grayscale image for processing, which leads to the neglect of the coupling information between the three channels of the color image. In this study, we propose a quaternion-based DMD (Q-DMD), which extends the DMD by quaternion matrix analysis , so as to ultimately preserve the inherent color structure of the color image and the color video. We exploit the standard eigenvalues of the quaternion matrix to compute its spectral decomposition and calculate the corresponding Q-DMD modes and eigenvalues. The results on the publicly available benchmark datasets prove that our Q-DMD outperforms the exact DMD method , and experiment results also demonstrate that the performance of our approach is comparable to that of the state-of-the-art ones.},
  archive      = {J_CVIU},
  author       = {Juan Han and Kit Ian Kou and Jifei Miao},
  doi          = {10.1016/j.cviu.2022.103560},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103560},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Quaternion-based dynamic mode decomposition for background modeling in color videos},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial introduction to the special issue on
“biometrics based methods for healthcare applications.” <em>CVIU</em>,
<em>224</em>, 103559. (<a
href="https://doi.org/10.1016/j.cviu.2022.103559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CVIU},
  author       = {Michele Nappi and Hugo Proença and Sambit Bakshi and Vittorio Murino},
  doi          = {10.1016/j.cviu.2022.103559},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103559},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Guest editorial introduction to the special issue on “Biometrics based methods for healthcare applications”},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transformed ROIs for capturing visual transformations in
videos. <em>CVIU</em>, <em>224</em>, 103558. (<a
href="https://doi.org/10.1016/j.cviu.2022.103558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the visual changes that an action brings to a scene is critical for video understanding . Currently, CNNs process one local neighborhood at a time, thus contextual relationships over longer ranges, while still learnable, are indirect. We present TROI, a plug-and-play module for CNNs to reason between mid-level feature representations that are otherwise separated in space and time. The module relates localized visual entities such as hands and interacting objects and transforms their corresponding regions of interest directly in the feature maps of convolutional layers . With TROI, we achieve state-of-the-art action recognition results on the large-scale datasets Something–Something-V2 and EPIC-Kitchens-100.},
  archive      = {J_CVIU},
  author       = {Abhinav Rai and Fadime Sener and Angela Yao},
  doi          = {10.1016/j.cviu.2022.103558},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103558},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transformed ROIs for capturing visual transformations in videos},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2.5D visual relationship detection. <em>CVIU</em>,
<em>224</em>, 103557. (<a
href="https://doi.org/10.1016/j.cviu.2022.103557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual 2.5D perception involves understanding the semantics and geometry of a scene through reasoning about object relationships with respect to the viewer. However, existing works in visual recognition primarily focus on the semantics. To bridge this gap, we study 2.5D visual relationship detection (2.5VRD), in which the goal is to jointly detect objects and predict their relative depth and occlusion relationships. Unlike general VRD, 2.5VRD is egocentric, using the camera’s viewpoint as a common reference for all 2.5D relationships. Unlike depth estimation, 2.5VRD is object-centric and does not only focus on depth. To enable progress on this task, we construct a new dataset consisting of 220K human-annotated 2.5D relationships among 512K objects from 11K images. We analyze this dataset and conduct extensive experiments including benchmarking multiple state-of-the-art VRD models on this task. Experimental results show that existing models largely rely on semantic cues and simple heuristics to solve 2.5VRD, motivating further research on models for 2.5D perception. We will make our dataset and source code publicly available.},
  archive      = {J_CVIU},
  author       = {Yu-Chuan Su and Soravit Changpinyo and Xiangning Chen and Sathish Thoppay and Cho-Jui Hsieh and Lior Shapira and Radu Soricut and Hartwig Adam and Matthew Brown and Ming-Hsuan Yang and Boqing Gong},
  doi          = {10.1016/j.cviu.2022.103557},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103557},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {2.5D visual relationship detection},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust real-world point cloud registration by inlier
detection. <em>CVIU</em>, <em>224</em>, 103556. (<a
href="https://doi.org/10.1016/j.cviu.2022.103556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world point cloud registration is challenging because of large outliers in correspondence search. The mixture variations, such as partial overlap, noise and cross sources, are the root cause of these large outliers. Existing methods face challenges in effectively removing the large outliers. We propose a novel coarse-to-fine framework to remove the outliers by detecting the accurate inlier correspondences. Specifically, our coarse module predicts the top-K accurate correspondences. The coarse module is trained by jointly leveraging global and local structured information. Then, our refinement module checks the correspondences further using our proposed novel higher-order filter, which enables the structure conformity of correspondences to improve the quality of inlier correspondences. The final transformation matrix is calculated by using the refined inlier correspondences. Furthermore, a new cross-source point cloud dataset is proposed to further demonstrate the robustness in real-world point clouds. Experimental results demonstrate that our algorithm achieves the state-of-the-art accuracy on both indoor and outdoor, same-source and newly proposed cross-source real-world point clouds.},
  archive      = {J_CVIU},
  author       = {Xiaoshui Huang and Yangfu Wang and Sheng Li and Guofeng Mei and Zongyi Xu and Yucheng Wang and Jian Zhang and Mohammed Bennamoun},
  doi          = {10.1016/j.cviu.2022.103556},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103556},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust real-world point cloud registration by inlier detection},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RocNet: Recursive octree network for efficient 3D
processing. <em>CVIU</em>, <em>224</em>, 103555. (<a
href="https://doi.org/10.1016/j.cviu.2022.103555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a deep recursive octree network for general-purpose 3D voxel data processing. Our network compresses a voxel grid of any size down to a very small latent space in an autoencoder-like network. We show results for compressing 3 2 3 323 , 6 4 3 643 and 12 8 3 1283 grids down to just 80 floats in the latent space. We demonstrate the effectiveness and efficiency of our proposed method on several publicly available datasets with four experiments: 3D shape classification, 3D shape reconstruction , shape generation and semantic segmentation . Experimental results show that our algorithm maintains accuracy while consuming less memory with shorter training times compared to existing methods, especially in 3D reconstruction tasks.},
  archive      = {J_CVIU},
  author       = {Juncheng Liu and Steven Mills and Brendan McCane},
  doi          = {10.1016/j.cviu.2022.103555},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103555},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RocNet: Recursive octree network for efficient 3D processing},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D semantic segmentation based on spatial-aware convolution
and shape completion for augmented reality applications. <em>CVIU</em>,
<em>224</em>, 103550. (<a
href="https://doi.org/10.1016/j.cviu.2022.103550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D semantic segmentation of indoor scenes is a popular research topic in the field of computer vision . For many applications, it is very important to know exactly what category each point in the scene belongs to. Benefiting from the development of deep learning , many neural networks based on voxels and points have been proposed to solve these segmentation problems. However, most of them do not fully consider the information of the spatial structure. Current voxel-based sparse convolutional neural networks can effectively extract 3D features in space. However, they assume that the feature in the empty space is zero, causing a loss of information in the spatial structure. In this paper, we propose a system that uses scene point clouds with color information to semantically segment an entire indoor scene. Based on the sparsity of spatial data, we design a novel spatial-aware sparse convolution operation . We encode the spatial information of the object’s existence as an additional feature and use the self-attention mechanism to effectively aggregate features. In addition, we introduce a completion network to refine the results from the segmentation network , so that each object in the scene is fitted into a more reasonable and complete shape. Through the above two methods, we build an accurate scene semantic segmentation network to obtain the semantic information of the entire scene. In the experimental part, we use two public datasets to perform quantitative and qualitative analysis. We compare our results with other state-of-the-art methods to prove the superiority of our method. Our models are also examined under different configurations to assure the effectiveness of the proposed method. Finally, the semantic segmentation model was integrated into a real-world application to demonstrate its usefulness. We expect that the proposed 3D scene semantic segmentation system can provide accurate and fast results for practical applications.},
  archive      = {J_CVIU},
  author       = {Yun-Chih Guo and Tzu-Hsuan Weng and Robin Fischer and Li-Chen Fu},
  doi          = {10.1016/j.cviu.2022.103550},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103550},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3D semantic segmentation based on spatial-aware convolution and shape completion for augmented reality applications},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully convolutional online tracking. <em>CVIU</em>,
<em>224</em>, 103547. (<a
href="https://doi.org/10.1016/j.cviu.2022.103547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning has turned out to be effective for improving tracking performance. However, it could be simply applied for classification branch, but still remains challenging to adapt to regression branch due to its complex design and intrinsic requirement for high-quality online samples. To tackle this issue, we present the fully convolutional online tracking framework, coined as FCOT, and focus on enabling online learning for both classification and regression branches by using a target filter based tracking paradigm. Our key contribution is to introduce an online regression model generator (RMG) for initializing weights of the target filter in the regression branch with online samples, and then optimizing this target filter weights based on the ground-truth samples at the first frame. Specifically, we devise a simple fully online tracker, composed of a feature extraction backbone, an up-sampling decoder, a multi-scale classification branch, and an anchor-free regression branch. Thanks to the unique design of RMG, our FCOT can not only handle the target variation along temporal dimension, but also overcome the issue of error accumulation during the tracking procedure. In addition, due to its simplicity in design, our FCOT could be trained and deployed in a fully convolutional manner with a real-time running speed. Our FCOT achieves promising performance on seven benchmarks, including VOT2018, LaSOT, TrackingNet, GOT-10k, OTB100, UAV123, and NFS. Code and models of our FCOT are available at: https://github.com/MCG-NJU/FCOT .},
  archive      = {J_CVIU},
  author       = {Yutao Cui and Cheng Jiang and Limin Wang and Gangshan Wu},
  doi          = {10.1016/j.cviu.2022.103547},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103547},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fully convolutional online tracking},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating 3D body mesh without SMPL annotations via
alternating successive convex approximation. <em>CVIU</em>,
<em>224</em>, 103539. (<a
href="https://doi.org/10.1016/j.cviu.2022.103539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focused on extracting effective human shape and pose information from solely joint annotations. With few training datasets and without using SMPL annotations from MoSh, we proposed a method based on alternating successive convex approximation (ASCA) to estimate the 3D human shape and pose. Previous methods tended to utilize a large number of mixed 2D and 3D datasets for training. These methods extract shape and pose information from SMPL annotations containing ground-truth shape and pose annotations. It is challenging to learn useful information of human shape and pose from solely joint annotations independently because 2D and 3D joint positions can be expressed as a non-convex function of coupled SMPL shape and pose parameters. The proposed method decouples the function into joint-shape and joint-pose functions making the training focus on shape and pose separately during each procedure. A minimum number of datasets are used to train the proposed method (InstaVariety and MPI-INF-3DHP) compare to previous methods. We make a comparison with other methods that have been trained with SMPL annotations. The result shows that the proposed method is competitive with other algorithms trained with additional SMPL annotations. What is more, when ground-truth SMPL annotations exist, ASAC can equally extract useful shape and pose information from image sequences. The result trained with a 3DPW training set outperforms most current video-based algorithms.},
  archive      = {J_CVIU},
  author       = {Wenzhang Sun and Lu Wang and Shaopeng Ma and Qinwen Ma},
  doi          = {10.1016/j.cviu.2022.103539},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103539},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Estimating 3D body mesh without SMPL annotations via alternating successive convex approximation},
  volume       = {224},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multispectral interaction convolutional neural network for
pedestrian detection. <em>CVIU</em>, <em>223</em>, 103554. (<a
href="https://doi.org/10.1016/j.cviu.2022.103554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion of multispectral data in object detection is inevitable in order to cover various environments. However, there is still insufficient research on how to fuse information between two-stream multispectral networks. This paper proposes a novel multispectral interaction convolutional neural network (MICNN) for fusing information between multispectral networks. Unlike existing fusion methods that do not have interactions between multispectral networks, the proposed MICNN reflects information from each multispectral network in the training process. The MICNN is a simple way of forcing interactions by exchanging weights of feature maps between multispectral networks. It does not require any parameters to learn, and does not need additional computations or modifications to the network structure. We verified the effectiveness of the MICNN with the KAIST multispectral pedestrian dataset and YU far-infrared (FIR) pedestrian dataset.},
  archive      = {J_CVIU},
  author       = {Junhwan Ryu and Jongchan Kim and Heegon Kim and Sungho Kim},
  doi          = {10.1016/j.cviu.2022.103554},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103554},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multispectral interaction convolutional neural network for pedestrian detection},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequency aware face hallucination generative adversarial
network with semantic structural constraint. <em>CVIU</em>,
<em>223</em>, 103553. (<a
href="https://doi.org/10.1016/j.cviu.2022.103553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the issue of face hallucination. Most current face hallucination methods rely on two-dimensional facial priors to generate high resolution face images from low resolution face images. These methods are only capable of assimilating global information into the generated image. Still there exist some inherent problems in these methods, such as, local features , subtle structural details and depth information are missing in final output image. This work proposes a generative adversarial network (GAN) based novel progressive face hallucination (FH) network to address these issues present among current methods. The generator of the proposed model comprises of FH network and two sub-networks, assisting FH network to generate high resolution images. The first sub-network leverages on explicitly adding high frequency components into the model. To explicitly encode the high frequency components, an auto encoder is proposed to generate high resolution coefficients of discrete cosine transform (DCT). To add three dimensional parametric information into the network, second sub-network is proposed. This network uses a shape model of 3D morphable models (3DMM) to add structural constraint to the FH network. Extensive experimentation evaluation show the usefulness of proposed architecture in the form of state-of-the-art quantitative results.},
  archive      = {J_CVIU},
  author       = {Shailza Sharma and Abhinav Dhall and Vinay Kumar},
  doi          = {10.1016/j.cviu.2022.103553},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103553},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Frequency aware face hallucination generative adversarial network with semantic structural constraint},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on bias in visual datasets. <em>CVIU</em>,
<em>223</em>, 103552. (<a
href="https://doi.org/10.1016/j.cviu.2022.103552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer Vision (CV) has achieved remarkable results, outperforming humans in several tasks. Nonetheless, it may result in significant discrimination if not handled properly. Indeed, CV systems highly depend on training datasets and can learn and amplify biases that such datasets may carry. Thus, the problem of understanding and discovering bias in visual datasets is of utmost importance ; yet, it has not been studied in a systematic way to date. Hence, this work aims to: (i) describe the different kinds of bias that may manifest in visual datasets; (ii) review the literature on methods for bias discovery and quantification in visual datasets; (iii) discuss existing attempts to collect visual datasets in a bias-aware manner. A key conclusion of our study is that the problem of bias discovery and quantification in visual datasets is still open, and there is room for improvement in terms of both methods and the range of biases that can be addressed. Moreover, there is no such thing as a bias-free dataset, so scientists and practitioners must become aware of the biases in their datasets and make them explicit. To this end, we propose a checklist to spot different types of bias during visual dataset collection.},
  archive      = {J_CVIU},
  author       = {Simone Fabbrizzi and Symeon Papadopoulos and Eirini Ntoutsi and Ioannis Kompatsiaris},
  doi          = {10.1016/j.cviu.2022.103552},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103552},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A survey on bias in visual datasets},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual adversarial model: Exploring low-dimensional space
features for point clouds generating and completing. <em>CVIU</em>,
<em>223</em>, 103551. (<a
href="https://doi.org/10.1016/j.cviu.2022.103551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is a fundamental 3D representation that has drawn increasing attention due to the popularity of various depth scanning devices. Effectively and accurately synthesizing point cloud is a challenging task due to the high-frequency (spatial) geometric details and the high-dimensionality of the extrinsic observation space. In this article, we focus on how to capture the informative intrinsic structure from the latent low-dimensional space and the diversity from the ambient space simultaneously. As a result, a new framework consisting of dual alternating generator and discriminator pairs is proposed to create various diverse and realistic geometries. We evaluate our model on both generation and completion tasks, covering several public datasets(ModelNet40, ShapeNet, Kitti, et al). Extensive experiments demonstrate the effectiveness of the framework on 3D point cloud synthesis. Based on this unified framework, we can not only achieve the state-of-the-art performance compared with several well-known point cloud generative models , but also get the competitive result in the task of completion by making some minor adjustments to the network structure. Moreover, the proposed method exhibits competitive performance on MNIST 2D dataset.},
  archive      = {J_CVIU},
  author       = {Yuhang Zhang and Zhenwei Miao and Tiebin Mi and Jie Li and Robert C. Qiu},
  doi          = {10.1016/j.cviu.2022.103551},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103551},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dual adversarial model: Exploring low-dimensional space features for point clouds generating and completing},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervision &amp; meta-learning for one-shot
unsupervised cross-domain detection. <em>CVIU</em>, <em>223</em>,
103549. (<a href="https://doi.org/10.1016/j.cviu.2022.103549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep detection approaches are powerful in controlled conditions, but appear brittle and fail when source models are used off-the-shelf on unseen domains. Most of the existing works on domain adaptation simplify the setting and access jointly both a large source dataset and a sizable amount of target samples. However this scenario is unrealistic in many practical cases as when monitoring image feeds from social media: only a pretrained source model is available and every target image uploaded by the users belongs to a different domain not foreseen during training. We address this challenging setting by presenting an object detection algorithm able to exploit a pre-trained source model and perform unsupervised adaptation by using only one target sample seen at test time. Our multi-task architecture includes a self-supervised branch that we exploit to meta-train the whole model with single-sample cross-domain episodes, and prepare to the test condition. At deployment time the self-supervised task is iteratively solved on any incoming sample to one-shot adapt on it. We introduce a new dataset of social media image feeds and present a thorough benchmark with the most recent cross-domain detection methods showing the advantages of our approach.},
  archive      = {J_CVIU},
  author       = {Francesco Cappio Borlino and Salvatore Polizzotto and Barbara Caputo and Tatiana Tommasi},
  doi          = {10.1016/j.cviu.2022.103549},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103549},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervision &amp; meta-learning for one-shot unsupervised cross-domain detection},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning spectral transform for 3D human motion prediction.
<em>CVIU</em>, <em>223</em>, 103548. (<a
href="https://doi.org/10.1016/j.cviu.2022.103548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing motion prediction methods that use graph convolution networks , motion sequences are transformed to a spectral domain , and future motions are predicted through graph spectral filtering for the transformed spectral sequences. However, because the conventional spectral transform method uses a predetermined spectral basis, the motion prediction does not work well for aperiodic or complicated motions. To overcome this problem, we propose a method to learn spectral domain transforms from motion sequences in the training dataset. To this end, two methods are attempted: one for learning the frequency of each spectral basis, and another for learning the values of the basis function directly. Through experiments on representative 3D human motion benchmarks, H3.6M and CMU Mocap, we demonstrate that both of the proposed methods consistently outperform the baseline method . In particular, the method of directly learning the basis function outperforms the state-of-the-art methods. We also demonstrate that the proposed method yields realistic predictions, even for aperiodic and complicated action categories.},
  archive      = {J_CVIU},
  author       = {Boeun Kim and Jin Young Choi},
  doi          = {10.1016/j.cviu.2022.103548},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103548},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning spectral transform for 3D human motion prediction},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsic image decomposition using physics-based cues and
CNNs. <em>CVIU</em>, <em>223</em>, 103538. (<a
href="https://doi.org/10.1016/j.cviu.2022.103538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition is the decomposition of an image into its reflectance and shading components. The intrinsic image decomposition problem is inherently ill-posed, since there can be multiple solutions to compute the intrinsic components forming the same image. In this paper, we explore the use of physics-based priors. We also propose a new architecture that separates the learning components in a stacked manner. We explore various ways of integrating such priors into a deep learning system. Our method is trained and tested on a large synthetic garden dataset to assess its performance. It is evaluated and compared to state-of-the-art methods using two standard intrinsic datasets. Finally, the pre-trained network is tested on real world images to show the generalisation capabilities of the network.},
  archive      = {J_CVIU},
  author       = {Partha Das and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2022.103538},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103538},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Intrinsic image decomposition using physics-based cues and CNNs},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Target-aware and spatial-spectral discriminant feature joint
correlation filters for hyperspectral video object tracking.
<em>CVIU</em>, <em>223</em>, 103535. (<a
href="https://doi.org/10.1016/j.cviu.2022.103535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking has been considered a promising task in computer vision . Most existing trackers construct tracking frameworks based on color video which provides information in limit visible spectrums, while hyperspectral video gives more material-based information for targets and distractors in background. Although hyperspectral video contains abundant spectral information , high-dimensional data brings negative influence for visual tracking due to redundant information. To exploit the intrinsic characteristics in hyperspectral video, a novel hyperspectral video-based tracking algorithm is proposed in this paper. A target-aware band selection (TABS) method is designed to select discriminative information which is beneficial to distinguish a target from complex background. To take advantage of the spatial–spectral relationship in hyperspectral video, an adaptive spatial–spectral discriminant analysis method (ASSDA) is designed to embed high-dimensional hyperspectral data into low-dimensional space. In the tracking process, two false-color video branches generated from TABS and ASSDA are put into correlation filters-based tracker, respectively. After that, the output responses of two branches are combined to obtain a joint estimation in hyperspectral video. Extensive experimental results illustrate the effectiveness of our method compared with those state-of-the-art color and hyperspectral trackers.},
  archive      = {J_CVIU},
  author       = {Yiming Tang and Yufei Liu and Hong Huang},
  doi          = {10.1016/j.cviu.2022.103535},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103535},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Target-aware and spatial-spectral discriminant feature joint correlation filters for hyperspectral video object tracking},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defending against attacks tailored to transfer learning via
feature distancing. <em>CVIU</em>, <em>223</em>, 103533. (<a
href="https://doi.org/10.1016/j.cviu.2022.103533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is preferable for training a deep neural network with a small training dataset by leveraging a pre-trained teacher model. However, transfer learning opens a door for new attacks that generate adversarial examples using the pre-trained teacher model. In this paper, we propose a novel method called feature distancing to defend against adversarial attacks tailored to transfer learning. The method aims to train a student model with a distinct feature representation from the teacher model. We generate adversarial examples of the mimic attack with the teacher model, and the examples are used to train the student model. We use triplet loss to put the mimic attack examples close to their source images and far from their target images in the feature space of the student model. The proposed method is evaluated on three different transfer learning tasks with diverse attack configurations. It is the only method that achieves high “robust accuracy” and high “test accuracy” on every task we evaluate.},
  archive      = {J_CVIU},
  author       = {Sangwoo Ji and Namgyu Park and Dongbin Na and Bin Zhu and Jong Kim},
  doi          = {10.1016/j.cviu.2022.103533},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103533},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Defending against attacks tailored to transfer learning via feature distancing},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-segmentation inspired attention module for video-based
computer vision tasks. <em>CVIU</em>, <em>223</em>, 103532. (<a
href="https://doi.org/10.1016/j.cviu.2022.103532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based computer vision tasks can benefit from estimation of the salient regions and interactions between those regions. Traditionally, this has been done by identifying the object regions in the images by utilizing pre-trained models to perform object detection, object segmentation and/or object pose estimation. Although using pre-trained models is a viable approach, it has several limitations in the need for an exhaustive annotation of object categories, a possible domain gap between datasets and a bias that is typically present in pre-trained models. In this work, we propose to utilize the common rationale that a sequence of video frames capture a set of common objects and interactions between them, thus a notion of co-segmentation between the video frame features may equip the model with the ability to automatically focus on task-specific salient regions and improve the underlying task’s performance in an end-to-end manner. In this regard, we propose a generic module called “Co-Segmentation inspired Attention Module” (COSAM) that can be plugged in to any CNN model to promote the notion of co-segmentation based attention among a sequence of video frame features. We show the application of COSAM in three video-based tasks namely: (1) Video-based person re-ID, (2) Video captioning, &amp; (3) Video action classification and demonstrate that COSAM is able to capture the task-specific salient regions in video frames, thus leading to notable performance improvements along with interpretable attention maps for a variety of video-based vision tasks, with possible application to other video-based vision tasks as well.},
  archive      = {J_CVIU},
  author       = {Arulkumar Subramaniam and Jayesh Vaidya and Muhammed Abdul Majeed Ameen and Athira Nambiar and Anurag Mittal},
  doi          = {10.1016/j.cviu.2022.103532},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103532},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Co-segmentation inspired attention module for video-based computer vision tasks},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive multi-scale fusion network for RGB-d salient
object detection. <em>CVIU</em>, <em>223</em>, 103529. (<a
href="https://doi.org/10.1016/j.cviu.2022.103529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) aims at locating the most significant object within a given image. In recent years, great progress has been made in applying SOD on many vision tasks. The depth map could provide additional spatial prior and boundary cues to boost the performance. Combining the depth information with image data obtained from standard visual cameras has been widely used in recent SOD works, however, introducing depth information in a suboptimal fusion strategy may have negative influence in the performance of SOD. In this paper, we discuss about the advantages of the so-called progressive multi-scale fusion method and propose a mask-guided feature aggregation module (MGFA). The proposed framework can effectively combine the two features of different modalities and, furthermore, alleviate the impact of erroneous depth features, which are inevitably caused by the variation of depth quality. We further introduce a mask-guided refinement module (MGRM) to complement the high-level semantic features and reduce the irrelevant features from multi-scale fusion, leading to an overall refinement of detection. Experiments on five challenging benchmarks demonstrate that the proposed method outperforms 11 state-of-the-art methods under different evaluation metrics .},
  archive      = {J_CVIU},
  author       = {Guangyu Ren and Yanchun Xie and Tianhong Dai and Tania Stathaki},
  doi          = {10.1016/j.cviu.2022.103529},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103529},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Progressive multi-scale fusion network for RGB-D salient object detection},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A formal approach to good practices in pseudo-labeling for
unsupervised domain adaptive re-identification. <em>CVIU</em>,
<em>223</em>, 103527. (<a
href="https://doi.org/10.1016/j.cviu.2022.103527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of pseudo-labels prevails to tackle Unsupervised Domain Adaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed, this family of approaches has given rise to several UDA re-identification-specific frameworks, which are effective. In these works, research directions to improve Pseudo-Labeling UDA re-ID performance are varied and primarily based on intuition and experiments: refining pseudo-labels, reducing the impact of errors in pseudo-labels... It can be hard to deduce from them general good practices that we can implement in any Pseudo-Labeling method to improve its performance consistently. We propose a new theoretical view on Pseudo-Labeling UDA re-ID to address this fundamental question. The contributions are threefold: (i) A novel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a new general learning upper bound on the UDA re-ID performance. (ii) General good practices for Pseudo-Labeling are directly deduced from the interpretation of the proposed theoretical framework to improve the target re-ID performance. (iii) Extensive experiments on challenging person and vehicle cross-dataset re-ID tasks, showing consistent performance improvements for various state-of-the-art methods and various proposed implementations of good practices.},
  archive      = {J_CVIU},
  author       = {Fabian Dubourvieux and Romaric Audigier and Angélique Loesch and Samia Ainouz and Stéphane Canu},
  doi          = {10.1016/j.cviu.2022.103527},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103527},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A formal approach to good practices in pseudo-labeling for unsupervised domain adaptive re-identification},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for deepfakes creation and detection: A
survey. <em>CVIU</em>, <em>223</em>, 103525. (<a
href="https://doi.org/10.1016/j.cviu.2022.103525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully applied to solve various complex problems ranging from big data analytics to computer vision and human-level control. Deep learning advances however have also been employed to create software that can cause threats to privacy, democracy and national security. One of those deep learning-powered applications recently emerged is deepfake . Deepfake algorithms can create fake images and videos that humans cannot distinguish them from authentic ones. The proposal of technologies that can automatically detect and assess the integrity of digital visual media is therefore indispensable. This paper presents a survey of algorithms used to create deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. We present extensive discussions on challenges, research trends and directions related to deepfake technologies. By reviewing the background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive overview of deepfake techniques and facilitates the development of new and more robust methods to deal with the increasingly challenging deepfakes.},
  archive      = {J_CVIU},
  author       = {Thanh Thi Nguyen and Quoc Viet Hung Nguyen and Dung Tien Nguyen and Duc Thanh Nguyen and Thien Huynh-The and Saeid Nahavandi and Thanh Tam Nguyen and Quoc-Viet Pham and Cuong M. Nguyen},
  doi          = {10.1016/j.cviu.2022.103525},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103525},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep learning for deepfakes creation and detection: A survey},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet-based multi-level generative adversarial networks
for face aging. <em>CVIU</em>, <em>223</em>, 103524. (<a
href="https://doi.org/10.1016/j.cviu.2022.103524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face aging has received increasing attention from the computer vision community due to wide applications in the real world. Age accuracy and identity preserving are two important indicators for face aging. Previous works usually rely on an extra pre-trained module for identity preserving and multi-level discriminators for fine-grained features extraction. In this work, we propose a cycle-consistent loss based method for face aging with wavelet-based multi-level facial attributes extraction from both generator and discriminators. The proposed model consists of one generator with three-level encoders and three levels of discriminators with an age and a gender classifier on top of each discriminator. Experiment results on both MORPH and CACD show that the application of multi-level generator can improve the identity preserving effects in face aging and reduce the training time significantly by eliminating the rely of an identity preserving module. Our model can outperform most of the existing approaches include the state-of-the-art techniques on two benchmark aging databases in terms of both aging accuracy and identity verification confidence, demonstrating the effectiveness and superiority of our method.},
  archive      = {J_CVIU},
  author       = {Jun Shao and Tien D. Bui},
  doi          = {10.1016/j.cviu.2022.103524},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103524},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Wavelet-based multi-level generative adversarial networks for face aging},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GAFL: Global adaptive filtering layer for computer vision.
<em>CVIU</em>, <em>223</em>, 103519. (<a
href="https://doi.org/10.1016/j.cviu.2022.103519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a universal global adaptive filtering layer, GAFL, capable of “learning” optimal frequency filter for each image in a dataset together with the weights of the base neural network that performs some computer vision task. The proposed approach takes the source image in the spatial domain, selects the best frequencies in the Fourier domain for the benefit of the global task, and prepends the inverse-transform image to the main neural network for a joint training. Remarkably, such a simple add-on layer, capable of optimizing the frequency content of an input for a specific task, dramatically improves the performance of the main network regardless of its design. We observe that the light networks gain a noticeable boost in the performance metrics; whereas, the training of the heavy ones converges faster when GAFL is prepended to the main architecture. We showcase the performance of the layer in four classical computer vision tasks: classification, segmentation, denoising , and erasing, considering popular natural and medical data benchmarks.},
  archive      = {J_CVIU},
  author       = {Viktor Shipitsin and Iaroslav Bespalov and Dmitry V. Dylov},
  doi          = {10.1016/j.cviu.2022.103519},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103519},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GAFL: Global adaptive filtering layer for computer vision},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive feature denoising based deep convolutional network
for single image super-resolution. <em>CVIU</em>, <em>223</em>, 103518.
(<a href="https://doi.org/10.1016/j.cviu.2022.103518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the feature map recalibration (FMR) mechanism has been widely explored in single image super-resolution (SISR) and obtained remarkable performances. However, the existing FMR-based SISR methods directly incorporate the attention module into a deeper network structure (e.g. EDSR), while neglecting the differences between the low-level and high-level vision problems. In this paper, we design a low-level specific FMR mechanism for SISR task based on a new observation by examining current SISR methods, which all demonstrate a solid correlation between the SISR performance and the convolutional feature noise. Inspired by this, we extend the classic soft thresholding technique in the way of deep network, and develop an Adaptive Soft Thresholding (AST) module for feature noise suppression . Comparing to existing attention modules, AST is light-weighted and can be taken as an easy plug-in module in any SISR networks. To this end, we construct a adaptive Feature Denoising Super-Resolution (FDSR) network by combining the baseline EDSR and the proposed AST. Extensive experimental results show that the proposed FDSR network could achieve the state-of-the-art performances on SISR benchmarks, and significantly reduce the parameter (28.8\% for EDSR, 74.6\% for RCAN,s 76.0\% for SAN) with respect to FMR module.},
  archive      = {J_CVIU},
  author       = {Rui Cheng and Yuzhe Wu and Jia Wang and Mingming Ma and Yi Niu and Guangming Shi},
  doi          = {10.1016/j.cviu.2022.103518},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103518},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive feature denoising based deep convolutional network for single image super-resolution},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive gradients and weight projection based on quantized
neural networks for efficient image classification. <em>CVIU</em>,
<em>223</em>, 103516. (<a
href="https://doi.org/10.1016/j.cviu.2022.103516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization of weights and activations has been introduced into optimization methodologies of deep neural networks (DNNs) to address issues like memory consumption and massive computational demands that impede edge applications of neural networks . There are multiple approaches to quantization which can probably all be classified into two categories: uniform quantization and non-uniform quantization. The key difference between these methods is whether unequal quantization intervals are used to match the non-uniform distribution of weights. However, conventional techniques train weights layer by layer to suit previously determined quantization points, which poses difficulty for reaching optimal points. We proposed a framework called Deep Projection (DP) to train quantized weights with adaptive gradients. Gradients and weights are projected to high dimensional training space through projection layers, resulting in more complicated update paths and randomness, which benefit the generalization of models. Renewal of weights in the network is conducted by values composited by self-updating high dimensional tensors, which means that the resulting gradients are softer and more adaptive to network training. We applied our method via a uniform quantization approach, and the results showed improvements even when we limited the first and last layers with lower bit-width. The results could be comparable with the non-uniform quantization method with 4-bit precision. Distributions of weights in different quantized networks are analyzed to display the advantages of our method. Besides, all of these steps can be realized in a conventional training process. There are no limitations to quantization bit-width and function. Namely, our framework is readily implemented with a concise architecture.},
  archive      = {J_CVIU},
  author       = {Ran Wu and Huanyu Liu and Jun-Bao Li},
  doi          = {10.1016/j.cviu.2022.103516},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103516},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive gradients and weight projection based on quantized neural networks for efficient image classification},
  volume       = {223},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised face frontalization using disentangled
representation-learning CycleGAN. <em>CVIU</em>, <em>222</em>, 103526.
(<a href="https://doi.org/10.1016/j.cviu.2022.103526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face frontalization aims to normalize profile face images to frontal ones for pose-invariant face-related recognition tasks. Current works have achieved outstanding results in face frontalization by using deep learning techniques. However, paired training data is usually required to train a deep face frontalization model and it is always difficult and time-consuming to acquire this kind of data. To solve this problem, we propose an unsupervised face frontalization framework, named Disentangled Representation-learning Cycle Generative Adversarial Network (DRCycleGAN). The model can be trained with unpaired data through embedding face images onto two spaces, identity feature space and pose feature space, and jointly inputting the identity feature and the pose feature to the generators to implement a paired forward and backward mapping (i.e., face frontalization and its inverse process). To adapt to the face frontalization task, a semantic-level cycle consistency loss is proposed, which implements consistency constraint supervision by measuring high-level semantic feature differences. Extensive experiment results demonstrate that the proposed method can achieve promising face frontalization performance and improve the pose-invariant face recognition performance.},
  archive      = {J_CVIU},
  author       = {Yanfei Liu and Junhua Chen},
  doi          = {10.1016/j.cviu.2022.103526},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103526},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised face frontalization using disentangled representation-learning CycleGAN},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling modality interactions for RGB-t salient object
detection. <em>CVIU</em>, <em>222</em>, 103514. (<a
href="https://doi.org/10.1016/j.cviu.2022.103514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB and thermal (RGB-T) salient object detection (SOD) techniques focus on investing multi-modality feature fusion strategies for capturing cross-modality complementary information within RGB and thermal images . However, most of these strategies do not allow explicitly extracting the interactions among the features of different modalities, thus leading to insignificant cross-modality complementary information exploitation. In this paper, we propose a novel RGB-T SOD model that alleviates this issue by leveraging a modality-aware and scale-aware feature fusion module. Such a module is capable of capturing the cross-modality complementary information by exploiting the interactions of single-modality features across modalities and the interactions of multi-modality features across scales. A stage-wise feature aggregation module is also proposed to thoroughly exploit the cross-level complementary information and reduce their redundancies for generating accurate saliency maps with sharp boundaries. To this end, a novel multi-level feature aggregation structure with two types of feature aggregation nodes is employed. Experimental results on several benchmark datasets verify the effectiveness and superiorities of our proposed model over some state-of-the-art models.},
  archive      = {J_CVIU},
  author       = {Qiang Zhang and Ruida Xi and Tonglin Xiao and Nianchang Huang and Yongjiang Luo},
  doi          = {10.1016/j.cviu.2022.103514},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103514},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enabling modality interactions for RGB-T salient object detection},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive CNN filter pruning using global importance metric.
<em>CVIU</em>, <em>222</em>, 103511. (<a
href="https://doi.org/10.1016/j.cviu.2022.103511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The depth and width of CNNs have increased over the years so as to learn a better representation of the input–output mapping of a dataset. However, a significant amount of redundancy exists among different convolutional kernels. Several methods on pruning suggest that trimming redundant parameters can produce compact structures with minor degradation in classification performance. Existing pruning methods reduce the number of filters at a uniform rate (i.e. pruning same percentage of filters from each layer) in every convolutional layer , which is suboptimal. In this paper, we conduct experiments to observe the sensitivity of each and every filter towards the final performance of the neural network . The essence of comparing filter importance on a global scale and subsequently pruning the neural network adaptively, is highlighted for the first time in this paper. Based on our observations, we propose a novel method named ‘Global Filter Importance based Adaptive Pruning (GFI-AP)’ that assigns importance scores to all filters based on how the network learns the input–output mapping of a dataset, which can then be compared across all the other convolutional filters . Our results show that non-uniform pruning achieves better compression as compared to uniform pruning. We demonstrate that GFI-AP significantly decreases the number of FLOPs (floating point operations) of VGG and ResNet networks in ImageNet and CIFAR datasets, without substantial drop in classification accuracy . GFI-AP reduces more number of FLOPs compared to existing pruning methods, for example, the ResNet50 variant of GFI-AP provides an additional 11\% reduction in FLOPs over Taylor-FO-BN-72\% while achieving higher accuracy.},
  archive      = {J_CVIU},
  author       = {Milton Mondal and Bishshoy Das and Sumantra Dutta Roy and Pushpendra Singh and Brejesh Lall and Shiv Dutt Joshi},
  doi          = {10.1016/j.cviu.2022.103511},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103511},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive CNN filter pruning using global importance metric},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SimpleCut: A simple and strong 2D model for multi-person
pose estimation. <em>CVIU</em>, <em>222</em>, 103509. (<a
href="https://doi.org/10.1016/j.cviu.2022.103509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a simple and efficient multi-person pose estimation model which follows a bottom-up approach and is based on a few deconvolutional layers added on a U-net lookalike ResNet featuremap. SimpleCut contains four independent modules: joints module, coordinates (coords) module, main-joint pairing module, and other-joints pairing module. The joints module builds a score on joints of each individual on the image, whereas the coords module encodes the location of those joints, and both the pairing modules generate image-conditioned pairing of the joints on a small scale. The pairing modules help set up the proposals into a variable number of consistent body part configurations by an optimization strategy that efficiently brings significant speed-up factors. We demonstrated that simultaneously inferring these bottom-up representations of detection and association encode global context sufficiently well to allow a greedy parse to attain high-quality results with low computational cost. SimpleCut evaluated on three publicly available large-scale dataset benchmarks such as ms-coco, lspet, and mpii human pose dataset.},
  archive      = {J_CVIU},
  author       = {Tewodros Legesse Munea and Chenhui Yang and Chenxi Huang and Mohammed A.M. Elhassan and Qingkai Zhen},
  doi          = {10.1016/j.cviu.2022.103509},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103509},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SimpleCut: A simple and strong 2D model for multi-person pose estimation},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual object tracking: A survey. <em>CVIU</em>,
<em>222</em>, 103508. (<a
href="https://doi.org/10.1016/j.cviu.2022.103508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is an important area in computer vision , and many tracking algorithms have been proposed with promising results. Existing object tracking approaches can be categorized into generative trackers, discriminative trackers, and collaborative trackers. Recently, object tracking algorithms based on deep neural networks have emerged and obtained great attention from researchers due to their outstanding tracking performance. To summarize the development of object tracking, a few surveys give analyses on either deep or non-deep trackers. In this paper, we provide a comprehensive overview of state-of-the-art tracking frameworks including both deep and non-deep trackers. We present both quantitative and qualitative tracking results of various trackers on five benchmark datasets and conduct a comparative analysis of their results. We further discuss challenging circumstances such as occlusion, illumination, deformation, and motion blur . Finally, we list the challenges and the future work in this fast-growing field.},
  archive      = {J_CVIU},
  author       = {Fei Chen and Xiaodong Wang and Yunxiang Zhao and Shaohe Lv and Xin Niu},
  doi          = {10.1016/j.cviu.2022.103508},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103508},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Visual object tracking: A survey},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-person 3D pose estimation from a single image captured
by a fisheye camera. <em>CVIU</em>, <em>222</em>, 103505. (<a
href="https://doi.org/10.1016/j.cviu.2022.103505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person 3D pose estimation with absolute depths for a fisheye camera is a challenging task but with valuable applications in daily life, especially for video surveillance. However, to the best of our knowledge, such problem has not been explored so far, leaving a gap in practical applications. In this work, we first propose a method for multi-person 3D pose estimation from a single image taken by a fisheye camera. Our method consists of two branches to estimate absolute 3D human poses: (1) a 2D-to-3D lifting module to predict root-relative 3D human poses (HPoseNet); (2) a root regression module to estimate absolute root locations in the camera coordinate (HRootNet). Finally, we propose a fisheye re-projection module without using ground-truth camera parameters to connect two branches, alleviating the impact of image distortions on 3D pose estimation and further regularizing prediction absolute 3D poses. Experimental results demonstrate that our method achieves the state-of-the-art performance on two public multi-person 3D pose datasets with synthetic fisheye images and our newly collected dataset with real fisheye images. The code and new dataset will be made publicly available.},
  archive      = {J_CVIU},
  author       = {Yahui Zhang and Shaodi You and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2022.103505},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103505},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-person 3D pose estimation from a single image captured by a fisheye camera},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta conditional variational auto-encoder for domain
generalization. <em>CVIU</em>, <em>222</em>, 103503. (<a
href="https://doi.org/10.1016/j.cviu.2022.103503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization has recently generated increasing attention in machine learning in that it tackles the challenging out-of-distribution problem. The huge domain shift from source domains to target domains induces great uncertainty in making predictions on the target domains to which no data is accessible during learning. In this paper, we propose meta conditional variational auto-encoder (Meta-CVAE), a new meta probabilistic latent variable framework for domain generalization. The Meta-CVAE can better model the uncertainty across domains by inheriting the strong ability of probabilistic modeling from VAE. By leveraging the meta-learning framework to mimic the generalization from source to target domains during learning, our Meta-CVAE learns to acquire the capability of generalization by episodically transferring knowledge across domains. Meta-CVAE is optimized with a variational objective based on a newly derived evidence lower bound under the meta-learning setting. To further enhance prediction performance, we develop the Wasserstein Meta-CVAE by imposing a Wasserstein distance based discriminative constraint on the latent representations, which essentially separate different classes in the semantic space. Extensive experiments on diverse benchmarks demonstrate that our methods outperforms previous approaches consistently, and comprehensive ablation studies further validate its effectiveness on domain generalization.},
  archive      = {J_CVIU},
  author       = {Zhiqiang Ge and Zhihuan Song and Xin Li and Lei Zhang},
  doi          = {10.1016/j.cviu.2022.103503},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103503},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Meta conditional variational auto-encoder for domain generalization},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Encoder and decoder network with ResNet-50 and global
average feature pooling for local change detection. <em>CVIU</em>,
<em>222</em>, 103501. (<a
href="https://doi.org/10.1016/j.cviu.2022.103501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is a prevalent way of dealing with detecting the local changes from video scenes. Background subtraction divides an image frame into foreground and background. The proposed scheme is a unique attempt to detect the local changes in video using a combination of the feature pooling module (FPM) with a ResNet-50 encoder–decoder network. In this context, we proposed a robust encoder–decoder structured deep learning network that is trained with limited training data. The proposed scheme has several folds of novelties including as mentioned below. The use of the feature pooling module with the ResNet-50 encoder–decoder network is the first attempt to use background subtraction in complex video scenes. In the proposed scheme the weights of the ResNet-50 network are learnt by using the transfer learning mechanism. Further, due to the use of a selected number of layers in ResNet-50 architecture with a fewer number of trainable parameters, the proposed architecture becomes less complex as compared to competitive architecture like VGG-16. The proposed ResNet-50 encoder with the FPM module is capable of extracting relevant multi-scale features for local change detection from complex videos. The said encoder uses residual connections between the layers and is hence capable of extracting meaningful multi-scale features with a fewer number of parameters and a higher number of layers. We finally used an up-sampling in the decoder to learn a mapping from the feature space to the image-frame space. The model takes an RGB image frame as the input and generates a foreground segmented probability mask for the corresponding image. To evaluate our model, we have tested it on the three popular benchmark databases. The robustness of the proposed scheme is evaluated by comparing its results with twenty-eight state-of-the-art techniques. The evaluation of the results is carried out using visual and eight quantitative evaluation measures.},
  archive      = {J_CVIU},
  author       = {Manoj Kumar Panda and Akhilesh Sharma and Vatsalya Bajpai and Badri Narayan Subudhi and Veerakumar Thangaraj and Vinit Jakhetiya},
  doi          = {10.1016/j.cviu.2022.103501},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103501},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Encoder and decoder network with ResNet-50 and global average feature pooling for local change detection},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frame-level refinement networks for skeleton-based gait
recognition. <em>CVIU</em>, <em>222</em>, 103500. (<a
href="https://doi.org/10.1016/j.cviu.2022.103500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait is considered as a promising biometric feature due to its support for long-distance and non-contact recognition. Most existing skeleton-based gait recognition methods deliver the same topology of skeleton graph for each frame and treat them equally in the process of temporal feature extraction and fusion, which undoubtedly limits the expressive capability of the model. In this paper, we propose a frame-level refinement network to adaptively learn specific topology in different frames and capture long-range dependencies between frames through transformer self-attention. Specifically, we design a frame-level topology refinement graph convolution (FTR-GC) to dynamically model different correlations between joints for each frame. In addition, we introduce transformer self-attention at the frame level, which can learn inter-frame long-range relations between the same joint. Finally, an attention-based frame-level feature aggregation module (FFAM) is presented to produce discriminative global features of input gait sequences. Experiments on the popular public dataset CASIA-B and OUMVLP-Pose show that our method notably surpasses state-of-the-art model-based methods, verifying the effectiveness of the proposed modules.},
  archive      = {J_CVIU},
  author       = {Likai Wang and Jinyan Chen and Yuxin Liu},
  doi          = {10.1016/j.cviu.2022.103500},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103500},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Frame-level refinement networks for skeleton-based gait recognition},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AWDMC-net: Classification of adversarial weather degraded
multiclass scenes using a convolution neural network. <em>CVIU</em>,
<em>222</em>, 103498. (<a
href="https://doi.org/10.1016/j.cviu.2022.103498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision systems in outdoor environments are strongly affected by different atmospheric/weather conditions. Therefore, understanding the actual behavior of outdoor scenes is necessary for effective removal and improvement of the overall performance of computer vision systems. Although the classification of atmospheric/weather conditions has been well explored, reporting on the same in multiclass problem using Convolutional Neural Networks (CNNs) has received very little attention. In response to address this disparity, we propose a new CNN architecture named the “Adversarial Weather Degraded Multi-class scenes Classification Network (AWDMC-Net)” for outdoor scene classification degraded by different atmospheric/weather conditions. The proposed network is based on adopting different combinations of skip connections in building blocks of CNN there after adaptively pruning the least important convolutional kernels from the network. For effective pruning, we proposed a new pruning criterion named “Entropy Guided Mean- l 1 l1 Norm” that can adaptively evaluate the importance of convolutional kernels by considering the filters and their corresponding output feature maps . The prediction performance of our proposed model was evaluated on our newly designed E-TUVD (Extended Tripura University Video Dataset) and on publicly available benchmark datasets. Our newly created video dataset, E-TUVD, consists of 147 video clips (approximately 793800 frames) that represent six atmospheric/weather conditions, namely, fog, dust, rain, haze, poor illumination, and clear day conditions. Our proposed model achieves an accuracy of 93.85\%, a specificity of 93.79\%, and a sensitivity of 94.18\% on our dataset, which outperforms the prevailing standard CNN models and recent state-of-the-art methods for atmospheric/weather classification tasks . Furthermore, our network also reduces the time consumption for atmospheric/weather classification tasks, and therefore mostly meets the requirements of practical applications in real-world scenarios.},
  archive      = {J_CVIU},
  author       = {Sourav Dey Roy and Mrinal Kanti Bhowmik},
  doi          = {10.1016/j.cviu.2022.103498},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103498},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AWDMC-net: Classification of adversarial weather degraded multiclass scenes using a convolution neural network},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physically-admissible polarimetric data augmentation for
road-scene analysis. <em>CVIU</em>, <em>222</em>, 103495. (<a
href="https://doi.org/10.1016/j.cviu.2022.103495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarimetric imaging, along with deep learning , has shown improved performances on different tasks including scene analysis . However, its robustness may be questioned because of the small size of the training datasets. Though the issue could be solved by data augmentation , polarization modalities are subject to physical feasibility constraints unaddressed by classical data augmentation techniques. To address this issue, we propose to use CycleGAN, an image translation technique based on deep generative models that solely relies on unpaired data, to transfer large labeled road scene datasets to the polarimetric domain. We design several auxiliary loss terms that, alongside the CycleGAN losses, deal with the physical constraints of polarimetric images. The efficiency of this solution is demonstrated on road scene object detection tasks where generated realistic polarimetric images allow to improve performances on cars and pedestrian detection up to 9\%. The resulting constrained CycleGAN is publicly released, allowing anyone to generate their own polarimetric images.},
  archive      = {J_CVIU},
  author       = {Cyprien Ruffino and Rachel Blin and Samia Ainouz and Gilles Gasso and Romain Hérault and Fabrice Meriaudeau and Stéphane Canu},
  doi          = {10.1016/j.cviu.2022.103495},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103495},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Physically-admissible polarimetric data augmentation for road-scene analysis},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning test-time augmentation for content-based image
retrieval. <em>CVIU</em>, <em>222</em>, 103494. (<a
href="https://doi.org/10.1016/j.cviu.2022.103494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-the-shelf convolutional neural network features achieve outstanding results in many image retrieval tasks. However, their invariance to target data is pre-defined by the network architecture and training data. Existing image retrieval approaches require fine-tuning or modification of pre-trained networks to adapt to variations unique to the target data. In contrast, our method enhances the invariance of off-the-shelf features by aggregating features extracted from images augmented at test-time, with augmentations guided by a policy learned through reinforcement learning . The learned policy assigns different magnitudes and weights to the selected transformations, which are selected from a list of image transformations. Policies are evaluated using a metric learning protocol to learn the optimal policy . The model converges quickly and the cost of each policy iteration is minimal as we propose an off-line caching technique to greatly reduce the computational cost of extracting features from augmented images. Experimental results on large trademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k and RParis6k scene datasets) tasks show that the learned ensemble of transformations is highly effective for improving performance, and is practical, and transferable.},
  archive      = {J_CVIU},
  author       = {Osman Tursun and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2022.103494},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103494},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning test-time augmentation for content-based image retrieval},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An asymmetrical-structure auto-encoder for unsupervised
representation learning of skeleton sequences. <em>CVIU</em>,
<em>222</em>, 103491. (<a
href="https://doi.org/10.1016/j.cviu.2022.103491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework for unsupervised representation learning using a structure-asymmetrical auto-encoder in which a 2D-CNN-based encoder learns separable spatiotemporal representations in a low-dimensional feature space under the supervision of salient skeleton motion cues. This study addresses the problem of learning action representations of skeleton sequences. The network captures not only correlations of adjacent joints but also long-term motion dependencies by using the proposed unsupervised training, which leads to the advantage that similar movements are gathered around the same cluster, whereas different movements are gathered around distinct clusters. Our method is unsupervised and does not rely on annotations to associate skeleton sequences with actions. Experimental results clearly showed the effectiveness of the proposed representation learning, and improvements compared with skeleton-based generative learning methods. When the proposed network was fine-tuned with partial labeled data, our results still outperformed some fully supervised methods.},
  archive      = {J_CVIU},
  author       = {Jiaxin Zhou and Takashi Komuro},
  doi          = {10.1016/j.cviu.2022.103491},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103491},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An asymmetrical-structure auto-encoder for unsupervised representation learning of skeleton sequences},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on RGB-d datasets. <em>CVIU</em>, <em>222</em>,
103489. (<a href="https://doi.org/10.1016/j.cviu.2022.103489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D data is essential for solving many problems in computer vision . Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.},
  archive      = {J_CVIU},
  author       = {Alexandre Lopes and Roberto Souza and Helio Pedrini},
  doi          = {10.1016/j.cviu.2022.103489},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103489},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A survey on RGB-D datasets},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi camera unsupervised domain adaptation pipeline for
object detection in cultural sites through adversarial learning and
self-training. <em>CVIU</em>, <em>222</em>, 103487. (<a
href="https://doi.org/10.1016/j.cviu.2022.103487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection algorithms allow to enable many interesting applications which can be implemented in different devices, such as smartphones and wearable devices . In the context of a cultural site, implementing these algorithms in a wearable device, such as a pair of smart glasses, allow to enable the use of augmented reality (AR) to show extra information about the artworks and enrich the visitors’ experience during their tour. However, object detection algorithms require to be trained on many well annotated examples to achieve reasonable results. This brings a major limitation since the annotation process requires human supervision which makes it expensive in terms of time and costs. A possible solution to reduce these costs consist in exploiting tools to automatically generate synthetic labeled images from a 3D model of the site. However, models trained with synthetic data do not generalize on real images acquired in the target scenario in which they are supposed to be used. Furthermore, object detectors should be able to work with different wearable devices or different mobile devices , which makes generalization even harder. In this paper, we present a new dataset collected in a cultural site to study the problem of domain adaptation for object detection in the presence of multiple unlabeled target domains corresponding to different cameras and a labeled source domain obtained considering synthetic images for training purposes. We present a new domain adaptation method which outperforms current state-of-the-art approaches combining the benefits of aligning the domains at the feature and pixel level with a self-training process. We release the dataset at the following link https://iplab.dmi.unict.it/OBJ-MDA/ and the code of the proposed architecture at https://github.com/fpv-iplab/STMDA-RetinaNet .},
  archive      = {J_CVIU},
  author       = {Giovanni Pasqualino and Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1016/j.cviu.2022.103487},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103487},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A multi camera unsupervised domain adaptation pipeline for object detection in cultural sites through adversarial learning and self-training},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-budget label query through domain alignment enforcement.
<em>CVIU</em>, <em>222</em>, 103485. (<a
href="https://doi.org/10.1016/j.cviu.2022.103485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning revolution happened thanks to the availability of a massive amount of labeled data which contributed to the development of models with extraordinary inference capabilities. Despite the public availability of large-scale datasets, to address specific requirements it is often necessary to generate a new set of labeled data whose production is often costly and require specific know-how to be fulfilled. In this work, we propose the new problem of low-budget label query , which aims at maximizing the classification performance by selecting a convenient and small set of samples ( i.e. , low budget) to be manually labeled from an arbitrary big set of unlabeled data . While a first solution might be the use of pre-trained models with standard selection metrics, i.e. confidence and entropy, we argue that domain shift affects their reliability. We deem that Unsupervised Domain Adaptation (UDA) can be used to reduce domain shift, making selection metrics more reliable and less noisy. Therefore, we first improve an UDA method to better align source and target domains using consistency constraints, reaching comparable performance with the state of-the-art on several UDA tasks. After adaptation, we conduct an extensive experimental study with commonly used confidence metrics and sampling strategies to achieve low-budget label query on a large variety of publicly available datasets and under different setups.},
  archive      = {J_CVIU},
  author       = {Cristiano Saltori and Paolo Rota and Nicu Sebe and Jurandy Almeida},
  doi          = {10.1016/j.cviu.2022.103485},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103485},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Low-budget label query through domain alignment enforcement},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient dual attention SlowFast networks for video action
recognition. <em>CVIU</em>, <em>222</em>, 103484. (<a
href="https://doi.org/10.1016/j.cviu.2022.103484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video data mainly differ in temporal dimension compared with static image data. Various video action recognition networks choose two-stream models to learn spatial and temporal information separately and fuse them to further improve performance. We proposed a cross-modality dual attention fusion module named CMDA to explicitly exchange spatial–temporal information between two pathways in two-stream SlowFast networks. Besides, considering the computational complexity of these heavy models and the low accuracy of existing lightweight models, we proposed several two-stream efficient SlowFast networks based on well-designed efficient 2D networks, such as GhostNet, ShuffleNetV2 and so on. Experiments demonstrate that our proposed fusion model CMDA improves the performance of SlowFast, and our efficient two-stream models achieve a consistent increase in accuracy with a little overhead in FLOPs . Our code and pre-trained models will be made available at https://github.com/weidafeng/Efficient-SlowFast .},
  archive      = {J_CVIU},
  author       = {Dafeng Wei and Ye Tian and Liqing Wei and Hong Zhong and Siqian Chen and Shiliang Pu and Hongtao Lu},
  doi          = {10.1016/j.cviu.2022.103484},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103484},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient dual attention SlowFast networks for video action recognition},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Animal pose estimation: A closer look at the
state-of-the-art, existing gaps and opportunities. <em>CVIU</em>,
<em>222</em>, 103483. (<a
href="https://doi.org/10.1016/j.cviu.2022.103483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, research on animal pose estimation in computer vision field has grown in many aspects such as 2D and 3D pose estimation, 3D mesh reconstruction, and behavior prediction. Promoted by deep learning , more and more animal pose estimation tools and animal pose datasets have also been made publicly available. However, compared to human pose estimation, which already has high accuracy and high applicability for complex scenes, animal pose estimation is still at a preliminary stage. The huge domain shift between each species, the scarce datasets, and uncooperative research subjects all pose intractable challenges to the development of robust and accurate animal pose estimation algorithms . In this review paper, we summarize the recent (from 2013 to 2021) work in animal pose estimation from computer vision perspective in order to present the state-of-the-art approaches and highlight the challenges they face in this field. We first categorize the various methods of animal pose estimation and present them according to several keywords. Also, we sort and introduce the released annotated image, video, and 3D models of animal poses as well as a promising substitute for real dataset. We also report the performances of the existing algorithms and visualize their results. Finally, we provide an in-depth analysis of the persisting obstacles in this field based on existing work, and offer potential solutions.},
  archive      = {J_CVIU},
  author       = {Le Jiang and Caleb Lee and Divyang Teotia and Sarah Ostadabbas},
  doi          = {10.1016/j.cviu.2022.103483},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103483},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Animal pose estimation: A closer look at the state-of-the-art, existing gaps and opportunities},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep convolutional correlation iterative particle filter for
visual tracking. <em>CVIU</em>, <em>222</em>, 103479. (<a
href="https://doi.org/10.1016/j.cviu.2022.103479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel framework for visual tracking based on the integration of an iterative particle filter, a deep convolutional neural network , and a correlation filter. The iterative particle filter enables the particles to correct themselves and converge to the correct target position. We employ a novel strategy to assess the likelihood of the particles after the iterations by applying K-means clustering. Our approach ensures a consistent support for the posterior distribution . Thus, we do not need to perform resampling at every video frame, improving the utilization of prior distribution information. Experimental results on three different benchmark datasets show that our tracker performs favorably against state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Reza Jalil Mozhdehi and Henry Medeiros},
  doi          = {10.1016/j.cviu.2022.103479},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103479},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep convolutional correlation iterative particle filter for visual tracking},
  volume       = {222},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantically accurate super-resolution generative
adversarial networks. <em>CVIU</em>, <em>221</em>, 103464. (<a
href="https://doi.org/10.1016/j.cviu.2022.103464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problems of semantic segmentation and image super-resolution by jointly considering the performance of both in training a Generative Adversarial Network (GAN). We propose a novel architecture and domain-specific feature loss, allowing super-resolution to operate as a pre-processing step to increase the performance of downstream computer vision tasks , specifically semantic segmentation. We demonstrate this approach using Nearmap’s aerial imagery dataset which covers hundreds of urban areas at 5–7 cm per pixel resolution. We show the proposed approach improves perceived image quality as well as quantitative segmentation accuracy across all prediction classes, yielding an average accuracy improvement of 11.8\% and 108\% at 4 × × and 32 × × super-resolution, compared with state-of-the art single-network methods. This work demonstrates that jointly considering image-based and task-specific losses can improve the performance of both, and advances the state-of-the-art in semantic-aware super-resolution of aerial imagery.},
  archive      = {J_CVIU},
  author       = {Tristan Frizza and Donald G. Dansereau and Nagita Mehr Seresht and Michael Bewley},
  doi          = {10.1016/j.cviu.2022.103464},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103464},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantically accurate super-resolution generative adversarial networks},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LocoGAN — locally convolutional GAN. <em>CVIU</em>,
<em>221</em>, 103462. (<a
href="https://doi.org/10.1016/j.cviu.2022.103462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose LocoGAN — a fully convolutional GAN model with latent space given by image-like noises of possibly different resolutions. Its learning procedure is local and processes not the whole image-like noises but only the sub-images of a fixed size. Consequently, LocoGAN produces images of arbitrary dimensions, which we present using the LSUN bedroom data set. By leveraging local learning and incorporating the position channels, our model gains an uncommon ability to generate fully periodic (e.g., cylindrical panoramic images) or almost periodic ”infinitely long” images.},
  archive      = {J_CVIU},
  author       = {Łukasz Struski and Szymon Knop and Przemysław Spurek and Wiktor Daniec and Jacek Tabor},
  doi          = {10.1016/j.cviu.2022.103462},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103462},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LocoGAN — locally convolutional GAN},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning the compositional domains for generalized zero-shot
learning. <em>CVIU</em>, <em>221</em>, 103454. (<a
href="https://doi.org/10.1016/j.cviu.2022.103454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of Generalized Zero-shot Learning (G-ZSL), whose goal is to classify instances from both seen and unseen classes at the test time. We propose a novel domain division method to solve G-ZSL. Some previous models with domain division operations only calibrate the confident prediction of source classes (W-SVM (Scheirer et al., 2014)) or take target-class instances as outliers (Socher et al., 2013). In contrast, we propose to directly estimate and fine-tune the decision boundary between the source and the target classes. Specifically, we put forward a framework that enables to learn compositional domains by splitting the instances into Source , Target , and Uncertain domains and perform recognition in each domain, where the uncertain domain contains instances whose labels cannot be confidently predicted. We use two statistical tools, namely, bootstrapping and Kolmogorov–Smirnov (K–S) Test, to learn the compositional domains for G-ZSL. We validate our method extensively on multiple G-ZSL benchmarks, on which it achieves state-of-the-art performances. The codes are available on https://github.com/hendrydong/demo_zsl_domain_division .},
  archive      = {J_CVIU},
  author       = {Hanze Dong and Yanwei Fu and Sung Ju Hwang and Leonid Sigal and Xiangyang Xue},
  doi          = {10.1016/j.cviu.2022.103454},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103454},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning the compositional domains for generalized zero-shot learning},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video captioning using semantically contextual generative
adversarial network. <em>CVIU</em>, <em>221</em>, 103453. (<a
href="https://doi.org/10.1016/j.cviu.2022.103453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a Semantically Contextual Generative Adversarial Network (SC-GAN) for video captioning. The semantic features extracted from a video are used in the discriminator to weigh the word embedding vectors. The weighted word embedding vectors along with the visual features are used to discriminate the ground truth descriptions from the descriptions generated by the generator. The manager in the generator uses the features from the discriminator to generate a goal vector for the worker. The worker is trained using: a goal based reward and a semantics based reward in generating the description. The semantics based reward ensures that the worker generates descriptions that incorporate the semantic features. The goal based reward calculated from discriminator features ensures the generation of descriptions similar to the ground truth descriptions. We have used MSVD and MSR-VTT datasets to demonstrate the effectiveness of the proposed approach to video captioning.},
  archive      = {J_CVIU},
  author       = {Hemalatha Munusamy and Chandra Sekhar C.},
  doi          = {10.1016/j.cviu.2022.103453},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103453},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video captioning using semantically contextual generative adversarial network},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Camouflaged object detection via neighbor connection and
hierarchical information transfer. <em>CVIU</em>, <em>221</em>, 103450.
(<a href="https://doi.org/10.1016/j.cviu.2022.103450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) aims to detect objects with high similarity to the background. Unlike general object detection, COD is a more challenging task because the target boundaries are vague and the location is difficult to determine. In this paper, we propose a novel COD framework, which consists of two main components, namely, Neighbor Connection Mode (NCM) and Hierarchical Information Transfer (HIT). NCM aggregates the features from the neighboring layers of the encoder network to enhance the complementation of various level information. Our NCM not only reduces the burden of dense connection that consumes a lot of computing memory and redundant features but also weakens the phenomenon of the long-term transmission of context. We also propose a HIT module to transfer the features of different dilated rates inside each level hierarchically, which expands the receptive field of each branch and enhances the relationship between different features. Our method accurately detects camouflaged objects by considering full level information and a large receptive field. The experiments on three COD datasets show that our model achieves state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Cong Zhang and Kang Wang and Hongbo Bi and Ziqi Liu and Lina Yang},
  doi          = {10.1016/j.cviu.2022.103450},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103450},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Camouflaged object detection via neighbor connection and hierarchical information transfer},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-aware consistency regularization for
cross-domain semantic segmentation. <em>CVIU</em>, <em>221</em>, 103448.
(<a href="https://doi.org/10.1016/j.cviu.2022.103448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to adapt existing models of the source domain to a new target domain with only unlabeled data . Most existing methods suffer from noticeable negative transfer resulting from either the error-prone discriminator network or the unreasonable teacher model. Besides, the local regional consistency in UDA has been largely neglected, and only extracting the global-level pattern information is not powerful enough for feature alignment due to the abuse use of contexts. To this end, we propose an uncertainty-aware consistency regularization method for cross-domain semantic segmentation . Firstly, we introduce an uncertainty-guided consistency loss with a dynamic weighting scheme by exploiting the latent uncertainty information of the target samples. As such, more meaningful and reliable knowledge from the teacher model can be transferred to the student model. We further reveal the reason why the current consistency regularization is often unstable in minimizing the domain discrepancy. Besides, we design a ClassDrop mask generation algorithm to produce strong class-wise perturbations. Guided by this mask, we propose a ClassOut strategy to realize effective regional consistency in a fine-grained manner. Experiments demonstrate that our method outperforms the state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV → → Cityscapes, SYNTHIA → → Cityscapes, Virtual KITTI ⟶ ⟶ KITTI and Cityscapes ⟶ ⟶ KITTI.},
  archive      = {J_CVIU},
  author       = {Qianyu Zhou and Zhengyang Feng and Qiqi Gu and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
  doi          = {10.1016/j.cviu.2022.103448},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103448},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Uncertainty-aware consistency regularization for cross-domain semantic segmentation},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIFNet: Free-form image inpainting using color
split-inpaint-fuse approach. <em>CVIU</em>, <em>221</em>, 103446. (<a
href="https://doi.org/10.1016/j.cviu.2022.103446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning-based approaches have shown outstanding performance in generating visually plausible and refined contents for the missing regions in free-form image inpainting tasks. However, most of the existing methods employ a coarse-to-refine approach where the refinement process depends on a single coarse estimation, often leading to texture and structure inconsistencies. Though several existing methods focus on incorporating additional inputs to mitigate this problem, no learning-based studies have investigated the effects of decomposing input corrupted image into luma and chroma images and performing decoupled inpainting of the decomposed components. To this end, we propose a Split-Inpaint-Fuse Network (SIFNet), an end-to-end two-stage inpainting approach that uses a split-inpaint sub-network for separately inpainting the corrupted luma and chroma images using two decoupled branches in the coarse stage and a fusion sub-network for fusing the inpainted luma and chroma images into a refined image in the refinement stage. Additionally, we propose two attention mechanisms for the coarse stage – a progressive context module to find the patch-level feature similarity for the luma image reconstruction and a spatial-channel context module to find important spatial and channel features for the chroma image reconstruction. Experimental results reveal that our Split-Inpaint-Fuse approach outperforms the existing inpainting methods by comparative margins. In addition, extensive ablation studies confirm the effectiveness of the proposed approach, constituting modules and architectural choices .},
  archive      = {J_CVIU},
  author       = {S.M. Nadim Uddin and Yong Ju Jung},
  doi          = {10.1016/j.cviu.2022.103446},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103446},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SIFNet: Free-form image inpainting using color split-inpaint-fuse approach},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anchor pruning for object detection. <em>CVIU</em>,
<em>221</em>, 103445. (<a
href="https://doi.org/10.1016/j.cviu.2022.103445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes anchor pruning for object detection in one-stage anchor-based detectors. While pruning techniques are widely used to reduce the computational cost of convolutional neural networks , they tend to focus on optimizing the backbone networks where often most computations are. In this work we demonstrate an additional pruning technique, specifically for object detection: anchor pruning. With more efficient backbone networks and a growing trend of deploying object detectors on embedded systems where post-processing steps such as non-maximum suppression can be a bottleneck, the impact of the anchors used in the detection head is becoming increasingly more important. In this work, we show that many anchors in the object detection head can be removed without any loss in accuracy. With additional retraining, anchor pruning can even lead to improved accuracy. Extensive experiments on SSD and MS COCO show that the detection head can be made up to 44\% more efficient while simultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL VOC show the general effectiveness of our approach. We also introduce ‘overanchorized’ models that can be used together with anchor pruning to eliminate hyperparameters related to the initial shape of anchors. Code and models are available at .},
  archive      = {J_CVIU},
  author       = {Maxim Bonnaerens and Matthias Freiberger and Joni Dambre},
  doi          = {10.1016/j.cviu.2022.103445},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103445},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Anchor pruning for object detection},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deducing health cues from biometric data. <em>CVIU</em>,
<em>221</em>, 103438. (<a
href="https://doi.org/10.1016/j.cviu.2022.103438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical diagnosis involves the expert opinion of trained health care professionals based on causal inference from medical data. While medical data are typically collected using specialized medical-grade sensors, similar data characteristics useful for medical diagnosis are sometimes present in biometric data (e.g., face images, ocular images, and speech signals). In this paper, we explore the biometrics and medical literature to study the following questions. 1) What kind of health cues are embedded in the commonly utilized forms of audio-visual biometric data? 2) How can these health cues be gleaned from the biometric data, and what kind of diseases can it help diagnose? 3) What are some of the implications of using biometric data for medical diagnosis?},
  archive      = {J_CVIU},
  author       = {Arun Ross and Sudipta Banerjee and Anurag Chowdhury},
  doi          = {10.1016/j.cviu.2022.103438},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103438},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deducing health cues from biometric data},
  volume       = {221},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-human fall detection and localization in videos.
<em>CVIU</em>, <em>220</em>, 103442. (<a
href="https://doi.org/10.1016/j.cviu.2022.103442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a delay in the exploration of the benefits of deep learning for human action and activity recognition applications. Within these fields, the detection of falls attracts attention due to its excellent public utility. Fall detection can be implemented in facilities such as nursing homes, areas with public cameras, and the homes of older people who live alone, as the vast majority of fatalities related to falls occur in these locations. The YOLO object detection algorithm is combined with temporal classification models and the Kalman filter tracking algorithm, which are used to detect falls individually on video streams. The following steps are taken when the proposed approach is used: (i) the region of the image in which the fall occurred is located; (ii) the features that comprise the fall in a temporal sequence of images are tracked and extracted, and a series of actions associated with a given person are formed; and (iii) a model is built to classify the consecutive sequence of images and aggregate the temporal information. Based on these steps, two versions of the proposed approach, YOLOK+3DCNN and YOLOK+2DCNN+LSTM, are created. The proposed model is compared with other methods in the literature using well-known metrics. Experimental simulations using a custom dataset and state-of-the-art models show that the best results in most evaluated metrics are achieved using the proposed approach.},
  archive      = {J_CVIU},
  author       = {Mouglas Eugênio Nasário Gomes and David Macêdo and Cleber Zanchettin and Paulo Salgado Gomes de-Mattos-Neto and Adriano Oliveira},
  doi          = {10.1016/j.cviu.2022.103442},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103442},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-human fall detection and localization in videos},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised learning of multi-object 3D scene
decompositions using deep shape priors. <em>CVIU</em>, <em>220</em>,
103440. (<a href="https://doi.org/10.1016/j.cviu.2022.103440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing scenes at the granularity of objects is a prerequisite for scene understanding and decision making. We propose PriSMONet, a novel approach based on Pri or S hape knowledge for learning M ulti- O bject 3D scene decomposition and representations from single images. Our approach learns to decompose images of synthetic scenes with multiple objects on a planar surface into its constituent scene objects and to infer their 3D properties from a single view. A recurrent encoder regresses a latent representation of 3D shape, pose and texture of each object from an input RGB image . By differentiable rendering , we train our model to decompose scenes from RGB-D images in a self-supervised way. The 3D shapes are represented continuously in function-space as signed distance functions which we pre-train from example shapes in a supervised way. These shape priors provide weak supervision signals to better condition the challenging overall learning task. We evaluate the accuracy of our model in inferring 3D scene layout, demonstrate its generative capabilities, assess its generalization to real images, and point out benefits of the learned representation.},
  archive      = {J_CVIU},
  author       = {Cathrin Elich and Martin R. Oswald and Marc Pollefeys and Joerg Stueckler},
  doi          = {10.1016/j.cviu.2022.103440},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103440},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised learning of multi-object 3D scene decompositions using deep shape priors},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Are 3D convolutional networks inherently biased towards
appearance? <em>CVIU</em>, <em>220</em>, 103437. (<a
href="https://doi.org/10.1016/j.cviu.2022.103437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D convolutional networks , as direct inheritors of 2D convolutional networks for images, have placed their mark on action recognition in videos. Combined with pretraining on large-scale video data, high classification accuracies have been obtained on numerous video benchmarks. In an effort to better understand why 3D convolutional networks are so effective, several works have highlighted their bias towards static appearance and towards the scenes in which actions occur. In this work, we seek to find the source of this bias and question whether the observed biases towards static appearances are inherent to 3D convolutional networks or represent limited significance of motion in the training data. We resolve this by presenting temporality measures that estimate the data-to-model motion dependency at both the layer-level and the kernel-level. Moreover, we introduce two synthetic datasets where motion and appearance are decoupled by design, which allows us to directly observe their effects on the networks. Our analysis shows that 3D architectures are not inherently biased towards appearance. When trained on the most prevalent video sets, 3D convolutional networks are indeed biased throughout, especially in the final layers of the network. However, when training on data with motions and appearances explicitly decoupled and balanced, such networks adapt to varying levels of temporality. To this end, we see the proposed measures as a reliable method to estimate motion relevance for activity classification in datasets and use them to uncover the differences between popular pre-training video collections, such as Kinetics, IG-65M and Howto100 m.},
  archive      = {J_CVIU},
  author       = {Petr Byvshev and Pascal Mettes and Yu Xiao},
  doi          = {10.1016/j.cviu.2022.103437},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103437},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Are 3D convolutional networks inherently biased towards appearance?},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse coding and normalization for deep fisher score
representation. <em>CVIU</em>, <em>220</em>, 103436. (<a
href="https://doi.org/10.1016/j.cviu.2022.103436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher scores have been shown to be accurate global image features for classification. However, their performance is very dependent on the quality of the input features as well as the normalization steps applied to them. In this paper, we propose to embed the Fisher scores in an end-to-end trainable deep network by concentrating on two elements: adapting the encoding to the deep features and normalizing the extracted second-order statistics. Therefore, we make use of a deep sparse coding module that allows to sample the center of each Gaussian function from a learned subspace and thus to better fit the high dimensional data distribution. Second, we introduce a new normalization module that computes an approximate square root matrix normalization well adapted to the Fisher scores. These processing steps are embedded in a deep network so that all the modules work together for the sole purpose of improving classification performance. Experimental results show that this solution outperforms many alternatives in the context of material, indoor scene and fine-grained image classification .},
  archive      = {J_CVIU},
  author       = {Sixiang Xu and Damien Muselet and Alain Trémeau},
  doi          = {10.1016/j.cviu.2022.103436},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103436},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Sparse coding and normalization for deep fisher score representation},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFCNet: Enhancing urban segmentation using regularization,
fusion, and completion. <em>CVIU</em>, <em>220</em>, 103435. (<a
href="https://doi.org/10.1016/j.cviu.2022.103435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a fundamental task that has benefited from recent advances in machine learning . One type of segmentation, of particular interest to computer vision , is that of urban segmentation. Although recent solutions have leveraged on deep neural networks , approaches usually do not consider regularities appearing in facade structures (e.g., windows are often in groups of similar alignment, size, or spacing patterns) as well as additional urban structures such as building footprints and roofs. Moreover, both satellite and street-view images are often noisy and occluded, thus getting the complete structure segmentation from a partial observation is difficult. Our key observations are that facades and other urban structures exhibit regular structures, and additional views are often available. In this paper, we present a novel framework ( RFCNet ) that consists of three modules to achieve multiple goals. Specifically, we propose Regularization to improve the regularities given an initial segmentation, Fusion that fuses multiple views of the segmentation, and Completion that can infer the complete structure if necessary. Experimental results show that our method outperforms previous state-of-the-art methods quantitatively and qualitatively for multiple facade datasets. Furthermore, by applying our framework to other urban structures (e.g., building footprints and roofs), we demonstrate our approach can be generalized to various pattern types.},
  archive      = {J_CVIU},
  author       = {Xiaowei Zhang and Daniel Aliaga},
  doi          = {10.1016/j.cviu.2022.103435},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103435},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RFCNet: Enhancing urban segmentation using regularization, fusion, and completion},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video frame interpolation via down–up scale generative
adversarial networks. <em>CVIU</em>, <em>220</em>, 103434. (<a
href="https://doi.org/10.1016/j.cviu.2022.103434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frame interpolation finds many applications in video applications, including frame rate up-conversion and video compression . Deep learning-based methods have been proposed for frame interpolation, but a long runtime is typically required to achieve good visual quality. In this paper, we introduce an efficient frame interpolation method based on a modified generative adversarial network . The proposed framework consists of a generator with a pair of down–up scale modules, where the down-scaled-input module attempts to capture the overall structure of the scene while the original-scale-input module aims to restore finer textures. Skip connections and an input processing block are further incorporated into the minimal two-scale generator design to expedite processing without losing image features . The difference between the synthesized frame and the ground truth is measured by a combined loss function, including one adversarial loss and three reconstruction losses. Compared to the state-of-the-art motion compensation and deep-learning based frame interpolation approaches, the proposed framework achieves the most satisfactory trade-off between the synthesis quality and runtime.},
  archive      = {J_CVIU},
  author       = {Quang Nhat Tran and Shih-Hsuan Yang},
  doi          = {10.1016/j.cviu.2022.103434},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103434},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video frame interpolation via down–up scale generative adversarial networks},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STURE: Spatial–temporal mutual representation learning for
robust data association in online multi-object tracking. <em>CVIU</em>,
<em>220</em>, 103433. (<a
href="https://doi.org/10.1016/j.cviu.2022.103433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-object tracking (MOT) is a longstanding task for computer vision and intelligent vehicle platform. At present, the main paradigm is tracking-by-detection, and the main difficulty of this paradigm is how to associate current candidate detections with historical tracklets. However, in the MOT scenarios, each historical tracklet is composed of an object sequence, while each candidate detection is just a flat image, which lacks temporal features of the object sequence. The feature difference between current candidate detections and historical tracklets makes the object association much harder. Therefore, we propose a Spatial–Temporal Mutual Representation Learning (STURE) approach which learns spatial–temporal representations between current candidate detections and historical sequences in a mutual representation space. For historical tracklets, the detection learning network is forced to match the representations of sequence learning network in a mutual representation space. The proposed approach is capable of extracting more distinguishing detection and sequence representations by using various designed losses in object association. As a result, spatial–temporal feature is learned mutually to reinforce the current detection features, and the feature difference can be relieved. To prove the robustness of the STURE, it is applied to the public MOT challenge benchmarks and performs well compared with various state-of-the-art online MOT trackers based on identity-preserving metrics.},
  archive      = {J_CVIU},
  author       = {Haidong Wang and Zhiyong Li and Yaping Li and Ke Nai and Ming Wen},
  doi          = {10.1016/j.cviu.2022.103433},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103433},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {STURE: Spatial–Temporal mutual representation learning for robust data association in online multi-object tracking},
  volume       = {220},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-parametric scene parsing: Label transfer methods and
datasets. <em>CVIU</em>, <em>219</em>, 103418. (<a
href="https://doi.org/10.1016/j.cviu.2022.103418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene parsing is the problem of densely labeling every pixel in an image with a meaningful class label. Driven by powerful methods, remarkable progress has been achieved in scene parsing over a short period of time. With growing data, non-parametric scene parsing or label transfer approach has emerged as an exciting and rapidly growing research area within Computer Vision . This paper constitutes a first survey examining label transfer methods through the lens of non-parametric, data-driven philosophy. We provide insights on non-parametric system design and its working stages, i.e. algorithmic components such as scene retrieval, scene correspondence, contextual smoothing, etc. We propose a synthetic categorization of all the major existing methods, discuss the necessary background, the design choices, followed by an overview of the shortcomings and challenges for a better understanding of label transfer. In addition, we introduce the existing standard benchmark datasets, the evaluation metrics , and the comparisons of model-based and data-driven methods. Finally, we provide our recommendations and discuss the current challenges and promising research directions in the field.},
  archive      = {J_CVIU},
  author       = {Alexy Bhowmick and Sarat Saharia and Shyamanta M. Hazarika},
  doi          = {10.1016/j.cviu.2022.103418},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103418},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Non-parametric scene parsing: Label transfer methods and datasets},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting abnormality with separated foreground and
background: Mutual generative adversarial networks for video abnormal
event detection. <em>CVIU</em>, <em>219</em>, 103416. (<a
href="https://doi.org/10.1016/j.cviu.2022.103416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most important tasks in intelligent video analysis, video abnormal event detection has been extensively studied. Prior arts have made a great process in designing frameworks to capture spatio-temporal features of video frames. However, video frames usually consist of various objects. It is challenging to grasp the nuances of anomalies against noisy backgrounds. To tackle the bottleneck, we propose a novel Foreground–Background Separation Mutual Generative Adversarial Network (FSM-GAN) framework. The FSM-GAN permits the separation of video frames into the foreground and background. The separated foreground and background are utilized as the input of mutual generative adversarial networks, which transform raw-pixel images in optical-flow representations and vice versa. In the networks, the background is regarded as known conditions and the model focuses on learning the high-level spatio-temporal foreground features to represent the event with the given conditions during the mutual adversarial training . In the test stage, these high-level features instead of low-level visual primitives are utilized to measure the abnormality in the semantic level . Compared with state-of-the-art methods and other abnormal event detection approaches, the proposed framework demonstrates its effectiveness and reliability across various scenes and events.},
  archive      = {J_CVIU},
  author       = {Zhi Zhang and Sheng-hua Zhong and Ahmed Fares and Yan Liu},
  doi          = {10.1016/j.cviu.2022.103416},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103416},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Detecting abnormality with separated foreground and background: Mutual generative adversarial networks for video abnormal event detection},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A non-alternating graph hashing algorithm for large-scale
image search. <em>CVIU</em>, <em>219</em>, 103415. (<a
href="https://doi.org/10.1016/j.cviu.2022.103415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, methods for improving memory and computational efficiency have become crucial for the successful deployment of technology. Hashing is one of the most effective approaches to deal with the computational limitations associated with big data. One natural way to formulate this problem is spectral hashing, which directly incorporates an affinity to learning binary codes . However, owing to the binary constraints , the optimization becomes intractable. To mitigate this challenge, different relaxation approaches have been proposed to reduce the computational load required to obtain binary codes and still attain a good solution. The problem with all existing relaxation methods involves the use of one or more additional auxiliary variables to attain high-quality binary codes while relaxing the problem. The existence of auxiliary variables leads to the coordinate descent approach, which increases the computational complexity . We argue that the introduction of these variables is unnecessary. To this end, we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore, instead of solving the problem in the original space where the number of variables is equal to the data points, we solve the problem in a much smaller space and retrieve the binary codes from this solution. This technique reduces both the memory and computational complexity simultaneously. We apply two optimization techniques, namely, the projected gradient and optimization on the manifold, to obtain the solution. Using comprehensive experiments on four public datasets, we show that the proposed efficient spectral hashing (ESH) algorithm achieves a highly competitive retrieval performance compared with the state-of-the-art algorithms at low complexity.},
  archive      = {J_CVIU},
  author       = {Sobhan Hemati and Mohammad Hadi Mehdizavareh and Shojaeddin Chenouri and Hamid R. Tizhoosh},
  doi          = {10.1016/j.cviu.2022.103415},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103415},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A non-alternating graph hashing algorithm for large-scale image search},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning geodesic-aware local features from RGB-d images.
<em>CVIU</em>, <em>219</em>, 103409. (<a
href="https://doi.org/10.1016/j.cviu.2022.103409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing handcrafted and learning-based local descriptors are still at best approximately invariant to affine image transformations, often disregarding deformable surfaces. In this paper, we take one step further by proposing a new approach to compute descriptors from RGB-D images (where RGB refers to the pixel color brightness and D stands for depth information) that are invariant to isometric non-rigid deformations, as well as to scale changes and rotation. Our proposed description strategies are grounded on the key idea of learning feature representations on undistorted local image patches using surface geodesics. We design two complementary local descriptors strategies to compute geodesic-aware features efficiently: one efficient binary descriptor based on handcrafted binary tests (named GeoBit), and one learning-based descriptor (GeoPatch) with convolutional neural networks (CNNs) to compute features. In different experiments using real and publicly available RGB-D data benchmarks, they consistently outperforms state-of-the-art handcrafted and learning-based image and RGB-D descriptors in matching scores, as well as in object retrieval and non-rigid surface tracking experiments, with comparable processing times. We also provide to the community a new dataset with accurate matching annotations of RGB-D images of different objects (shirts, cloths, paintings, bags), subjected to strong non-rigid deformations, for evaluation benchmark of deformable surface correspondence algorithms.},
  archive      = {J_CVIU},
  author       = {Guilherme Potje and Renato Martins and Felipe Cadar and Erickson R. Nascimento},
  doi          = {10.1016/j.cviu.2022.103409},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103409},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning geodesic-aware local features from RGB-D images},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TCLR: Temporal contrastive learning for video
representation. <em>CVIU</em>, <em>219</em>, 103406. (<a
href="https://doi.org/10.1016/j.cviu.2022.103406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations, and has also been explored for videos. However, prior work on contrastive learning for video data has not explored the effect of explicitly encouraging the features to be distinct across the temporal dimension. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The local–local temporal contrastive loss adds the task of discriminating between non-overlapping clips from the same video, whereas the global–local temporal contrastive aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the learned features. Our proposed temporal contrastive learning framework achieves significant improvement over the state-of-the-art results in various downstream video understanding tasks such as action recognition, limited-label action classification , and nearest-neighbor video retrieval on multiple video datasets and backbones. We also demonstrate significant improvement in fine-grained action classification for visually similar classes. With the commonly used 3D ResNet-18 architecture with UCF101 pretraining, we achieve 82.4\% (+5.1\% increase over the previous best) top-1 accuracy on UCF101 and 52.9\% (+5.4\% increase) on HMDB51 action classification, and 56.2\% (+11.7\% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval. Code released at https://github.com/DAVEISHAN/TCLR .},
  archive      = {J_CVIU},
  author       = {Ishan Dave and Rohit Gupta and Mamshad Nayeem Rizve and Mubarak Shah},
  doi          = {10.1016/j.cviu.2022.103406},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103406},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TCLR: Temporal contrastive learning for video representation},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative semantic transitive consistency for
cross-modal learning. <em>CVIU</em>, <em>219</em>, 103404. (<a
href="https://doi.org/10.1016/j.cviu.2022.103404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval is generally performed by projecting and aligning the data from two different modalities onto a shared representation space. This shared space often also acts as a bridge for translating the modalities. We address the problem of learning such representation space by proposing and exploiting the property of Discriminative Semantic Transitive Consistency —ensuring that the data points are correctly classified even after being transferred to the other modality. Along with semantic transitive consistency, we also enforce the traditional distance minimizing constraint which makes the projections of the corresponding data points from both the modalities to come closer in the representation space. We analyze and compare the contribution of both the loss terms and their interaction, for the task. In addition, we incorporate semantic cycle-consistency for each of the modality. We empirically demonstrate better performance owing to the different components with clear ablation studies. We also provide qualitative results to support the proposals.},
  archive      = {J_CVIU},
  author       = {Kranti Kumar Parida and Gaurav Sharma},
  doi          = {10.1016/j.cviu.2022.103404},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103404},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Discriminative semantic transitive consistency for cross-modal learning},
  volume       = {219},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot sketch-based image retrieval with structure-aware
asymmetric disentanglement. <em>CVIU</em>, <em>218</em>, 103412. (<a
href="https://doi.org/10.1016/j.cviu.2022.103412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Sketch-Based Image Retrieval (SBIR) is using free-hand sketches to retrieve images of the same category from a natural image gallery. However, SBIR requires all test categories to be seen during training, which cannot be guaranteed in real-world applications. So we investigate more challenging Zero-Shot SBIR (ZS-SBIR), in which test categories do not appear in the training stage. After realizing that sketches mainly contain structure information while images contain additional appearance information, we attempt to achieve structure-aware retrieval via asymmetric disentanglement. For this purpose, we propose our STRucture-aware Asymmetric Disentanglement (STRAD) method, in which image features are disentangled into structure features and appearance features while sketch features are only projected to structure space. Through disentangling structure and appearance space, bi-directional domain translation is performed between the sketch domain and the image domain. Extensive experiments demonstrate that our STRAD method remarkably outperforms state-of-the-art methods on three large-scale benchmark datasets.},
  archive      = {J_CVIU},
  author       = {Jiangtong Li and Zhixin Ling and Li Niu and Liqing Zhang},
  doi          = {10.1016/j.cviu.2022.103412},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103412},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Zero-shot sketch-based image retrieval with structure-aware asymmetric disentanglement},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An anchor-free object detector based on soften optimized
bi-directional FPN. <em>CVIU</em>, <em>218</em>, 103410. (<a
href="https://doi.org/10.1016/j.cviu.2022.103410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an anchor-free object detector that combines a weighted bi-directional Feature Pyramid Network (BiFPN) and Soft Anchor Point Detector to address the object detection problem in a pixel-wise paradigm. The current mainstream object detection methods are anchor-based, which require to set hyper parameters such as scale and aspect ratio. This requires strong prior knowledge and can be difficult to design. Therefore, we propose an anchor-free detector that completely avoids the complex calculations and all the hyper parameters related to the anchor box by eliminating the predefined set of anchor boxes in an anchor-free way. Anchor-free detectors are essentially dense prediction methods. Although the huge solution space can yield high recall, simple anchor-free methods tend to return too many false positives , which leads to the problem of semantic ambiguity caused by the high overlap of object centers. Therefore, we propose BiFPN to alleviate the impact of high overlap which also effectively addresses the problems related to multi-scale features. Moreover, in order to utilize the power of feature pyramid better, we tackle the issues with a novel training strategy that involves two soften optimization techniques, i.e., soft-weighted anchor points and soft-selected pyramid levels. This training strategy further re-weights the quality of the detection results to make our detection results more stable.},
  archive      = {J_CVIU},
  author       = {Tao Zhang and Bo Jin and Wenjing Jia},
  doi          = {10.1016/j.cviu.2022.103410},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103410},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An anchor-free object detector based on soften optimized bi-directional FPN},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised fine-grained image classification via
two-level attention activation model. <em>CVIU</em>, <em>218</em>,
103408. (<a href="https://doi.org/10.1016/j.cviu.2022.103408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image classification is challenging due to the fact that categories can only be distinguished by subtle and local differences. Existing weakly supervised fine-grained image classification methods usually directly extract discriminative regions from the high-level feature maps. We observe that the operation of overlaying local receptive fields in the convolutional neural network makes the discriminative regions spread in the high-level feature maps, which can cause inaccurate region localization . In this paper, we propose an end-to-end Two-Level Attention Activation Model (TL-AAM), which can solve the problem of discriminative region spreading and obtain more effective fine-grained features. Specifically, the TL-AAM consists of: (1) an object attention activation module (OAAM), which links the correct classification score with the object region localization through gradient reflow to accurately localize the object region in a mutually reinforcing way, (2) a multi-scale pyramid attention localization module (MPALM), which locates local feature region by selecting the region with the largest response value in the feature channel, and this module can accurately obtain the detailed features in the local region, (3) a local cross-channel attention module (LCAM), which can filter irrelevant information in the high-level semantic feature maps by giving higher weights to the feature channels with high response values in the feature maps. Extensive experiments verify that TL-AAM yields the state-of-the-art performance under the same settings with the most competitive approaches, in CUB-200-2011, FGVC-Aircrafts, and Stanford-Cars datasets.},
  archive      = {J_CVIU},
  author       = {Xiao Ke and Yanyan Huang and WenZhong Guo},
  doi          = {10.1016/j.cviu.2022.103408},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103408},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised fine-grained image classification via two-level attention activation model},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CUFD: An encoder–decoder network for visible and infrared
image fusion based on common and unique feature decomposition.
<em>CVIU</em>, <em>218</em>, 103407. (<a
href="https://doi.org/10.1016/j.cviu.2022.103407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for visible and infrared image fusion by decomposing feature information, which is termed as CUFD. It adopts two pairs of encoder–decoder networks to implement feature map extraction and decomposition, respectively. On the one hand, the shallow features of the image contain abundant information while the deep features focus more on extracting the thermal targets. Thus, we use an encoder–decoder network to extract both shallow and deep features. Unlike existing methods, both of the shallow and deep features are used for fusion and reconstruction with different emphases. On the other hand, the infrared and visible features of the same layer have both similarities and differences. Therefore, we train the other encoder–decoder network to decompose the feature maps into common and unique information based on their similarities and differences. After that, we apply different fusion rules according to the flexible requirements. This operation is more beneficial to retain the significant feature information in the fusion results. Qualitative and quantitative experiments on publicly available TNO and RoadScene datasets demonstrate the superiority of our CUFD over the state-of-the-art.},
  archive      = {J_CVIU},
  author       = {Han Xu and Meiqi Gong and Xin Tian and Jun Huang and Jiayi Ma},
  doi          = {10.1016/j.cviu.2022.103407},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103407},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CUFD: An encoder–decoder network for visible and infrared image fusion based on common and unique feature decomposition},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive capsule network. <em>CVIU</em>, <em>218</em>,
103405. (<a href="https://doi.org/10.1016/j.cviu.2022.103405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Typically, the capsule is produced by applying convolution layers called the primary capsule layer to group scalar neurons. However, randomly grouping the scalar neurons into capsule vectors can cause two problems: (i) The capsule vectors are difficult to obtain a better representation of the entities. (ii) The capsule vectors generated by the primary capsule layer lack spatial information and cannot effectively model the underlying spatial relationship among entities. In this paper, we present a flexible and efficient capsule network architecture called Adaptive Capsule CapsuleNet (AC-CapsNet) . We replace the primary capsule layer of CapsNet with the adaptive capsule (AC) layer. In the AC-CapsNet, the adaptive capsule vector combines capsule vector and adaptive value generated by the AC layer. The adaptive value preserves spatial information of each capsule vector and local relationship among the scalar neurons contained in each capsule vector. Therefore, the adaptive capsule vector can not only dynamically adjust their state values according to the content information of scalar neurons inside capsule vector, but also model the spatial relationship between capsule vectors for low-level clusters. Extensive experiments in some public datasets such as CIFAR-100, CIFAR-10, SmallNORB, and SVHN show that the AC-CapsNet outperforms other variants of CapsNets with respect to classification accuracy and robustness to affine transformations and white-box adversarial attacks .},
  archive      = {J_CVIU},
  author       = {Jianwei Tao and Xiankun Zhang and Xuexiong Luo and Yuan Wang and Chen Song and Yue Sun},
  doi          = {10.1016/j.cviu.2022.103405},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103405},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive capsule network},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BacklitNet: A dataset and network for backlit image
enhancement. <em>CVIU</em>, <em>218</em>, 103403. (<a
href="https://doi.org/10.1016/j.cviu.2022.103403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backlit images are usually taken when the light source is opposite to the camera. The uneven exposure (e.g., underexposure on the foreground and overexposure on the background) makes the backlit images more challenging than general image enhancement tasks that only need to increase or decrease the exposure on the whole images. Compared to traditional approaches, Convolutional Neural Networks perform well in enhancing images due to the abilities of exploiting contextual features. However, the lack of large benchmark datasets and specially designed models impedes the development of backlit image enhancement. In this paper, we build the first large-scale BAcklit Image Dataset (BAID), which contains 3000 backlit images and the corresponding ground truth manually adjusted by trained photographers. It covers a broad range of categories under different backlit conditions in both indoor and outdoor scenes. Furthermore, we propose a saliency guided backlit image enhancement network, namely BacklitNet, for robust and natural restoration of backlit images. In particular, our model innovatively combines a nested U-structure with bilateral grids, which enables fully extracting multi-scale saliency information and rapidly enhancing arbitrary resolution images. Moreover, a carefully designed loss function based on prior knowledge of brightness distribution of backlit images is proposed to enforce the network to focus more on backlit regions during the training phase. We evaluate the proposed method on the BAID dataset and two public small-scale backlit image datasets. Experimental results demonstrate that our method performs favorably against the state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Xiaoqian Lv and Shengping Zhang and Qinglin Liu and Haozhe Xie and Bineng Zhong and Huiyu Zhou},
  doi          = {10.1016/j.cviu.2022.103403},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103403},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {BacklitNet: A dataset and network for backlit image enhancement},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-camera 3D head fitting for mixed reality clinical
applications. <em>CVIU</em>, <em>218</em>, 103384. (<a
href="https://doi.org/10.1016/j.cviu.2022.103384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of estimating the shape of a person’s head, defined as the geometry of the complete head surface, from a video taken with a single moving camera, and determining the alignment of the fitted 3D head for all video frames, irrespective of the person’s pose. 3D head reconstructions commonly tend to focus on perfecting the face reconstruction, leaving the scalp to a statistical approximation . Our goal is to reconstruct the head model of each person to enable future mixed reality applications. To do this, we recover a dense 3D reconstruction and camera information via structure-from-motion and multi-view stereo. These are then used in a new two-stage fitting process to recover the 3D head shape by iteratively fitting a 3D morphable model of the head with the dense reconstruction in canonical space and fitting it to each person’s head, using both traditional facial landmarks and scalp features extracted from the head’s segmentation mask. Our approach recovers consistent geometry for varying head shapes, from videos taken by different people, with different smartphones, and in a variety of environments from living rooms to outdoor spaces.},
  archive      = {J_CVIU},
  author       = {Tejas Mane and Aylar Bayramova and Kostas Daniilidis and Philippos Mordohai and Elena Bernardis},
  doi          = {10.1016/j.cviu.2022.103384},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103384},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Single-camera 3D head fitting for mixed reality clinical applications},
  volume       = {218},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online real-time pedestrian tracking from medium altitude
aerial footage with camera motion cancellation. <em>CVIU</em>,
<em>217</em>, 103386. (<a
href="https://doi.org/10.1016/j.cviu.2022.103386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using medium altitude footage in various image based pedestrian detection and tracking tasks have several advantages as opposed to low altitude footage. For instance, much larger covered area, or weaker perspective distortions and occlusions. Furthermore, it does not pose privacy issues. However, it also has a disadvantage that is the irregular camera motion. Such motions can be observed with some ground level footage as well. Some state-of-the-art multi-object tracking (MOT) algorithms, which excel at pedestrian tracking, handle these implicitly, however this is insufficient in the case of medium altitude footage. The problem is further enhanced in online real-time tracking where frames may need to be skipped. Although there are explicit frame registration methods developed for wide area motion imagery (WAMI), they operate on assumptions that are not true in medium altitude footage such as the background is mostly stationary. We show that by augmenting the pedestrian tracking algorithms with a simple visual object tracking (VOT) based frame registration method their accuracy on medium altitude footage can be substantially improved without hindering the execution speed. Moreover, we show that the algorithms’ execution speed can be significantly increased by better utilization of the resources. To evaluate these, we introduce helicopter video sequences with MOT annotation.},
  archive      = {J_CVIU},
  author       = {Gergely Csönde and Yoshihide Sekimoto and Takehiro Kashiyama},
  doi          = {10.1016/j.cviu.2022.103386},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103386},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Online real-time pedestrian tracking from medium altitude aerial footage with camera motion cancellation},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to combine the modalities of language and video for
temporal moment localization. <em>CVIU</em>, <em>217</em>, 103375. (<a
href="https://doi.org/10.1016/j.cviu.2022.103375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal moment localization aims to retrieve the best video segment matching a moment specified by a query. The existing methods generate the visual and semantic embeddings independently and fuse them without full consideration of the long-term temporal relationship between them. To address these shortcomings, we introduce a novel recurrent unit, cross-modal long short-term memory (CM-LSTM), by mimicking the human cognitive process of localizing temporal moments that focuses on the part of a video segment related to the part of a query, and accumulates the contextual information across the entire video recurrently. In addition, we devise a two-stream attention mechanism for both attended and unattended video features by the input query to prevent necessary visual information from being neglected. To obtain more precise boundaries, we propose a two-stream attentive cross-modal interaction network (TACI) that generates two 2D proposal maps obtained globally from the integrated contextual features, which are generated by using CM-LSTM, and locally from boundary score sequences and then combines them into a final 2D map in an end-to-end manner. On the TML benchmark dataset, ActivityNet-Captions, the TACI outperforms state-of-the-art TML methods with R@1 of 45.50\% and 27.23\% for IoU@0.5 and IoU@0.7, respectively. In addition, we show that the revised state-of-the-arts methods by replacing original LSTM with our CM-LSTM achieves performance gains.},
  archive      = {J_CVIU},
  author       = {Jungkyoo Shin and Jinyoung Moon},
  doi          = {10.1016/j.cviu.2022.103375},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103375},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning to combine the modalities of language and video for temporal moment localization},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When CNNs meet random RNNs: Towards multi-level analysis for
RGB-d object and scene recognition. <em>CVIU</em>, <em>217</em>, 103373.
(<a href="https://doi.org/10.1016/j.cviu.2022.103373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding. Meanwhile, deep neural networks , specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. To cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach achieves superior or on-par performance compared to state-of-the-art methods both in object and scene recognition tasks. Code is available at https://github.com/acaglayan/CNN_randRNN .},
  archive      = {J_CVIU},
  author       = {Ali Caglayan and Nevrez Imamoglu and Ahmet Burak Can and Ryosuke Nakamura},
  doi          = {10.1016/j.cviu.2022.103373},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103373},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {When CNNs meet random RNNs: Towards multi-level analysis for RGB-D object and scene recognition},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive image segmentation based on the appearance model
and orientation energy. <em>CVIU</em>, <em>217</em>, 103371. (<a
href="https://doi.org/10.1016/j.cviu.2022.103371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tang et al. (2013) proposed a graph-based image segmentation model by minimizing the distance between the object and background appearance overlap models. This model is very effective for interactive image segmentation. However, it is prone to isolated nodes when the colors or other appearances characteristic of the object and background are very similar. To improve the performance of this algorithm and related algorithms, we add new spatial distance and contour orientation energy terms to the energy function. Accordingly, we modify the construction of the energy graph. We add terminal nodes S and T and add prior constraints. Finally, we use the pseudoflow algorithm proposed by Hochbaum to calculate the maximum flow of the new energy graph. A large number of experiments on the MSRA dataset, BSD dataset and GrabCut dataset show that the results of the proposed method are better than those of many recently proposed image segmentation methods. The code is available at https://github.com/powerhope/AMOE .},
  archive      = {J_CVIU},
  author       = {Shaojun Qu and Huang Tan and Qiaoliang Li and Zili Peng},
  doi          = {10.1016/j.cviu.2022.103371},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103371},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Interactive image segmentation based on the appearance model and orientation energy},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FRIDA — generative feature replay for incremental domain
adaptation. <em>CVIU</em>, <em>217</em>, 103367. (<a
href="https://doi.org/10.1016/j.cviu.2022.103367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the novel problem of incremental unsupervised domain adaptation (IDA) in this paper. We assume that a labeled source domain and different unlabeled target domains are incrementally observed with the constraint that data corresponding to the current domain is only available at a time. The goal is to preserve the accuracies for all the past domains while generalizing well for the current domain. The IDA setup suffers due to the abrupt differences among the domains and the unavailability of past data including the source domain. Inspired by the notion of generative feature replay, we propose a novel framework called Feature Replay based Incremental Domain Adaptation (FRIDA) which leverages a new incremental generative adversarial network (GAN) called domain-generic auxiliary classification GAN (DGAC-GAN) for producing domain-specific feature representations seamlessly. For domain alignment, we propose a simple extension of the popular domain adversarial neural network (DANN) called DANN-IB which encourages discriminative domain-invariant and task-relevant feature learning . Experimental results on Office–Home, Office-CalTech, and DomainNet datasets confirm that FRIDA maintains superior stability-plasticity trade-off than the literature.},
  archive      = {J_CVIU},
  author       = {Sayan Rakshit and Anwesh Mohanty and Ruchika Chavhan and Biplab Banerjee and Gemma Roig and Subhasis Chaudhuri},
  doi          = {10.1016/j.cviu.2022.103367},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103367},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FRIDA — generative feature replay for incremental domain adaptation},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust detection of dehazed images via dual-stream CNNs with
adaptive feature fusion. <em>CVIU</em>, <em>217</em>, 103357. (<a
href="https://doi.org/10.1016/j.cviu.2022.103357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is common post-processing in automatic driving and video surveillance, which can improve image visual quality. However, it might also be used as an image forgery that is difficult to be perceived by naked eyes. Though image dehazing has attracted wide attention, there are still no works specially designed for this kind of forgery detection . By making extensive experiments and preliminary analysis, we observe that a dehazed image easily loses its illumination consistency that can be captured by inverse-intensity chromaticity (IIC) transformation. IIC is a transformed color space that well represents image illuminance map. In this work, a dehazing detection network (DDNet) is proposed to distinguish dehazed images from natural haze-free images. The proposed DDNet accepts RGB images and IIC images as inputs, which are fed into the backbone network , namely EfficientNet-B0, to learn features, respectively. To effectively fuse the learned RGB and IIC features, we also propose an adaptive feature fusion method, thereby improving detection capability. In addition, we build a dehazed image dataset, which includes 11432 pairs artificial, 367 pairs natural hazy images and their corresponding dehazed images, as the benchmark. Experimental results prove that the proposed DDNet achieves desirable detection accuracies and satisfactory robustness.},
  archive      = {J_CVIU},
  author       = {Jiyou Chen and Gaobo Yang and Xiangling Ding and Zhiqing Guo and Shuai Wang},
  doi          = {10.1016/j.cviu.2022.103357},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103357},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust detection of dehazed images via dual-stream CNNs with adaptive feature fusion},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MC-calib: A generic and robust calibration toolbox for
multi-camera systems. <em>CVIU</em>, <em>217</em>, 103353. (<a
href="https://doi.org/10.1016/j.cviu.2021.103353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present MC-Calib, a novel and robust toolbox dedicated to the calibration of complex synchronized multi-camera systems using an arbitrary number of fiducial marker-based patterns. Calibration results are obtained via successive stages of refinement to reliably estimate both the poses of the calibration boards and cameras in the system. Our method is not constrained by the number of cameras, their overlapping field-of-view (FoV), or the number of calibration patterns used. Moreover, neither prior information about the camera system nor the positions of the checkerboards are required. As a result, minimal user interaction is needed to achieve an accurate and robust calibration which makes this toolbox accessible even with limited computer vision expertise. In this work, we put a strong emphasis on the versatility and the robustness of our technique. Specifically, the hierarchical nature of our strategy allows to reliably calibrate complex vision systems even under the presence of noisy measurements. Additionally, we propose a new strategy for best-suited image selection and initial parameters estimation dedicated to non-overlapping FoV cameras. Finally, our calibration toolbox is compatible with both, perspective and fisheye cameras. Our solution has been validated on a large number of real and synthetic sequences including monocular, stereo, multiple overlapping cameras, non-overlapping cameras, and converging camera systems. Project page: https://github.com/rameau-fr/MC-Calib},
  archive      = {J_CVIU},
  author       = {Francois Rameau and Jinsun Park and Oleksandr Bailo and In So Kweon},
  doi          = {10.1016/j.cviu.2021.103353},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103353},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MC-calib: A generic and robust calibration toolbox for multi-camera systems},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive editing with stacked generative adversarial
network for multiple facial attribute editing. <em>CVIU</em>,
<em>217</em>, 103347. (<a
href="https://doi.org/10.1016/j.cviu.2021.103347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Network (GAN) based facial attribute editing has been successfully applied to many real world applications . However, most of existing methods suffer from semantic entanglement and imprecise editing when handling multiple facial attributes. The situation is worse when the samples with minority attribute values are insufficient, and majority attribute values dominate the learning easily. A stacked conditional GAN (cGAN) is proposed in this study aiming at solving these problems. Multiple attribute editing is broken down into several single attribute editing tasks which have been learned by base cGANs individually. Moreover, samples with a minority attribute value are paid more attention in learning. This proposed method not only reduces the difficulty of multiple attribute editing but also mitigates the imbalance problem. The residual image learning is applied to our model to reduce the difficulty of the image generation . The superiority of our model is demonstrated and compared with popular GAN-based facial attribute editing methods experimentally in terms of image quality, editing accuracy and training cost. The results confirm that our proposed model outperforms the other methods, especially in imbalance situations.},
  archive      = {J_CVIU},
  author       = {Patrick P.K. Chan and Xiaotian Wang and Zhe Lin and Daniel S. Yeung},
  doi          = {10.1016/j.cviu.2021.103347},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103347},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Progressive editing with stacked generative adversarial network for multiple facial attribute editing},
  volume       = {217},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plug-and-play video super-resolution using edge-preserving
filtering. <em>CVIU</em>, <em>216</em>, 103359. (<a
href="https://doi.org/10.1016/j.cviu.2022.103359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution objective is based on high resolution video frame reconstruction after downsampling. Such technology overcomes the inherent resolution limitations in videos. The mentioned task considers the inverse problem of original high resolution video frame recovery using prior information and reasonable suppositions. Motivated by the notable results of learning-based super-resolution strategies in obtaining high resolution outcomes from low resolution inputs, we leverage student-t mixture model as a promising reconstruction tool regarding video super-resolution. Student-t mixture model has a heavy tail which makes it robust and suitable for video frame patch prior and a rich mixture model in terms of log likelihood for information retrieval. Furthermore, in order to overcome the potential data uncertainties, edge-preserving filtering is applied to detect and preserve the video frame areas where the light sharply changes. For this purpose, we consider video frames in the framework of patches which will be selected on each frame based on their high amount of information. Afterwards, we use Plug-and-Play structure in order to apply student-t mixture prior model along with edge preserving filtering for representing the video frame patch prior in super-resolution algorithm. Finally, the proposed algorithm is evaluated empirically over 5 video frame sets, escalator, fountain, tree, wave and traffic. In addition, the results are compared with eight other state of the art super-resolution methods including SAN , RCAN, RDN, ANR, A+, SRCNN, FSRCNN-s and SelfExSR and it is proved that the proposed framework generally provides the best results in comparison with other techniques over four different super-resolution scales.},
  archive      = {J_CVIU},
  author       = {Vahid Khorasani Ghassab and Nizar Bouguila},
  doi          = {10.1016/j.cviu.2022.103359},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103359},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Plug-and-play video super-resolution using edge-preserving filtering},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic mode decomposition via convolutional autoencoders
for dynamics modeling in videos. <em>CVIU</em>, <em>216</em>, 103355.
(<a href="https://doi.org/10.1016/j.cviu.2021.103355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting the underlying dynamics of objects in image sequences is one of the challenging problems in computer vision . Besides, dynamic mode decomposition (DMD) has recently attracted attention as a method for obtaining modal representations of nonlinear dynamics from general multivariate time-series data without explicit prior information about the dynamics. In this paper, we propose a convolutional autoencoder (CAE)-based DMD (CAE-DMD) to perform accurate modeling of underlying dynamics in videos. We develop a modified CAE model that encodes images to latent vectors and incorporated DMD on the latent vectors to extract DMD modes. These modes are split into background and foreground modes for foreground modeling in videos, or used for video classification tasks . And the latent vectors are mapped so as to recover the input image sequences through a decoder. We perform the network training in an end-to-end manner, i.e., by minimizing the mean square error between the original and reconstructed images. As a result, we obtain accurate extraction of underlying dynamic information in the videos. We empirically investigate the performance of CAE-DMD in two applications background foreground extraction and video classification on synthetic and publicly available datasets.},
  archive      = {J_CVIU},
  author       = {Israr Ul Haq and Tomoharu Iwata and Yoshinobu Kawahara},
  doi          = {10.1016/j.cviu.2021.103355},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103355},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic mode decomposition via convolutional autoencoders for dynamics modeling in videos},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal distillation for RGB-depth person
re-identification. <em>CVIU</em>, <em>216</em>, 103352. (<a
href="https://doi.org/10.1016/j.cviu.2021.103352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is a key challenge for surveillance across multiple sensors. Prompted by the advent of powerful deep learning models for visual recognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic platforms, e.g. self-driving vehicles, we investigate the relatively unexplored problem of cross-modal re-identification of persons between RGB (color) and depth images. The considerable divergence in data distributions across different sensor modalities introduces additional challenges to the typical difficulties like distinct viewpoints, occlusions, and pose and illumination variation. While some work has investigated re-identification across RGB and infrared, we take inspiration from successes in transfer learning from RGB to depth in object detection tasks. Our main contribution is a novel method for cross-modal distillation for robust person re-identification, which learns a shared feature representation space of person’s appearance in both RGB and depth images. In addition, we propose a cross-modal attention mechanism where the gating signal from one modality can dynamically activate the most discriminant CNN filters of the other modality. The proposed distillation method is compared to conventional and deep learning approaches proposed for other cross-domain re-identification tasks. Results obtained on the public BIWI and RobotPKU datasets indicate that the proposed method can significantly outperform the state-of-the-art approaches by up to 16.1\% in mean Average Precision (mAP), demonstrating the benefit of the distillation paradigm. The experimental results also indicate that using cross-modal attention allows to improve recognition accuracy considerably with respect to the proposed distillation method and relevant state-of-the-art approaches. 1},
  archive      = {J_CVIU},
  author       = {Frank M. Hafner and Amran Bhuyian and Julian F.P. Kooij and Eric Granger},
  doi          = {10.1016/j.cviu.2021.103352},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103352},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-modal distillation for RGB-depth person re-identification},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Superclass-aware network for few-shot learning.
<em>CVIU</em>, <em>216</em>, 103349. (<a
href="https://doi.org/10.1016/j.cviu.2021.103349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can learn to recognize a novel object by just going through its images a few times. It might because that they do not recognize the novel object purely on the visual information, but also based on their prior knowledge. Inspired from this, we propose a novel framework named Superclass-aware Network (Sup-Net) to tackle the few-shot learning problem. We first present a knowledge extraction schema in Sup-Net, which can acquire superclass information, and compute superclass semantic relations between different categories. We introduce a novel soft label supervised contrastive loss to help extract discriminative superclass features from images so that the superclass relation can be captured by these features. A novel model architecture that is jointly trained by images and prior knowledge has been proposed. The model encodes image features that minimize the cross-entropy loss at the category level, while it also extracts the superclass feature that minimizes the soft label contrastive loss at the superclass level. Experimental results demonstrate that Sup-Net achieves competitive results on miniImageNet datasets. In addition, we conduct experiments on a large-scale dataset tieredImageNet; the results further demonstrate the effectiveness of our Sup-Net.},
  archive      = {J_CVIU},
  author       = {Shuang Wu and Mohan Kankanhalli and Anthony K.H. Tung},
  doi          = {10.1016/j.cviu.2021.103349},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103349},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Superclass-aware network for few-shot learning},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced discriminative graph convolutional network with
adaptive temporal modelling for skeleton-based action recognition.
<em>CVIU</em>, <em>216</em>, 103348. (<a
href="https://doi.org/10.1016/j.cviu.2021.103348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have achieved promising results in skeleton-based action recognition due to their capability in analysing irregular grids with non-Euclidean geometry. Considering the fact that skeleton-based action recognition is a classification problem, the suitable graph representation in GCN-based approaches is the key for the ultimate goal of classification. Despite the practical success of GCN-based approaches for solving this problem over the past few years, learning better representation is still a challenging issue and the existing approaches fail in distinguishing similar actions. Besides, most existing GCN-based frameworks focus on modelling the spatial information and use one fixed kernel to model the temporal information. Such modelling does not pay enough attention to diversifying representations among different skeleton frames, leading to inefficiency in obtaining more discriminative temporal features for different actions and therefore, such modelling is inconvenient with the diversity of human movements. Our main concern in this work is the adaptive feature extraction of highly discriminative information for both the spatial and the temporal dimensions. To achieve that, a novel Enhanced Discriminative Graph Convolutional Network (ED-GCN) based on the attention mechanism for skeleton-based action recognition is proposed. Discriminative channel-wise features are obtained by fusing the Squeeze and Excitation (SE) module to the GCN to selectively enhance the significant features and suppress the non-significant ones. The adaptively enhanced feature map is then fused to the graph convolutional layer to improve the capability of learning better representation. For the temporal dimension inspired by temporal modelling in videos, we introduce our adaptive temporal modelling block (ATB), which is able to flexibly capture temporal structure for skeleton-based action recognition. Here, the proposed ATB is a two-stage module comprising re-calibration and motion-interaction stage, designed to learn temporal features by integrating the modelling of channel correlation and temporal evolution , respectively. Experimental results on two large-scale datasets, NTU-RGB+D and Kinetics-skeleton demonstrate the importance of discriminatively learned information and the effectiveness of the proposed ED-GCN for skeleton-based action recognition.},
  archive      = {J_CVIU},
  author       = {Tamam Alsarhan and Usman Ali and Hongtao Lu},
  doi          = {10.1016/j.cviu.2021.103348},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103348},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhanced discriminative graph convolutional network with adaptive temporal modelling for skeleton-based action recognition},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic detection and localization of thighbone fractures
in x-ray based on improved deep learning method. <em>CVIU</em>,
<em>216</em>, 103345. (<a
href="https://doi.org/10.1016/j.cviu.2021.103345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is continuously promoting the development of fracture detection in medical images. In this study, we propose a novel two-stage region-based convolutional neural network for thighbone fractures detection. In this framework, the new network structure is designed to balance the information of each feature map in the feature pyramid of ResNeXt. In experiments, the pre-trained model is implemented on the dataset reported in the previous study, which includes 3842 thighbone X-ray radiographs. To compare the proposed framework with the latest detection techniques, transfer learning is employed to test all the state-of-the-art generic object detection algorithms on the same thighbone fracture dataset. Moreover, a few ablation experiments are given to demonstrate the effects of each component employed in the proposed framework and different hyperparameter settings on fracture detection. The experimental results show that the Average Precision of the proposed detection framework reaches 88.9\% in thighbone fracture detection. This result proves the effectiveness of our framework and its superiority over other state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Bin Guan and Jinkun Yao and Shaoquan Wang and Guoshan Zhang and Yueming Zhang and Xinbo Wang and Mengxuan Wang},
  doi          = {10.1016/j.cviu.2021.103345},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103345},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Automatic detection and localization of thighbone fractures in X-ray based on improved deep learning method},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Light-weight shadow detection via GCN-based annotation
strategy and knowledge distillation. <em>CVIU</em>, <em>216</em>,
103341. (<a href="https://doi.org/10.1016/j.cviu.2021.103341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses shadow detection problem, and proposes a light-weight network to achieve both accurate detection results and high computation efficiency. Firstly, we begin by presenting a compact network for real-time shadow detection. Secondly, to improve the performance of our light-weight networks, we propose two complementary and necessary strategies, i.e. , the use of extra training data and knowledge distillation . Note that collecting a large amount of extra data will lead to the following challenge: shadow scenes is various, while annotating for those complex scenarios is time-consuming and expensive, sometimes even need expert help. To solve it, in the first step, we introduce a novel shadow annotation strategy based on graph convolutional networks , namely Anno-GCN, to provide extra training pairs, which obtains a complete shadow mask via only several annotation scribbles. In the second step, we can combine knowledge distillation with these sufficient GCN-labeled training data to further improve the performance of the light-weight network. Extensive experiments demonstrate that our method can achieve a state-of-the-art inference accuracy, computational efficiency, and generalizability with only about 2.97 M parameters.},
  archive      = {J_CVIU},
  author       = {Wen Wu and Kai Zhou and Xiao-Diao Chen and Jun-Hai Yong},
  doi          = {10.1016/j.cviu.2021.103341},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103341},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Light-weight shadow detection via GCN-based annotation strategy and knowledge distillation},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SnapshotNet: Self-supervised feature learning for point
cloud data segmentation using minimal labeled data. <em>CVIU</em>,
<em>216</em>, 103339. (<a
href="https://doi.org/10.1016/j.cviu.2021.103339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene . The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part contrasting and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features . Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. Then trained SVM is further used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed weakly-supervised method has shown advantages when comparing to the state of the art method on weakly-supervised point cloud semantic segmentation.},
  archive      = {J_CVIU},
  author       = {Xingye Li and Ling Zhang and Zhigang Zhu},
  doi          = {10.1016/j.cviu.2021.103339},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103339},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SnapshotNet: Self-supervised feature learning for point cloud data segmentation using minimal labeled data},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anti-jamming heart rate estimation using a spatial–temporal
fusion network. <em>CVIU</em>, <em>216</em>, 103327. (<a
href="https://doi.org/10.1016/j.cviu.2021.103327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote heart rate estimation based on face video has attracted increasing research attention. The previous methods are mostly based on ideal face features, where face videos are obtained under controllable experimental conditions, with uniform lighting and little noise interference. However, there is a big deviation from the real world scene. The lighting changes, head movements, and object occlusion are inevitable in complex scenes, which will lead to the failure of face positioning and feature extraction from video, and the instability of the facial signal required for heart rate estimation. In this paper, an anti-jamming network is proposed to improve the robustness of handling less-constrained scenarios. Specifically, a new spatial–temporal map generation mechanism is designed to enhance the spatial and temporal features representation by equivalent padding for low-quality video frame fragments. Meanwhile, a heart rate estimation stability module is built to evaluate the quality of the face signals and assign reasonable weights to video clips. Our approach significantly outperforms all current state-of-the-art methods on VIPL-HR dataset.},
  archive      = {J_CVIU},
  author       = {Chunlei Wu and Ziyu Yuan and Shaohua Wan and Leiquan Wang and Weishan Zhang},
  doi          = {10.1016/j.cviu.2021.103327},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103327},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Anti-jamming heart rate estimation using a spatial–temporal fusion network},
  volume       = {216},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hallucinating uncertain motion and future for static image
action recognition. <em>CVIU</em>, <em>215</em>, 103337. (<a
href="https://doi.org/10.1016/j.cviu.2021.103337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static image action recognition, aiming to recognize human action in a single image, is a challenging task due to the lack of motion and temporal information in static images. Therefore, some previous works have leveraged freely available unlabeled videos to assist image action recognition, which can be categorized into the following two research lines: predict motion or future information of static images using the predictor learned from unlabeled videos. In this paper, following the above two research lines, we propose a novel Multi-modal Motion feature Generator (MMG) and a novel Multi-modal Future feature Generator (MFG) to hallucinate multiple plausible motion features and future visual features for a static image, which could significantly facilitate the image action recognition task. Extensive experiments on two video datasets and two static action image datasets demonstrate the effectiveness of our methods.},
  archive      = {J_CVIU},
  author       = {Li Niu and Shengyuan Huang and Xing Zhao and Liwei Kang and Yiyi Zhang and Liqing Zhang},
  doi          = {10.1016/j.cviu.2021.103337},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103337},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hallucinating uncertain motion and future for static image action recognition},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The semantic typology of visually grounded paraphrases.
<em>CVIU</em>, <em>215</em>, 103333. (<a
href="https://doi.org/10.1016/j.cviu.2021.103333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visually grounded paraphrases (VGPs) are different phrasal expressions describing the same visual concept in an image. Previous studies treat VGP identification as a binary classification task , which ignores various phenomena behind VGPs (i.e., different linguistic interpretation of the same visual concept) such as linguistic paraphrases and VGPs from different aspects. In this paper, we propose semantic typology for VGPs, aiming to elucidate the VGP phenomena and deepen the understanding about how human beings interpret vision with language. We construct a large VGP dataset that annotates the class to which each VGP pair belongs according to our typology. In addition, we present a classification model that fuses language and visual features for VGP classification on our dataset. Experiments indicate that joint language and vision representation learning is important for VGP classification. We further demonstrate that our VGP typology can boost the performance of visually grounded textual entailment.},
  archive      = {J_CVIU},
  author       = {Chenhui Chu and Vinicius Oliveira and Felix Giovanni Virgo and Mayu Otani and Noa Garcia and Yuta Nakashima},
  doi          = {10.1016/j.cviu.2021.103333},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103333},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {The semantic typology of visually grounded paraphrases},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SdcNet for object recognition. <em>CVIU</em>, <em>215</em>,
103332. (<a href="https://doi.org/10.1016/j.cviu.2021.103332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a CNN architecture for object recognition is proposed, aiming at achieving a good processing-quality at the lowest computation-cost. The work includes the design of SdcBlock, a convolution module, for feature extraction, and that of SdcNet, an end-to-end CNN architecture. The module is designed to extract the maximum amount of high-density feature information from a given set of data channels. To this end, successive depthwise convolutions (Sdc) are applied to each group of data to produce feature elements of different filtering orders. To optimize the functionality of these convolutions, a particular pre-and-post-convolution data control is applied. The pre-convolution control is to organize the input channels of the module so that the depthwise convolutions can be performed with a single channel or a combination of multiple data channels, depending on the nature of the data. The post-convolution control is to combine the critical feature elements of different filtering orders to enhance the quality of the convolved results. The SdcNet is mainly composed of cascaded SdcBlocks. The hyper-parameters in the architecture can be adjusted easily so that each module can be tuned to suit its input signals in order to optimize the processing-quality of the entire network. Three different versions of SdcNet have been proposed and tested using CIFAR dataset, and the results demonstrate that the architecture gives a better processing-quality at a significantly lower computation cost, compared with networks performing similar tasks. Two other versions have also been tested with samples from ImageNet to prove the applicability of SdcNet in object recognition with images of ImageNet format. Also, a SdcNet for brain tumor detection has been designed and tested successfully to illustrate that SdcNet can effectively perform the detection with a high computation efficiency.},
  archive      = {J_CVIU},
  author       = {Yunlong Ma and Chunyan Wang},
  doi          = {10.1016/j.cviu.2021.103332},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103332},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SdcNet for object recognition},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pros and cons of GAN evaluation measures: New developments.
<em>CVIU</em>, <em>215</em>, 103329. (<a
href="https://doi.org/10.1016/j.cviu.2021.103329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is an update of my previous paper on the same topic published a few years ago (Borji, 2019). With the dramatic progress in generative modeling, a suite of new quantitative and qualitative techniques to evaluate models has emerged. Although some measures such as Inception Score, Fréchet Inception Distance, Precision–Recall, and Perceptual Path Length are relatively more popular, GAN evaluation is not a settled issue and there is still room for improvement. Here, I describe new dimensions that are becoming important in assessing models ( e.g. bias and fairness) and discuss the connection between GAN evaluation and deepfakes . These are important areas of concern in the machine learning community today and progress in GAN evaluation can help mitigate them.},
  archive      = {J_CVIU},
  author       = {Ali Borji},
  doi          = {10.1016/j.cviu.2021.103329},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103329},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pros and cons of GAN evaluation measures: New developments},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dissected 3D CNNs: Temporal skip connections for efficient
online video processing. <em>CVIU</em>, <em>215</em>, 103318. (<a
href="https://doi.org/10.1016/j.cviu.2021.103318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks with 3D kernels (3D CNNs) currently achieve state-of-the-art results in video recognition tasks due to their supremacy in extracting spatiotemporal features within video frames. There have been many successful 3D CNN architectures surpassing state-of-the-art results successively. However, nearly all of them are designed to operate offline, creating several serious handicaps during online operation. Firstly, conventional 3D CNNs are not dynamic since their output features represent the complete input clip instead of the most recent frame in the clip. Secondly, they are not temporal resolution-preserving due to their inherent temporal downsampling. Lastly, 3D CNNs are constrained to be used with fixed temporal input size limiting their flexibility. In order to address these drawbacks, we propose dissected 3D CNNs, where the intermediate volumes of the network are dissected and propagated over depth (time) dimension for future calculations, substantially reducing the number of computations at online operation. For action classification , the dissected version of ResNet models performs 77\%–90\% fewer computations at online operation while achieving ∼ ∼ 5\% better classification accuracy on the Kinetics-600 dataset than conventional 3D-ResNet models. Moreover, the advantages of dissected 3D CNNs are demonstrated by deploying our approach onto several vision tasks, which consistently improved the performance.},
  archive      = {J_CVIU},
  author       = {Okan Köpüklü and Stefan Hörmann and Fabian Herzog and Hakan Cevikalp and Gerhard Rigoll},
  doi          = {10.1016/j.cviu.2021.103318},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103318},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dissected 3D CNNs: Temporal skip connections for efficient online video processing},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial landmark points detection using knowledge
distillation-based neural networks. <em>CVIU</em>, <em>215</em>, 103316.
(<a href="https://doi.org/10.1016/j.cviu.2021.103316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection is a vital step for numerous facial image analysis applications. Although some deep learning-based methods have achieved good performances in this task, they are often not suitable for running on mobile devices . Such methods rely on networks with many parameters, which makes the training and inference time-consuming. Training lightweight neural networks such as MobileNets are often challenging, and the models might have low accuracy. Inspired by knowledge distillation (KD), this paper presents a novel loss function to train a lightweight Student network (e.g., MobileNetV2) for facial landmark detection. We use two Teacher networks, a Tolerant-Teacher and a Tough-Teacher in conjunction with the Student network. The Tolerant-Teacher is trained using Soft-landmarks created by active shape models, while the Tough-Teacher is trained using the ground truth (aka Hard-landmarks) landmark points. To utilize the facial landmark points predicted by the Teacher networks, we define an Assistive Loss (ALoss) for each Teacher network. Moreover, we define a loss function called KD-Loss that utilizes the facial landmark points predicted by the two pre-trained Teacher networks (EfficientNet-b3) to guide the lightweight Student network towards predicting the Hard-landmarks. Our experimental results on three challenging facial datasets show that the proposed architecture will result in a better-trained Student network that can extract facial landmark points with high accuracy.},
  archive      = {J_CVIU},
  author       = {Ali Pourramezan Fard and Mohammad H. Mahoor},
  doi          = {10.1016/j.cviu.2021.103316},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103316},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Facial landmark points detection using knowledge distillation-based neural networks},
  volume       = {215},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HSGAN: Reducing mode collapse in GANs by the latent code
distance of homogeneous samples. <em>CVIU</em>, <em>214</em>, 103314.
(<a href="https://doi.org/10.1016/j.cviu.2021.103314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose HSGAN, a novel generative adversarial network (GAN) variant that plays an adversarial game on the distance between two homogeneous samples (HS) in the latent space. HSGAN alleviates the notorious problem of mode collapse by maintaining a certain distance between the latent code of the generated data. Moreover, HSGAN is directly trained on the encoder and the generator, thereby gaining the ability to conduct inference without introducing any other model complexity. We prove theoretically that the objective function is designed to minimize the f f -divergence between the distributions of the generated data and the real data. Extensive experiments on a series of synthetic and real image benchmark datasets demonstrate that HSGAN generates diverse images while keeping high quality, and it generally outperforms other GANs that target at the mode collapse problem.},
  archive      = {J_CVIU},
  author       = {Simin Yu and Kuntian Zhang and Chuan Xiao and Joshua Zhexue Huang and Mark Junjie Li and Makoto Onizuka},
  doi          = {10.1016/j.cviu.2021.103314},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103314},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {HSGAN: Reducing mode collapse in GANs by the latent code distance of homogeneous samples},
  volume       = {214},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MTCD: Cataract detection via near infrared eye images.
<em>CVIU</em>, <em>214</em>, 103303. (<a
href="https://doi.org/10.1016/j.cviu.2021.103303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, cataract is a common eye disease and one of the leading causes of blindness and vision impairment . The traditional process of detecting cataracts involves eye examination using a slit-lamp microscope or ophthalmoscope by an ophthalmologist, who checks for clouding of the normally clear lens of the eye. The lack of resources and unavailability of a sufficient number of experts pose a burden to the healthcare system throughout the world, and researchers are exploring the use of AI solutions for assisting the experts. Inspired by the progress in iris recognition , in this research, we present a novel algorithm for cataract detection using near-infrared eye images. The NIR cameras, which are popularly used in iris recognition, are of relatively low cost and easy to operate compared to ophthalmoscope setup for data capture. However, such NIR images have not been explored for cataract detection. We present deep learning-based eye segmentation and multitask network classification networks for cataract detection using NIR images as input. The proposed segmentation algorithm efficiently and effectively detects non-ideal eye boundaries and is cost-effective, and the classification network yields very high classification performance on the cataract dataset.},
  archive      = {J_CVIU},
  author       = {Pavani Tripathi and Yasmeena Akhter and Mahapara Khurshid and Aditya Lakra and Rohit Keshari and Mayank Vatsa and Richa Singh},
  doi          = {10.1016/j.cviu.2021.103303},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103303},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MTCD: Cataract detection via near infrared eye images},
  volume       = {214},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous multi-person tracking and activity recognition
based on cohesive cluster search. <em>CVIU</em>, <em>214</em>, 103301.
(<a href="https://doi.org/10.1016/j.cviu.2021.103301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a bootstrapping framework to simultaneously improve multi-person tracking and activity recognition at individual, interaction and social group activity levels. The inference consists of identifying trajectories of all pedestrian actors, individual activities, pairwise interactions, and collective activities, given the observed pedestrian detections. Our method uses a graphical model to represent and solve the joint tracking and recognition problems via three stages: (i) activity-aware tracking, (ii) joint interaction recognition and occlusion recovery, and (iii) collective activity recognition. This full-stack problem induces great complexity in learning the representations for the sub-problems at each stage, and the complexity increases as with more stages in the system. Our solution is to make use of symbolic cues for inference at higher stages, inspired by the observations of cohesive clusters at different stages. This also avoids learning more ambiguous representations in the higher stages. High-order correlations among the visible and occluded individuals, pairwise interactions, groups, and activities are then solved using the cohesive cluster search within a Bayesian framework . Experiments on several benchmarks show the advantages of our approach over the existing methods.},
  archive      = {J_CVIU},
  author       = {Wenbo Li and Yi Wei and Siwei Lyu and Ming-Ching Chang},
  doi          = {10.1016/j.cviu.2021.103301},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103301},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Simultaneous multi-person tracking and activity recognition based on cohesive cluster search},
  volume       = {214},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple instance learning on deep features for weakly
supervised object detection with extreme domain shifts. <em>CVIU</em>,
<em>214</em>, 103299. (<a
href="https://doi.org/10.1016/j.cviu.2021.103299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) using only image-level annotations has attracted a growing attention over the past few years. Whereas such task is typically addressed with a domain-specific solution focused on natural images, we show that a simple multiple instance approach applied on pre-trained deep features yields excellent performances on non-photographic datasets, possibly including new classes. The approach does not include any fine-tuning or cross-domain learning and is therefore efficient and possibly applicable to arbitrary datasets and classes. We investigate several flavors of the proposed approach, some including multi-layers perceptron and polyhedral classifiers. Despite its simplicity, our method shows competitive results on a range of publicly available datasets, including paintings (People-Art, IconArt), watercolors, cliparts and comics and allows to quickly learn unseen visual categories.},
  archive      = {J_CVIU},
  author       = {Nicolas Gonthier and Saïd Ladjal and Yann Gousseau},
  doi          = {10.1016/j.cviu.2021.103299},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103299},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multiple instance learning on deep features for weakly supervised object detection with extreme domain shifts},
  volume       = {214},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep structural information fusion for 3D object detection
on LiDAR–camera system. <em>CVIU</em>, <em>214</em>, 103295. (<a
href="https://doi.org/10.1016/j.cviu.2021.103295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection on LiDAR–camera system is a challenging task, for 3D LiDAR point and 2D RGB image have different data representation. In this paper, We consider that the geometrical consistency in the local 3D and 2D regions is helpful for the regression task in 3D object detection, and propose 3D–2D consistent feature. It is based on hand-crafted 3D and 2D descriptors, generates primary structure feature, and has stable performance in outdoor scenes. Considering that material feature can be used to distinguish different objects, material coefficients ratio (MCR) is proposed to generate primary semantic feature , benefiting the classification task in 3D object detection. It is based on Lambertian model . To take advantage of both 3D–2D consistent feature and MCR, we propose deep 3D–2D structural information fusion (SIF) for 3D object detection. It provides attentional structural voxel feature, used as the input of LiDAR voxel based 3D object detectors. SIF is a light, effective, and explainable module. In the outdoor 3D object detection dataset, extensive experiments demonstrate that SIF improves the performance for both LiDAR voxel based single stage and multi-stage 3D detectors.},
  archive      = {J_CVIU},
  author       = {Pei An and Junxiong Liang and Kun Yu and Bin Fang and Jie Ma},
  doi          = {10.1016/j.cviu.2021.103295},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103295},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep structural information fusion for 3D object detection on LiDAR–camera system},
  volume       = {214},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
