<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mla---138">MLA - 138</h2>
<ul>
<li><details>
<summary>
(2022). A non-convex optimization framework for large-scale low-rank
matrix factorization. <em>MLA</em>, <em>10</em>, 100440. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank matrix factorization problems such as non negative matrix factorization (NMF) can be categorized as a clustering or dimension reduction technique. The latter denotes techniques designed to find representations of some high dimensional dataset in a lower dimensional manifold without a significant loss of information. If such a representation exists, the features ought to contain the most relevant features of the dataset. Many linear dimensionality reduction techniques can be formulated as a matrix factorization. In this paper, we combine the conjugate gradient (CG) method with the Barzilai and Borwein (BB) gradient method , and propose a BB scaling CG method for NMF problems. The new method does not require to compute and store matrices associated with Hessian of the objective functions. Moreover, adopting a suitable BB step size along with a proper nonmonotone strategy which comes by the size of convex parameter η k ηk , results in a new algorithm that can significantly improve the CPU time, efficiency, the number of function evaluation. Convergence result is established and numerical comparisons of methods on both synthetic and real-world datasets show that the proposed method is efficient in comparison with existing methods and demonstrate the superiority of our algorithms.},
  archive      = {J_MLA},
  author       = {Sajad Fathi Hafshejani and Saeed Vahidian and Zahra Moaberfard and Bill Lin},
  doi          = {10.1016/j.mlwa.2022.100440},
  journal      = {Machine Learning with Applications},
  pages        = {100440},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A non-convex optimization framework for large-scale low-rank matrix factorization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation with post-adaptation labeled
domain performance preservation. <em>MLA</em>, <em>10</em>, 100439. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is a machine learning-oriented application that aims to transfer knowledge learned from a seen (source) domain with labeled data to an unseen (target) domain with only unlabeled data . Recently developed techniques apply adversarial learning to learn domain-transferable features. However, current adversarial domain adaptation models suffer from the training instability of adversarial networks. Furthermore, it is unclear what the source domain pays in terms of performance during learning the domain-transferable representation. To address this issue, we propose a novel approach termed U nsupervised D omain A daptation with S ource P reservation (UDA-SP). It shares the same objective of obtaining a generalization representation between different distributions as domain adaptation techniques. Additionally, it has the new objective of preserving efficient performance in the source domain. This is accomplished by learning representations of shared and source-specific features that are separately learned from two distinct networks. Then, they are concatenated with available class information to train a new classifier that has the ability to exploit both shared and domain-specific features. We conducted a comprehensive experimental analysis on three benchmark text datasets. Experiments validate that our proposed method outperforms their competing state-of-the-art methods. Further experiments demonstrate that UDA-SP has a good ability to generalize learned knowledge to unseen domains while maintaining seen domain performance.},
  archive      = {J_MLA},
  author       = {Haidi Badr and Nayer Wanas and Magda Fayek},
  doi          = {10.1016/j.mlwa.2022.100439},
  journal      = {Machine Learning with Applications},
  pages        = {100439},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Unsupervised domain adaptation with post-adaptation labeled domain performance preservation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Erratum regarding missing declaration of competing interest
statements in previously published articles. <em>MLA</em>, <em>10</em>,
100438. (<a href="https://doi.org/10.1016/j.mlwa.2022.100438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLA},
  doi          = {10.1016/j.mlwa.2022.100438},
  journal      = {Machine Learning with Applications},
  pages        = {100438},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Erratum regarding missing declaration of competing interest statements in previously published articles},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated treatment plan alert system to safeguard cancer
treatments in radiation therapy. <em>MLA</em>, <em>10</em>, 100437. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In radiation oncology, the intricate process of delivering radiation to a patient is detailed by the patient’s treatment plan, which is data describing the geometry, construction and strength of the radiation machine and the radiation beam it emits. The patient’s life depends upon the accuracy of the treatment plan, which is left in the hands of the vendor-specific software automatically generating the plan after an initial patient consultation and planning with a medical professional. However, corrupted and erroneous treatment plan data have previously resulted in severe patient harm when errors go undetected and radiation proceeds. The aim of this paper is to develop an automatic error-checking system to prevent the accidental delivery of radiation treatment to an area of the human body (i.e., the treatment site) that differs from the plan’s documented intended site. To this end, we develop a method for structuring treatment plan data in order to feed machine-learning (ML) classifiers and predict a plan’s treatment site. In practice, a warning may be raised if the prediction disagrees with the documented intended site. The contribution of this paper is in the strategic structuring of the complex, intricate, and nonuniform data of modern treatment planning and from multiple vendors in order to easily train ML algorithms . A three-step process utilizing up- and down-sampling and dimension reduction, the method we develop in this paper reduces the thousands of parameters comprising a single treatment plan to a single two-dimensional heat map that is independent of the specific vendor or construction of the machine used for treatment. Our heat-map structure lends itself well to feed well-established ML algorithms, and we train–test random forest , softmax, k-nearest neighbors, shallow neural network , and support vector machine using real clinical treatment plans from several hospitals in the United States. The paper demonstrates that the proposed method characterizes treatment sites so well that ML classifiers may predict head-neck, breast, and prostate treatment sites with an accuracy of about 94\%. The proposed method is the first step towards a thorough, fully automated error-checking system in radiation therapy.},
  archive      = {J_MLA},
  author       = {Paul M. Kump and Junyi Xia and Sridhar Yaddanapudi and Erwei Bai},
  doi          = {10.1016/j.mlwa.2022.100437},
  journal      = {Machine Learning with Applications},
  pages        = {100437},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An automated treatment plan alert system to safeguard cancer treatments in radiation therapy},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved u-net model for concrete crack detection.
<em>MLA</em>, <em>10</em>, 100436. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection plays an important role in disease assessment of concrete buildings. However, factors such as complex background, irregular edge, and the real-time and accuracy requirement also make crack detection a challenging task. Aiming at the above challenges, an improved U-Net model for concrete crack detection is proposed, which has strong capability to extract the linear object, improving the performance in crack detection. The model is named Residual Linear Attention U-Net (RLAU-Net). There are three key measures in this paper. First, mirror padding the source image before convolution. Second, the multi-level features are obtained by aggregating the multi-scale features level by level. Third, strip pooling kernels are used to extract global contextual information, reducing information interference from the background. We tested the performance of RLAU-Net on our crack dataset, and the experimental results exhibited that it can improve the quantitative results of mean Intersection Over Union to 81.69\%. In addition, F1 score has increased to, 78.21\%, the Intersection Over Union of crack increased to 64.47\%. We also compared the detect time-consuming of RLAU-Net and that of the original U-Net. Results demonstrate that the proposed model has a short processing time while maintaining a high detection accuracy for crack detection.},
  archive      = {J_MLA},
  author       = {Chenglong Yu and Jianchao Du and Meng Li and Yunsong Li and Weibin Li},
  doi          = {10.1016/j.mlwa.2022.100436},
  journal      = {Machine Learning with Applications},
  pages        = {100436},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An improved U-net model for concrete crack detection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applications of machine learning in cricket: A systematic
review. <em>MLA</em>, <em>10</em>, 100435. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cricket has become a famous team game around the globe, and it is considered the world’s second most popular sport (Pathak and Wadhwa, 2016). The plethora of available cricket data and the development of Machine Learning (ML) technology have created a massive demand for cricket data analytics . The applications of ML in the cricket domain have increased dramatically during the last two decades. This study conducts a systematic review of the published research work during the last two decades (2001–2021) on the applications of ML in cricket.},
  archive      = {J_MLA},
  author       = {Indika Wickramasinghe},
  doi          = {10.1016/j.mlwa.2022.100435},
  journal      = {Machine Learning with Applications},
  pages        = {100435},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Applications of machine learning in cricket: A systematic review},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constructing portfolios using stable distributions: The case
of s&amp;p 500 sectors exchange-traded funds. <em>MLA</em>, <em>10</em>,
100434. (<a href="https://doi.org/10.1016/j.mlwa.2022.100434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio construction is an important practical problem in finance. In the traditional approach, introduced by Markowitz, one assumes normally distributed returns and constructs a portfolio with a minimum risk (measured by the standard deviation of portfolio returns) for a specified (and minimally acceptable) return. In practice, returns are not normally distributed and have heavy tails. As a result, the normality assumption severely underestimates risk. It has been long suggested that a more appropriate way is to model returns by using alpha-stable distributions. In this paper, we use elliptical stable distributions for optimal stable portfolio construction. We illustrate this by considering portfolios from S&amp;P 500 sector exchange-traded funds (ETFs). Our main results indicate that stable portfolios are in general comparable with standard Markowitz portfolios. But we discovered a few properties of stable distributions that are promising for optimal portfolio selection problem. Stable risk estimation for next year is much closer to the actual portfolio risk, stable portfolios are more balanced, so they better handle maximum restriction on component weights, and stable portfolios are more resilient to extreme market conditions. Both stable and normal optimal portfolios outperform S&amp;P 500 index and equally-weighted ETF portfolio in most years. We propose an investment strategy that uses both stable and normal distributions for building optimal portfolios. The return of this strategy exceeds the return of the strategies which use only stable or only normal distributions.},
  archive      = {J_MLA},
  author       = {Andrei Vasiukevich and Eugene Pinsky},
  doi          = {10.1016/j.mlwa.2022.100434},
  journal      = {Machine Learning with Applications},
  pages        = {100434},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Constructing portfolios using stable distributions: The case of S&amp;P 500 sectors exchange-traded funds},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph-based approach to client relationship management in
fund administration. <em>MLA</em>, <em>10</em>, 100433. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring effective and timely communication in the fund administration industry has become an essential element of client relationship management (CRM). A CRM team periodically evaluates asset managers perception of service quality using surveys, where it can be difficult to evaluate service quality objectively. Within the asset management industry, despite ongoing technological advances, the main channel of communication remains email. The sheer volume of email, and the industry’s reliance on it, can lead to practical problems that impact CRM. In this work we draw insights from the email communications between an operations team and two-sample clients to understand client relationships in a way not previously possible. The results are presented and we discuss how these can quantitatively support and improve service quality evaluations in CRM. For this application, we exploit the social relations in emails via a graph-based approach. A deep learning framework is described that allows a graph-based inspection of the email communications between asset managers and their fund administrators operations teams. The presented framework integrates a natural language processing model to transform email subject lines to embedding representations, a knowledge graph to transform the email communication links into a graph representation , and a graph neural network to process the embedding representations and classify the email communications. The classification of critical conversations via email is a demonstrative example of a scalable graph-based approach that allows the use of machine learning to process, learn, and explore the relations existing between the emails.},
  archive      = {J_MLA},
  author       = {Michalis Frangos and Fergal O’Shea},
  doi          = {10.1016/j.mlwa.2022.100433},
  journal      = {Machine Learning with Applications},
  pages        = {100433},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A graph-based approach to client relationship management in fund administration},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of financial distress of companies with
artificial neural networks and decision trees models. <em>MLA</em>,
<em>10</em>, 100432. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational failures are closely related to many interest groups within and outside of the companies. Businesses may face financial failure as a result of the market conditions of the economy as much as the internal factors while maintaining their activities. Failure in managing the risks they face in these circumstances can lead to bankruptcy. For this reason, companies should be able to foresee their failures and consider correct measures by analyzing their current situation. With this motivation, to estimate and classify the financial failures of companies operating in different sectors a model is constructed using artificial neural networks (ANN) and decision trees (DTs). Unique models have been developed for each sector and it is aimed to compare the correct classification of non-bankrupt rates based on sector and to determine the most important variables affecting the financial failures on sectoral basis. In this context, for the companies listed in the BIST, 25 financial ratios and 2 non-financial variable were selected from 240 companies operating in manufacturing, service and trade sectors. In the model, near-zero error value has been targeted and non-bankrupt and bankrupt companies in the model have been classified correctly.},
  archive      = {J_MLA},
  author       = {Nezir Aydin and Nida Sahin and Muhammet Deveci and Dragan Pamucar},
  doi          = {10.1016/j.mlwa.2022.100432},
  journal      = {Machine Learning with Applications},
  pages        = {100432},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of financial distress of companies with artificial neural networks and decision trees models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing a supervised feature selection technique for mixed
attribute data analysis. <em>MLA</em>, <em>10</em>, 100431. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying optimal features is critical for increasing the overall performance of data classification . This paper introduces a supervised feature selection technique for analyzing mixed attribute data. It measures data classification performances of features with a user-defined performance criterion and determines optimal features to boost the overall data analysis performance. A performance evaluation is managed to highlight the usefulness of the technique with existing feature selection techniques such as analysis of variance test, chi-square test, principal component analysis, and mutual information. Visualization is also utilized to understand the differences in classifying instances with different features. From a comparative performance testing and evaluation, we found 5 ∼ ∼ 10\% performance improvements with the proposed technique. Overall, evaluation results showed the usefulness of our proposed feature selection technique in mixed attribute data analysis.},
  archive      = {J_MLA},
  author       = {Dong Hyun Jeong and Bong Keun Jeong and Nandi Leslie and Charles Kamhoua and Soo-Yeon Ji},
  doi          = {10.1016/j.mlwa.2022.100431},
  journal      = {Machine Learning with Applications},
  pages        = {100431},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Designing a supervised feature selection technique for mixed attribute data analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fully automatic framework for evaluating cosmetic results
of breast conserving therapy. <em>MLA</em>, <em>10</em>, 100430. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breast cosmetic outcome after breast conserving therapy is essential for evaluating breast treatment and determining patient’s remedy selection. This prompts the need of objective and efficient methods for breast cosmesis evaluations. However, current evaluation methods rely on ratings from a small group of physicians or semi-automated pipelines, making the processes time-consuming and their results inconsistent. To solve the problem, in this study, we proposed: 1. a fully-automatic Machine Learning Breast Cosmetic evaluation algorithm leveraging the state-of-the-art Deep Learning algorithms for breast detection and contour annotation, 2. a novel set of Breast Cosmesis features, 3. a new Breast Cosmetic dataset consisting 3k+ images from three clinical trials with human annotations on both breast components and their cosmesis scores. We show our fully-automatic framework can achieve comparable performance to state-of-the-art without the need of human inputs, leading to a more objective, low-cost and scalable solution for breast cosmetic evaluation in breast cancer treatment.},
  archive      = {J_MLA},
  author       = {Chenqi Guo and Tamara L. Smith and Qianli Feng and Fabian Benitez-Quiroz and Frank Vicini and Douglas Arthur and Julia White and Aleix Martinez},
  doi          = {10.1016/j.mlwa.2022.100430},
  journal      = {Machine Learning with Applications},
  pages        = {100430},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A fully automatic framework for evaluating cosmetic results of breast conserving therapy},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tabular machine learning using conjunctive threshold neural
networks. <em>MLA</em>, <em>10</em>, 100429. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel three-layer neural network architecture with threshold activations for tabular data classification problems. The hidden layer units correspond to trainable neurons with arbitrary weights and biases and a step activation. These neurons are logically equivalent to threshold logic functions. The output layer neuron is also a threshold function that implements a conjunction of the hidden layer threshold functions. This neural network architecture can leverage state-of-the-art network training methods to achieve high prediction accuracy, and the network is designed so that minimal human understandable explanations can be readily derived from the model. Further, we employ a sparsity-promoting regularization approach to sparsify the threshold functions to simplify them, and to sparsify the output neuron so that it only depends on a small subset of hidden layer threshold functions. Experimental results show that our approach outperforms other state-of-the-art interpretable decision models in prediction accuracy.},
  archive      = {J_MLA},
  author       = {Weijia Wang and Litao Qiao and Bill Lin},
  doi          = {10.1016/j.mlwa.2022.100429},
  journal      = {Machine Learning with Applications},
  pages        = {100429},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Tabular machine learning using conjunctive threshold neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised bidirectional RNN for misinformation
detection. <em>MLA</em>, <em>10</em>, 100428. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misinformation refers to inaccurate information created to misguide the readers. It spreads on social platforms like Twitter with various presentations such as fake news and rumors that usually contain numbers, categorical information, texts, images, etc., which has become a global issue of cybersecurity. We propose a semi-supervised deep model based on bidirectional recurrent neural networks (Bi-RNN) to detect misinformation with limited labeled data and large unlabeled data . The proposed model consists of three components, namely, shared Bi-RNN, supervised Bi-RNN, and unsupervised Bi-RNN. Specifically, the shared Bi-RNN provides common features that input to the supervised Bi-RNN and unsupervised Bi-RNN, and they jointly optimize two losses, namely, cross-entropy loss and mean-square-error loss, using both labeled data and a large amount of unlabeled data. We validate our proposed model by testing on two benchmark datasets of misinformation: LIAR and PHEME. It is observed that the proposed model is able to achieve promising performance even with very limited labeled data for training when compared to baselines with supervised deep learning .},
  archive      = {J_MLA},
  author       = {Xishuang Dong and Lijun Qian},
  doi          = {10.1016/j.mlwa.2022.100428},
  journal      = {Machine Learning with Applications},
  pages        = {100428},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Semi-supervised bidirectional RNN for misinformation detection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic social distance estimation for photographic
studies: Performance evaluation, test benchmark, and algorithm.
<em>MLA</em>, <em>10</em>, 100427. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social distancing regulations introduced to slow down the spread of COVID-19 virus directly affect a basic form of non-verbal communication, and there may be longer term impacts on human behavior and culture that remain to be analyzed in proxemics studies. To obtain quantitative results for such studies, large media and/or personal photo collections must be analyzed. Several social distance monitoring methods have been proposed for safety purposes, but they are not directly applicable to general photo collections with large variations in the imaging setup. In such studies, the interest shifts from safety to analyzing subtle differences in social distances. Currently, there is no suitable benchmark for developing such algorithms. Collecting images with measured ground-truth pair-wise distances using different camera settings is cumbersome. Moreover, performance evaluation for these algorithms is not straightforward, and there is no widely accepted evaluation protocol. In this paper, we provide an image dataset with measured pair-wise social distances under different camera positions and settings. We suggest a performance evaluation protocol and provide a benchmark to easily evaluate such algorithms. We also propose an automatic social distance estimation method that can be applied on general photo collections. Our method is a hybrid method that combines deep learning-based object detection and human pose estimation with projective geometry. The method can be applied on uncalibrated single images with known focal length and sensor size. The results on our benchmark are encouraging with 91\% human detection rate and only 38.24\% average relative distance estimation error among the detected people.},
  archive      = {J_MLA},
  author       = {Mert Seker and Anssi Männistö and Alexandros Iosifidis and Jenni Raitoharju},
  doi          = {10.1016/j.mlwa.2022.100427},
  journal      = {Machine Learning with Applications},
  pages        = {100427},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Automatic social distance estimation for photographic studies: Performance evaluation, test benchmark, and algorithm},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of bradycardia in preterm infants using
artificial neural networks. <em>MLA</em>, <em>10</em>, 100426. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bradycardia is common in preterm infants and associated with a range of adverse outcomes, including end organ damage and developmental problems. This paper proposes a method to develop a generalised model to predict the onset of bradycardia in preterm infants by monitoring vital signs using artificial neural networks (ANN). Data used for network development was collected from a study conducted at the Royal Hobart Hospital involving 31 preterm infants, and comprising 3591 h of electrocardiogram (ECG) and respiratory motion recordings. ANNs with a multilayer perceptron architecture were employed with features from the ECG and respiratory signals as inputs. The ANN was trained to predict bradycardia within a pre-bradycardia period beginning 15 s prior to each bradycardic event. The ANN’s prediction capability was assessed using the area under the curve (AUC) of the receiver operating characteristic. Heart rate variability and respiration patterns were found to be indicative markers of an impending bradycardic event. When applied to new infants, the ANN using only ECG features achieved a mean AUC of 0.63, and the ANN using both respiratory features and ECG features achieved a mean AUC of 0.69. This approach has improved on previous attempts to predict bradycardia and should be further investigated.},
  archive      = {J_MLA},
  author       = {Haimin Jiang and Brian P. Salmon and Timothy J. Gale and Peter A. Dargaville},
  doi          = {10.1016/j.mlwa.2022.100426},
  journal      = {Machine Learning with Applications},
  pages        = {100426},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of bradycardia in preterm infants using artificial neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep bidirectional LSTM for the signal detection of
universal filtered multicarrier systems. <em>MLA</em>, <em>10</em>,
100425. (<a href="https://doi.org/10.1016/j.mlwa.2022.100425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal filtered multicarrier (UFMC) has emerged as a potential waveform contender of orthogonal frequency division multiplexing (OFDM) for the fifth generation (5G) and beyond wireless systems . In this paper, we propose a bidirectional long short-term memory (Bi-LSTM)-based detector for the UFMC system. The proposed detector directly detects the transmitted symbols using the deep learning (DL)-based training data. The system is first trained with the aid of training data and pilot symbols. The training tunes the DL-based network parameters. During the testing phase, the signal is detected using the trained network. The performance of the proposed scheme is compared with that of the DL-aided OFDM system, and with the signal detection strategies using the conventional channel estimation techniques. Our simulations show that the proposed Bi-LSTM-based DL can flexibly and effectively detect UFMC signals.},
  archive      = {J_MLA},
  author       = {Md. Ferdous Ahammed and A. Alim Molla and Rafiul Kadir and Mohammad Ismat Kadir},
  doi          = {10.1016/j.mlwa.2022.100425},
  journal      = {Machine Learning with Applications},
  pages        = {100425},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep bidirectional LSTM for the signal detection of universal filtered multicarrier systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A natural language processing and deep learning based model
for automated vehicle diagnostics using free-text customer service
reports. <em>MLA</em>, <em>10</em>, 100424. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Initial fault detection and diagnostics are imperative measures to improve the efficiency, safety, and stability of vehicle operation. In recent years, numerous studies have investigated data-driven approaches to improve the vehicle diagnostics process using available vehicle data. Moreover, data-driven methods are employed to enhance customer-service agent interactions. In this study, we demonstrate a machine learning pipeline to improve automated vehicle diagnostics. First, Natural Language Processing (NLP) is used to automate the extraction of crucial information from free-text failure reports (generated during customers’ calls to the service department). Then, deep learning algorithms are employed to validate service requests and filter vague or misleading claims. Ultimately, different classification algorithms are implemented to classify service requests so that valid service requests can be directed to the relevant service department. The proposed model – Bidirectional Long Short Term Memory (BiLSTM) along with Convolution Neural Network (CNN) – shows more than 18\% accuracy improvement in validating service requests compared to technicians’ capabilities. In addition, using domain-based NLP techniques at preprocessing and feature extraction stages along with CNN-BiLSTM based request validation enhanced the accuracy ( &gt; &amp;gt; 25\%), sensitivity ( &gt; &amp;gt; 39\%), specificity ( &gt; &amp;gt; 11\%), and precision ( &gt; &amp;gt; 11\%) of Gradient Tree Boosting (GTB) service classification model . The Receiver Operating Characteristic Area Under the Curve (ROC-AUC) reached 0.82.},
  archive      = {J_MLA},
  author       = {Ali Khodadadi and Soroush Ghandiparsi and Chen-Nee Chuah},
  doi          = {10.1016/j.mlwa.2022.100424},
  journal      = {Machine Learning with Applications},
  pages        = {100424},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A natural language processing and deep learning based model for automated vehicle diagnostics using free-text customer service reports},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive review of stacking methods for semantic
similarity measurement. <em>MLA</em>, <em>10</em>, 100423. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive review of stacking methods commonly used to address the challenge of automatic semantic similarity measurement in the literature. Since more than two decades of research have left various semantic similarity measures , scientists and practitioners often find many difficulties in choosing the best method to put into production. For this reason, a novel generation of strategies has been proposed to use basic semantic similarity measures using base estimators to achieve a better performance than could be gained from any of the semantic similarity measures. In this work, we analyze different stacking techniques, ranging from the classical algebraic methods to the most powerful ones based on hybridization, including blending, neural, fuzzy, and genetic-based stacking. Each technique excels in aspects such as simplicity, robustness, accuracy, interpretability , transferability, or a favorable combination of several of those aspects. The goal is that the reader can have an overview of the state-of-the-art in this field.},
  archive      = {J_MLA},
  author       = {Jorge Martinez-Gil},
  doi          = {10.1016/j.mlwa.2022.100423},
  journal      = {Machine Learning with Applications},
  pages        = {100423},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comprehensive review of stacking methods for semantic similarity measurement},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep convolution neural network sharing for the multi-label
images classification. <em>MLA</em>, <em>10</em>, 100422. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing issues related to multi-label classification is relevant in many fields of applications. In this work. We present a multi-label classification architecture based on Multi-Branch Neural Network Model (MBNN) that permits the network to encode data from multiple semi-parallel subnetworks or layers outputs separately. Different types of neural networks can be used in the MBNN, but the proposal is made with Convolutional Neural Networks subnetworks, trained, and joined in classifying the outputs (i.e., labels). The proposed work makes it possible to perform incremental changes on existing Multitask Learning architectures for an adaptation to the multi-label classification. These transformations lead us to define two new architectures (neural network multi-outputs and neural network multi-features) using the feature extractors from the pre-trained neural networks. The empirical and statistical results verify that the proposed multibranch neural network architecture performs better than other simple multi-label classification architectures. Later, the “network with multi-features” obtained the highest classification score than other deep neural networks with 83.31\% of the f1-score for the Amazon rainforest dataset. The f1-score values are 88.81\% for Pascal VOC 2007 dataset, 87.71\% for Nuswide, and 88.64\% for Pascal VOC 2012.},
  archive      = {J_MLA},
  author       = {Solemane Coulibaly and Bernard Kamsu-Foguem and Dantouma Kamissoko and Daouda Traore},
  doi          = {10.1016/j.mlwa.2022.100422},
  journal      = {Machine Learning with Applications},
  pages        = {100422},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep convolution neural network sharing for the multi-label images classification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-quality fracture network mapping using high frequency
logging while drilling (LWD) data: MSEEL case study. <em>MLA</em>,
<em>10</em>, 100421. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Marcellus Shale and Energy Environmental Laboratory (MSEEL) provides a comprehensive dataset and field tests that can be used to study the significance of preexisting natural fractures in different subsurface engineering problems such as the effectiveness of the stimulation of an unconventional reservoir, optimized geothermal fluids movement and integrity of the CO2 storage site. Conventionally natural fracture intensity is obtained using sonic and micro-resistivity imaging logs. However, these techniques significantly suffer from two major deficiencies: Human bias in log interpretation and extremely long interpretation time. These two deficiencies are well-recognized in the industry; however, no standard procedures exist to address them. In this study, a new automated machine learning workflow (AMLW) is introduced that uses the LWD high-resolution acceleration data along the horizontal laterals to predict the natural fracture intensities originally obtained using sonic and micro-resistivity imaging. The accuracy and robustness of the new workflow to predict the near wellbore fracture intensities are tested using both regression and classification approaches . Both the regression and classification approaches were able to predict the fracture intensities with high accuracy (average Mean Squared Error of 0.0085 for regression and average accuracy of 0.94 in the confusion matrix for classification). We have shown that only 10\%–15\% of the labeled resistivity image log is required for training and validation of the machine-learning model. The Automated workflow resulted in K-Neighbors Regressor and classifier algorithms as the best algorithms with a 52.74\% and 139.3\% improvement in comparison to the Gradient Boosting Regression algorithm (i.e. the fifth best algorithm).},
  archive      = {J_MLA},
  author       = {Ebrahim Fathi and Timothy R. Carr and Mohammad Faiq Adenan and Brian Panetta and Abhash Kumar and B.J. Carney},
  doi          = {10.1016/j.mlwa.2022.100421},
  journal      = {Machine Learning with Applications},
  pages        = {100421},
  shortjournal = {Mach. Learn. Appl.},
  title        = {High-quality fracture network mapping using high frequency logging while drilling (LWD) data: MSEEL case study},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Outliers in financial time series data: Outliers, margin
debt, and economic recession. <em>MLA</em>, <em>10</em>, 100420. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers in financial time series data are different from that in cross-sectional data in terms of the treatment and the detection . First, outliers in time series can be the focus of analysis itself, such as outliers in margin debt to indicate an overheating market. Second, the outlier detection in time series should be accompanied by decomposition to exclude inherent patterns. Unfortunately, there is a lack of consensus on the best decomposition method . Thus, we propose an ensemble model that combines multiple decomposition methods. Using the approach, we found that the outliers in margin debt are strong predictors of a recession.},
  archive      = {J_MLA},
  author       = {Kangbok Lee and Yeasung Jeong and Sunghoon Joo and Yeo Song Yoon and Sumin Han and Hyeoncheol Baik},
  doi          = {10.1016/j.mlwa.2022.100420},
  journal      = {Machine Learning with Applications},
  pages        = {100420},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Outliers in financial time series data: Outliers, margin debt, and economic recession},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Who believes they are good navigators? A machine learning
pipeline highlights the impact of gender, commuting time, and education.
<em>MLA</em>, <em>10</em>, 100419. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale digital data, which are becoming more prevalent, offer the potential to alleviate reproducibility concerns in psychology research findings. However, large scale digital data are not sufficient in and of themselves, thus necessitating the need for the development of machine learning (ML) pipelines that are capable of handling high dimensional datasets at scale. Such ML-based methodologies enable the analysis of complex relationships, which allows for the consideration of complicated demographics, a factor that is likely to play a role in the generalizability of research. We introduce a novel ML pipeline and demonstrate its potential on a large-scale digital dataset, Sea Hero Quest, a mobile game with data from nearly 770,000 players (ages 19 to 70, men N = 404,455, women N = 367,173). We analyzed how demographics are related to self-reported navigation ability using exploratory analysis, supervised and unsupervised learning . The results suggest that gender is the most important demographic factor in predicting self-reported navigation ability, followed by daily commuting time, age, and education, such that men (compared to women), long commuters (compared to those whose commuting time is shorter than 1 h), and older people with tertiary education (compared to younger people with secondary education) tended to evaluate themselves as better navigators. The large-scale dataset and ML pipeline capture influential factors, such as daily commuting time and education level, which have often been overlooked and are difficult to investigate with in-laboratory studies that use limited samples and traditional analytical techniques.},
  archive      = {J_MLA},
  author       = {You Cheng and Chuanxiuyue He and Mary Hegarty and Elizabeth R. Chrastil},
  doi          = {10.1016/j.mlwa.2022.100419},
  journal      = {Machine Learning with Applications},
  pages        = {100419},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Who believes they are good navigators? a machine learning pipeline highlights the impact of gender, commuting time, and education},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of machine learning in kidney disease diagnosis.
<em>MLA</em>, <em>10</em>, 100418. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of Machine learning (ML) in health informatics have gained increasing attention. The timely diagnosis of kidney disease and the subsequent immediate response to it are of the cases that shed light on the substantial role of ML diagnostic algorithms. ML in Kidney Disease Diagnosis (MLKDD) is an active research topic that aims at assisting physicians with computer-aided systems. Various investigations have tried to test the feasibility, applicability, and superiority of different ML methods over each other. However, lacking a holistic survey for this literature has always been a noticeable shortcoming. Hence, this paper provides a comprehensive literature review of ML utilizations in kidney disease diagnosis by introducing two different frameworks, one for MLs, classifying various aspects of kidney disease diagnosis, and the other is the framework of medical sub-fields related to MLKDD. In addition, research gaps are discovered, and future study directions are discussed.},
  archive      = {J_MLA},
  author       = {Jaber Qezelbash-Chamak and Saeid Badamchizadeh and Kourosh Eshghi and Yasaman Asadi},
  doi          = {10.1016/j.mlwa.2022.100418},
  journal      = {Machine Learning with Applications},
  pages        = {100418},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A survey of machine learning in kidney disease diagnosis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). INGARCH-based fuzzy clustering of count time series with a
football application. <em>MLA</em>, <em>10</em>, 100417. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there are many contributions in the time series clustering literature, few studies still deal with count time series data . This paper aims to develop a fuzzy clustering procedure for count time series data. We propose an Integer GARCH-based Fuzzy C C -medoids (INGARCH-FCMd) method for clustering count time series based on a Mahalanobis distance between the parameters estimated by an INGARCH model. We show how the proposed clustering method works by clustering football teams according to the number of scored goals.},
  archive      = {J_MLA},
  author       = {Roy Cerqueti and Pierpaolo D’Urso and Livia De Giovanni and Raffaele Mattera and Vincenzina Vitale},
  doi          = {10.1016/j.mlwa.2022.100417},
  journal      = {Machine Learning with Applications},
  pages        = {100417},
  shortjournal = {Mach. Learn. Appl.},
  title        = {INGARCH-based fuzzy clustering of count time series with a football application},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning of monocular depth, optical flow and
ego-motion with geometric guidance for UAV navigation in dynamic
environments. <em>MLA</em>, <em>10</em>, 100416. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision-based depth estimation and visual odometry provide perceptual information useful for robot navigation tasks like obstacle avoidance. However, despite the proliferation of state-of-the-art convolutional neural network (CNN) models for monocular depth, ego-motion and optical flow estimation , a relatively low volume of work has been reported on their practical applications in unmanned aerial vehicle (UAV) navigation. This is due to well-known challenges — embedded hardware constraints, viewpoint variations, scarcity of aerial image datasets, and intricacies of dynamic environments. We address these limitations to facilitate real-world deployment of CNN in UAV navigation. First, we devise efficient co nfidence w eighted a daptive n etwork ( Cowan ) training framework that iteratively leverages intermediate prediction confidences to enforce cross-task consistency over corresponding image regions. This achieves competitive accuracy with a lightweight CNN capable of real-time execution on resource-constrained embedded systems. Second, we devise a test-time refinement method that adapts the network to dynamic environments while simultaneously improving accuracy. To accomplish this, we first update ego-motion using pose information from on-board inertial measurement unit (IMU). Then, we decompose the UAV’s motion into constituent vectors, and for each axis, we formulate geometric relationships between depth and translation. Based on this information, we triangulate corresponding points acquired through optical flow. Finally, we enforce geometric consistency between the initially updated pose and triangulated depth. Cowan with geometric guided refinement ( Cowan-GGR ) achieves significant accuracy and robustness. Field tests show the proposed model is capable of accurate depth and object-level motion perception in real-world dynamic environments, thus proving its efficacy in facilitating UAV navigation.},
  archive      = {J_MLA},
  author       = {Fuseini Mumuni and Alhassan Mumuni and Christian Kwaku Amuzuvi},
  doi          = {10.1016/j.mlwa.2022.100416},
  journal      = {Machine Learning with Applications},
  pages        = {100416},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning of monocular depth, optical flow and ego-motion with geometric guidance for UAV navigation in dynamic environments},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple method to detect extreme events from financial time
series data. <em>MLA</em>, <em>10</em>, 100415. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock prices and other financial data fluctuate dramatically as a result of extreme events. We offer a technique for automatically detecting and ranking these events with financial time series data . Our technique works by fitting the tail of a moving window’s return distribution to a power law. We find that rapid changes in the tail slope may be easily connected with major events in the real world when applied to airline stock prices. Our technique also displays a distinct periodicity in tail behavior, implying that it may be utilized to anticipate long-term financial market developments. We also compare our technique to standard deviation, proving that our model outperforms it in several aspects.},
  archive      = {J_MLA},
  author       = {Tingyu Qu and Ko Wai Mei and Arnold Doray},
  doi          = {10.1016/j.mlwa.2022.100415},
  journal      = {Machine Learning with Applications},
  pages        = {100415},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A simple method to detect extreme events from financial time series data},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven market segmentation in hospitality using
unsupervised machine learning. <em>MLA</em>, <em>10</em>, 100414. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within hospitality, marketing departments use segmentation to create tailored strategies to ensure personalized marketing. This study provides a data-driven approach by clustering guest profiles via hierarchical clustering , based on an extensive set of features. The industry requires understandable outcomes that contribute to adaptability for marketing departments to make data-driven decisions and ultimately driving profit. A marketing department specified a business question that guides the unsupervised machine learning algorithm . Features of guests change over time; therefore, there is a probability that guests transition from one cluster to another. The purpose of the study is to provide steps in the process from raw data to actionable insights, which serve as a guideline for how hospitality companies can adopt an algorithmic approach .},
  archive      = {J_MLA},
  author       = {Rik van Leeuwen and Ger Koole},
  doi          = {10.1016/j.mlwa.2022.100414},
  journal      = {Machine Learning with Applications},
  pages        = {100414},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Data-driven market segmentation in hospitality using unsupervised machine learning},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated defect detection for coatings via height profiles
obtained by laser-scanning microscopy. <em>MLA</em>, <em>10</em>,
100413. (<a href="https://doi.org/10.1016/j.mlwa.2022.100413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern application technology can never entirely rule out defects during refinishing. Only the correct assessment creates the prerequisite for eliminating defect causes and for meaningful damage repair on the painted object. Intelligent software tools open up novel solutions in material sciences by combining chemistry, engineering, and machine learning with high-throughput-formulation-screening (HTFS) technologies. In this paper, we present a data-driven method for automated defect detection on coatings. In contrast to other approaches based on machine learning, our method requires relatively few annotated samples as it belongs to the Few-Shot Learning paradigm. We demonstrate the benefits of few-shot algorithms in a real-world industrial application in chemistry. The data is collected effectively via laser-scanning microscopy, providing a 3D image with a height profile. In addition, we propose a continuous metric to quantify the quality of the evaluated coating sample.},
  archive      = {J_MLA},
  author       = {Sayed Hoseini and Gaoyuan Zhang and Alexander Jongbloed and Christian Schmitz and Christoph Quix},
  doi          = {10.1016/j.mlwa.2022.100413},
  journal      = {Machine Learning with Applications},
  pages        = {100413},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Automated defect detection for coatings via height profiles obtained by laser-scanning microscopy},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating a face generator from a human perspective.
<em>MLA</em>, <em>10</em>, 100412. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN2 is able to generate very realistic and high-quality faces of humans using a training set ( FFHQ ). Instead of using one of the many commonly used metrics to evaluate the performance of a face generator (e.g., FID , IS and P&amp;R ), this paper uses a more humanlike approach providing a different outlook on the performance of StyleGAN2. The generator within StyleGAN2 tries to learn the distribution of the input dataset. However, this does not necessarily mean that higher-level human concepts are preserved. We examine if general human attributes, such as age and gender, are transferred to the output dataset and if StyleGAN2 is able to generate actual new persons according to facial recognition methods. It is crucial for practical implementations that a face generator not only generates new humans, but that these humans are not clones of the original identities. This article addresses these questions. Although our approach can be used for other face generators, we only focused on StyleGAN2. First, multiple models are used to predict general human attributes. This shows that the generated images have the same attribute distributions as the input dataset. However, if truncation is applied to limit the latent variable space, the attribute distributions change towards the attributes corresponding with the latent variable used in truncation. Second, by clustering using face recognition models, we demonstrate that the generated images do not belong to an existing person from the input dataset. Thus, StyleGAN2 is able to generate new persons with similar human characteristics as the input dataset.},
  archive      = {J_MLA},
  author       = {Joris Pries and Sandjai Bhulai and Rob van der Mei},
  doi          = {10.1016/j.mlwa.2022.100412},
  journal      = {Machine Learning with Applications},
  pages        = {100412},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Evaluating a face generator from a human perspective},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Re-annotation of training samples for robust maritime object
detection. <em>MLA</em>, <em>10</em>, 100411. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and specifically deep learning techniques address many of the issues faced in visual object detection and classification tasks . However, they have the caveat of needing large amounts of annotated training data. In the maritime domain one may encounter objects fairly infrequently, depending on weather and location. This creates an issue of data collection. Areas such as harbors and channels see a lot of traffic, but the ships are of a specific class. Furthermore, the variability of the buoys from region to region and within regions is difficult and expensive to sample. Thus the amount and quality of available data is severely lacking. Furthermore very few publicly available maritime datasets exist In this work, we present a novel approach that detects possible “poor” training samples and automatically re-annotates them, based on the current state of the object detector. We show the applicability of our approach on real-life maritime data and show that the poor annotation quality of the datasets used can be mitigated. We show performance gain with respect to a baseline approach is proportional to the amount of poorly annotated data in the dataset. When 25\% of the data is poor we achieve a 5.5\%, 13.7\%, and 8.0\% increase in performance on 3 separate datasets, compared to a baseline model . With 50\% noise we reach 58.5\%, 18.7\% and 94.2\% increase respectively. Our approach also allows for the iterative improvement of a given dataset by providing a set of pseudo-annotations to replace the current incorrect ones.},
  archive      = {J_MLA},
  author       = {Jonathan Becktor and Evangelos Boukas and Lazaros Nalpantidis},
  doi          = {10.1016/j.mlwa.2022.100411},
  journal      = {Machine Learning with Applications},
  pages        = {100411},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Re-annotation of training samples for robust maritime object detection},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using machine learning to analyze and predict entry patterns
of low-cost airlines: A study of southwest airlines. <em>MLA</em>,
<em>10</em>, 100410. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper uses machine learning to analyze and predict entry patterns of Southwest Airlines into various city pair. The purpose is to understand the parameters impacting the decision to enter into a city pair, by a low cost airline. Decision to enter (exit) from a market depends on endogenous factors and exogenous factors, such as decisions of other airlines, passenger profile, airport profile, competition on that sector, global economic conditions. The paper uses supervised machine learning to understand and predict a low cost airlines decision to enter (exit) a specific city pair. The uniqueness of this paper is that this kind of prediction was done for the first time using the existing data pre-processing and machine learning techniques. Moreover, an analysis of Southwest’s entry patterns would help (1) competing airlines to better plan their networks, (2) non-competing airlines to learn about how to plan their entry patterns and (3) airports to better plan their operations and slots. The analysis clearly showed a shift in Southwest’s entry and exit strategy over the last several years.},
  archive      = {J_MLA},
  author       = {Sri Lakshmi Vadlamani and M. Omair Shafiq and Olga Baysal},
  doi          = {10.1016/j.mlwa.2022.100410},
  journal      = {Machine Learning with Applications},
  pages        = {100410},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Using machine learning to analyze and predict entry patterns of low-cost airlines: A study of southwest airlines},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stability-certified reinforcement learning control via
spectral normalization. <em>MLA</em>, <em>10</em>, 100409. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, two types of methods from different perspectives based on spectral normalization (SN) are described for ensuring the stability of a feedback system controlled by a neural network (NN). The first one is that the L 2 L2 gain of the feedback system is bounded less than 1 to satisfy a stability condition derived from the small-gain theorem. When explicitly including the stability condition, the first type of method may provide an insufficient performance on the NN controller due to its strict stability condition. To overcome this difficulty, the second type of method is proposed, ensuring local stability with a larger region of attraction. In this second type, the stability is ensured by solving linear matrix inequalities after training the NN controller. SN improves the feasibility of the a posteriori stability test by constructing tighter local sectors. Numerical experiments show that the second type of method provides sufficient performance compared with the first one and ensures sufficient stability compared with existing reinforcement learning algorithms. 1},
  archive      = {J_MLA},
  author       = {Ryoichi Takase and Nobuyuki Yoshikawa and Toshisada Mariyama and Takeshi Tsuchiya},
  doi          = {10.1016/j.mlwa.2022.100409},
  journal      = {Machine Learning with Applications},
  pages        = {100409},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Stability-certified reinforcement learning control via spectral normalization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictors of COVID-19 vaccination rate in USA: A machine
learning approach. <em>MLA</em>, <em>10</em>, 100408. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we examine state-level features and policies that are most important in achieving a threshold level vaccination rate to curve the effects of the COVID-19 pandemic. We employ CHAID, a decision tree algorithm , on three different model specifications to answer this question based on a dataset that includes all the states in the United States. Workplace travel emerges as the most important predictor; however, the governors’ political affiliation (PA) replaces it in a more conservative feature set that includes economic features and the growth rate of COVID-19 cases. We also employ several alternative algorithms as a robustness check. Results from these checks confirm our original findings regarding workplace travels and political affiliation. The accuracy under different model specifications ranges from 80\%–88\%, whereas the sensitivity is between 92.5\%–100\%. Our findings provide actionable policy insights to increase vaccination rates and combat the COVID-19 pandemic.},
  archive      = {J_MLA},
  author       = {Syed Muhammad Ishraque Osman and Ahmed Sabit},
  doi          = {10.1016/j.mlwa.2022.100408},
  journal      = {Machine Learning with Applications},
  pages        = {100408},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predictors of COVID-19 vaccination rate in USA: A machine learning approach},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal-stochastic tensor features for action recognition.
<em>MLA</em>, <em>10</em>, 100407. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Temporal-Stochastic Product Grassmann Manifold (TS-PGM), an efficient method for tensor classification in tasks such as gesture and action recognition. Our approach builds on the idea of representing tensors as points on Product Grassmann Manifold (PGM). This is achieved by mapping tensor modes to linear subspaces , where each subspace can be seen as a point on a Grassmann Manifold (GM) of the corresponding mode. Subsequently, it is possible to unify factor manifolds of respective modes in a natural way via PGM. However, this approach possibly discards discriminative information by treating all modes equally, and not considering the nature of temporal tensors such as videos. Therefore, we introduce Temporal-Stochastic Tensor features (TST features) to extract temporal information from tensors and encode them in a sequence-preserving TST subspace. These features and regular tensor modes can then be simultaneously used on PGM. Our framework addresses the problem of classification of temporal tensors while inheriting the unified mathematical interpretation of PGM because the TST subspace can be naturally integrated into PGM as a new factor manifold. Additionally, we enhance our method in two ways: (1) we improve the discrimination ability by projecting subspaces onto a Generalized Difference Subspace, and (2) we utilize kernel mapping to construct kernelized subspaces able to handle nonlinear data distribution. Experimental results on gesture and action recognition datasets show that our methods based on subspace representation with explicit TST features outperform pure spatio-temporal approaches.},
  archive      = {J_MLA},
  author       = {Bojan Batalo and Lincon S. Souza and Bernardo B. Gatto and Naoya Sogi and Kazuhiro Fukui},
  doi          = {10.1016/j.mlwa.2022.100407},
  journal      = {Machine Learning with Applications},
  pages        = {100407},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Temporal-stochastic tensor features for action recognition},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Increasing trust and fairness in machine learning
applications within the mortgage industry. <em>MLA</em>, <em>10</em>,
100406. (<a href="https://doi.org/10.1016/j.mlwa.2022.100406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of machine learning in applications provides opportunities for increased efficiency in many organisations. However, the deployment of such systems is often hampered by the lack of insight into how their decisions are reached, resulting in concerns about trust and fairness. In this article, we investigate to what extent the addition of explainable AI components to ML applications can contribute to alleviating these issues. As part of this research, explainable AI functionality was developed for an existing ML model used for mortgage fraud detection at a large international financial institution based in The Netherlands A system implementing local explanation techniques was deployed to support the day-to-day work of fraud detection experts working with the model. In addition, a second system implementing global explanation techniques was developed to support the model management processes involving data-scientists, legal experts and compliance officers. A controlled experiment using actual mortgage applications was carried out to measure the effectiveness of these two systems, using both quantitative and qualitative assessment methods. Our results show that the addition of explainable AI functionality results in a statistically significant improvement in the levels of trust and usability by its daily users. The explainable AI system implementing global interpretability was found to considerably increase confidence in the ability to perform the processes focused on compliance and fairness. In particular, bias detection towards demographic groups successfully aided in the identification and removal of bias towards applicants with a migration background.},
  archive      = {J_MLA},
  author       = {W. van Zetten and G.J. Ramackers and H.H. Hoos},
  doi          = {10.1016/j.mlwa.2022.100406},
  journal      = {Machine Learning with Applications},
  pages        = {100406},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Increasing trust and fairness in machine learning applications within the mortgage industry},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the danger of snow avalanches with a machine
learning approach using a comprehensive snow cover model. <em>MLA</em>,
<em>10</em>, 100405. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The avalanche warning services use the regional avalanche danger level and activity as key metrics. In this study a dataset of a ski resort from 2012 to 2020 was used. The local avalanche danger level, the success of artificial avalanche release by snow groomer or blasting, and the decision to try an avalanche release by blasting are target variables. Five meteorological recordings and twenty five variables of the modeled snowpack with an hourly resolution were used as input for the machine learning approach . An artificial neural network consists of recurrent layers and convolutional layers for merging the temporal and spatial data. Support vector machines were adapted to calculate the probabilities for the target variables. A logistic regression was used as ensemble method as stacked generalization. The accuracy of the models, i.e., the precision for predicting the local avalanche danger level was 0.73 and 0.52 for the train and test data, respectively. The precisions for artificial avalanche release by snow groomer were 0.94 and 0.85, for artificial avalanche release by blasting 0.75 and 0.72, and for the decision to try an avalanche release by blasting 0.75 and 0.70. Unlike previous studies, which used regional avalanche danger levels as target variable, we used local warning levels and additional target variables. The comparison shows that despite differences similar accuracy can be achieved. We further identified opportunities to optimize the model by examining the relevance of the input variables and performing a sensitivity analysis.},
  archive      = {J_MLA},
  author       = {Reinhard Fromm and Christine Schönberger},
  doi          = {10.1016/j.mlwa.2022.100405},
  journal      = {Machine Learning with Applications},
  pages        = {100405},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Estimating the danger of snow avalanches with a machine learning approach using a comprehensive snow cover model},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving text classification with transformers and layer
normalization. <em>MLA</em>, <em>10</em>, 100403. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than 25,000 injuries and 25 fatalities occur each year due to unstable furniture tip-over incidents. Classifying these furniture tip-over incidents is an essential task for understanding incident patterns and building safer products. For example, this classification can help standards development organizations (SDOs) and policy makers discover hidden insights, which can be used to develop standards and regulations that help improve furniture and make homes safer. Since 2000, the U.S. Consumer Product Safety Commission (CPSC) has published data related to consumer product injuries. The amount of data has grown rapidly, and the process of manually reviewing and classifying individual incidents has correspondingly become very resource intensive. This paper proposes an improved method that employs a combination of natural language processing (NLP) techniques and machine learning (ML) algorithms to classify textual data. Machine learning models can help reduce time and effort by streamlining incident narrative classification for determining whether incidents are related to furniture tip-overs. Challenges often presented by real-world data sets (such as the CPSC data used in our experiment) include imbalanced target classes and narratives requiring domain knowledge, since the data sets contain abbreviations and jargon. Using out-of-the-box, default classification models such as bidirectional encoder representations from transformers (BERT) might not yield adequate results. Our proposed method adds layer normalization and dropout layers to a transformer-based language model , which achieves better classification results than using a transformer-based language alone with imbalanced classes. We carefully measure the impact of hidden layers in order to fine-tune the model.},
  archive      = {J_MLA},
  author       = {Ben Rodrawangpai and Witawat Daungjaiboon},
  doi          = {10.1016/j.mlwa.2022.100403},
  journal      = {Machine Learning with Applications},
  pages        = {100403},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Improving text classification with transformers and layer normalization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural networks: A bibliometrics overview.
<em>MLA</em>, <em>10</em>, 100401. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks (GNNs) have become a hot topic in machine learning community. This paper presents a Scopus-based bibliometric overview of the GNNs’ research since 2004 when GNN papers were first published. The study aims to evaluate GNN research trends, both quantitatively and qualitatively. We provide the trend of research, distribution of subjects, active and influential authors and institutions, sources of publications, most cited documents, and hot topics. Our investigations reveal that the most frequent subject categories in this field are computer science, engineering, and telecommunications. In addition, the most active source of GNN publications is Lecture Notes in Computer Science. The most prolific or impactful institutions are found in the United States, China, and Canada. We also provide must-read papers based on citation count and future directions. Our analysis reveals that node classification is the most popular task, followed by link prediction, and graph classification in the GNN literature. Moreover, the results suggest that the application of graph convolutional networks and attention mechanisms are now among hot topics of GNN research. Finally, scalability, generalization, over-smoothing, and explainability of graph neural networks are some research directions to pursue.},
  archive      = {J_MLA},
  author       = {Abdalsamad Keramatfar and Mohadeseh Rafiee and Hossein Amirkhani},
  doi          = {10.1016/j.mlwa.2022.100401},
  journal      = {Machine Learning with Applications},
  pages        = {100401},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Graph neural networks: A bibliometrics overview},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved SqueezeNet model for the diagnosis of lung
cancer in CT scans. <em>MLA</em>, <em>10</em>, 100399. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is the leading cause of cancer deaths nowadays and its early detection and treatment plays an important role in survival of patients. The main challenge is to acquire an accurate diagnosis in a limited time and without the need of massive computing power. Here, we propose SqueezeNodule-Net, a light and accurate convolutional neural network (CNN) that can rapidly classify nodules into malignant and benign, requiring only a mid-range computing system. It is based on the compact CNN model SqueezeNet and its Fire Module, whose structure we modified in two different ways and compared them with state-of-the-art models. We used 888 CT scans from the public dataset LUNA16 from which, after appropriate preprocessing, we generated 2D 50 × 50 images of benign and malignant nodules. We, also, produced 3D images in order to prove that our models can run successfully with more spatial information by using the same computing system. For 2D images, SqueezeNodule-Net V1 achieves 93.2\% accuracy, 94.6\% specificity and 89.2\% sensitivity, while the SqueezeNodule-Net V2 achieves 94.3\% accuracy, 95.3\% specificity and 91.3\% sensitivity. In 3D space, SqueezeNodule-Net V1 gives 94.3\% accuracy, 96.0\% specificity and 87.4\% sensitivity, while SqueezeNodule-Net V2 gives 95.8\% accuracy, 96.2\% specificity and 90.2\% sensitivity. Overall, compared to Squeeze-Net, SqueezeNodule-Net V1 is 1.2–1.06 times smaller, 1.31–1.5 times faster and has 0.8–2.5 better classification performance, while SqueezeNodule-Net V2 is 1.4–1.5 time larger, 0.04–1.5 times faster and has 0.1–2.7 times better classification performance.},
  archive      = {J_MLA},
  author       = {Michail Tsivgoulis and Thomas Papastergiou and Vasilis Megalooikonomou},
  doi          = {10.1016/j.mlwa.2022.100399},
  journal      = {Machine Learning with Applications},
  pages        = {100399},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An improved SqueezeNet model for the diagnosis of lung cancer in CT scans},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting stress through 2D ECG images using pretrained
models, transfer learning and model compression techniques.
<em>MLA</em>, <em>10</em>, 100395. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress is a major part of our everyday life, associated with most activities we perform on a daily basis and if we are not careful about managing stress, it can have a detrimental impact on our health. Despite recent advances in this domain, HRV analysis is still the most common method to detect stress, and although the results that have been produced are admirable, feature extraction is complicated and time consuming. We propose an algorithm to convert 1D (dimensional) ECG data from WESAD (wearable stress and affect detection dataset) into 2D ECG images, which are representative of stress/not stress. It does not require time consuming processes such as feature extraction and filtering. We utilize transfer learning to obtain competitive results. We also demonstrate that model compression techniques can significantly reduce the computational size of the algorithms, without sacrificing much of the performance, as evident from a classification accuracy of 90.62\% using the quantization technique. Results substantiate the effectiveness of our proposed method and empirically demonstrates the potential of deep learning algorithms for edge computing and mobile applications, which utilizes low performing hardware.},
  archive      = {J_MLA},
  author       = {Syem Ishaque and Naimul Khan and Sri Krishnan},
  doi          = {10.1016/j.mlwa.2022.100395},
  journal      = {Machine Learning with Applications},
  pages        = {100395},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Detecting stress through 2D ECG images using pretrained models, transfer learning and model compression techniques},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A single supervised learning model to detect fake access
points, frequency sweeping jamming and deauthentication attacks in IEEE
802.11 networks. <em>MLA</em>, <em>10</em>, 100389. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless networks are nowadays indispensable components of telecommunication infrastructures. They offer flexibility, mobility and rapid expansion of telecommunication infrastructures. In wireless networks, transmissions are unisolated and most commonly emitted using omnidirectional antennas . This makes wireless networks more vulnerable to some specific attacks as compared to wired networks. For instance, attacks such as fake access points, intentional jamming and deauthentication can be easily perpetrated against IEEE 802.11 networks using freely accessible software and cheap hardware. Intentional jamming and deauthentication attacks are standalone attacks, but they can be combined with the fake access point attack to increase the latter’s effectiveness. In our research, we work on methods to detect the three different attacks when they are perpetrated independently (one at a time) or concurrently (several at the same time). In this contribution, we present a model that can detect the three attacks, when perpetrated independently, by analysing a set of features (frame interval, Received Signal Strength Indicator, sequence number gap and management frame subtype) extracted from IEEE 802.11 management frame and radiotap headers. We have implemented the model using several supervised learning algorithms. The model with Random Forest and the K-Nearest Neighbour predictors have best detection precision (over 96\%) for fake access point and deauthentication attacks and perfectible detection precision for the intentional jamming attack (over 81\%).},
  archive      = {J_MLA},
  author       = {Andy Amoordon and Virginie Deniau and Anthony Fleury and Christophe Gransart},
  doi          = {10.1016/j.mlwa.2022.100389},
  journal      = {Machine Learning with Applications},
  pages        = {100389},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A single supervised learning model to detect fake access points, frequency sweeping jamming and deauthentication attacks in IEEE 802.11 networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A climate classification for corrosion control in electronic
system design. <em>MLA</em>, <em>9</em>, 100397. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate factors such as humidity and temperature have a significant impact on the corrosion reliability of electronic products. Given the huge geographical variability in climate conditions globally, a climate classification is a useful tool that simplifies the problem of considering climate when designing electronics packaging. Most current guidelines for electronic product design rely on the Köppen–Geiger classification first developed by Köppen over a century ago. Köppen devised a set of heuristics to separate climates to match different vegetation types. These climate classes are unlikely to be the optimal for electronic product design. This paper presents a new climate classification using parameters important for corrosion reliability of electronics. The classification is based on real climate data measured every 3 h during a 5-year period at over 9000 locations globally. A key step is defining relevant features of climate affecting corrosion in electronics. Features related to temperature are defined, but also the amount of time that the difference between Temperature and Dew Point is less than 1, 2 or 3 ℃. These features relate to the risk of condensation in electronic products. The features are defined such that diurnal, seasonal and yearly variation is taken into account. The locations are then clustered using K K -means clustering to obtain the relevant climate classes. This data-driven classification, based on key features for corrosion reliability of electronics, will be a useful aid for product design, reliability testing and lifetime estimation.},
  archive      = {J_MLA},
  author       = {Max Spooner and Rajan Ambat and Hélène Conseil-Gudla and Murat Kulahci},
  doi          = {10.1016/j.mlwa.2022.100397},
  journal      = {Machine Learning with Applications},
  pages        = {100397},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A climate classification for corrosion control in electronic system design},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-trial stimuli classification from detected p300 for
augmented brain–computer interface: A deep learning approach.
<em>MLA</em>, <em>9</em>, 100393. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of advanced Brain–Computer Interfaces (BCIs) is to connect the human brain with an external device without using the muscular system. To do this, they must effectively process mental activity and infer information on the users’ intentions and directives. This work proposes a novel and explainable BCI system capable of recognizing P300 deflection in single-trial EEGs with higher accuracy compared to the literature gold standard. Moreover, the proposed deep-learning approach allows us to go beyond the mere P300 detection, which is, to our best knowledge, the current state of the art. Indeed, we first identify the P300-related signal in the single-trial EEG signal, and then, we further discriminate the ERPs associated to the detected P300 between visual and auditory stimuli-related. To do this, we employ a CNN–LSTM neural network , which manages a 3D data representation of the acquired EEG signals. The performance of the approach is tested on experiments carried out on 22 subjects, revealing a 82.4\% F1-score in P300 identification and 82.4\% discriminating between visual and auditory stimuli. The employed algorithmic procedure also reports the most relevant each EEG channels in determining the predictions, adding interpretability to the proposed AI-based tools. These results pave the way for more sophisticated BCIs, capable of extending the set of available actions for the patients. The project was pre-approved by the Research Assessment Committee of the Department of Psychology (CRIP) for minimal risk projects, under the aegis of the Ethical Committee of University of Milano-Bicocca, on May 27th, 2019, protocol number RM-2019-193.},
  archive      = {J_MLA},
  author       = {Jessica Leoni and Silvia Carla Strada and Mara Tanelli and Alessandra Brusa and Alice Mado Proverbio},
  doi          = {10.1016/j.mlwa.2022.100393},
  journal      = {Machine Learning with Applications},
  pages        = {100393},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Single-trial stimuli classification from detected p300 for augmented Brain–Computer interface: A deep learning approach},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noninvasive acoustic time-of-flight measurements in heated,
hermetically-sealed high explosives using a convolutional neural
network. <em>MLA</em>, <em>9</em>, 100391. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a data-driven technique for measuring the time-of-flight through material sealed within a container. Time-of-flight measurement provides a noninvasive means of quantifying the sound speed profile within a material by transmitting an acoustic burst and then measuring the time required for the burst to arrive at an opposing receiver. In a hermetically-sealed cylindrical container, a portion of the acoustic energy propagates through the material as a bulk wave, while the remainder of the acoustic energy propagates around the container walls as guided waves. As a result, interference from the guided waves obscures the bulk arrival, inhibiting measurement of the sound speed. The technique uses a Convolutional Neural Network (CNN) to identify critical features in the measured waveforms and identify bulk wave arrivals. We demonstrate this time-of-flight measurement technique on high explosive-filled containers as they are heated from room temperature to detonation. This is a particularly challenging application for acoustic time-of-flight measurements as the high explosives have significant sound speed gradients as they undergo heating, and they lead to significant attenuation of the bulk wave, as opposed to the guided waves, which do not suffer significant attenuation. We characterize the performance of the CNN as a function of the high explosive temperature and as a function of the CNN hyperparameters. We then provide physical insight into the error trends.},
  archive      = {J_MLA},
  author       = {John Greenhall and David Zerkle and Eric Sean Davis and Robert Broilo and Cristian Pantea},
  doi          = {10.1016/j.mlwa.2022.100391},
  journal      = {Machine Learning with Applications},
  pages        = {100391},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Noninvasive acoustic time-of-flight measurements in heated, hermetically-sealed high explosives using a convolutional neural network},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ABC: Artificial intelligence for bladder cancer grading
system. <em>MLA</em>, <em>9</em>, 100387. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bladder cancer tissue grading, which assigns a numerical grade reflecting how aggressive a tumor looks under a microscope, is essential to determine the proper course of treatment, design a therapeutic plan and determine prognosis. The major problem is that there are considerable and clinically relevant variations in grading by pathologists – as they are humans with different opinions and experience – including in bladder cancer. This work presents a solution, i.e., Artificial Intelligence for Bladder Cancer grading (ABC) system, that is developed based on deep neural network architectures to provide a more reliable and accurate diagnosis for patients affected by this deadly disease and ultimately improve management and clinical outcomes. Whole Slide Images (WSI) are split up into equally-sized square tiles and annotated to build a training dataset. ABC introduces a new grading system concept that can provide a percentage distribution of each different grade in a specific tumor, unlike the current numerical grade value between 1 and 3 based on the general impression of the pathologist. This new approach aims to provide a more granular grading of bladder cancer tissues and better capture tumor grade heterogeneity. This new concept may offer a more precise prognosis and optimize management in the future. The ABC learning model is fully configurable, and any deep architecture model can be trained and used by ABC. Some trained models developed by ABC have shown high accuracy and consistency in grading and intra-observer variability. The combination of a loosely coupled architecture and fully integrated tiles’ utilization makes ABC a universal, scalable, and versatile system that could be configured and deployed worldwide.},
  archive      = {J_MLA},
  author       = {Khashayar Habibi and Kayvan Tirdad and Alex Dela Cruz and Kenneth Wenger and Andrea Mari and Mayada Basheer and Cynthia Kuk and Bas W.G. van Rhijn and Alexandre R. Zlotta and Theodorus H. van der Kwast and Alireza Sadeghian},
  doi          = {10.1016/j.mlwa.2022.100387},
  journal      = {Machine Learning with Applications},
  pages        = {100387},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ABC: Artificial intelligence for bladder cancer grading system},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting NEPSE index price using deep learning models.
<em>MLA</em>, <em>9</em>, 100385. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock price prediction is a prevalent research field in both industry and academia. There is a pressing demand to develop a prediction model that captures the pattern of the financial activities with high precision to make an informed decision. Stock price prediction is challenging due to the complex, incomplete, fuzzy, nonlinear, and volatile nature of financial data. However, developing a robust model is possible due to advancements in artificial intelligence , availability of large-scale data, and increased access to computational capability. This study performs a comparative analysis of three deep learning models—the Long Short-term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN)—in predicting the next day’s closing price of the Nepal Stock Exchange (NEPSE) index. A set of sixteen predictors is carefully chosen under the domain of the fundamental market data, macroeconomic data, technical indicators, and financial text data of the stock market of Nepal. The performances of employed models are compared using the standard assessment metrics—Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Correlation Coefficient (R). The experimental results show that the LSTM model architecture provides a superior fit with high prediction accuracy. Moreover, statistical evidences are presented to validate the models’ reliability and robustness.},
  archive      = {J_MLA},
  author       = {Nawa Raj Pokhrel and Keshab Raj Dahal and Ramchandra Rimal and Hum Nath Bhandari and Rajendra K.C. Khatri and Binod Rimal and William Edward Hahn},
  doi          = {10.1016/j.mlwa.2022.100385},
  journal      = {Machine Learning with Applications},
  pages        = {100385},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting NEPSE index price using deep learning models},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hourly electricity price forecasting with NARMAX.
<em>MLA</em>, <em>9</em>, 100383. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity price prediction through statistical and machine learning techniques captures market trends and would be a useful tool for energy traders to observe price fluctuations and increase their profits over time. A Nonlinear AutoRegressive Moving Average model with eXogenous inputs (NARMAX) identifies key energy-related factors that influence hourly electricity price through prediction modelling. We propose to use a transparent NARMAX model and analyse Irish Integrated Single Electricity Market (ISEM) data from May 2019 until April 2020 to determine which external factors have a significant impact on the electricity pricing. The experimental results indicate that historical electricity price, demand, and system generation are the most significant factors with historical electricity price being the most weighted factor and the largest Error Reduction Ratio (ERR). A NARMAX model generated using correlated lags was also considered to identify key energy-related lag factors that influence the electricity price. For justification, the significant lag factors are included as inputs in a Seasonal AutoRegressive Integrated Moving Average model with eXogenous input (SARIMAX) to determine if model performance improves with refinement. To conclude, using the NARMAX methodology with energy-related input factors helps to determine the significant factors and results in accurate predictions of electricity price.},
  archive      = {J_MLA},
  author       = {Catherine McHugh and Sonya Coleman and Dermot Kerr},
  doi          = {10.1016/j.mlwa.2022.100383},
  journal      = {Machine Learning with Applications},
  pages        = {100383},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hourly electricity price forecasting with NARMAX},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid u-net: Semantic segmentation of high-resolution
satellite images to detect war destruction. <em>MLA</em>, <em>9</em>,
100381. (<a href="https://doi.org/10.1016/j.mlwa.2022.100381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Destruction caused by violent conflicts play a big role in understanding the dynamics and consequences of conflicts, which is now the focus of a large body of ongoing literature in economics and political science. However, existing data on conflict largely come from news or eyewitness reports, which makes it incomplete, potentially unreliable, and biased for ongoing conflicts. Using satellite images and deep learning techniques , we can automatically extract objective information on violent events. To automate this process, we created a dataset of high-resolution satellite images of Syria and manually annotated the destroyed areas pixel-wise. Then, we used this dataset to train and test semantic segmentation networks to detect building damage of various size. We specifically utilized a U-Net model for this task due to its promising performance on small and imbalanced datasets. However, the raw U-Net architecture does not fully exploit multi-scale feature maps, which are among the important factors for generating fine-grained segmentation maps , especially for high-resolution images. To address this deficiency, we propose a multi-scale feature fusion approach and design a multi-scale skip-connected Hybrid U-Net for segmenting high-resolution satellite images. In our experiments, U-Net and its variants demonstrated promising segmentation results to detect various war-related building destruction. In addition, Hybrid U-Net resulted in significant improvement in segmentation performance compared to U-Net and other baselines. In particular, the mean intersection over union and mean dice score improved by 7.05\% and 8.09\%, respectively, compared to those in the raw U-Net.},
  archive      = {J_MLA},
  author       = {Shima Nabiee and Matthew Harding and Jonathan Hersh and Nader Bagherzadeh},
  doi          = {10.1016/j.mlwa.2022.100381},
  journal      = {Machine Learning with Applications},
  pages        = {100381},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hybrid U-net: Semantic segmentation of high-resolution satellite images to detect war destruction},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting customer purpose of travel in a low-cost travel
environment—a machine learning approach. <em>MLA</em>, <em>9</em>,
100379. (<a href="https://doi.org/10.1016/j.mlwa.2022.100379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the airline, business a passenger’s purpose of travel (business or leisure) has a strong relationship with the price elasticity of that passenger. Full-service network carriers (FSNCs) have long since recognized and monetized this concept by creating different cabins and products for the different types of customers. Conversely, until now, low-cost carriers (LCCs) have done little to differentiate between different types of customers. Recently though, even low-cost carriers have recognized the importance of the business travellers and are attempting to diversify their product offering to cater for different passenger requirements. In this paper, we use machine learning techniques to predict whether a passenger is travelling for business or leisure purposes. Although the problem is formulated as a prediction task, the primary objective is to model the behavioural differences between the two types of customers. In this respect, we discuss the importance and need for effective and interpretable machine learning techniques to facilitate communication with business stakeholders. This is a key requirement to improve the application and results obtained in research in daily practice, particularly in industries that are not primarily information technology based.},
  archive      = {J_MLA},
  author       = {Eyden Samunderu and Michael Farrugia},
  doi          = {10.1016/j.mlwa.2022.100379},
  journal      = {Machine Learning with Applications},
  pages        = {100379},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting customer purpose of travel in a low-cost travel environment—A machine learning approach},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the application of clustering for extracting driving
scenarios from vehicle data. <em>MLA</em>, <em>9</em>, 100377. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If we want to extract test cases from driving data for the purpose of testing vehicles, we want to avoid using similar test cases. In this paper, we focus on this topic. We provide a method for extracting driving episodes from data utilizing clustering algorithms . This method starts with clustering driving data. Afterward, data points representing time-ordered sequences are obtained from the cluster forming a driving episode. Besides outlying the foundations, we present the results of an experimental evaluation where we considered six different clustering algorithms and available driving data from three German cities. To evaluate the cluster quality, we utilize three cluster validity metrics. In addition, we introduce a measure for the quality of extracted episodes relying on the Pearson coefficient. Experimental evaluation showed that the Pearson coefficient can rank clustering algorithms better than the three cluster validity metrics. We can extract meaningful episodes from driving data using any clustering algorithm considering four to eight clusters. Combining k-means clustering with auto-encoders leads to the best Pearson correlation . SOM is the slowest clustering method , and Canopy is the fastest.},
  archive      = {J_MLA},
  author       = {Nour Chetouane and Franz Wotawa},
  doi          = {10.1016/j.mlwa.2022.100377},
  journal      = {Machine Learning with Applications},
  pages        = {100377},
  shortjournal = {Mach. Learn. Appl.},
  title        = {On the application of clustering for extracting driving scenarios from vehicle data},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving the class imbalance problem using a counterfactual
method for data augmentation. <em>MLA</em>, <em>9</em>, 100375. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from class imbalanced datasets poses challenges for many machine learning algorithms. Many real-world domains are, by definition, class imbalanced by virtue of having a majority class that naturally has many more instances than its minority class (e.g., genuine bank transactions occur much more often than fraudulent ones). Many methods have been proposed to solve the class imbalance problem , among the most popular being oversampling techniques (such as SMOTE). These methods generate synthetic instances in the minority class, to balance the dataset, performing data augmentations that improve the performance of predictive machine learning (ML). In this paper, we advance a novel, data augmentation method (adapted from eXplainable AI), that generates synthetic, counterfactual instances in the minority class. Unlike other oversampling techniques, this method adaptively combines existing instances from the dataset, using actual feature-values rather than interpolating values between instances. Several experiments using four different classifiers and 25 datasets involving binary classes are reported, which show that this Counterfactual Augmentation (CFA) method generates useful synthetic datapoints in the minority class. The experiments also show that CFA is competitive with many other oversampling methods, many of which are variants of SMOTE. The basis for CFA’s performance is discussed, along with the conditions under which it is likely to perform better or worse in future tests.},
  archive      = {J_MLA},
  author       = {Mohammed Temraz and Mark T. Keane},
  doi          = {10.1016/j.mlwa.2022.100375},
  journal      = {Machine Learning with Applications},
  pages        = {100375},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Solving the class imbalance problem using a counterfactual method for data augmentation},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent sampling for surrogate modeling, hyperparameter
optimization, and data analysis. <em>MLA</em>, <em>9</em>, 100373. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling techniques are used in many fields, including design of experiments, image processing , and graphics. The techniques in each field are designed to meet the constraints specific to that field such as uniform coverage of the range of each dimension or random samples that are at least a certain distance apart from each other. When an application imposes new constraints, for example, by requiring samples in a non-rectangular domain or the addition of new samples to an existing set, a common solution is to modify the algorithm currently in use, often with less than satisfactory results. As an alternative, we propose the concept of intelligent sampling, where we devise solutions specifically tailored to meet our sampling needs, either by improving existing algorithms or by modifying suitable algorithms from other fields. Surprisingly, both qualitative and quantitative comparisons indicate that some relatively simple algorithms can be easily modified to meet the many sampling requirements of surrogate modeling, hyperparameter optimization, and data analysis; these algorithms outperform their more sophisticated counterparts currently in use, resulting in better use of time and computer resources.},
  archive      = {J_MLA},
  author       = {Chandrika Kamath},
  doi          = {10.1016/j.mlwa.2022.100373},
  journal      = {Machine Learning with Applications},
  pages        = {100373},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Intelligent sampling for surrogate modeling, hyperparameter optimization, and data analysis},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep convolutional neural network-based approach for
detecting burn severity from skin burn images. <em>MLA</em>, <em>9</em>,
100371. (<a href="https://doi.org/10.1016/j.mlwa.2022.100371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burns being one of the leading causes of clinically significant morbidity can lead to a dramatic physiological reaction with prolonged repercussions, metabolic disturbance, severe scarring, catastrophic organ failure, and death if not properly treated. Appropriate burn treatment management is associated with the severity of burn wounds which can be extremely challenging to anticipate at an early stage due to various factors using traditional clinical methods. Therefore, this study proposed a Deep Convolutional Neural Network (DCNN) based approach for detecting the severity of burn injury utilizing real-time images of skin burns. The DCNN architecture leverage the utilization of transfer learning with fine tuning employing three types of pretrained models on top of multiple convolutional layers with hyperparameter tuning for feature extraction from the images and then a fully connected feed forward neural network to classify the images into three categories according to their burn severity : first, second and third degree burns. In order to validate the efficacy of the suggested strategy, the study also applies a traditional solution to mitigate this multi-class categorization problem, incorporating rigorous digital image processing steps with several conventional machine learning classifiers and then conducts a comparative performance assessment. The study’s findings demonstrate that using pretrained models, the recommended DCNN model has gained significantly greater accuracy, with the highest accuracy being obtained using the VGG16 pretrained model for transfer learning with an accuracy of 95.63\% . Thus, through the use of intelligent technologies, the proposed DCNN-based technique can aid healthcare practitioners in evaluating the burn damage condition and providing appropriate treatments in the shortest feasible time, remarkably reducing the unfavorable consequences of burns.},
  archive      = {J_MLA},
  author       = {Sayma Alam Suha and Tahsina Farah Sanam},
  doi          = {10.1016/j.mlwa.2022.100371},
  journal      = {Machine Learning with Applications},
  pages        = {100371},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A deep convolutional neural network-based approach for detecting burn severity from skin burn images},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A metaheuristic with a neural surrogate function for word
sense disambiguation. <em>MLA</em>, <em>9</em>, 100369. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word Sense Disambiguation (WSD) is one of the earliest problems in natural language processing which aims to determine the correct sense of words in context. The semantic information provided by WSD systems is highly beneficial to many tasks such as machine translation, information extraction, and semantic parsing . In this work, a new approach for WSD is proposed which uses a neural network as a surrogate fitness function in a metaheuristic algorithm . Also, a new method for simultaneous training of word and sense embeddings is proposed in this work. Accordingly, the node2vec algorithm is employed on the WordNet graph to generate sequences containing both words and senses. These sequences are then used along with paragraphs from Wikipedia in the word2vec algorithm to generate embeddings for words and senses at the same time. In order to address data imbalance in this task, sense probability distribution data extracted from the training corpus is used in the search process of the proposed simulated annealing algorithm. Furthermore, we introduce a new approach for clustering and mapping senses in the WordNet graph, which considerably improves the accuracy of the proposed method. In this approach, nodes in the WordNet graph are clustered on the condition that no two senses of the same word be present in one cluster. Then, repeatedly, all nodes in each cluster are mapped to a randomly selected node from that cluster, meaning that the representative node can take advantage of the training instances of all the other nodes in the cluster. Training the proposed method in this work is done using the SemCor dataset and the SemEval-2015 dataset has been used as the validation set. The final evaluation of the system is performed on SensEval-2, SensEval-3, SemEval-2007, SemEval-2013, SemEval-2015, and the concatenation of all five mentioned datasets. The performance of the system is also evaluated on the four content word categories, namely, nouns, verbs, adjectives, and adverbs. Experimental results show that the proposed method achieves accuracies in the range of 74.8 to 84.6 percent in the ten aforementioned evaluation categories which are close to and in some cases better than the state of the art in this task.},
  archive      = {J_MLA},
  author       = {Azim Keshavarzian Nodehi and Nasrollah Moghadam Charkari},
  doi          = {10.1016/j.mlwa.2022.100369},
  journal      = {Machine Learning with Applications},
  pages        = {100369},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A metaheuristic with a neural surrogate function for word sense disambiguation},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning approaches for predicting the onset time of
the adverse drug events in oncology. <em>MLA</em>, <em>9</em>, 100367.
(<a href="https://doi.org/10.1016/j.mlwa.2022.100367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the onset time of adverse drug events can substantially lessen the negative impact on the prognosis of cancer patients who are often subject of aggressive and highly toxic treatment regimens . However, the laboratory verification of each patient case to study the mechanics of adverse drug events requires costly, time-intensive research. Thus, to alleviate the efforts required to tackle this problem, using computational models is highly desirable. To provide a suite of such applicable models, we used openly available adverse drug event data resources called FAERS and explored various machine learning paradigms to assess their performance in predicting adverse effect onset days (since the beginning of the treatment). Among various machine learning approaches , we observed that the graph-based embedding model, particularly ComplEx, performed better than other, more traditional machine learning approaches. The embedding learned from the ComplEX trained with k-NN regression for the downstream predictive task obtained the lowest root mean square error, which we consider very promising for further research.},
  archive      = {J_MLA},
  author       = {Mohan Timilsina and Meera Tandan and Vít Nováček},
  doi          = {10.1016/j.mlwa.2022.100367},
  journal      = {Machine Learning with Applications},
  pages        = {100367},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning approaches for predicting the onset time of the adverse drug events in oncology},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-to-event modeling for hospital length of stay
prediction for COVID-19 patients. <em>MLA</em>, <em>9</em>, 100365. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing timely patient care while maintaining optimal resource utilization is one of the central operational challenges hospitals have been facing throughout the pandemic. Hospital length of stay (LOS) is an important indicator of hospital efficiency, quality of patient care, and operational resilience. Numerous researchers have developed regression or classification models to predict LOS. However, conventional models suffer from the lack of capability to make use of typically censored clinical data. We propose to use time-to-event modeling techniques, also known as survival analysis, to predict the LOS for patients based on individualized information collected from multiple sources. The performance of six proposed survival models is evaluated and compared based on clinical data from COVID-19 patients.},
  archive      = {J_MLA},
  author       = {Yuxin Wen and Md Fashiar Rahman and Yan Zhuang and Michael Pokojovy and Honglun Xu and Peter McCaffrey and Alexander Vo and Eric Walser and Scott Moen and Tzu-Liang (Bill) Tseng},
  doi          = {10.1016/j.mlwa.2022.100365},
  journal      = {Machine Learning with Applications},
  pages        = {100365},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Time-to-event modeling for hospital length of stay prediction for COVID-19 patients},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Renewable energy management in smart grids by using big data
analytics and machine learning. <em>MLA</em>, <em>9</em>, 100363. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96\% for the model implemented using 70\% of the data as a training set. Using the random forest tree model has shown 84\% accuracy, and the decision tree model has shown 78\% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87\% for the classification model . The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.},
  archive      = {J_MLA},
  author       = {Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk},
  doi          = {10.1016/j.mlwa.2022.100363},
  journal      = {Machine Learning with Applications},
  pages        = {100363},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Renewable energy management in smart grids by using big data analytics and machine learning},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting severely imbalanced data disk drive failures with
machine learning models. <em>MLA</em>, <em>9</em>, 100361. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets related to hard drive failure, particularly BackBlaze Hard Drive Data, have been widely studied in the literature using many statistical, machine learning , and deep learning techniques . These datasets are severely imbalanced due to the presence of a small number of failed drives compared to huge amounts of healthy drives in the operational data centers . It is challenging to mitigate the adverse consequence of the class imbalance due to the presence of bias towards the majority class during learning. SMART (self monitoring analysis and reporting technology) attributes of the disk drives were utilized in the past to design standard classification or regression algorithms. Although few machine learning (ML) models, for instance, tree based methods and ensemble learning algorithms , addressed the failure prediction, the effects of class imbalance were rarely properly considered under the ML framework. This study, based on a review of the state-of-the-art in the area, evaluates current methodologies to identify areas that were either overlooked or lacking, proposes methods for remediating these issues, and performs some baseline experiments to demonstrate the proposed methodologies including data sampling techniques and cost-sensitive learning.},
  archive      = {J_MLA},
  author       = {Jishan Ahmed and Robert C. Green II},
  doi          = {10.1016/j.mlwa.2022.100361},
  journal      = {Machine Learning with Applications},
  pages        = {100361},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting severely imbalanced data disk drive failures with machine learning models},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine-learning models for spatially-explicit forecasting
of future racial segregation in US cities. <em>MLA</em>, <em>9</em>,
100359. (<a href="https://doi.org/10.1016/j.mlwa.2022.100359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residential racial segregation in large US cities is a complex phenomenon with important social, political, and economic ramifications. In this paper, we demonstrate that the prediction of future segregation can be achieved by using an empirical model generated by a machine learning (ML) algorithm. Specifically, we predict a future map of neighborhood types — racial compositions quantized to several archetypes. Within such a framework, the prediction of segregation is tantamount to the prediction of a thematic map of future neighborhood types. An ML model of change is trained on historical changes and used to make predictions. The key predicate of an ML model is the choice of attributes — variables that drive the change. We hypothesize that neighborhood type’s change of a spatial unit depends only on its present type and statistics of types in surrounding units. The paper asks and positively answers three questions. Is our hypothesis validated by the results? Does the proposed methodology yield useful predictions? Do our results agree with competing predictions? To answer these questions we train and validate a number of change models using, as the case study, 1990, 2000, 2010, and 2020 US Census Bureau block-level data for Cook County, IL (Chicago). We investigated four different algorithms, Random Forest , Gradient Boosted Trees, Neural Network , and Self-Normalizing Net, and have found that Gradient Boosted Trees (GBT) yields the best predictions. Using the GBT-generated model we make a prediction of residential segregation in Cook County in the year 2030.},
  archive      = {J_MLA},
  author       = {Tomasz F. Stepinski and Anna Dmowska},
  doi          = {10.1016/j.mlwa.2022.100359},
  journal      = {Machine Learning with Applications},
  pages        = {100359},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine-learning models for spatially-explicit forecasting of future racial segregation in US cities},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-device behavioral consistency: Benchmarking and
implications for effective android malware detection. <em>MLA</em>,
<em>9</em>, 100357. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the proposed solutions using dynamic features for Android malware detection collect and test their systems using a single and particular data collection device, either a real device or an emulator. The results obtained using these particular devices are then generalized to any Android platform. This extensive generalization is based on the assumption of consistent behavior of apps across devices. This study performs an extensive benchmarking of this assumption for system calls, executing Android malware and benign samples under the same conditions in 9 different collection devices, including real and virtual devices. The results indicate the existence of significant differences between real devices and emulators in system calls usage and, consequently, in the collected behavioral profiles obtained from running the same set of applications on different devices. Furthermore, the impact of these differences on machine learning-based malware detection models is evaluated. In this regard, a significant degenerative effect on the detection performance of the model is produced when data collected on different devices are used in the training and testing sets. Therefore, the empirical findings do not support the assumption of cross-device consistent behavior of Android apps when system calls are used as descriptive features.},
  archive      = {J_MLA},
  author       = {Alejandro Guerra-Manzanares and Martin Välbe},
  doi          = {10.1016/j.mlwa.2022.100357},
  journal      = {Machine Learning with Applications},
  pages        = {100357},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Cross-device behavioral consistency: Benchmarking and implications for effective android malware detection},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting bitcoin price direction with random forests: How
important are interest rates, inflation, and market volatility?
<em>MLA</em>, <em>9</em>, 100355. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin has grown in popularity and has now attracted the attention of individual and institutional investors. Accurate Bitcoin price direction forecasts are important for determining the trend in Bitcoin prices and asset allocation. This paper addresses several unanswered questions. How important are business cycle variables like interest rates, inflation, and market volatility for forecasting Bitcoin prices? Does the importance of these variables change across time? Are the most important macroeconomic variables for forecasting Bitcoin prices the same as those for gold prices? To answer these questions, we utilize tree-based machine learning classifiers, along with traditional logit econometric models. The analysis reveals several important findings. First, random forests predict Bitcoin and gold price directions with a higher degree of accuracy than logit models. Prediction accuracy for bagging and random forests is between 75\% and 80\% for a five-day prediction. For 10-day to 20-day forecasts bagging and random forests record accuracies greater than 85\%. Second, technical indicators are the most important features for predicting Bitcoin and gold price direction, suggesting some degree of market inefficiency. Third, oil price volatility is important for predicting Bitcoin and gold prices indicating that Bitcoin is a substitute for gold in diversifying this type of volatility. By comparison, gold prices are more influenced by inflation than Bitcoin prices, indicating that gold can be used as a hedge or diversification asset against inflation.},
  archive      = {J_MLA},
  author       = {Syed Abul Basher and Perry Sadorsky},
  doi          = {10.1016/j.mlwa.2022.100355},
  journal      = {Machine Learning with Applications},
  pages        = {100355},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Forecasting bitcoin price direction with random forests: How important are interest rates, inflation, and market volatility?},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personality trait prediction by machine learning using
physiological data and driving behavior. <em>MLA</em>, <em>9</em>,
100353. (<a href="https://doi.org/10.1016/j.mlwa.2022.100353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the influence of personality on physiological data while driving in reaction to near crashes and risky situations using Machine Learning (ML). The objective is to improve the driving assistance systems in considering drivers’ characteristics. Physiological and behavioral data were recorded in sixty-three healthy volunteers during risky urban situations and analyzed using 5 ML algorithms to discriminate the driver’s personality according to Big Five Inventory and STAI trait. Seven step process was performed including data pre-processing, Electrodermal Activity (EDA) time windows selection (one by one backward and forward approach comparison with a pseudo-wrapped), personality traits assessment, input algorithms parameters optimization, algorithm comparison and personality trait cluster prediction. ROC Area Under the Curve (AUC) was used to describe improvement. The pseudo-wrapped/all possibilities method comparison resulted in 8.3\% on average for all personality traits and all algorithms (\% of ROC AUC of backward and forward approach). The ROC AUC for the detection of the personality ranged between 0.968 to 0.974 with better detection of Openness, Agreeability and Neuroticism. Use of association between Neuroticism, Extraversion and Conscientiousness previously defined in the literature slightly improve personality detection (maximum ROC AUC of 0.961 to 0.993 for cluster). Results are discussed in terms of contribution to driving aids. This study is one of the first to use machine learning techniques to detect personality traits using behavioral and physiological measures in a driving context. Additionally, it questions input parameters optimization approach, time windows selection, as well as clustering and association of personality trait for detection improvement.},
  archive      = {J_MLA},
  author       = {Morgane Evin and Antonio Hidalgo-Munoz and Adolphe James Béquet and Fabien Moreau and Helène Tattegrain and Catherine Berthelon and Alexandra Fort and Christophe Jallais},
  doi          = {10.1016/j.mlwa.2022.100353},
  journal      = {Machine Learning with Applications},
  pages        = {100353},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Personality trait prediction by machine learning using physiological data and driving behavior},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jasmine: A new active learning approach to combat
cybercrime. <em>MLA</em>, <em>9</em>, 100351. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the reasons that the deployment of network intrusion detection methods falls short is the lack of realistic labeled datasets, which makes it challenging to develop and compare techniques. It is caused by the large amounts of effort that it takes for a cyber expert to classify network connections. This has raised the need for methods that learn from both labeled and unlabeled data which observations are best to present to the human expert. Hence, Active Learning (AL) methods are of interest. In this paper, we propose a new hybrid AL method called Jasmine. Firstly, it uses the uncertainty score and anomaly score to determine how suitable each observation is for querying, i.e., how likely it is to enhance classification. Secondly, Jasmine introduces dynamic updating. This allows the model to adjust the balance between querying uncertain, anomalous and randomly selected observations. To this end, Jasmine is able to learn the best query strategy during the labeling process. This is in contrast to the other AL methods in cybersecurity that all have static, predetermined query functions. We show that dynamic updating, and therefore Jasmine, is able to consistently obtain good and more robust results than querying only uncertainties, only anomalies or a fixed combination of the two.},
  archive      = {J_MLA},
  author       = {Jan Klein and Sandjai Bhulai and Mark Hoogendoorn and Rob van der Mei},
  doi          = {10.1016/j.mlwa.2022.100351},
  journal      = {Machine Learning with Applications},
  pages        = {100351},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Jasmine: A new active learning approach to combat cybercrime},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of the VNS heuristic for feature selection in
credit scoring problems. <em>MLA</em>, <em>9</em>, 100349. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit scoring plays a major role for financial institutions when making credit-granting decisions. In this context, machine learning techniques have been used to develop credit scoring models, as they seek to recognize existing patterns in databases containing the credit history of borrowers to infer potential defaulters. However, these databases often contain a large number of variables, some of which can be noisy, leading to imprecise results and loss of performance/accuracy. In the present work, a feature selection technique is proposed based on a variable neighborhood concept, so-called VNS . The applicability of the method is assessed in conjunction with seven of the main techniques used to make default prediction in credit analysis problems. Its performance was compared to the feature selection obtained by the well-known PCA statistical method. The results indicate superior performance of the VNS in most of the applied tests, suggesting the robustness of the method.},
  archive      = {J_MLA},
  author       = {Victor Gomes Helder and Tiago Pascoal Filomena and Luciano Ferreira and Guilherme Kirch},
  doi          = {10.1016/j.mlwa.2022.100349},
  journal      = {Machine Learning with Applications},
  pages        = {100349},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of the VNS heuristic for feature selection in credit scoring problems},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A semi-supervised learning approach for bladder cancer
grading. <em>MLA</em>, <em>9</em>, 100347. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in semi-supervised learning algorithms (SSL) have made great strides in reducing the training dependency on labeled datasets and requiring that only a subset of the data be labeled. The presented work explores a class of semi-supervised learning algorithms that uses consistency regularization and self-ensembling to leverage the unlabeled portion of the dataset. Labeling medical image datasets are time-consuming and prohibitively expensive, requiring hundreds of hours of effort from expert diagnosticians. This research presents an approach for building and training a deep learning model to grade medical images while requiring only a minimal number of labels. Consistency regularization has been used in SSL to great success in datasets of natural images but not for more complex images such as pathology slides where the dataset consists of cell patterns. This research successfully proposes and applies an SSL algorithm based on the VGG-16 neural network, which combines techniques introduced by the Π model and FixMatch algorithms to a cell pattern-based pathology image dataset. The results presented in this research show that using the proposed approach, it is possible to label only 3\% of the samples in a dataset, use the remaining 97\% of samples as unlabeled data, and achieve a 19\% increase over the baseline accuracy. The second contribution of this research shows a ratio of labeled vs. unlabeled images in a dataset beyond which continuing to label the data increases the cost but offers little performance gains.},
  archive      = {J_MLA},
  author       = {Kenneth Wenger and Kayvan Tirdad and Alex Dela Cruz and Andrea Mari and Mayada Basheer and Cynthia Kuk and Bas W.G. van Rhijn and Alexandre R. Zlotta and Theodorus H. van der Kwast and Alireza Sadeghian},
  doi          = {10.1016/j.mlwa.2022.100347},
  journal      = {Machine Learning with Applications},
  pages        = {100347},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A semi-supervised learning approach for bladder cancer grading},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact shapley values for local and model-true explanations
of decision tree ensembles. <em>MLA</em>, <em>9</em>, 100345. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive feature explanations using Shapley values have become popular for providing transparency into the relative importance of each feature to an individual prediction of a machine learning model. While Shapley values provide a unique additive feature attribution in cooperative game theory , the Shapley values that can be generated for even a single machine learning model are far from unique, with theoretical and implementational decisions affecting the resulting attributions. Here, we consider the application of Shapley values for explaining decision tree ensembles and present a novel approach to Shapley value-based feature attribution that can be applied to random forests and boosted decision trees. This new method provides attributions that accurately reflect details of the model prediction algorithm for individual instances, while being computationally competitive with one of the most widely used current methods. We explain the theoretical differences between the standard and novel approaches and compare their performance using synthetic and real data.},
  archive      = {J_MLA},
  author       = {Thomas W. Campbell and Heinrich Roder and Robert W. Georgantas III and Joanna Roder},
  doi          = {10.1016/j.mlwa.2022.100345},
  journal      = {Machine Learning with Applications},
  pages        = {100345},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Exact shapley values for local and model-true explanations of decision tree ensembles},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bankruptcy prediction using synthetic sampling.
<em>MLA</em>, <em>9</em>, 100343. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of corporate bankruptcy has been widely studied. However, low bankruptcy rates lead to highly imbalanced bankruptcy class distributions, increasing the difficulty of accurately predicting a firm’s bankruptcy. Based on a large sample of 1824 U.S. firms, the study shows that classification accuracy significantly improves when the training dataset is balanced using the synthetic minority oversampling technique or one of its extensions. The results indicate that combining SMOTE with cluster-based undersampling leads to the best classification performance, and the increase in accuracy, specifically in terms of recall and AUC, is significant, thus justifying synthetic sampling when training a bankruptcy prediction classifier.},
  archive      = {J_MLA},
  author       = {John Garcia},
  doi          = {10.1016/j.mlwa.2022.100343},
  journal      = {Machine Learning with Applications},
  pages        = {100343},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Bankruptcy prediction using synthetic sampling},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MLPro 1.0 - standardized reinforcement learning and game
theory in python. <em>MLA</em>, <em>9</em>, 100341. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays there are numerous powerful software packages available for most areas of machine learning (ML). These can be roughly divided into frameworks that solve detailed aspects of ML and those that pursue holistic approaches for one or two learning paradigms. For the implementation of own ML applications, several packages often have to be involved and integrated through individual coding. The latter aspect in particular makes it difficult for newcomers to get started. It also makes a comparison with other works difficult, if not impossible. Especially in the area of reinforcement learning (RL), there is a lack of frameworks that fully implement the current concepts up to multi-agents (MARL) and model-based agents (MBRL). For the related field of game theory (GT), there are hardly any packages available that aim to solve real-world applications. Here we would like to make a contribution and propose the new framework MLPro, which is designed for the holistic realization of hybrid ML applications across all learning paradigms. This is made possible by an additional base layer in which the fundamentals of ML (interaction, adaptation, training, hyperparameter optimization) are defined on an abstract level. In contrast, concrete learning paradigms are implemented in higher sub-frameworks that build on the conventions of this additional base layer. This ensures a high degree of standardization and functional recombinability. Proven concepts and algorithms of existing frameworks can still be used. The first version of MLPro includes sub-frameworks for RL and cooperative GT .},
  archive      = {J_MLA},
  author       = {Detlef Arend and Steve Yuwono and Mochammad Rizky Diprasetya and Andreas Schwung},
  doi          = {10.1016/j.mlwa.2022.100341},
  journal      = {Machine Learning with Applications},
  pages        = {100341},
  shortjournal = {Mach. Learn. Appl.},
  title        = {MLPro 1.0 - standardized reinforcement learning and game theory in python},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autohighlight: Highlight detection in league of legends
esports broadcasts via crowd-sourced data. <em>MLA</em>, <em>9</em>,
100338. (<a href="https://doi.org/10.1016/j.mlwa.2022.100338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every minute, 500 h of footage is uploaded to Youtube.com, and ∼ 1900 ∼1900 h of footage is livestreamed on Twitch.tv. It can therefore be challenging for viewers to find the content they are most likely to enjoy. Highlight videos can entertain users who did not watch a broadcast, e.g. due to a lack of awareness, availability, or willingness. Furthermore, livestream content creators can grow their audiences by using highlights as advertisement, while also engaging casual followers who do not watch full broadcasts. However, hand-generating these videos is laborious, thus automatic highlight detection is an active research challenge. We examine automatic highlight detection by focusing on esports broadcasts. Esports are an emerging genre of sport played using a video games . We focus on League of Legends , a popular title with multiple professional leagues. Esports broadcasts are high-quality and professionally produced, mirroring traditional sports. We tackle the problem in a weakly supervised manner, utilising two datasets, one of ‘crowd-sourced’ highlight videos and one of unedited broadcasts. These datasets allow us to leverage massive data while hugely reducing the human cost of data curation and annotation. We propose two novel extensions to state-of-the-art rank-based highlight detection architectures. Firstly, a multimodal hybrid–fusion architecture that enables audio-visual highlight detection, and secondly, a smoothing step to incorporate context into decision making. Both extensions show significant improvement over state-of-the-art ranking models, in places performing nearly twice as well as competing architectures. Additionally, we examine the effectiveness of each modality and compare ranking models with classification based systems.},
  archive      = {J_MLA},
  author       = {Charles Ringer and Mihalis A. Nicolaou and James Alfred Walker},
  doi          = {10.1016/j.mlwa.2022.100338},
  journal      = {Machine Learning with Applications},
  pages        = {100338},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Autohighlight: Highlight detection in league of legends esports broadcasts via crowd-sourced data},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compressed video ensemble based pseudo-labeling for
semi-supervised action recognition. <em>MLA</em>, <em>9</em>, 100336.
(<a href="https://doi.org/10.1016/j.mlwa.2022.100336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some recent studies have focused on deep learning based semi-supervised learning for action recognition. However, it is difficult to scale up their training because their input is RGB frames, the obtainment of which incurs computational and storage costs. In this paper, we propose a semi-supervised action recognition method that makes it easy to scale up the training by using features stored in compressed videos. Our method directly extracts multiple types of input features from compressed videos without any decoding and generates artificial labels of unlabeled videos through the ensembling of the predictions from these features. In addition to the standard supervised training on labeled videos, our models are trained to predict the artificial labels from strongly augmented features in unlabeled compressed videos. We show that our method is more efficient and achieves a better classification performance on some widely used datasets than conventional semi-supervised learning methods applying RGB frames.},
  archive      = {J_MLA},
  author       = {Hayato Terao and Wataru Noguchi and Hiroyuki Iizuka and Masahito Yamamoto},
  doi          = {10.1016/j.mlwa.2022.100336},
  journal      = {Machine Learning with Applications},
  pages        = {100336},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Compressed video ensemble based pseudo-labeling for semi-supervised action recognition},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pre-trained transformers: An empirical comparison.
<em>MLA</em>, <em>9</em>, 100334. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained transformers have rapidly become very popular in the Natural Language Processing (NLP) community, surpassing the previous state of the art in a wide variety of tasks. While their effectiveness is indisputable, these methods are expensive to fine-tune on the target domain due to the high number of hyper-parameters; this aspect significantly affects the model selection phase and the reliability of the experimental assessment. This paper serves a double purpose: we first describe five popular transformer models and survey their typical use in previous literature, focusing on reproducibility; then, we perform comparisons in a controlled environment over a wide range of NLP tasks. Our analysis reveals that only a minority of recent NLP papers that use pre-trained transformers reported multiple runs (20\%), standard deviation or statistical significance (10\%), and other crucial information, seriously hurting replicability and reproducibility. Through a vast empirical comparison on real-world datasets and benchmarks, we also show how the hyper-parameters and the initial seed impact results, and highlight the low models’ robustness.},
  archive      = {J_MLA},
  author       = {Silvia Casola and Ivano Lauriola and Alberto Lavelli},
  doi          = {10.1016/j.mlwa.2022.100334},
  journal      = {Machine Learning with Applications},
  pages        = {100334},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Pre-trained transformers: An empirical comparison},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional neural networks for vehicle damage detection.
<em>MLA</em>, <em>9</em>, 100332. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle damages are increasingly becoming a liability for shared mobility services. The large number of handovers between drivers demands for an accurate and fast inspection system, which locates small damages and classifies these into the correct damage category. To address this, a damage detection model is developed to locate vehicle damages and classify these into twelve categories. Multiple deep learning algorithms are used, and the effect of different transfer learning and training strategies is evaluated, to optimize the detection performance. The final model, trained on more than 10,000 damage images, is able to accurately detect small damages under various conditions such as water and dirt. A performance evaluation with domain experts shows, that the model achieves comparable performance. In addition, the model is evaluated in a specially designed light street, indicating that strong reflections complicate the detection performance.},
  archive      = {J_MLA},
  author       = {R.E. van Ruitenbeek and S. Bhulai},
  doi          = {10.1016/j.mlwa.2022.100332},
  journal      = {Machine Learning with Applications},
  pages        = {100332},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Convolutional neural networks for vehicle damage detection},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of the performance of feature optimization
techniques for the diagnosis of machine learning-based chronic kidney
disease. <em>MLA</em>, <em>9</em>, 100330. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) slowly decreases one’s kidney ability. A machine learning (ML) based early CKD diagnosis scheme can be an effective solution to reduce this harm. The efficiency of ML techniques depends on the selection and use of the appropriate features. Hence, this research analysis several feature optimization approaches along with a max voting ensemble model to establish a highly accurate CKD diagnosis system by using an appropriate feature set. The ensemble model of this research is structured with five existing classifiers. Three types of feature optimization namely feature importance, feature reduction, and feature selection where for each approach two most proficient techniques are analyzed with the mentioned ensemble model. Based on all analysis the research gets a feature optimization technique called Linear discriminant analysis belonging to the feature selection approach provides the most outstanding result of 99.5\% accuracy by using 10-fold cross-validation. The results of this research indicate the efficiency of feature optimization for the diagnosis of ML-based CKD.},
  archive      = {J_MLA},
  author       = {Muhammad Minoar Hossain and Reshma Ahmed Swarna and Rafid Mostafiz and Pabon Shaha and Lubna Yasmin Pinky and Mohammad Motiur Rahman and Wahidur Rahman and Md. Selim Hossain and Md. Elias Hossain and Md. Sadiq Iqbal},
  doi          = {10.1016/j.mlwa.2022.100330},
  journal      = {Machine Learning with Applications},
  pages        = {100330},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Analysis of the performance of feature optimization techniques for the diagnosis of machine learning-based chronic kidney disease},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Origin of novel coronavirus causing COVID-19: A
computational biology study using artificial intelligence. <em>MLA</em>,
<em>9</em>, 100328. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Origin of the COVID-19 virus (SARS-CoV-2) has been intensely debated in the scientific community since the first infected cases were detected in December 2019. The disease has caused a global pandemic, leading to deaths of thousands of people across the world and thus finding origin of this novel coronavirus is important in responding and controlling the pandemic. Recent research results suggest that bats or pangolins might be the hosts for SARS-CoV-2 based on comparative studies using its genomic sequences. This paper investigates the SARS-CoV-2 origin by using artificial intelligence (AI)-based unsupervised learning algorithms and raw genomic sequences of the virus. More than 300 genome sequences of COVID-19 infected cases collected from different countries are explored and analysed using unsupervised clustering methods . The results obtained from various AI-enabled experiments using clustering algorithms demonstrate that all examined SARS-CoV-2 genomes belong to a cluster that also contains bat and pangolin coronavirus genomes. This provides evidence strongly supporting scientific hypotheses that bats and pangolins are probable hosts for SARS-CoV-2. At the whole genome analysis level, our findings also indicate that bats are more likely the hosts for the COVID-19 virus than pangolins.},
  archive      = {J_MLA},
  author       = {Thanh Thi Nguyen and Mohamed Abdelrazek and Dung Tien Nguyen and Sunil Aryal and Duc Thanh Nguyen and Sandeep Reddy and Quoc Viet Hung Nguyen and Amin Khatami and Thanh Tam Nguyen and Edbert B. Hsu and Samuel Yang},
  doi          = {10.1016/j.mlwa.2022.100328},
  journal      = {Machine Learning with Applications},
  pages        = {100328},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Origin of novel coronavirus causing COVID-19: A computational biology study using artificial intelligence},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpecRp: A spectral-based community embedding algorithm.
<em>MLA</em>, <em>9</em>, 100326. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community embeddings are useful in node classification since they allow nodes to aggregate relevant information regarding the network structure. Modularity maximization-based algorithms are the most common approach to detect communities in networks. Moreover, spectral theory plays an important role in community detection by providing a matrix representation of the network structure. However, the literature is still scarce on spectral and modularity maximization-based community embedding methods. Besides, the node features of attributed graphs are usually not considered for producing the community embeddings. This paper introduces a community embedding algorithm based on spectral theory, called SpecRp , that has an overlapping modularity maximization-based step also herein proposed. SpecRp is a community detection method that considers node attributes and vertex proximity to obtain community embeddings. Computational experiments showed that SpecRp outperformed the literature in most of the tested benchmark datasets for the node classification task. Moreover, we observed that to detect disjoint communities, SpecRp and the reference literature algorithms presented a conflicting behavior concerning performance measures. While the reference methods achieved better results for modularity, SpecRp performed better concerning the Normalized Mutual Information to the ground-truth partitions. On detecting overlapping communities, SpecRp was considerably faster than the state-of-the-art algorithms, despite presenting worse results in most of the datasets.},
  archive      = {J_MLA},
  author       = {Camila P.S. Tautenhain and Mariá C.V. Nascimento},
  doi          = {10.1016/j.mlwa.2022.100326},
  journal      = {Machine Learning with Applications},
  pages        = {100326},
  shortjournal = {Mach. Learn. Appl.},
  title        = {SpecRp: A spectral-based community embedding algorithm},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Summarization of financial reports with TIBER. <em>MLA</em>,
<em>9</em>, 100324. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports an approach for summarizing financial texts that combine several techniques for sentence representation and neural document modeling. Our approach is extractive and it follows the classic pipeline of ranking and consequent selecting of the top-ranked text chunks. We evaluate our method on the financial reports provided in the Financial Narrative Summarization (FNS 2021) shared task. The data for the shared task was created and collected from publicly available UK annual reports published by firms listed on the London Stock Exchange. The reports composed FNS 2021 dataset are very long, have many sections, and are written in “financial” language using various special terms, numerical data, and tables. The results show that our approach outperforms the FNS topline with a very serious advantage. In addition to its performance, our approach is also time-efficient.},
  archive      = {J_MLA},
  author       = {Natalia Vanetik and Marina Litvak and Sophie Krimberg},
  doi          = {10.1016/j.mlwa.2022.100324},
  journal      = {Machine Learning with Applications},
  pages        = {100324},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Summarization of financial reports with TIBER},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep attention-based neural networks for explainable heart
sound classification. <em>MLA</em>, <em>9</em>, 100322. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases are the leading cause of death and severely threaten human health in daily life. There have been dramatically increasing demands from both the clinical practice and the smart home application for monitoring the heart status of individuals suffering from chronic cardiovascular diseases. However, experienced physicians who can perform efficient auscultation are still lacking in terms of number. Automatic heart sound classification leveraging the power of advanced signal processing and deep learning technologies has shown encouraging results. Nevertheless, a lack of explanation for deep neural networks is a limitation for the applications of automatic heart sound classification. To this end, we propose explaining deep neural networks for heart sound classification with an attention mechanism . We evaluate the proposed approach on the heart sounds shenzhen corpus. Our approach achieves an unweighted average recall of 51.2\% for classifying three categories of heart sounds, i. e., normal, mild, and moderate/severe. The experimental results also demonstrate that the global attention pooling layer improves the performance of the learnt representations by estimating the contribution of each unit in high-level features. We further analyse the deep neural networks by visualising the attention tensors.},
  archive      = {J_MLA},
  author       = {Zhao Ren and Kun Qian and Fengquan Dong and Zhenyu Dai and Wolfgang Nejdl and Yoshiharu Yamamoto and Björn W. Schuller},
  doi          = {10.1016/j.mlwa.2022.100322},
  journal      = {Machine Learning with Applications},
  pages        = {100322},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep attention-based neural networks for explainable heart sound classification},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting stock market index using LSTM. <em>MLA</em>,
<em>9</em>, 100320. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement in artificial intelligence and machine learning techniques , availability of large-scale data, and increased computational capabilities of the machine opens the door to develop sophisticated methods in predicting stock price. In the meantime, easy access to investment opportunities has made the stock market more complex and volatile than ever. The world is looking for an accurate and reliable predictive model which can capture the market’s highly volatile and nonlinear behavior in a holistic framework. This study uses a long short-term memory (LSTM), a particular neural network architecture , to predict the next-day closing price of the S&amp;P 500 index. A well-balanced combination of nine predictors is carefully constructed under the umbrella of the fundamental market data, macroeconomic data, and technical indicators to capture the behavior of the stock market in a broader sense. Single layer and multilayer LSTM models are developed using the chosen input variables, and their performances are compared using standard assessment metrics–Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Correlation Coefficient (R). The experimental results show that the single layer LSTM model provides a superior fit and high prediction accuracy compared to multilayer LSTM models.},
  archive      = {J_MLA},
  author       = {Hum Nath Bhandari and Binod Rimal and Nawa Raj Pokhrel and Ramchandra Rimal and Keshab R. Dahal and Rajendra K.C. Khatri},
  doi          = {10.1016/j.mlwa.2022.100320},
  journal      = {Machine Learning with Applications},
  pages        = {100320},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting stock market index using LSTM},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distinctive features of nonverbal behavior and mimicry in
application interviews through data analysis and machine learning.
<em>MLA</em>, <em>9</em>, 100318. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reveals the characteristics and effects of nonverbal behavior and human mimicry in the context of application interviews. It discloses a novel analyzation method for psychological research by utilizing machine learning . In comparison to traditional manual data analysis, machine learning proves to be able to analyze the data more deeply and to discover connections in the data invisible to the human eye. The paper describes an experiment to measure and analyze the reactions of evaluators to job applicants who adopt specific behaviors: mimicry, suppress, immediacy and natural behavior. First, evaluation of the applicant qualifications by the interviewer reveals how behavioral self-management can improve the interviewer’s opinion of the candidate. Secondly, the underlying mechanics of mimicry behavior are exposed through analysis of seven nonverbal actions. Manual data analysis determines the frequency features of the actions and answers how often the actions are performed and how often they are mimicked during application interviews. Two of the seven actions are here deemed negligible due too low frequency features. Finally, machine learning is employed to analyze the data in great detail and distinguish the four behavior categories from each other. A Random Forest classifier is able to achieve 55.2\% accuracy for predicting the behavior condition of the interviews while human observers reach an accuracy of 32.9\%. The feature set for the classifier is reduced to 130 features with the most important features relating to the correlations between the leaning forward actions of the interview participants.},
  archive      = {J_MLA},
  author       = {Sanne Roegiers and Elias Corneillie and Filip Lievens and Frederik Anseel and Peter Veelaert and Wilfried Philips},
  doi          = {10.1016/j.mlwa.2022.100318},
  journal      = {Machine Learning with Applications},
  pages        = {100318},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Distinctive features of nonverbal behavior and mimicry in application interviews through data analysis and machine learning},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining commit messages to enhance software refactorings
recommendation: A machine learning approach. <em>MLA</em>, <em>9</em>,
100316. (<a href="https://doi.org/10.1016/j.mlwa.2022.100316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software refactoring is the common practice that is applied to improve the internal structure of software systems without altering their external behaviors. Software developers sometimes apply refactoring to prepare software systems for further extensions of requirements or adaptation to new requirements often presented as feature requests. However, in such context, identifying where and what type of refactoring to use is very challenging and mostly relies on developer’s intuition and experience. To facilitate refactorings selection during feature requests implementation, existing studies have relied on the past software change history to predict and recommend future refactorings. However, none of these approaches have attempted to exploit the potential of commit messages to drive refactoring recommendation. To this end, this paper proposes a machine-learning approach trained with the past history of previously applied refactorings detected using both traditional refactoring detectors and analysis of commit messages. The approach implements binary classifier to predict the need for refactoring, and a multi-label classifier to recommend required refactorings. The evaluation of the proposed approach based on the dataset comprised of commit messages of 65 open source projects suggest that, the approach significantly outperforms the state-of-the-art approach.},
  archive      = {J_MLA},
  author       = {Ally S. Nyamawe},
  doi          = {10.1016/j.mlwa.2022.100316},
  journal      = {Machine Learning with Applications},
  pages        = {100316},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Mining commit messages to enhance software refactorings recommendation: A machine learning approach},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior prediction based on a commodity utility-behavior
sequence model. <em>MLA</em>, <em>9</em>, 100314. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of social e-commerce and live broadcast e-commerce, real-time recommendations based on consumers’ browsing behavior are becoming more and more important. Traditional utility recommendation has some problems such as cold start and subjectivity. And traditional sequential recommendation relies on behavior sequences. However, these data contain some irrelevant data of products, which will lead to wrong dependencies and affect the accuracy of recommendation. To solve such problems, this research builds a Commodity Utility-Behavior Sequence (CUBS) dual-utility model. CUBS consists of two models: Commodity Utility (CU) and Behavior Sequence (BS). The Commodity Utility can evaluate the psychological motivation of consumer and transform it into commodity utility. The Behavior Sequence can predict preference by calculating the behavior sequences. CUBS combines the advantages of the Commodity Utility and Behavior Sequence. At the same time, it makes up for the shortcomings of only using a single model. A total of 22,417 records of 214 customers are randomly selected from the JD.com database as the test set. These customers are divided into 4 groups and calculated by CU, BS and CUBS respectively. The results show that the accuracy of CUBS model is the highest. This also confirms that customers’ click behaviors and behavior sequences have an important influence on purchase intention prediction.},
  archive      = {J_MLA},
  author       = {Li Chen and Hui Zhu},
  doi          = {10.1016/j.mlwa.2022.100314},
  journal      = {Machine Learning with Applications},
  pages        = {100314},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Behavior prediction based on a commodity utility-behavior sequence model},
  volume       = {9},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face detection and grimace scale prediction of white furred
mice. <em>MLA</em>, <em>8</em>, 100312. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the facial expressions of humans has been one of the major applications of computer vision. An open question is whether common machine learning techniques can also be used to track behaviors of animals, which is a less explored research problem. Since animals are not capable of verbal communication, computer vision solutions can provide valuable information to track the animal’s state. We are particularly interested in pain neurobiology research, where rodent models are extensively used to investigate pain interventions. A grimace scale is used to understand the suffering of a mouse in the presence of interventions, which is inferred from various facial features such as the shape of the eyes and ears. In this work, we automate the prediction of the grimace scale on white furred mice using a machine learning approach , following the same principles used for human facial expression recognition: face detection, landmark region extraction, and expression recognition. We demonstrate the use of the you only look once (YOLO) framework for face detection of the mice with outstanding results. For eye region extraction and grimace pain prediction, we propose a novel structure based on a dilated convolutional network . The experimental results are promising, showing that it is possible to differentiate among the pain scale of the mice.},
  archive      = {J_MLA},
  author       = {Andrea Vidal and Sumit Jha and Shayne Hassler and Theodore Price and Carlos Busso},
  doi          = {10.1016/j.mlwa.2022.100312},
  journal      = {Machine Learning with Applications},
  pages        = {100312},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Face detection and grimace scale prediction of white furred mice},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging the momentum effect in machine learning-based
cryptocurrency trading. <em>MLA</em>, <em>8</em>, 100310. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency trading has become more and more popular among private investors. According to recent studies, the momentum effect influences the underlying market. Quantitative trading systems can leverage momentum indicators to open and close trading positions. However, existing approaches that exploit the momentum effect in cryptocurrency trading do not rely on machine learning . Since these systems are based on human generated rules they are not suited to highly volatile market conditions, which are quite common in cryptocurrency markets. This paper proposes to leverage machine learning approaches to automatically detect the momentum effect in cryptocurrency market data. For each cryptocurrency it estimates the likelihood of being affected by the momentum effect on the next trading day as well as the momentum direction. A backtesting session, performed on three very popular cryptocurrencies, shows that the machine learning models are able to predict, to a good approximation, short-term price volatility thus reducing the number of false trading signals and increasing the return on investments compared to state-of-the-art approaches.},
  archive      = {J_MLA},
  author       = {Gian Pietro Bellocca and Giuseppe Attanasio and Luca Cagliero and Jacopo Fior},
  doi          = {10.1016/j.mlwa.2022.100310},
  journal      = {Machine Learning with Applications},
  pages        = {100310},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Leveraging the momentum effect in machine learning-based cryptocurrency trading},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural network surrogate models for absorptivity and
emissivity spectra of multiple elements. <em>MLA</em>, <em>8</em>,
100308. (<a href="https://doi.org/10.1016/j.mlwa.2022.100308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emissivity and absorptivity of radiation by matter are fundamental physical parameters required to adequately simulate inertial confinement fusion experiments. We present novel neural network models for predicting these interaction coefficients. This research extends the methods used by Kluth et al. (2020) that made similar predictions for a single chemical element, krypton. Our models are based on fully-connected or convolutional autoencoders coupled with a deep jointly-informed neural network (DJINN) to predict the emissivity and absorptivity of a given element and temperature/radiative field. We show that the previous work could not be directly extended to lower atomic number elements due to a thresholding effect on small values caused by a log 10 log10 transformation. Thus, in order to create a multi-element model, or even a single-element model, with low atomic number elements a different transformation is necessary. Utilization of a cube-root transform enables the creation of a multi-element model can achieve mean relative errors between 1\% and 2\%. Our work demonstrates that a single neural network can predict the results of atomic physics calculations, but additional work is necessary to consider mixtures of elements or a wider range of elements than those used in our study.},
  archive      = {J_MLA},
  author       = {Michael D. Vander Wal and Ryan G. McClarren and Kelli D. Humbird},
  doi          = {10.1016/j.mlwa.2022.100308},
  journal      = {Machine Learning with Applications},
  pages        = {100308},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Neural network surrogate models for absorptivity and emissivity spectra of multiple elements},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-aware and interpretable predictive monitoring system
for anti-money laundering. <em>MLA</em>, <em>8</em>, 100306. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Money laundering is a global threat to society nowadays. Governments and governmental authorities fight money laundering, in part, by regulating banks and financial institutions. Financial institutions, in turn, are obligated to implement mechanisms to prevent money laundering. Usually, these prevention mechanisms include automated monitoring systems. In this paper, we propose an Anti-Money Laundering monitoring system based on machine learning techniques, which addresses three requirements: (i) generating accurate and non-redundant alerts; (ii) generating timely alerts; and (iii) associating explanations and risk estimates to each alert. The first requirement is addressed by training a machine learning classification model on different snapshots of customers’ history and then feeding the classification scores generated by this model into an alerting policy designed to prevent redundant alerts. The second requirement is addressed via custom metrics for assessing the performance of classification models, which take into account the timeliness requirements that arise in the Anti-Money Laundering domain. Finally, the third requirement is addressed by applying a method for class probability calibration and by an interpretability layer based on Shapley values. The proposed monitoring system has been designed based on requirements provided by an investigation unit at a financial institution and evaluated using real-life data as well as multiple rounds of feedback from specialized subject-matter experts.},
  archive      = {J_MLA},
  author       = {Pavlo Tertychnyi and Mariia Godgildieva and Marlon Dumas and Madis Ollikainen},
  doi          = {10.1016/j.mlwa.2022.100306},
  journal      = {Machine Learning with Applications},
  pages        = {100306},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Time-aware and interpretable predictive monitoring system for anti-money laundering},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gender bias recognition in political news articles.
<em>MLA</em>, <em>8</em>, 100304. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gender bias exists not only in the stereotypes used to portray male and female figures, but more importantly, in the word choice media outlets use to describe political figures. Research in computational linguistics has not yet focused on the problem of identifying gender bias in political news from a variety of leanings, despite decades of research from the political science perspective. In this work, we introduce a new dataset – News-Bias – and demonstrate through extensive experimentation that there exists significant gender bias in political news, even with all gendered terms, and personally identifiable information removed. We show that bias persists through the document embeddings, sentiment and word choice across news outlets and political leanings.},
  archive      = {J_MLA},
  author       = {Sara R. Davis and Cody J. Worsnop and Emily M. Hand},
  doi          = {10.1016/j.mlwa.2022.100304},
  journal      = {Machine Learning with Applications},
  pages        = {100304},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Gender bias recognition in political news articles},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting performance of wavelet neural networks and other
neural network topologies: A comparative study based on financial market
data sets. <em>MLA</em>, <em>8</em>, 100302. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we analyse the advantageous effects of neural networks in combination with wavelet functions on the performance of financial market predictions. We implement different approaches in multiple experiments and test their predictive abilities with different financial time series. We demonstrate experimentally that both wavelet neural networks and neural networks with data pre-processed by wavelets outperform classical network topologies . However, the precision of conducted forecasts implementing neural network algorithms still propose potential for further refinement and enhancement. Hence, we discuss our findings, comparisons with “buy-and-hold” strategies and ethical considerations critically and elaborate on future prospects.},
  archive      = {J_MLA},
  author       = {Markus Vogl and Peter Gordon Rötzel LL.M and Stefan Homes},
  doi          = {10.1016/j.mlwa.2022.100302},
  journal      = {Machine Learning with Applications},
  pages        = {100302},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Forecasting performance of wavelet neural networks and other neural network topologies: A comparative study based on financial market data sets},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of chaotic time series using recurrent neural
networks and reservoir computing techniques: A comparative study.
<em>MLA</em>, <em>8</em>, 100300. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine-learning techniques, particularly deep learning , have outperformed traditional time-series forecasting approaches in many contexts, including univariate and multivariate predictions. This study aims to investigate the capability of (i) gated recurrent neural networks , including long short-term memory (LSTM) and gated recurrent unit (GRU) networks, (ii) reservoir computing (RC) techniques, such as echo state networks (ESNs) and hybrid physics-informed ESNs, and (iii) the nonlinear vector autoregression (NVAR) approach, which has recently been introduced as the next generation RC, for the prediction of chaotic time series and to compare their performance in terms of accuracy, efficiency, and robustness. We apply the methods to predict time series obtained from two widely used chaotic benchmarks, the Mackey–Glass and Lorenz-63 models, as well as two other chaotic datasets representing a bursting neuron and the dynamics of the El Niño Southern Oscillation, and to one experimental dataset representing a time series of cardiac voltage with complex dynamics. We find that even though gated RNN techniques have been successful in forecasting time series generally, they can fall short in predicting chaotic time series for the methods, datasets, and ranges of hyperparameter values considered here. In contrast, for the chaotic datasets studied, we found that reservoir computing and NVAR techniques are more computationally efficient and offer more promise in long-term prediction of chaotic time series.},
  archive      = {J_MLA},
  author       = {Shahrokh Shahi and Flavio H. Fenton and Elizabeth M. Cherry},
  doi          = {10.1016/j.mlwa.2022.100300},
  journal      = {Machine Learning with Applications},
  pages        = {100300},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of chaotic time series using recurrent neural networks and reservoir computing techniques: A comparative study},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning based medical image deepfake detection: A
comparative study. <em>MLA</em>, <em>8</em>, 100298. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative networks in recent years have reinforced the need for caution while consuming various modalities of digital information. One avenue of deepfake creation is aligned with injection and removal of tumors from medical scans. Failure to detect medical deepfakes can lead to large setbacks on hospital resources or even loss of life. This paper attempts to address the detection of such attacks with a structured case study. Specifically, we evaluate eight different machine learning algorithms, which include three conventional machine learning methods (Support Vector Machine, Random Forest , Decision Tree) and five deep learning models (DenseNet121, DenseNet201, ResNet50, ResNet101, VGG19) in distinguishing between tampered and untampered images. For deep learning models, the five models are used for feature extraction, then each pre-trained model is fine-tuned. The findings of this work show near perfect accuracy in detecting instances of tumor injections and removals.},
  archive      = {J_MLA},
  author       = {Siddharth Solaiyappan and Yuxin Wen},
  doi          = {10.1016/j.mlwa.2022.100298},
  journal      = {Machine Learning with Applications},
  pages        = {100298},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning based medical image deepfake detection: A comparative study},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual and structural feature combination in an interactive
machine learning system for medical image segmentation. <em>MLA</em>,
<em>8</em>, 100294. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, Convolutional Neural Networks achieve good performance in automatic image segmentation situations; however, they have not demonstrated sufficiently accurate and robust results in the case of more general and interactive systems. Also, they have been designed specifically for visual features and cannot integrate enough anatomical knowledge inside the learned models they produce. To address these problems, we propose a novel machine-learning-based framework for interactive medical image segmentation. The proposed method incorporates local anatomical knowledge learning capabilities into a bounding box-based segmentation pipeline. Region specific voxel classifiers can be learned and combined to make the model adaptive to different anatomical structures or image modalities. In addition, a spatial relationship learning mechanism is integrated to capture and use additional topological (anatomical) information. New learning procedures have been defined to integrate both types of information (visual features to characterize each substructure and spatial relationships for a relative positioning between the substructures) in a unified model. During incremental and interactive segmentation, local substructures are localized one by one, enabling partial image segmentation. Bounding box positioning within the entire image is performed automatically using previously learned spatial relationships or by the user when necessary. Inside each bounding box, atlas-based methods or CNNs that are dedicated to each substructure can be applied to automatically obtain each local segmentation. Experimental results show that (1) the proposed model is robust for segmenting objects with a small amount of training images; (2) the accuracy is similar to other methods but allows partial segmentation without requiring a global registration; and (3) the proposed method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods due to its spatial relationship learning capabilities.},
  archive      = {J_MLA},
  author       = {Gaëtan Galisot and Jean-Yves Ramel and Thierry Brouard and Elodie Chaillou and Barthélémy Serres},
  doi          = {10.1016/j.mlwa.2022.100294},
  journal      = {Machine Learning with Applications},
  pages        = {100294},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Visual and structural feature combination in an interactive machine learning system for medical image segmentation},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning-based identification of craniosynostosis in
newborns. <em>MLA</em>, <em>8</em>, 100292. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early closure of cranial vault sutures, defined as craniosynostosis is a relatively common condition with somehow specific head and face abnormality for each subtype. Early diagnosis results in a better prognosis but pediatricians and primary care providers are not so familiar with these abnormalities while taking 3D CT scan of skull, predisposes the growing brain to harmful effects of radiation. Thus, developing a user-friendly and accurate diagnostic system would be helpful. This study aimed to diagnose simple suture synostosis by using machine learning based methods in digital photographs of child head. Digital photos of 145 craniosynostosis infants, operated in Mofid children hospital (Tehran, Iran) are used in this study. Head border is identified by GrabCut algorithm segmentation method and then several anthropometric indices such as cranial index (CI), cranial vault asymmetry index (CVAI), anterior–midline width ratio (AMWR) and anterior–posterior​ width ratio (APWR) and left–right height ratio (LRHR) are calculated. Moreover, statistical pattern matching indices (Chi-square (CS), Hu moment invariants (HuMI), absolute difference of white pixels probability (AbsDifWPP) and pixel intensity (PI)) are calculated and compared to anthropometric indices. The classification results for statistical pattern matching indices varied in the range of 85\%–92\% which is statistically higher than hand crafted indices. Our proposed approach could diagnose and classify common subtypes of single suture craniosynostosis and could help pediatricians and parents in early diagnosis and follow-up of this disorder.},
  archive      = {J_MLA},
  author       = {Malihe Sabeti and Reza Boostani and Ehsan Moradi and Mohammad Hossein Shakoor},
  doi          = {10.1016/j.mlwa.2022.100292},
  journal      = {Machine Learning with Applications},
  pages        = {100292},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning-based identification of craniosynostosis in newborns},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of brain networks for autism spectrum disorder
classification using resting-state functional connectivity.
<em>MLA</em>, <em>8</em>, 100290. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive and practical review of autism spectrum disorder (ASD) classification using several traditional machine learning and deep learning methods on data from the Autism Brain Imaging Data Exchange (ABIDE) repository. The objective of this study was to investigate different brain networks and determine their functional connectivity to distinguish between subjects with ASD and those considered typically developing (TD). In the experiments of this paper, functional connectivity was used as a classification feature for 871 resting-state functional magnetic resonance imaging (rs-fMRI) samples collected from the ABIDE repository. The methodology and results of this paper have three main parts. First, we reviewed eight different brain parcellation techniques used for ASD classification from structural, functional, and data-driven perspectives to identify the most promising brain atlas. Second, we evaluate the stability and efficiency of the correlation, partial correlation, and tangent space functional connectivity metrics, and identify the most stable functional connectivity metric. Third, we compared four different supervised learning models used in the ASD classification domain and evaluated the learning performance of each model. In summary, our experimental results show that Bootstrap Analysis of Stable Clusters (BASC) provides the most predictive power for ASD classification, while the correlation metric is the most stable candidate among those models considered. Furthermore, by comparing different classifiers, we conclude that among all the experimentally compared classifiers in this paper, the kernel support vector machine (kSVM) is the optimal classifier for classifying ABIDE fMRI data. The highest sensitivity 64.57\% is identified in Table 7. This result was produced using the correlation metric with functional atlas BASC444 and RBF kernel SVM. The corresponding specificity is 73.61\%, and the accuracy is 69.43\%. Overall, this is the optimal result.},
  archive      = {J_MLA},
  author       = {Xin Yang and Ning Zhang and Paul Schrader},
  doi          = {10.1016/j.mlwa.2022.100290},
  journal      = {Machine Learning with Applications},
  pages        = {100290},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A study of brain networks for autism spectrum disorder classification using resting-state functional connectivity},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based tensile strength approximation of random
nonwoven materials by interpretable regression. <em>MLA</em>,
<em>8</em>, 100288. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonwoven materials consist of random fiber structures. They are essential to diverse application areas such as clothing, insulation and filtering. A long term goal in industry is the simulation-based optimization of material properties in dependence of the manufacturing parameters. Recent works developed a framework to predict tensile strength properties representing the fiber structure as a stochastic graph. In this paper we present an efficient machine learning approach using a regression model trained on features extracted from the graph, for which we develop a novel graph stretching algorithm. We demonstrate that applying our method to a practically relevant dataset yields similar prediction results as the original ODE approach ( R 2 = 0 . 98 R2=0.98 ), while achieving a significant speedup by up to three orders of magnitude. This opens the field to optimization, as Monte Carlo simulations accounting for the stochastic nature of nonwovens become easily accessible. Our model generalizes well to unseen parameter combinations. Additionally, our approach produces interpretable results by using a simple linear model for the regression task .},
  archive      = {J_MLA},
  author       = {Dario Antweiler and Marc Harmening and Nicole Marheineke and Andre Schmeißer and Raimund Wegener and Pascal Welke},
  doi          = {10.1016/j.mlwa.2022.100288},
  journal      = {Machine Learning with Applications},
  pages        = {100288},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Graph-based tensile strength approximation of random nonwoven materials by interpretable regression},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Text mining methodologies with r: An application to central
bank texts. <em>MLA</em>, <em>8</em>, 100286. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review several existing text analysis methodologies and explain their formal application processes using the open-source software R and relevant packages. Several text mining applications to analyze central bank texts are presented.},
  archive      = {J_MLA},
  author       = {Jonathan Benchimol and Sophia Kazinnik and Yossi Saadon},
  doi          = {10.1016/j.mlwa.2022.100286},
  journal      = {Machine Learning with Applications},
  pages        = {100286},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Text mining methodologies with r: An application to central bank texts},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene-adaptive radar tracking with deep reinforcement
learning. <em>MLA</em>, <em>8</em>, 100284. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target tracking with radars is a highly challenging problem due to detection artifacts, sensor noise, and interference sources. The traditional signal processing chain is, therefore, a complex combination of various algorithms with several tunable tracking-parameters. Usually, these are initially set by engineers and are independent of the scene tracked. For this reason, they are often non-optimal and generate poorly performing tracking. In this context, scene-adaptive radar processing refers to algorithms that can sense, understand and learn information related to detected targets as well as the environment and adapt its tracking-parameters to optimize the desired goal. In this paper, we propose a Deep Reinforcement Learning framework that guides the scene-adaptive choice of radar tracking-parameters towards an improved performance on multi-target tracking.},
  archive      = {J_MLA},
  author       = {Michael Stephan and Lorenzo Servadei and José Arjona-Medina and Avik Santra and Robert Wille and Georg Fischer},
  doi          = {10.1016/j.mlwa.2022.100284},
  journal      = {Machine Learning with Applications},
  pages        = {100284},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Scene-adaptive radar tracking with deep reinforcement learning},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Masking technique based attention mechanism for off-type
identification in plants. <em>MLA</em>, <em>8</em>, 100282. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-type plants, or other distinct varieties of the same plant, need to be detected and removed at early stage from the fields to preserve the genetic integrity and significant traits of a given plant variety. At present, these off-type plants are manually identified in the cultivation fields through morphological analysis of leaves. This method is time-consuming and involves skilled labor to define the off-type plant. Hence, masking technique based attention mechanism is proposed for identifying the off-type from the leaf images captured in the cultivation fields. The proposed method recognizes the plant variety by locating and visualizing the prominent parts of the leaf which is similar to the human visual attention mechanism. When the prominent parts of the leaf image are masked, there is a significant drop in the class probability of the convolutional network . This reflects the importance of such prominent parts which directly account for its variety. An attention module is designed using neural networks which focuses on such significant regions and improves the performance of the network. The proposed method is tested on a field image dataset consisting of 1235 images of Sunflower leaves and achieves the highest mean accuracy of 98.25\% in identifying the plant variety.},
  archive      = {J_MLA},
  author       = {G. Jaya Brindha and E.S. Gopi},
  doi          = {10.1016/j.mlwa.2022.100282},
  journal      = {Machine Learning with Applications},
  pages        = {100282},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Masking technique based attention mechanism for off-type identification in plants},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal recovery of unsecured debt via interpretable
reinforcement learning. <em>MLA</em>, <em>8</em>, 100280. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the issue of interpretability and auditability of reinforcement-learning agents employed in the recovery of unsecured consumer debt. To this end, we develop a deterministic policy-gradient method that allows for a natural integration of domain expertise into the learning procedure so as to encourage learning of consistent, and thus interpretable, policies. Domain knowledge can often be expressed in terms of policy monotonicity and/or convexity with respect to relevant state inputs. We augment the standard actor–critic policy approximator using a monotonically regularized loss function which integrates domain expertise into the learning. Our formulation overcomes the challenge of learning interpretable policies by constraining the search to policies satisfying structural-consistency properties. The resulting state-feedback control laws can be readily understood and implemented by human decision makers. This new domain-knowledge enhanced learning approach is applied to the problem of optimal debt recovery which features a controlled Hawkes process and an asynchronous action–feedback relationship.},
  archive      = {J_MLA},
  author       = {Michael Mark and Naveed Chehrazi and Huanxi Liu and Thomas A. Weber},
  doi          = {10.1016/j.mlwa.2022.100280},
  journal      = {Machine Learning with Applications},
  pages        = {100280},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Optimal recovery of unsecured debt via interpretable reinforcement learning},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot image classification using coupled dictionary
embedding. <em>MLA</em>, <em>8</em>, 100278. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is a framework to classify images that belong to unseen visual classes using their semantic descriptions about the unseen classes. We develop a new ZSL algorithm based on coupled dictionary learning. The core idea is to enforce the visual features and the semantic attributes of an image to share the same sparse representation in an intermediate embedding space, modeled as the shared input space of two sparsifying dictionaries. In the ZSL training stage, we use images from a number of seen classes for which we have access to both the visual and the semantic attributes to train two coupled dictionaries that can represent both the visual and the semantic feature vectors of an image using a single sparse vector. In the ZSL testing stage and in the absence of labeled data, images from unseen classes are mapped into the attribute space by finding the joint-sparse representations using solely the visual dictionary via solving a LASSO problem. The image is then classified in the attribute space given semantic descriptions of unseen classes. We also provide attribute-aware and transductive formulations to tackle the “domain-shift” and the “hubness” challenges for ZSL, respectively. Experiments on four primary datasets using VGG19 and GoogleNet visual features, are provided. Our performances using VGG19 features are 91.0\%, 48.4\%, and 89.3\% on the SUN, the CUB, and the AwA1 datasets, respectively. Our performances on the SUN, the CUB, and the AwA2 datasets are 57.0\%,49.7\%, and 71.7\%, respectively, when GoogleNet features are used. Comparison with existing methods demonstrates that our method is effective and compares favorably against the state-of-the-art. In particular, our algorithm leads to decent performance on the all four datasets. 2},
  archive      = {J_MLA},
  author       = {Mohammad Rostami and Soheil Kolouri and Zak Murez and Yuri Owechko and Eric Eaton and Kuyngnam Kim},
  doi          = {10.1016/j.mlwa.2022.100278},
  journal      = {Machine Learning with Applications},
  pages        = {100278},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Zero-shot image classification using coupled dictionary embedding},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HARadNet: Anchor-free target detection for radar point
clouds using hierarchical attention and multi-task learning.
<em>MLA</em>, <em>8</em>, 100275. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target localization and classification from radar point clouds is a challenging task due to the inherently sparse nature of the data with highly non-uniform target distribution. This work presents HARadNet, a novel attention based anchor free target detection and classification network architecture in a multi-task learning framework for radar point clouds data. A direction field vector is used as motion modality to achieve attention inside the network. The attention operates at different hierarchy of the feature abstraction layer with each point sampled according to a conditional direction field vector, allowing the network to exploit and learn a joint feature representation and correlation to its neighborhood. This leads to a significant improvement in the performance of the classification. Additionally, a parameter-free target localization is proposed using Bayesian sampling conditioned on a pre-trained direction field vector. The extensive evaluation on a public radar dataset shows an substantial increase in localization and classification performance.},
  archive      = {J_MLA},
  author       = {Anand Dubey and Avik Santra and Jonas Fuchs and Maximilian Lübke and Robert Weigel and Fabian Lurz},
  doi          = {10.1016/j.mlwa.2022.100275},
  journal      = {Machine Learning with Applications},
  pages        = {100275},
  shortjournal = {Mach. Learn. Appl.},
  title        = {HARadNet: Anchor-free target detection for radar point clouds using hierarchical attention and multi-task learning},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling strategies for learning-based 3D medical image
compression. <em>MLA</em>, <em>8</em>, 100273. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent achievements of sequence prediction models in numerous domains, including compression, provide great potential for novel learning-based codecs. In such models, the input sequence’s shape and size play a crucial role in learning the mapping function of the data distribution to the target output. This work examines numerous input configurations and sampling schemes for a many-to-one sequence prediction model, specifically for compressing 3D medical images (16-bit depth) losslessly. The main objective is to determine the optimal practice for enabling the proposed Long Short-Term Memory (LSTM) model to achieve high compression ratio and fast encoding–decoding performance. Our LSTM models are trained with 4-fold cross-validation on 12 high-resolution CT dataset while measuring model’s compression ratios and execution time. Several configurations of sequences have been evaluated, and our results demonstrate that pyramid-shaped sampling represents the best trade-off between performance and compression ratio (up to 3× ). We solve a problem of non-deterministic environments that allow our models to run in parallel without much compression performance drop. Experimental evaluation was carried out on datasets acquired by different hospitals, representing different body segments, and distinct scanning modalities (CT and MRI). Our new methodology allows straightforward parallelisation that speeds-up the decoder by up to 37× compared to previous methods. Overall, the trained models demonstrate efficiency and generalisability for compressing 3D medical images losslessly while still outperforming well-known lossless methods by approximately 17\% and 12\%. To the best of our knowledge, this is the first study that focuses on voxel-wise predictions of volumetric medical imaging for lossless compression .},
  archive      = {J_MLA},
  author       = {Omniah H. Nagoor and Joss Whittle and Jingjing Deng and Benjamin Mora and Mark W. Jones},
  doi          = {10.1016/j.mlwa.2022.100273},
  journal      = {Machine Learning with Applications},
  pages        = {100273},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Sampling strategies for learning-based 3D medical image compression},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer (machine) learning approaches coupled with target
data augmentation to predict the mechanical properties of concrete.
<em>MLA</em>, <em>8</em>, 100271. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning , a machine learning technique which employs prior knowledge from solving a source problem to solve a related target problem, is utilized in this work to predict the compressive strength and modulus of elasticity of different concrete mixtures. The use of data augmentation through empirical models to estimate missing data outputs allows for the use of inductive parameter transfer-learning artificial neural network (ANN) models for fast convergence. The paper considers two distinct cases: one where the domain of the target lies somewhat outside that of the source — termed domain expansion, and another where the target output (e.g., elastic modulus) is different, but related to the source output (e.g., compressive strength) — termed domain adaptation . Transfer learning is found to be most accurate when the source dataset is more complex than the target dataset, since more features could be learned. Data augmentation and the coupling of traditional machine learning with transfer learning are demonstrated to greatly enhance the predictive capability for important concrete properties, from mixture proportions. Limited experimental data can be used to transfer-learn the properties (output) of a new dataset from a reliable source model for a related system.},
  archive      = {J_MLA},
  author       = {Emily Ford and Kailasnath Maneparambil and Aditya Kumar and Gaurav Sant and Narayanan Neithalath},
  doi          = {10.1016/j.mlwa.2022.100271},
  journal      = {Machine Learning with Applications},
  pages        = {100271},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Transfer (machine) learning approaches coupled with target data augmentation to predict the mechanical properties of concrete},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fraud prediction using machine learning: The case of
investment advisors in canada. <em>MLA</em>, <em>8</em>, 100269. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper contributes to a growing body of empirical work on regulatory technology by proposing machine learning models to detect fraud in financial markets. The recent spate of investment fraud in Canada has exposed regulators’ inability to protect vulnerable investors and the financial markets from financial abuse. As evident by the numerous regulatory task force commissioned in the past two years, Canadian regulators have been looking for ways to detect and prevent fraudulent activities before they occur and support enhanced enforcement powers. The purpose of this study is to use data collected from the Investment Industry Regulatory Organization of Canada (IIROC) to build a machine-learning algorithm to predict fraud in the Canadian securities industry. Data for this project were collected from IIROC’s tribunal cases covering June 2008 to December 2019. In total, 406 cases were retrieved from the IIROC’s website. The results from four machine learning models reveal that across all the features, the amount of money invested and whether the offender was from a bank-owned investment firm were the high predictors of fraud in terms of the standardized coefficient. Branch managers and regulators should pay careful attention to portfolios that continuously incur losses as a sign of potential fraud. The findings are particularly relevant to regulators seeking new and effective fraud detection techniques while providing enhanced clarity to Canada’s financial markets’ self-regulation.},
  archive      = {J_MLA},
  author       = {Mark Eshwar Lokanan and Kush Sharma},
  doi          = {10.1016/j.mlwa.2022.100269},
  journal      = {Machine Learning with Applications},
  pages        = {100269},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Fraud prediction using machine learning: The case of investment advisors in canada},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An extension of synthetic minority oversampling technique
based on kalman filter for imbalanced datasets. <em>MLA</em>,
<em>8</em>, 100267. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More often than not, data collected in real-time tends to be imbalanced i.e., the samples belonging to a particular class are significantly more than the others. This degrades the performance of the predictor. One of the most notable algorithms to handle such an imbalance in the dataset by fabricating synthetic data, is the “Synthetic Minority Oversampling Technique (SMOTE)”. However, data imbalance is not solely responsible for the poor performance of the classifier. Certain research works have demonstrated that noisy samples can have a significant role in misclassifying the dataset. Also, handling large data is computationally expensive. Hence, data reduction is imperative. In this work, we put forth a novel extension of SMOTE by integrating it with the Kalman filter . The proposed method, Kalman-SMOTE (KSMOTE), filters out the noisy samples in the final dataset after SMOTE, which includes both the raw data and the synthetically generated samples, thereby reducing the size of the dataset. Our model is validated with a wide range of datasets. An experimental analysis of the results shows that our model outperforms the presently available techniques.},
  archive      = {J_MLA},
  author       = {Thejas G.S. and Yashas Hariprasad and S.S. Iyengar and N.R. Sunitha and Prajwal Badrinath and Shasank Chennupati},
  doi          = {10.1016/j.mlwa.2022.100267},
  journal      = {Machine Learning with Applications},
  pages        = {100267},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An extension of synthetic minority oversampling technique based on kalman filter for imbalanced datasets},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning in materials chemistry: An invitation.
<em>MLA</em>, <em>8</em>, 100265. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Materials chemistry is being profoundly influenced by the uptake of machine learning methodologies. Machine learning techniques , in combination with established techniques from computational physics, promise to accelerate the discovery of new materials by elucidating complex structure–property relationships from massive material databases. Despite exciting possibilities, further methodological developments call for a greater synergism between materials chemists, physicists, and engineers on one side, with computer science and math majors on the other. In this review, we provide a non-exhaustive account of machine learning in materials chemistry for computer scientists and applied mathematicians, with an emphasis on molecule datasets and materials chemistry problems. The first part of this review provides a tutorial on how to prepare such datasets for subsequent model building, with an emphasis on the construction of feature vectors. We also provide a self-contained introduction to density functional theory , a method from computational physics which is widely used to generate datasets and compute response variables. The second part reviews two machine learning methodologies which represent the status quo in materials chemistry at present – kernelized machine learning and Bayesian machine learning – and discusses their application to real datasets. In the third part of the review, we introduce some emerging machine learning techniques which have not been widely adopted by materials scientists and therefore present potential avenues for computer science and applied math majors. In the final concluding section, we discuss some recent machine learning-based approaches to real materials discovery problems and speculate on some promising future directions.},
  archive      = {J_MLA},
  author       = {Daniel Packwood and Linh Thi Hoai Nguyen and Pierluigi Cesana and Guoxi Zhang and Aleksandar Staykov and Yasuhide Fukumoto and Dinh Hoa Nguyen},
  doi          = {10.1016/j.mlwa.2022.100265},
  journal      = {Machine Learning with Applications},
  pages        = {100265},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning in materials chemistry: An invitation},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated IoT attack detection using decentralized edge
data. <em>MLA</em>, <em>8</em>, 100263. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) devices are mass-produced and designed for different applications, ranging from monitoring of the environment to on-demand electrical switches, and so on. These IoT devices are often heterogeneous in nature, only to receive updates at infrequent intervals, and can remain ‘out of sight’ on a home or office network for extended periods. In other words, security and privacy are two key (research and operational) challenges in IoT systems. In recent years, there have been attempts to design deep learning-based solutions to mitigate limitations associated with detection systems designed for typical operational technology (OT) systems, although a number of challenges remain. This paper proposes a federated-based approach that employs a deep autoencoder to detect​ botnet attacks using on-device decentralized traffic data. Through the suggested federated solution, privacy is addressed by ensuring the device’s data is not transferred or moved off the network edge. Instead, the machine learning computation itself is brought to where the data is born (i.e. the edge layer), with the added benefit of data security. We demonstrate that using our proposed model, we can achieve up to 98\% accuracy rate in the anomaly detection when using features such as source IP, MAC-IP, and destination IP, etc., for training. The overall comparative performance analysis between our decentralized proposed approach and a centralized format demonstrates a significant improvement in the accuracy rate of attack detection.},
  archive      = {J_MLA},
  author       = {Christopher Regan and Mohammad Nasajpour and Reza M. Parizi and Seyedamin Pouriyeh and Ali Dehghantanha and Kim-Kwang Raymond Choo},
  doi          = {10.1016/j.mlwa.2022.100263},
  journal      = {Machine Learning with Applications},
  pages        = {100263},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Federated IoT attack detection using decentralized edge data},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning application in wine quality prediction.
<em>MLA</em>, <em>8</em>, 100261. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wine business relies heavily on wine quality certification. The excellence of New Zealand Pinot noir wines is well-known worldwide. Our major goal in this research is to predict wine quality by generating synthetic data and construct a machine learning model based on this synthetic data and available experimental data collected from different and diverse regions across New Zealand. We utilised 18 Pinot noir wine samples with 54 different characteristics (7 physiochemical and 47 chemical features). We generated 1381 samples from 12 original samples using the SMOTE method, and six samples were preserved for model testing. The findings were compared using four distinct feature selection approaches. Important attributes (referred as essential variables) that were shown to be relevant in at least three feature selection methods were utilised to predict wine quality. Seven machine learning algorithms were trained and tested on a holdout original sample. Adaptive Boosting (AdaBoost) classifier showed 100\% accuracy when trained and evaluated without feature selection, with feature selection (XGB), and with essential variables (features found important in at least three feature selection methods). In the presence of essential variables, the Random Forest (RF) classifier performance was increased.},
  archive      = {J_MLA},
  author       = {Piyush Bhardwaj and Parul Tiwari and Kenneth Olejar Jr and Wendy Parr and Don Kulasiri},
  doi          = {10.1016/j.mlwa.2022.100261},
  journal      = {Machine Learning with Applications},
  pages        = {100261},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning application in wine quality prediction},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deriving mapping functions to tie anthropometric
measurements to body mass index via interpretable machine learning.
<em>MLA</em>, <em>8</em>, 100259. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at leveraging recent advancements in interpretable machine learning to better understand how anthropometric measurements can be tied to body mass index (BMI). Two objectives are of interest to this work, the first is to develop a properly validated interpretable machine learning (ML) ensemble capable of accurately predicting BMI, and the second is to derive a mapping function (i.e., a ML-based expression) that can describe the relationship between BMI and anthropometric measurements. This paper analyzes a historical database published by Penrose et al. (1985) containing thirteen body circumference measurements for 252 men. Four ML algorithms are then blended into an ensemble to examine the collected anthropometric measurements, namely, Extreme Gradient Boosted Trees, Light Gradient Boosted Trees, Random Forest , and Keras Slim Residual Network . The ensemble was then augmented with the SHAP (SHapley Additive exPlanations) and partial dependence plot techniques to understand the effect of each anthropometric measurement on BMI and the interaction between these anthropometric measurements. The proposed ensemble not only can comprehend the relation between anthropometric measurements and BMI index and derive a new non-parametric expression to predict BMI, but can also be used to interpret such relation and help us understand the logic driving ML’s predictions. Further, the interpretability analysis reveals that the main anthropometric measurements influencing BMI are the chest, abdomen, and hip circumference. Finally, clinicians and researchers are encouraged to leverage interpretability ML tools in their works instead of those of “black-box” nature.},
  archive      = {J_MLA},
  author       = {M.Z. Naser},
  doi          = {10.1016/j.mlwa.2022.100259},
  journal      = {Machine Learning with Applications},
  pages        = {100259},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deriving mapping functions to tie anthropometric measurements to body mass index via interpretable machine learning},
  volume       = {8},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A self-adaptive deep learning model for building electricity
load prediction with moving horizon. <em>MLA</em>, <em>7</em>, 100257.
(<a href="https://doi.org/10.1016/j.mlwa.2022.100257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A self-adaptive deep learning model powered by ranking selection-based particle swarm optimisation (RSPSO) is developed to predict electricity load in buildings with moving horizons. The main features of the load prediction model include its self-adaptability, repeatability, robustness and accuracy. In real-world building applications, the relationship among weather data, time signature and electricity load is quite complicated. In the proposed self-adaptive deep learning model, a deep learning model with multiple hidden layers is implemented to improve prediction precision. Meanwhile, RSPSO is implemented to select the network’s optimum architecture, which involves discrete variables (i.e. the quantity of neurons in each layer and the quantity of hidden layers) and categorical variables (i.e. activation function in each layer and learning approach). Moreover, the moving horizon approach is adopted to update the architecture and structure of the dynamic deep learning model while enabling its capability in capturing the latest featuring patterns in the electricity load of the building. The proposed load prediction model is tested with the local meteorological profile and electricity load of an educational building. The self-adaptive load prediction model is identified to be the most effective at forecasting the next horizon’s energy consumption, while its prediction performance would deteriorate with the increase of time. The mean squared error, mean absolute error , and coefficient of determination of the proposed prediction model are within the range of 4.48 kW–11.23 kW, 1.28 kW–2.31 kW and 97.52\%–98.92\%, respectively, demonstrating its prediction accuracy and repeatability. When Gaussian white noise is added to meteorological data , the increase in mean absolute error is within the range of 2.08\%–15.33\%, demonstrating the robustness of the proposed prediction model in overcoming uncertainty in the weather forecast. Therefore, the proposed accurate, robust, repeatable and self-adaptive load prediction model can be rooted in practical energy management systems thus facilitate building operation and system control.},
  archive      = {J_MLA},
  author       = {Xiaojun Luo and Lukumon O. Oyedele},
  doi          = {10.1016/j.mlwa.2022.100257},
  journal      = {Machine Learning with Applications},
  pages        = {100257},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A self-adaptive deep learning model for building electricity load prediction with moving horizon},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-term financial predictions based on feynman–dirac path
integrals, deep bayesian networks and temporal generative adversarial
networks. <em>MLA</em>, <em>7</em>, 100255. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new deep learning framework, QuantumPath, for long-term stock price prediction, which is of great significance in portfolio management and risk mitigation , especially when the market becomes volatile due to unpredictable circumstances such as a pandemic. Our approach is based on stochastic equations, the Feynman–Dirac path integral, deep Bayesian networks , and temporal generative adversarial neural networks (t-GAN). The expected financial trajectory is evaluated with a Feynman–Dirac path integral. The latter involves summing all possible financial trajectories that could have been taken by the financial instrument. These trajectories are generated with a t-GAN. A probability is attributed to each point of each path. The probability is a function of the Lagrangian, which is derived from a stochastic equation describing the temporal evolution of the stock. The drift and the volatility at each point, which are required in order to evaluate the Lagrangian, are predicted with a deep Bayesian neural network. Given that the evolution of a stock’s price is isomorphic to a time series, our temporal GAN consists of long short-term memory (LSTM) neural networks, which introduce a memory mechanism, and temporal convolutional neural networks (TCN), which ensure causality. Stock prices are predicted over periods of twenty and thirty days for nine stocks, eight of which are included in the S&amp;P 500 index. Our experimental results clearly demonstrate the efficiency of our approach.},
  archive      = {J_MLA},
  author       = {Farzan Soleymani and Eric Paquet},
  doi          = {10.1016/j.mlwa.2022.100255},
  journal      = {Machine Learning with Applications},
  pages        = {100255},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Long-term financial predictions based on Feynman–Dirac path integrals, deep bayesian networks and temporal generative adversarial networks},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparative analysis of contextual and context-free
embeddings in disaster prediction from twitter data. <em>MLA</em>,
<em>7</em>, 100253. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter is a social media site where people post their personal experiences, opinions, and news. Due to the ubiquitous real-time data availability, many rescue agencies monitor this data regularly to identify disasters, reduce risk, and save lives. However, it is impossible for humans to manually check the mass amount of data and identify disasters in real-time. For this purpose, many research have been proposed to present words in machine-understandable representations and apply machine learning methods on the word representations to identify the sentiment of a text. The previous research methods provide a single vector representation or embedding of a word from a given document. However, the recent advanced contextual embedding method (BERT — Bidirectional Encoder Representations from Transformers) constructs different vectors for the same word in different contexts. The BERT embeddings have been used successfully in various Natural Language Processing (NLP) tasks, yet there is no concrete analysis of how these representations are helpful in disaster-type tweet analysis. This research study explores the efficacy of the BERT embeddings on predicting disaster from Twitter data and compares these to traditional context-free word embedding methods. We provide both quantitative and qualitative results for this study. The results show that the contextual embeddings have the best results in disaster prediction task than the traditional word embeddings. Furthermore, we discuss the opportunities and challenges of contextual embeddings on sentiment analysis of Twitter data.},
  archive      = {J_MLA},
  author       = {Sumona Deb and Ashis Kumar Chanda},
  doi          = {10.1016/j.mlwa.2022.100253},
  journal      = {Machine Learning with Applications},
  pages        = {100253},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Comparative analysis of contextual and context-free embeddings in disaster prediction from twitter data},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing ensemble weights and hyperparameters of machine
learning models for regression problems. <em>MLA</em>, <em>7</em>,
100251. (<a href="https://doi.org/10.1016/j.mlwa.2022.100251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregating multiple learners through an ensemble of models aim to make better predictions by capturing the underlying distribution of the data more accurately. Different ensembling methods, such as bagging, boosting, and stacking/blending, have been studied and adopted extensively in research and practice. While bagging and boosting focus more on reducing variance and bias, respectively, stacking approaches target both by finding the optimal way to combine base learners. In stacking with the weighted average, ensembles are created from weighted averages of multiple base learners. It is known that tuning hyperparameters of each base learner inside the ensemble weight optimization process can produce better performing ensembles. To this end, an optimization-based nested algorithm that considers tuning hyperparameters as well as finding the optimal weights to combine ensembles (Generalized Weighted Ensemble with Internally Tuned Hyperparameters (GEM-ITH)) is designed. Besides, Bayesian search was used to speed-up the optimizing process and a heuristic was implemented to generate diverse and well-performing base learners. The algorithm is shown to be generalizable to real data sets through analyses with ten publicly available data sets.},
  archive      = {J_MLA},
  author       = {Mohsen Shahhosseini and Guiping Hu and Hieu Pham},
  doi          = {10.1016/j.mlwa.2022.100251},
  journal      = {Machine Learning with Applications},
  pages        = {100251},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Optimizing ensemble weights and hyperparameters of machine learning models for regression problems},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Initialization of profile and social network analyses robot
and platform with a concise systematic review. <em>MLA</em>, <em>7</em>,
100249. (<a href="https://doi.org/10.1016/j.mlwa.2022.100249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents profile and social network analyses on concise systematic review corpora. It suggests two new robots and platforms for profile and social network analyses, that will serve previously proposed data, expert, and event-driven robots and platforms for energy and power industry. The literature is collected and stored in three topic clusters “ location ”, “ investment ”, and “ DEMATEL ” to prepare corpora. Twenty-five publications are selected in each sample corpus. A sample dataset of each corpus is prepared for thirty-one features such as “author’s full name and surname”, “applied methods”, and “publisher”. Afterward, “authors network matrices” are prepared in spreadsheet software. Data input files (*.csv) are prepared for each dataset. Gephi 0.9.2 201709241107 (free open-source software) is used for social network analyses with built-in layout and statistics algorithms on a desktop Windows 10 Pro, Intel(R) Core(TM) i5 CPU 650 @ 3.20 GHz, 6,00 GB RAM personal computer in an offline and active cybersecurity software environment. Force Atlas, Force Atlas 2, Fruchterman–Reingold, OpenOrd, Yifan Hu, and Yifan Hu Proportional layout algorithms with Noverlap layout algorithm are run one by one. Runtimes range 2–120 s. All default statistic algorithms are run for several metrics like average degree, average weighted degree, betweenness centrality , closeness centrality , harmonic closeness centrality, eccentricity, and density. Authors in “ location ” cluster have a centralized network, but authors in “ investment ” and “ DEMATEL ” clusters have distributed networks. General profile analyses are conducted based on authors’ publications in the literature without any data and information on social media sites and platforms. Two new profile analysis metrics are proposed as “researcher’s past research focus index”, and “researcher’s future research focus prediction index”. Detailed profile analysis is performed for only Burak Omer Saracoglu. All analyses and findings are compared and summarized in the end.},
  archive      = {J_MLA},
  author       = {Burak Omer Saracoglu},
  doi          = {10.1016/j.mlwa.2022.100249},
  journal      = {Machine Learning with Applications},
  pages        = {100249},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Initialization of profile and social network analyses robot and platform with a concise systematic review},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fatigue damage detection of aerospace-grade aluminum alloys
using feature-based and feature-less deep neural networks. <em>MLA</em>,
<em>7</em>, 100247. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fatigue damage is one of the most common causes of failure in aerospace structural components. While numerical modeling and laboratory-scale experimentation provide much insight to the physics of failure evolution, it is extremely challenging to account for all variabilies that a component may be subjected to during on-field operation. Human-supervised continuous monitoring of such components using sensors, therefore, provides a much-needed alternative for reliable operation of these components. By leveraging the concepts in deep learning , such human supervision can be assisted with an automated pre-trained deep network for damage detection. To that end, this article studies two distinct deep neural network (DNN) architectures for fatigue damage detection in aluminum using ultrasonic time-series data obtained from a novel customized fatigue testing apparatus. The first DNN, called as a feature-based network, is built by using two predefined features viz. frequency domain amplitude and autocorrelation from the ultrasonic data as the inputs. The second DNN, called as a feature-less network, uses the ultrasonic data as-is without any pre-processing and relies on the black-box features generated during training. The capability of fatigue crack detection for both DNN architectures is evaluated at two distinct stages of fatigue failure. The feature-less network is observed to outperform the feature-based network with an accuracy of 94.26\% and 98.94\% for the two stages. The result indicates that feature-less DNNs, owing to their construction, can formulate better, albeit black-box features, and simplify the process of choosing customized signal processing methods for similar problems.},
  archive      = {J_MLA},
  author       = {Susheel Dharmadhikari and Amrita Basak},
  doi          = {10.1016/j.mlwa.2021.100247},
  journal      = {Machine Learning with Applications},
  pages        = {100247},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Fatigue damage detection of aerospace-grade aluminum alloys using feature-based and feature-less deep neural networks},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistep networks for roll force prediction in hot strip
rolling mill. <em>MLA</em>, <em>7</em>, 100245. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hot rolling processes consist of multiple single rolling stand operating at high temperature and speed to achieve desired steel shapes and superior properties, via exerting roll forces that need to be accurately predicted by a model. The currently used model of the mill of this study shows prediction instability and is unable to accurately accommodate changes in steel grade. In this paper, we propose a machine learning based framework to establish a model that accurately predicts roll forces at each mill stands of the hot strip rolling mill. In contrast to the traditional models, the proposed expert system considers an individual model for each rolling stand and employs rolling history when predicting roll forces. The proposed model includes both steel chemistry and physical process parameters for its predictions. Our experimental results demonstrate that the proposed framework improves both prediction accuracy and stability by 40\%–50\% over the currently used mill model. The enhanced prediction accuracy will greatly improve dimensional and microstructural control, as well as ensuring the avoidance of mill overloads.},
  archive      = {J_MLA},
  author       = {Shuhong Shen and Denzel Guye and Xiaoping Ma and Stephen Yue and Narges Armanfard},
  doi          = {10.1016/j.mlwa.2021.100245},
  journal      = {Machine Learning with Applications},
  pages        = {100245},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multistep networks for roll force prediction in hot strip rolling mill},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AddGBoost: A gradient boosting-style algorithm based on
strong learners. <em>MLA</em>, <em>7</em>, 100243. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present AddGBoost , a gradient boosting-style algorithm, wherein the decision tree is replaced by a succession of (possibly) stronger learners, which are optimized via a state-of-the-art hyperparameter optimizer. Through experiments over 90 regression datasets we show that AddGBoost emerges as the top performer for 33\% (with 2 stages) up to 42\% (with 5 stages) of the datasets, when compared with seven well-known machine-learning algorithms: KernelRidge , LassoLars, SGDRegressor, LinearSVR , DecisionTreeRegressor, HistGradientBoostingRegressor, and LGBMRegressor.},
  archive      = {J_MLA},
  author       = {Moshe Sipper and Jason H. Moore},
  doi          = {10.1016/j.mlwa.2021.100243},
  journal      = {Machine Learning with Applications},
  pages        = {100243},
  shortjournal = {Mach. Learn. Appl.},
  title        = {AddGBoost: A gradient boosting-style algorithm based on strong learners},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overcoming the ordinal imbalanced data problem by combining
data processing and stacked generalizations. <em>MLA</em>, <em>7</em>,
100241. (<a href="https://doi.org/10.1016/j.mlwa.2021.100241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal imbalanced datasets are pervasive in real world applications but remain challenging to analyse as they require specific methods to account for the ordering information and imbalanced classes. Failure to account for both those characteristics can substantially impact the model predictive performance . However, existing methods tend to focus either on ordinality or imbalance, rather than addressing both simultaneously. The few approaches that do account for both characteristics are not always easy to implement for non-advanced analysts and simpler approaches are needed to facilitate appropriate data processing. Here, we developed a general approach using some of the most popular machine learning algorithms to ensure appropriate processing of ordinal imbalanced datasets and to optimize the predictions of all classes. After transforming the multi-class ordinal problem into a well-known binary problem, we implemented several different resampling methods in a decision-tree classifier. We then used a stacked generalization algorithm to combine the classifiers to improve model predictive performance. To test our approach, we used two ordinal imbalanced datasets on student performance and wine quality. Individual resampling techniques tended to improve the accuracy of minority classes, while simultaneously increasing the number of false positives in those classes. This resulted in a decrease, sometimes substantial, in accuracy of other classes. The stacking model offered a good compromise between improvement in accuracy of minority classes and mitigation of reduced accuracy in other classes. Our approach provided useful insights into modelling strategies that should be favoured for implementation in production that involve these common datasets, depending on the end-user interests.},
  archive      = {J_MLA},
  author       = {Marine Desprez and Kyle Zawada and Daniel Ramp},
  doi          = {10.1016/j.mlwa.2021.100241},
  journal      = {Machine Learning with Applications},
  pages        = {100241},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Overcoming the ordinal imbalanced data problem by combining data processing and stacked generalizations},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning outperforms classical forecasting on
horticultural sales predictions. <em>MLA</em>, <em>7</em>, 100239. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting future demand is of high importance for many companies as it affects operational decisions. This is especially relevant for products with a short shelf life due to the potential disposal of unsold items. Horticultural products are highly influenced by this, however with limited attention in forecasting research so far. Beyond that, many forecasting competitions show a competitive performance of classical forecasting methods. For the first time, we empirically compared the performance of nine state-of-the-art machine learning and three classical forecasting algorithms for horticultural sales predictions. We show that machine learning methods were superior in all our experiments, with the gradient boosted ensemble learner XGBoost being the top performer in 14 out of 15 comparisons. This advantage over classical forecasting approaches increased for datasets with multiple seasons. Further, we show that including additional external factors, such as weather and holiday information, as well as meta-features led to a boost in predictive performance . In addition, we investigated whether the algorithms can capture the sudden increase in demand of horticultural products during the SARS-CoV-2 pandemic in 2020. For this special case, XGBoost was also superior. All code and data is publicly available on GitHub: https://github.com/grimmlab/HorticulturalSalesPredictions .},
  archive      = {J_MLA},
  author       = {Florian Haselbeck and Jennifer Killinger and Klaus Menrad and Thomas Hannus and Dominik G. Grimm},
  doi          = {10.1016/j.mlwa.2021.100239},
  journal      = {Machine Learning with Applications},
  pages        = {100239},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning outperforms classical forecasting on horticultural sales predictions},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting help desk ticket reassignments with graph
convolutional networks. <em>MLA</em>, <em>7</em>, 100237. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient triaging of incident tickets is a critical task in Information Technology Service Management . Introducing interventional measures on tickets that are difficult to resolve can help improve the triaging of complex tickets. This work reports a method to predict the resolution complexity of a reported incident. The number of times a ticket is reassigned is a measure of difficulty in resolving the incident. Ticket resolution is associated with a variable workflow. A graph representation of ticket resolution offers advantages from the standpoint of running ad hoc queries. Predicting ticket reassignments requires the application of machine learning to this graph. A Relational Graph Convolutional Network is used for this purpose. The developed model provides benefits beyond predicting ticket reassignments accurately. It provides embeddings that can be used to derive insights about the operation of the help desk organization and the users of the help desk.},
  archive      = {J_MLA},
  author       = {Jörg Schad and Rajiv Sambasivan and Christopher Woodward},
  doi          = {10.1016/j.mlwa.2021.100237},
  journal      = {Machine Learning with Applications},
  pages        = {100237},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting help desk ticket reassignments with graph convolutional networks},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A causal direction test for heterogeneous populations.
<em>MLA</em>, <em>7</em>, 100235. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A probabilistic expert system emulates the decision-making ability of a human expert through a directional graphical model . The first step in building such systems is to understand data generation mechanism. To this end, one may try to decompose a multivariate distribution into product of several conditionals, and evolving a blackbox machine learning predictive models towards transparent cause-and-effect discovery. Most causal models assume a single homogeneous population, an assumption that may fail to hold in many applications. We show that when the homogeneity assumption is violated, causal models developed based on such assumption can fail to identify the correct causal direction. We propose an adjustment to a commonly used causal direction test statistic by using a k k -means type clustering algorithm where both the labels and the number of components are estimated from the collected data to adjust the test statistic. Our simulation result show that the proposed adjustment significantly improves the performance of the causal direction test statistic for heterogeneous data . We study large sample behaviour of our proposed test statistic and demonstrate the application of the proposed method using real data.},
  archive      = {J_MLA},
  author       = {Vahid Partovi Nia and Xinlin Li and Masoud Asgharian and Shoubo Hu and Yanhui Geng and Zhitang Chen},
  doi          = {10.1016/j.mlwa.2021.100235},
  journal      = {Machine Learning with Applications},
  pages        = {100235},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A causal direction test for heterogeneous populations},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An applied deep learning approach for estimating soybean
relative maturity from UAV imagery to aid plant breeding decisions.
<em>MLA</em>, <em>7</em>, 100233. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a global breeding organization, identifying the next generation of superior crops is vital for its success. Recognizing new genetic varieties requires years of in-field testing to gather data about the crop’s yield, pest resistance, heat resistance, etc. At the conclusion of the growing season, organizations need to determine which varieties will be advanced to the next growing season (or sold to farmers) and which ones will be discarded from the candidate pool. Specifically for soybeans, identifying their relative maturity is a vital piece of information used for advancement decisions. However, this trait needs to be physically observed, and there are resource limitations (time, money, etc.) that bottleneck the data collection process. To combat this, breeding organizations are moving towards advanced image capturing devices. In this paper, we develop a robust and automatic approach for estimating the relative maturity of soybeans using a time series of UAV images. An end-to-end hybrid model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) is proposed to extract features and capture the sequential behavior of time series data . The proposed deep learning model was tested on six different environments across the United States Results suggest the effectiveness of our proposed CNN-LSTM model compared to the local regression method . Furthermore, we demonstrate how this newfound information can be used to aid in plant breeding advancement decisions.},
  archive      = {J_MLA},
  author       = {Saba Moeinizade and Hieu Pham and Ye Han and Austin Dobbels and Guiping Hu},
  doi          = {10.1016/j.mlwa.2021.100233},
  journal      = {Machine Learning with Applications},
  pages        = {100233},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An applied deep learning approach for estimating soybean relative maturity from UAV imagery to aid plant breeding decisions},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting density of serious crime incidents using a
multiple-input hidden markov maximization a posteriori model.
<em>MLA</em>, <em>7</em>, 100231. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the displacement of crime from a given state such as cold to another state such as warm or hot, facilitates the efficient allocation of resources and the mitigation of crime threats. In this study, a crime forecasting model was developed, based on Spearman’s Correlations and a clustering technique (DBSCAN), which captures significant groupings in a geospatial dataset . A Multi-Input Hidden Markov Model (MI-HMM) machine learning framework was developed to train the dataset. The results from the MI-HMM were then used to make a Maximum a Posteriori (MAP) decision over the possible state of crime for the next month. This novel model, MI-HMM-MAP, was used to predict the density of crime including criminal hot spots over time. The model was evaluated using real-world dataset. Findings show an average of 72.5\% accuracy and 81.7\% correctness. The model was compared to 5 classical predictive models. Results show that our model significantly outperforms a linear regression model, a neural network model , and two machine learning approaches . It slightly outperforms a deep learning approach as demonstrated statistically by an application to the crime of murder in Trinidad and Tobago.},
  archive      = {J_MLA},
  author       = {Devon L. Robertson and Wayne S. Goodridge},
  doi          = {10.1016/j.mlwa.2021.100231},
  journal      = {Machine Learning with Applications},
  pages        = {100231},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting density of serious crime incidents using a multiple-input hidden markov maximization a posteriori model},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cryptocurrency ecosystems and social media environments: An
empirical analysis through hawkes’ models and natural language
processing. <em>MLA</em>, <em>7</em>, 100229. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse, using a mixture of statistical models and natural language process techniques, what happened in social media from June 2019 onwards to understand the relationships between Cryptocurrencies’ prices and social media, focusing on the rise of the Bitcoin and Ethereum prices. In particular, we identify and model the relationship between the cryptocurrencies market price changes , and sentiment and topic discussion occurrences on social media, using Hawkes’ Model. We find that some topics occurrences and rise of sentiment in social media precedes certain types of price movements. Specifically, discussions concerning governments, trading, and Ethereum cryptocurrency as an exchange currency appear to negatively affect Bitcoin and Ethereum prices. Those concerning investments, appear to explain price rises, whilst discussions related to new decentralized realities and technological applications explain price falls. Finally, we validate our model using a real case study: the already famous case of ”Wallstreetbet and GameStop” 1 that took place in January 2021.},
  archive      = {J_MLA},
  author       = {Marco Ortu and Stefano Vacca and Giuseppe Destefanis and Claudio Conversano},
  doi          = {10.1016/j.mlwa.2021.100229},
  journal      = {Machine Learning with Applications},
  pages        = {100229},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Cryptocurrency ecosystems and social media environments: An empirical analysis through hawkes’ models and natural language processing},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting conditional maximum contaminant level exceedance
probabilities for drinking water after wildfires with bayesian
regularized network ensembles. <em>MLA</em>, <em>7</em>, 100227. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severity and frequency of wildfires have increased throughout the Pacific Northwest in recent decades, costing lives and destroying large amounts of valuable resources and assets. This trend is predicted to persist because of climate change and the associated increased fire risk caused by prolonged droughts in combination with changes in land cover and land use, including rapid increases in wildland urban interface areas. The threat of benzene and other contaminants in drinking water from water distribution systems after wildfires is a relatively recently discovered problem that gained attention because of the significant health hazards that high levels of benzene in drinking water pose for humans. Driving processes leading to post-fire benzene contamination in water distribution systems are largely unknown. Currently, no deterministic process models exist to predict the risk of exceeded benzene levels in water distribution systems after wildfires. To address the lack of predictive models, we developed and tested an approach based on neural network models to spatially predict the conditional probabilities of exceeding maximum contaminant levels for benzene after wildfires. The Bayesian regularized neural networks were trained using high-resolution data layers comprising topography, soil properties, landcover, vegetation, meteorological parameters, fuel load, and infrastructure data for two wildland urban interface areas in northern California. The generalized model ensemble encompassing data from both communities exhibits an accuracy of 83\% to 88\% in spatially predicting the post-fire exceedance of benzene levels, offering a planning tool for emergency response and future risk mitigation efforts.},
  archive      = {J_MLA},
  author       = {Andres Schmidt and Lisa M. Ellsworth and Jenna H. Tilt and Mike Gough},
  doi          = {10.1016/j.mlwa.2021.100227},
  journal      = {Machine Learning with Applications},
  pages        = {100227},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting conditional maximum contaminant level exceedance probabilities for drinking water after wildfires with bayesian regularized network ensembles},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alzheimer’s disease diagnosis using genetic programming
based on higher order spectra features. <em>MLA</em>, <em>7</em>,
100225. (<a href="https://doi.org/10.1016/j.mlwa.2021.100225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Alzheimer’s diagnosis field, Computer-Aided Diagnosis (CADx) technology can improve the work performance of medical researchers and practitioners since it gives early chances to patient’s eligibility for clinical trials. The aim of this study is to develop a novel CADx system for the diagnosis of Alzheimer’s disease (AD) by utilizing genetic programming (GP) as data-driven evolutionary computation based modeling. The proposed method invokes a majority voting based scheme to select a set of most discriminant features which leads to the highest diagnosis accuracy of the final classification. The effectiveness of GP in categorizing patients with Alzheimer’s versus healthy group was revealed by developing models according to their performance in terms of higher-order spectra (HOS) features. The results show that the GP method achieved better performance compared to other the-state-of-the-art approaches. It is also found that the highest accuracy index was yielded by using the proposed data-driven modeling technique. The results of this study emphasize the practicality of GP-based method for developing CADx systems, on the basis of spontaneous speech analysis; can efficiently assist in the diagnosis of Alzheimer’s disease.},
  archive      = {J_MLA},
  author       = {Mahda Nasrolahzadeh and Shahryar Rahnamayan and Javad Haddadnia},
  doi          = {10.1016/j.mlwa.2021.100225},
  journal      = {Machine Learning with Applications},
  pages        = {100225},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Alzheimer’s disease diagnosis using genetic programming based on higher order spectra features},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microfluidic droplet detection via region-based and
single-pass convolutional neural networks with comparison to
conventional image analysis methodologies. <em>MLA</em>, <em>7</em>,
100222. (<a href="https://doi.org/10.1016/j.mlwa.2021.100222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the complexity of microfluidic experiments and the associated image data volumes scale, traditional feature extraction approaches begin to struggle at both detection and analysis pipeline throughput. Deep-neural networks trained to detect certain objects are rapidly emerging as data gathering tools that can either match or outperform the analysis capabilities of the conventional methods used in microfluidic emulsion science. We demonstrate that two types of neural-networks, You Only Look Once (YOLOv3, YOLOv5) and Faster R-CNN, can be trained on a dataset which comprises of droplets generated across several microfluidic experiments and systems. The latitude of droplets used for training and validation, produce model weights which are easily transitive to emulsion systems at large, while completely circumventing any necessity of manual feature extraction. In flow cell experiments which comprised of greater than either 10,000 mono- or polydisperse droplets, the models show excellent or superior statistical symmetry to classical implementations of the Hough transform or widely utilized ImageJ plugins. In more complex chip architectures which simulate porous media, the produced image data typically requires heavy pre-processing to extrapolate valid data, where the models were able to handle raw input and produce size distributions with accuracy of ± 2 μ m 2μm for intermediate magnifications. This data harvesting fidelity is extended to foreign datasets not included in the training such as micrograph observation of various emulsified systems. Implementing these neural networks as the sole feature extraction tools in these microfluidic systems not only makes the data pipelining more efficient but opens the door for live detection and development of autonomous microfluidic experimental platforms due to inference times of greater than 100 frames per second.},
  archive      = {J_MLA},
  author       = {Gregory Philip Rutkowski and Ilgar Azizov and Evan Unmann and Marcin Dudek and Brian Arthur Grimes},
  doi          = {10.1016/j.mlwa.2021.100222},
  journal      = {Machine Learning with Applications},
  pages        = {100222},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Microfluidic droplet detection via region-based and single-pass convolutional neural networks with comparison to conventional image analysis methodologies},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to evaluate classifier performance in the presence of
additional effects: A new POD-based approach allowing certification of
machine learning approaches. <em>MLA</em>, <em>7</em>, 100220. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifiers are useful and well-known machine learning algorithms allowing classifications. A classifier may be suited for a specific task depending on the application and datasets. To select an approach for a task, performance evaluation may be imperative. Existing approaches like the receiver operating characteristic and precision–recall curves are popular in evaluating classifier performance, however both measures do not directly address the influence of additional and possibly unknown (process) parameters on the classification results . In this contribution, this limitation is discussed and addressed by adapting the Probability of Detection (POD) measure. The POD is a probabilistic method to quantify the reliability of a diagnostic procedure taking into account statistical variability of sensor and measurements properties. In this contribution the POD approach is adapted and extended. The introduced approach is implemented on driving behavior prediction data serving as illustrative example. Based on the introduced POD-related evaluation, different classifiers can be clearly distinguished with respect to their ability to predict the correct intended driver behavior as a function of remaining time (here assumed as process parameter) before the event itself. The introduced approach provides a new diagnostic and comprehensive interpretation of the quality of a classification model .},
  archive      = {J_MLA},
  author       = {Daniel Adofo Ameyaw and Qi Deng and Dirk Söffker},
  doi          = {10.1016/j.mlwa.2021.100220},
  journal      = {Machine Learning with Applications},
  pages        = {100220},
  shortjournal = {Mach. Learn. Appl.},
  title        = {How to evaluate classifier performance in the presence of additional effects: A new POD-based approach allowing certification of machine learning approaches},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge loss functions for deep-learning depth-map.
<em>MLA</em>, <em>7</em>, 100218. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth computation from an image is useful for many robotic systems like obstacle recognition, autonomous navigation , and 3D measurements. The estimation is best solved with Deep Neural Networks (DNN) as these are non-linear and ill-posed problems. The network takes single-color images with corresponding ground truth to predict depth-map after training. The depth accuracy, here, is dependent on the quality of ground truth and training images. Images have inherent blurs, which impact depth prediction and accuracy. In our work, we study different combinations of loss functions involving various edge functions to improve the depth of images. We use DenseNet and transfer learning method for learning and prediction of depth. Our analysis shows improvement in performance parameters as well as in the visual depth-map. We achieve 85\% δ δ 1 accuracy and improve l o g 10 log10 error using NYU Depth V2 dataset.},
  archive      = {J_MLA},
  author       = {Sandip Paul and Bhuvan Jhamb and Deepak Mishra and M. Senthil Kumar},
  doi          = {10.1016/j.mlwa.2021.100218},
  journal      = {Machine Learning with Applications},
  pages        = {100218},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Edge loss functions for deep-learning depth-map},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning framework for guided wave-based damage
detection of rail head using surface-bonded piezo-electric wafer
transducers. <em>MLA</em>, <em>7</em>, 100216. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to repeated heavy loads, environmental conditions and non-frequent monitoring, the rail is subjected to heavy damage resulting in sudden failure. Hence, a frequent, faster, and efficient monitoring strategy is required. This paper attempts to investigate the application of guided wave (GW) generated through surface-bonded piezo-electric wafer transducer (PWT) to detect damages in rail at high frequencies. Firstly, a combined experimental and simulation study is presented in an effort to understand the dispersion characteristics of guided wave and its interaction with head damages in a relatively small rail specimen. The numerical simulation results are validated with those obtained from the experiments showing a good agreement between them. Secondly, a framework based on the machine learning algorithm is proposed to efficiently detect damage in rail head. Numerous inseparable guided wave modes are observed at higher frequencies implying the inability to detect damage through specific mode. Therefore, a machine learning framework is trained using time, frequency, and time–frequency domain features of the signal. Total 672 numerical simulations of different types of damage with different severity and location in the rail head are carried out to train and validate the model. It is found that GW generated through surface bonded PWTs is able to detect minimum defect size of 5\% of head area with 1 mm thickness. Finally, the proposed framework is tested using simulation and experiment results of arbitrary damage in the rail head. The error in estimating severity was found to be in the range from 2.00\% to 16.67\%.},
  archive      = {J_MLA},
  author       = {Harsh Mahajan and Sauvik Banerjee},
  doi          = {10.1016/j.mlwa.2021.100216},
  journal      = {Machine Learning with Applications},
  pages        = {100216},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning framework for guided wave-based damage detection of rail head using surface-bonded piezo-electric wafer transducers},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making personnel selection smarter through word embeddings:
A graph-based approach. <em>MLA</em>, <em>7</em>, 100214. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper employs techniques and algorithms from the fields of natural language processing , graph representation learning and word embeddings to assist project managers in the task of personnel selection. To do so, our approach initially represents multiple textual documents as a single graph. Then, it computes word embeddings through representation learning on graphs and performs feature selection. Finally, it builds a classification model that is able to estimate how qualified a candidate employee is to work on a given task, taking as input only the descriptions of the tasks and a list of word embeddings. Our approach differs from the existing ones in that it does not require the calculation of key performance indicators or any other form of structured data in order to operate properly. For our experiments, we retrieved data from the Jira issue tracking system of the Apache Software Foundation. The evaluation results show, in most cases, an increase of 0.43\% in the accuracy of the proposed classification models when compared against a widely-adopted baseline method , while their validation loss is significantly decreased by 65.54\%.},
  archive      = {J_MLA},
  author       = {Nikos Kanakaris and Nikolaos Giarelis and Ilias Siachos and Nikos Karacapilidis},
  doi          = {10.1016/j.mlwa.2021.100214},
  journal      = {Machine Learning with Applications},
  pages        = {100214},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Making personnel selection smarter through word embeddings: A graph-based approach},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced performance of dark-nets for brain tumor
classification and segmentation using colormap-based superpixel
techniques. <em>MLA</em>, <em>7</em>, 100212. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain tumor is the deadliest disease in adults as it arises due to an abnormal mass of cells that grows rapidly and it alters the proper functioning of the organs. In clinical practice, radiographic images of different modalities are used to diagnose types of brain tumors, their size, and location. The proposed work aims to automatically classify, localize, and segment brain tumors from T1W-CE Magnetic Resonance Image (MRI) datasets. The T1W-CE MRI dataset is divided into 8:1:1, i.e., 80\% training set, 10\% of each validation, and testing set. To address the overfitting issues, the training data set is augmented using 2-levels wavelet decomposition and geometrical operations (scaling, rotation, translation). Performance of pre-trained DarkNet model (DarkNet-19 and DarkNet-53) is evaluated for the multi-class classification and localization of brain tumors. The best performing pre-trained DarkNet model achieved 99.60\% of training accuracy and 98.81\% of validation accuracy. The performance evaluation parameters confirm the superiority of the proposed methodology in comparison to the state-of-the-art on the T1W-CE MRI dataset. On 1070 T1W-CE testing images, the best-performing pre-trained DarkNet-53 model obtained a testing accuracy of 98.54\% and Area Under Curve (AUC) of 0.99. The tumor is segmented using a 2-D superpixel segmentation technique with an average dice index of 0.94 ± ± 2.6\% on the 793 brain tumor testing data. To prove the superiority of the proposed technique, it is implemented on MRI images from the BraTS2018 dataset. The comparative analysis of performance evaluation parameters of the proposed methodology with the state-of-the-art technique proves its robustness and clinical significance.},
  archive      = {J_MLA},
  author       = {Sakshi Ahuja and Bijaya Ketan Panigrahi and Tapan Kumar Gandhi},
  doi          = {10.1016/j.mlwa.2021.100212},
  journal      = {Machine Learning with Applications},
  pages        = {100212},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhanced performance of dark-nets for brain tumor classification and segmentation using colormap-based superpixel techniques},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A-iLearn: An adaptive incremental learning model for spoof
fingerprint detection. <em>MLA</em>, <em>7</em>, 100210. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning enables the learner to accommodate new knowledge without retraining the existing model. It is a challenging task that requires learning from new data and preserving the knowledge extracted from the previously accessed data. This challenge is known as the stability-plasticity dilemma. We propose A-iLearn, a generic model for incremental learning which overcomes the stability-plasticity dilemma by carefully integrating the ensemble of base classifiers trained on new data with the current ensemble without retraining the model from scratch using entire data. We demonstrate the efficacy of the proposed A-iLearn model on spoof fingerprint detection application. One of the significant challenges associated with spoof fingerprint detection is the performance drop on spoofs generated using new fabrication materials. A-iLearn is an adaptive incremental learning model that adapts to the features of the “live” and “spoof” fingerprint images and efficiently recognizes the new spoof fingerprints and the known spoof fingerprints when the new data is available. To the best of our knowledge, A-iLearn is the first attempt in incremental learning algorithms that adapts to the properties of data for generating a diverse ensemble of base classifiers. From the experiments conducted on standard high-dimensional datasets LivDet 2011, LivDet 2013 and LivDet 2015, we show that the performance gain on new fake materials is significantly high. On average, we achieve 49.57\% improvement in accuracy between the consecutive learning phases.},
  archive      = {J_MLA},
  author       = {Shivang Agarwal and Ajita Rattani and C. Ravindranath Chowdary},
  doi          = {10.1016/j.mlwa.2021.100210},
  journal      = {Machine Learning with Applications},
  pages        = {100210},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A-iLearn: An adaptive incremental learning model for spoof fingerprint detection},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Airbnb rental price modeling based on latent dirichlet
allocation and MESF-XGBoost composite model. <em>MLA</em>, <em>7</em>,
100208. (<a href="https://doi.org/10.1016/j.mlwa.2021.100208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airbnb price modeling is an important decision-making tool that determines the acceptability and profitability of the service. In this study, we demonstrated how proper descriptions of an Airbnb listing and location could influence determining the prices. We assumed the proper description of a listing property positively influences the renter’s decision making; therefore, we applied a Latent Dirichlet Allocation (LDA) based topic model for generating synthetic variables from the textual description of property aiming to improve price prediction accuracy. Additionally, we applied a Moran Eigenvector Spatial Filtering based XGBoost (MESF-XGBoost) model to address the spatial dependence of location data and improve prediction accuracy. Our study at the San Jose County Airbnb dataset found that the number of bedrooms, accommodations, property types, and the total number of reviews positively influence the listing price, whereas the absence of a super host badge and cancellation policy negatively influence the price. The experiment demonstrates that incorporating synthetic variables from both LDA and MESF into the model specification improves the prediction accuracy. The experiment reveals that the XGBoost model with only non-spatial features is not strong enough to address spatial dependence; therefore, it cannot minimize spatial autocorrelation issues.},
  archive      = {J_MLA},
  author       = {Md Didarul Islam and Bin Li and Kazi Saiful Islam and Rakibul Ahasan and Md. Rimu Mia and Md Emdadul Haque},
  doi          = {10.1016/j.mlwa.2021.100208},
  journal      = {Machine Learning with Applications},
  pages        = {100208},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Airbnb rental price modeling based on latent dirichlet allocation and MESF-XGBoost composite model},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heart sound classification using signal processing and
machine learning algorithms. <em>MLA</em>, <em>7</em>, 100206. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to global statistics and the world health organization (WHO), about 17.5 million people die each year from cardiovascular disease. In this paper, the heart sounds gathered by a stethoscope are analyzed to diagnose several diseases caused by heart failure. This research’s primary process is to identify and classify the data related to the heart sounds categorized in four general groups of S 1 S1 to S 4 S4 . The sounds S 1 S1 and S 2 S2 are considered as the heart’s normal sounds, and the sounds S 3 S3 and S 4 S4 are the abnormal sounds of the heart (heart murmurs), each expressing a specific type of heart disease. In this regard, the desired features are first extracted after retrieving the data by signal processing algorithms . In the next step, feature selection algorithms are used to select the compelling features to reduce the problem’s dimensions and obtain the optimal answer faster. While the existing algorithms in the literature classify the sound into two groups of normal and abnormal, in the final section, some of the most popular classification algorithms are utilized to classify the type of sound into three classes of normal, S 3 S3 and S 4 S4 categories. The proposed methodology obtained an accuracy rate of 87.5\% and 95\% for multiclass data (3 classes) and 98\% for binary classification (normal vs. abnormal) problems.},
  archive      = {J_MLA},
  author       = {Yasser Zeinali and Seyed Taghi Akhavan Niaki},
  doi          = {10.1016/j.mlwa.2021.100206},
  journal      = {Machine Learning with Applications},
  pages        = {100206},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Heart sound classification using signal processing and machine learning algorithms},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rainfall prediction: A comparative analysis of modern
machine learning algorithms for time-series forecasting. <em>MLA</em>,
<em>7</em>, 100204. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainfall forecasting has gained utmost research relevance in recent times due to its complexities and persistent applications such as flood forecasting and monitoring of pollutant concentration levels, among others. Existing models use complex statistical models that are often too costly, both computationally and budgetary, or are not applied to downstream applications. Therefore, approaches that use Machine Learning algorithms in conjunction with time-series data are being explored as an alternative to overcome these drawbacks. To this end, this study presents a comparative analysis using simplified rainfall estimation models based on conventional Machine Learning algorithms and Deep Learning architectures that are efficient for these downstream applications. Models based on LSTM , Stacked-LSTM, Bidirectional-LSTM Networks, XGBoost , and an ensemble of Gradient Boosting Regressor, Linear Support Vector Regression , and an Extra-trees Regressor were compared in the task of forecasting hourly rainfall volumes using time-series data. Climate data from 2000 to 2020 from five major cities in the United Kingdom were used. The evaluation metrics of Loss, Root Mean Squared Error, Mean Absolute Error , and Root Mean Squared Logarithmic Error were used to evaluate the models’ performance. Results show that a Bidirectional-LSTM Network can be used as a rainfall forecast model with comparable performance to Stacked-LSTM Networks. Among all the models tested, the Stacked-LSTM Network with two hidden layers and the Bidirectional-LSTM Network performed best. This suggests that models based on LSTM-Networks with fewer hidden layers perform better for this approach; denoting its ability to be applied as an approach for budget-wise rainfall forecast applications.},
  archive      = {J_MLA},
  author       = {Ari Yair Barrera-Animas and Lukumon O. Oyedele and Muhammad Bilal and Taofeek Dolapo Akinosho and Juan Manuel Davila Delgado and Lukman Adewale Akanbi},
  doi          = {10.1016/j.mlwa.2021.100204},
  journal      = {Machine Learning with Applications},
  pages        = {100204},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Rainfall prediction: A comparative analysis of modern machine learning algorithms for time-series forecasting},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frog calling activity detection using lightweight CNN with
multi-view spectrogram: A case study on kroombit tinker frog.
<em>MLA</em>, <em>7</em>, 100202. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frogs play an important role in ecological systems, while frog species across the globe are threatened and declining. Therefore, it is valuable to estimate the frog population based on an intelligent computer system. Due to the success of deep learning (DL) in various pattern recognition tasks, previous studies have used DL-based methods for frog call analysis. However, the performance of DL-based systems is highly affected by their input (feature representation). In this study, we develop a frog calling activity detection system for continuous field recordings using a light convolutional neural network (CNN) with multi-view spectrograms. To be specific, a sliding window is first applied to continuous recordings for obtaining audio segments with a fixed duration. Then, the background noise is filtered out. Next, a multi-view spectrogram is used for characterizing those segments, which has more distinctive information than a single-view spectrogram. Finally, a lightweight CNN model is used for the detection of frog calling activity with a twin loss, where different train and test sets are used to validate the model’s robustness. Our experimental results indicate that the highest macro F1-score was 99.6 ± 0.2 and 96.4 ± 2.0 using 2016 and 2017 as the train data respectively, where CNN-GAP is used as the model with multi-view spectrogram as the input.},
  archive      = {J_MLA},
  author       = {Jie Xie and Mingying Zhu and Kai Hu and Jinglan Zhang and Harry Hines and Ya Guo},
  doi          = {10.1016/j.mlwa.2021.100202},
  journal      = {Machine Learning with Applications},
  pages        = {100202},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Frog calling activity detection using lightweight CNN with multi-view spectrogram: A case study on kroombit tinker frog},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective forecasting of key features in hospital emergency
department: Hybrid deep learning-driven methods. <em>MLA</em>,
<em>7</em>, 100200. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting the different types of emergency department (ED) demands (patient flows) in hospital systems much aids ED managers in looking into various options to appropriately allocating the restricted resources available per patient attendance. Deep learning networks have recently gained great success in modeling time-dependent in time series data . Thus, this work advocates the use of deep learning-driven models for patient flows forecasting. Notably, we examine and compare seven deep learning models , Deep Belief Network (DBN), Restricted Boltzmann machines (RBM), Long Short Term Memory (LSTM), Gated recurrent unit (GRU), combined GRU and convolutional neural networks (CNN-GRU), LSTM-CNN, and Generative Adversarial Network based on Recurrent Neural Networks (GAN-RNN), to forecast patient flow in a hospital emergency department. We introduce a forecaster layer as output for each model to enable traffic flow forecasting. Patient flow data from different ED services, including biology, radiology, scanner, and echography, in Lille regional hospital in France, is used as a case study in assessing the considered forecasting models. Four metrics of effectiveness are adopted for evaluating and comparing the forecasting methods. The results show the promising performance of deep learning models for ED patient flow forecasting compared to shallow methods (i.e., ridge regression and support vector regression). In addition, the results highlighted the superior performance of the DBN compared to the other models by achieving an averaged mean absolute percentage error of around 4.097\% and R2 of 0.973.},
  archive      = {J_MLA},
  author       = {Fouzi Harrou and Abdelkader Dairi and Farid Kadri and Ying Sun},
  doi          = {10.1016/j.mlwa.2021.100200},
  journal      = {Machine Learning with Applications},
  pages        = {100200},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Effective forecasting of key features in hospital emergency department: Hybrid deep learning-driven methods},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self supervised contrastive learning for digital
histopathology. <em>MLA</em>, <em>7</em>, 100198. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised learning has been a long-standing goal of machine learning and is especially important for medical image analysis, where the learning can compensate for the scarcity of labeled datasets. A promising subclass of unsupervised learning is self-supervised learning, which aims to learn salient features using the raw input as the learning signal. In this work, we tackle the issue of learning domain-specific features without any supervision to improve multiple task performances that are of interest to the digital histopathology community. We apply a contrastive self-supervised learning method to digital histopathology by collecting and pretraining on 57 histopathology datasets without any labels. We find that combining multiple multi-organ datasets with different types of staining and resolution properties improves the quality of the learned features. Furthermore, we find using more images for pretraining leads to a better performance in multiple downstream tasks, albeit there are diminishing returns as more unlabeled images are incorporated into the pretraining. Linear classifiers trained on top of the learned features show that networks pretrained on digital histopathology datasets perform better than ImageNet pretrained networks, boosting task performances by more than 28\% in F 1 F1 scores on average. Interestingly, we did not observe a consistent correlation between the pretraining dataset site or the organ versus the downstream task (e.g., pretraining with only breast images does not necessarily lead to a superior downstream task performance for breast-related tasks). These findings may also be useful when applying newer contrastive techniques to histopathology data. Pretrained PyTorch models are made publicly available at https://github.com/ozanciga/self-supervised-histopathology .},
  archive      = {J_MLA},
  author       = {Ozan Ciga and Tony Xu and Anne Louise Martel},
  doi          = {10.1016/j.mlwa.2021.100198},
  journal      = {Machine Learning with Applications},
  pages        = {100198},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Self supervised contrastive learning for digital histopathology},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing the pattern recognition capacity of machine
learning techniques: The importance of feature positioning.
<em>MLA</em>, <em>7</em>, 100196. (<a
href="https://doi.org/10.1016/j.mlwa.2021.100196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design several algorithms representing evaluation processes of different complexity, ranging from basic environments based on a predetermined number of features to complex structures involving alternatives defined through decision trees whose number of nodes is determined by the cardinality of the respective power sets. The sequential structure of these evaluation processes builds on the information retrieval behavior of users in online search environments. The algorithms generate two strings of data, namely, numerical evaluations determining the retrieval behavior of users and the subsequent choices made by the latter. The way the output obtained from the algorithms is inputted within the vectors summarizing the complexity of the evaluation processes conditions the capacity of machine learning techniques to categorize them correctly. The main purpose of the research is to illustrate numerically two main results. First, machine learning techniques categorize processes correctly even if their characteristic features are presented in a way that prevents their identification using standard statistical techniques. Second, the accuracy of the categorization capacities of these techniques can be substantially enhanced by describing the retrieval processes in the way required to implement standard statistical analyses. We perform a battery of tests using machine learning techniques to demonstrate and analyze these results. Their applicability to classification and prediction problems in medical environments, particularly those constrained by the quality of the data available, is emphasized.},
  archive      = {J_MLA},
  author       = {Debora Di Caprio and Francisco J. Santos-Arteaga},
  doi          = {10.1016/j.mlwa.2021.100196},
  journal      = {Machine Learning with Applications},
  pages        = {100196},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing the pattern recognition capacity of machine learning techniques: The importance of feature positioning},
  volume       = {7},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
