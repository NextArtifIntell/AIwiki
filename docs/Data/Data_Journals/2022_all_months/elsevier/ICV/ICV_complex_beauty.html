<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---147">ICV - 147</h2>
<ul>
<li><details>
<summary>
(2022). Loss reweight in scale dimension: A simple while effective
feature selection strategy for anchor-free detectors. <em>ICV</em>,
<em>128</em>, 104593. (<a
href="https://doi.org/10.1016/j.imavis.2022.104593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays an important role during training in object detection and it is becoming a hotspot these years. Here we present a simple yet effective feature selection strategy called Loss Reweight in Scale Dimension(LRSD) and train a single-stage anchor-free detector, termed LRD. To optimize the heuristic-guided feature selection process across FPN levels, for each instance, LRD dynamically reweights the training loss of positive samples from selected top-k feature levels by introducing a reweight function. Because of its rigorous and nonlinear mathematical properties , a precise sample loss reweighting procedure across scale dimension could be done. Without adding extra meta-nets or branches, LRD improves detection performance economically without sacrificing inference speed. Moreover, our detection framework can be further improved by recently proposed transformer-based feature extraction networks such as swin-transformer. Extensive experiments show that LRD achieves 40.4% AP at a speed of 16.7 fps with ResNet-50 as the backbone and helps to improve detection performance by around 1.9% ∼ 2.6% compared with our baseline, symbolic one-stage anchor-free detector Foveabox using ResNet-101 as the backbone. Codes are released at ( https://github.com/PanffeeReal/LRSD-LRD ).},
  archive      = {J_ICV},
  author       = {Pengfei Liu and Yuhan Guo and Jiubin Tan and Weibo Wang},
  doi          = {10.1016/j.imavis.2022.104593},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104593},
  shortjournal = {Image Vis. Comput.},
  title        = {Loss reweight in scale dimension: A simple while effective feature selection strategy for anchor-free detectors},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning based nonlinear two-stage framework for
full-reference image quality assessment: An application for biometric.
<em>ICV</em>, <em>128</em>, 104588. (<a
href="https://doi.org/10.1016/j.imavis.2022.104588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-linearity in medical image processing is a critical issue. Because the privacy of the medical image and loss of data is a major concern in recent years. Federated learning is a most advanced form of machine learning in which, rather than transmitting data to local server, a machine learning (ML) algorithm is installed to various devices to train on the information. The parameters from the separate modules will then be transferred to a master ML/ (deep learning) DL model for global training. The research of Image Quality Assessment (IQA) aims to simulate the process of human perception of image quality and construct an objective image quality model as consistent as possible with subjective assessment. The existing IQA methods can be roughly divided into traditional methods and deep learning methods. Traditional methods are knowledge-driven, using prior knowledge or assumptions about the human visual system (HVS) to heuristically design image quality index. Deep learning methods are data-driven, using a large amount of annotated data to learn the mapping from the image to its visual quality end-to-end. To effectively integrate traditional methods into deep networks and investigate the knowledge (model)-driven deep learning methods are the current mainstream trends in IQA research. In this paper, we take the contrary direction and improve traditional methods guided by the cue from deep learning methods. The main works include: 1. the employment of activation function ensure the nonlinear approximation ability of the neural network , here we first extend the two-stage framework widely used in the field of full-reference image quality assessment and propose a nonlinear two-stage framework. 2. Within this framework, we revisit the Edge Strength SIMlarity (ESSIM) algorithm that we previously published in IEEE Signal Processing Letters, and proposed the Nonlinear Edge Strength SIMlarity (NESSIM) algorithm. Experiments on public databases show that NESSIM can obtain good assessment results in traditional methods.},
  archive      = {J_ICV},
  author       = {Lan Tianyi and Saleem Riaz and Zhang Xuande and Alina Mirza and Farkhanda Afzal and Zeshan Iqbal and Muhammad Attique Khan and Majed Alhaisoni and Abdullah Alqahtani},
  doi          = {10.1016/j.imavis.2022.104588},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104588},
  shortjournal = {Image Vis. Comput.},
  title        = {Federated learning based nonlinear two-stage framework for full-reference image quality assessment: An application for biometric},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose-guided counterfactual inference for occluded person
re-identification. <em>ICV</em>, <em>128</em>, 104587. (<a
href="https://doi.org/10.1016/j.imavis.2022.104587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (ReID) is a challenging task as the body is partially occluded by obstacles in crowd scenarios. Many previous studies employed an attention mechanism to focus on fine-grained local information with conventional likelihood while ignoring the inherent causality between the final prediction results and attention, especially occluded person always possesses biased clues. To address this problem, we propose a Pose-Guided Multi-Attention Network (PGMA-Net) for occluded person ReID in an end-to-end manner. PGMA-Net contains two main novel components: Pose-Guided Counterfactual Inference Branch (PGCIB) and Striped- and Patched-Attention Module (SPAM). The PGCIB jointly explores the causality between the predicted identities and input clues to alleviate the negative effects brought by occluded bias. Specifically, the counterfactual inference can directly guide the attention learning process via the counterfactual intervention. The SPAM generates a set of attention vectors for storing part prototypes over multiple rounds of attention. We empirically demonstrate that PGMA-Net can improve the recognition in both occluded and non-occluded ReID. With the above designs, our framework achieves 52.2% in mAP and 62.0% in top-1 on the Occluded-DukeMTMC dataset, surpassing the baseline by a large margin.},
  archive      = {J_ICV},
  author       = {Ying Chen and Yuzhen Yang and Wenfeng Liu and Yuwen Huang and Jinming Li},
  doi          = {10.1016/j.imavis.2022.104587},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104587},
  shortjournal = {Image Vis. Comput.},
  title        = {Pose-guided counterfactual inference for occluded person re-identification},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-aligned reinforced attention model for zero-shot
learning. <em>ICV</em>, <em>128</em>, 104586. (<a
href="https://doi.org/10.1016/j.imavis.2022.104586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen images from invisible classes, by transferring semantic knowledge from visible classes to invisible classes. Such as, although humans have never seen a zebra, if we know that “a horse with stripes is a zebra”, then we can easily recognize it when we see a zebra. Given semantic descriptions , the human can capture intrinsic visual clues from different channels or appearance factors (e.g., color, texture) on salient parts. But computers are not smart enough to recognize it with high accuracy, they still need to make progress in the learning of semantic-aligned visual representations. Therefore, we propose a semantic-aligned reinforced attention (SRA) model to improve the attributes localization ability. We aim to discover invariable features related to class-level semantic attributes from variable intra-class vision information, and thereby avoid misalignment between much visual information and simple semantic representations . Specially, during the localization of spatial attention, we develop an efficient constraint directly on feature map to ensure the intra-attention compactness and inter-attention dispersion characteristics like human gaze. While for the channel, we proposed a novel attributes attention cross entropy loss to exploit the supervision effect of each semantic attribute subset . Experiments on three ZSL benchmarks, i.e., CUB, SUN and AWA2, indicate the competitiveness of our proposed method against the state-of-the-art ZSL methods.},
  archive      = {J_ICV},
  author       = {Zaiquan Yang and Yuqi Zhang and Yuxin Du and Chao Tong},
  doi          = {10.1016/j.imavis.2022.104586},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104586},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic-aligned reinforced attention model for zero-shot learning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforced pedestrian attribute recognition with group
optimization reward. <em>ICV</em>, <em>128</em>, 104585. (<a
href="https://doi.org/10.1016/j.imavis.2022.104585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian Attribute Recognition (PAR) is a challenging task in intelligent video surveillance. Two key challenges in PAR include complex alignment relations between images and attributes, and imbalanced data distribution . Existing approaches usually formulate PAR as a recognition task. Different from them, this paper addresses it as a decision-making task via a reinforcement learning framework, which is dubbed as Rein-PAR. Specifically, PAR is formulated as a Markov decision process (MDP) to efficiently explore semantic alignments between images and attributes. To alleviate the inter-attribute imbalance problem, we apply an Attribute Grouping Strategy (AGS) by dividing all attributes into subgroups according to their region and category information. Then we employ an agent to recognize each group of attributes, which is trained with Deep Q-learning algorithm. We also propose a Group Optimization Reward (GOR) function to alleviate the intra-attribute imbalance problem. Experimental results on the three benchmark datasets of PETA, RAP and PA100K illustrate the effectiveness and competitiveness of the proposed approach and demonstrate that the application of reinforcement learning to PAR is a valuable research direction.},
  archive      = {J_ICV},
  author       = {Zhong Ji and Zhenfei Hu and Yaodong Wang and Zhuang Shao and Yanwei Pang},
  doi          = {10.1016/j.imavis.2022.104585},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104585},
  shortjournal = {Image Vis. Comput.},
  title        = {Reinforced pedestrian attribute recognition with group optimization reward},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal deep transfer learning based ethnicity recognition
on face images. <em>ICV</em>, <em>128</em>, 104584. (<a
href="https://doi.org/10.1016/j.imavis.2022.104584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, deep learning driven face image analysis has gained significant interest among several application areas like surveillance, security, biometrics , etc. The facial analysis intends to compute facial soft biometrics like ethnicity, expression, identification, age, gender, and so on. Among several biometrics, ethnicity recognition remains a hot research area. Recent advancements in computer vision (CV) and artificial intelligence (AI) models form the basis of an effective design of ethnicity recognition models. With this motivation, this paper introduces a novel Harris Hawks optimization with deep transfer learning based fusion model for face ethnicity recognition (HHODTLF-FER) model. The proposed HHODTLF-FER model is to determine the different kinds of ethnicity for applied facial images . A fusion of three pre-trained DL models , namely VGG16, Inception v3 , and capsule networks (CapsNet) models, are employed. In addition, bidirectional long short term memory (BiLSTM) model is applied for ethnicity recognition and Classification. Finally, HHO algorithm is utilized to fine tune the hyperparameters contained in the BiLSTM model, showing the novelty of the work. In order to ensure the improved recognition performance of the HHODTLF-FER model, a wide ranging experimental analysis is performed using benchmark databases. The comprehensive comparative study highlighted the promising performance of the HHODTLF-FER model over the other approaches.},
  archive      = {J_ICV},
  author       = {Marwa Obayya and Saud S. Alotaibi and Sami Dhahb and Rana Alabdan and Mesfer Al Duhayyim and Manar Ahmed Hamza and Mohammed Rizwanullah and Abdelwahed Motwakel},
  doi          = {10.1016/j.imavis.2022.104584},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104584},
  shortjournal = {Image Vis. Comput.},
  title        = {Optimal deep transfer learning based ethnicity recognition on face images},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent facial expression recognition and classification
using optimal deep transfer learning model. <em>ICV</em>, <em>128</em>,
104583. (<a href="https://doi.org/10.1016/j.imavis.2022.104583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression is commonly utilized by humans to deliver their mood and emotional state to other people. Facial expression recognition (FER) becomes a hot research area in recent days, and it is a tedious process owing to the presence of high intra-class variation. The conventional methods for FEC are mainly based on handcrafted features with a classification model trained on image or video datasets. Since the facial datasets involve large variations in the images and comprise partial faces , it is needed to design automated FER models. The latest advancements in artificial intelligence (AI) and deep learning (DL) models find useful for better understanding of facial emotions related to face images. In this aspect, this paper presents an intelligent FER using optimal deep transfer learning (IFER-DTFL) model. The proposed IFER-DTFL technique aims to detect the face and identify the facial expressions automatically. The IFER-DTFL technique encompasses a three state process: face detection, feature extraction, and expression classification. In addition, a mask RCNN model is used for the detection of faces. Moreover, the Adam optimizer with Densely Connected Networks (DenseNet121) model is employed for feature extraction process. Furthermore, the weighted kernel extreme learning machine (WKELM) model is utilized to classify the facial expressions. A comprehensive set of simulations were carried out on benchmark dataset and the results are inspected under varying aspects. The experimental results pointed out the supremacy of the IFER-DTFL technique over the other recent techniques interms of several performance measures .},
  archive      = {J_ICV},
  author       = {Amani Abdulrahman Albraikan and Jaber S. Alzahrani and Reem Alshahrani and Ayman Yafoz and Raed Alsini and Anwer Mustafa Hilal and Ahmed Alkhayyat and Deepak Gupta},
  doi          = {10.1016/j.imavis.2022.104583},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104583},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent facial expression recognition and classification using optimal deep transfer learning model},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-guided transformer for image captioning.
<em>ICV</em>, <em>128</em>, 104575. (<a
href="https://doi.org/10.1016/j.imavis.2022.104575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based frameworks have shown superiorities in image captioning . However, such frameworks are strenuous to consider geometric interrelations among visual contents in an image, as well as fail to prevent changes in the distribution of each layer’s input in self-attention. In this work, we first propose a Bi-Positional Attention (BPA) module, which incorporates absolute and relative position encoding to precisely explore internal relations between objects and their geometric information in an image. Additionally, we use a Group Normalization (GN) method inside BPA to relieve shifts of the distribution and better exploit the channel dependence of visual features. To validate our proposals, we apply BPA and GN into the original Transformer to constitute our Position-Guided Transformer (PGT) network, which learns a more comprehensive positional representations to augment spatial interactions among objects for image captioning. We conduct extensive experiments to verify the effectiveness of our model. Compared with non-pretraining state-of-the-art methods, experimental results on the MSCOCO benchmark dataset demonstrate that our PGT achieves competitive performance, reaching 134.2% CIDEr score on the Karpathy split with a single model, and 136.2% CIDEr score on the official testing server with an ensemble configuration.},
  archive      = {J_ICV},
  author       = {Juntao Hu and You Yang and Lu Yao and Yongzhi An and Longyue Pan},
  doi          = {10.1016/j.imavis.2022.104575},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104575},
  shortjournal = {Image Vis. Comput.},
  title        = {Position-guided transformer for image captioning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual global-aware propagation for few-shot learning.
<em>ICV</em>, <em>128</em>, 104574. (<a
href="https://doi.org/10.1016/j.imavis.2022.104574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning remains a challenging problem because it needs to classify unseen categories with only a few samples as limited supervision. The tasks and samples can be extremely different in various few-shot problems, which makes it even more difficult. Due to the local connectivity in CNN, it can not capture the global description of the samples and the features are not discriminative enough from a global viewpoint. Meanwhile, a sample usually reveals similar features in different tasks, which does not consider the global information of the task and weakens the discrimination of features. To address the above issue, we proposed a Dual Global-Aware method for label Propagation (DGAP) to encode two kinds of global description to enhance the discriminative power of the learned features. On the sample level, the global-aware sample module (GSM) is employed to get the contextual description and enhance the feature representation capability of each sample. On the task level, the global-aware task module (GTM) is used to embed the features in the current task to a more appropriate and discriminative position in the feature space which is task-oriented. In the end, a feature fusion module is adopted to combine the features obtained from both global sample and global task respects. Based on the label propagation method, the proposed DGAP improves the performance approximately 2–5% over the baseline on different benchmarks (mini-Imagenet and tiered-Imagenet) across different structures (Conv4 structure and ResNet12 structure), which reaches the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Zhiyan Cui and Na Lu and Weifeng Wang and Guangshuai Guo},
  doi          = {10.1016/j.imavis.2022.104574},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104574},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual global-aware propagation for few-shot learning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep hybrid learning for facial expression binary
classifications and predictions. <em>ICV</em>, <em>128</em>, 104573. (<a
href="https://doi.org/10.1016/j.imavis.2022.104573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing is a technique used for applying different operations to an image to produce an improved image or extract relevant information. Image processing has multiple applications in numerous fields, such as robotics, vision, pattern recognition, video processing, and the medical industry. One prominent application of facial recognition in image processing is identifying human expression. This research examines the accuracy of categorizing human facial expressions as happy or angry with deep learning and transfer learning methods such as CNN , LSTM , Inception, ResNet, VGG, Xception, and InceptionResnet. The proposed deep hybrid learning (DHL) approach classifies facial expressions using transfer learning and deep neural networks. This approach emphasizes the enhancement of prediction and classification by combining multiple deep learning models to perform better than a single model. The proposed model has a testing accuracy of 81.42% and a training accuracy of 95.93% with a multisource image dataset.},
  archive      = {J_ICV},
  author       = {Ram Krishn Mishra and Siddhaling Urolagin and J. Angel Arul Jothi and Pramod Gaur},
  doi          = {10.1016/j.imavis.2022.104573},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104573},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep hybrid learning for facial expression binary classifications and predictions},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive learning based facial action unit detection in
children with hearing impairment for a socially assistive robot
platform. <em>ICV</em>, <em>128</em>, 104572. (<a
href="https://doi.org/10.1016/j.imavis.2022.104572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a contrastive learning-based facial action unit detection system for children with hearing impairments to be used on a socially assistive humanoid robot platform. The spontaneous facial data of children with hearing impairments was collected during an interaction study with Pepper humanoid robot, and tablet-based game. Since the collected dataset is composed of limited number of instances, a novel domain adaptation extension is applied to improve facial action unit detection performance, using some well-known labelled datasets of adults and children. Furthermore, since facial action unit detection is a multi-label classification problem, a new smoothing parameter, β β , is introduced to adjust the contribution of similar samples to the loss function of the contrastive learning. The results show that the domain adaptation approach using children’s data (CAFE) performs better than using adult’s data (DISFA). In addition, using the smoothing parameter β β leads to a significant improvement on the recognition performance.},
  archive      = {J_ICV},
  author       = {Cemal Gurpinar and Seyma Takir and Erhan Bicer and Pinar Uluer and Nafiz Arica and Hatice Kose},
  doi          = {10.1016/j.imavis.2022.104572},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104572},
  shortjournal = {Image Vis. Comput.},
  title        = {Contrastive learning based facial action unit detection in children with hearing impairment for a socially assistive robot platform},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advances in deep learning-based image recognition of product
packaging. <em>ICV</em>, <em>128</em>, 104571. (<a
href="https://doi.org/10.1016/j.imavis.2022.104571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information about the goods on the shelves needed to be obtained by the dealers in real time, so that they could serve the customers better. The visual-based shelf commodity detection methods have attracted extensive attention in recent years. Among them, the target detection method based on deep learning could automatically understand the picture features, which greatly promoted the development of goods shelf identification. This paper made a comparison and analysis between the common target detecting data set and the data set of goods on shelf from their features, advantages and application. It came up with a method to build an excellent data set of goods, and sorted out the data enhancement methods to perfect the data set for defective data sets. Then, from the three angles of large-scale package identification, small target identification and partial occluded recognition, the latest product package identification method was summarized. At last, the limitations of deep learning method in product identification were discussed and the future development direction was looked forward.},
  archive      = {J_ICV},
  author       = {Siyuan Chen and Danfei Liu and Yumei Pu and Yunfei Zhong},
  doi          = {10.1016/j.imavis.2022.104571},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104571},
  shortjournal = {Image Vis. Comput.},
  title        = {Advances in deep learning-based image recognition of product packaging},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ArCo: Attention-reinforced transformer with contrastive
learning for image captioning. <em>ICV</em>, <em>128</em>, 104570. (<a
href="https://doi.org/10.1016/j.imavis.2022.104570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is a significant step toward achieving automatic interactions between humans and computers, in which a textual sequence of the content of an image is generated. Recently, the transformer-based encoder–decoder paradigm has made great achievements in image captioning. This method is usually trained with a cross-entropy loss function. However, for various captions of images with the same meaning, the computed losses may be different. The result is that the descriptions of images tend to be consistent, which limits the diversity of image captioning. In this paper, we present an attention-reinforced transformer, a transformer-based architecture for image captioning. The architecture improves the image encoding stage, which exploits the relationships between image regions by integrating a feature attention block (FAB). During the training phase, we trained the model with a combination of cross-entropy loss and contrastive loss. We experimentally explored the performance of ArCo and other fully attentive models. We also validated the baseline of the transformer for image captioning with different pre-trained models. Our proposed approach was demonstrated to achieve a new state-of-the-art performance on the offline ‘Karpathy’ test split and online test server.},
  archive      = {J_ICV},
  author       = {Zhongan Wang and Shuai Shi and Zirong Zhai and Yingna Wu and Rui Yang},
  doi          = {10.1016/j.imavis.2022.104570},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104570},
  shortjournal = {Image Vis. Comput.},
  title        = {ArCo: Attention-reinforced transformer with contrastive learning for image captioning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion detection and face recognition of drivers in
autonomous vehicles in IoT platform. <em>ICV</em>, <em>128</em>, 104569.
(<a href="https://doi.org/10.1016/j.imavis.2022.104569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has been used in many fields, such as medical treatment, video monitoring, and transport. In these regions, the rapid adoption and development of IoT produce a large amount of evidence. For instance, IoT devices such as cameras capture the different facial emotions of drivers in Autonomous driving systems. In completely autonomous vehicles, drivers have trouble in takeover transfers as they become disconnected from the actual aspect of driving. Factors affecting takeover effectiveness, such as lead time and the participation of non-driving-related tasks, have been concentrated. Nevertheless, considering the vital role of emotions, human communication and manual driving affect drivers&#39; takeover efficiency. Face identity is essential for emotional sensitivity detection for drivers in autonomous vehicles. Automated and intelligent face recognition (FR) devices are highly accurate in a comfortable condition and unregulated, with poor reliability in autonomous vehicles. Artificial Intelligence (AI) can significantly perceive and express feelings in well-being and similar fields. This study suggests an optimized IoT architecture that facilitates the physiological signal with wireless transmission to the database management center. Face Recognition and Emotion Detection based on IoT (FRED-IoT) has been proposed to track drivers&#39; emotional and face recognition in autonomous vehicles. A low delay of 2 milliseconds is achieved in the proposed IoT Protocols. In contrast with cutting-edge technology, FRED-IoT improves reliability and achieves a high rating (F-score) of 96%.},
  archive      = {J_ICV},
  author       = {Zhongshan Chen and Xinning Feng and Shengwei Zhang},
  doi          = {10.1016/j.imavis.2022.104569},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104569},
  shortjournal = {Image Vis. Comput.},
  title        = {Emotion detection and face recognition of drivers in autonomous vehicles in IoT platform},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistage temporal convolution transformer for action
segmentation. <em>ICV</em>, <em>128</em>, 104567. (<a
href="https://doi.org/10.1016/j.imavis.2022.104567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses fully supervised action segmentation. Transformers have been shown to have large model capacity and powerful sequence modeling abilities, and hence seem quite suitable for capturing action grammar in videos. However, their performance in video understanding still lags behind that of temporal convolutional networks , or ConvNets for short. We hypothesize that this is because: (i) ConvNets tend to generalize better than Transformers, and (ii) Transformer&#39;s large model capacity requires significantly larger training datasets than existing action segmentation benchmarks. We specify a new hybrid model, TCTr , that combines the strengths from both frameworks. TCTr seamlessly unifies depth-wise convolution and self-attention in a principled manner. Also, TCTr addresses the Transformer&#39;s quadratic computational and memory complexity in the sequence length by learning how to adaptively estimate attention from local temporal neighborhoods, instead of all frames. Our experiments show that TCTr significantly outperforms the state of the art on the Breakfast, GTEA, and 50Salads datasets.},
  archive      = {J_ICV},
  author       = {Nicolas Aziere and Sinisa Todorovic},
  doi          = {10.1016/j.imavis.2022.104567},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104567},
  shortjournal = {Image Vis. Comput.},
  title        = {Multistage temporal convolution transformer for action segmentation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ST-VTON: Self-supervised vision transformer for image-based
virtual try-on. <em>ICV</em>, <em>127</em>, 104568. (<a
href="https://doi.org/10.1016/j.imavis.2022.104568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual try-on techniques are in the spotlight due to the promise of enhancing the user experience of online shopping in the apparel industry . Among these technologies, image-based virtual try-on methods have received more attention due to their efficiency and low cost. These methods commonly warp the specific clothing item to the desired shape and then fuse it with the target person image. However, existing approaches demand the laborious paired dataset of clothes and persons. While recent works alleviate the dependence on paired datasets by transferring clothing from one person to another, the pose information is still a rigid demand to guide the synthesis. To solve the above-mentioned problems, we propose ST-VTON, a two-stage self-supervised try-on approach that can synthesize photo-realistic try-on results with only person images and a standard ViT structure. Specifically, we first utilize massive person and clothing images to pre-train a powerful ViT encoder following the MAE approach to enhance the generalization ability and accelerate model training. Secondly, we propose the Clothing Recovery Module (CRM) to reconstruct in-shop clothes from person images and use the paired generated clothing and corresponding person images to train a new ViT decoder while optimizing the pre-trained encoder. Noted that, the CRM, as an independent module, can be used to effectively expand existing paired try-on datasets and enhance other try-on approaches in terms of training data . Extensive experiments conducted on the VITON dataset and UnPaired virtual Try-on (UPT) dataset with both paired and unpaired approaches demonstrate that try-on results rendered by our model can match or even outperform supervised methods.},
  archive      = {J_ICV},
  author       = {Zheng Chong and Lingfei Mo},
  doi          = {10.1016/j.imavis.2022.104568},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104568},
  shortjournal = {Image Vis. Comput.},
  title        = {ST-VTON: Self-supervised vision transformer for image-based virtual try-on},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A controllable face forgery framework to enrich
face-privacy-protection datasets. <em>ICV</em>, <em>127</em>, 104566.
(<a href="https://doi.org/10.1016/j.imavis.2022.104566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning not only brings convenience to people, but also promotes the development of facial forgery technology. Considering the current personal portrait security issues, the tampering and forgery of facial data has attracted more and more attention. In order to solve the above issues, we try to implement from another novel angle, that is, enrich the face-privacy-protection dataset to improve the detection ability of forgery faces. Therefore, we propose a controllable face forgery framework. In this work, we firstly analyze the identity information in the latent features and construct an identity latent space based on StyleGAN’s w + latent space. Then, we propose an adaptive identity mapping network to edit the latent codes of the image through encoder and realize the identity transform. Finally, we further enhance the authenticity of the image through post-processing. In order to verify the superiority of our proposed method, we design extensive experiments. Experiments show the effectiveness of identity latent space and the controllability of our model. At the same time, it also shows that our proposed network can generate photo-level results and achieve excellent results in the comparison of other face swapping methods.},
  archive      = {J_ICV},
  author       = {Jiachen Yang and Yong Zhu and Shuai Xiao and Guipeng Lan and Yang Li},
  doi          = {10.1016/j.imavis.2022.104566},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104566},
  shortjournal = {Image Vis. Comput.},
  title        = {A controllable face forgery framework to enrich face-privacy-protection datasets},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D-VDNet: Exploiting the vertical distribution
characteristics of point clouds for 3D object detection and
augmentation. <em>ICV</em>, <em>127</em>, 104557. (<a
href="https://doi.org/10.1016/j.imavis.2022.104557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D object detection is limited by the sparsity of LiDAR-based point clouds. The vertical distribution characteristics (VDCs) of point clouds in pillars are robust to point-sparsity and provide informative semantic information on objects. Based on this, we propose a novel 3D object detection framework where the VDCs of point clouds are exploited to optimize feature extraction and object augmentation. More specifically, a Spatial Feature Aggregation module is proposed to perform robust feature extraction by decorating pillars with the VDCs. To spatially enhance semantic embeddings, we employ VDCs to construct a voxelized semantic map, acting as an additional input stream. Moreover, we develop an Adaptive Object Augmentation (AOA) paradigm, which adopts the VDC searching of suitable ground regions to “paste” virtual objects, thus avoiding conflicts with new scenes. Extensive experiments on the KITTI dataset demonstrate that our framework can significantly outperform the baseline, achieving 3.74%/1.59% moderate AP improvements on the Car 3D/BEV benchmarks with 38 FPS inference speed. Furthermore, we prove the stable performance of our AOA module across different detectors.},
  archive      = {J_ICV},
  author       = {Weiping Xiao and Xiaomao Li and Chang Liu and Jiantao Gao and Jun Luo and Yan Peng and Yang Zhou},
  doi          = {10.1016/j.imavis.2022.104557},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104557},
  shortjournal = {Image Vis. Comput.},
  title        = {3D-VDNet: Exploiting the vertical distribution characteristics of point clouds for 3D object detection and augmentation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation and context augmentation network for facial
expression recognition. <em>ICV</em>, <em>127</em>, 104556. (<a
href="https://doi.org/10.1016/j.imavis.2022.104556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) is a challenging task due to the complex properties of human facial expression. Recently, convolutional neural networks (CNNs) have been widely adopted by most FER approaches. However, CNN-models extract features by using convolutional and pooling operations which ignore the relations between pixels and channels. The relations among spatial positions and channels provide crucial information which can be leveraged for facial expression classification. Another important aspect of FER is utilization of global and local contextual information to improve recognition performance. In this work, we present a deep network, the Relation and Context Augmentation Network (RCANet), for facial expression classification. RCANet consists of two relation modules and a context module. The relation modules compute global relations in spatial and channel dimensions. The context module is composed of cascaded context units to capture multi-scale contextual information. Extensive experiments are conducted on two popular in-the-wild FER datasets, including RAF-DB and AffectNet. Experimental results demonstrate that our proposed method achieves 90.15% and 65.65% accuracy rate on the RAF-DB and AffectNet datasets respectively.},
  archive      = {J_ICV},
  author       = {Xin Ma and Yingdong Ma},
  doi          = {10.1016/j.imavis.2022.104556},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104556},
  shortjournal = {Image Vis. Comput.},
  title        = {Relation and context augmentation network for facial expression recognition},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative learning network for head pose estimation.
<em>ICV</em>, <em>127</em>, 104555. (<a
href="https://doi.org/10.1016/j.imavis.2022.104555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation is an important task in many real-world applications, such as human–computer interaction, driver monitoring, face localization and gaze estimation. In this paper, we present a novel collaborative learning framework based on Convolutional Neural Networks (CNNs) for head pose estimation. The proposed framework consists of a landmark-based branch and a landmark-free branch. The former first estimates facial landmarks and then follows the Landmark-MLP-Mixer module which models the complex nonlinear mapping relationship from facial landmarks to head pose angles. While the later adopts a label distribution learning strategy to estimate head pose. The two branches both dedicate themselves to head pose estimation task, and they collaborate with each other for mutual promotion and complementary semantic learning. Specifically, we introduce a dual-branch transfer module in the middle of the network to achieve explicit semantic interaction and introduce a multi-loss strategy that induces to implicit information interaction. We conduct extensive experiments on several popular benchmarks, including AFLW, AFLW2000 and BIWI, the results show that our method is competitive compared to other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Haiying Xia and Gan Liu and Luhui Xu and Yanling Gan},
  doi          = {10.1016/j.imavis.2022.104555},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104555},
  shortjournal = {Image Vis. Comput.},
  title        = {Collaborative learning network for head pose estimation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weather-degraded image semantic segmentation with multi-task
knowledge distillation. <em>ICV</em>, <em>127</em>, 104554. (<a
href="https://doi.org/10.1016/j.imavis.2022.104554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of degraded image in adverse weather is of great importance for the navigation system of autonomous driving . However, weather-degraded images increase the difficulty of semantic segmentation as well as decrease the accuracy. It is natural to integrate image enhancement into degraded image semantic segmentation to improve the accuracy, which is computation intensive and time consuming. To meet the challenge, we propose a fast degraded image semantic segmentation with Multi-Task Knowledge Distillation called MTKD. The proposed MTKD method encourages image enhancement and semantic segmentation networks to learn from each other to make full use of the correlation between two tasks. Additionally, we propose shift operator to realize a lightweight model design. Extensive experiments demonstrate that the proposed MTKD outperforms state-of-the-art methods not only with better semantic segmentation performance but also with higher speed in weather-degraded images, which achieves 0.038 s in semantic segmentation for a 2048 × 1024 image.},
  archive      = {J_ICV},
  author       = {Zhi Li and Xing Wu and Jianjia Wang and Yike Guo},
  doi          = {10.1016/j.imavis.2022.104554},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104554},
  shortjournal = {Image Vis. Comput.},
  title        = {Weather-degraded image semantic segmentation with multi-task knowledge distillation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online multi-object tracking with δ-GLMB filter based on
occlusion and identity switch handling. <em>ICV</em>, <em>127</em>,
104553. (<a href="https://doi.org/10.1016/j.imavis.2022.104553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an online multi-object tracking (MOT) method based on the delta Generalized Labeled Multi-Bernoulli ( δ δ -GLMB) filter framework to address occlusion and miss-detection issues and recover identity switch (ID switch). Along with the principal δ δ -GLMB filter that performs multi-object tracking, we propose a one-step δ δ -GLMB filter to handle occlusion and miss-detection. The one-step δ δ -GLMB filter is non-iterative and only requires current measurements. The filter is based on a proposed measurement-to-reappeared track association method and addresses MOT issues by incorporating all occluded and miss-detected objects. We introduce a novel similarity metric to apply in the measurement-to-reappeared track association process to define the weight of hypothesized reappeared tracks. To ensure the track consistency, we also extend the principal δ δ -GLMB filter to efficiently recover switched IDs using the cardinality density, size, and visual features of the hypothesized tracks. In addition, we perform an ablation study to demonstrate the contribution of the main parts of the proposed method. We evaluate the proposed method on well-known and publicly available test datasets focused on pedestrian tracking. Note that our proposed method is online and not based on the learning paradigm. So it does not use any additional source of information such as private detections and pre-trained networks. Despite that, we achieved a reliable performance in multiple persons tracking at complex scenes by applying occlusion/miss-detection and ID switch handlers. Experimental results show that the proposed tracker performs better or at least at the same level of the state-of-the-art online and offline MOT methods.},
  archive      = {J_ICV},
  author       = {Mohammadjavad Abbaspour and Mohammad Ali Masnadi-Shirazi},
  doi          = {10.1016/j.imavis.2022.104553},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104553},
  shortjournal = {Image Vis. Comput.},
  title        = {Online multi-object tracking with δ-GLMB filter based on occlusion and identity switch handling},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NullSpaceRDAR: Regularized discriminative adaptive nullspace
for object tracking. <em>ICV</em>, <em>127</em>, 104550. (<a
href="https://doi.org/10.1016/j.imavis.2022.104550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, discriminative-based and Siamese-based trackers have achieved outstanding performance on most tracking benchmarks. However, these trackers use the pre-trained backbone networks that have been mainly designed for classification to extract target-specific features without taking into consideration the visual object tracking task. In this paper, we propose NullSpaceRDAR, a novel tracker that learns a robust target-specific feature representation specifically designed for object tracking. This feature representation is learned by projecting the traditional backbone feature space onto a novel discriminative nullspace that is used to regularize the backbone loss function. We refer to the discriminative nullspace herein as joint-nullspace. The same target features (i.e., target-specific) in the proposed joint-nullspace are collapsed into a single point, and different target-specific features are collapsed into different points. Consequently, the joint-nullspace forces the network to be sensitive to the object’s variations from the same class (i.e., intra-class variations). Moreover, a modified adaptive loss function is developed for bounding box estimation to select the most suitable loss function from a super set family of loss functions based on the training data. This makes NullSpaceRDAR more robust to different challenges such as occlusions and background clutter. Extensive experiments have been conducted on six benchmarks to evaluate NullSpaceRDAR: OTB100, VOT variations (VOT2018, VOT2019, and VOT2020), LaSOT, TrackingNet, UAV123, and GOT10k. The results show that NullSpaceRDAR outperforms the state-of-the-art trackers.},
  archive      = {J_ICV},
  author       = {Mohamed H. Abdelpakey and Mohamed S. Shehata},
  doi          = {10.1016/j.imavis.2022.104550},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104550},
  shortjournal = {Image Vis. Comput.},
  title        = {NullSpaceRDAR: Regularized discriminative adaptive nullspace for object tracking},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiaTrans: Siamese transformer network for RGB-d salient
object detection with depth image classification. <em>ICV</em>,
<em>127</em>, 104549. (<a
href="https://doi.org/10.1016/j.imavis.2022.104549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D SOD uses depth information to handle challenging scenes and obtain high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection methods overwhelmingly rely on the strategy of directly fusing depth information. Although these methods improve the accuracy of saliency prediction through various cross-modality fusion strategies, misinformation provided by some poor-quality depth images can affect the saliency prediction result. To address this issue, a novel RGB-D salient object detection model (SiaTrans) is proposed in this paper, which allows training on depth image quality classification at the same time as training on SOD. In light of the common information between RGB and depth images on salient objects, SiaTrans uses a Siamese transformer network with shared weight parameters as the encoder and extracts RGB and depth features concatenated on the batch dimension, saving space resources without compromising performance. SiaTrans uses the class token in the backbone network (T2T-ViT) to classify the quality of depth images without preventing the token sequence from going on with the saliency detection task. The greatest benefit of our cross-modality fusion (CMF) and decoder is that they maintain consistency between RGB and RGB-D information decoding. In the test, SiaTrans decides whether to perform an RGB-D or RGB saliency detection task according to the quality classification signal of the depth image. Comprehensive experiments on nine RGB-D SOD benchmark datasets show that SiaTrans has the best overall performance and the least computation compared with recent state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {XingZhao Jia and ChangLei DongYe and YanJun Peng},
  doi          = {10.1016/j.imavis.2022.104549},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104549},
  shortjournal = {Image Vis. Comput.},
  title        = {SiaTrans: Siamese transformer network for RGB-D salient object detection with depth image classification},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGB-t tracking by modality difference reduction and feature
re-selection. <em>ICV</em>, <em>127</em>, 104547. (<a
href="https://doi.org/10.1016/j.imavis.2022.104547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracking has attracted increasing attention, since visible and thermal data have strong complementary advantages to improve the robustness of trackers. Most existing models focus on investigating efficient ways of fusing the complementary information from RGB and thermal images for better tracking performance. However, the modality differences caused by different imaging mechanisms may degrade the discriminability of the fused features. Meanwhile, compared with the unimodal features, the fused features may not always improve the tracking performance, especially when one of the input images contain much noisy information. In view of this, we propose a novel RGB-T tracking model by simultaneously reducing modality difference and re-selecting discriminative features from the fused features as well as from the unimodal features. To this end, a Modality Difference Compensation module (MDC) and a Feature Re-selection module (FRS) are presented. The former one reduces the modality differences between RGB and thermal features and obtains the fused features. The latter one adaptively selects such discriminative features from the unimodal features and the fused features for the subsequent classification and regression. Exhausted experiments are conducted on three RGB-T tracking benchmark datasets, which verify that our proposed tracker performs favorably against some state-of-the-art tracking algorithms.},
  archive      = {J_ICV},
  author       = {Qiang Zhang and Xueru Liu and Tianlu Zhang},
  doi          = {10.1016/j.imavis.2022.104547},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104547},
  shortjournal = {Image Vis. Comput.},
  title        = {RGB-T tracking by modality difference reduction and feature re-selection},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep shape-from-template: Single-image quasi-isometric
deformable registration and reconstruction. <em>ICV</em>, <em>127</em>,
104531. (<a href="https://doi.org/10.1016/j.imavis.2022.104531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape-from-Template (SfT) solves 3D vision from a single image and a deformable 3D object model, called a template. Concretely, SfT computes registration (the correspondence between the template and the image) and reconstruction (the depth in camera frame). It constrains the object deformation to quasi-isometry. Real-time and automatic SfT represents an open problem for complex objects and imaging conditions. We present four contributions to address core unmet challenges to realise SfT with a Deep Neural Network (DNN). First, we propose a novel DNN called DeepSfT, which encodes the template in its weights and hence copes with highly complex templates. Second, we propose a semi-supervised training procedure to exploit real data. This is a practical solution to overcome the render gap that occurs when training only with simulated data. Third, we propose a geometry adaptation module to deal with different cameras at training and inference. Fourth, we combine statistical learning with physics-based reasoning. DeepSfT runs automatically and in real-time and we show with numerous experiments and an ablation study that it consistently achieves a lower 3D error than previous work. It outperforms in generalisation and achieves great performance in terms of reconstruction and registration error with wide-baseline, occlusions, illumination changes, weak texture and blur.},
  archive      = {J_ICV},
  author       = {David Fuentes-Jimenez and Daniel Pizarro and David Casillas-Pérez and Toby Collins and Adrien Bartoli},
  doi          = {10.1016/j.imavis.2022.104531},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104531},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep shape-from-template: Single-image quasi-isometric deformable registration and reconstruction},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time human-centric segmentation for complex video
scenes. <em>ICV</em>, <em>126</em>, 104552. (<a
href="https://doi.org/10.1016/j.imavis.2022.104552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing video tasks related to “human” focus on the segmentation of salient humans, ignoring the unspecified others in the video. Few studies have focused on segmenting and tracking all humans in a complex video, including pedestrians and humans of other states (e.g., seated, riding, or occluded). In this paper, we propose a novel framework, abbreviated as HVISNet, that segments and tracks all presented people in given videos based on a one-stage detector. To better evaluate complex scenes, we offer a new benchmark called HVIS (Human Video Instance Segmentation), which comprises 1447 human instance masks in 805 high-resolution videos in diverse scenes. Extensive experiments show that our proposed HVISNet outperforms the state-of-the-art methods in terms of accuracy at a real-time inference speed (30 FPS), especially on complex video scenes. We also notice that using the center of the bounding box to distinguish different individuals severely deteriorates the segmentation accuracy, especially in heavily occluded conditions. This common phenomenon is referred to as the ambiguous positive samples problem. To alleviate this problem, we propose a mechanism named Inner Center Sampling to improve the accuracy of instance segmentation. Such a plug-and-play inner center sampling mechanism can be incorporated in any instance segmentation model based on a one-stage detector to improve the performance. In particular, it gains 4.1 mAP improvement on the state-of-the-art method in the case of occluded humans. Code and data are available at https://github.com/IIGROUP/HVISNet .},
  archive      = {J_ICV},
  author       = {Ran Yu and Chenyu Tian and Weihao Xia and Xinyuan Zhao and Liejun Wang and Yujiu Yang},
  doi          = {10.1016/j.imavis.2022.104552},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104552},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time human-centric segmentation for complex video scenes},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning diverse and deep clues for person reidentification.
<em>ICV</em>, <em>126</em>, 104551. (<a
href="https://doi.org/10.1016/j.imavis.2022.104551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting robust features has been the core of person re-identification(ReID). The existing convolutional neural network-based methods pay more attention to local features rather than the connection between local features. Given that human bodies possess certain structural information, it is absolutely necessary to strengthen the connection of local features for the ReID task. This paper proposes a two-stage attention network termed Width and Depth Channel Attention Network (WDC-Net) for ReID. Unlike conventional attention-based methods, which only focus on the single local features, our network exploits diverse feature representations to alleviate the missing information problem caused by occlusion. Precisely, for the first stage, it splits the local associations of the feature map through a multi-scale perspective to extract relatively independent multi-level local features of the human body. For the second stage, the correlation of multi-level local features is reconstructed through grouped pyramid structure to obtain a more robust global feature representation. We also propose an adaptive margin weight adjustment strategy to enhance the adaptability of the attention weights. Large-scale ReID datasets are tested to evaluate our method. On Market1501 and DukeMTMC, the proposed method achieves 90.7 % /96.4% mAP/R-1 and 81.8 % /90.8% mAP/R-1, respectively. It is worth highlighting that the proposed method also achieves 55.3 % /65.3% mAP/R-1 on the challenging Occluded-Duke dataset. Extensive experimental results demonstrate the superiority of our method, which achieves state-of-the-art performance on ReID.},
  archive      = {J_ICV},
  author       = {Wencheng Qin and Baojin Huang and Pinzhong Qin and Zhiyong Huang and Daidi Zhong},
  doi          = {10.1016/j.imavis.2022.104551},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104551},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning diverse and deep clues for person reidentification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label out-of-distribution detection via exploiting
sparsity and co-occurrence of labels. <em>ICV</em>, <em>126</em>,
104548. (<a href="https://doi.org/10.1016/j.imavis.2022.104548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Out-Of-Distribution (OOD) detection has been extensively studied, the OOD detection under multi-label settings, which is closer to the real world, is still in its infancy. The pioneer work ignores some unique properties of multi-label images, such as the sparsity and co-occurrence of labels. Here, we empirically observe that these properties readily distinguish OOD and in-distribution data. Motivated by this observation, we propose a novel multi-label OOD detection approach named Sparse Label Co-occurrence Scoring (SLCS) to exploit the sparsity and co-occurrence information of labels. SLCS follows conventions and deems the logits outputted by the penultimate layer of the trained multi-label image classification model as the prediction confidences of a sample to categories in the training label set. A logit sparse filtering process is employed to filter out the low-confidence logits for avoiding the interference of low-confidence predictions while preserving the high-confidence logits to obtain the label sparsity. Then, the label co-occurrence pairs are counted for each sample based on its predicted categories and the label co-occurrence matrix constructed on the training set. Finally, the preserved logits are weighted by the label co-occurrence information and accumulated to produce the OOD detection score for each sample. Extensive experimental results on three well-known multi-label image datasets demonstrate the discriminating power of SLCS, which achieves greatly improved performances compared with the only multi-label OOD detection approach — JointEnergry and the state-of-the-art single-label OOD detection approaches. The performance improvements of SLCS over JointEnergy in FPR95 are 12.85%, 12.41%, and 9.50% on MS-COCO, VOC 2012, and NUS-WIDE datasets respectively.},
  archive      = {J_ICV},
  author       = {Lei Wang and Sheng Huang and Luwen Huangfu and Bo Liu and Xiaohong Zhang},
  doi          = {10.1016/j.imavis.2022.104548},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104548},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-label out-of-distribution detection via exploiting sparsity and co-occurrence of labels},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rotation-aware dynamic temporal consistency with spatial
sparsity correlation tracking. <em>ICV</em>, <em>126</em>, 104546. (<a
href="https://doi.org/10.1016/j.imavis.2022.104546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, discriminative correlation filter (DCF)-based trackers have been widely applied to visual tracking . However, a significant problem of DCF-based trackers is that the model uses the fixed patterns of temporal modeling and fails to suppress the distractive features. To obviate the issue, we propose the dynamic temporal consistency with spatial sparsity correlation filter. The dynamics refers to the adaptive temporal consistency hidden in the response maps and dynamic lasso constraint moderated by the prior knowledge. Unlike the classical temporal modeling method applied to filter model, we exploit the consistency of the response maps to perceive adaptive temporal continuity modeling to enable the filter to have self-regularized ability. Temporal modeling and spatial sparsity are incorporated in a unified optimization learning model and optimized together with ADMM algorithm and Sherman-Morrison formula. In particular, the tracking performance is hindered by the weak capacity to rotation variation. For the sake of estimating the rotation angle accurately to maintain rotation invariance, we explore the coarse-to-fine rotation estimation module. The coarse rotation is supported by the angle pool and the part-based tracker. By introducing the connected hyper ellipse fitting strategy to eliminate fake distractors to ensure a pure target region, the fine level attempts to achieve the optimal rotation angle with the minimum second order statistical bias. The design enables the filter to train with the recovered rotated image instead of axis-aligned bounding box , which attributes to alleviating the impact of ambiguous region and concentrating on the interested target. Extensive experimental results validate the superiority of the proposed method against other state-of-the art trackers and exhibit a remarkable generality in the rotated challenging scenarios.},
  archive      = {J_ICV},
  author       = {Mingxin Yu and Changlong Wang and Yuhua Zhang and Zhilong Lin},
  doi          = {10.1016/j.imavis.2022.104546},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104546},
  shortjournal = {Image Vis. Comput.},
  title        = {Rotation-aware dynamic temporal consistency with spatial sparsity correlation tracking},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bald eagle search optimization with deep transfer learning
enabled age-invariant face recognition model. <em>ICV</em>,
<em>126</em>, 104545. (<a
href="https://doi.org/10.1016/j.imavis.2022.104545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial aging variation is a challenging process in the design of face recognition system because of high intra-personal differences instigated by age progression. Age-invariant face recognition (AIFR) models find applicability in several real time applications such as criminal identification, missing person detection, and so on. The main issue is the high intra-personal disparities because of complicated and non-linear age progression process. An essential component of face recognition model is the extraction of important features from the facial images for reducing intrapersonal differences produced by illumination, expression, pose, age, etc. The recent advances of machine learning (ML) and deep learning (DL) models pave a way for effective design of AIFR models. In this view, this study presents a new Bald Eagle Search Optimization with Deep Transfer Learning Enabled AFIR (BESDTL-AIFR) model. The presented BESDTL-AIFR model primarily pre-processes the facial images to enhance the quality. Besides, the BESDTL-AIFR model utilizes Inception v3 model for learning high level deep features. Next, these features are passed into the optimal deep belief network (DBN) model for face recognition. Finally, the hyperparameters of the DBN model are optimally chosen by the use of BES algorithm. Experimentation analysis on challenging benchmark datasets pointed out the promising outcomes of the BESDTL-AIFR model compared to recent approaches.},
  archive      = {J_ICV},
  author       = {Shtwai Alsubai and Monia Hamdi and Sayed Abdel-Khalek and Abdullah Alqahtani and Adel Binbusayyis and Romany F. Mansour},
  doi          = {10.1016/j.imavis.2022.104545},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104545},
  shortjournal = {Image Vis. Comput.},
  title        = {Bald eagle search optimization with deep transfer learning enabled age-invariant face recognition model},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complementary characteristics fusion network for weakly
supervised salient object detection. <em>ICV</em>, <em>126</em>, 104536.
(<a href="https://doi.org/10.1016/j.imavis.2022.104536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is a challenging and fundamental research in computer vision and image processing . Since the cost of pixel-level annotations is high, scribble annotations are usually used as weak supervisions. However, scribble annotations are too sparse and always located inside the objects with lacking annotations close to the semantic boundaries, which can&#39;t make confident predictions. To alleviate these issues, we propose a novel and effective scribble-based weakly supervised approach for SOD, named complementary characteristics fusion network (CCFNet). To be more specific, we design an edge fusion module (EFM) by taking account of local and high-level semantic information to equip our model, which would be beneficial to enhance the power of aggregating edge information . Then to achieve the complementary role of different features, a series of feature correlation modules (FCMs) are employed to strengthen the localization information and details learning. This is based on low-level, high-level global and edge information, which will complement each other to obtain relatively complete salient regions . Alternatively, to encourage the network to learn structural information and further improve the results of saliency maps in foreground and background, we propose a self-supervised salient detection (SSD) loss. Extensive experiments using five benchmark datasets demonstrate that our proposed approach performs favorably against the state-of-the-art weakly supervised algorithms, and even surpasses the performance of those fully supervised.},
  archive      = {J_ICV},
  author       = {Yan Liu and Yunzhou Zhang and Zhenyu Wang and Fei Yang and Cao Qin and Feng Qiu and Sonya Coleman and Dermot Kerr},
  doi          = {10.1016/j.imavis.2022.104536},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104536},
  shortjournal = {Image Vis. Comput.},
  title        = {Complementary characteristics fusion network for weakly supervised salient object detection},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards generalized morphing attack detection by learning
residuals. <em>ICV</em>, <em>126</em>, 104535. (<a
href="https://doi.org/10.1016/j.imavis.2022.104535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition systems (FRS) are vulnerable to different kinds of attacks. Morphing attack combines multiple face images to obtain a single face image that can verify equally against all contributing subjects. Various Morphing Attack Detection (MAD) algorithms have been proposed in recent years albeit limited generalizability. We present a new approach for MAD in this work with better generalization than state-of-the-art (SOTA) algorithms. We propose an end-to-end multi-stage encoder-decoder network for learning the residuals of morphing process to detect attacks. Leveraging the residuals, we learn an efficient classifier using cross-entropy loss and asymmetric loss. The use of asymmetric loss in our approach is motivated by imbalanced distribution of morphs and bona fides. An extensive set of experiments are conducted on five different datasets consisting of two landmark based and three Generative Adversarial Network (GAN) based morphs in various settings such as digital, print-scan and print-scan-compression. We first demonstrate a near-ideal performance of the proposed MAD with Detection Equal Error Rate (D-EER) of 0% in the best case and 2.58% in the worst case in the digital domain in closed-set protocol, i.e., known attacks. Further, we demonstrate the applicability of the proposed approach on 60 different combinations where the testing set contains unknown morphing attacks in open-set protocol to illustrate the generalization ability of our proposed approach. Through training the proposed approach on landmark-based morph generation data alone, we obtain an EER of 3.59% in the best case and 12.89% in the worst case for morphed images in the digital domain, reducing the error rates from 45.67% and 30.23% respectively, in open-set protocol. We further present an extensive analysis of the proposed approach through Class Activation Maps (CAM) to explain the decisions using by making use of three complementary CAM analysis.},
  archive      = {J_ICV},
  author       = {Kiran Raja and Gourav Gupta and Sushma Venkatesh and Raghavendra Ramachandra and Christoph Busch},
  doi          = {10.1016/j.imavis.2022.104535},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104535},
  shortjournal = {Image Vis. Comput.},
  title        = {Towards generalized morphing attack detection by learning residuals},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AGA-GAN: Attribute guided attention generative adversarial
network with u-net for face hallucination. <em>ICV</em>, <em>126</em>,
104534. (<a href="https://doi.org/10.1016/j.imavis.2022.104534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of facial super-resolution methods relies on their ability to recover facial structures and salient features effectively. Even though the convolutional neural network and generative adversarial network-based methods deliver impressive performances on face hallucination tasks, the ability to use attributes associated with the low-resolution images to improve performance is unsatisfactory. In this paper, we propose an Attribute Guided Attention Generative Adversarial Network which employs novel attribute guided attention (AGA) modules to identify and focus the generation process on various facial features in the image. Stacking multiple AGA modules enables the recovery of both high and low-level facial structures. We design the discriminator to learn discriminative features by exploiting the relationship between the high-resolution image and their corresponding facial attribute annotations. We then explore the use of U-Net based architecture to refine existing predictions and synthesize further facial details. Extensive experiments across several metrics show that our AGA-GAN and AGA-GAN + U-Net framework outperforms several other cutting-edge face hallucination state-of-the-art methods. We also demonstrate the viability of our method when every attribute descriptor is not known and thus, establishing its application in real-world scenarios. Our code is available at https://github.com/NoviceMAn-prog/AGA-GAN .},
  archive      = {J_ICV},
  author       = {Abhishek Srivastava and Sukalpa Chanda and Umapada Pal},
  doi          = {10.1016/j.imavis.2022.104534},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104534},
  shortjournal = {Image Vis. Comput.},
  title        = {AGA-GAN: Attribute guided attention generative adversarial network with U-net for face hallucination},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A motion model based on recurrent neural networks for visual
object tracking. <em>ICV</em>, <em>126</em>, 104533. (<a
href="https://doi.org/10.1016/j.imavis.2022.104533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking algorithms typically leverage on either or both appearance and motion features of target(s). It is common in multi-object tracking to use both features, whereas the role of motion features in single-object trackers has less been explored. Based on the Long Short-Term Memory (LSTM) architecture of recurrent neural networks , we train a novel motion model to be incorporated into the off-the-shelf single-object trackers. The developed model predicts the target location in each frame based on the history of processed motion features in a few prior frames. This aids the tracking algorithm in dynamically updating the search region location, as apposed to static or probabilistic region settings. We incorporate the model into three state-of-the-art CNN-based trackers, namely GOTURN, SiamFC, and DiMP and illustrate the tracking performance enhancements on popular benchmarks. Significant improvements are achieved specially on the sequences rendering challenging situations such as Low Resolution, Out-of-Plane Rotation, Motion Blur , Fast Motion, and Occlusion. The motion model has a low computational cost and complies with the real-time execution of the base trackers.},
  archive      = {J_ICV},
  author       = {Mohammad Shahbazi and Mohammad Hosein Bayat and Bahram Tarvirdizadeh},
  doi          = {10.1016/j.imavis.2022.104533},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104533},
  shortjournal = {Image Vis. Comput.},
  title        = {A motion model based on recurrent neural networks for visual object tracking},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised moment localization with natural language
based on semantic reconstruction. <em>ICV</em>, <em>126</em>, 104532.
(<a href="https://doi.org/10.1016/j.imavis.2022.104532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of cross-modal moment localization is to find the temporal moment in the untrimmed video that semantically corresponds to the natural language query . The majority of current approaches learn the cross-modal moment localization models from fine-grained temporal annotations in the video, which are extremely time-consuming and labor-intensive to obtain. In this paper, we offer a novel framework for weakly supervised cross-modal moment localization that incorporates a proposal generation module and a semantic reconstruction module. The proposal generation module uses a two-dimensional temporal feature map to model cross-modal video representations and can encode the moment-by-moment temporal relationships of moment candidates. The semantic reconstruction module, which is based on the generated proposals, assesses a proposal&#39;s capacity to restore the text query and provides weak supervision for network training. Besides, a punishment loss is also proposed to further eliminate the effect of the invalid area. Extensive experimental results show that the proposed method achieves state-of-the-art performance, demonstrating its effectiveness for weakly supervised moment localization with natural language.},
  archive      = {J_ICV},
  author       = {Tingting Han and Kai Wang and Jun Yu and Jianping Fan},
  doi          = {10.1016/j.imavis.2022.104532},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104532},
  shortjournal = {Image Vis. Comput.},
  title        = {Weakly supervised moment localization with natural language based on semantic reconstruction},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rich global feature guided network for monocular depth
estimation. <em>ICV</em>, <em>125</em>, 104520. (<a
href="https://doi.org/10.1016/j.imavis.2022.104520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is a classical but challenging task in the field of computer vision . In recent years, Convolutional Neural Network (CNN) based models have been developed to estimate high-quality depth map from a single image. Most recently, some Transformer based models have led to great improvements. All the researchers are looking for a better way to handle the global processing of information which is crucial for depth relation inference but of high computational complexity . In this paper, we take advantage of both the Transformer and CNN then propose a novel network architecture , called Rich Global Feature Guided Network (RGFN), with which rich global features are extracted from both encoder and decoder. The framework of the RGFN is the typical encoder-decoder for dense prediction. A hierarchical transformer is implemented as the encoder to capture multi-scale contextual information and model long-range dependencies. In the decoder, the Large Kernel Convolution Attention (LKCA) is adopted to extract global features from different scales and guide the network to recover fine depth maps from low spatial resolution feature maps progressively. What&#39;s more, we apply the depth-specific data augmentation method, Vertical CutDepth, to boost the performance. Experimental results on both the indoor and outdoor datasets demonstrate the superiority of the RGFN compared to other state-of-the-art models. Compared with the most recent method AdaBins, RGFN improves the RMSE score by 4.66% on the KITTI dataset and 4.67% on the NYU Depth v2 dataset.},
  archive      = {J_ICV},
  author       = {Bingyuan Wu and Yongxiong Wang},
  doi          = {10.1016/j.imavis.2022.104520},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104520},
  shortjournal = {Image Vis. Comput.},
  title        = {Rich global feature guided network for monocular depth estimation},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Late feature supplement network for early action prediction.
<em>ICV</em>, <em>125</em>, 104519. (<a
href="https://doi.org/10.1016/j.imavis.2022.104519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early action prediction is a new hotspot in the field of computer vision . To improve the accuracy of early action prediction, a new end-to-end late feature supplement-based early action prediction network is proposed in this work. Different from the existing methods that use the model transfer strategy, a feature transfer strategy is defined in this work. Specifically, the features of the late clip are regarded as labels, and a feature transfer model is built to achieve mapping from the features of the early clip to the late features. After feature transfer, the generated late feature is fused into the early feature to form the final video feature. Finally, the final video feature is applied to action classification . The proposed method is evaluated on the action classification task of the early clip. The experimental results show that compared with the existing methods, the proposed method has better performance at different observation ratios. The ablation study verifies that the proposed feature transfer strategy can significantly improve the accuracy of early action prediction.},
  archive      = {J_ICV},
  author       = {Zhe Li and Hong-Bo Zhang and Miao-Hui Zhang and Qing Lei and Ji-Xiang Du},
  doi          = {10.1016/j.imavis.2022.104519},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104519},
  shortjournal = {Image Vis. Comput.},
  title        = {Late feature supplement network for early action prediction},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved YOLOv5 method for large objects detection with
multi-scale feature cross-layer fusion network. <em>ICV</em>,
<em>125</em>, 104518. (<a
href="https://doi.org/10.1016/j.imavis.2022.104518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SSD and YOLOv5 are the one-stage object detector representative algorithms. An improved one-stage object detector based on the YOLOv5 method is proposed in this paper, named Multi-scale Feature Cross-layer Fusion Network (M-FCFN). Firstly, we extract shallow features and deep features from the PANet structure for cross-layer fusion and obtain a feature scale different from 80 × 80, 40 × 40, and 20 × 20 as output. Then, according to the single shot multi-box detector, we propose the different scale features which are obtained by cross-layer fusion for dimension reduction and use it as another output for prediction. Therefore, two completely different feature scales are added as the output. Features of different scales are necessary for detecting objects of different sizes, which can increase the probability of object detection and significantly improve detection accuracy. Finally, aiming at the Autoanchor mechanism proposed by YOLOv5, we propose an EIOU k-means calculation. We have compared the four model structures of S , M , L , and X of YOLOv5 respectively. The problem of missed and false detections for large objects is improved which has better detection results. The experimental results show that our methods achieve 89.1% and 67.8% mAP @0.5 on the PASCAL VOC and MS COCO datasets. Compared with the YOLOv5_S, our methods improve by 4.4% and 1.4% mAP @ [0.5:0.95] on the PASCAL VOC and MS COCO datasets. Compared with the four models of YOLOv5, our methods have better detection accuracy for large objects. It should be more attention that our method on the large-scale mAP @ [0.5:0.95] is 5.4% higher than YOLOv5_S on the MS COCO datasets.},
  archive      = {J_ICV},
  author       = {Zhong Qu and Le-yuan Gao and Sheng-ye Wang and Hao-nan Yin and Tu-ming Yi},
  doi          = {10.1016/j.imavis.2022.104518},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104518},
  shortjournal = {Image Vis. Comput.},
  title        = {An improved YOLOv5 method for large objects detection with multi-scale feature cross-layer fusion network},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controllable face editing for video reconstruction in human
digital twins. <em>ICV</em>, <em>125</em>, 104517. (<a
href="https://doi.org/10.1016/j.imavis.2022.104517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High fidelity and controllable manipulation are critical to facial video reconstruction in human digital twins. Current generative adversarial networks (GANs) have achieved impressive performance in realistic face generation with high resolution, motivating several recent works to perform face editing via pretrained GANs. However, existing works suffer identity loss and semantic entanglement while editing real faces. To tackle these limitations, we propose a framework to perform controllable facial editing in video reconstruction. First, we propose to train a semantic inversion network to embed the target attribute change into the latent space of GANs. Disentangled semantic manipulation is performed during the semantic inversion by changing only the target attribute with the other unrelated attributes kept. Furthermore, we propose a novel personalized GAN inversion for the real face cropped from videos via retraining the generator of GANs, which can embed the real face into the latent space of GANs and preserve identity details for the real face. Finally, the realistic edited face is fused back into the original video. We use the identity preservation rate and disentanglement rate to evaluate the performance of our controllable face editing. Both qualitative and quantitative evaluations show that our method achieves prominent identity preservation and semantic disentanglement in controllable face editing, outperforming recent state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Chengde Lin and Shengwu Xiong},
  doi          = {10.1016/j.imavis.2022.104517},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104517},
  shortjournal = {Image Vis. Comput.},
  title        = {Controllable face editing for video reconstruction in human digital twins},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting semi-supervised face recognition with raw faces.
<em>ICV</em>, <em>125</em>, 104512. (<a
href="https://doi.org/10.1016/j.imavis.2022.104512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep facial recognition benefits significantly from large-scale training data ; however, the bottleneck of high labeling costs persists. Therefore, to reduce the labeling costs, it is desirable to train a model using limited labeled data and abundant unlabeled data ( i.e. , semi-supervised learning). However, existing semi-supervised learning methods present two primary challenges: (1) The possibility of identity overlaps between the unlabeled and labeled data. These overlaps can affect the correctness of pseudo-labels of the unlabeled set. (2) Different pseudo-labels generated by the clustering algorithm may belong to the same individual ( i.e. , over-decomposition problem). Thus, in this study, instead of experimenting with non-overlapping conditions, we apply smooth labels to exploit the potential of those samples that are similar to the identities in the labeled set. For samples that are not similar to the labeled set, we introduce a dual clustering strategy to remedy the over-decomposition problem caused by single clustering. With the upgraded semi-supervised framework, we recycle the discarded samples during purification of MS-Celeb-1 M (MS1M) to further scale up the training set, which offers a considerable performance boost of 94.39% on the IJB-C dataset.},
  archive      = {J_ICV},
  author       = {Yunze Chen and Junjie Huang and Zheng Zhu and Xianlei Long and Qingyi Gu},
  doi          = {10.1016/j.imavis.2022.104512},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104512},
  shortjournal = {Image Vis. Comput.},
  title        = {Boosting semi-supervised face recognition with raw faces},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Output-targeted baseline for neuron attribution calculation.
<em>ICV</em>, <em>124</em>, 104516. (<a
href="https://doi.org/10.1016/j.imavis.2022.104516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribution methods explaining a particular decision for a given convolutional neural network (CNN) have gained a lot of attention over the last few years. Among them, approximation methods of Shapley values are considered to be better ways of assigning attribution scores such that several desirable axioms are satisfied. Nevertheless, these attribution scores may still be misleading or inaccurate due to the inappropriate selection of a baseline which is necessary to apply Shapley values to CNNs. Previous baseline studies have focused on developing a generic baseline selection method for all approximation methods; however, we find that designing a baseline under the essence of the selected approximation method itself produces better results than generic ones. With this observation, we propose two primal baseline properties for Aumann–Shapley-based attributions and design a general objective function of generating a baseline iteratively by gradient descent . To increase efficiency, we further reduce the objective function into a quadratic optimization problem where the gradients only need to be calculated once. We show that our method produces better attribution results than several state-of-the-art baseline selections and attribution methods on both qualitative and quantitative experiments.},
  archive      = {J_ICV},
  author       = {Rui Shi and Tianxing Li and Yasushi Yamaguchi},
  doi          = {10.1016/j.imavis.2022.104516},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104516},
  shortjournal = {Image Vis. Comput.},
  title        = {Output-targeted baseline for neuron attribution calculation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Caption generation on scenes with seen and unseen object
categories. <em>ICV</em>, <em>124</em>, 104515. (<a
href="https://doi.org/10.1016/j.imavis.2022.104515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image caption generation is one of the most challenging problems at the intersection of vision and language domains. In this work, we propose a realistic captioning task where the input scenes may incorporate visual objects with no corresponding visual or textual training examples. For this problem, we propose a detection-driven approach that consists of a single-stage generalized zero-shot detection model to recognize and localize instances of both seen and unseen classes, and a template-based captioning model that transforms detections into sentences. To improve the generalized zero-shot detection model, which provides essential information for captioning, we define effective class representations in terms of class-to-class semantic similarities , and leverage their special structure to construct an effective unseen/seen class confidence score calibration mechanism. We also propose a novel evaluation metric that provides additional insights for the captioning outputs by separately measuring the visual and non-visual contents of generated sentences. Our experiments highlight the importance of studying captioning in the proposed zero-shot setting, and verify the effectiveness of the proposed detection-driven zero-shot captioning approach.},
  archive      = {J_ICV},
  author       = {Berkan Demirel and Ramazan Gokberk Cinbis},
  doi          = {10.1016/j.imavis.2022.104515},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104515},
  shortjournal = {Image Vis. Comput.},
  title        = {Caption generation on scenes with seen and unseen object categories},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tackling multiple object tracking with complicated motions —
re-designing the integration of motion and appearance. <em>ICV</em>,
<em>124</em>, 104514. (<a
href="https://doi.org/10.1016/j.imavis.2022.104514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although numerous data association methods have been proposed for Multiple Object Tracking (MOT), how to integrate different features in the data association remains an open problem. For instance, over-relying on the motion feature may fail to do the necessary data association when object movements are complicated, while only depending on the appearance feature may lead to incorrect association results when intra-frame objects have similar appearances. To make an improved trade-off between the appearance feature and motion feature, we re-designed the integration of motion and appearance. In our online approach, the location and motion of each object are cast to adaptive searching windows, and within searching windows, matching is only related to the similarity of appearance features. In our offline approach, tracklets generated from our online approach are refined by forming the motion feature as spatiotemporal constraints and utilizing the appearance for clustering. We conduct experiments on multiple MOT datasets from diverse perspectives, including variant motion speed, illumination condition , object categories, etc. , and verify that our method can reach robust performance. Moreover, this method also demonstrates its effectiveness by further improving our previous 1 st place solutions in two CVPR 2020 MOT challenges.},
  archive      = {J_ICV},
  author       = {Fan Yang and Zheng Wang and Yang Wu and Sakriani Sakti and Satoshi Nakamura},
  doi          = {10.1016/j.imavis.2022.104514},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104514},
  shortjournal = {Image Vis. Comput.},
  title        = {Tackling multiple object tracking with complicated motions — re-designing the integration of motion and appearance},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distinguishing foreground and background alignment for
unsupervised domain adaptative semantic segmentation. <em>ICV</em>,
<em>124</em>, 104513. (<a
href="https://doi.org/10.1016/j.imavis.2022.104513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive semantic segmentation uses the knowledge learned from the labeled source domain dataset to guide the segmentation of the target domain. However, this domain migration method will cause a large inter-domain difference due to the different feature distributions between the source domain and the target domain. We use the self-supervised learning method to generate pseudo labels for the target domain, so that the corresponding pixels are directly aligned with the source domain according to the segmentation loss. Through observation, it is found that the spatial distribution of the background class in the source domain and the target domain has a small difference, and the appearance of the same class of the foreground class will also be quite different. We use the method of distinguishing alignment between foreground and background classes. We understand that acquiring the rich space and channel information on the feature map during the convolution process is essential for fine-grained semantic segmentation. Therefore, in order to obtain the dependency relationship between the channels of the feature map and the spatial position information , we use a channel and spatial parallel attention module. This module enables the network to select and amplify valuable space and channel information from the global information and suppress useless information. In addition, we introduce focal loss to solve the problem of class imbalance in the data set. Experiments show that our method achieves better segmentation performance in unsupervised domain adaptive semantic segmentation.},
  archive      = {J_ICV},
  author       = {Jia Zhang and Wei Li and Zhixin Li},
  doi          = {10.1016/j.imavis.2022.104513},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104513},
  shortjournal = {Image Vis. Comput.},
  title        = {Distinguishing foreground and background alignment for unsupervised domain adaptative semantic segmentation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit neural refinement based multi-view stereo network
with adaptive correlation. <em>ICV</em>, <em>124</em>, 104511. (<a
href="https://doi.org/10.1016/j.imavis.2022.104511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose ACINR-MVSNet, an end-to-end trainable framework with adaptive group-wise correlation and implicit neural depth refinement for multi-view stereo (MVS). Previous learning-based MVS methods have demonstrated their outstanding performance, and most of them estimate depth maps in a coarse-to-fine manner. However, in a commonly used multi-stage cascaded framework, the previous wrong estimation might lead to error propagation . In contrast, we focus on another coarse-to-fine structure, i.e., one-stage MVS architecture followed by refinement modules. Inspired by implicit neural representation , we propose an implicit neural refinement module to refine the coarse depth map. Guided by the corresponding reference image , it can better recover finer details, especially those in boundary areas. To solve the visibility problem in complex scenarios while maintaining efficiency, we propose an adaptive group-wise correlation similarity measure for cost volume construction. Besides, we present a pyramid-based feature extraction network with a repeated top-down and bottom-up structure to gather more context-aware information, which can better meet the challenges in ill-posed regions. This novel feature extractor is also utilized to construct an enhanced Gauss-Newton refinement module for further upsampling and optimizing. Extensive experiments on the DTU, the Tanks &amp; Temples and the BlendedMVS datasets demonstrate the effectiveness and generalization of our approach, which can achieve better or competitive results compared to state-of-the-art methods. The code will be available at https://github.com/BoyangSONG/ACINR-MVSNet .},
  archive      = {J_ICV},
  author       = {Boyang Song and Xiaoguang Hu and Jin Xiao and Guofeng Zhang and Tianyou Chen},
  doi          = {10.1016/j.imavis.2022.104511},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104511},
  shortjournal = {Image Vis. Comput.},
  title        = {Implicit neural refinement based multi-view stereo network with adaptive correlation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight multi-scale convolutional neural network for
real time stereo matching. <em>ICV</em>, <em>124</em>, 104510. (<a
href="https://doi.org/10.1016/j.imavis.2022.104510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to accurately estimate disparities in textureless and slim regions, spatial pyramid pooling and stacked 3D CNN , which can capture global context information, are widely used in state-of-the-art stereo matching algorithms. Unfortunately, the computational complexity and high memory consumption make these methods not friendly to real-time applications such as autonomous driving and augmented realities . In order to balance the real-time performance and accuracy, we design lightweight multi-scale convolutional neural network for real-time stereo matching . First, Lightweight multi-scale 2D and 3D CNN modules are proposed for feature extraction and initial disparity computation respectively. Both of above modules only run on a low resolution to further reduce the amount of calculation. Second, multi-scale RGB images guided network is utilized to refine the final disparity estimation . Experiments on several datasets show that the proposed algorithm can achieve competitive results with speed of 64fps on a NIVDIA 1080 GPU .},
  archive      = {J_ICV},
  author       = {Yanbing Xue and Doudou Zhang and Leida Li and Shiyin Li and Yuxin Wang},
  doi          = {10.1016/j.imavis.2022.104510},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104510},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight multi-scale convolutional neural network for real time stereo matching},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved real-time three-dimensional stereo matching with
local consistency. <em>ICV</em>, <em>124</em>, 104509. (<a
href="https://doi.org/10.1016/j.imavis.2022.104509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing stereo matching algorithms are unable to meet the real-time and high-performance dual requirements in practical applications. To address this problem, a novel stereo network is proposed, which utilizes the prior of local disparity consistency to improve the performance of real-time disparity estimation . Based on the initial disparity estimation by the light-weight pyramid matching network , novel spatial consistency refinement (SCR) module and time consistency refinement (TCR) module are designed for further disparity refinement. SCR module propagates the neighborhood high-confidence predictions of sparse sampling to unreliable regions for disparity refinement. A single-layer Dynamic Local Filter (DLF) is designed to realize the content-adaptive propagation, which effectively improves the disparity quality without significantly increasing the burden of computation and memory. For real-time disparity estimation of consecutive frames, novel TCR module is further proposed to refine the disparity estimation based on the time local consistency of disparity. The proposed method is evaluated on the Scene Flow and KITTI 2015 datasets with comprehensive experiments. Experimental results demonstrated that our method can achieve high-accuracy disparity estimation and real-time running speed of over 40 FPS, which significantly outperforms the compared networks with similar runtimes.},
  archive      = {J_ICV},
  author       = {Xiaoqian Ye and Binbin Yan and Boyang Liu and Huachun Wang and Shuai Qi and Duo Chen and Peng Wang and Kuiru Wang and Xinzhu Sang},
  doi          = {10.1016/j.imavis.2022.104509},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104509},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved real-time three-dimensional stereo matching with local consistency},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate and efficient salient object detection via position
prior attention. <em>ICV</em>, <em>124</em>, 104508. (<a
href="https://doi.org/10.1016/j.imavis.2022.104508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The excessive pursuit of accuracy has resulted in complex and huge structures of most existing salient object detection (SOD) models, and the meticulously designed lightweight SOD models cannot accurately detect salient objects. To improve the practicality of SOD, we design a novel position prior attention network (PPANet) for fast and accurate salient object detection in this paper. In detail, we propose a position prior attention module (PPAM), which first assigns different weights to positions based on the prior that objects near the image center are more attractive to people, and then perceives object context information through different receptive fields. In addition, we propose a context fusion module (CFM) to prevent the coarse resolution of high-level features from diluting the salient object boundaries during fusion. We present two PPANet versions: a heavyweight PPANet-R aimed at high accuracy SOD and a lightweight PPANet-M that achieves a good balance between accuracy and efficiency. Besides, we construct a structural polishing loss that gives more attention to object boundary and solves the problem of sample imbalance. Experimental results on 5 popular benchmark datasets demonstrate that the proposed PPANet-R outperforms existing SOD models, and PPANet-M achieves accuracy comparable to the state-of-the-art heavyweight SOD methods with 150 FPS real-time detection speed.},
  archive      = {J_ICV},
  author       = {Jin Zhang and Qiuwei Liang and Yanjiao Shi},
  doi          = {10.1016/j.imavis.2022.104508},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104508},
  shortjournal = {Image Vis. Comput.},
  title        = {Accurate and efficient salient object detection via position prior attention},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained bidirectional attentional generation and
knowledge-assisted networks for cross-modal retrieval. <em>ICV</em>,
<em>124</em>, 104507. (<a
href="https://doi.org/10.1016/j.imavis.2022.104507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, most existing cross-modal retrieval methods only consider global or local semantic embeddings , lacking fine-grained dependencies between objects. At the same time, it is usually ignored that the mutual transformation between modalities also facilitates the embedding of modalities. Given these problems, we propose a method called BiKA (Bidirectional Knowledge-assisted embedding and Attention-based generation). The model uses a bidirectional graph convolutional neural network to establish dependencies between objects. In addition, it employs a bidirectional attention-based generative network to achieve the mutual transformation between modalities. Specifically, the knowledge graph is used for local matching to constrain the local expression of the modalities, in which the generative network is used for mutual transformation to constrain the global expression of the modalities. In addition, we also propose a new position relation embedding network to embed position relation information between objects. The experiments on two public datasets show that the performance of our method has been dramatically improved compared to many state-of-the-art models.},
  archive      = {J_ICV},
  author       = {Jianwei Zhu and Zhixin Li and Jiahui Wei and Yufei Zeng and Huifang Ma},
  doi          = {10.1016/j.imavis.2022.104507},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104507},
  shortjournal = {Image Vis. Comput.},
  title        = {Fine-grained bidirectional attentional generation and knowledge-assisted networks for cross-modal retrieval},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Twin relaxed least squares regression with classwise mean
constraint for image classification. <em>ICV</em>, <em>124</em>, 104506.
(<a href="https://doi.org/10.1016/j.imavis.2022.104506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a twin relaxed least squares regression (TRLSR) framework with classwise mean constraint for image classification . The primary objective of TRLSR is to learn discriminative projections with enhanced interclass margins while preserving the intrinsic structure of the data. To this end, we introduce a relaxed regression target matrix together with a twin matrix to allow greater flexibility in learning the projections compared to using the conventional strict binary label matrix. In addition, a classwise mean constraint is introduced to retain the intraclass similarity of the data, which is beneficial in learning more discriminative projections. An ℓ ℓ 2,1 -norm based regularization on the optimized projections is incorporated to extract more significant features while limiting the impact of noise and overfitting. The performance of the proposed technique on several public data sets for face recognition, object classification, action recognition and scene classification applications is demonstrated. The proposed method is shown to outperform the state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Meenakshi and Seshan Srirangarajan},
  doi          = {10.1016/j.imavis.2022.104506},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104506},
  shortjournal = {Image Vis. Comput.},
  title        = {Twin relaxed least squares regression with classwise mean constraint for image classification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling content-attribute preference for personalized image
esthetics assessment. <em>ICV</em>, <em>124</em>, 104505. (<a
href="https://doi.org/10.1016/j.imavis.2022.104505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized image esthetics assessment (PIAA) models user&#39;s personalized esthetic preference and predicts a unique esthetic score for the user, which has become a hot topic due to its usefulness in social media, album curation, etc. People&#39;s esthetic preference is typically determined by a mixture of diversified factors, which makes it extremely challenging to model. Intuitively, people with different personalities have distinct preferences for various photographic contents, while image content and esthetic attributes are always tightly coupled. Motivated by this, this paper presents a content-attribute preference (CAP) framework for PIAA, which models the intricate relationship between user personality and image “content-attribute” coupling features. Specifically, a multi-task learning network is utilized to extract “content-attribute” coupling features, and another parallel network is used to extract personality features. Then, a personalized esthetic prior model is trained to represent the content-attribute preferences of people with different personalities based on the cross attention mechanism . Finally, a PIAA model is obtained by fine-tuning the esthetic prior model using target user data. Extensive experiments on three public databases demonstrate that the proposed CAP model outperforms the state-of-the-arts in terms of both prediction performance and generalization ability .},
  archive      = {J_ICV},
  author       = {Yuanyang Wang and Yihua Huang and Xiumin Chen and Leida Li and Guangming Shi},
  doi          = {10.1016/j.imavis.2022.104505},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104505},
  shortjournal = {Image Vis. Comput.},
  title        = {Modeling content-attribute preference for personalized image esthetics assessment},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution regularized self-supervised learning for domain
adaptation of semantic segmentation. <em>ICV</em>, <em>124</em>, 104504.
(<a href="https://doi.org/10.1016/j.imavis.2022.104504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel pixel-level distribution regularization scheme (DRSL) for self-supervised domain adaptation of semantic segmentation . In a typical setting, the classification loss forces the semantic segmentation model to greedily learn the representations that capture inter-class variations in order to determine the decision (class) boundary. Due to the domain-shift, this decision boundary is unaligned in the target domain, resulting in noisy pseudo labels adversely affecting self-supervised domain adaptation. To overcome this limitation, along with capturing inter-class variation, we capture pixel-level intra-class variations through class-aware multi-modal distribution learning (MMDL). Thus, the information necessary for capturing the intra-class variations is explicitly disentangled from the information necessary for inter-class discrimination. Features captured thus are much more informative, resulting in pseudo-labels with low noise. This disentanglement allows us to perform separate alignments in discriminative space and multi-modal distribution space, using cross-entropy based self-learning for former. For later, we propose novel stochastic mode alignment method, by explicitly decreasing the distance between the target and source pixels that map to the same mode. The distance metric learning loss, computed over pseudo-labels and backpropagated from multi-modal modeling head, acts as the regularizer over the base network shared with the segmentation head. The results from comprehensive experiments on synthetic to real domain adaptation setups, i.e., GTA-V/SYNTHIA to Cityscapes, show that DRSL outperforms many existing approaches (a minimum margin of 2.3% and 2.5% in mIoU for SYNTHIA to Cityscapes).},
  archive      = {J_ICV},
  author       = {Javed Iqbal and Hamza Rawal and Rehan Hafiz and Yu-Tseh Chi and Mohsen Ali},
  doi          = {10.1016/j.imavis.2022.104504},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104504},
  shortjournal = {Image Vis. Comput.},
  title        = {Distribution regularized self-supervised learning for domain adaptation of semantic segmentation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object aspect classification and 6DoF pose estimation.
<em>ICV</em>, <em>124</em>, 104495. (<a
href="https://doi.org/10.1016/j.imavis.2022.104495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to the problem of estimating a given object&#39;s 6DoF pose from a single RGB image. Recent works focus on a multi-stage approach, which first detects key-points followed by perspective -n-point pose estimation algorithm and a pose refinement procedure. We show that adding a classifier block estimating the predefined aspects of the objects improves the multi-stage process. This is due to the fact that the additional classifier acts as a constraint simplifying the required neural network and at the same time yielding better key-point selection. We reduce the search space for the key-point selection and exclude false-positives by mapping the appearance of an object to an aspect. The simplified neural network allows faster inference and a smaller footprint. Our experiments show that our hypothesis performs similar to the state-of-the-art on three different datasets. We also show that an off-the-shelf refinement process can further improve our results to surpass state-of-the-art on several objects. Another advantage is, the proposed pipeline can run efficiently on real-time due to the smaller neural network backbone used. The code to replicate this research will be publicly available at https://github.com/greymad/6DoFPoseAspects},
  archive      = {J_ICV},
  author       = {Muhammet Ali Dede and Yakup Genc},
  doi          = {10.1016/j.imavis.2022.104495},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104495},
  shortjournal = {Image Vis. Comput.},
  title        = {Object aspect classification and 6DoF pose estimation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspherically regularized networks for self-supervision.
<em>ICV</em>, <em>124</em>, 104494. (<a
href="https://doi.org/10.1016/j.imavis.2022.104494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrap Your Own Latent (BYOL) introduced an approach to self-supervised learning avoiding the contrastive paradigm and subsequently removing the computational burden of negative sampling associated with such methods. However, we empirically find that the image representations produced under the BYOL&#39;s self-distillation paradigm are poorly distributed in representation space compared to contrastive methods. This work empirically demonstrates that feature diversity enforced by contrastive losses is beneficial to image representation uniformity when employed in BYOL, and as such, provides greater inter-class representation separability. Additionally, we explore and advocate the use of regularization methods , specifically the layer-wise minimization of hyperspherical energy (i.e. maximization of entropy) of network weights to encourage representation uniformity. We show that directly optimizing a measure of uniformity alongside the standard loss, or regularizing the networks of the BYOL architecture to minimize the hyperspherical energy of neurons can produce more uniformly distributed and therefore better performing representations for downstream tasks.},
  archive      = {J_ICV},
  author       = {Aiden Durrant and Georgios Leontidis},
  doi          = {10.1016/j.imavis.2022.104494},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104494},
  shortjournal = {Image Vis. Comput.},
  title        = {Hyperspherically regularized networks for self-supervision},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). H-net: Unsupervised domain adaptation person
re-identification network based on hierarchy. <em>ICV</em>,
<em>124</em>, 104493. (<a
href="https://doi.org/10.1016/j.imavis.2022.104493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of manual labeling for supervised person re-identification (re-ID), unsupervised domain adaptation (UDA) person re-ID has been attracting the attention of many scholars. In this research, target domain datasets and source domain datasets are two indispensable datasets, and although there are many different pictures of the same person in the target domain, these pictures are precious to the network in different degrees. However, the existing UDA person re-ID algorithms does not treat different samples in the target domain differently, they just treat positive samples as indistinguishable samples. Not only that, although the triplet loss has been re-identified by unsupervised person re-ID, the noise of the hardest sample hasn&#39;t been carried out well. In this paper, a novel and robust network model named unsupervised domain adaptation hierarchical person re-identification network (H-Net) is proposed, which not only effectively reduces the impact of inaccurate identification of the hardest sample but also treats different positive samples differently by hierarchical feature collection. Numerous experimental results on Market-1501 and DukeMTMC-reID demonstrate that the proposed H-Net outperforms the existing methods and can significantly improve the accuracy of person re-ID.},
  archive      = {J_ICV},
  author       = {Deqiang Cheng and Jiahan Li and Qiqi Kou and Kai Zhao and Ruihang Liu},
  doi          = {10.1016/j.imavis.2022.104493},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104493},
  shortjournal = {Image Vis. Comput.},
  title        = {H-net: Unsupervised domain adaptation person re-identification network based on hierarchy},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential SfM and image correction for a rolling shutter
stereo rig. <em>ICV</em>, <em>124</em>, 104492. (<a
href="https://doi.org/10.1016/j.imavis.2022.104492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern consumer-grade cameras are equipped with an electronic rolling shutter (RS), leading to image distortions when the camera moves during image acquisition. We explore the first structure and motion estimation problem of a dynamic generalized RS stereo camera. Such a general configuration is commonplace in robots and autonomous driving applications. We propose a tractable RS stereo differential structure from motion (SfM) algorithm, taking into account the RS effect during consecutive imaging, which effectively compensates for the RS-stereo image distortion by a linear scaling operation on each optical flow. We further propose embedding the cheirality into RANSAC and develop a robust RS-stereo-aware full-motion estimation framework. We demonstrate that the RS stereo motion and depth map refined by our non-linear optimization schemes within the maximum likelihood criterion can be used for image correction to recover high-quality global shutter (GS) stereo images. Moreover, using the proposed generalized RS stereo differential SfM pipeline, the corrected images produce an accurate 3D scene structure as the ground-truth structure. Extensive experiments on both synthetic and real RS stereo data demonstrate the effectiveness of our model and method in various configurations.},
  archive      = {J_ICV},
  author       = {Bin Fan and Yuchao Dai and Zhiyuan Zhang and Ke Wang},
  doi          = {10.1016/j.imavis.2022.104492},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104492},
  shortjournal = {Image Vis. Comput.},
  title        = {Differential SfM and image correction for a rolling shutter stereo rig},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot personalized saliency prediction using
meta-learning. <em>ICV</em>, <em>124</em>, 104491. (<a
href="https://doi.org/10.1016/j.imavis.2022.104491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized saliency maps (PSMs) reflect the gaze patterns of different subjects. Current works of saliency prediction explore the common trend in fixation distribution across all observers as a task. Personalized saliency prediction takes the personal preference of each individual into account. In other words, it considers each subject as a task. Due to the difficulty of obtaining individual labeled data, limited works are focusing on personalized saliency prediction. Our goal is to train a model that can quickly adapt to a new subject by using a few labeled data from the new subject. This paper proposes a meta-learning-based method to solve the few-shot personalized saliency prediction problem and predict better saliency maps of different subjects. Our method learns model parameters to fast adapt to new subjects. In addition, we design a Hard Samples Selection (HSS) strategy to make the training process more effective. Specifically, due to the adaptability of the model to different subjects is reflected in the value of the loss function during training, we regard subjects with high loss function values as hard samples. Then we can select hard samples online and retrain the model based on them to improve the saliency prediction performance. Experimental results show that our proposed method is better than existing methods on the PSM dataset for personalized saliency prediction.},
  archive      = {J_ICV},
  author       = {Xinhui Luo and Zhi Liu and Weijie Wei and Linwei Ye and Tianhong Zhang and Lihua Xu and Jijun Wang},
  doi          = {10.1016/j.imavis.2022.104491},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104491},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-shot personalized saliency prediction using meta-learning},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dense open-set recognition based on training with noisy
negative images. <em>ICV</em>, <em>124</em>, 104490. (<a
href="https://doi.org/10.1016/j.imavis.2022.104490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional models often produce inadequate predictions for inputs which are foreign to the training distribution. Consequently, the problem of detecting outlier images has recently been receiving a lot of attention. Unlike most previous work, we address this problem in the dense prediction context. Our approach is based on two reasonable assumptions. First, we assume that the inlier dataset is related to some narrow application field (e.g. road driving). Second, we assume that there exists a general-purpose dataset which is much more diverse than the inlier dataset (e.g. ImageNet-1 k). We consider pixels from the general-purpose dataset as noisy negative samples since most (but not all) of them are outliers. We encourage the model to recognize borders between the known and the unknown by pasting jittered negative patches over inlier training images. Our experiments target two dense open-set recognition benchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition dataset (StreetHazard). Extensive performance evaluation indicates competitive potential of the proposed approach.},
  archive      = {J_ICV},
  author       = {Petra Bevandić and Ivan Krešo and Marin Oršić and Siniša Šegvić},
  doi          = {10.1016/j.imavis.2022.104490},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104490},
  shortjournal = {Image Vis. Comput.},
  title        = {Dense open-set recognition based on training with noisy negative images},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot unsupervised image-to-image translation via
exploiting semantic attributes. <em>ICV</em>, <em>124</em>, 104489. (<a
href="https://doi.org/10.1016/j.imavis.2022.104489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown remarkable success in unsupervised image-to-image translation. However, if there is no access to enough images in target classes, learning a mapping from source classes to the target classes always suffers from mode collapse, especially the zero shot case, which limits the application of the existing methods. In this work, we propose a zero-shot unsupervised image-to-image translation framework to address this limitation, by effectively associating categories with their side information like attributes. To generalize the translator to previously unseen classes, we introduce two strategies for exploiting the semantic attribute space. First, we propose to preserve semantic relations to the visual space for effective guidance on where to map the input image. Second, expanding attribute space is introduced by utilizing attribute vectors of unseen classes, which alleviates the mapping bias for unseen classes. Both of these strategies encourage the translator to explore the modes of unseen classes. Quantitative and qualitative results on different datasets validate the effectiveness of our proposed approach. Moreover, we demonstrate that our framework can be applied to fashion design task. The code is available at https://github.com/cyq373/ZUNIT .},
  archive      = {J_ICV},
  author       = {Yuanqi Chen and Xiaoming Yu and Shan Liu and Wei Gao and Ge Li},
  doi          = {10.1016/j.imavis.2022.104489},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104489},
  shortjournal = {Image Vis. Comput.},
  title        = {Zero-shot unsupervised image-to-image translation via exploiting semantic attributes},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Texture classification-based feature processing for
violence-based anomaly detection in crowded environments. <em>ICV</em>,
<em>124</em>, 104488. (<a
href="https://doi.org/10.1016/j.imavis.2022.104488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection from video surveillance inputs helps to improve security in crowded places and outdoors. The captured image is analyzed to identify human faces, objects, and abnormal events through computer-aided analytics. This article proposes a Texture-Classification-based Feature Processing (TCFP) technique for distinguishing anomalies in captured video inputs. The anomalies are identified as events from the sequence frames wherein the dynamic inputs are distinguished using their features. Deep learning is employed for temporal training features based on frame characteristics in this distinguishing process. The input frame is segregated using textural boundaries separated using non-dimensional features. The learning process trains dimensional and non-dimensional features for identifying anomalies and maximizing detection accuracy. The textural boundaries are defined using the non-dimensional vectors present in the frame series in the different face classifications. Therefore, the errors are confined within selective boundaries without impacting the preceding feature. This improves the F1score with less processing time.},
  archive      = {J_ICV},
  author       = {Abdallah A. Mohamed and Fayez Alqahtani and Ahmed Shalaby and Amr Tolba},
  doi          = {10.1016/j.imavis.2022.104488},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104488},
  shortjournal = {Image Vis. Comput.},
  title        = {Texture classification-based feature processing for violence-based anomaly detection in crowded environments},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monocular depth estimation with spatially coherent sliced
network. <em>ICV</em>, <em>124</em>, 104487. (<a
href="https://doi.org/10.1016/j.imavis.2022.104487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation from a single still image is one of the most challenging fundamental problems in scenes understanding. As a pixel level regression problem, the inherent continuous large range of depth itself causes three main difficulties: (1) The unbalance between large value and small value during regressing; (2) Exploiting multi-scale contextual information; (3) Preserving spatial and semantic structures. To overcome these difficulties, this paper presents a novel spatially coherent sliced network for monocular depth estimation (SCS-Net). It first uses feature pyramids network to form the feature fusions of hierarchical feature maps. Then, the depth is sliced to supervise the estimation in different ranges generated from the feature fusions of multi-scale contexts. The holistic depth is invoked as the supervised signal of aggregated feature maps to ensure the learning of global structure of the scene. The self-spatial-attention mechanism further takes advantage of the semantics of objects and the spatial pixel relations to maintain the coherence of space and semantics in depth sliced estimation. Finally, the regulations of second-order depth information further make the estimated boundaries not too smooth. Numerous ablation experiments and comparisons on three popular indoor and outdoor benchmark datasets indicate the effectiveness and robustness of the proposed approach.},
  archive      = {J_ICV},
  author       = {Wen Su and Haifeng Zhang and Yuan Su and Jun Yu and Zengfu Wang},
  doi          = {10.1016/j.imavis.2022.104487},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104487},
  shortjournal = {Image Vis. Comput.},
  title        = {Monocular depth estimation with spatially coherent sliced network},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LocalFace: Learning significant local features for deep face
recognition. <em>ICV</em>, <em>123</em>, 104484. (<a
href="https://doi.org/10.1016/j.imavis.2022.104484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely mentioned topic in face recognition, the margin-based loss function enhances the discriminability of face recognition models by applying margin between class decision boundaries. However, there is still room to improve the representation of face features. Local face feature extraction has been employed in traditional face recognition methods , but with the increase of network depth in deep learning , the traditional approach requires a large number of computational resources . In this paper, we propose a novel face recognition architecture called LocalFace to extract local face features. First, by analyzing the distribution of significant features in face images, we propose an efficient face fixed-point local feature extraction approach and improve this method to propose a more effective face dynamic local feature extraction scheme. Subsequently, we propose a block-based random occlusion method for the limitations of the random face occlusion method to better simulate the occlusion situation in real scenes. In the end, we present a detailed discussion on the channel attention method that is more appropriate for face recognition and classification tasks . Our method enhances the representation of face features by ensembling local features into global features without extra parameters, which is efficient and easy to implement. Extensive experiments on various benchmarks demonstrate the superiority of our LocalFace, and part of the experimental results achieve SOTA results.},
  archive      = {J_ICV},
  author       = {Xiao Ke and BingHui Lin and WenZhong Guo},
  doi          = {10.1016/j.imavis.2022.104484},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104484},
  shortjournal = {Image Vis. Comput.},
  title        = {LocalFace: Learning significant local features for deep face recognition},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MEmoR: A multimodal emotion recognition using affective
biomarkers for smart prediction of emotional health for people analytics
in smart industries. <em>ICV</em>, <em>123</em>, 104483. (<a
href="https://doi.org/10.1016/j.imavis.2022.104483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intersection of people, data and intelligent machines has a far-reaching impact on the productivity, efficiency and operations of a smart industry. Internet-of-things (IoT) offers a great potential for workplace gains using the “quantified self” and the computer vision strategies. Their goal is to focus on productivity, fitness, wellness, and improvement of the work environment . Recognizing and regulating human emotion is vital to people analytics as it plays an important role in workplace productivity. Within the smart industry setting, various non-invasive IoT devices can be used to recognize emotions and study the behavioral outcomes in various situations. This research puts forward a deep learning model for detection of human emotional state in real-time using multimodal data from the Emotional Internet-of-things (E-IoT). The proposed multimodal emotion recognition model, MEmoR makes use of two data modalities: visual and psychophysiological. The video signals are sampled to obtain image frames and a ResNet50 model pre-trained for face recognition is fine-tuned for emotion classification. Simultaneously, CNN is trained on the psychophysiological signals and the results of the two modality networks are combined using decision-level weighted fusion. The model is tested on the benchmark Bio Vid Emo DB multimodal dataset and compared to the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Akshi Kumar and Kapil Sharma and Aditi Sharma},
  doi          = {10.1016/j.imavis.2022.104483},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104483},
  shortjournal = {Image Vis. Comput.},
  title        = {MEmoR: A multimodal emotion recognition using affective biomarkers for smart prediction of emotional health for people analytics in smart industries},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradual adaption with memory mechanism for image-based 3D
model retrieval. <em>ICV</em>, <em>123</em>, 104482. (<a
href="https://doi.org/10.1016/j.imavis.2022.104482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of 3D modeling technology and its wide application in different fields, the number of 3D models increases rapidly, making 3D model retrieval a hot topic in current research. Compared with other 3D model retrieval methods , 2D image-based unsupervised 3D model retrieval takes the 2D images which have rich labels and are easy to obtain as the queries, and also takes into account the difficulties of labeling 3D models. 2D image-based unsupervised 3D model retrieval is a retrieval task involving cross-domain adaptation problem, which main challenge is the excessive domain gap. In this paper, we propose a cross-domain 3D model retrieval method of memory mechanism based on disentangled feature learning . The disentangled feature learning enables to disentangle the twisted original features into the isolated domain-invariant features and domain-specific features, where the former is to be aligned to narrow the domain gap. On this basis, the memory mechanism selects feature vectors from class memory modules constructed by class representative features of the opposite domain for every sample, which are used to update the domain-invariant features with gradient weight. The memory mechanism can gradually improve the adaptability of the model to the very different two domains. Experiments are conducted on the public datasets MI3DOR and MI3DOR-2 to verify the feasibility and the superiority of the proposed method. Especially on MI3DOR-2 dataset, our method outperforms the current state-of-the-art methods with gains of 7.71% for the strictest retrieval metric NN.},
  archive      = {J_ICV},
  author       = {Dan Song and Yuting Ling and Tianbao Li and Ting Zhang and Guoqing Jin and Junbo Guo and Xuanya Li},
  doi          = {10.1016/j.imavis.2022.104482},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104482},
  shortjournal = {Image Vis. Comput.},
  title        = {Gradual adaption with memory mechanism for image-based 3D model retrieval},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STCA: Utilizing a spatio-temporal cross-attention network
for enhancing video person re-identification. <em>ICV</em>,
<em>123</em>, 104474. (<a
href="https://doi.org/10.1016/j.imavis.2022.104474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based re-identification (ReID) is a crucial task in computer vision that draws increasing attention due to advances in deep learning (DL) and modern computational devices. Despite recent success with CNN architectures, single models (e.g., 2D-CNNs or 3D-CNNs) alone failed to leverage temporal information with spatial cues. This is due to uncontrolled surveillance scenarios and variable poses leading to inevitable misalignment of ROIs across the tracklets, which is accompanied by occlusion and motion blur. In this context, designing temporal and spatial cues for two different models and their combinations can be beneficial, considering the global of a video-tracklet. 3D-CNNs allow encoding of temporal information while 2D-CNNs extract spatial or appearance information. In this paper, we propose a Spatio-Temporal Cross Attention (STCA) network to utilize both 2D-CNNs and 3D-CNNs that calculate the cross attention mapping both from the layer of 3D-CNNs and 2D-CNNs along a person&#39;s trajectory to gate the following layers of 2D-CNNs; and highlight relevant appearance features for the person ReID . Given an input tracklet, the proposed cross attention (CA) is able to capture the salient regions that propagate throughout the tracklet to obtain the global view. This provides a spatio-temporal attention approach that can be dynamically aggregated with spatial features of 2D-CNNs to perform finer-grained recognition. Additionally, we exploit the advantage of utilizing cosine similarity while triplet sampling as well as for calculating the final recognition score. Experimental analyses on three challenging benchmark datasets indicate that integrating spatio-temporal cross attention into the state-of-the-art video ReID backbone CNN architecture allows for improving their recognition accuracy .},
  archive      = {J_ICV},
  author       = {Amran Bhuiyan and Jimmy Xiangji Huang},
  doi          = {10.1016/j.imavis.2022.104474},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104474},
  shortjournal = {Image Vis. Comput.},
  title        = {STCA: Utilizing a spatio-temporal cross-attention network for enhancing video person re-identification},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Person re-ID through unsupervised hypergraph rank selection
and fusion. <em>ICV</em>, <em>123</em>, 104473. (<a
href="https://doi.org/10.1016/j.imavis.2022.104473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-ID has been gaining a lot of attention and nowadays is of fundamental importance in many camera surveillance applications. The task consists of identifying individuals across multiple cameras that have no overlapping views. Most of the approaches require labeled data, which is not always available, given the huge amount of demanded data and the difficulty of manually assigning a class for each individual. Recently, studies have shown that re-ranking methods are capable of achieving significant gains, especially in the absence of labeled data. Besides that, the fusion of feature extractors and multiple-source training is another promising research direction not extensively exploited. We aim to fill this gap through a manifold rank aggregation approach capable of exploiting the complementarity of different person Re-ID rankers. In this work, we perform a completely unsupervised selection and fusion of diverse ranked lists obtained from multiple and diverse feature extractors. Among the contributions, this work proposes a query performance prediction measure that models the relationship among images considering a hypergraph structure and does not require the use of any labeled data. Expressive gains were obtained in four datasets commonly used for person Re-ID. We achieved results competitive to the state-of-the-art in most of the scenarios.},
  archive      = {J_ICV},
  author       = {Lucas Pascotti Valem and Daniel Carlos Guimarães Pedronette},
  doi          = {10.1016/j.imavis.2022.104473},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104473},
  shortjournal = {Image Vis. Comput.},
  title        = {Person re-ID through unsupervised hypergraph rank selection and fusion},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-based detection from the perspective of small
or tiny objects: A survey. <em>ICV</em>, <em>123</em>, 104471. (<a
href="https://doi.org/10.1016/j.imavis.2022.104471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small or tiny objects is always a difficult and challenging issue in computer vision . In this paper, we provide a latest and comprehensive survey of deep learning-based detection approaches from the perspective of small or tiny objects. Our survey is featured by thorough and exhaustive analysis of small or tiny object detection. We comprehensively introduce 30 existing datasets about small or tiny objects, and summarize different definitions of small or tiny objects based on different application scenarios, such as pedestrian detection, traffic signs detection, face detection, remote sensing target detection and object detection in common life. Then small or tiny object detection techniques are overviewed systematically from seven aspects, including super-resolution techniques, context-based information, multi-scale representation learning , anchor mechanism, training strategy, data augmentation , and schemes based on loss function. Finally, the detection performance of small or tiny objects on 12 popular datasets is analyzed in depth. Based on performance analysis, we also discuss the promising research directions in the future. We hope this survey could provide researchers guidance to catalyze understanding of small or tiny object detection and further facilitate research on small or tiny object detection systems.},
  archive      = {J_ICV},
  author       = {Kang Tong and Yiquan Wu},
  doi          = {10.1016/j.imavis.2022.104471},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104471},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning-based detection from the perspective of small or tiny objects: A survey},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time semantic segmentation with local spatial pixel
adjustment. <em>ICV</em>, <em>123</em>, 104470. (<a
href="https://doi.org/10.1016/j.imavis.2022.104470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research of semantic segmentation networks has achieved a significant breakthrough recently. However, most part of methods have difficulty in utilizing information generated at each stage, which resulting in pixel value dislocation and blurred boundaries for small-scale objects. To overcome these challenges, a local spatial pixel adjustment network (LSPANet) is proposed in this paper, which mainly consists of a dual-branch decoding fusion (DDF) module and a spatial pixel cross-correlation (SPCC) block. Specifically, the DDF module takes the high-level and low-level feature maps with different stages as the input, and gradually eliminates the discrepancy in the information of the feature map to fuse a variety of information extracted in the encoder stage. The SPCC block adopts the horizontal spatial pixel adjustment (HSPA) module and the vertical spatial pixel adjustment (VSPA) module to capture the relationship of each pixel value in the local horizontal and vertical space respectively, and then assign the importance to all values based on this relationship. LSPANet is evaluated on Cityscapes and Camvid datasets. The experimental results show that our network achieves 77.1% mIoU with 2 M parameters on the challenging Cityscapes dataset and the inference speed exceeds 30 FPS in a single GTX 2080 Ti GPU .},
  archive      = {J_ICV},
  author       = {Cunjun Xiao and Xingjun Hao and Haibin Li and Yaqian Li and Wenming Zhang},
  doi          = {10.1016/j.imavis.2022.104470},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104470},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time semantic segmentation with local spatial pixel adjustment},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving neural network robustness through neighborhood
preserving layers. <em>ICV</em>, <em>123</em>, 104469. (<a
href="https://doi.org/10.1016/j.imavis.2022.104469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional embeddings are often projected via fully connected layers while training neural networks . A major vulnerability that makes neural networks fail to be robust against adversarial attack is their use of overparameterized fully connected layers. We present a dimension reducing layer which preserves high-dimensional neighborhoods across the entire manifold. Atypically, our neighborhood preserving layer operates on non-static high dimensional inputs and can be trained efficiently via gradient descent . Our interest is in developing a trainable manifold representation, whose low-dimensional embeddings can be re-used for other purposes, and in investigating its robustness against adversarial attack. Our layer internally uses nearest-neighbor attractive and repulsive forces to create a low dimensional output representation. We demonstrate a novel neural network architecture which can incorporate such a layer, and also can be trained efficiently. Our theoretical results show why linear layers, which have many parameters, are innately less robust. This is corroborated by experiments on MNIST and CIFAR10 replacing the first fully-connected layer with a neighborhood preserving layer by our proposed model.},
  archive      = {J_ICV},
  author       = {Bingyuan Liu and Christopher Malon and Lingzhou Xue and Erik Kruus},
  doi          = {10.1016/j.imavis.2022.104469},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104469},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving neural network robustness through neighborhood preserving layers},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi–feature fusion tracking algorithm based on
peak–context learning. <em>ICV</em>, <em>123</em>, 104468. (<a
href="https://doi.org/10.1016/j.imavis.2022.104468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is considered a critical process in most applications of computer vision . Recently, tracking algorithms that include correlation filter in their frameworks have gained massive popularity due to their high efficiency. The previous algorithms aim to learn the correlation filter by leveraging over all features of the target and its neighbors. However, in this paper, a new tracking algorithm that merges an elastic net constraint and a contextual information into the training scheme is proposed to estimate the target location successfully. The novel optimization problem can significantly strengthen the peak value of the target and effectively eliminate the distractive features. Moreover, most of the correlation filter trackers only use one single feature , which has poor ability under a sophisticated environment. For this reason, a multi–feature fusion strategy is proposed in the framework that embeds multiple features to enhance the tracking performance. Consequently, a multi–scale adaptive model is implemented to improve the tracking stability through scale variations. Besides, an updating mechanism is applied within the proposed framework to reduce the tracking drift. Extensive quantitative and qualitative experiments on challenging benchmarks show that this unified tracker model achieves impressive performance compared to correlation filter and deep trackers.},
  archive      = {J_ICV},
  author       = {Tayssir Bouraffa and Zihang Feng and Liping Yan and Yuanqing Xia and Bo Xiao},
  doi          = {10.1016/j.imavis.2022.104468},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104468},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi–feature fusion tracking algorithm based on peak–context learning},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E2E-VSDL: End-to-end video surveillance-based deep learning
model to detect and prevent criminal activities. <em>ICV</em>,
<em>123</em>, 104467. (<a
href="https://doi.org/10.1016/j.imavis.2022.104467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime detection and their prediction is a fundamental process to reduce criminal activities before they actually happen. Moreover, the detection method is vital since can it potentially can save the victim&#39;s life, avoid all-time strain, and harm to the public/private property. In addition, it can be useful in predicting the possible terrorist activities. Crime detection using deep learning models is an attention-grabbing research area. Detecting and reducing the criminal activities is imperative to develop a peaceful society. Video surveillance automates the hazardous situations and enables a law enforcement system to take effective steps towards public safety. In this paper, an end-to-end deep learning model is proposed which is based on Bi-directional gated recurrent unit (BiGRU) and Convolutional neural network (CNN) to detect and prevent criminal activities. The CNN extracts the spatial features from video frames whereas temporal and local motion features are extracted by the BiGRU from multiple frames CNN extracted features. The focused bag is created to select those video frames which indicate certain actions. Moreover, ranked-based loss is used to effectively detect and classify the suspicious activities. For classification of activities, various machine learning classifiers are used. The proposed deep learning video surveillance technique is able to track human trails and detect criminal events. The CAVIAR dataset is used to examine the proposed technique for video surveillance-based crime detection with a performance accuracy of almost 98.86%. The alerts received from the proposed technique can also be examined, demonstrates that the practiced video surveillance cameras systems can effectively detect unusual and criminal activities. In addition, the proposed technique showed considerable performance accuracy and outscored the related state-of-the-art (SOTA)DL models including CNN-LSTM, CNN, HMM, and DBN and achieved 21.88% absolute improvement in crime detection accuracy.},
  archive      = {J_ICV},
  author       = {Maryam Qasim Gandapur},
  doi          = {10.1016/j.imavis.2022.104467},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104467},
  shortjournal = {Image Vis. Comput.},
  title        = {E2E-VSDL: End-to-end video surveillance-based deep learning model to detect and prevent criminal activities},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature fusion for object detection at one map.
<em>ICV</em>, <em>123</em>, 104466. (<a
href="https://doi.org/10.1016/j.imavis.2022.104466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale feature plays a crucial role in the detector, and existing methods adopt the feature pyramid based on multiple maps. This paper focuses on a single map and proposes an encoder called SFMF which can employ multi-scale feature fusion on a map. One of the crucial techniques underlying SFMF is a fine-grained weighting method that is used to fast discard unneeded pixel channels during the fusion process. YOLOF (you only look one-level feature) with SFMF (single feature map fusion) achieve 38.5 mAP in the ResNet50 and 40.3 mAP in the ResNet101, which improves 0.8 and 0.5 mAP than the baseline, respectively. Meta-ACON is used to auto-learn activate the neurons or not in the backbone. With the Meta-ACON and SFMF, YOLOF can achieve 39.1 and 40.4 mAP, surpassing the baseline by 1.4 and 0.6 mAP on COCO val-dev. In addition, YOLOF with SFMF achieves 54.8 mAP, improving the performance by an absolute 4.9 mAP on the aircraft detection dataset, with a slight sacrificing efficiency (1 FPS) in inference.},
  archive      = {J_ICV},
  author       = {Xing Xi and Yuanqing Wu and Canming Xia and Shenghuang He},
  doi          = {10.1016/j.imavis.2022.104466},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104466},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature fusion for object detection at one map},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handcrafted localized phase features for human action
recognition. <em>ICV</em>, <em>123</em>, 104465. (<a
href="https://doi.org/10.1016/j.imavis.2022.104465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is one of the most important topics in computer vision . Monitoring elderly people and children, smart surveillance systems and human-computer interaction are a few examples of its applications. The aim of this study is to recognize human activities by utilizing the phase information extracted from the frequency domain of the video data as handcrafted features. Rather than estimating optical flow or computing motion vectors , we aim to utilize the localized phase information as descriptors of the motion dynamics of the scene. Phase correlation information extracted from each two co-sited blocks from each two consecutive frames of video clips were used to train a model using KNN classifier to model the action. To evaluate the performance of our method, an extensive work has been done on three large and complex datasets: UCF101, Kinetics-400 and Kinetics-700. The results show that our approach succeeds on recognizing human actions across all these datasets with high accuracy.},
  archive      = {J_ICV},
  author       = {Seyed Mostafa Hejazi and Charith Abhayaratne},
  doi          = {10.1016/j.imavis.2022.104465},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104465},
  shortjournal = {Image Vis. Comput.},
  title        = {Handcrafted localized phase features for human action recognition},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent camera-invariant and noise-tolerant learning for
unsupervised person re-identification. <em>ICV</em>, <em>123</em>,
104462. (<a href="https://doi.org/10.1016/j.imavis.2022.104462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (re-ID) is still a challenging task. Existing methods usually utilize an alternative manner of generating pseudo labels by clustering and optimizing the model based on pseudo labels. Although these methods achieve great accuracy, there remain two problems unsolved : (1) Noise labels caused by camera variations and other factors. (2) The training of the model is inconsistent or unstable. In this paper, we propose a cluster memory-based meta learning (CMML) strategy with a cluster memory-based additive margin (CMAM) loss to deal with noise labels caused by camera variations and the model training problem, and a cluster memory-based noise-tolerant (CMNT) loss to tackle the rest noise labels. Extensive experimental results on three re-ID datasets (i.e., DukeMTMC-reID, Market1501 and MSMT17) validate the effectiveness of our proposed method. Our method achieves 70.3%, 81.8%, and 39.2% mAP on these three datasets, yielding comparable performance against the state-of-the-art purely unsupervised re-ID methods.},
  archive      = {J_ICV},
  author       = {Yiyu Chen and Zheyi Fan and Shuni Chen},
  doi          = {10.1016/j.imavis.2022.104462},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104462},
  shortjournal = {Image Vis. Comput.},
  title        = {Consistent camera-invariant and noise-tolerant learning for unsupervised person re-identification},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep isometric maps. <em>ICV</em>, <em>123</em>, 104461. (<a
href="https://doi.org/10.1016/j.imavis.2022.104461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Isometric feature mapping is an established time-honored algorithm in manifold learning and non-linear dimensionality reduction. Its prominence can be attributed to the output of a coherent global low-dimensional representation of data by preserving intrinsic distances. In order to enable an efficient and more applicable isometric feature mapping, a diverse set of sophisticated advancements have been proposed to the original algorithm to incorporate important factors like sparsity of computation, conformality, topological constraints and spectral geometry. However, a significant shortcoming of most approaches is the dependence on large-scale dense-spectral decompositions and the inability to generalize to points far away from the sampling of the manifold. In this paper, we explore an unsupervised deep learning approach for computing distance-preserving maps for non-linear dimensionality reduction. We demonstrate that our framework is general enough to incorporate all previous advancements and show a significantly improved local and non-local generalization of the isometric mapping . Our approach involves training with only a few landmark points and avoids the need for population of dense matrices as well as computing their spectral decomposition.},
  archive      = {J_ICV},
  author       = {Gautam Pai and Alex Bronstein and Ronen Talmon and Ron Kimmel},
  doi          = {10.1016/j.imavis.2022.104461},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104461},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep isometric maps},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Double chain networks for monocular 3D human pose
estimation. <em>ICV</em>, <em>123</em>, 104452. (<a
href="https://doi.org/10.1016/j.imavis.2022.104452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2D-3D lifting task for Human Pose Estimation is a highly nonlinear mapping problem, which requires mutual constraints among human joints . In this paper, we mainly discuss how to represent the constraints among the joints by their global and local dependencies, and how to fuse them more effectively. Therefore, we propose a novel end-to-end neural network architecture named Double chain Networks (DCN), for monocular 2D-3D human pose lifting task. The DCN consists of two parts. One is a global constraint module based on full connection layer, which extracts the global dependency information of human body joints; the other is graph convolution based local constraint module, which represents the local dependency information of human body joints. The global and local constraint features are fused by interleaved addition to combine the global and local spatial constraint features of human joints in DCN regression, which outperforms the way of concatenating them. We perform abundant ablation experiments on Human3.6M dataset to verify the advantages of the constraint modules, and compare the performance of the single/double chain networks. Experimental results show that the DCN achieves state-of-the-art performance and exhibits strong generalization ability , and its performance is comparable to the methods based on temporal information. Finally, we apply the DCN to reconstruct 3D skeletons in 3D anime characters. The equipotential-joints connection mode to extract the constraint relationships between the human equipotential-joints; The DCN can effectively extract and combine the global and local constraints of human joints; The DCN achieves state-of-the-art performance and exhibits strong generalization ability; We apply the DCN to the posture construction of 3D virtual characters.},
  archive      = {J_ICV},
  author       = {Guihu Bai and Yanmin Luo and Xueliang Pan and Youjie Wang and Jia Wang and Jing-Ming Guo},
  doi          = {10.1016/j.imavis.2022.104452},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104452},
  shortjournal = {Image Vis. Comput.},
  title        = {Double chain networks for monocular 3D human pose estimation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression recognition system based on variational
mode decomposition and whale optimized KELM. <em>ICV</em>, <em>123</em>,
104445. (<a href="https://doi.org/10.1016/j.imavis.2022.104445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a methodology for the Facial Expression Recognition (FER) system is proposed using the Variational Mode Decomposition (VMD) and Whale Optimization (WO) with Kernel Extreme Learning Machine (KELM) classifier. A non-stationary, adaptive, and variational signal analysis technique called VMD is adopted in this work, which depends on the signal&#39;s frequency information content. The VMD decomposed the input image into four modes, and the 4 th mode of the VMD decomposition is considered for feature representation, a high-frequency band. This VMD mode preserves the edge and shape features from the face image efficiently. The high dimensional features are reduced using the Principal Component Analysis + Linear Discriminant Analysis (PCA + LDA) method, which minimizes the feature dimension and retains the high variance among emotion classes. A hybrid classifier with better scalability and faster learning speed than SVM and Least Squares SVM (LS-SVM), namely WO-KELM, is proposed to discriminate facial expressions accurately. The WO algorithm is employed for optimal parameter tuning of the KELM with the RBF kernel . The performance of the proposed framework are compared with state-of-the-art methods. Extensive experiments are assessed on the two benchmark datasets, namely Japanese Female Facial Expression (JAFFE) and the Extended Cohn-Kanade (CK +). Experimental results founded on 5-fold Stratified Cross-Validation (SCV) test reveal the superiority of the proposed method over state-of-the-art systems.},
  archive      = {J_ICV},
  author       = {Nikunja Bihari Kar and Korra Sathya Babu and Sambit Bakshi},
  doi          = {10.1016/j.imavis.2022.104445},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104445},
  shortjournal = {Image Vis. Comput.},
  title        = {Facial expression recognition system based on variational mode decomposition and whale optimized KELM},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MMSNet: Multi-modal scene recognition using multi-scale
encoded features. <em>ICV</em>, <em>122</em>, 104453. (<a
href="https://doi.org/10.1016/j.imavis.2022.104453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing multi-level features has been proven to improve RGB-D scene recognition performance. However, simply fusing features after conducting RGB and depth data separately may not satisfy multi-modal integrity. In this work, we propose an effective multi-modal RGB-D scene recognition model that integrates global or local multi-scale/multi-semantic features. The proposed approach is built on two key components. In the first stage, multiple random recursive neural networks (RNNs) are employed on a baseline CNN model to obtain multi-scale encoded features from multi-level feature hierarchy. In the second stage, multi-layer perceptrons (MLPs) learn global/local features at multiple levels while encouraging the correlation of multi-modal mutual features. Our learning design is based on the insight that correlated multi-modal features provide the complementary relation between the two modalities that promotes better performance of RGB-D scene recognition. In addition, the network is trained using a decisive fusion based on modality prediction confidence weights to yield RGB-D multi-modal recognition. Experiments on three RGB-D scene datasets verify the effectiveness of the proposed approach by achieving superior or highly competitive results compared to other state-of-the-art counterpart methods. Evaluation code and models are available at https://github.com/acaglayan/MMSNet .},
  archive      = {J_ICV},
  author       = {Ali Caglayan and Nevrez Imamoglu and Ryosuke Nakamura},
  doi          = {10.1016/j.imavis.2022.104453},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104453},
  shortjournal = {Image Vis. Comput.},
  title        = {MMSNet: Multi-modal scene recognition using multi-scale encoded features},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning language to symbol and language to vision mapping
for visual grounding. <em>ICV</em>, <em>122</em>, 104451. (<a
href="https://doi.org/10.1016/j.imavis.2022.104451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Grounding (VG) is a task of locating a specific object in an image semantically matching a given linguistic expression . The mapping of the linguistic and visual contents and the understanding of diverse linguistic expressions are the two challenges of this task. The performance of visual grounding is consistently improved by deep visual features in the last few years. While deep visual features contain rich information, they could also be noisy, biased and easily over-fitted. In contrast, symbolic features are discrete, easy to map and usually less noisy. In this work, we propose a novel modular network learning to match both the object&#39;s symbolic features and conventional visual features with the linguistic information. Moreover, the Residual Attention Parser is designed to alleviate the difficulty of understanding diverse expressions. Our model achieves competitive performance on three popular datasets of VG.},
  archive      = {J_ICV},
  author       = {Su He and Xiaofeng Yang and Guosheng Lin},
  doi          = {10.1016/j.imavis.2022.104451},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104451},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning language to symbol and language to vision mapping for visual grounding},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised multi-view stereo network based on multi-stage
depth estimation. <em>ICV</em>, <em>122</em>, 104449. (<a
href="https://doi.org/10.1016/j.imavis.2022.104449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current years, supervised learning multi-view stereo (MVS) methods have achieved impressive performance. However, these methods still suffer the limitation of hard to acquire large-scale depth supervision data, which hinders the generalization ability in never-seen-before scenarios. Recently, some unsupervised-learning methods have been proposed, which relieved the requirement of depth supervision data. However, the generated depth map with lower resolution since the memory consumption grows cubically. In this paper, we propose a novel unsupervised multi-view stereo network based on multi-stage depth estimation, which can increase depth map resolution and generate a dense 3D model with rich details without relying on depth supervision data. To reduce the 3D cost volume highly memory consumption, the progressive coarse-to-fine multiple stages are adopted. Besides, a multi-view group-wise correlation (MV-GwC) module is designed to introduce multi-view correlation prior, which can enhance network performance and further reduce memory consumption. Qualitative and quantitative experiment results show the effectiveness of our method. We outperform some previous supervised and unsupervised MVS methods on DTU and Tanks &amp; Temples benchmarks.},
  archive      = {J_ICV},
  author       = {Shuai Qi and Xinzhu Sang and Binbin Yan and Peng Wang and Duo Chen and Huachun Wang and Xiaoqian Ye},
  doi          = {10.1016/j.imavis.2022.104449},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104449},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised multi-view stereo network based on multi-stage depth estimation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining complementary trackers for enhanced long-term
visual object tracking. <em>ICV</em>, <em>122</em>, 104448. (<a
href="https://doi.org/10.1016/j.imavis.2022.104448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several different algorithms have been studied to combine the capabilities of baseline trackers in the context of short-term visual object tracking. Despite such an extended interest, the long-term setting has not been taken into consideration by previous studies. In this paper, we explicitly consider long-term tracking scenarios and provide a framework to fuse the characteristics of complementary state-of-the-art trackers to achieve enhanced tracking performance. Our strategy perceives whether the two trackers are following the target object through an online learned deep verification model. Such a target recognition strategy enables the activation of a decision strategy which selects the best performing tracker as well as it corrects their performance when failing. The proposed solution is studied extensively and the comparison with several other approaches reveals that it beats the state-of-the-art on the long-term visual tracking benchmarks LTB-35, LTB-50, TLP, and LaSOT.},
  archive      = {J_ICV},
  author       = {Matteo Dunnhofer and Kristian Simonato and Christian Micheloni},
  doi          = {10.1016/j.imavis.2022.104448},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104448},
  shortjournal = {Image Vis. Comput.},
  title        = {Combining complementary trackers for enhanced long-term visual object tracking},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust depth estimation on real-world light field images
using gaussian belief propagation. <em>ICV</em>, <em>122</em>, 104447.
(<a href="https://doi.org/10.1016/j.imavis.2022.104447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation on light field images has been well researched on synthetic datasets. However, existing methods often fail on real-world light field images because of the real-world noises. Due to the lack of real-world light field image datasets for depth estimation, the learning-based methods, e.g., convolutional neural networks, are not capable of performing well on real-world light field images with only synthetic datasets available. In this paper, we adopt the optical flow estimation method to estimate depth from light field images since (i) the existing optical flow methods have shown robustness with significant real-world noises; (ii) the adopted optical flow estimation method does not require any training data, thus not limited to the synthetic datasets. The depth can be solved via the approach of optical flow estimation because (i) a sub-aperture image array can be converted from a light field image; (ii) the optical flow between every two adjacent sub-aperture images of the image array is exactly the disparity between the two images. Furthermore, since a sub-aperture array contains numerous adjacent image pairs, numerous optical flow maps can be generated from the sub-aperture array. We show the depth can be well refined from these numerous optical flow maps through modeling these maps in a single graphical model (a pairwise Gaussian MRF to be precise) and inferencing this graphical model by Gaussian belief propagation (GaBP). Experiments show that i) the proposed method achieves better depth estimation than the state-of-the-art methods on real-world light field images; ii) the proposed method can estimate the depth of objects at a far distance (3.5–9.5 m) much more accurately than the state-of-the-art convolutional neural networks.},
  archive      = {J_ICV},
  author       = {Zhihao Zhao and Samuel Cheng and Lihua Li},
  doi          = {10.1016/j.imavis.2022.104447},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104447},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust depth estimation on real-world light field images using gaussian belief propagation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch covariance neural network for image recognition.
<em>ICV</em>, <em>122</em>, 104446. (<a
href="https://doi.org/10.1016/j.imavis.2022.104446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has shown that convolutional neural networks (CNN) can achieve state of the art if the datasets are well built. However, the existing convolutional layer is affected by various datasets with the inevitable problems of local abnormal features, i.e., illumination intensity and feature interaction. This paper replaces the convolutional layer with a batch covariance layer (BCL) to locate the category-related region unaffected by the problems. The BCL is regarded as a 3D covariance operation, which calculates the correlation between the kernels and feature maps in kernel size of all channels. Forward propagation , backward propagation, gradient updating, and testing procedure of the BCL are described. The comparison between BCL and convolutional layer shows the ability of BCL to reduce the influence of illumination intensity and feature interaction for discriminating and generating tasks. Complexity analysis shows that BCL can improve the accuracy with a thimbleful time consumption increase. Besides, the batch covariance neural network (BCovNN) is extended from the CNN by replacing the convolutional layer with BCL. Ablation experiment verifies the improvement of BCovNN is provided by BCL separately. BCovNN is evaluated on several popular datasets (i.e., MNIST, STL-10, CIFAR-10, and ImageNet) for image recognition and PASCAL VOC (2007 and 2012) datasets for object localization. Experimental results reveal that BCovNN achieves significant improvements over the corresponding CNN.},
  archive      = {J_ICV},
  author       = {Tianyou Zheng and Qiang Wang and Yue Shen and Xiang Ma and Xiaotian Lin},
  doi          = {10.1016/j.imavis.2022.104446},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104446},
  shortjournal = {Image Vis. Comput.},
  title        = {Batch covariance neural network for image recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic sample weighting for weakly supervised object
detection. <em>ICV</em>, <em>122</em>, 104444. (<a
href="https://doi.org/10.1016/j.imavis.2022.104444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The framework based on Multiple Instance Learning (MIL) greatly improves the performance of Weakly Supervised Object Detection (WSOD), which enjoys a promising development. However, the detection results tend to be the most discriminative parts of the object, which is still an open problem. In this paper, we analyze the causes of the problem from the perspective of sample balance. Considering the inaccuracy of pseudo supervised information in WSOD, a Dynamic Sample Weighting strategy (DSW) is proposed to focus on samples which closely cover the object, making the detection results cover the object more comprehensively. The performance of DSW on PASCAL VOC 2007, PASCAL VOC 2012 and MS-COCO is significantly enhanced through the simple and effective method in this paper. Code will be made available.},
  archive      = {J_ICV},
  author       = {Xuewei Li and Song Yi and Ruixuan Zhang and Xuzhou Fu and Han Jiang and Chenhan Wang and Zhiqiang Liu and Jie Gao and Jian Yu and Mei Yu and Ruiguo Yu},
  doi          = {10.1016/j.imavis.2022.104444},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104444},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic sample weighting for weakly supervised object detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning based video segmentation of moving scene by
motion index using IO detector and shot segmentation. <em>ICV</em>,
<em>122</em>, 104443. (<a
href="https://doi.org/10.1016/j.imavis.2022.104443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video segmentation has a major role in several applications like autonomous vehicles, medical image analysis, video surveillance, and augmented reality . Portrait segmentation, a subset of semantic image segmentation, is the key step of preprocessing step in numerous applications and a few among them are entertainment applications, security systems, video conferences, and so on. For the given video, every object exhibiting independent motion in at least a single frame is segmented. This is formulated as a learning problem and designed as an Ensemble Soft Voting Algorithm based on Supervised Machine Learning (ESVA-SML). The motion stream requires an ensemble learning method trained on synthetic videos for segmenting independent objects at motion in the optical flow field . The spatial–temporal features are extracted from each sub-bands which efficiently resembles the human visual system characteristics concerning the feature extract using Gray Level Size Zone Matrix (GLSZM). This method linear combines the previous vectors within a stipulated time interval thereby describing each spatial–temporal feature vector. Based on the prediction errors, shot boundaries of every shot are efficiently identified from which at least a single keyframe is extracted for further analysis. The proposed method is extensively evaluated using SegTrack-v2, DAVIS, and Fusion databases by comparing eight standard methods in various parameters. By analyzing all these three databases, SegTrack-v2 is best in terms of accuracy, precision, and recall. While both DAVIS and Fusion databases produce less MAE . Finally, Fusion produces less RMSE and RAE with minimal response time .},
  archive      = {J_ICV},
  author       = {G. Balachandran and J. Venu Gopala Krishnan},
  doi          = {10.1016/j.imavis.2022.104443},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104443},
  shortjournal = {Image Vis. Comput.},
  title        = {Machine learning based video segmentation of moving scene by motion index using IO detector and shot segmentation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale feature aggregation and boundary awareness
network for salient object detection. <em>ICV</em>, <em>122</em>,
104442. (<a href="https://doi.org/10.1016/j.imavis.2022.104442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection aims to detect the most visually distinctive objects in an image. Although existing FCN-based methods have shown strong advantages in this field, scale variation and complex boundary are still great challenges. In this paper, we propose a multi-scale feature aggregation and boundary awareness network to overcome the problems. Multi-scale feature aggregation module is proposed to integrate adjacent hierarchical features and the multiple aggregation strategy solves the problem of scale variation. To obtain more effective multi-scale features from integrated features, a cross feature refinement module is proposed to compose the decoder. For the issue of complex boundary, we design a boundary pixel awareness loss function to enable the network to acquire boundary information and generate high-quality saliency maps with better boundary. Experiments on five benchmark datasets show that our network outperforms recent state-of-the-art detectors quantitatively and qualitatively.},
  archive      = {J_ICV},
  author       = {Qin Wu and Jianzhe Wang and Zhilei Chai and Guodong Guo},
  doi          = {10.1016/j.imavis.2022.104442},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104442},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale feature aggregation and boundary awareness network for salient object detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Atlas generative models and geodesic interpolation.
<em>ICV</em>, <em>122</em>, 104433. (<a
href="https://doi.org/10.1016/j.imavis.2022.104433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative neural networks have a well recognized ability to estimate underlying manifold structure of high dimensional data . However, if a single latent space is used, it is not possible to faithfully represent a manifold with topology different from Euclidean space . In this work we define the general class of Atlas Generative Models (AGMs), models with hybrid discrete-continuous latent space that estimate an atlas on the underlying data manifold together with a partition of unity on the data space. We identify existing examples of models from various popular generative paradigms that fit into this class. Due to the atlas interpretation, ideas from non-linear latent space analysis and statistics , e.g. geodesic interpolation, which has previously only been investigated for models with simply connected latent spaces, may be extended to the entire class of AGMs in a natural way. We exemplify this by generalizing an algorithm for graph based geodesic interpolation to the setting of AGMs, and verify its performance experimentally.},
  archive      = {J_ICV},
  author       = {Jakob Stolberg-Larsen and Stefan Sommer},
  doi          = {10.1016/j.imavis.2022.104433},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104433},
  shortjournal = {Image Vis. Comput.},
  title        = {Atlas generative models and geodesic interpolation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Person re-identification: A taxonomic survey and the path
ahead. <em>ICV</em>, <em>122</em>, 104432. (<a
href="https://doi.org/10.1016/j.imavis.2022.104432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (PRId) is one of the most challenging tasks in automated video surveillance and has been an area of intense research spanning the past decade. PRId aims at finding a person who has previously been identified using some unique descriptor of the person. This survey comprises a wide spectrum of PRId methods spanning from traditional to deep learning-based being analyzed and compared. This survey also discusses different PRId frameworks on the basis of machine learning and deep learning . It offers a multi-dimensional taxonomy to classify the most pertinent researches according to different perspectives and tries to unify the categorization of PRId methods and fill the gap between the recently published surveys. This study highlights the challenges in building PRId systems . It presents a critical overview of recent progress and the state-of-the-art approaches to solving some major challenges of existing PRId systems. Furthermore, we discuss the performance comparisons of the various state-of-the-art in different datasets. Finally, we discuss several open issues and directions for future studies.},
  archive      = {J_ICV},
  author       = {Nayan Kumar Subhashis Behera and Pankaj Kumar Sa and Sambit Bakshi and Ram Prasad Padhy},
  doi          = {10.1016/j.imavis.2022.104432},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104432},
  shortjournal = {Image Vis. Comput.},
  title        = {Person re-identification: A taxonomic survey and the path ahead},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grassmann manifold based framework for automated fall
detection from a camera. <em>ICV</em>, <em>122</em>, 104431. (<a
href="https://doi.org/10.1016/j.imavis.2022.104431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Grassmann manifold based novel, real-time framework for automated fall detection in an indoor environment using a single camera. Fall is an activity that is uncontrolled, unintentional, involuntary and can occur while a person is doing any daily living activity. This is especially true for the elderly and the sick, for whom a fall can lead to further complications that may cause irreversible damage to their health. Therefore, it is important to develop a non-intrusive, automated fall detection method such that an alert can be raised in case a fall occurs. We propose a Grassmann manifold based framework for fall detection from a single camera that is also capable of recognizing other daily living activities (DLA) such as walking, sitting, etc. We perform experiments using publicly available datasets and our experimental results show that the fall detection and recognition accuracy of our proposed framework is comparable with the state of the art.},
  archive      = {J_ICV},
  author       = {Pramod Kumar Soni and Ayesha Choudhary},
  doi          = {10.1016/j.imavis.2022.104431},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104431},
  shortjournal = {Image Vis. Comput.},
  title        = {Grassmann manifold based framework for automated fall detection from a camera},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Does depth estimation help object detection? <em>ICV</em>,
<em>122</em>, 104427. (<a
href="https://doi.org/10.1016/j.imavis.2022.104427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ground-truth depth, when combined with color data, helps improve object detection accuracy over baseline models that only use color. However, estimated depth does not always yield improvements. Many factors affect the performance of object detection when estimated depth is used. In this paper, we comprehensively investigate these factors with detailed experiments, such as using ground-truth vs. estimated depth, effects of different state-of-the-art depth estimation networks, effects of using different indoor and outdoor RGB-D datasets as training data for depth estimation, and different architectural choices for integrating depth to the base object detector network. We propose an early concatenation strategy of depth, which yields higher mAP than previous works&#39; while using significantly fewer parameters.},
  archive      = {J_ICV},
  author       = {Bedrettin Cetinkaya and Sinan Kalkan and Emre Akbas},
  doi          = {10.1016/j.imavis.2022.104427},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104427},
  shortjournal = {Image Vis. Comput.},
  title        = {Does depth estimation help object detection?},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view dynamic facial action unit detection.
<em>ICV</em>, <em>122</em>, 103723. (<a
href="https://doi.org/10.1016/j.imavis.2018.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel convolutional neural network approach to address the fine-grained recognition problem of multi-view dynamic facial action unit detection. We leverage recent gains in large-scale object recognition by formulating the task of predicting the presence or absence of a specific action unit in a still image of a human face as holistic classification. We then explore the design space of our approach by considering both shared and independent representations for separate action units, and also different CNN architectures for combining color and motion information. We then move to the novel setup of the FERA 2017 Challenge, in which we propose a multi-view extension of our approach that operates by first predicting the viewpoint from which the video was taken, and then evaluating an ensemble of action unit detectors that were trained for that specific viewpoint. Our approach is holistic, efficient, and modular, since new action units can be easily included in the overall system. Our approach significantly outperforms the baseline of the FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric. Additionally, it compares favorably against the winner of the FERA 2017 Challenge.},
  archive      = {J_ICV},
  author       = {Andrés Romero and Juán León and Pablo Arbeláez},
  doi          = {10.1016/j.imavis.2018.09.014},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103723},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-view dynamic facial action unit detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RE-Det3D: RoI-enhanced 3D object detector. <em>ICV</em>,
<em>121</em>, 104430. (<a
href="https://doi.org/10.1016/j.imavis.2022.104430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a two-stage point-based detector referred to as RoI-enhanced 3D object detector (RE-Det3D), which enhances the RoI-feature extraction ability for two-stage point-based 3D object detectors. The proposed detector is characterized by an RoI shape-aware module (RSAM), RoI keypoints sampling module (RKSM), and RoI self-attention module (RAM). More precisely, the RSAM uses the proposal which possesses accurate boundary information as auxiliary supervision, to reinforce the framework to be more aware of the object shape. Simultaneously, RKSM uses the tilted sampling strategy to obtain more representative keypoints from the RoI. Afterwards, the plug-and-play module RAM cascades the set-abstraction and self-attention, exploring the interactions of keypoints and aggregating local features, to produce discriminative feature representations for 3D box refinement. Comprehensive experiments are conducted on the widely used KITTI dataset and the latest large-scale dataset (ONCE). The results demonstrate that the RE-Det3D can bolster the baseline by a significant margin and achieve comparable accuracy as several strong voxel-based detectors.},
  archive      = {J_ICV},
  author       = {Yiqiang Wu and Weiping Xiao and Chang Liu and Jiantao Gao and Jiacheng Sun and Guozhu Tan and Xiaomao Li},
  doi          = {10.1016/j.imavis.2022.104430},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104430},
  shortjournal = {Image Vis. Comput.},
  title        = {RE-Det3D: RoI-enhanced 3D object detector},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and reliable probabilistic face embeddings based on
constrained data uncertainty estimation. <em>ICV</em>, <em>121</em>,
104429. (<a href="https://doi.org/10.1016/j.imavis.2022.104429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic Face Embeddings (PFE) can improve face recognition performance in unconstrained scenarios by integrating data uncertainty into the feature representation. However, the matching speed of PFE is too slow to be applied to large-scale face recognition or retrieval applications. Moreover, since deep learning-based data uncertainty estimation tends to be over-confident, the recognition performance of PFE is unstable. This paper proposes a probabilistic face embedding method to improve the robustness and speed of PFE. Specifically, the mutual likelihood score (MLS) metric used in PFE is simplified by using a one-dimensional variance to approximate the data uncertainty of face feature, to speed up the matching of probabilistic embedding pairs. Then, a unilateral constraint loss is proposed to limit the variation range of the lower part of the estimated data uncertainties, which can solve the problem of accuracy degradation in high-quality images. In addition, a feature fusion method based on temperature scaling is proposed, which can adjust the fusion weights of different quality images to improve the performance of video face recognition. Comprehensive experiments show that the proposed method can achieve comparable or better results in 6 benchmarks than the state-of-the-art methods with a less computational cost at the matching process . The code of our work is publicly available in GitHub ( https://github.com/KaenChan/ProbFace ).},
  archive      = {J_ICV},
  author       = {Kai Chen and Taihe Yi and Qi Lv},
  doi          = {10.1016/j.imavis.2022.104429},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104429},
  shortjournal = {Image Vis. Comput.},
  title        = {Fast and reliable probabilistic face embeddings based on constrained data uncertainty estimation},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face anti-spoofing detection based on multi-scale image
quality assessment. <em>ICV</em>, <em>121</em>, 104428. (<a
href="https://doi.org/10.1016/j.imavis.2022.104428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an effective face PAD algorithm based on multiscale perceptual image quality assessment features. Unique hand-crafted texture features extracted from face images are exploited for spoofing detection. The proposed features are classified into three major models: generalized Gaussian density-based, asymmetric generalized Gaussian density-based, and top gradient similarity deviation features. In light of the essential attributes of these models, a total of 21 multiscale features are acquired for classification, which is performed through a support vector machine (SVM). Extensive experiments on five benchmark databases, CASIA, Replay-Attack, UVAD, OULU-NPU, and SiW along with our new dataset demonstrated the effectiveness of the proposed framework. Experimental results indicated that our face PAD algorithm produced satisfactory detection accuracy on the tested datasets based on both intra-dataset and cross-dataset protocols. While outperforming a number of traditional face PAD methods, the proposed scheme achieved comparable results with many state-of-the-art deep learning-based networks. The introduction of the image quality assessment features with multiscale analysis into face PAD is promising for detection accuracy improvement.},
  archive      = {J_ICV},
  author       = {Herng-Hua Chang and Chun-Hsiao Yeh},
  doi          = {10.1016/j.imavis.2022.104428},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104428},
  shortjournal = {Image Vis. Comput.},
  title        = {Face anti-spoofing detection based on multi-scale image quality assessment},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continual coarse-to-fine domain adaptation in semantic
segmentation. <em>ICV</em>, <em>121</em>, 104426. (<a
href="https://doi.org/10.1016/j.imavis.2022.104426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are typically trained in a single shot for a specific task and data distribution , but in real world settings both the task and the domain of application can change. The problem becomes even more challenging in dense predictive tasks, such as semantic segmentation , and furthermore most approaches tackle the two problems separately. In this paper we introduce the novel task of coarse-to-fine learning of semantic segmentation architectures in presence of domain shift. We consider subsequent learning stages progressively refining the task at the semantic level ; i.e. , the finer set of semantic labels at each learning step is hierarchically derived from the coarser set of the previous step. We propose a new approach (CCDA) to tackle this scenario. First, we employ the maximum squares loss to align source and target domains and, at the same time, to balance the gradients between well-classified and harder samples. Second, we introduce a novel coarse-to-fine knowledge distillation constraint to transfer network capabilities acquired on a coarser set of labels to a set of finer labels. Finally, we design a coarse-to-fine weight initialization rule to spread the importance from each coarse class to the respective finer classes. To evaluate our approach, we design two benchmarks where source knowledge is extracted from the GTA5 dataset and it is transferred to either the Cityscapes or the IDD datasets, and we show how it outperforms the main competitors.},
  archive      = {J_ICV},
  author       = {Donald Shenaj and Francesco Barbato and Umberto Michieli and Pietro Zanuttigh},
  doi          = {10.1016/j.imavis.2022.104426},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104426},
  shortjournal = {Image Vis. Comput.},
  title        = {Continual coarse-to-fine domain adaptation in semantic segmentation},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning dynamic background for weakly supervised moving
object detection. <em>ICV</em>, <em>121</em>, 104425. (<a
href="https://doi.org/10.1016/j.imavis.2022.104425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving Object Detection (MOD) aims at extracting foreground moving objects in videos from static cameras. While low-rank based approaches have achieved impressive success in the MOD task, their performance remains limited on dynamics background scenes. The main reason is that dynamic clutters, e.g. , swaying leaves and rippers , are easy to mix up with moving objects in the decomposition model which simply classify the sparse noise as foregrounds. In order to improve the generalization ability of low-rank based moving object detectors, we suggest adding an explicit dynamic clutter component in the decomposition framework with realistic dynamic background modeling . Then the dynamic clutter can be learned through object-free video data due to their self-similarity across time and space. Thus, the moving objects can be naturally separated by a tensor-based decomposition model which formulates the static background by a unidirectional low-rank tensor, learns the dynamic clutter by a two-stream neural network , and constrains moving objects with spatiotemporal continuity. To further provide a more accurate object detection result, an objectness prior is embedded into our model in an attention manner. Extensive experimental results on the challenging datasets of dynamic background clearly demonstrate the superior performance of our model over the state-of-the-art in terms of quantitative metrics and visual quality.},
  archive      = {J_ICV},
  author       = {Zhijun Zhang and Yi Chang and Sheng Zhong and Luxin Yan and Xu Zou},
  doi          = {10.1016/j.imavis.2022.104425},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104425},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning dynamic background for weakly supervised moving object detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monocular contextual constraint for stereo matching with
adaptive weights assignment. <em>ICV</em>, <em>121</em>, 104424. (<a
href="https://doi.org/10.1016/j.imavis.2022.104424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching-based stereo disparity estimation has difficulty in dealing with occlusion, weak and repetitive textures in binocular vision. By contrast, monocular vision, estimating depth from a single image, is not subject to these challenges. Inspired by this, in this study, we propose an adaptive co-learning framework with monocular and stereo branches named CLStereo to improve stereo performance. This framework introduces a monocular branch as contextual constraints to transfer the prior knowledge learned from the monocular branch to the stereo branch. An adaptive weights assignment is further proposed to balance the co-learning of both branches without mutually tuning. CLStereo can be seamlessly embedded into many existing deep stereo models to boost their performance, especially in occluded, weak, and repetitive texture areas. Extensive experiments demonstrate that we achieve the state-of-the-art performance on the Scene Flow dataset and improve deep stereo models by at least 4% on KITTI 2012 and 2015 benchmarks.},
  archive      = {J_ICV},
  author       = {Chenghao Zhang and Gaofeng Meng and Bing Su and Shiming Xiang and Chunhong Pan},
  doi          = {10.1016/j.imavis.2022.104424},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104424},
  shortjournal = {Image Vis. Comput.},
  title        = {Monocular contextual constraint for stereo matching with adaptive weights assignment},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective actor-centric human-object interaction detection.
<em>ICV</em>, <em>121</em>, 104422. (<a
href="https://doi.org/10.1016/j.imavis.2022.104422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Human-Object Interaction (HOI) Detection has achieved tremendous advances in recent, it still remains challenging due to complex interactions with multiple humans and objects occurring in images, which would inevitably lead to ambiguities. Most existing methods either generate all human-object pair candidates and infer their relationships by cropped local features successively in a two-stage manner, or directly predict interaction points in a one-stage procedure. However, the lack of spatial configurations or reasoning steps of two- or one- stage methods respectively limits their performance in such complex scenes. To avoid this ambiguity, we propose a novel actor-centric framework. The main ideas are that when inferring interactions: 1) the non-local features of the entire image guided by actor position are obtained to model the relationship between the actor and context, and then 2) we use an object branch to generate pixel-wise interaction area prediction, where the interaction area denotes the object central area. Moreover, we also use an actor branch to get interaction prediction of the actor and propose a novel composition strategy based on center-point indexing to generate the final HOI prediction. Thanks to the usage of the non-local features and the partly-coupled property of the human-objects composition strategy, our proposed framework can detect HOI more accurately especially for complex images. Extensive experimental results show that our method achieves the state-of-the-art on the challenging V-COCO and HICO-DET benchmarks and is more robust especially in multiple persons and/or objects scenes.},
  archive      = {J_ICV},
  author       = {Kunlun Xu and Zhimin Li and Zhijun Zhang and Leizhen Dong and Wenhui Xu and Luxin Yan and Sheng Zhong and Xu Zou},
  doi          = {10.1016/j.imavis.2022.104422},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104422},
  shortjournal = {Image Vis. Comput.},
  title        = {Effective actor-centric human-object interaction detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MDCS with fully encoding the information of local shape
description for 3D rigid data matching. <em>ICV</em>, <em>121</em>,
104421. (<a href="https://doi.org/10.1016/j.imavis.2022.104421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature description is the fundamental research topic for 3D rigid data matching. However, how to achieve a good balanced performance of the local shape descriptor among descriptiveness, robustness, compactness and efficiency remains a challenging task. For this purpose, we propose a novel feature representation of 3D local surface called multi-view depth and contour signatures (MDCS). Key to MDCS descriptor is multi-view and multi-attribute description to provide a comprehensive and effective geometric information . Specifically, we first construct a repeatable Local Reference Frame (LRF) for the local surface to achieve rotation invariance. Then we integrate the depth information characterized in a local coordinate manner and the 2D contour cue derived from 3D-to-2D projection, forming the depth and contour signatures (DCS). Finally, MDCS is generated by concatenating all the DCS descriptors captured from three orthogonal view planes in the LRF into a vector. The performance of the MDCS method is evaluated on several data modalities (i.e., LiDAR, Kinect, and Space Time) with respect to Gaussian noise , varying mesh resolutions, clutter and occlusion. Experimental results and rigorous comparisons with the state-of-the-arts validate that our approach achieves the superior performance in terms of descriptiveness, robustness, compactness and efficiency. Moreover, we further demonstrate the feasibility of MDCS in matching of both LiDAR and Kinect point clouds for 3D vision applications and evaluate the generalization ability of the proposed method on real-world datasets.},
  archive      = {J_ICV},
  author       = {Zhihua Du and Yong Zuo and Jifang Qiu and Xiang Li and Yan Li and Hongxiang Guo and Xiaobin Hong and Jian Wu},
  doi          = {10.1016/j.imavis.2022.104421},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104421},
  shortjournal = {Image Vis. Comput.},
  title        = {MDCS with fully encoding the information of local shape description for 3D rigid data matching},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local information fusion network for 3D shape classification
and retrieval. <em>ICV</em>, <em>121</em>, 104405. (<a
href="https://doi.org/10.1016/j.imavis.2022.104405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide applications of 3D shapes, exploring effective 3D shape recognition algorithms has attracted much research attention. Various approaches have been proposed in recent years, within which the multi-view based methods show their promising performances. Previous multi-view based methods mainly extract global features of views by employing the well-established CNN and then explore the correlations between the view-level descriptors for 3D shape representation. However, these approaches ignore the local characteristics extraction of view images. Besides, they also lack the consideration for the relationships between image regions and feature map channels, which could provide more detailed and descriptive information to improve the discrimination of shape descriptors . To address these issues, we propose a novel Local Information Fusion Network (LIFN) for local characteristics extraction and relationships exploration based on the feature maps during the convolution process. Concretely, the Region Organization Module (ROM) is introduced for feature map reorganization and region-wise feature extraction. Besides, the Region-wise Attention (RWA) and Channel-wise Attention (CWA) are designed for region-wise and channel-wise interaction exploration, respectively. Extensive experiments on the ModelNet40 database demonstrate the superiority of our proposed network against the state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Feng Zhu and Junyu Xu and Chuanming Yao},
  doi          = {10.1016/j.imavis.2022.104405},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104405},
  shortjournal = {Image Vis. Comput.},
  title        = {Local information fusion network for 3D shape classification and retrieval},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent deep learning based ethnicity recognition and
classification using facial images. <em>ICV</em>, <em>121</em>, 104404.
(<a href="https://doi.org/10.1016/j.imavis.2022.104404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computer vision-based face image analysis has sparked considerable interest in a variety of applications such as surveillance, security, biometrics and so on. The goal of the facial analysis was to derive facial soft biometrics such as identification, gender, age, ethnicity, expression and so on. Among these, ethnicity recognition remains a hot study topic, a major aspect of society with profound linkages to a variety of environmental and social concerns. The introduction of machine learning (ML) and deep learning (ML) technologies has proven advantageous for effective ethnicity recognition and classification. In this regard, the IDL-ERCFI technique, which is based on intelligent DL, is designed in this paper. The IDL-ERCFI technique&#39;s purpose is to distinguish and classify ethnicity based on facial photos. The IDL-ERCFI technique uses face landmarks to align photos before sending them to the network. Furthermore, the proposed model employs an Exception network as a feature extractor. Because the retrieved features are high-dimensional, the feature reduction procedure employs the principal component analysis (PCA) technique, which is effective in overcoming the “curse of dimensionality.” Furthermore, the ethnicity classification procedure is carried out using an optimal kernel extreme learning machine (KELM), with parameter tuning of the KELM model carried out using the glow worm swarm optimization (GSO) technique. A complete experimental analysis is carried out to demonstrate the superiority of the IDL-ERCFI technique over the other techniques.},
  archive      = {J_ICV},
  author       = {Gurram Sunitha and K. Geetha and S. Neelakandan and Aditya Kumar Singh Pundir and S. Hemalatha and Vinay Kumar},
  doi          = {10.1016/j.imavis.2022.104404},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104404},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent deep learning based ethnicity recognition and classification using facial images},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A polar-edge context-aware (PECA) network for mirror
segmentation. <em>ICV</em>, <em>121</em>, 104402. (<a
href="https://doi.org/10.1016/j.imavis.2022.104402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Polar-Edge Contrast-Aware Network (PECA) for mirror instance segmentation in images of indoor scenes. General instance segmentation methods typically rely on the surface appearance of the object to identify the foreground from background. However, these approaches do not directly apply to mirrors as the mirror surfaces are less reliable due to reflections of the surroundings. On the other hand, the existing saliency-based mirror segmentation methods are prone to predict false positives in images with no mirrors, especially for the indoor scenes that have mirror-shape objects, such as doors and windows. In this work, we propose a novel boundary-based mirror localization method PECA that achieves both high segmentation accuracy on mirrors and low false positive rate on negative samples. PECA uses a context-aware module to extract features along the instance contour and in this way incorporates boundary information for improving mirror detection. The predicted mirror candidates are further refined with a local contrast module for the final mirror instance segmentation. PECA achieves IoU 80 . 29% on the benchmark Mirror Segmentation dataset (MSD), outperforming the state-of-the-art method MirrorNet (IoU = 78 . 95%) by 1 . 34%. It also produces a significantly smaller false positive rate (43 . 37%) than existing methods (91 . 39%) on our challenging Negative Mirror Dataset (NMD) without retraining. After training on both MSD and NMD training sets, our model further reduces the false positive rate to 0 . 08% on NMD testing set, while keeping IoU of 73 . 07% on MSD, enabling realistic real-world applications.},
  archive      = {J_ICV},
  author       = {Liqiang He and Lu Xia and Jiajia Luo and Ke Zhang and Yuyin Sun and Nan Qiao and Cheng-Hao Kuo and Sinisa Todorovic},
  doi          = {10.1016/j.imavis.2022.104402},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104402},
  shortjournal = {Image Vis. Comput.},
  title        = {A polar-edge context-aware (PECA) network for mirror segmentation},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R2Net: Residual refinement network for salient object
detection. <em>ICV</em>, <em>120</em>, 104423. (<a
href="https://doi.org/10.1016/j.imavis.2022.104423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-scale features and the fusion strategy of contextual features are the keys to salient object detection task. Previous multi-scale-based works often overlooked the completeness of features when acquiring multi-scale features. Moreover, the decoders were hard to accurately capture the salient object and refine the object&#39;s boundaries simultaneously when in a complex environment, which leads to unsatisfactory saliency maps. To address the above problems, we present a Residual Refinement Network (R 2 Net), which is composed of the Residual Pyramid Module (RPM), the Residual Fusion Module (RFM) and the Feature Optimize Module (FOM), for salient object detection. RPM integrates different feature information through different receptive fields, can not only obtain multi-scale information but also retain the local details information of features. RFM can better locate salient objects and refine the boundaries through the interweaving and fusion of multi-layer features. And FOM is designed to further refine the fused features. Furthermore, we propose a Structural Polishing (SP) loss, which better guides the network through pixel-level supervision, global supervision and boundary supervision to generate high-quality saliency maps with fine boundaries. Experimental results on 6 benchmark datasets demonstrate that the proposed method has superior performance compared with 18 state-of-the-art methods. The code and results of our method are available at https://github.com/zhangjin12138/R2Net},
  archive      = {J_ICV},
  author       = {Jin Zhang and Qiuwei Liang and Qianqian Guo and Jinyu Yang and Qing Zhang and Yanjiao Shi},
  doi          = {10.1016/j.imavis.2022.104423},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104423},
  shortjournal = {Image Vis. Comput.},
  title        = {R2Net: Residual refinement network for salient object detection},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot learning for face recognition in the presence of
image discrepancies for limited multi-class datasets. <em>ICV</em>,
<em>120</em>, 104420. (<a
href="https://doi.org/10.1016/j.imavis.2022.104420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary limitations of deep learning is data-hungry techniques. Deep learning approaches do not typically generalize well for limited datasets with fewer samples. Drawing the inspiration from the way human beings are capable of detecting a face from very few images seen in past (experience), Few-Shot Learning methods are reported in the literature. The problem is more challenging for face recognition tasks for limited dataset where the facial images are captured in various unfavorable conditions (i.e. discrepancies). To that end, in this work, we propose the Siamese Network-based Few-Shot Learning method for multi-class face recognition from a training dataset consisting of only a handful of images per class. We consider three such face image discrepancies namely, low light, head rotation and occlusion. Our work offers novelty primarily in the way the image discrepancies are overcome via Few-Shot learning while recognizing the face with reasonable accuracy. The results are obtained on our manually collected primary dataset (SCAAI_FSL) for multiple classes. Our approach presents a unique solution for face recognition tasks where the images in the training and testing dataset have different discrepancies which is the typical real-world scenario. We have experimented with various face embeddings models and demonstrated our approach for simultaneously handling multiple image discrepancies for SCAAI_FSL dataset and reported the testing accuracy of 72.72%.},
  archive      = {J_ICV},
  author       = {Ashwamegha Holkar and Rahee Walambe and Ketan Kotecha},
  doi          = {10.1016/j.imavis.2022.104420},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104420},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-shot learning for face recognition in the presence of image discrepancies for limited multi-class datasets},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neural network aided attuned scheme for gun detection in
video surveillance images. <em>ICV</em>, <em>120</em>, 104406. (<a
href="https://doi.org/10.1016/j.imavis.2022.104406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Closed Circuit Television (CCTV) cameras are installed and monitored in private and open spaces for security purposes. The video and image footages are used for rapid actions, identity, and object detection in commercial and residential security. Object and human detection require different classifications based on the features exhibited from the static/ mobile footages. This article introduces an Attuned Object Detection Scheme (AODS) for harmful object detection from CCTV inputs. The proposed scheme relies on a convolution neural network (CNN) for object detection and classification. The classification is performed based on the Object&#39;s features extracted and analyzed using CNN. The hidden layer processes are split into different feature-constraint-based analyses for identifying the Object. In the classification process, feature attenuation between the dimensional representation and segmented input is performed. Based on this process, the input is classified for hazardous objects detection. The consecutive processing layer of CNN identifies deviations in dimensional feature representation, preventing multi-object errors. The proposed scheme&#39;s performance is verified using the metrics accuracy, precision, and F1-Score.External dataset training has improved accuracy by 8.08% and reduced error and complexity by 7.47 and 8.23 percentage points, respectively, in this process. Object classification based on labels is expected to be implemented in the future.},
  archive      = {J_ICV},
  author       = {V.P. Manikandan and U. Rahamathunnisa},
  doi          = {10.1016/j.imavis.2022.104406},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104406},
  shortjournal = {Image Vis. Comput.},
  title        = {A neural network aided attuned scheme for gun detection in video surveillance images},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-view action recognition with small-scale datasets.
<em>ICV</em>, <em>120</em>, 104403. (<a
href="https://doi.org/10.1016/j.imavis.2022.104403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view action recognition refers to the task of recognizing actions observed from view-points that are unfamiliar to the system. To address the complexity of the problem, state of the art methods often rely on large-scale datasets, where the variability of viewpoints is appropriately represented. However, this comes to a significant price, in terms of computational power, time, costs, energy for both gathering data annotation and training the model. We propose a methodological pipeline that tackles the same challenges with specific focus on small-scale datasets and attention to the amount of resources required. The core idea of our method is to transfer knowledge from an intermediate, pre-trained representation, under the hypothesis that it already may implicitly incorporate relevant cues for the task. We rely on an effective domain adaptation strategy coupled with the design of a robust classifier that promotes view-invariant properties and allows us to efficiently generalise to action recognition to unseen viewpoints. In contrast to other state-of-art methods employing also alternative data modalities, our approach is purely video-based and thus has a wider field of applications. We present a thorough experimental analysis justifying the choices on the design of the pipeline, and providing a comparison with existing approaches in the two main scenarios of one-one learning and multiple view learning, where our approach provides superior performance.},
  archive      = {J_ICV},
  author       = {Gaurvi Goyal and Nicoletta Noceti and Francesca Odone},
  doi          = {10.1016/j.imavis.2022.104403},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104403},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-view action recognition with small-scale datasets},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on 2D instance segmentation based on deep neural
networks. <em>ICV</em>, <em>120</em>, 104401. (<a
href="https://doi.org/10.1016/j.imavis.2022.104401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image instance segmentation involves labeling pixels of images with classes and instances, which is one of the pivotal technologies in many domains, such as natural scenes understanding, intelligent driving, augmented reality and medical image analysis. With the power of deep learning , instance segmentation methods that use this technique have recently achieved remarkable progress. In this survey, we mainly discuss the representative 2D instance segmentation methods based on deep neural networks . Firstly, we summarize current fully-, weakly- and semi-supervised instance segmentation methods, and divide existing fully-supervised methods into three sub-categories depending on the number of stages. Based on our investigation, we conclude that currently, two-stage methods dominate the frontier of general instance segmentation; single-stage methods can achieve a better speed-accuracy trade-off, and multi-stage methods can achieve higher accuracy. Secondly, we introduce eleven datasets and three evaluation metrics for evaluating instance segmentation methods that can help researchers decide which one to choose to meet their needs and goals. Then the innovation and quantitative results of state-of-the-art general instance segmentation methods and specific instance segmentation methods (including salient instance segmentation, person instance segmentation, and amodal instance segmentation) are reviewed. In what follows, the common backbone networks are reviewed to better explain the reasons that why deep neural networks-based instance segmentation methods can achieve excellent performance. Finally, the future research directions and potential applications of instance segmentation are discussed, which can facilitates researchers to realize the existing technical difficulties and recent research hotspots.},
  archive      = {J_ICV},
  author       = {Wenchao Gu and Shuang Bai and Lingxing Kong},
  doi          = {10.1016/j.imavis.2022.104401},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104401},
  shortjournal = {Image Vis. Comput.},
  title        = {A review on 2D instance segmentation based on deep neural networks},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAMT-GAN: Realistic and accurate makeup transfer with
generative adversarial network. <em>ICV</em>, <em>120</em>, 104400. (<a
href="https://doi.org/10.1016/j.imavis.2022.104400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic makeup transfer aims to precisely transfer the makeup style from a given reference makeup face image to a source image while preserving face identity and background information. Current works have achieved promising results in makeup transfer by using deep learning techniques. However, existing methods are focused on only one or two requirements, thus cannot simultaneously achieve face identity preservation, background retention, and accurate makeup transfer. Besides, it is also difficult to acquire a pair of well-aligned face images with different makeup styles. Aimed at these problems, we propose RAMT-GAN (Realistic and Accurate Makeup Transfer Generative Adversarial Network), a GAN-based image transformation framework for achieving realistic and accurate makeup style transfer. Specifically, we utilize a dual input/output network that builds on the BeautyGAN architecture to achieve cross-domain image transformation. Then, identity preservation loss and background invariant loss are introduced in RAMT-GAN to help synthesize realistic and accurate face makeup images. Extensive experiments demonstrate that the proposed makeup transfer model can synthesize makeup faces with accurate reference style as well as maintaining the identity information and the background information.},
  archive      = {J_ICV},
  author       = {Qiang-Lin Yuan and Han-Ling Zhang},
  doi          = {10.1016/j.imavis.2022.104400},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104400},
  shortjournal = {Image Vis. Comput.},
  title        = {RAMT-GAN: Realistic and accurate makeup transfer with generative adversarial network},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive weight based on overlapping blocks network for
facial expression recognition. <em>ICV</em>, <em>120</em>, 104399. (<a
href="https://doi.org/10.1016/j.imavis.2022.104399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions which contain rich behavioral information are the primary vehicle to express emotions. It is important to analyze people&#39;s emotions with computer to achieve human-computer interaction. Feature extraction is the most important factor affecting the recognition effect. However, the existing deep learning for expression recognition is mainly based on global feature extraction. Local feature extraction provides more fine-grained information than global features. To strengthen the local discrimination of the image and pay more attention to the small targets in the local region, we propose an innovative Adaptive Weight Based on Overlapping Blocks Network (AWOBNet) for learning feature representation. First, we spatially overlay the feature maps to obtain the local features of the face . Considering the correlation and proportion between different features, we model the correlation between feature channels after overlapping blocks. Moreover, a new adaptive weighting method is developed to enhance significant features. We evaluate the proposed network on two public datasets, including the Real-World Affective Faces Database (RAFDB) and the Static Facial Expressions in the Wild (SFEW), and show the performance using the visualization method . The accuracy rates of our method obtain 89.863% on RAFDB and 62.410% on SFEW, which is significantly higher than the existing technical level.},
  archive      = {J_ICV},
  author       = {Xiaoyun Tong and Songlin Sun and Meixia Fu},
  doi          = {10.1016/j.imavis.2022.104399},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104399},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive weight based on overlapping blocks network for facial expression recognition},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot object detection via baby learning. <em>ICV</em>,
<em>120</em>, 104398. (<a
href="https://doi.org/10.1016/j.imavis.2022.104398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is proposed to overcome the problem of scarce training data in novel classes. Recently, few-shot learning has been well adopted in various computer vision tasks such as object recognition and object detection. However, the state-of-the-art (SOTA) methods have less attention to effectively reuse the information from previous stages. In this paper, we propose a new framework of few-shot learning for object detection. In particular, we adopt Baby Learning mechanism along with the multiple receptive fields to effectively utilize the former knowledge in novel domain. The propoed framework imitates the learning process of a baby through visual cues. The extensive experiments demonstrate the superiority of the proposed method over the SOTA methods on the benchmarks (improve average 7.0% on PASCAL VOC and 1.6% on MS COCO).},
  archive      = {J_ICV},
  author       = {Anh-Khoa Nguyen Vu and Nhat-Duy Nguyen and Khanh-Duy Nguyen and Vinh-Tiep Nguyen and Thanh Duc Ngo and Thanh-Toan Do and Tam V. Nguyen},
  doi          = {10.1016/j.imavis.2022.104398},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104398},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-shot object detection via baby learning},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch feature standardization network with triplet loss for
weakly-supervised video anomaly detection. <em>ICV</em>, <em>120</em>,
104397. (<a href="https://doi.org/10.1016/j.imavis.2022.104397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection refers to detecting anomalies automatically without manual labor, which is of great significance to intelligent security. With the emergence of weakly-supervised learning, the performance of video anomaly detection has been greatly advanced. However, the abnormal frames and their adjacent normal frames often make slight differences, increasing the difficulty and complexity of video anomaly detection. To address this problem, we propose a batch feature standardization module using a special standardization approach to facilitate the identification of obscure abnormal events. Meanwhile, we propose a novel strategy to refine the anomaly degree to classify the anomalous videos into two categories, i.e., weak anomalies and strong anomalies. Then the triplet loss is utilized to further improve the discriminative power of the model. Extensive experiments results demonstrate that our method works well on two benchmark datasets, and obtains a frame-level AUC 97.65% on ShanghaiTech and 84.29% on UCF-Crime, achieving comparable performance with the recent state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Shuhan Yi and Zheyi Fan and Di Wu},
  doi          = {10.1016/j.imavis.2022.104397},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104397},
  shortjournal = {Image Vis. Comput.},
  title        = {Batch feature standardization network with triplet loss for weakly-supervised video anomaly detection},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SalFBNet: Learning pseudo-saliency distribution via feedback
convolutional networks. <em>ICV</em>, <em>120</em>, 104395. (<a
href="https://doi.org/10.1016/j.imavis.2022.104395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feed-forward only convolutional neural networks (CNNs) may ignore intrinsic relationships and potential benefits of feedback connections in vision tasks such as saliency detection , despite their significant representation capabilities. In this work, we propose a feedback-recursive convolutional framework (SalFBNet) for saliency detection. The proposed feedback model can learn abundant contextual representations by bridging a recursive pathway from higher-level feature blocks to low-level layers. Moreover, we create a large-scale Pseudo-Saliency dataset to alleviate the problem of data deficiency in saliency detection. We first use the proposed feedback model to learn saliency distribution from pseudo-ground-truth. Afterwards, we fine-tune the feedback model on existing eye-fixation datasets. Furthermore, we present a novel Selective Fixation and Non-Fixation Error (sFNE) loss to facilitate the proposed feedback model to better learn distinguishable eye-fixation-based features. Extensive experimental results show that our SalFBNet with fewer parameters achieves competitive results on the public saliency detection benchmarks, which demonstrate the effectiveness of proposed feedback model and Pseudo-Saliency data.},
  archive      = {J_ICV},
  author       = {Guanqun Ding and Nevrez İmamoğlu and Ali Caglayan and Masahiro Murakawa and Ryosuke Nakamura},
  doi          = {10.1016/j.imavis.2022.104395},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104395},
  shortjournal = {Image Vis. Comput.},
  title        = {SalFBNet: Learning pseudo-saliency distribution via feedback convolutional networks},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised part segmentation via motion imitation.
<em>ICV</em>, <em>120</em>, 104393. (<a
href="https://doi.org/10.1016/j.imavis.2022.104393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given two images with the same appearance and different poses, the motion imitation is to warp the source image into the reference pose. Can the network learn the part information of the object in this process? In this paper, we take motion simulation as the pretext task to learn part information. It is based on the assumption that the generated image will be similar to the reference image only if the part information is sufficiently accurate. Different from the existing work, the key idea of this paper is to minimize the “creativity” of the motion imitation network to avoid that the network can generate an image similar to the reference image even if the part information is not learned. In particular, we investigate the complementarity of key point information and part information, and propose a joint learning module to make them benefit from each other. We constructed a multi-source fusion module to fuse missing information from multiple images to reduce the importance of the inpainting network with “creative” capabilities in the entire framework. And warping the image in the image space forces the network to directly use the original information to generate the image. In this way, the network can only move pixels based on the learned part information, not modify pixels. Compared with the existing self-supervised methods, the proposed method in this paper can produce more semantically consistent and meaningful parts without the utilization of any pre-computed information or pre-training weights. The effectivity and validity of the proposed method have verified through extensive experiments on Tai-Chi-HD and VoxCeleb datasets.},
  archive      = {J_ICV},
  author       = {Yanping Zhang and Qiaokang Liang and Kunlin Zou and Zhengwei Li and Wei Sun and Yaonan Wang},
  doi          = {10.1016/j.imavis.2022.104393},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104393},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-supervised part segmentation via motion imitation},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LP-GAN: Learning perturbations based on generative
adversarial networks for point cloud adversarial attacks. <em>ICV</em>,
<em>120</em>, 104370. (<a
href="https://doi.org/10.1016/j.imavis.2021.104370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of technologies for 3D model construction and analysis, the 3D model has been utilized in many applications, such as 3D reconstruction , object detection, and self-driving vehicles. Moreover, the development of deep learning theory has also enabled 3D model analysis to achieve revolutionary performance in many tasks. However, the adversarial robustness of these models has not received sufficient attention. Few methods focus on the generation of better adversarial samples for attacking the point cloud models that are utilized for testing the adversarial robustness. Many approaches only focus on shifting the critical points based on the perturbation metric but ignore the characteristics of adversarial points. In this work, we propose a novel generative model to produce adversarial samples. Based on the GAN , the generator can learn the representation of the adversarial points effectively, and adjust the adversarial samples to fool the point cloud models according to different point clouds. Furthermore, we hope that the adversarial samples not only fool the point cloud models but also can fool the human visual system with unnoticeable points, which is also the characteristic of the adversarial examples . Here, we propose a perception loss to improve the quality of the adversarial samples according to the original sample . Based on the generative model and perception loss, the final adversarial samples can effectively be utilized to improve the adversarial robustness of the point cloud models. Extensive attack experiments conducted on the ModelNet with state-of-the-art point cloud models demonstrate the effectiveness of our method.},
  archive      = {J_ICV},
  author       = {Qi Liang and Qiang Li and Song Yang},
  doi          = {10.1016/j.imavis.2021.104370},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104370},
  shortjournal = {Image Vis. Comput.},
  title        = {LP-GAN: Learning perturbations based on generative adversarial networks for point cloud adversarial attacks},
  volume       = {120},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and computationally faster hypermetropic
convolutional neural network for small size object detection.
<em>ICV</em>, <em>119</em>, 104396. (<a
href="https://doi.org/10.1016/j.imavis.2022.104396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has been an active area of research over the past two decades. The complexity of detecting an object increases with the increase in object speed and decrease in object size. Similar scenarios are observed in sports video analysis, vision systems of robots, driverless cars and much more. This led to the need for an efficient neural network that can detect small size objects. Further, most of the real-time applications use single board computers such as Jetson Nano, TX2, Xavier, Raspberry Pi and the like. The state-of-the-art of Deep Learning models such as YOLOv4, v3, YOLOR, YOLOX and SSD show poor run-time performance on these devices. Their lighter versions YOLOv3-tiny, YOLOv4-tiny and YOLOX-nano run nearly at 24 frames per second (fps) on Jetson Nano; however, their detection accuracy on small-sized objects is unsatisfactory. This paper focuses on developing a computationally lighter Convolutional Neural network(CNN) to detect small-sized objects efficiently. A novel hypermetropic CNN was developed to meet the above requirements. The improvement in detection is made by extracting more features from the shallow layers and transferring low-level features to the deeper layers. The network is hypermetropic because it performs well on distant objects and lags on nearby objects. The proposed model&#39;s performance is compared with the state-of-the-art models on various public datasets such as the VEDAI dataset, Visdrone dataset, and a few classes from the MS COCO and OID dataset. The proposed model shows impressive improvements in detecting small-size objects, and a 32% increase in the fps is observed on Jetson Nano.},
  archive      = {J_ICV},
  author       = {Amudhan A.N. and Sudheer A.P.},
  doi          = {10.1016/j.imavis.2022.104396},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104396},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight and computationally faster hypermetropic convolutional neural network for small size object detection},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-based person re-identification methods: A
survey and outlook of recent works. <em>ICV</em>, <em>119</em>, 104394.
(<a href="https://doi.org/10.1016/j.imavis.2022.104394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the increasing demand for public safety and the rapid development of intelligent surveillance networks, person re-identification (Re-ID) has become one of the hot research topics in the computer vision field. The main research goal of person Re-ID is to retrieve persons with the same identity from different cameras. However, traditional person Re-ID methods require manual marking of person targets, which consumes a lot of labor cost. With the widespread application of deep neural networks , many deep learning-based person Re-ID methods have emerged. Therefore, this paper is to facilitate researchers to understand the latest research results and the future trends in the field. Firstly, we summarize the studies of several recently published person Re-ID surveys and complement the latest research methods to systematically classify deep learning-based person Re-ID methods. Secondly, we propose a multi-dimensional taxonomy that classifies current deep learning-based person Re-ID methods into four categories according to metric and representation learning , including methods for deep metric learning, local feature learning, generative adversarial learning and sequence feature learning. Furthermore, we subdivide the above four categories according to their methodologies and motivations, discussing the advantages and limitations of part subcategories. Finally, we discuss some challenges and possible research directions for person Re-ID.},
  archive      = {J_ICV},
  author       = {Zhangqiang Ming and Min Zhu and Xiangkun Wang and Jiamin Zhu and Junlong Cheng and Chengrui Gao and Yong Yang and Xiaoyong Wei},
  doi          = {10.1016/j.imavis.2022.104394},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104394},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning-based person re-identification methods: A survey and outlook of recent works},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How robust are discriminatively trained zero-shot learning
models? <em>ICV</em>, <em>119</em>, 104392. (<a
href="https://doi.org/10.1016/j.imavis.2022.104392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data shift robustness has been primarily investigated from a fully supervised perspective, and robustness of zero-shot learning (ZSL) models have been largely neglected. In this paper, we present novel analyses on the robustness of discriminative ZSL to image corruptions. We subject several ZSL models to a large set of common corruptions and defenses. In order to realize the corruption analysis, we curate and release the first ZSL corruption robustness datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account the dataset characteristics, class imbalance, class transitions between seen and unseen classes and the discrepancies between ZSL and GZSL performances. Our results show that discriminative ZSL suffers from corruptions and this trend is further exacerbated by the severe class imbalance and model weakness inherent in ZSL methods. We then combine our findings with those based on adversarial attacks in ZSL, and highlight the different effects of corruptions and adversarial examples, such as the pseudo-robustness effect present under adversarial attacks. We also obtain new strong baselines for both models with the defense methods. Finally, our experiments show that although existing methods to improve robustness somewhat work for ZSL models, they do not produce a tangible effect.},
  archive      = {J_ICV},
  author       = {Mehmet Kerim Yucel and Ramazan Gokberk Cinbis and Pinar Duygulu},
  doi          = {10.1016/j.imavis.2022.104392},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104392},
  shortjournal = {Image Vis. Comput.},
  title        = {How robust are discriminatively trained zero-shot learning models?},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-trained prediction model and novel anomaly score
mechanism for video anomaly detection. <em>ICV</em>, <em>119</em>,
104391. (<a href="https://doi.org/10.1016/j.imavis.2022.104391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is important in various practical applications. This paper proposes an unsupervised method for video anomaly detection. In the core of the method lies a new prediction model for anomaly detection with novel anomaly score mechanism and self-training mechanism combined with prediction model. In the first stage, we use two conventional unsupervised anomaly detection methods to obtain pseudo normal and anomalous frames from the original unlabeled data . In the second stage, we train the prediction model with the pseudo normal frames to learn normal patterns. In the last stage, a three-branch decision module is constructed using prediction model and decision function to calculate the anomaly score of frames and update the pseudo frames for subsequent iterative training. The model then enters the second stage, until the last iterative training is completed. After several iterative training and evaluations, the optimal anomaly scores of the original unlabeled data are finally obtained, and a stable model is generated at the same time. Experimental results on four real-world video datasets demonstrate that the proposed method outperforms state-of-the-art methods without labeled data by a significant margin.},
  archive      = {J_ICV},
  author       = {Aibin Guo and Lijun Guo and Rong Zhang and Yirui Wang and Shangce Gao},
  doi          = {10.1016/j.imavis.2022.104391},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104391},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-trained prediction model and novel anomaly score mechanism for video anomaly detection},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FastNet: Fast high-resolution network for human pose
estimation. <em>ICV</em>, <em>119</em>, 104390. (<a
href="https://doi.org/10.1016/j.imavis.2022.104390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of developing efficient models for human pose estimation algorithms under computation-limited resources. In this paper, we proposed an efficient high-resolution network for human pose estimation, which named FastNet. First of all, The Lite-HRNet was constructed by using four parallel subnetworks. Each subnetwork contained multiple bottleneck block for feature extraction. The Lite-HRNet can effectively learn the high-resolution representation features of key points because of maintaining high-resolution. Secondly, instead of standard convolution, the asymmetric convolution was introduced to build an asymmetric bottleneck module. The asymmetric bottleneck module has different aspect ratios, and can be used to exact image features of keypoints with multi-scale characteristics and reduce the number of parameters. Thirdly, the waterfall module composed of multiple parallel convolutions were proposed to aggregates features with the same spatial size which can efficiently obtain delicate local representations. It retains rich spatial information and results in precise keypoint localization. Finally, the bottleneck blocks were replaced with asymmetric bottleneck modules in the third subnetwork of LiteHRNet. By which, The waterfall module is embedded into the structure of FastNet. Comprehensive experiments demonstrate that the proposed method achieves superior results on two benchmark datasets, MSCOCO and MPII. Moreover, FastNet demonstrates superior results on human pose estimation over popular lightweight networks.},
  archive      = {J_ICV},
  author       = {Yanmin Luo and Zhilong Ou and Tianjun Wan and Jing-Ming Guo},
  doi          = {10.1016/j.imavis.2022.104390},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104390},
  shortjournal = {Image Vis. Comput.},
  title        = {FastNet: Fast high-resolution network for human pose estimation},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E2E-V2SResNet: Deep residual convolutional neural networks
for end-to-end video driven speech synthesis. <em>ICV</em>,
<em>119</em>, 104389. (<a
href="https://doi.org/10.1016/j.imavis.2022.104389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speechreading which infers spoken message from a visually detected articulated facial trend is a challenging task. In this paper, we propose an end-to-end ResNet (E2E-ResNet) model for synthesizing speech signals from the silent video of a speaking individual. The model is the convolutional encoder-decoder framework which captures the frames of video and encodes into a latent space of visual features. The outputs of the decoder are spectrograms which are converted into waveforms corresponding to a speech articulated in the input video. The speech waveforms are then fed to a waveform critic used to decide the real or synthesized speech. The experiments show that the proposed E2E-V2SResNet model is apt to synthesize speech with realism and intelligibility/quality for GRID database. To further demonstrate the potentials of the proposed model, we also conduct experiments on the TCD-TIMIT database. We examine the synthesized speech in unseen speakers using three objective metrics use to measure the intelligibility, quality, and word error rate (WER) of the synthesized speech. We show that E2E-V2SResNet model outscores the competing approaches in most metrics on the GRID and TCD-TIMIT databases. By comparing with the baseline, the proposed model achieved 3.077% improvement in speech quality and 2.593% improvement in speech intelligibility.},
  archive      = {J_ICV},
  author       = {Nasir Saleem and Jiechao Gao and Muhammad Irfan and Elena Verdu and Javier Parra Fuente},
  doi          = {10.1016/j.imavis.2022.104389},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104389},
  shortjournal = {Image Vis. Comput.},
  title        = {E2E-V2SResNet: Deep residual convolutional neural networks for end-to-end video driven speech synthesis},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel micro-expression detection algorithm based on BERT
and 3DCNN. <em>ICV</em>, <em>119</em>, 104378. (<a
href="https://doi.org/10.1016/j.imavis.2022.104378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special type of facial expressions, the spontaneous micro-expressions can reveal the genuine emotions that people attempt to hide, therefore can provide potential information in criminal detection, lie detection, etc. Compared to ordinary facial expressions, micro-expressions are involuntary, transient and of low intensity. Consequently, micro-expression detection is difficult and overly dependent on expert experiences. Thus, we propose a novel micro-expression detection method based on the Bidirectional Encoder Representation from Transformers (BERT) network, namely R3D_BERT + Group, which includes the candidate segment generation module, spatio-temporal feature extraction module and grouping module. Specifically, the candidate segments are generated by the candidate segment generation module, then each candidate segment is divided into smaller time slots by the spatio-temporal feature extraction module, where spatio-temporal features are extracted through 3DCNN and BERT network. Finally, consecutive segments are merged and overlapping segments are suppressed by the grouping module to locate the position of the onset and offset frames of the micro-expression more accurately. Comprehensive experiments on CASME 2 and SDU_spotting databases firmly demonstrate the effectiveness of our method over other state-of-the-art detection methods.},
  archive      = {J_ICV},
  author       = {Ying Zhou and Yanxin Song and Lei Chen and Yang Chen and Xianye Ben and Yewen Cao},
  doi          = {10.1016/j.imavis.2022.104378},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104378},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel micro-expression detection algorithm based on BERT and 3DCNN},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing single-view 3D mesh reconstruction with the aid of
implicit surface learning. <em>ICV</em>, <em>119</em>, 104377. (<a
href="https://doi.org/10.1016/j.imavis.2022.104377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The open and ill-posed problem of single-view 3D object reconstruction has been tackled using different approaches with varying degrees of success. Previous approaches are constrained in the quality of their outputs to the in-network shape representation used. This work proposes to enhance the output of a mesh-based single-view object reconstruction model with the aid of additional implicit surface learning. Specifically, it proposes a two-branch network that learns both explicit and implicit representations in an end-to-end fashion. The explicit branch learns to deform a template spherical mesh and the implicit branch learns to regress to the real signed distance values of an implicit surface function. Practically, the mesh output of the proposed hybrid method is enhanced through a surface matching loss by which it is ensured that the mesh output aligns with the zero-isosurface of the learned implicit function. Experiments show that this hybrid approach results in meshes with a lower surface-to-surface error when compared to meshes produced from the explicit branch alone. The proposed hybrid method also outperforms the state-of-the-art mesh reconstruction methods using different evaluation metrics and compares favorably to prior work incorporating a hybrid approach. It also generalizes well to real-world images despite being trained with synthetic images.},
  archive      = {J_ICV},
  author       = {George Fahim and Khalid Amin and Sameh Zarif},
  doi          = {10.1016/j.imavis.2022.104377},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104377},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing single-view 3D mesh reconstruction with the aid of implicit surface learning},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An encoder–decoder based thermo-visible image translation
for disguised and undisguised faces. <em>ICV</em>, <em>119</em>, 104376.
(<a href="https://doi.org/10.1016/j.imavis.2022.104376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal cameras can capture images even in low light conditions. However, humans cannot recognize human faces in thermal images. Translation of thermal images to visible domain is one solution to the problem of face recognition in thermal images. Most of the research works have proposed Generative Adversarial Networks (GANs) based solutions for thermal to visible image translation. However, GAN is a heavy network that consumes huge amount of resource for thermal to visible image translation. In this paper, we propose an encoder–decoder architecture for thermal to visible image translation of human faces. Since our proposed architecture is not based on GANs, it is lightweight. The proposed method works well for both disguised and non-disguised thermal facial images. Standard comparison parameters such as Peak Signal-to-noise Ratio (PSNR), Structural Similarity Index (SSIM), and Multiscale Structural Similarity Index (MS-SSIM) are used to evaluate the quality of the generated visible images with respect to the ground truth. It has been found that our proposed architecture outperforms the current state-of-the-art image translator architectures namely pix2pix, Cycle-GAN, modified thermal to visible GAN and Dual GAN by a considerable margin for both disguised as well as non-disguised dataset.},
  archive      = {J_ICV},
  author       = {Sumit Kumar and Satish Kumar Singh and Nayaneesh Kumar Mishra and Mainak Dutta},
  doi          = {10.1016/j.imavis.2022.104376},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104376},
  shortjournal = {Image Vis. Comput.},
  title        = {An encoder–decoder based thermo-visible image translation for disguised and undisguised faces},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive ShallowNet for large scale dynamic and
spontaneous facial behaviour analysis in children. <em>ICV</em>,
<em>119</em>, 104375. (<a
href="https://doi.org/10.1016/j.imavis.2022.104375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has severely disrupted every aspect of society and left negative impact on our life. Resisting the temptation in engaging face-to-face social connection is not as easy as we imagine. Breaking ties within social circle makes us lonely and isolated, that in turns increase the likelihood of depression related disease and even can leads to death by increasing the chance of heart disease. Not only adults, children&#39;s are equally impacted where the contribution of emotional competence to social competence has long term implications. Early identification skill for facial behaviour emotions, deficits, and expression may help to prevent the low social functioning. Deficits in young children&#39;s ability to differentiate human emotions can leads to social functioning impairment. However, the existing work focus on adult emotions recognition mostly and ignores emotion recognition in children. By considering the working of pyramidal cells in the cerebral cortex, in this paper, we present progressive lightweight shallow learning for the classification by efficiently utilizing the skip-connection for spontaneous facial behaviour recognition in children. Unlike earlier deep neural networks, we limit the alternative path for the gradient at the earlier part of the network by increase gradually with the depth of the network. Progressive ShallowNet is not only able to explore more feature space but also resolve the over-fitting issue for smaller data, due to limiting the residual path locally, making the network vulnerable to perturbations. We have conducted extensive experiments on benchmark facial behaviour analysis in children that showed significant performance gain comparatively.},
  archive      = {J_ICV},
  author       = {Abdul Qayyum and Imran Razzak and Nour Moustafa and Moona Mazher},
  doi          = {10.1016/j.imavis.2022.104375},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104375},
  shortjournal = {Image Vis. Comput.},
  title        = {Progressive ShallowNet for large scale dynamic and spontaneous facial behaviour analysis in children},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LTST: Long-term segmentation tracker with memory attention
network. <em>ICV</em>, <em>119</em>, 104374. (<a
href="https://doi.org/10.1016/j.imavis.2022.104374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent interest in the combination of visual object tracking (VOT) and video object segmentation (VOS) has yielded rapid progress. However, existing segmentation methods are still restricted to the target model created in the first frame, which leads to the lack of long-term adaptability. To overcome this limitation, we propose a novel long-term segmentation tracker LTST leveraging memory attention network to achieve the effect of online learning without additional training. Specifically, we first combine a discriminative correlation filter with a matching-based paradigm for the segmentation task, then we develop a memory attention network based on partial cost volume to extract relevant historical information and dynamically reform the segmentation template. Moreover, we extend our LTST for long-term tracking by introducing a multi-scale verification network to identify tracking failures, and a global detector to re-locate the missing target. Experimental results on VOT-LT2018, VOT-LT2019, LaSOT and TLP benchmarks show that our proposed tracker achieves comparable performance to the state-of-the-art long-term tracking algorithms.},
  archive      = {J_ICV},
  author       = {Lang Yu and Baojun Qiao and Huanlong Zhang and Junyang Yu and Xin He},
  doi          = {10.1016/j.imavis.2022.104374},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104374},
  shortjournal = {Image Vis. Comput.},
  title        = {LTST: Long-term segmentation tracker with memory attention network},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater bubble plume image generative model based on
noise prior and multi conditional labels. <em>ICV</em>, <em>119</em>,
104373. (<a href="https://doi.org/10.1016/j.imavis.2022.104373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characteristics of underwater bubble plume dispersion behavior are important to assess and manage potential risks of underwater gas pipelines . In order to generate sufficient image samples for underwater bubble plume research based on artificial intelligence, we try to design an optimized generative adversarial network. In this paper, a novel underwater bubble plume image generative model that combines noise prior and multi conditional labels is proposed and evaluated using different testing dataset . In the proposed framework, the noise prior which contains the image attributes and the category features are trained with the help of VAEs , prior noise and multi conditional labels are utilized to construct the generative and discriminative models. Finally, 3 types of the underwater bubble plume images generated from the experiment is contrasted and evaluated. The experimental results show that，compared with the existing methods text2img and CGANs，the FID values of the generated images are reduced by 97.4% and 22.8% on SUIM dataset, and the FID values are reduced by 97.4% and 22.8% on BUBBLE dataset, our proposed framework achieves satisfactory and promising performance.},
  archive      = {J_ICV},
  author       = {Xue Yang and Shiming Sun and Wei Chen and Jing Liu},
  doi          = {10.1016/j.imavis.2022.104373},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104373},
  shortjournal = {Image Vis. Comput.},
  title        = {Underwater bubble plume image generative model based on noise prior and multi conditional labels},
  volume       = {119},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic keypoint selection network for 6DoF pose
estimation. <em>ICV</em>, <em>118</em>, 104372. (<a
href="https://doi.org/10.1016/j.imavis.2022.104372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6 DoF pose estimation problem aims to estimate the rotation and translation parameters between two coordinates, such as object world coordinate and camera world coordinate. Although some advances are made with the help of deep learning , how to full use scene information is still a problem. Prior works tackle the problem by pixel-wise feature fusion but need to randomly select numerous points from images, which can not satisfy the demands of fast inference simultaneously and accurate pose estimation. In this work, we present a novel deep neural network based on dynamic keypoint selection designed for 6DoF pose estimation from a single RGBD image. Our network includes three parts, instance semantic segmentation , edge points detection and 6DoF pose estimation. Given an RGBD image, our network is trained to predict pixel category and the translation to edge points and center points. Then, a least-square fitting manner is applied to estimate the 6DoF pose parameters. Specifically, we propose a dynamic keypoint selection algorithm to choose keypoints from the foreground feature map. It allows us to leverage geometric and appearance information. During 6DoF pose estimation, we utilize the instance semantic segmentation result to filter out background points and only use foreground points to finish edge points detection and 6DoF pose estimation. Experiments on two commonly used 6DoF estimation benchmark datasets, YCB-Video and LineMoD, demonstrate that our method outperforms the state-of-the-art methods and achieves significant improvements over other same category methods time efficiency.},
  archive      = {J_ICV},
  author       = {Haowen Sun and Taiyong Wang and Enlin Yu},
  doi          = {10.1016/j.imavis.2022.104372},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104372},
  shortjournal = {Image Vis. Comput.},
  title        = {A dynamic keypoint selection network for 6DoF pose estimation},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PU-GACNet: Graph attention convolution network for point
cloud upsampling. <em>ICV</em>, <em>118</em>, 104371. (<a
href="https://doi.org/10.1016/j.imavis.2021.104371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-scanned point clouds are often sparse and non-uniform. To conquer the problem, researchers propose point cloud upsampling techniques, whose efficiency and effectiveness heavily rely on their feature extractors and feature expanders used therein. Therefore, in this paper, to capture the global and local structured features of point clouds, we first design a Graph Attention Convolution (GAC) module as a feature extractor by assigning different attentional weights to combine spatial positions and feature attributes dynamically. Furthermore, we propose an Edge-aware NodeShuffle (ENS) module as a feature expander to upsampling point features smoothly, in an effort to better preserve local geometric details and emphasize local edges. Finally, we combine GAC module with ENS module into a novel point cloud upsampling pipeline, named as PU-GACNet. Extensive experiments as well as theoretical analysis demonstrate this pipeline significantly outperforms previous methods in network performance for 3D point cloud upsampling, to obtain more efficient inference with much fewer parameters.},
  archive      = {J_ICV},
  author       = {Bing Han and Xinyun Zhang and Shuang Ren},
  doi          = {10.1016/j.imavis.2021.104371},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104371},
  shortjournal = {Image Vis. Comput.},
  title        = {PU-GACNet: Graph attention convolution network for point cloud upsampling},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deformable convolutions in multi-view stereo. <em>ICV</em>,
<em>118</em>, 104369. (<a
href="https://doi.org/10.1016/j.imavis.2021.104369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-View Stereo (MVS) is a key process in the photogrammetry workflow. It is responsible for taking the camera&#39;s views and finding the maximum number of matches between the images yielding a dense point cloud of the observed scene. Since this process is based on the matching between images it greatly depends on the ability of features matching throughout different images. To improve the matching performance several researchers have proposed the use of Convolutional Neural Networks (CNNs) to solve the MVS problem. Despite the progress in the MVS problem with the usage of CNNs, the Video RAM (VRAM) consumption within these approaches is usually far greater than classical methods, that rely more on RAM, which is cheaper to expand than VRAM. This work then follows the progress made in CasMVSNet in the reduction of GPU memory usage, and further study the changes in the feature extraction process. The Average Group-wise Correlation is used in the cost volume generation, to reduce the number of channels in the cost volume, yielding a reduction in GPU memory usage without noticeable penalties in the result. The deformable convolutions are applied in the feature extraction network to augment the spatial sampling locations with learning offsets, without additional supervision, to further improve the network&#39;s ability to model transformations. The impact of these changes is measured using quantitative and qualitative tests using the DTU and the Tanks and Temples datasets. The modifications reduced the GPU memory usage by 32% and improved the completeness by 9% with a penalty of 6.6% in accuracy on the DTU dataset.},
  archive      = {J_ICV},
  author       = {Juliano Emir Nunes Masson and Marcelo Roberto Petry and Daniel Ferreira Coutinho and Leonardo de Mello Honório},
  doi          = {10.1016/j.imavis.2021.104369},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104369},
  shortjournal = {Image Vis. Comput.},
  title        = {Deformable convolutions in multi-view stereo},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep-shallow and global–local multi-feature fusion network
for photometric stereo. <em>ICV</em>, <em>118</em>, 104368. (<a
href="https://doi.org/10.1016/j.imavis.2021.104368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering 3D surfaces based on the photometric stereo is a challenging task, due to the non-Lambertian surface of real-world objects. Although much effort has been made to address this issue, existing photometric stereo methods based on deep learning did not fully consider the influence of global–local features and deep-shallow features on the training process. How to combine multi-feature into a framework effectively to overcome their drawbacks has not been explored. Therefore, we propose a novel multi-feature fusion photometric stereo network (MF-PSN), focusing on both local–global and deep-shallow features fusion. Global–local feature fusion maintains the features under different illuminations and the most salient features of all illuminations, thereby effectively uses the information of each input image. Deep-shallow feature fusion keeps the features from deep and shallow layers with different receptive fields, which effectively improves the accuracy and robustness of the model. Experiments show that multi-feature fusion can make full use of the information of the input image to achieve a better reconstruction of surface normals of the object. Extensive ablation studies and experiments on the widely used DiLiGenT benchmark dataset have well verified the effectiveness of our proposed method. In addition, testing on the Gourd &amp; Apple dataset and Light Stage Data Gallery verifies the generalization of our method.},
  archive      = {J_ICV},
  author       = {Yanru Liu and Yakun Ju and Muwei Jian and Feng Gao and Yuan Rao and Yeqi Hu and Junyu Dong},
  doi          = {10.1016/j.imavis.2021.104368},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104368},
  shortjournal = {Image Vis. Comput.},
  title        = {A deep-shallow and global–local multi-feature fusion network for photometric stereo},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View knowledge transfer network for multi-view action
recognition. <em>ICV</em>, <em>118</em>, 104357. (<a
href="https://doi.org/10.1016/j.imavis.2021.104357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As many data in practical applications occur or can be captured in multiple views form, multi-view action recognition has received much attention recently, due to utilizing certain complementary and heterogeneous information in various views to promote the downstream task. However, most existing methods assume that multi-view data is complete, which may not always be met in real-world applications.To this end, in this paper, a novel View Knowledge Transfer Network (VKTNet) is proposed to handle multi-view action recognition, even when some views are incomplete. Specifically, the view knowledge transferring is utilized using conditional generative adversarial network(cGAN) to reproduce each view&#39;s latent representation, conditioning on the other view&#39;s information. As such, the high-level semantic features are effectively extracted to bridge the semantic gap between two different views. In addition, in order to efficiently fuse the decision result achieved by each view, a Siamese Scaling Network(SSN) is proposed instead of simply using a classifier. Experimental results show that our model achieves the superiority performance, on three public datasets, against others when all the views are available. Meanwhile, the degradation of performance is avoided under the case that some views are missing.},
  archive      = {J_ICV},
  author       = {Zixi Liang and Ming Yin and Junli Gao and Yicheng He and Weitian Huang},
  doi          = {10.1016/j.imavis.2021.104357},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104357},
  shortjournal = {Image Vis. Comput.},
  title        = {View knowledge transfer network for multi-view action recognition},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial temporal and channel aware network for video-based
person re-identification. <em>ICV</em>, <em>118</em>, 104356. (<a
href="https://doi.org/10.1016/j.imavis.2021.104356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging computer vision task , video-based person Re-IDentification (Re-ID) has been intensively studied, and recent works have achieved a series of satisfactory results by capturing spatial temporal relationships. However, extensive observations have found that the same feature vector generated by a convolutional neural network contains considerable redundant information in the channel dimension. This issue is seldom investigated. A Spatial Temporal and Channel Aware Network (STCAN) for video-based ReID is studied in this paper. It jointly considers spatial temporal and channel information. Firstly, the Spatial Attention Enhanced (SAE) convolutional network is developed as the backbone network to learn spatial enhanced features from video frames. Secondly, a Channel Segmentation and Group Shuffle (CSGS) convolution module is designed to jointly address temporal and channel relations. Finally, a Two Branch Weighted Fusion (TBWF) mechanism is introduced to enhance the robustness of the Re-ID network by fusing the output of the SAE backbone network and CSGS. Comprehensive experiments are conducted on three large-scale datasets MARS, LSVID, and P-DESTRE. The experimental results imply that the STCAN can effectively improve the performance of video-based Re-ID and outperform several state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Hui Fu and Ke Zhang and Haoyu Li and Jingyu Wang and Zhen Wang},
  doi          = {10.1016/j.imavis.2021.104356},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104356},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial temporal and channel aware network for video-based person re-identification},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial to special issue on cross-media learning for
visual question answering. <em>ICV</em>, <em>118</em>, 104355. (<a
href="https://doi.org/10.1016/j.imavis.2021.104355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Shaohua Wan and Chen Chen and Alexandros Iosifidis},
  doi          = {10.1016/j.imavis.2021.104355},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104355},
  shortjournal = {Image Vis. Comput.},
  title        = {Editorial to special issue on cross-media learning for visual question answering},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation pedestrian re-identification
based on an improved dissimilarity space. <em>ICV</em>, <em>118</em>,
104354. (<a href="https://doi.org/10.1016/j.imavis.2021.104354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian re-identification is a key and challenging research topic in intelligent security applications. Because of the need for large-scale manual labeling, pedestrian re-identification methods based on supervised learning cannot be widely used in practical applications. Research on unsupervised pedestrian re-identification has therefore become a hot topic. The DMMD model published in ECCV2020 successfully applied the dissimilarity space to unsupervised pedestrian re-identification. Based on the analysis of the deficiencies of DMMD, a new strong baseline model is composed, an improved dissimilarity space is constructed, a new transfer learning optimization method is proposed, and a time-space-appearance constraint is proposed. The test results show that the R1 accuracy of our model is improved by 21.4% compared with DMMD in the experiment from Market1501 to DukeMTMC.},
  archive      = {J_ICV},
  author       = {Xiaofeng Yang and Qianshan Wang and Wenkuan Li and Zihao Zhou and Haifang Li},
  doi          = {10.1016/j.imavis.2021.104354},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104354},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised domain adaptation pedestrian re-identification based on an improved dissimilarity space},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Does explainable machine learning uncover the black box in
vision applications? <em>ICV</em>, <em>118</em>, 104353. (<a
href="https://doi.org/10.1016/j.imavis.2021.104353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) in general and deep learning (DL) in particular has become an extremely popular tool in several vision applications (like object detection, super resolution, segmentation, object tracking etc.). Almost in parallel, the issue of explainability in ML (i.e. the ability to explain/elaborate the way a trained ML model arrived at its decision) in vision has also received fairly significant attention from various quarters. However, we argue that the current philosophy behind explainable ML suffers from certain limitations, and the resulting explanations may not meaningfully uncover black box ML models. To elaborate our assertion, we first raise a few fundamental questions which have not been adequately discussed in the corresponding literature. We also provide perspectives on how explainablity in ML can benefit by relying on more rigorous principles in the related areas.},
  archive      = {J_ICV},
  author       = {Manish Narwaria},
  doi          = {10.1016/j.imavis.2021.104353},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104353},
  shortjournal = {Image Vis. Comput.},
  title        = {Does explainable machine learning uncover the black box in vision applications?},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual guidance enhanced network for light field salient
object detection. <em>ICV</em>, <em>118</em>, 104352. (<a
href="https://doi.org/10.1016/j.imavis.2021.104352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection models using light field data as input have not been thoroughly explored. Existing deep saliency models usually treat multi-focus images as independent information and extract their features separately, which may be cumbersome and over-rely on well-designed network structure. Besides, they do not fully explore the cross-modal complementarity and cross-level continuity of information, and rarely consider edge cues. Based on the above observations, in this paper, we investigate a novel Dual Guidance Enhanced Network (DGENet), which considers both spatial content and explicit boundary cues. Specifically, DGENet contains two key modules, i.e., the recurrent global-guided focus module (RGFM) and the boundary-guided semantic accumulation module (BSAM). These two modules are composed of multiple units, and the units in each module are not independent of each other. RGFM is used to distill out effective squeezed information of focal slices and RGB images between different levels. The learned global context features guide the network to focus on the salient region via a progressive reverse attention-driven strategy. Furthermore, BSAM introduces salient edge features to guide the accumulation of salient object features to generate salient maps with sharp boundaries. Extensive experiments on three challenging light field datasets demonstrate that our DGENet is superior to cutting-edge 2D, 3D and 4D methods.},
  archive      = {J_ICV},
  author       = {Yanhua Liang and Guihe Qin and Minghui Sun and Jun Qin and Jie Yan and Zhonghan Zhang},
  doi          = {10.1016/j.imavis.2021.104352},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104352},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual guidance enhanced network for light field salient object detection},
  volume       = {118},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Double cross-modality progressively guided network for RGB-d
salient object detection. <em>ICV</em>, <em>117</em>, 104351. (<a
href="https://doi.org/10.1016/j.imavis.2021.104351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of depth sensors, the use of RGB and depth(D) information for salient object detection (SOD) has been explored extensively in recent years. However, the depth quality from the different scenes usually varies, leading to fusing and achieving complementary between RGB and low-quality depth is still a challenging problem. In this paper, we first design a Double Dilated Merge Module (DDMM) to extract comprehensive and beneficial high-level cross-modality features and explore further global context information at multi-scales to obtain a coarse saliency map. Then, we propose a Cross-Modality Enhance Module (CMEM) to enhance cross-modality features compatibility and fuse them with the previous predicted coarse saliency map to generate a more accurate saliency map. Furthermore, we introduce a Distribution-Region Combination Loss (DRcom Loss) to optimize our proposed Double Cross-Modality Progressively Guided Network (DCPGNet) in a coarse-to-fine manner. DCPGNet achieves satisfactory performance on five public benchmarks compared with recent state-of-the-art algorithms.},
  archive      = {J_ICV},
  author       = {Cuili Yao and Lin Feng and Yuqiu Kong and Shengming Li and Hang Li},
  doi          = {10.1016/j.imavis.2021.104351},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104351},
  shortjournal = {Image Vis. Comput.},
  title        = {Double cross-modality progressively guided network for RGB-D salient object detection},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized hardmining loss for face recognition.
<em>ICV</em>, <em>117</em>, 104343. (<a
href="https://doi.org/10.1016/j.imavis.2021.104343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For face recognition using deep learning architectures, loss functions have become a topic of research these days. This is because of the fact that when the discriminative ability of the loss function increases, then the face recognition accuracy increases. Hardmining loss is one such generic loss function that can be used with any basic loss function and has the ability to enhance the face recognition accuracy of the given basic loss function. Hardmining loss achieves the improvement by introducing greater penalty for hard examples. However, the problem with Hardmining loss is that the easy examples are allocated loss value near to zero. This limits the contribution of the easy examples specially in the later training stages. We therefore propose an improved Hardmining loss called the Regularized Hardmining loss. The Regularized Hardmining loss allocates a reasonable loss value to the easier examples as well. It thus helps easy examples maintain their contribution in the later training stage while still giving relatively greater penalty for hard examples thus preserving the property of the Hardmining loss. The Regularized Hardmining loss fine-tunes the behavior of Hardmining loss for better performance. The results of Regularized Hardmining loss when applied with Cross Entropy loss show an increased accuracy of face recognition from 93.78% to 95.55% on LFW dataset.},
  archive      = {J_ICV},
  author       = {Nayaneesh Kumar Mishra and Satish Kumar Singh},
  doi          = {10.1016/j.imavis.2021.104343},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104343},
  shortjournal = {Image Vis. Comput.},
  title        = {Regularized hardmining loss for face recognition},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facial expression recognition using densely connected
convolutional neural network and hierarchical spatial attention.
<em>ICV</em>, <em>117</em>, 104342. (<a
href="https://doi.org/10.1016/j.imavis.2021.104342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to eliminating the impact of redundant information from emotional-unrelated regions on facial expression recognition (FER). To this end, a densely connected convolutional neural network with hierarchical spatial attention is proposed. Specifically, it can adaptively locate salient regions and focus on the emotional related features so that the facial expressions can be represented more efficiently. This superior performance is also verified by some experiments. Experimental results reveal that the proposed method can distinguish facial expression more accurately than existing state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Chenquan Gan and Junhao Xiao and Zhangyi Wang and Zufan Zhang and Qingyi Zhu},
  doi          = {10.1016/j.imavis.2021.104342},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104342},
  shortjournal = {Image Vis. Comput.},
  title        = {Facial expression recognition using densely connected convolutional neural network and hierarchical spatial attention},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FMD-yolo: An efficient face mask detection method for
COVID-19 prevention and control in public. <em>ICV</em>, <em>117</em>,
104341. (<a href="https://doi.org/10.1016/j.imavis.2021.104341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is a world-wide epidemic and efficient prevention and control of this disease has become the focus of global scientific communities. In this paper, a novel face mask detection framework FMD-Yolo is proposed to monitor whether people wear masks in a right way in public, which is an effective way to block the virus transmission. In particular, the feature extractor employs Im-Res2Net-101 which combines Res2Net module and deep residual network, where utilization of hierarchical convolutional structure, deformable convolution and non-local mechanisms enables thorough information extraction from the input. Afterwards, an enhanced path aggregation network En-PAN is applied for feature fusion, where high-level semantic information and low-level details are sufficiently merged so that the model robustness and generalization ability can be enhanced. Moreover, localization loss is designed and adopted in model training phase, and Matrix NMS method is used in the inference stage to improve the detection efficiency and accuracy. Benchmark evaluation is performed on two public databases with the results compared with other eight state-of-the-art detection algorithms. At IoU = 0.5 level, proposed FMD-Yolo has achieved the best precision AP 50 of 92.0% and 88.4% on the two datasets, and AP 75 at IoU = 0.75 has improved 5.5% and 3.9% respectively compared with the second one, which demonstrates the superiority of FMD-Yolo in face mask detection with both theoretical values and practical significance.},
  archive      = {J_ICV},
  author       = {Peishu Wu and Han Li and Nianyin Zeng and Fengping Li},
  doi          = {10.1016/j.imavis.2021.104341},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104341},
  shortjournal = {Image Vis. Comput.},
  title        = {FMD-yolo: An efficient face mask detection method for COVID-19 prevention and control in public},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving image captioning with pyramid attention and
SC-GAN. <em>ICV</em>, <em>117</em>, 104340. (<a
href="https://doi.org/10.1016/j.imavis.2021.104340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing image captioning models mainly use global attention, which represents the whole image features, local attention, representing the object features, or a combination of them; there are few models to integrate the relationship information between various object regions of the image. But this relationship information is also very instructive for caption generation. For example, if a football appears, there is a high probability that the image also contains people near the football. In this article, the relationship feature is embedded into the global-local attention to constructing a new Pyramid Attention mechanism, which can explore the internal visual and semantic relationship between different object regions. Besides, to alleviate the exposure bias problem and make the training process more efficient, we propose a new method to apply the Generative Adversarial Network into sequence generation. The greedy decoding method is used to generate an efficient baseline reward for self-critical training. Finally, experiments on MSCOCO dataset show that the model can generate more accurate and vivid captions and outperforms many recent advanced models in various prevailing evaluation metrics on both local and online test sets.},
  archive      = {J_ICV},
  author       = {Tianyu Chen and Zhixin Li and Jingli Wu and Huifang Ma and Bianping Su},
  doi          = {10.1016/j.imavis.2021.104340},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104340},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving image captioning with pyramid attention and SC-GAN},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric feature statistics histogram for both real-valued
and binary feature representations of 3D local shape. <em>ICV</em>,
<em>117</em>, 104339. (<a
href="https://doi.org/10.1016/j.imavis.2021.104339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D local feature description is now at the core of many 3D vision technologies. However, most of the existing 3D feature descriptors can&#39;t strike a balance among descriptiveness, robustness, compactness, and efficiency. To overcome the challenges, we propose a real-valued 3D local feature descriptor named Geometric Feature Statistics Histogram (GFSH) and its binary extension descriptor named B-GFSH. A GFSH descriptor first constructs an improved-weighted covariance matrix to solve a stable and reliable Local Reference Frame (LRF), and then achieves a comprehensive description of the 3D local surface by performing statistics on multiple geometric distribution features, namely voxel density, voxel centroid, and projection density. A particular trait of our GFSH descriptor is its seamless extension to the binary representation to reduce storage consumption and accelerate feature matching. For each sub-feature of GFSH, B-GFSH respectively adopts the corresponding binarization strategy, i.e., improved Gray code quantization, thresholding based on coordinates, and neighbor comparison. Extensive experiments on six public datasets prove that both GFSH and B-GFSH have high descriptiveness, strong robustness, and fast real-time performance. In addition, B-GFSH further has the characteristics of fast matching speed, low memory footprint , and high compactness. Finally, we conduct 3D scene registration and 3D object recognition experiments to visually demonstrate the actual effectiveness of GFSH and B-GFSH.},
  archive      = {J_ICV},
  author       = {Linbo Hao and Huaming Wang},
  doi          = {10.1016/j.imavis.2021.104339},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104339},
  shortjournal = {Image Vis. Comput.},
  title        = {Geometric feature statistics histogram for both real-valued and binary feature representations of 3D local shape},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning an augmentation strategy for sparse datasets.
<em>ICV</em>, <em>117</em>, 104338. (<a
href="https://doi.org/10.1016/j.imavis.2021.104338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited quantity of training data can hamper supervised machine learning methods, that generally need large amounts of data to avoid overfitting. Data augmentation has a long history of use with machine learning algorithms and is a straightforward method to overcome overfitting and improve model generalisation. However, data augmentation schemes are typically designed by hand and demand substantial domain knowledge to create suitable data transformations. This paper introduces a GAN based method that automatically learns an augmentation strategy appropriate for sparse datasets and can improve pixel-level semantic segmentation accuracy by filling the gaps in the training set. Our method can also be combined with other augmentation techniques to further improve performance. We evaluate the proposed method&#39;s feasibility on four datasets and three semantic segmentation models, leading to improvement in the mean intersection-over-union (mIoU) score of between 0.5 and 14 percentage points, under different circumstances.},
  archive      = {J_ICV},
  author       = {Renato B. Arantes and George Vogiatzis and Diego R. Faria},
  doi          = {10.1016/j.imavis.2021.104338},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104338},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning an augmentation strategy for sparse datasets},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention guided contextual feature fusion network for
salient object detection. <em>ICV</em>, <em>117</em>, 104337. (<a
href="https://doi.org/10.1016/j.imavis.2021.104337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Convolutional Neural Network (CNN) has been widely used in various visual tasks because of its powerful feature extraction ability. Salient object detection methods based on CNN have also achieved great performance. Although a large number of feature information can be obtained through CNN, the key to improve the quality of the saliency maps is how to make full use of the high and low-level features and their relationships. Some previous works merged high and low-level features without processing the features, which resulted in the blurring of the saliency map, and even the inability to distinguish the foreground from the background in a complex environment. In order to solve the above problem, we propose an Attention guided Contextual Feature Fusion Network (ACFFNet) for salient object detection. There are mainly three modules in the proposed ACFFNet, including the Multi-field Channel Attention (MCA) module, Contextual Feature Fusion (CFF) module, and the feature Self-Refinement (SR) module. The MCA module selects features from different receptive fields, the CFF module can efficiently aggregate contextual features, and the SR module is able to repair the holes in the prediction maps caused by the contradictory response of different layers. In addition, we propose a Cross-Consistency Enhancement (CCE) loss to guide the network to focus on more detailed information and highlight the difference between foreground and background. Experimental results on six benchmark datasets show that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jin Zhang and Yanjiao Shi and Qing Zhang and Liu Cui and Ying Chen and Yugen Yi},
  doi          = {10.1016/j.imavis.2021.104337},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104337},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention guided contextual feature fusion network for salient object detection},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge supervision and multi-scale cost volume for stereo
matching. <em>ICV</em>, <em>117</em>, 104336. (<a
href="https://doi.org/10.1016/j.imavis.2021.104336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, methods based on Convolutional Neural Network have achieved huge progress in stereo matching . However, it is still difficult to find accurate matching points in inherently ill-posed regions (e.g., weak texture areas and around object edges), in which the accuracy of disparity estimate can be improved by the corresponding geometric constraints. To tackle this problem, we innovatively generate the depth ground-truth boundary dataset by mining the instance segmentation and semantic segmentation datasets and propose RDNet, which incorporates edge cues into stereo matching. The network learns geometric information through a separate processing branch edge stream, which can process feature information in parallel with the stereo stream. The edge stream removes noise and only focuses on processing the relevant boundary information. Besides, we introduce a multi-scale cost volume in hierarchical cost aggregation to enlarge the receptive fields and capture structural and global representations that can significantly improve the ability of scene understanding and disparity estimation accuracy. Moreover, a disparity refinement network with several dilated convolutions is applied to further improve the accuracy of the final disparity estimation. The proposed method is evaluated on Sceneflow, KITTI 2015 and KITTI 2012 benchmark datasets, and the qualitative and quantitative results demonstrate that the proposed RDNet significantly achieves the state-of-the-art stereo matching performance.},
  archive      = {J_ICV},
  author       = {Xiaowei Yang and Zhiguo Feng and Yong Zhao and Guiying Zhang and Lin He},
  doi          = {10.1016/j.imavis.2021.104336},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104336},
  shortjournal = {Image Vis. Comput.},
  title        = {Edge supervision and multi-scale cost volume for stereo matching},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IRANet: Identity-relevance aware representation for
cloth-changing person re-identification. <em>ICV</em>, <em>117</em>,
104335. (<a href="https://doi.org/10.1016/j.imavis.2021.104335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person re-identification methods mainly focus on searching the target person across disjoint camera views in a short period of time. With this setting, these methods rely on the assumption that both query and gallery images of the same person have the same clothing. To tackle the challenges of clothing changes over a long duration, this paper proposes an identity-relevance aware neural network (IRANet) for cloth-changing person re-identification. Specifically, a human head detection module is designed to localize the human head part with the help of the human parsing estimation. The detected human head part contains abundant identity information, including facial features and head type. Then, raw person images in conjunction with detected head areas are respectively transformed into feature representation with the feed-forward network. The learned features of raw person images contain more attributes of global context, meanwhile the learned features of head areas contain more identity-relevance attributes. Finally, a head-guided attention module is employed to guide the global features learned by raw person images to focus more on the identity-relevance head areas. The proposed method achieves mAP accuracy of 25.4% on the Celeb-reID-light dataset, 19.0% on the Celeb-reID dataset, and 53.0% (Cloth-changing setting) on the PRCC dataset, which shows the superiority of our approach for the cloth-changing person re-identification task.},
  archive      = {J_ICV},
  author       = {Wei Shi and Hong Liu and Mengyuan Liu},
  doi          = {10.1016/j.imavis.2021.104335},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104335},
  shortjournal = {Image Vis. Comput.},
  title        = {IRANet: Identity-relevance aware representation for cloth-changing person re-identification},
  volume       = {117},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
