<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nn---406">NN - 406</h2>
<ul>
<li><details>
<summary>
(2022a). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>156</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00440-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00440-3},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). CURRENT EVENTS. <em>NN</em>, <em>156</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00439-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00439-7},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {CURRENT EVENTS},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive graph convolutional clustering network with optimal
probabilistic graph. <em>NN</em>, <em>156</em>, 271–284. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph convolutional network (GCN)-based clustering approaches have achieved the impressive performance due to strong ability of exploiting the topological structure . The adjacency graph seriously affects the clustering performance, especially for non-graph data. Existing approaches usually conduct two independent steps, i.e., constructing a fixed graph structure and then graph embedding representation learning by GCN. However, the constructed graph structure may be unreliable one due to noisy data, resulting in sub-optimal graph embedding representation. In this paper, we propose an adaptive graph convolutional clustering network (AGCCN) to alternatively learn the similarity graph structure and node embedding representation in a unified framework. Our AGCCN learns the weighted adjacency graph adaptively from the node representations by solving the optimization problem of graph learning, in which adaptive and optimal neighbors for each sample are assigned with probabilistic way according to local connectivity. Then, the attribute feature extracted by parallel Auto-Encoder (AE) module is fused into the input of adaptive graph convolution module layer-by-layer to learn the comprehensive node embedding representation and strengthen its representation ability. This also skillfully alleviates the over-smoothing problem of GCN. To further improve the discriminant ability of node representation, a dual self-supervised clustering mechanism is designed to guide model optimization with pseudo-labels information. Extensive experimental results on various real-world datasets consistently show the superiority and effectiveness of the proposed deep graph clustering method .},
  archive      = {J_NN},
  author       = {Jiayi Zhao and Jipeng Guo and Yanfeng Sun and Junbin Gao and Shaofan Wang and Baocai Yin},
  doi          = {10.1016/j.neunet.2022.09.017},
  journal      = {Neural Networks},
  pages        = {271-284},
  shortjournal = {Neural Netw.},
  title        = {Adaptive graph convolutional clustering network with optimal probabilistic graph},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered h∞ consensus for uncertain nonlinear systems
using integral sliding mode based adaptive dynamic programming.
<em>NN</em>, <em>156</em>, 258–270. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a robust optimal consensus problem for uncertain nonlinear multi-agent systems, where the uncertainties include both input and external disturbances . Adaptive distributed observer, integral sliding mode control and H ∞ H∞ adaptive dynamic programming are integrated to obtain a sub-optimal control protocol for each follower. Firstly, an adaptive distributed observer is designed for state estimation of the leader, which serves as the reference of the ADP algorithm . Then, an H ∞ H∞ ADP algorithm is presented to make each follower track the reference in real-time. An integral sliding manifold-based discontinuous control is designed to eliminate the matched uncertainty, and continuous control is obtained by solving the Hamilton–Jacobi–Isaac equation under the H ∞ H∞ tracking framework. Two event-triggered rules are developed to relieve the communication pressure. For simplicity, a critic-only structure is used to numerically implement the proposed algorithm, and a concurrent learning technique is employed to update weights of neural networks . All signals in the closed-loop system are proven to be uniformly ultimately bounded. Finally, a simulation is conducted to demonstrate demonstrates the effectiveness of the method.},
  archive      = {J_NN},
  author       = {Zitao Chen and Kairui Chen and Si-Zhe Chen and Yun Zhang},
  doi          = {10.1016/j.neunet.2022.09.024},
  journal      = {Neural Networks},
  pages        = {258-270},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered h∞ consensus for uncertain nonlinear systems using integral sliding mode based adaptive dynamic programming},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature extraction framework based on contrastive learning
with adaptive positive and negative samples. <em>NN</em>, <em>156</em>,
244–257. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction is an efficient approach for alleviating the issue of dimensionality in high-dimensional data. As a popular self-supervised learning method, contrastive learning has recently garnered considerable attention. In this study, we propose a unified feature extraction framework based on contrastive learning with adaptive positive and negative samples (CL-FEFA) that is suitable for unsupervised, supervised, and semi-supervised feature extraction. CL-FEFA constructs adaptively positive and negative samples from the result of feature extraction, which makes them more appropriate and accurate. Meanwhile, the discriminative features are extracted based on adaptive positive and negative samples, which will make the intra-class embedded samples more compact and the inter-class embedded samples more dispersed. In the process, using the potential structure information of subspace samples to dynamically construct positive and negative samples can make our framework more robust to noisy data. Furthermore, it is proven that CL-FEFA actually maximizes the mutual information of positive samples, which captures non-linear statistical dependencies between similar samples in potential structure space and thus can act as a measure of true dependence. This also provides theoretical support for its advantages in feature extraction. The final numerical experiments prove that the proposed framework has a strong advantage over traditional feature extraction methods and contrastive learning methods.},
  archive      = {J_NN},
  author       = {Hongjie Zhang and Siyu Zhao and Wenwen Qiang and Yingyi Chen and Ling Jing},
  doi          = {10.1016/j.neunet.2022.09.029},
  journal      = {Neural Networks},
  pages        = {244-257},
  shortjournal = {Neural Netw.},
  title        = {Feature extraction framework based on contrastive learning with adaptive positive and negative samples},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Synchronization of multi-cluster complex networks.
<em>NN</em>, <em>156</em>, 239–243. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a class of multi-cluster complex networks is discussed. Complete synchronization of such networks is analysed in detail. It is explored how the synchronization depends on the intra coupling matrices , the inter coupling matrices and the corresponding coupling strengths. The approach proposed also applies to more general networks composed of multi-cluster networks, too.},
  archive      = {J_NN},
  author       = {Tianping Chen},
  doi          = {10.1016/j.neunet.2022.09.027},
  journal      = {Neural Networks},
  pages        = {239-243},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of multi-cluster complex networks},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extraction of bouton-like structures from neuropil calcium
imaging data. <em>NN</em>, <em>156</em>, 218–238. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neuropil , the plexus of axons and dendrites, plays a critical role in operating the circuit processing of the nervous system . Revealing the spatiotemporal activity pattern within the neuropil would clarify how the information flows throughout the nervous system. However, calcium imaging to examine the circuit dynamics has mainly focused on the soma population due to their discrete distribution. The development of a methodology to analyze the calcium imaging data of a densely packed neuropil would provide us with new insights into the circuit dynamics. Here, we propose a new method to decompose calcium imaging data of the neuropil into populations of bouton-like synaptic structures with a standard desktop computer. To extract bouton-like structures from calcium imaging data, we introduced a new type of modularity, a widely used quality measure in graph theory, and optimized the clustering configuration by a simulated annealing algorithm , which is established in statistical physics. To assess this method’s performance, we conducted calcium imaging of the neuropil of Drosophila larvae. Based on the obtained data, we established artificial neuropil imaging datasets. We applied the decomposition procedure to the artificial and experimental calcium imaging data and extracted individual bouton-like structures successfully. Based on the extracted spatiotemporal data , we analyzed the network structure of the central nervous system of fly larvae and found it was scale-free. These results demonstrate that neuropil calcium imaging and its decomposition could provide new insight into our understanding of neural processing.},
  archive      = {J_NN},
  author       = {Kazushi Fukumasu and Akinao Nose and Hiroshi Kohsaka},
  doi          = {10.1016/j.neunet.2022.09.033},
  journal      = {Neural Networks},
  pages        = {218-238},
  shortjournal = {Neural Netw.},
  title        = {Extraction of bouton-like structures from neuropil calcium imaging data},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boundary heat diffusion classifier for a semi-supervised
learning in a multilayer network embedding. <em>NN</em>, <em>156</em>,
205–217. (<a
href="https://doi.org/10.1016/j.neunet.2022.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of high-quality annotations in many application scenarios has recently led to an increasing interest in devising learning techniques that combine unlabeled data with labeled data in a network. In this work, we focus on the label propagation problem in multilayer networks. Our approach is inspired by the heat diffusion model, which shows usefulness in machine learning problems such as classification and dimensionality reduction. We propose a novel boundary-based heat diffusion algorithm that guarantees a closed-form solution with an efficient implementation. We experimentally validated our method on synthetic networks and five real-world multilayer network datasets representing scientific coauthorship , spreading drug adoption among physicians, two bibliographic networks, and a movie network. The results demonstrate the benefits of the proposed algorithm, where our boundary-based heat diffusion dominates the performance of the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Mohan Timilsina and Vít Nováček and Mathieu d’Aquin and Haixuan Yang},
  doi          = {10.1016/j.neunet.2022.10.005},
  journal      = {Neural Networks},
  pages        = {205-217},
  shortjournal = {Neural Netw.},
  title        = {Boundary heat diffusion classifier for a semi-supervised learning in a multilayer network embedding},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One-shot many-to-many facial reenactment using bi-layer
graph convolutional networks. <em>NN</em>, <em>156</em>, 193–204. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial reenactment is aimed at animating a source face image into a new place using a driving facial picture. In a few shot scenarios, the present strategies are designed with one or more identities or identity-sustained suffering protection challenges. These current solutions are either developed with one or more identities in mind, or face identity protection issues in one or more shot situations. Multiple pictures from the same entity have been used in previous research to model facial reenactment. In contrast, this paper presents a novel model of one-shot many-to-many facial reenactments that uses only one facial image of a face. The proposed model produces a face that represents the objective representation of the same source identity. The proposed technique can simulate motion from a single image by decomposing an object into two layers. Using bi-layer with Convolutional Neural Network (CNN), we named our model Bi-Layer Graph Convolutional Layers (BGCLN) which utilized to create the latent vector’s optical flow representation. This yields the precise structure and shape of the optical stream. Comprehensive studies suggest that our technique can produce high-quality results and outperform most recent techniques in both qualitative and quantitative data comparisons. Our proposed system can perform facial reenactment at 15 fps, which is approximately real time. Our code is publicly available at https://github.com/usaeed786/BGCLN .},
  archive      = {J_NN},
  author       = {Uzair Saeed and Ammar Armghan and Wang Quanyu and Fayadh Alenezi and Sun Yue and Prayag Tiwari},
  doi          = {10.1016/j.neunet.2022.09.031},
  journal      = {Neural Networks},
  pages        = {193-204},
  shortjournal = {Neural Netw.},
  title        = {One-shot many-to-many facial reenactment using bi-layer graph convolutional networks},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lag h∞ synchronization in coupled reaction–diffusion neural
networks with multiple state or derivative couplings. <em>NN</em>,
<em>156</em>, 179–192. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly attempts to discuss lag H ∞ H∞ synchronization in multiple state or derivative coupled reaction–diffusion neural networks without and with parameter uncertainties. Firstly, we respectively propose two types of reaction–diffusion neural networks with multiple state and derivative couplings subject to parameter uncertainties. Secondly, by exploiting designed state feedback controllers , several criteria of the lag H ∞ H∞ synchronization for these two networks are developed based on Lyapunov functional and inequality techniques. Thirdly, lag H ∞ H∞ synchronization issues of these two networks are also coped with by virtue of devised adaptive control strategies. Finally, we provide two numerical examples to verify the obtained lag H ∞ H∞ synchronization criteria.},
  archive      = {J_NN},
  author       = {Lu Wang and Yougang Bian and Zhenyuan Guo and Manjiang Hu},
  doi          = {10.1016/j.neunet.2022.09.030},
  journal      = {Neural Networks},
  pages        = {179-192},
  shortjournal = {Neural Netw.},
  title        = {Lag h∞ synchronization in coupled reaction–diffusion neural networks with multiple state or derivative couplings},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shared subspace-based radial basis function neural network
for identifying ncRNAs subcellular localization. <em>NN</em>,
<em>156</em>, 170–178. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-coding RNAs (ncRNAs) play an important role in revealing the mechanism of human disease for anti-tumor and anti-virus substances. Detecting subcellular locations of ncRNAs is a necessary way to study ncRNA. Traditional biochemical methods are time-consuming and labor-intensive, and computational-based methods can help detect the location of ncRNAs on a large scale. However, many models did not consider the correlation information among multiple subcellular localizations of ncRNAs. This study proposes a radial basis function neural network based on shared subspace learning (RBFNN-SSL), which extract shared structures in multi-labels. To evaluate performance, our classifier is tested on three ncRNA datasets. Our model achieves better performance in experimental results.},
  archive      = {J_NN},
  author       = {Yijie Ding and Prayag Tiwari and Fei Guo and Quan Zou},
  doi          = {10.1016/j.neunet.2022.09.026},
  journal      = {Neural Networks},
  pages        = {170-178},
  shortjournal = {Neural Netw.},
  title        = {Shared subspace-based radial basis function neural network for identifying ncRNAs subcellular localization},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perturbation of deep autoencoder weights for model
compression and classification of tabular data. <em>NN</em>,
<em>156</em>, 160–169. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully connected deep neural networks (DNN) often include redundant weights leading to overfitting and high memory requirements. Additionally, in tabular data classification , DNNs are challenged by the often superior performance of traditional machine learning models. This paper proposes periodic perturbations (prune and regrow) of DNN weights, especially at the self-supervised pre-training stage of deep autoencoders . The proposed weight perturbation strategy outperforms dropout learning or weight regularization (L1 or L2) for four out of six tabular data sets in downstream classification tasks . Unlike dropout learning, the proposed weight perturbation routine additionally achieves 15\% to 40\% sparsity across six tabular data sets, resulting in compressed pretrained models. The proposed pretrained model compression improves the accuracy of downstream classification, unlike traditional weight pruning methods that trade off performance for model compression . Our experiments reveal that a pretrained deep autoencoder with weight perturbation can outperform traditional machine learning in tabular data classification , whereas baseline fully-connected DNNs yield the worst classification accuracy . However, traditional machine learning models are superior to any deep model when a tabular data set contains uncorrelated variables. Therefore, the performance of deep models with tabular data is contingent on the types and statistics of constituent variables.},
  archive      = {J_NN},
  author       = {Sakib Abrar and Manar D. Samad},
  doi          = {10.1016/j.neunet.2022.09.020},
  journal      = {Neural Networks},
  pages        = {160-169},
  shortjournal = {Neural Netw.},
  title        = {Perturbation of deep autoencoder weights for model compression and classification of tabular data},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural network-based event-triggered data-driven control of
disturbed nonlinear systems with quantized input. <em>NN</em>,
<em>156</em>, 152–159. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to design an event-triggered data-driven control for a class of disturbed nonlinear systems with quantized input. A uniform quantizer reconstructed with decreasing quantization intervals is employed to reduce the quantization error . A neural network-based estimation strategy is proposed to estimate both the pseudo partial derivative and disturbances. Consequently, an input triggering rule for single-input single-output systems is provided by incorporating the estimated disturbances, the quantization error bound and tracking errors. Resorting to the Lyapunov method , sufficient conditions for synthesized error systems to be uniformly ultimately bounded are presented. The validity of the proposed scheme is demonstrated via a simulation example.},
  archive      = {J_NN},
  author       = {Xianming Wang and Hamid Reza Karimi and Mouquan Shen and Dan Liu and Li-Wei Li and Jiantao Shi},
  doi          = {10.1016/j.neunet.2022.09.021},
  journal      = {Neural Networks},
  pages        = {152-159},
  shortjournal = {Neural Netw.},
  title        = {Neural network-based event-triggered data-driven control of disturbed nonlinear systems with quantized input},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG decoding method based on multi-feature information
fusion for spinal cord injury. <em>NN</em>, <em>156</em>, 135–151. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop an efficient brain–computer interface (BCI) system, electroencephalography (EEG) measures neuronal activities in different brain regions through electrodes. Many EEG-based motor imagery (MI) studies do not make full use of brain network topology . In this paper, a deep learning framework based on a modified graph convolution neural network (M-GCN) is proposed, in which temporal-frequency processing is performed on the data through modified S-transform (MST) to improve the decoding performance of original EEG signals in different types of MI recognition. MST can be matched with the spatial position relationship of the electrodes. This method fusions multiple features in the temporal-frequency-spatial domain to further improve the recognition performance. By detecting the brain function characteristics of each specific rhythm, EEG generated by imaginary movement can be effectively analyzed to obtain the subjects’ intention. Finally, the EEG signals of patients with spinal cord injury (SCI) are used to establish a correlation matrix containing EEG channel information, the M-GCN is employed to decode relation features. The proposed M-GCN framework has better performance than other existing methods. The accuracy of classifying and identifying MI tasks through the M-GCN method can reach 87.456\%. After 10-fold cross-validation, the average accuracy rate is 87.442\%, which verifies the reliability and stability of the proposed algorithm. Furthermore, the method provides effective rehabilitation training for patients with SCI to partially restore motor function.},
  archive      = {J_NN},
  author       = {Fangzhou Xu and Jincheng Li and Gege Dong and Jianfei Li and Xinyi Chen and Jianqun Zhu and Jinglu Hu and Yang Zhang and Shouwei Yue and Dong Wen and Jiancai Leng},
  doi          = {10.1016/j.neunet.2022.09.016},
  journal      = {Neural Networks},
  pages        = {135-151},
  shortjournal = {Neural Netw.},
  title        = {EEG decoding method based on multi-feature information fusion for spinal cord injury},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surface similarity parameter: A new machine learning loss
metric for oscillatory spatio-temporal data. <em>NN</em>, <em>156</em>,
123–134. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning approaches require the formulation of a loss functional to be minimized in the training phase. Sequential data are ubiquitous across many fields of research, and are often treated with Euclidean distance-based loss functions that were designed for tabular data. For smooth oscillatory data, those conventional approaches lack the ability to penalize amplitude, frequency and phase prediction errors at the same time, and tend to be biased towards amplitude errors. We introduce the surface similarity parameter (SSP) as a novel loss function that is especially useful for training machine learning models on smooth oscillatory sequences. Our extensive experiments on chaotic spatio-temporal dynamical systems indicate that the SSP is beneficial for shaping gradients, thereby accelerating the training process, reducing the final prediction error, increasing weight initialization robustness, and implementing a stronger regularization effect compared to using classical loss functions. The results indicate the potential of the novel loss metric particularly for highly complex and chaotic data, such as data stemming from the nonlinear two-dimensional Kuramoto–Sivashinsky equation and the linear propagation of dispersive surface gravity waves in fluids.},
  archive      = {J_NN},
  author       = {Mathies Wedler and Merten Stender and Marco Klein and Svenja Ehlers and Norbert Hoffmann},
  doi          = {10.1016/j.neunet.2022.09.023},
  journal      = {Neural Networks},
  pages        = {123-134},
  shortjournal = {Neural Netw.},
  title        = {Surface similarity parameter: A new machine learning loss metric for oscillatory spatio-temporal data},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D face-model reconstruction from a single image: A feature
aggregation approach using hierarchical transformer with weak
supervision. <em>NN</em>, <em>156</em>, 108–122. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) have gained popularity as the de-facto model for any computer vision task. However, CNN have drawbacks, i.e. they fail to extract long-range perceptions in images. Due to their ability to capture long-range dependencies, transformer networks are adopted in computer vision applications, where they show state-of-the-art (SOTA) results in popular tasks like image classification , instance segmentation , and object detection. Although they gained ample attention, transformers have not been applied to 3D face reconstruction tasks. In this work, we propose a novel hierarchical transformer model, added to a feature pyramid aggregation structure, to extract the 3D face parameters from a single 2D image. More specifically, we use pre-trained Swin Transformer backbone networks in a hierarchical manner and add the feature fusion module to aggregate the features in multiple stages. We use a semi-supervised training approach and train our model in a supervised way with the 3DMM parameters from a publicly available dataset and unsupervised training with a differential renderer on other parameters like facial keypoints and facial features . We also train our network on a hybrid unsupervised loss and compare the results with other SOTA approaches. When evaluated across two public datasets on face reconstruction and dense 3D face alignment tasks, our method can achieve comparable results to the current SOTA performance and in some instances do better than the SOTA methods . A detailed subjective evaluation also shows that our method performs better than the previous works in realism and occlusion resistance.},
  archive      = {J_NN},
  author       = {Shubhajit Basak and Peter Corcoran and Rachel McDonnell and Michael Schukat},
  doi          = {10.1016/j.neunet.2022.09.019},
  journal      = {Neural Networks},
  pages        = {108-122},
  shortjournal = {Neural Netw.},
  title        = {3D face-model reconstruction from a single image: A feature aggregation approach using hierarchical transformer with weak supervision},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series (re)sampling using generative adversarial
networks. <em>NN</em>, <em>156</em>, 95–107. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel bootstrap procedure for time series data based on Generative Adversarial networks (GANs). We show that the dynamics of common stationary time series processes can be learned by GANs and demonstrate that GANs trained on a single sample path can be used to generate additional samples from the process. We find that temporal convolutional neural networks provide a suitable design for the generator and discriminator , and that convincing samples can be generated on the basis of a vector drawn from a normal distribution with zero mean and an identity variance–covariance matrix. We demonstrate the finite sample properties of GAN sampling and the suggested bootstrap using simulations where we compare the performance to circular block bootstrapping in the case of resampling an AR(1) time series processes. We find that resampling using the GAN can outperform circular block bootstrapping in terms of empirical coverage. Finally, we provide an empirical application to the Sharpe ratio.},
  archive      = {J_NN},
  author       = {Christian M. Dahl and Emil N. Sørensen},
  doi          = {10.1016/j.neunet.2022.09.010},
  journal      = {Neural Networks},
  pages        = {95-107},
  shortjournal = {Neural Netw.},
  title        = {Time series (re)sampling using generative adversarial networks},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust image hashing for content identification through
contrastive self-supervised learning. <em>NN</em>, <em>156</em>, 81–94.
(<a href="https://doi.org/10.1016/j.neunet.2022.09.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content identification systems are an essential technology for many applications. These systems identify query multimedia items using a database of known identities. A hash-based system uses a perceptual hashing function that generates a hash value invariant against a set of expected manipulations in an image, later compared to perform identification. Usually, this set of manipulations is well-known, and the researcher creates the perceptual hashing function that best adapts to the set. However, a new manipulation may break the hashing function, requiring to create a new one, which may be costly and time-consuming. Therefore, we propose to let the hashing function learn an invariant feature space automatically. For this, we exploit the recent advances in self-supervised learning, where a model uses unlabeled data to generate a feature representation by solving a metric learning-based pretext task that enforces the robust image hashing properties for content identification systems. To achieve model transferability on unseen data, our pretext task enforces the feature vector invariance against the manipulation set, and through random sampling on the unlabeled training set, we present the model a wide variety of perceptual information to work on. As exhaustive experimentation shows, this method achieves excellent robustness against a comprehensive set of manipulations, even difficult ones such as horizontal flip and rotation, with excellent identification performance. Also, the trained model is highly discriminative against the presence of near-duplicate images. Furthermore, this method does not need re-training or fine-tuning on a new dataset to achieve the observed performance, indicating an excellent generalization capacity.},
  archive      = {J_NN},
  author       = {Jesús Fonseca-Bustos and Kelsey Alejandra Ramírez-Gutiérrez and Claudia Feregrino-Uribe},
  doi          = {10.1016/j.neunet.2022.09.028},
  journal      = {Neural Networks},
  pages        = {81-94},
  shortjournal = {Neural Netw.},
  title        = {Robust image hashing for content identification through contrastive self-supervised learning},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAT: Block and token self-attention for speech emotion
recognition. <em>NN</em>, <em>156</em>, 67–80. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved great success in many artificial intelligence fields, such as computer vision (CV), audio processing and natural language processing (NLP). In speech emotion recognition (SER), transformer-based architectures usually compute attention in a token-by-token (frame-by-frame) manner, but this approach lacks adequate capacity to capture local emotion information and is easily affected by noise. This paper proposes a novel SER architecture, referred to as block and token self-attention (BAT), that splits a mixed spectrogram into blocks and computes self-attention by combining these blocks with tokens, which can alleviate the effect of local noise while capturing authentic sentiment expressions. Furthermore, we present a cross-block attention mechanism to facilitate information interaction among blocks while integrating a frequency compression and channel enhancement (FCCE) module to smooth the attention biases between blocks and tokens. BAT achieves 73.2\% weighted accuracy (WA) and 75.2\% unweighted accuracy (UA) on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset, surpassing the results of previously developed state-of-the-art approaches with the same dataset partitioning operation. Further experimental results reveal that our proposed method is also well suited for cross-database and cross-domain tasks, achieving 89\% WA and 87.4\% UA on Emo-DB and producing a top-1 recognition accuracy of 88.32\% with only 15.01 Mb of parameters on the CIFAR-10 image dataset under a scenario with no data augmentation or pretraining.},
  archive      = {J_NN},
  author       = {Jianjun Lei and Xiangwei Zhu and Ying Wang},
  doi          = {10.1016/j.neunet.2022.09.022},
  journal      = {Neural Networks},
  pages        = {67-80},
  shortjournal = {Neural Netw.},
  title        = {BAT: Block and token self-attention for speech emotion recognition},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Not all edges are peers: Accurate structure-aware graph
pooling networks. <em>NN</em>, <em>156</em>, 58–66. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have achieved state-of-the-art performance in graph-related tasks. For graph classification task , an elaborated pooling operator is vital for learning graph-level representations. Most pooling operators derived from existing GNNs generate a coarsen graph through ordering the nodes and selecting some top-ranked ones. However, these methods fail to explore the fundamental elements other than nodes in graphs, which may not efficiently utilize the structure information. Besides, all edges attached to the low-ranked nodes are discarded, which destroys graphs’ connectivity and loses information. Moreover, the selected nodes tend to concentrate on some substructures while overlooking information in others. To address these challenges, we propose a novel pooling operator called Accurate Structure-Aware Graph Pooling ( ASPool ), which can be integrated into various GNNs to learn graph-level representation. Specifically, ASPool adaptively retains a subset of edges to calibrate the graph structure and learns the abstracted representations, wherein all the edges are viewed as non-peers instead of simply connecting nodes. To preserve the graph’s connectivity, we further introduce the selection strategy considering both top-ranked nodes and dropped edges . Additionally, ASPool performs a two-stage calculation process to promise that the sampled nodes are distributed throughout the graph. Experiment results on 9 widely used benchmarks show that ASPool achieves superior performance over the state-of-the-art graph representation learning methods.},
  archive      = {J_NN},
  author       = {Hualei Yu and Jinliang Yuan and Yirong Yao and Chongjun Wang},
  doi          = {10.1016/j.neunet.2022.09.004},
  journal      = {Neural Networks},
  pages        = {58-66},
  shortjournal = {Neural Netw.},
  title        = {Not all edges are peers: Accurate structure-aware graph pooling networks},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). MG-CNN: A deep CNN to predict saddle points of matrix
games. <em>NN</em>, <em>156</em>, 49–57. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the saddle point of a matrix game is a classical problem that arises in various fields, e.g., economics, computer science, and engineering. The standard problem-solving methods consist of formulating the problem as a linear program (LP). However, this approach seems less efficient when many instances need to be solved. In this paper, we propose a Convolutional Neural Network based approach, which is able to predict both the strategy profile ( x , y ) (x,y) and the optimal value v v of the game. We call this approach Matrix Game-Conventional Neural Network or MG-CNN for short. Thanks to a global pooling technique, MG-CNN can solve matrix games with different shapes. We propose a specialized algorithm to train MG-CNN, which includes both data generation and model training. Our numerical experiments show that MG-CNN outperforms standard LP solvers in terms of computational CPU time and provides a high-quality prediction.},
  archive      = {J_NN},
  author       = {Dawen Wu and Abdel Lisser},
  doi          = {10.1016/j.neunet.2022.09.014},
  journal      = {Neural Networks},
  pages        = {49-57},
  shortjournal = {Neural Netw.},
  title        = {MG-CNN: A deep CNN to predict saddle points of matrix games},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Label smoothing and task-adaptive loss function based on
prototype network for few-shot learning. <em>NN</em>, <em>156</em>,
39–48. (<a href="https://doi.org/10.1016/j.neunet.2022.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at solving the problems of prototype network that the label information is not reliable enough and that the hyperparameters of the loss function cannot follow the changes of image feature information, we propose a method that combines label smoothing and hyperparameters. First, the label information of an image is processed by label smoothing regularization . Then, according to different classification tasks , the distance matrix and logarithmic operation of the image feature are used to fuse the distance matrix of the image with the hyperparameters of the loss function. Finally, the hyperparameters are associated with the smoothed label and the distance matrix for predictive classification. The method is validated on the miniImageNet, FC100 and tieredImageNet datasets. The results show that, compared with the unsmoothed label and fixed hyperparameters methods, the classification accuracy of the flexible hyperparameters in the loss function under the condition of few-shot learning is improved by 2\%–3\%. The result shows that the proposed method can suppress the interference of false labels, and the flexibility of hyperparameters can improve classification accuracy .},
  archive      = {J_NN},
  author       = {Farong Gao and Xingsheng Luo and Zhangyi Yang and Qizhong Zhang},
  doi          = {10.1016/j.neunet.2022.09.018},
  journal      = {Neural Networks},
  pages        = {39-48},
  shortjournal = {Neural Netw.},
  title        = {Label smoothing and task-adaptive loss function based on prototype network for few-shot learning},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Switching pinning control for memristive neural networks
system with markovian switching topologies. <em>NN</em>, <em>156</em>,
29–38. (<a href="https://doi.org/10.1016/j.neunet.2022.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work concentrates on the issue of leader-following bipartite synchronization of multiple memristive neural networks with Markovian jump topology. In contrast to conventional coupled neural network systems, the coupled neural network model under consideration possesses both cooperative and competitive connections among neuron nodes. Specifically, the interaction between neighbors’ nodes is described by a signed graph, in which a positive weight represents an alliance relationship between two neuron nodes while a negative weight represents an adversarial relationship between two neuron nodes. By designing a pinning discontinuous controller that makes full use of the mode information, some effective criteria that ensure the stability of bipartite synchronization error states are obtained. All network nodes can synchronize the target node state bipartitely. Finally, two simulation examples are provided to demonstrate the viability of the suggested bipartite synchronization control approach.},
  archive      = {J_NN},
  author       = {Ning Li and Wei Xing Zheng},
  doi          = {10.1016/j.neunet.2022.09.011},
  journal      = {Neural Networks},
  pages        = {29-38},
  shortjournal = {Neural Netw.},
  title        = {Switching pinning control for memristive neural networks system with markovian switching topologies},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DEFEAT: Decoupled feature attack across deep neural
networks. <em>NN</em>, <em>156</em>, 13–28. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks pose a security challenge for deep neural networks , motivating researchers to build various defense methods. Consequently, the performance of black-box attacks turns down under defense scenarios. A significant observation is that some feature-level attacks achieve an excellent success rate to fool undefended models, while their transferability is severely degraded when encountering defenses, which give a false sense of security. In this paper, we explain one possible reason caused this phenomenon is the domain-overfitting effect, which degrades the capabilities of feature perturbed images and makes them hardly fool adversarially trained defenses. To this end, we study a novel feature-level method, referred to as De coupled Fe ature At tack (DEFEAT). Unlike the current attacks that use a round-robin procedure to estimate gradient estimation and update perturbation, DEFEAT decouples adversarial example generation from the optimization process. In the first stage, DEFEAT learns an distribution full of perturbations with high adversarial effects. And it then iteratively samples the noises from learned distribution to assemble adversarial examples. On top of that, we can apply transformations of existing methods into the DEFEAT framework to produce more robust perturbations. We also provide insights into the relationship between transferability and latent features that helps the community to understand the intrinsic mechanism of adversarial attacks. Extensive experiments evaluated on a variety of black-box models suggest the superiority of DEFEAT, i.e., our method fools defenses at an average success rate of 88.4\%, remarkably outperforming state-of-the-art transferable attacks by a large margin of 11.5\%. The code is publicly available at https://github.com/mesunhlf/DEFEAT .},
  archive      = {J_NN},
  author       = {Lifeng Huang and Chengying Gao and Ning Liu},
  doi          = {10.1016/j.neunet.2022.09.009},
  journal      = {Neural Networks},
  pages        = {13-28},
  shortjournal = {Neural Netw.},
  title        = {DEFEAT: Decoupled feature attack across deep neural networks},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A leader-following paradigm based deep reinforcement
learning method for multi-agent cooperation games. <em>NN</em>,
<em>156</em>, 1–12. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent deep reinforcement learning algorithms with centralized training with decentralized execution (CTDE) paradigm has attracted growing attention in both industry and research community. However, the existing CTDE methods follow the action selection paradigm that all agents choose actions at the same time, which ignores the heterogeneous roles of different agents. Motivated by the human wisdom in cooperative behaviors , we present a novel leader-following paradigm based deep multi-agent cooperation method (LFMCO) for multi-agent cooperative games. Specifically, we define a leader as someone who broadcasts a message representing the selected action to all subordinates. After that, the followers choose their individual action based on the received message from the leader. To measure the influence of leader’s action on followers, we introduced a concept of information gain, i.e., the change of followers’ value function entropy, which is positively correlated with the influence of leader’s action. We evaluate the LFMCO on several cooperation scenarios of StarCraft2. Simulation results confirm the significant performance improvements of LFMCO compared with four state-of-the-art benchmarks on the challenging cooperative environment.},
  archive      = {J_NN},
  author       = {Feiye Zhang and Qingyu Yang and Dou An},
  doi          = {10.1016/j.neunet.2022.09.012},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {A leader-following paradigm based deep reinforcement learning method for multi-agent cooperation games},
  volume       = {156},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>155</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00397-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00397-5},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). CURRENT EVENTS. <em>NN</em>, <em>155</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00396-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00396-3},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {CURRENT EVENTS},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neurodynamic approaches for sparse recovery problem with
linear inequality constraints. <em>NN</em>, <em>155</em>, 592–601. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops two neurodynamic approaches for solving the L 1 L1 -minimization problem with the linear inequality constraints . First, a centralized neurodynamic approach is proposed based on projection operator and nonnegative quadrant. The stability and global convergence of the centralized neurodynamic approach are analyzed by the Lyapunov method in detail. Considering that the distributed optimization problem has the advantages of information protection and scalability, the L 1 L1 -minimization problem with linear inequality constraints is transformed into a distributed sparse optimization problem under mild conditions. Then, using the centralized neurodynamic approach and multi-agent consensus theory, a distributed neurodynamic approach is proposed for the distributed optimization problem. Furthermore, relevant theories show that each agent globally converges to an optimal solution of the distributed optimization problem. Finally, the presented centralized neurodynamic approach is applied to sparse recovery problem with L ∞ L∞ -norm noise constraints and the effectiveness of distributed approach is shown by several experiments on sparse signal recovery.},
  archive      = {J_NN},
  author       = {Jiao Yang and Xing He and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.09.013},
  journal      = {Neural Networks},
  pages        = {592-601},
  shortjournal = {Neural Netw.},
  title        = {Neurodynamic approaches for sparse recovery problem with linear inequality constraints},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Natural reweighted wake–sleep. <em>NN</em>, <em>155</em>,
574–591. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Helmholtz Machines (HMs) are a class of generative models composed of two Sigmoid Belief Networks (SBNs), acting respectively as an encoder and a decoder. These models are commonly trained using a two-step optimization algorithm called Wake–Sleep (WS) and more recently by improved versions, such as Reweighted Wake–Sleep (RWS) and Bidirectional Helmholtz Machines (BiHM). The locality of the connections in an SBN induces sparsity in the Fisher Information Matrices associated to the probabilistic models, in the form of a finely-grained block-diagonal structure. In this paper we exploit this property to efficiently train SBNs and HMs using the natural gradient. We present a novel algorithm, called Natural Reweighted Wake–Sleep (NRWS), that corresponds to the geometric adaptation of its standard version. In a similar manner, we also introduce Natural Bidirectional Helmholtz Machine (NBiHM). Differently from previous work, we will show how for HMs the natural gradient can be efficiently computed without the need of introducing any approximation in the structure of the Fisher information matrix. The experiments performed on standard datasets from the literature show a consistent improvement of NRWS and NBiHM not only with respect to their non-geometric baselines but also with respect to state-of-the-art training algorithms for HMs. The improvement is quantified both in terms of speed of convergence as well as value of the log-likelihood reached after training.},
  archive      = {J_NN},
  author       = {Csongor Várady and Riccardo Volpi and Luigi Malagò and Nihat Ay},
  doi          = {10.1016/j.neunet.2022.09.006},
  journal      = {Neural Networks},
  pages        = {574-591},
  shortjournal = {Neural Netw.},
  title        = {Natural reweighted Wake–Sleep},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantization-aware training for low precision photonic
neural networks. <em>NN</em>, <em>155</em>, 561–573. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Learning (DL) fueled the interest in developing neuromorphic hardware accelerators that can improve the computational speed and energy efficiency of existing accelerators. Among the most promising research directions towards this is photonic neuromorphic architectures, which can achieve femtojoule per MAC efficiencies. Despite the benefits that arise from the use of neuromorphic architectures, a significant bottleneck is the use of expensive high-speed and precision analog-to-digital (ADCs) and digital-to-analog conversion modules (DACs) required to transfer the electrical signals, originating from the various Artificial Neural Networks (ANNs) operations (inputs, weights, etc.) in the photonic optical engines. The main contribution of this paper is to study quantization phenomena in photonic models, induced by DACs/ADCs, as an additional noise/uncertainty source and to provide a photonics-compliant framework for training photonic DL models with limited precision, allowing for reducing the need for expensive high precision DACs/ADCs. The effectiveness of the proposed method is demonstrated using different architectures, ranging from fully connected and convolutional networks to recurrent architectures, following recent advances in photonic DL.},
  archive      = {J_NN},
  author       = {M. Kirtas and A. Oikonomou and N. Passalis and G. Mourgias-Alexandris and M. Moralis-Pegios and N. Pleros and A. Tefas},
  doi          = {10.1016/j.neunet.2022.09.015},
  journal      = {Neural Networks},
  pages        = {561-573},
  shortjournal = {Neural Netw.},
  title        = {Quantization-aware training for low precision photonic neural networks},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TaskDrop: A competitive baseline for continual learning of
sentiment classification. <em>NN</em>, <em>155</em>, 551–560. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the multi-task sentiment classification problem in the continual learning setting, i.e., a model is sequentially trained to classify the sentiment of reviews of products in a particular category. The use of common sentiment words in reviews of different product categories leads to large cross-task similarity, which differentiates it from continual learning in other domains. This knowledge sharing nature renders forgetting reduction focused approaches less effective for the problem under consideration. Unlike existing approaches, where task-specific masks are learned with specifically presumed training objectives, we propose an approach called Task-aware Dropout (TaskDrop) to randomly sample a binary mask for each task. While the standard dropout generates and applies random masks for each training instance per epoch for regularization , random masks in TaskDrop are used for model capacity allocation and reuse to each coming task. We conducted experimental studies on Amazon review data and made comparison to various baselines and state-of-the-art approaches. Our empirical results show that regardless of simplicity, TaskDrop overall achieved competitive performance, especially after relatively long term learning. This demonstrates that the proposed random capacity allocation mechanism works well for continual sentiment classification.},
  archive      = {J_NN},
  author       = {Jian-Ping Mei and Yilun Zhen and Qianwei Zhou and Rui Yan},
  doi          = {10.1016/j.neunet.2022.08.033},
  journal      = {Neural Networks},
  pages        = {551-560},
  shortjournal = {Neural Netw.},
  title        = {TaskDrop: A competitive baseline for continual learning of sentiment classification},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integral representations of shallow neural network with
rectified power unit activation function. <em>NN</em>, <em>155</em>,
536–550. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we characterize the set of functions that can be represented by infinite width neural networks with RePU activation function max ( 0 , x ) p max(0,x)p , when the network coefficients are regularized by an ℓ 2 / p ℓ2/p (quasi)norm. Compared to the more well-known ReLU activation function (which corresponds to p = 1 p=1 ), the RePU activation functions exhibit a greater degree of smoothness which makes them preferable in several applications. Our main result shows that such representations are possible for a given function if and only if the function is κ κ -order Lipschitz and its R R -norm is finite. This extends earlier work on this topic that has been restricted to the case of the ReLU activation function and coefficient bounds with respect to the ℓ 2 ℓ2 norm. Since for q q&amp;lt;2 , ℓ q ℓq regularizations are known to promote sparsity , our results also shed light on the ability to obtain sparse neural network representations.},
  archive      = {J_NN},
  author       = {Ahmed Abdeljawad and Philipp Grohs},
  doi          = {10.1016/j.neunet.2022.09.005},
  journal      = {Neural Networks},
  pages        = {536-550},
  shortjournal = {Neural Netw.},
  title        = {Integral representations of shallow neural network with rectified power unit activation function},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential safe feature elimination rule for l1-regularized
regression with kullback–leibler divergence. <em>NN</em>, <em>155</em>,
523–535. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The L 1 L1 -regularized regression with Kullback–Leibler divergence (KL- L 1 L1 R) is a popular regression technique . Although many efforts have been devoted to its efficient implementation, it remains challenging when the number of features is extremely large. In this paper, to accelerate KL- L 1 L1 R, we introduce a novel and fast sequential safe feature elimination rule (FER) based on its sparsity , local regularity properties , and duality theory . It takes negligible time to select and delete most redundant features before and during the training process. Only one reduced model needs to be solved, which makes the computational time shortened. To further speed up the reduced model, the Newton coordinate descent method (Newton-CDM) is chosen as a solver. The superiority of FER is safety, i.e., its solution is exactly the same as the original KL- L 1 L1 R. Numerical experiments on three artificial datasets, five real-world datasets, and one handwritten digit dataset demonstrate the feasibility and validity of our FER.},
  archive      = {J_NN},
  author       = {Hongmei Wang and Kun Jiang and Yitian Xu},
  doi          = {10.1016/j.neunet.2022.09.008},
  journal      = {Neural Networks},
  pages        = {523-535},
  shortjournal = {Neural Netw.},
  title        = {Sequential safe feature elimination rule for l1-regularized regression with Kullback–Leibler divergence},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous learning of spiking networks trained with local
rules. <em>NN</em>, <em>155</em>, 512–522. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) experience catastrophic forgetting (CF) during sequential learning . In contrast, the brain can learn continuously without any signs of catastrophic forgetting. Spiking neural networks (SNNs) are the next generation of ANNs with many features borrowed from biological neural networks . Thus, SNNs potentially promise better resilience to CF. In this paper, we study the susceptibility of SNNs to CF and test several biologically inspired methods for mitigating catastrophic forgetting. SNNs are trained with biologically plausible local training rules based on spike-timing-dependent plasticity (STDP). Local training prohibits the direct use of CF prevention methods based on gradients of a global loss function. We developed and tested the method to determine the importance of synapses (weights) based on stochastic Langevin dynamics without the need for the gradients. Several other methods of catastrophic forgetting prevention adapted from analog neural networks were tested as well. The experiments were performed on freely available datasets in the SpykeTorch environment.},
  archive      = {J_NN},
  author       = {D.I. Antonov and K.V. Sviatov and S. Sukhov},
  doi          = {10.1016/j.neunet.2022.09.003},
  journal      = {Neural Networks},
  pages        = {512-522},
  shortjournal = {Neural Netw.},
  title        = {Continuous learning of spiking networks trained with local rules},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Support vector machine embedding discriminative dictionary
pair learning for pattern classification. <em>NN</em>, <em>155</em>,
498–511. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative dictionary learning (DDL) aims to address pattern classification problems via learning dictionaries from training samples. Dictionary pair learning (DPL) based DDL has shown superiority as compared with most existing algorithms which only learn synthesis dictionaries or analysis dictionaries. However, in the original DPL algorithm, the discrimination capability is only promoted via the reconstruction error and the structures of the learned dictionaries, while the discrimination of coding coefficients is not considered in the process of dictionary learning. To address this issue, we propose a new DDL algorithm by introducing an additional discriminative term associated with coding coefficients. Specifically, a support vector machine (SVM) based term is employed to enhance the discrimination of coding coefficients. In this model, a structured dictionary pair and SVM classifiers are jointly learned, and an optimization method is developed to address the formulated optimization problem . A classification scheme based on both the reconstruction error and SVMs is also proposed. Simulation results on several widely used databases demonstrate that the proposed method can achieve competitive performance as compared with some state-of-the-art DDL algorithms.},
  archive      = {J_NN},
  author       = {Jing Dong and Liu Yang and Chang Liu and Wei Cheng and Wenwu Wang},
  doi          = {10.1016/j.neunet.2022.08.031},
  journal      = {Neural Networks},
  pages        = {498-511},
  shortjournal = {Neural Netw.},
  title        = {Support vector machine embedding discriminative dictionary pair learning for pattern classification},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tf-GCZSL: Task-free generalized continual zero-shot
learning. <em>NN</em>, <em>155</em>, 487–497. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning continually from a stream of training data or tasks with an ability to learn the unseen classes using a zero-shot learning framework is gaining attention in the literature. It is referred to as continual zero-shot learning (CZSL). Existing CZSL requires clear task-boundary information during training which is not practically feasible. This paper proposes a task-free generalized CZSL (Tf-GCZSL) method with short-term/long-term memory to overcome the requirement of task-boundary in training. A variational autoencoder (VAE) handles the fundamental ZSL tasks. The short-term and long-term memory help to overcome the condition of the task boundary in the CZSL framework. Further, the proposed Tf-GCZSL method combines the concept of experience replay with dark knowledge distillation and regularization to overcome the catastrophic forgetting issues in a continual learning framework. Finally, the Tf-GCZSL uses a fully connected classifier developed using the synthetic features generated at the latent space of the VAE. The performance of the proposed Tf-GCZSL is evaluated in the existing task-agnostic prediction setting and the proposed task-free setting for the generalized CZSL over the five ZSL benchmark datasets. The results clearly indicate that the proposed Tf-GCZSL improves the prediction at least by 12\%, 1\%, 3\%, 4\%, and 3\% over existing state-of-the-art and baseline methods for CUB, aPY, AWA1, AWA2, and SUN datasets, respectively in both settings (task-agnostic prediction and task-free learning). The source code is available at https://github.com/Chandan-IITI/Tf-GCZSL .},
  archive      = {J_NN},
  author       = {Chandan Gautam and Sethupathy Parameswaran and Ashish Mishra and Suresh Sundaram},
  doi          = {10.1016/j.neunet.2022.08.034},
  journal      = {Neural Networks},
  pages        = {487-497},
  shortjournal = {Neural Netw.},
  title        = {Tf-GCZSL: Task-free generalized continual zero-shot learning},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential multi-view subspace clustering. <em>NN</em>,
<em>155</em>, 475–486. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-representation based subspace learning has shown its effectiveness in many applications, but most existing methods do not consider the difference between different views. As a result, the learned self-representation matrix cannot well characterize the clustering structure . Moreover, some methods involve an undesired weighted vector of the tensor nuclear norm , which reduces the flexibility of the algorithm in practical applications. To handle these problems, we present a tensorized multi-view subspace clustering. Specifically, our method employs matrix factorization and decomposes the self-representation matrix to orthogonal projection matrix and affinity matrix . We also add ℓ 1 , 2 ℓ1,2 -norm regularization on affinity representation to characterize the cluster structure. Moreover, the proposed method uses weighted tensor Schatten p p -norm to explore higher-order structure and complementary information embedded in multi-view data, which can allocate the ideal weight for each view automatically without additional weight and penalty parameters. We apply the adaptive loss function to the model to maintain the robustness to outliers and efficiently learn the data distribution. Extensive experimental results on different datasets reveal that our method is superior to other state-of-the-art multi-view subspace clustering methods .},
  archive      = {J_NN},
  author       = {Fangyuan Lei and Qin Li},
  doi          = {10.1016/j.neunet.2022.09.007},
  journal      = {Neural Networks},
  pages        = {475-486},
  shortjournal = {Neural Netw.},
  title        = {Sequential multi-view subspace clustering},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HVIOnet: A deep learning based hybrid visual–inertial
odometry approach for unmanned aerial system position estimation.
<em>NN</em>, <em>155</em>, 461–474. (<a
href="https://doi.org/10.1016/j.neunet.2022.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor fusion is used to solve the localization problem in autonomous mobile robotics applications by integrating complementary data acquired from various sensors. In this study, we adopt Visual–Inertial Odometry (VIO), a low-cost sensor fusion method that integrates inertial data with images using a Deep Learning (DL) framework to predict the position of an Unmanned Aerial System (UAS). The developed system has three steps. The first step extracts features from images acquired from a platform camera and uses a Convolutional Neural Network (CNN) to project them to a visual feature manifold. Next, temporal features are extracted from the Inertial Measurement Unit (IMU) data on the platform using a Bidirectional Long Short Term Memory (BiLSTM) network and are projected to an inertial feature manifold. The final step estimates the UAS position by fusing the visual and inertial feature manifolds via a BiLSTM-based architecture. The proposed approach is tested with the public EuRoC (European Robotics Challenge) dataset and simulation environment data generated within the Robot Operating System (ROS). The result of the EuRoC dataset shows that the proposed approach achieves successful position estimations comparable to previous popular VIO methods. In addition, as a result of the experiment with the simulation dataset, the UAS position is successfully estimated with 0.167 Mean Square Error (RMSE). The obtained results prove that the proposed deep architecture is useful for UAS position estimation.},
  archive      = {J_NN},
  author       = {Muhammet Fatih Aslan and Akif Durdu and Abdullah Yusefi and Alper Yilmaz},
  doi          = {10.1016/j.neunet.2022.09.001},
  journal      = {Neural Networks},
  pages        = {461-474},
  shortjournal = {Neural Netw.},
  title        = {HVIOnet: A deep learning based hybrid visual–inertial odometry approach for unmanned aerial system position estimation},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low precision decentralized distributed training over IID
and non-IID data. <em>NN</em>, <em>155</em>, 451–460. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized distributed learning is the key to enabling large-scale machine learning (training) on the edge devices utilizing private user-generated local data, without relying on the cloud. However, practical realization of such on-device training is limited by the communication and compute bottleneck. In this paper, we propose and show the convergence of low precision decentralized training that aims to reduce the computational complexity and communication cost of decentralized training. Many feedback-based compression techniques have been proposed in the literature to reduce communication costs. To the best of our knowledge, there is no work that applies and shows compute efficient training techniques such as quantization, pruning etc., for peer-to-peer decentralized learning setups. Since real-world applications have a significant skew in the data distribution, we design ”Range-EvoNorm” as the normalization activation layer which is better suited for low precision training over non-IID data. Moreover, we show that the proposed low precision training can be used in synergy with other communication compression methods decreasing the communication cost further. Our experiments indicate that 8-bit decentralized training has minimal accuracy loss compared to its full precision counterpart even with non-IID data. However, when low precision training is accompanied by communication compression through sparsification we observe a 1 − 2\% 1−2\% drop in accuracy. The proposed low precision decentralized training decreases computational complexity , memory usage, and communication cost by ∼ 4 × ∼4× and compute energy by a factor of ∼ 20 × ∼20× , while trading off less than a 1\% accuracy for both IID and non-IID data. In particular, for higher skew values, we observe an increase in accuracy (by ∼ 0 . 5\% ∼0.5\% ) with low precision training, indicating the regularization effect of the quantization.},
  archive      = {J_NN},
  author       = {Sai Aparna Aketi and Sangamesh Kodge and Kaushik Roy},
  doi          = {10.1016/j.neunet.2022.08.032},
  journal      = {Neural Networks},
  pages        = {451-460},
  shortjournal = {Neural Netw.},
  title        = {Low precision decentralized distributed training over IID and non-IID data},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attentional feature pyramid network for small object
detection. <em>NN</em>, <em>155</em>, 439–450. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art detectors generally exploit the Feature Pyramid Networks (FPN) due to its advantage of detecting objects at different scales. Despite significant advances in object detection owing to the design of feature pyramids, it is still challenging to detect small objects with low resolution and dense distribution in complex scenes. To address these problems, we propose Attentional Feature Pyramid Network, a new feature pyramid architecture named AFPN which consists of three components to enhance the small object detection ability, specifically: Dynamic Texture Attention, Foreground-Aware Co-Attention, and Detail Context Attention. First, Dynamic Texture Attention augments the texture features dynamically by filtering out redundant semantics to highlight small objects in lower layers and amplifying credible details to emphasize large objects in higher layers. Then, Foreground-Aware Co-Attention is explored to detect densely arranged small objects by enhancing the objects feature via foreground-correlated contexts and suppressing the background noise. Finally, to better capture the features of small objects, Detail Context Attention adaptively aggregates detail cues of RoI features with different scales for a more accurate feature representation. By substituting FPN with AFPN in Faster R-CNN, our method performs on par with the state-of-the-art performance on Tsinghua-Tencent 100K. Furthermore, we achieve highly competitive results on small category of both PASCAL VOC and MS COCO.},
  archive      = {J_NN},
  author       = {Kyungseo Min and Gun-Hee Lee and Seong-Whan Lee},
  doi          = {10.1016/j.neunet.2022.08.029},
  journal      = {Neural Networks},
  pages        = {439-450},
  shortjournal = {Neural Netw.},
  title        = {Attentional feature pyramid network for small object detection},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Electrical coupling regulated by GABAergic nucleo-olivary
afferent fibres facilitates cerebellar sensory–motor adaptation.
<em>NN</em>, <em>155</em>, 422–438. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inferior olivary (IO) nucleus makes up the signal gateway for several organs to the cerebellar cortex . Located within the sensory–motor-cerebellum pathway, the IO axons, i.e., climbing fibres (CFs), massively synapse onto the cerebellar Purkinje cells (PCs) regulating motor learning whilst the olivary nucleus receives negative feedback through the GABAergic nucleo-olivary​ (NO) pathway. The NO pathway regulates the electrical coupling (EC) amongst the olivary cells thus facilitating synchrony and timing. However, the involvement of this EC regulation on cerebellar adaptive behaviour is still under debate. In our study we have used a spiking cerebellar model to assess the role of the NO pathway in regulating vestibulo-ocular-reflex (VOR) adaptation. The model incorporates spike-based synaptic plasticity at multiple cerebellar sites and an electrically-coupled olivary system. The olivary system plays a central role in regulating the CF spike-firing patterns that drive the PCs, whose axons ultimately shape the cerebellar output. Our results suggest that a systematic GABAergic NO deactivation decreases the spatio-temporal complexity of the IO firing patterns thereby worsening the temporal resolution of the olivary system. Conversely, properly coded IO spatio-temporal firing patterns, thanks to NO modulation, finely shape the balance between long-term depression and potentiation, which optimises VOR adaptation. Significantly, the NO connectivity pattern constrained to the same micro-zone helps maintain the spatio-temporal complexity of the IO firing patterns through time. Moreover, the temporal alignment between the latencies found in the NO fibres and the sensory–motor pathway delay appears to be crucial for facilitating the VOR. When we consider all the above points we believe that these results predict that the NO pathway is instrumental in modulating the olivary coupling and relevant to VOR adaptation.},
  archive      = {J_NN},
  author       = {Niceto R. Luque and Francisco Naveros and Ignacio Abadía and Eduardo Ros and Angelo Arleo},
  doi          = {10.1016/j.neunet.2022.08.020},
  journal      = {Neural Networks},
  pages        = {422-438},
  shortjournal = {Neural Netw.},
  title        = {Electrical coupling regulated by GABAergic nucleo-olivary afferent fibres facilitates cerebellar sensory–motor adaptation},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comprehensive analysis of fixed-time stability and energy
cost for delay neural networks. <em>NN</em>, <em>155</em>, 413–421. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on comprehensive analysis of fixed-time stability and energy consumed by controller in nonlinear neural networks with time-varying delays. A sufficient condition is provided to assure fixed-time stability by developing a global composite switched controller and employing inequality techniques. Then the specific expression of the upper of energy required for achieving control is deduced. Moreover, the comprehensive analysis of the energy cost and fixed-time stability is investigated utilizing a dual-objective optimization function. It illustrates that adjusting the control parameters can make the system converge to the equilibrium point under better control state. Finally, one numerical example is presented to verify the effectiveness of the provided control scheme.},
  archive      = {J_NN},
  author       = {Yuchun Wang and Song Zhu and Hu Shao and Yu Feng and Li Wang and Shiping Wen},
  doi          = {10.1016/j.neunet.2022.08.024},
  journal      = {Neural Networks},
  pages        = {413-421},
  shortjournal = {Neural Netw.},
  title        = {Comprehensive analysis of fixed-time stability and energy cost for delay neural networks},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel data-driven visualization of n-dimensional feasible
region using interpretable self-organizing maps (iSOM). <em>NN</em>,
<em>155</em>, 398–412. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical optimization allows solving one or two dimensional optimization problems visually by merely plotting the objective function and constraint function contours. In addition to the discovery of optima, such a visualization-based approach enables understanding and interpretation of design variable and objective behavior with respect to feasibility and optimality , permitting intuitive decision making for designers. However, visualization of optimization problems in higher dimensions is challenging, though it is desirable. Interpretable self-organizing map (iSOM) is an artificial neural network that enables visualization of many dimensions via two-dimensional representations. We introduce iSOM to solve multidimensional optimization problems graphically. In the current work, a novel graphical representation of the n n -dimensional feasible region, called B-matrix is constructed using iSOM. B-matrix is used to represent feasible range of design variables and objective function on separate plots. Consequently, dimension-wise shrinkage in the search space is also obtained. The proposed approach is demonstrated on various benchmark analytical examples and engineering examples with dimensions ranging from 2 to 30.},
  archive      = {J_NN},
  author       = {Deepak Nagar and Kiran Pannerselvam and Palaniappan Ramu},
  doi          = {10.1016/j.neunet.2022.08.019},
  journal      = {Neural Networks},
  pages        = {398-412},
  shortjournal = {Neural Netw.},
  title        = {A novel data-driven visualization of n-dimensional feasible region using interpretable self-organizing maps (iSOM)},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent adversarial regularized autoencoder for
high-dimensional probabilistic time series prediction. <em>NN</em>,
<em>155</em>, 383–397. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many practical applications require probabilistic prediction of time series to model the distribution on future horizons. With ever-increasing dimensions, much effort has been invested into developing methods that often make an assumption about the independence between time series. Consequently, the probabilistic prediction in high-dimensional environments has become an essential topic with significant challenges. In this paper, we propose a novel probabilistic model called latent adversarial regularized autoencoder , abbreviated as TimeLAR, specifically for high-dimensional multivariate Time Series Prediction (TSP). It integrates the flexibility of Generative Adversarial Networks (GANs) and the capability of autoencoders in extracting higher-level non-linear features. Through flexible autoencoder mapping, TimeLAR learns cross-series relationships and encodes this global information into several latent variables. We design a modified Transformer for these latent variables to capture global temporal patterns and infer latent space prediction distributions, where only one step is required to output multi-step predictions. Furthermore, we employ the GAN to further refine the performance of latent space predictions, by using a discriminator to guide the training of the autoencoder and the Transformer in an adversarial process. Finally, complex distributions of multivariate time series data can be modeled by the non-linear decoder of the autoencoder. The effectiveness of TimeLAR is empirically underpinned by extensive experiments conducted on five real-world high-dimensional time series datasets in the fields of transportation, electricity, and web page views.},
  archive      = {J_NN},
  author       = {Jing Zhang and Qun Dai},
  doi          = {10.1016/j.neunet.2022.08.025},
  journal      = {Neural Networks},
  pages        = {383-397},
  shortjournal = {Neural Netw.},
  title        = {Latent adversarial regularized autoencoder for high-dimensional probabilistic time series prediction},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imbalanced low-rank tensor completion via latent matrix
factorization. <em>NN</em>, <em>155</em>, 369–382. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor completion has been widely used in computer vision and machine learning . Most existing tensor completion methods empirically assume the intrinsic tensor is simultaneous low-rank in all over modes. However, tensor data recorded from real-world applications may conflict with these assumptions, e.g., face images taken from different subjects often lie in a union of low-rank subspaces, which may result in a quite high rank or even full rank structure in its sample mode. To this aim, in this paper, we propose an imbalanced low-rank tensor completion method, which can flexibly estimate the low-rank incomplete tensor via decomposing it into a mixture of multiple latent tensor ring (TR) rank components. Specifically, each latent component is approximated using low-rank matrix factorization based on TR unfolding matrix. In addition, an effective proximal alternating minimization algorithm is developed and theoretically proved to maintain the global convergence property , that is, the whole sequence of iterates is convergent and converges to a critical point. Extensive experiments on both synthetic and real-world tensor data demonstrate that the proposed method achieves more favorable completion results with less computational cost when compared to the state-of-the-art tensor completion methods.},
  archive      = {J_NN},
  author       = {Yuning Qiu and Guoxu Zhou and Junhua Zeng and Qibin Zhao and Shengli Xie},
  doi          = {10.1016/j.neunet.2022.08.023},
  journal      = {Neural Networks},
  pages        = {369-382},
  shortjournal = {Neural Netw.},
  title        = {Imbalanced low-rank tensor completion via latent matrix factorization},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFINet: Attentive feature integration networks for image
classification. <em>NN</em>, <em>155</em>, 360–368. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have achieved tremendous success in a number of learning tasks including image classification . Residual-like networks, such as ResNets, mainly focus on the skip connection to avoid gradient vanishing. However, the skip connection mechanism limits the utilization of intermediate features due to simple iterative updates. To mitigate the redundancy of residual-like networks, we design Attentive Feature Integration (AFI) modules, which are widely applicable to most residual-like network architectures , leading to new architectures named AFI-Nets. AFI-Nets explicitly model the correlations among different levels of features and selectively transfer features with a little overhead. AFI-ResNet-152 obtains a 1.24\% relative improvement on the ImageNet dataset while decreases the FLOPs by about 10\% and the number of parameters by about 9.2\% compared to ResNet-152.},
  archive      = {J_NN},
  author       = {Xinglin Pan and Jing Xu and Yu Pan and Liangjian Wen and Wenxiang Lin and Kun Bai and Hongguang Fu and Zenglin Xu},
  doi          = {10.1016/j.neunet.2022.08.026},
  journal      = {Neural Networks},
  pages        = {360-368},
  shortjournal = {Neural Netw.},
  title        = {AFINet: Attentive feature integration networks for image classification},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast multiple graphs learning for multi-view clustering.
<em>NN</em>, <em>155</em>, 348–359. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view clustering has become an active topic due to the efficiency in characterizing both the complex structure and relationship between multimedia data. However, existing methods have the following shortcomings: (1) They are inefficient or even fail for graph learning in large scale due to the graph construction and eigen-decomposition. (2) They cannot well exploit both the complementary information and spatial structure embedded in graphs of different views. To well exploit complementary information and tackle the scalability issue plaguing graph-based multi-view clustering, we propose an efficient multiple graph learning model via a small number of anchor points and tensor Schatten p p -norm minimization. Specifically, we construct a hidden and tractable large graph by anchor graph for each view and well exploit complementary information embedded in anchor graphs of different views by tensor Schatten p p -norm regularizer. Finally, we develop an efficient algorithm, which scales linearly with the data size, to solve our proposed model. Extensive experimental results on several datasets indicate that our proposed method outperforms some state-of-the-art multi-view clustering algorithms .},
  archive      = {J_NN},
  author       = {Tianyu Jiang and Quanxue Gao},
  doi          = {10.1016/j.neunet.2022.08.027},
  journal      = {Neural Networks},
  pages        = {348-359},
  shortjournal = {Neural Netw.},
  title        = {Fast multiple graphs learning for multi-view clustering},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-granularity heterogeneous graph attention networks for
extractive document summarization. <em>NN</em>, <em>155</em>, 340–347.
(<a href="https://doi.org/10.1016/j.neunet.2022.08.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extractive document summarization is a fundamental task in natural language processing (NLP). Recently, several Graph Neural Networks (GNNs) are proposed for this task. However, most existing GNN-based models can neither effectively encode semantic nodes of multiple granularity level apart from sentences nor substantially capture different cross-sentence meta-paths. To address these issues, we propose MHgatSum , a novel M ulti-granularity H eterogeneous G raph AT tention networks for extractive document SUM marization. Specifically, we first build a multi-granularity heterogeneous graph (HetG) for each document, which is better to represent the semantic meaning of the document. The HetG contains not only sentence nodes but also multiple other granularity effective semantic units with different semantic levels , including keyphrases and topics. These additional nodes act as the intermediary between sentences to build the meta-paths involved in sentence node (i.e., Sentence-Keyphrase-Sentence and Sentence-Topic-Sentence). Then, we propose a heterogeneous graph attention networks to embed the constructed HetG for extractive summarization, which enjoys multi-granularity semantic representations . The model is based on a hierarchical attention mechanism, including node-level and semantic-level attentions. The node-level attention can learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. Moreover, to better integrate sentence global knowledge, we further incorporate sentence node global importance in local node-level attention. We conduct empirical experiments on two benchmark datasets, which demonstrates the superiority of MHgatSum over previous SOTA models on the task of extractive summarization.},
  archive      = {J_NN},
  author       = {Yu Zhao and Leilei Wang and Cui Wang and Huaming Du and Shaopeng Wei and Huali Feng and Zongjian Yu and Qing Li},
  doi          = {10.1016/j.neunet.2022.08.021},
  journal      = {Neural Networks},
  pages        = {340-347},
  shortjournal = {Neural Netw.},
  title        = {Multi-granularity heterogeneous graph attention networks for extractive document summarization},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel lyapunov stability analysis of neutral-type
cohen–grossberg neural networks with multiple delays. <em>NN</em>,
<em>155</em>, 330–339. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major target of this research article is to conduct a new Lyapunov stability analysis of a special model of Cohen–Grossberg neural networks that include multiple delay terms in state variables of systems neurons and multiple delay terms in time derivatives of state variables of systems neurons in the network structure. Employing some proper linear combinations of three different positive definite and positive semi-definite Lyapunov functionals, we obtain some novel sufficient criteria that guarantee global asymptotic stability of this type of multiple delayed Cohen–Grossberg type neural systems. These newly derived stability results are determined to be completely independent of the involved time delay terms and neutral delay terms, and they are totally characterized by the values of the interconnection parameters of Cohen–Grossberg neural system. Besides, the validation of the obtained stability criteria can be justified by applying some simple appropriate algebraic equations that form some particular relations among the constant system elements of the considered neutral neural systems. A useful and instructive numerical example is analysed to exhibit some major advantages and novelties of these newly proposed global stability results in this paper over some previously reported corresponding asymptotic stability conditions.},
  archive      = {J_NN},
  author       = {Ozlem Faydasicok and Sabri Arik},
  doi          = {10.1016/j.neunet.2022.08.022},
  journal      = {Neural Networks},
  pages        = {330-339},
  shortjournal = {Neural Netw.},
  title        = {A novel lyapunov stability analysis of neutral-type Cohen–Grossberg neural networks with multiple delays},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural networks special issue on artificial intelligence and
brain science. <em>NN</em>, <em>155</em>, 328–329. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Kenji Doya (Guest Editors) and Karl Friston and Masashi Sugiyama and Josh Tenenbaum},
  doi          = {10.1016/j.neunet.2022.08.018},
  journal      = {Neural Networks},
  pages        = {328-329},
  shortjournal = {Neural Netw.},
  title        = {Neural networks special issue on artificial intelligence and brain science},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised knowledge distillation for complementary
label learning. <em>NN</em>, <em>155</em>, 318–327. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle a new learning paradigm called learning from complementary labels, where the training data specifies classes that instances do not belong to, instead of the accuracy labels. In general, it is more efficient to collect the complementary labels compared with collecting the supervised ones, with no need for selecting the correct one from a number of candidates. While current state-of-the-art methods design various loss functions to train competitive models by the limited supervised information , they overlook learning from the data and model themselves, which always contain fruitful information that can improve the performance of complementary label learning. In this paper, we propose a novel learning framework, which seamlessly integrates self-supervised and self-distillation to complementary learning. Based on the general complementary learning framework, we employ an entropy regularization term to guarantee the network outputs exhibit a sharper state. Then, to intensively learn information from the data, we leverage the self-supervised learning based on rotation and transformation operations as a plug-in auxiliary task to learn better transferable representations. Finally, knowledge distillation is introduced to further extract the “dark knowledge” from a network to guide the training of a student network. In the extensive experiments, our method surprisingly demonstrates compelling performance in accuracy over several state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Jiabin Liu and Biao Li and Minglong Lei and Yong Shi},
  doi          = {10.1016/j.neunet.2022.08.014},
  journal      = {Neural Networks},
  pages        = {318-327},
  shortjournal = {Neural Netw.},
  title        = {Self-supervised knowledge distillation for complementary label learning},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent based optimal equilibrium selection with
resilience constraints for traffic flow. <em>NN</em>, <em>155</em>,
308–317. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic guidance and traffic control are effective means to alleviate traffic problems. Formulating effective traffic guidance measures can improve the utilization rate of road resources and optimize the performance of the entire traffic network. Assuming that the traffic coordinator can capture traffic flow information in real-time utilizing sensors installed on each road, we consider the strong resilience constraints to construct an optimal selection problem of balanced flow in the traffic network. Based on multi-agent modeling, each agent has access to the corresponding traffic information of each link. We design a distributed optimization algorithm to tackle this optimization problem . In addition to the inherent advantages of distributed multi-agent algorithms, the communication topology among the sensors is allowed to be time-varying, which is more consistent with reality. To prove the effectiveness of the proposed algorithm, we also give a numerical simulation in the multi-agent environment. It should be reiterated that the optimization problem studied in this paper mainly focuses on traffic managers’ perspectives. The goal of the studied optimization problem is to minimize the overall cost of the traffic network by adjusting the optimal equilibrium traffic flow. This study provides a reference for solving congestion optimization in today’s cities.},
  archive      = {J_NN},
  author       = {Ping Liu and Iakov Korovin and Sergey Gorbachev and Nadezhda Gorbacheva and Jinde Cao},
  doi          = {10.1016/j.neunet.2022.08.013},
  journal      = {Neural Networks},
  pages        = {308-317},
  shortjournal = {Neural Netw.},
  title        = {Multi-agent based optimal equilibrium selection with resilience constraints for traffic flow},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Unsupervised robust discriminative subspace representation
based on discriminative approximate isometric embedding. <em>NN</em>,
<em>155</em>, 287–307. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace learning has shown a tremendous potential in the fields of machine learning and computer vision due to its effectiveness. Subspace representation is a key subspace learning method that encodes subspace membership information. To effectively encode the subspace memberships of data, some structured prior constraints are imposed on the subspace representation, such as low-rank, sparse, and so on. To handle various noises, existing methods tend to separate a specific type of noise using a specific way to obtain robust subspace representation. When encountering diversified noises, their subspace-preserving property may not be guaranteed. To address this issue, we propose a novel unsupervised robust discriminative subspace representation to mitigate the impacts of diversified noises via discriminative approximate isometric embedding, rather than directly separating noises from the high-dimensional space, as done like the existing methods. To ensure the performance of our approach, we provide a crucial theorem, termed as noisy Johnson–Lindenstrauss theorem. Meanwhile, Laplacian rank constraint is imposed on the discriminative subspace representation to uncover the ground truth subspace memberships of noisy data and improve the graph connectivity of subspaces. Extensive experiments on several benchmark datasets and two large-scale datasets validate the effectiveness and robustness of our approach with respect to diversified noises.},
  archive      = {J_NN},
  author       = {Jianwei Li},
  doi          = {10.1016/j.neunet.2022.06.003},
  journal      = {Neural Networks},
  pages        = {287-307},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised robust discriminative subspace representation based on discriminative approximate isometric embedding},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical binding in convolutional neural networks:
Making adversarial attacks geometrically challenging. <em>NN</em>,
<em>155</em>, 258–286. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We approach the issue of robust machine vision by presenting a novel deep-learning architecture, inspired by work in theoretical neuroscience on how the primate brain performs visual feature binding. Feature binding describes how separately represented features are encoded in a relationally meaningful way, such as an edge composing part of the larger contour of an object. We propose that the absence of such representations from current models might partly explain their vulnerability to small, often humanly-imperceptible distortions known as adversarial examples . It has been proposed that adversarial examples are a result of ‘off-manifold’ perturbations of images. Our novel architecture is designed to approximate hierarchical feature binding, providing explicit representations in these otherwise vulnerable directions. Having introduced these representations into convolutional neural networks , we provide empirical evidence of enhanced robustness against a broad range of L 0 L0 , L 2 L2 and L ∞ L∞ attacks, particularly in the black-box setting. While we eventually report that the model remains vulnerable to a sufficiently powerful attacker (i.e. the defense can be broken), we demonstrate that our main results cannot be accounted for by trivial, false robustness (gradient masking). Analysis of the representational geometry of our architectures shows a positive relationship between hierarchical binding, expanded manifolds, and robustness. Through hyperparameter manipulation, we find evidence that robustness emerges through the preservation of general low-level information alongside more abstract features, rather than by capturing which specific low-level features drove the abstract representation. Finally, we propose how hierarchical binding relates to the observation that, under appropriate viewing conditions, humans show sensitivity to adversarial examples.},
  archive      = {J_NN},
  author       = {Niels Leadholm and Simon Stringer},
  doi          = {10.1016/j.neunet.2022.07.003},
  journal      = {Neural Networks},
  pages        = {258-286},
  shortjournal = {Neural Netw.},
  title        = {Hierarchical binding in convolutional neural networks: Making adversarial attacks geometrically challenging},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LS-NTP: Unifying long- and short-range spatial correlations
for near-surface temperature prediction. <em>NN</em>, <em>155</em>,
242–257. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The near-surface temperature prediction (NTP) is an important spatial–temporal forecast problem, which can be used to prevent temperature crises. Most of the previous approaches fail to explicitly model the long- and short-range spatial correlations simultaneously, which is critical to making an accurate temperature prediction. In this study, both long- and short-range spatial correlations are captured to fill this gap by a novel convolution operator named Long- and Short-range Convolution (LS-Conv). The proposed LS-Conv operator includes three key components, namely, Node-based Spatial Attention (NSA), Long-range Adaptive Graph Constructor (LAGC), and Long- and Short-range Integrator (LSI). To capture long-range spatial correlations, NSA and LAGC are proposed to evaluate node importance aiming at auto-constructing long-range spatial correlations, which is named as Long-range aware Graph Convolution Network (LR-GCN). After that, the Short-range aware Convolution Neural Network (SR-CNN) accounts for the short-range spatial correlations. Finally, LSI is proposed to capture both long- and short-range spatial correlations by intra-unifying LR-GCN and SR-CNN. Upon the proposed LS-Conv operator, a new model called Long- and Short-range for NPT (LS-NTP) is developed. Extensive experiments are conducted on two real-world datasets and the results demonstrate that the proposed method outperforms state-of-the-art techniques. The source code is available on GitHub: https://github.com/xuguangning1218/LS_NTP .},
  archive      = {J_NN},
  author       = {Guangning Xu and Xutao Li and Shanshan Feng and Yunming Ye and Zhihua Tu and Kenghong Lin and Zhichao Huang},
  doi          = {10.1016/j.neunet.2022.07.022},
  journal      = {Neural Networks},
  pages        = {242-257},
  shortjournal = {Neural Netw.},
  title        = {LS-NTP: Unifying long- and short-range spatial correlations for near-surface temperature prediction},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-based image deconstruction method with
maintained saliency. <em>NN</em>, <em>155</em>, 224–241. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual properties that primarily attract bottom-up attention are collectively referred to as saliency. In this study, to understand the neural activity involved in top-down and bottom-up visual attention, we aim to prepare pairs of natural and unnatural images with common saliency. For this purpose, we propose an image transformation method based on deep neural networks that can generate new images while maintaining the consistent feature map, in particular the saliency map . This is an ill-posed problem because the transformation from an image to its corresponding feature map could be many-to-one, and in our particular case, the various images would share the same saliency map. Although stochastic image generation has the potential to solve such ill-posed problems, the most existing methods focus on adding diversity of the overall style/touch information while maintaining the naturalness of the generated images. To this end, we developed a new image transformation method that incorporates higher-dimensional latent variables so that the generated images appear unnatural with less context information but retain a high diversity of local image structures. Although such high-dimensional latent spaces are prone to collapse, we proposed a new regularization based on Kullback–Leibler divergence to avoid collapsing the latent distribution. We also conducted human experiments using our newly prepared natural and corresponding unnatural images to measure overt eye movements and functional magnetic resonance imaging, and found that those images induced distinctive neural activities related to top-down and bottom-up attentional processing.},
  archive      = {J_NN},
  author       = {Keisuke Fujimoto and Kojiro Hayashi and Risa Katayama and Sehyung Lee and Zhen Liang and Wako Yoshida and Shin Ishii},
  doi          = {10.1016/j.neunet.2022.08.015},
  journal      = {Neural Networks},
  pages        = {224-241},
  shortjournal = {Neural Netw.},
  title        = {Deep learning-based image deconstruction method with maintained saliency},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inertial neural network approach for loco-manipulation
trajectory tracking of mobile robot with redundant manipulator.
<em>NN</em>, <em>155</em>, 215–223. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel constrained optimization model to address the loco-manipulation problem of mobile robot with redundant manipulator for trajectory tracking. To alleviate the accumulative error of the end-effector’s position, a new control law is designed to eliminate the negative effect from the deviation of the initial position, leading to better performance than existing ones. To deal with the locomotion constraints in the loco-manipulation problem, the optimization model is converted to an augmented Lagrangian primal–dual problem. Furthermore, an inertial neural network approach is used to solve the problem and the corresponding Lyapunov proof guarantees the convergence of variables. The numerical simulations show that the proposed approach is more suitable for application since the model is more effective and the algorithm has better convergence rate.},
  archive      = {J_NN},
  author       = {Chentao Xu and Miao Wang and Guoyi Chi and Qingshan Liu},
  doi          = {10.1016/j.neunet.2022.08.012},
  journal      = {Neural Networks},
  pages        = {215-223},
  shortjournal = {Neural Netw.},
  title        = {An inertial neural network approach for loco-manipulation trajectory tracking of mobile robot with redundant manipulator},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synergetic learning structure-based neuro-optimal fault
tolerant control for unknown nonlinear systems. <em>NN</em>,
<em>155</em>, 204–214. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a synergetic learning structure-based neuro-optimal fault tolerant control (SLSNOFTC) method is proposed for unknown nonlinear continuous-time systems with actuator failures . Under the framework of the synergetic learning structure (SLS), the optimal control input and the actuator failure are viewed as two subsystems. Then, the fault tolerant control (FTC) problem can be regarded as a two-player zero-sum differential game according to the game theory . A radial basis function neural network-based identifier, which uses the measured input/output data, is constructed to identify the completely unknown system dynamics . To develop the SLSNOFTC method, the Hamilton–Jacobi–Isaacs equation is solved by an asymptotically stable critic neural network (ASCNN) which is composed of cooperative adaptive tuning laws. Besides, with the help of the Lyapunov stability analysis , the identification error, the weight error of ASCNN, and all signals of closed-loop system are guaranteed to be converged to zero asymptotically, rather than uniformly ultimately bounded. Numerical simulation examples further verify the effectiveness and reliability of the proposed method.},
  archive      = {J_NN},
  author       = {Hongbing Xia and Bo Zhao and Ping Guo},
  doi          = {10.1016/j.neunet.2022.08.010},
  journal      = {Neural Networks},
  pages        = {204-214},
  shortjournal = {Neural Netw.},
  title        = {Synergetic learning structure-based neuro-optimal fault tolerant control for unknown nonlinear systems},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyper-flexible convolutional neural networks based on
generalized lehmer and power means. <em>NN</em>, <em>155</em>, 177–203.
(<a href="https://doi.org/10.1016/j.neunet.2022.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network is one of the famous members of the deep learning family of neural network architectures, which is used for many purposes, including image classification . In spite of the wide adoption, such networks are known to be highly tuned to the training data (samples representing a particular problem), and they are poorly reusable to address new problems. One way to change this would be, in addition to trainable weights, to apply trainable parameters of the mathematical functions , which simulate various neural computations within such networks. In this way, we may distinguish between the narrowly focused task-specific parameters (weights) and more generic capability-specific parameters. In this paper, we suggest a couple of flexible mathematical functions (Generalized Lehmer Mean and Generalized Power Mean) with trainable parameters to replace some fixed operations (such as ordinary arithmetic mean or simple weighted aggregation), which are traditionally used within various components of a convolutional neural network architecture. We named the overall architecture with such an update as a hyper-flexible convolutional neural network. We provide mathematical justification of various components of such architecture and experimentally show that it performs better than the traditional one, including better robustness regarding the adversarial perturbations of testing data.},
  archive      = {J_NN},
  author       = {Vagan Terziyan and Diana Malyk and Mariia Golovianko and Vladyslav Branytskyi},
  doi          = {10.1016/j.neunet.2022.08.017},
  journal      = {Neural Networks},
  pages        = {177-203},
  shortjournal = {Neural Netw.},
  title        = {Hyper-flexible convolutional neural networks based on generalized lehmer and power means},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A differentiable approach to the maximum independent set
problem using dataless neural networks. <em>NN</em>, <em>155</em>,
168–176. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of machine learning solutions for reasoning about discrete structures has brought attention to its adoption within combinatorial optimization algorithms. Such approaches generally rely on supervised learning by leveraging datasets of the combinatorial structures of interest drawn from some distribution of problem instances. Reinforcement learning has also been employed to find such structures. In this paper, we propose a different approach in that no data is required for training the neural networks that produce the solution. In this sense, what we present is not a machine learning solution, but rather one that is dependent on neural networks and where backpropagation is applied to a loss function defined by the structure of the neural network architecture as opposed to a training dataset. In particular, we reduce the popular combinatorial optimization problem of finding a maximum independent set to a neural network and employ a dataless training scheme to refine the parameters of the network such that those parameters yield the structure of interest. Additionally, we propose a universal graph reduction procedure to handle large-scale graphs. The reduction exploits community detection for graph partitioning and is applicable to any graph type and/or density. Experimental results on both real and synthetic graphs demonstrate that our proposed method performs on par or outperforms state-of-the-art learning-based methods in terms of the size of the found set without requiring any training data.},
  archive      = {J_NN},
  author       = {Ismail R. Alkhouri and George K. Atia and Alvaro Velasquez},
  doi          = {10.1016/j.neunet.2022.08.008},
  journal      = {Neural Networks},
  pages        = {168-176},
  shortjournal = {Neural Netw.},
  title        = {A differentiable approach to the maximum independent set problem using dataless neural networks},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DualG-GAN, a dual-channel generator based generative
adversarial network for text-to-face synthesis. <em>NN</em>,
<em>155</em>, 155–167. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image synthesis is a fundamental and challenging task in computer vision , which aims to synthesize realistic images from given descriptions. Recently, text-to-image synthesis methods have achieved great improvements in the quality of synthesized images. However, very few works have explored its application in the scenario of face synthesis, which is of great potentials in face-related applications and the public safety domain. On the other side, the faces generated by existing methods are generally of poor quality and have low consistency to the given text. To tackle this issue, in this paper, we build a novel end-to-end dual-channel generator based generative adversarial network , named DualG-GAN, to improve the quality of the generated images and the consistency to the text description. In DualG-GAN, to improve the consistency between the synthesized image and the input description, a dual-channel generator block is introduced, and a novel loss is designed to improve the similarity between the generated image and the ground-truth in three different semantic levels . Extensive experiments demonstrate that DualG-GAN achieves state-of-the-art results on SCU-Text2face dataset. To further verify the performance of DualG-GAN, we compare it with the current optimal methods on text-to-image synthesis tasks, where quantitative and qualitative results show that the proposed DualG-GAN achieves optimal performance in both Fréchet inception distance (FID) and R-precision metrics. As only a few works are focusing on text-to-face synthesis, this work can be seen as a baseline for future research.},
  archive      = {J_NN},
  author       = {Xiaodong Luo and Xiaohai He and Xiang Chen and Linbo Qing and Jin Zhang},
  doi          = {10.1016/j.neunet.2022.08.016},
  journal      = {Neural Networks},
  pages        = {155-167},
  shortjournal = {Neural Netw.},
  title        = {DualG-GAN, a dual-channel generator based generative adversarial network for text-to-face synthesis},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure enhanced deep clustering network via a weighted
neighbourhood auto-encoder. <em>NN</em>, <em>155</em>, 144–154. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural deep clustering involves the use of neural networks for fusing semantic and structural representations for clustering tasks , and it has been receiving increasing attention. In some pioneering works, auto-encoder (AE)-specific representations were integrated with a graph convolutional network (GCN)-specific representation by delivering semantic information to the GCN module layer-by-layer. Although promising performance has been achieved in various applications, we observed that a vital aspect was overlooked in these works: the structural information may vanish in the learning process because of the over-smoothing problem of the GCN module, leading to non-representative features and, thus, deteriorating clustering performance. In this study, we address this issue by proposing a structure enhanced deep clustering network. The GCN-specific structural data representation is enhanced and supervised by its structural information. Specifically, the GCN-specific structural data representation is strengthened during the learning process by combining it with a structure enhanced semantic (SES) representation. A novel structure enhanced AE, named the weighted neighbourhood AE (wNAE), is employed to learn the SES representation for each data sample. Finally, we design a joint supervision strategy to uniformly guide the simultaneous learning of the wNAE and GCN modules and the clustering assignment. Experimental results for different datasets empirically validate the importance of semantic and neighbour-wise structure learning .},
  archive      = {J_NN},
  author       = {Ruina Bai and Ruizhang Huang and Luyi Zheng and Yanping Chen and Yongbin Qin},
  doi          = {10.1016/j.neunet.2022.08.006},
  journal      = {Neural Networks},
  pages        = {144-154},
  shortjournal = {Neural Netw.},
  title        = {Structure enhanced deep clustering network via a weighted neighbourhood auto-encoder},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three approaches to facilitate invariant neurons and
generalization to out-of-distribution orientations and illuminations.
<em>NN</em>, <em>155</em>, 119–143. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training data distribution is often biased towards objects in certain orientations and illumination conditions . While humans have a remarkable capability of recognizing objects in out-of-distribution (OoD) orientations and illuminations, Deep Neural Networks (DNNs) severely suffer in this case, even when large amounts of training examples are available. Neurons that are invariant to orientations and illuminations have been proposed as a neural mechanism that could facilitate OoD generalization, but it is unclear how to encourage the emergence of such invariant neurons. In this paper, we investigate three different approaches that lead to the emergence of invariant neurons and substantially improve DNNs in recognizing objects in OoD orientations and illuminations. Namely, these approaches are (i) training much longer after convergence of the in-distribution (InD) validation accuracy, i.e., late-stopping, (ii) tuning the momentum parameter of the batch normalization layers, and (iii) enforcing invariance of the neural activity in an intermediate layer to orientation and illumination conditions. Each of these approaches substantially improves the DNN’s OoD accuracy (more than 20\% in some cases). We report results in four datasets: two datasets are modified from the MNIST and iLab datasets, and the other two are novel (one of 3D rendered cars and another of objects taken from various controlled orientations and illumination conditions). These datasets allow to study the effects of different amounts of bias and are challenging as DNNs perform poorly in OoD conditions. Finally, we demonstrate that even though the three approaches focus on different aspects of DNNs, they all tend to lead to the same underlying neural mechanism to enable OoD accuracy gains — individual neurons in the intermediate layers become invariant to OoD orientations and illuminations. We anticipate this study to be a basis for further improvement of deep neural networks’ OoD generalization performance , which is highly demanded to achieve safe and fair AI applications.},
  archive      = {J_NN},
  author       = {Akira Sakai and Taro Sunagawa and Spandan Madan and Kanata Suzuki and Takashi Katoh and Hiromichi Kobashi and Hanspeter Pfister and Pawan Sinha and Xavier Boix and Tomotake Sasaki},
  doi          = {10.1016/j.neunet.2022.07.026},
  journal      = {Neural Networks},
  pages        = {119-143},
  shortjournal = {Neural Netw.},
  title        = {Three approaches to facilitate invariant neurons and generalization to out-of-distribution orientations and illuminations},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explaining aha! Moments in artificial agents through
IKE-XAI: Implicit knowledge extraction for eXplainable AI. <em>NN</em>,
<em>155</em>, 95–118. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the learning process, a child develops a mental representation of the task he or she is learning. A Machine Learning algorithm develops also a latent representation of the task it learns. We investigate the development of the knowledge construction of an artificial agent through the analysis of its behavior , i.e., its sequences of moves while learning to perform the Tower of Hanoï (TOH) task. The TOH is a well-known task in experimental contexts to study the problem-solving processes and one of the fundamental processes of children’s knowledge construction about their world. We position ourselves in the field of explainable reinforcement learning for developmental robotics, at the crossroads of cognitive modeling and explainable AI . Our main contribution proposes a 3-step methodology named Implicit Knowledge Extraction with eXplainable Artificial Intelligence (IKE-XAI) to extract the implicit knowledge, in form of an automaton , encoded by an artificial agent during its learning. We showcase this technique to solve and explain the TOH task when researchers have only access to moves that represent observational behavior as in human–machine interaction. Therefore, to extract the agent acquired knowledge at different stages of its training, our approach combines: first, a Q-learning agent that learns to perform the TOH task; second, a trained recurrent neural network that encodes an implicit representation of the TOH task; and third, an XAI process using a post-hoc implicit rule extraction algorithm to extract finite state automata . We propose using graph representations as visual and explicit explanations of the behavior of the Q-learning agent. Our experiments show that the IKE-XAI approach helps understanding the development of the Q-learning agent behavior by providing a global explanation of its knowledge evolution during learning. IKE-XAI also allows researchers to identify the agent’s Aha! moment by determining from what moment the knowledge representation stabilizes and the agent no longer learns.},
  archive      = {J_NN},
  author       = {Ikram Chraibi Kaadoud and Adrien Bennetot and Barbara Mawhin and Vicky Charisi and Natalia Díaz-Rodríguez},
  doi          = {10.1016/j.neunet.2022.08.002},
  journal      = {Neural Networks},
  pages        = {95-118},
  shortjournal = {Neural Netw.},
  title        = {Explaining aha! moments in artificial agents through IKE-XAI: Implicit knowledge extraction for eXplainable AI},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint clothes image detection and search via anchor free
framework. <em>NN</em>, <em>155</em>, 84–94. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clothes image search is an important learning task in fashion analysis to find the most relevant clothes in a database given a user-provided query. To address this problem, most existing methods employ a two-step approach, i.e. , first detect the target clothes, and then crop it to feed the model for similarity learning. But the two-step approach is time-consuming and resource-intensive. On the other hand, one-step methods provide efficient solutions to integrate clothes detection and search in a unified framework. However, since one-step methods usually explore anchor-based detectors, they inevitably inherit limitations, such as high computational complexity caused by dense anchors, and high sensitivity to hyperparameters. To address the aforementioned issues, we propose an anchor-free framework for joint clothes detection and search. Specifically, we first choose an anchor-free detector as backbone. We then add a mask prediction branch and a Re-ID embedding branch to the framework. The mask prediction branch aims to predict the masks of clothes, while Re-ID embedding branch aims to extract the rich embedding features of clothes, in which we aggregate the feature of clothes via a mask pooling module by referencing the estimated target clothes masks. In this way, the extracted target clothes features can grasp more information in the area of the clothes mask; finally, we further introduce a match loss to fine-tune the embedding feature in Re-ID branch for improving the retrieval performance . Simulation results based on real datasets demonstrate the effectiveness of the proposed work.},
  archive      = {J_NN},
  author       = {Mingbo Zhao and Shanchuan Gao and Jianghong Ma and Zhao Zhang},
  doi          = {10.1016/j.neunet.2022.08.011},
  journal      = {Neural Networks},
  pages        = {84-94},
  shortjournal = {Neural Netw.},
  title        = {Joint clothes image detection and search via anchor free framework},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Location-aware convolutional neural networks for graph
classification. <em>NN</em>, <em>155</em>, 74–83. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph patterns play a critical role in various graph classification tasks , e.g., chemical patterns often determine the properties of molecular graphs. Researchers devote themselves to adapting Convolutional Neural Networks (CNNs) to graph classification due to their powerful capability in pattern learning. The varying numbers of neighbor nodes and the lack of canonical order of nodes on graphs pose challenges in constructing receptive fields for CNNs. Existing methods generally follow a heuristic ranking-based framework, which constructs receptive fields by selecting a fixed number of nodes and dropping the others according to predetermined rules. However, such methods may lose important structure information through dropping nodes, and they also cannot learn task-oriented graph patterns. In this paper, we propose a Location learning-based Convolutional Neural Networks (LCNN) for graph classification. LCNN constructs receptive fields by learning the location of each node according to its embedding that contains structures and features information, then standard CNNs are applied to capture graph patterns. Such a location learning mechanism not only retains the information of all nodes, but also provides the ability for task-oriented pattern learning. Experimental results show the effectiveness of the proposed LCNN, and visualization results further illustrate the valid pattern learning ability of our method for graph classification.},
  archive      = {J_NN},
  author       = {Zhaohui Wang and Qi Cao and Huawei Shen and Bingbing Xu and Keting Cen and Xueqi Cheng},
  doi          = {10.1016/j.neunet.2022.07.035},
  journal      = {Neural Networks},
  pages        = {74-83},
  shortjournal = {Neural Netw.},
  title        = {Location-aware convolutional neural networks for graph classification},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpretable artificial intelligence through locality
guided neural networks. <em>NN</em>, <em>155</em>, 58–73. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current deep learning architectures, each of the deeper layers in networks tends to contain hundreds of unorganized neurons which makes it hard for humans to understand how they interact with each other. By organizing the neurons using correlation as the criteria, humans can observe how clusters of neighbouring neurons interact with each other. Research in Explainable Artificial Intelligence (XAI) aims to all alleviate the black-box nature of current AI methods and make them understandable by humans. In this paper, we extend our previous algorithm for XAI in deep learning , called Locality Guided Neural Network (LGNN). LGNN preserves locality between neighbouring neurons within each layer of a deep network during training. Motivated by Self-Organizing Maps (SOMs), the goal is to enforce a local topology on each layer of a deep network such that neighbouring neurons are highly correlated with each other. Our algorithm can be easily plugged into current state of the art Convolutional Neural Network (CNN) models to make the neighbouring neurons more correlated. A cluster of neighbouring neurons activating for a class makes the network both quantitatively and qualitatively more interpretable when visualized, as we show through our experiments. This paper focuses on image processing with CNNs, but can theoretically be applied to any type of deep learning architecture. In our experiments, we train VGG and WRN networks for image classification on CIFAR100 and Imagenette. Our experiments analyse different perceptible clusters of activations in response to different input classes.},
  archive      = {J_NN},
  author       = {Randy Tan and Lei Gao and Naimul Khan and Ling Guan},
  doi          = {10.1016/j.neunet.2022.08.009},
  journal      = {Neural Networks},
  pages        = {58-73},
  shortjournal = {Neural Netw.},
  title        = {Interpretable artificial intelligence through locality guided neural networks},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LGLNN: Label guided graph learning-neural network for
few-shot learning. <em>NN</em>, <em>155</em>, 50–57. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been employed for few-shot learning (FSL) tasks. The aim of GNN based FSL is to transform the few-shot learning problem into a graph node classification or edge labeling tasks, which can thus fully explore the relationships among samples in support and query sets. However, existing works generally consider the graph learned by node features which ignore the initial pairwise label constraints and thus are generally not guaranteed to be optimal for FSL tasks. Also, existing works generally learn graph edges independently based on node’s own features which lack of considering the consistent relationships among different edges. To address these issues, we propose a novel Label Guided Graph Learning-Neural network (LGLNN) model for FSL tasks. The aim of LGLNN is to incorporate the label information to learn an optimal metric graph for GNN by employing the pairwise constraint propagation . The main advantage of LGLNN is that it can learn the metrics (both similarity and dissimilarity) for each graph edge by aggregating the metric information from its neighboring edges and thus can conduct metric learning of all edges cooperatively and consistently. Experimental results demonstrate the effectiveness and better performance of the proposed LGLNN method.},
  archive      = {J_NN},
  author       = {Kangkang Zhao and Ziyan Zhang and Bo Jiang and Jin Tang},
  doi          = {10.1016/j.neunet.2022.08.003},
  journal      = {Neural Networks},
  pages        = {50-57},
  shortjournal = {Neural Netw.},
  title        = {LGLNN: Label guided graph learning-neural network for few-shot learning},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online spike sorting via deep contractive autoencoder.
<em>NN</em>, <em>155</em>, 39–49. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike sorting – the process of separating spikes from different neurons – is often the first and most critical step in the neural data analysis pipeline. Spike-sorting techniques isolate a single neuron’s activity from background electrical noise based on the shapes of the waveforms obtained from extracellular recordings. Despite several advancements in this area, an important remaining challenge in neuroscience is online spike sorting, which has the potential to significantly advance basic neuroscience research and the clinical setting by providing the means to produce real-time perturbations of neurons via closed-loop control. Current approaches to online spike sorting are not fully automated, are computationally expensive and are often outperformed by offline approaches. In this paper, we present a novel algorithm for fast and robust online classification of single neuron activity. This algorithm is based on a deep contractive autoencoder (CAE) architecture. CAEs are neural networks that can learn a latent state representation of their inputs. The main advantage of CAE-based approaches is that they are less sensitive to noise (i.e., small perturbations in their inputs). We therefore reasoned that they can form the basis for robust online spike sorting algorithms. Overall, our deep CAE-based online spike sorting algorithm achieves over 90\% accuracy in sorting unseen spike waveforms , outperforming existing models and maintaining a performance close to the offline case. In the offline scenario, our method substantially outperforms the existing models, providing an average improvement of 40\% in accuracy over different datasets.},
  archive      = {J_NN},
  author       = {Mohammadreza Radmanesh and Ahmad Asgharian Rezaei and Mahdi Jalili and Alireza Hashemi and Morteza Moazami Goudarzi},
  doi          = {10.1016/j.neunet.2022.08.001},
  journal      = {Neural Networks},
  pages        = {39-49},
  shortjournal = {Neural Netw.},
  title        = {Online spike sorting via deep contractive autoencoder},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast 2-step regularization on style optimization for real
face morphing. <em>NN</em>, <em>155</em>, 28–38. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN is now capable of achieving excellent results, especially high-quality face synthesis. Recently, some studies have tried to invert real face images into style latent space via StyleGAN. However, morphing real faces via latent representation is still in its infancy. Training costs are high and/or require huge samples with labels. By adding regularization to style optimization, we propose a novel method to morph real faces based on StyleGAN. To do the supervised task, we label latent vectors via synthesized faces and release the label set; then we utilize logistic regression to fast discover interpretable directions in latent space. Appropriate regularization helps us to optimize both latent vectors (faces and directions). Moreover, we use learned directions under different layer representations to handle real face morphing. Compared to the existing methods, our method faster yields a larger diverse and realistic output. Code and cases are available at https://github.com/disanda/RFM .},
  archive      = {J_NN},
  author       = {Cheng Yu and Wenmin Wang and Honglei Li and Roberto Bugiolacchi},
  doi          = {10.1016/j.neunet.2022.08.007},
  journal      = {Neural Networks},
  pages        = {28-38},
  shortjournal = {Neural Netw.},
  title        = {Fast 2-step regularization on style optimization for real face morphing},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A privacy preservation framework for feedforward-designed
convolutional neural networks. <em>NN</em>, <em>155</em>, 14–27. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A feedforward-designed convolutional neural network (FF-CNN) is an interpretable neural network with low training complexity. Unlike a neural network trained using backpropagation (BP) algorithms and optimizers (e.g., stochastic gradient descent (SGD) and Adam), a FF-CNN obtains the model parameters in one feed-forward calculation based on two methods of data statistics : subspace approximation with adjusted bias and least squares regression . Currently, models based on FF-CNN training methods have achieved outstanding performance in the fields of image classification and point cloud data processing. In this study, we analyze and verify that there is a risk of user privacy leakage during the training process of FF-CNN and existing privacy-preserving methods for model gradients or loss functions do not apply to FF-CNN models. Therefore, we propose a securely forward-designed convolutional neural network algorithm (SFF-CNN) to protect the privacy and security of data providers for the FF-CNN model. Firstly, we propose the DPSaab algorithm to add the corresponding noise to the one-stage Saab transform in the FF-CNN design for improved protection performance. Secondly, because noise addition brings the risk of model over-fitting and further increases the possibility of privacy leakage , we propose the SJS algorithm to filter the input features of the fully connected model layer. Finally, we theoretically prove that the proposed algorithm satisfies differential privacy and experimentally demonstrate that the proposed algorithm has strong privacy protection. The proposed algorithm outperforms the compared deep learning privacy-preserving algorithms in terms of utility and robustness.},
  archive      = {J_NN},
  author       = {De Li and Jinyan Wang and Qiyu Li and Yuhang Hu and Xianxian Li},
  doi          = {10.1016/j.neunet.2022.08.005},
  journal      = {Neural Networks},
  pages        = {14-27},
  shortjournal = {Neural Netw.},
  title        = {A privacy preservation framework for feedforward-designed convolutional neural networks},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain-inspired chaotic backpropagation for MLP. <em>NN</em>,
<em>155</em>, 1–13. (<a
href="https://doi.org/10.1016/j.neunet.2022.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation (BP) algorithm is one of the most basic learning algorithms in deep learning . Although BP has been widely used, it still suffers from the problem of easily falling into the local minima due to its gradient dynamics. Inspired by the fact that the learning of real brains may exploit chaotic dynamics, we propose the chaotic backpropagation (CBP) algorithm by integrating the intrinsic chaos of real neurons into BP . By validating on multiple datasets (e.g. cifar10), we show that, for multilayer perception (MLP), CBP has significantly better abilities than those of BP and its variants in terms of optimization and generalization from both computational and theoretical viewpoints. Actually, CBP can be regarded as a general form of BP with global searching ability inspired by the chaotic learning process in the brain. Therefore, CBP not only has the potential of complementing or replacing BP in deep learning practice, but also provides a new way for understanding the learning process of the real brain.},
  archive      = {J_NN},
  author       = {Peng Tao and Jie Cheng and Luonan Chen},
  doi          = {10.1016/j.neunet.2022.08.004},
  journal      = {Neural Networks},
  pages        = {1-13},
  shortjournal = {Neural Netw.},
  title        = {Brain-inspired chaotic backpropagation for MLP},
  volume       = {155},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>154</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00363-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00363-X},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>154</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00349-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00349-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning with workload-aware client scheduling in
heterogeneous systems. <em>NN</em>, <em>154</em>, 560–573. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a novel distributed machine learning , which allows thousands of edge devices to train models locally without uploading data to the central server. Since devices in real federated settings are resource-constrained, FL encounters systems heterogeneity , which causes considerable stragglers and incurs significant accuracy degradation. To tackle the challenges of systems heterogeneity and improve the robustness of the global model, we propose a novel adaptive federated framework in this paper. Specifically, we propose FedSAE that leverages the workload completion history of clients to adaptively predict the affordable training workload for each device. Consequently, FedSAE can significantly reduce stragglers in highly heterogeneous systems . We incorporate Active Learning into FedSAE to dynamically schedule participants. The server evaluates the devices’ training value based on their training loss in each round, and larger-value clients are selected with a higher probability . As a result, the model convergence is accelerated. Furthermore, we propose q-FedSAE that combines FedSAE and q-FFL to improve global fairness in highly heterogeneous systems . The evaluations conducted in a highly heterogeneous system demonstrate that both FedSAE and q-FedSAE converge faster than FedAvg . In particular, FedSAE outperforms FedAvg across multiple federated datasets — FedSAE improves testing accuracy by 22.19\% and reduces stragglers by 90.69\% on average. Moreover, holding the same accuracy as FedSAE , q-FedSAE allows for more robust convergence and fairer model performance than q-FedAvg , FedSAE .},
  archive      = {J_NN},
  author       = {Li Li and Duo Liu and Moming Duan and Yu Zhang and Ao Ren and Xianzhang Chen and Yujuan Tan and Chengliang Wang},
  doi          = {10.1016/j.neunet.2022.07.030},
  journal      = {Neural Networks},
  pages        = {560-573},
  shortjournal = {Neural Netw.},
  title        = {Federated learning with workload-aware client scheduling in heterogeneous systems},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event stream learning using spatio-temporal event surface.
<em>NN</em>, <em>154</em>, 543–559. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras sense changes in light intensity and record them as an asynchronous event stream. Efficiently encoding and learning spatiotemporal information of the event streams remain challenging. In this paper, we propose a novel event descriptor to encode the spatio-temporal features for event streams and a local-search based multi-spike learning algorithm for spiking neural networks to classify encoded features. The spatio-temporal event surface (STES) descriptor explicitly captures both spatial and temporal correlations among events, and thus can characterize spatiotemporal features more accurately than existing feature descriptors that focus only on temporal or spatial information. In classification with multi-spike learning, we introduce a local search and gradient clipping mechanism to ensure the efficiency and stability of learning, which avoids other multi-spike learning rules’ time-consuming global search and the gradient explosion problem. Experimental results demonstrate the superior classification performance of our proposed model, especially for event streams with rich spatiotemporal dynamics.},
  archive      = {J_NN},
  author       = {Junfei Dong and Runhao Jiang and Rong Xiao and Rui Yan and Huajin Tang},
  doi          = {10.1016/j.neunet.2022.07.010},
  journal      = {Neural Networks},
  pages        = {543-559},
  shortjournal = {Neural Netw.},
  title        = {Event stream learning using spatio-temporal event surface},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal neural networks better explain multivoxel
patterns in the hippocampus. <em>NN</em>, <em>154</em>, 538–542. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human hippocampus possesses “concept cells”, neurons that fire when presented with stimuli belonging to a specific concept, regardless of the modality. Recently, similar concept cells were discovered in a multimodal network called CLIP (Radford et al., 2021). Here, we ask whether CLIP can explain the fMRI activity of the human hippocampus better than a purely visual (or linguistic) model. We extend our analysis to a range of publicly available uni- and multi-modal models. We demonstrate that “multimodality” stands out as a key component when assessing the ability of a network to explain the multivoxel activity in the hippocampus.},
  archive      = {J_NN},
  author       = {Bhavin Choksi and Milad Mozafari and Rufin VanRullen and Leila Reddy},
  doi          = {10.1016/j.neunet.2022.07.033},
  journal      = {Neural Networks},
  pages        = {538-542},
  shortjournal = {Neural Netw.},
  title        = {Multimodal neural networks better explain multivoxel patterns in the hippocampus},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous pseudo-supervised learning for few-shot person
re-identification. <em>NN</em>, <em>154</em>, 521–537. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to obtain good retrieval performance in the case of few-shot labeled samples is the current research focus of Person Re-Identification. To facilitate formal analysis, we formally put forward the concept of Pseudo-Supervised Learning (PSL) to represent a series of research works based on label generation under few-shot condition. Through extensive investigations, we find that the main problem that needs to be solved of PSL is how we can improve the quality of pseudo-label. To solve this problem, in this work, we proposed a simple yet effective Heterogeneous Pseudo-Supervised Learning (H-PSL) framework based on classical PSL to implement asynchronous match, which boosts the feature expression and then a better label prediction in the following. Specifically, a novel isomer is constructed as the feature extractor and is trained with a much larger amount of pseudo-supervised data, i.e. , samples with pseudo-labels. In this way, the isomer obtains advanced feature expression. We then deliberately implement a cross-level asynchronous match mechanism between model and pseudo-supervised data. As a result, the quality of pseudo-label is greatly improved and the feature expression performance also be optimized accordingly. In addition, to make better use of pseudo-supervised data, we also designed a knowledge fusion strategy to integrate the pseudo labels and their confidence which are easily obtained by the base model and isomer. Encouragingly, knowledge fusion strategy further removes the noise-labeled samples from candidate data. We conduct experiments on four popular datasets to fully verify the universality of the proposed method. The experimental results show that the proposed method improves the performance of all compared baseline works.},
  archive      = {J_NN},
  author       = {Jing Zhao and Long Lan and Da Huang and Jing Ren and Wenjing Yang},
  doi          = {10.1016/j.neunet.2022.06.017},
  journal      = {Neural Networks},
  pages        = {521-537},
  shortjournal = {Neural Netw.},
  title        = {Heterogeneous pseudo-supervised learning for few-shot person re-identification},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A portable clustering algorithm based on compact neighbors
for face tagging. <em>NN</em>, <em>154</em>, 508–520. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the following problem: Given a collection of unlabeled facial images , group them into the individual identities where the number of subjects is not known. To this end, a Portable clustering algorithm based on Compact Neighbors called PCN is proposed. (1) Benefiting from the compact neighbor, the local density of each sample can be determined automatically and only one user-specified parameter, the number of nearest neighbors k k , is involved in our model. (2) More importantly, the performance of PCN is not sensitive to the number of nearest neighbors. Therefore this parameter is relatively easy to determine in practical applications. (3) The computational overhead of PCN is O ( n k ( k 2 + l o g ( n k ) ) ) O(nk(k2+log(nk))) that is nearly linear with respect to the number of samples, which means it is easily scalable to large-scale problems. In order to verify the effectiveness of PCN on the face clustering problem , extensive experiments based on a two-stage framework (extracting features using a deep model and performing clustering in the feature space) have been conducted on 16 middle- and 5 large-scale benchmark datasets. The experimental results have shown the efficiency and effectiveness of the proposed algorithm, compared with state-of-the-art methods. [code]},
  archive      = {J_NN},
  author       = {Shenfei Pei and Yuze Zhang and Rong Wang and Feiping Nie},
  doi          = {10.1016/j.neunet.2022.07.025},
  journal      = {Neural Networks},
  pages        = {508-520},
  shortjournal = {Neural Netw.},
  title        = {A portable clustering algorithm based on compact neighbors for face tagging},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended analysis on the global mittag-leffler
synchronization problem for fractional-order octonion-valued BAM neural
networks. <em>NN</em>, <em>154</em>, 491–507. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new case of neural networks called fractional-order octonion-valued bidirectional associative memory neural networks (FOOVBAMNNs) is established. First, the higher dimensional models are formulated for FOOVBAMNNs with general activation functions and the special linear threshold ones, respectively. On one hand, employing Cayley–Dichson construction in octonion multiplication which is essentially neither commutative nor associative, the system of FOOVBAMNNs is divided into four fractional-order complex-valued ones. On the other hand, Caputo fractional derivative’s character and BAM’s interactive feature are also properly dealt with. Second, the general criteria are obtained by the new design of LKFs, the application of the related inequalities and the construction of the linear feedback controllers for the global Mittag-Leffler synchronization problem of FOOVBAMNNs. Finally, we present two numerical examples to show the realizability and progress of the derived results.},
  archive      = {J_NN},
  author       = {Jianying Xiao and Xiao Guo and Yongtao Li and Shiping Wen and Kaibo Shi and Yiqian Tang},
  doi          = {10.1016/j.neunet.2022.07.031},
  journal      = {Neural Networks},
  pages        = {491-507},
  shortjournal = {Neural Netw.},
  title        = {Extended analysis on the global mittag-leffler synchronization problem for fractional-order octonion-valued BAM neural networks},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate time-series classification with hierarchical
variational graph pooling. <em>NN</em>, <em>154</em>, 481–490. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multivariate time-series classification (MTSC) has attracted considerable attention owing to the advancement of sensing technology. Existing deep-learning-based MTSC techniques, which mostly rely on convolutional or recurrent neural networks , focus primarily on the temporal dependency of a single time series. Based on this, complex pairwise dependencies among multivariate variables can be better described using advanced graph methods, where each variable is regarded as a node in the graph, and their dependencies are regarded as edges. Furthermore, current spatial–temporal modeling (e.g., graph classification) methodologies based on graph neural networks (GNNs) are inherently flat and cannot hierarchically aggregate node information. To address these limitations, we propose a novel graph-pooling-based framework, MTPool, to obtain an expressive global representation of MTS. We first convert MTS slices into graphs using the interactions of variables via a graph structure learning module and obtain the spatial–temporal graph node features via a temporal convolutional module. To obtain global graph-level representation, we design an “encoder-decoder”-based variational graph pooling module to create adaptive centroids for cluster assignments. Then, we combine GNNs and our proposed variational graph pooling layers for joint graph representation learning and graph coarsening, after which the graph is progressively coarsened to one node. Finally, a differentiable classifier uses this coarsened representation to obtain the final predicted class. Experiments on ten benchmark datasets showed that MTPool outperforms state-of-the-art strategies in the MTSC task.},
  archive      = {J_NN},
  author       = {Ziheng Duan and Haoyan Xu and Yueyang Wang and Yida Huang and Anni Ren and Zhongbin Xu and Yizhou Sun and Wei Wang},
  doi          = {10.1016/j.neunet.2022.07.032},
  journal      = {Neural Networks},
  pages        = {481-490},
  shortjournal = {Neural Netw.},
  title        = {Multivariate time-series classification with hierarchical variational graph pooling},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Mean-square stabilization of impulsive neural networks with
mixed delays by non-fragile feedback involving random uncertainties.
<em>NN</em>, <em>154</em>, 469–480. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of neural networks with mixed delays and impulsive interferences. Firstly, a sufficient condition is given to ensure the existence and uniqueness of the equilibrium point of the proposed neural networks by employing the contraction mapping theorem. Secondly, we discuss the issue of the exponential stability in mean-square of the equilibrium point by a non-fragilely delayed output coupling feedback which involves stochastically occurring gain oscillations. The designed feedback input can be tolerant of limited stochastic fluctuations of control gains and be robust against potential errors caused by factors like round-off. By combining methods of Lyapunov–Krasovskii functional and free-weighting matrix, a delay-dependent output coupling feedback with stochastically occurring uncertainties is designed and linear-matrix-inequalities(LMIs)-based sufficient conditions for the exponential stabilization in mean square are derived. Finally, three numerical examples are presented to illustrate the feasibility of theoretical results with a benchmark real-world problem.},
  archive      = {J_NN},
  author       = {Xiaoyu Zhang and Chuandong Li and Hongfei Li and Zhengran Cao},
  doi          = {10.1016/j.neunet.2022.07.006},
  journal      = {Neural Networks},
  pages        = {469-480},
  shortjournal = {Neural Netw.},
  title        = {Mean-square stabilization of impulsive neural networks with mixed delays by non-fragile feedback involving random uncertainties},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using source data to aid and build variational state–space
autoencoders with sparse target data for process monitoring.
<em>NN</em>, <em>154</em>, 455–468. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial processes, different operating conditions and ratios of ingredients are used to produce multi-grade products in the same production line. Yet, the production grade changes so quickly as the demand from customers varies from time to time. As a result, the process data collected in certain operating regions are often scarce. Process dynamics, nonlinearity, and process uncertainty increase the hardship in developing a reliable model to monitor the process status. In this paper, the source-aided variational state–space autoencoder (SA-VSSAE) is proposed. It integrates variational state–space autoencoder with the Gaussian mixture . With the additional information from the source grades, SA-VSSAE can be used for monitoring processes with sparse target data by performing information sharing to enhance the reliability of the target model. Unlike the past works which perform information sharing and modeling in a two-step procedure, the proposed model is designed for information sharing and modeling in a one-step procedure without causing information loss. In contrast to the traditional state–space model, which is linear and deterministic, the variational state–space autoencoder (VSSAE) extracts the dynamic and nonlinear features in the process variables using neural networks . Also, by taking process uncertainty into consideration, VSSAE describes the features in a probabilistic form. Probability density estimates of the residual and latent variables are given to design the monitoring indices for fault detection . A numerical example and an industrial polyvinyl chloride drying process are presented to show the advantages of the proposed method over the comparative methods .},
  archive      = {J_NN},
  author       = {Yi Shan Lee and Junghui Chen},
  doi          = {10.1016/j.neunet.2022.06.010},
  journal      = {Neural Networks},
  pages        = {455-468},
  shortjournal = {Neural Netw.},
  title        = {Using source data to aid and build variational state–space autoencoders with sparse target data for process monitoring},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SLIDE: A surrogate fairness constraint to ensure fairness
consistency. <em>NN</em>, <em>154</em>, 441–454. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As they take a crucial role in social decision makings, AI algorithms based on ML models should be not only accurate but also fair. Among many algorithms for fair AI , learning a prediction ML model by minimizing the empirical risk (e.g., cross-entropy) subject to a given fairness constraint has received much attention. To avoid computational difficulty, however, a given fairness constraint is replaced by a surrogate fairness constraint as the 0–1 loss is replaced by a convex surrogate loss for classification problems. In this paper, we investigate the validity of existing surrogate fairness constraints and propose a new surrogate fairness constraint called SLIDE, which is computationally feasible and asymptotically valid in the sense that the learned model satisfies the fairness constraint asymptotically and achieves a fast convergence rate. Numerical experiments confirm that the SLIDE works well for various benchmark datasets.},
  archive      = {J_NN},
  author       = {Kunwoong Kim and Ilsang Ohn and Sara Kim and Yongdai Kim},
  doi          = {10.1016/j.neunet.2022.07.027},
  journal      = {Neural Networks},
  pages        = {441-454},
  shortjournal = {Neural Netw.},
  title        = {SLIDE: A surrogate fairness constraint to ensure fairness consistency},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictions on multi-class terminal ballistics datasets
using conditional generative adversarial networks. <em>NN</em>,
<em>154</em>, 425–440. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ballistic impacts are a primary risk in both civil and military defence applications, where successfully predicting the dynamic response of a material or structure to impact crucial to the design of safe and fit-for-purpose protective structures. This study proposes a conditional Generative Adversarial Network (cGAN) architecture that can learn directly from available ballistic data and can be conditioned on additional information, such as class labels, to govern its output. A single Multi-Layer Perceptron (MLP) cGAN architecture is trained on a multi-class ballistic training set consisting of 10 classes labelled 0 − 9 0−9 where each class refers to a ballistic curve with a different ballistic limit velocity, v bl vbl . A total of 5 models are trained on datasets consisting of 5, 10, 15, 20 and 25 samples within each class. For integer class labels 0 − 9 0−9 , all cGAN models successfully predict the v bl vbl with a maximum error of 4.12\%. Additionally, for non-integer class labels between 0 − 9 0−9 the v bl vbl predictions are similar despite not explicitly appearing in the training set. Moreover, each cGAN model is challenged to generate new samples for class labels that exist beyond the scope of the training set for class labels between 9 − 20 9−20 . Four of the models predict the v bl vbl with an error of less than 1.5\% in all cases. This study showcases how a cGAN model can learn directly from a multi-class ballistic dataset and generate additional samples representative of that data for classes that do not appear explicitly in the training set.},
  archive      = {J_NN},
  author       = {S. Thompson and F. Teixeira-Dias and M. Paulino and A. Hamilton},
  doi          = {10.1016/j.neunet.2022.07.034},
  journal      = {Neural Networks},
  pages        = {425-440},
  shortjournal = {Neural Netw.},
  title        = {Predictions on multi-class terminal ballistics datasets using conditional generative adversarial networks},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent neighborhood-based heterogeneous graph
representation. <em>NN</em>, <em>154</em>, 413–424. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph, as a powerful data structure , has shown superior capability on modeling complex systems. Since real-world objects and their interactions are often multi-modal and multi-typed, compared with traditional homogeneous graphs, heterogeneous graphs can represent real-world objects more effectively. Meanwhile, rich semantic information brings great challenges for learning heterogeneous graph representation (HGR). Most existing HGR methods are based on the concept of meta-path, which is constructed based on direct neighbors and define composite semantic relations in heterogeneous graph. However, when the direct neighbor information is inadequate, which always happens due to insufficient observation, the quality of meta-paths cannot be guaranteed. Therefore, we propose a novel HGR framework based on latent direct neighbors. Specifically, random walks are first utilized to discover the potential candidates from indirect neighbors. Then HodgeRank is introduced to determine the latent direct neighbors according to their importance to the target. After that, neighborhood relationships are augmented with the selected latent direct neighbors, and the adjacency tensor of the heterogeneous graph is refactored correspondingly. Finally, Graph Transformer Network is adopted to construct semantic meta-paths automatically and generate HGR. Numerical experiments on different real-world heterogeneous networks show that our new approach can produce more meta-path instances and introduce more complex and diverse semantic information, and consequently achieves more accurate predictions compared with several state-of-the-art baselines.},
  archive      = {J_NN},
  author       = {Yang Xiao and Pei Quan and MingLong Lei and Lingfeng Niu},
  doi          = {10.1016/j.neunet.2022.07.028},
  journal      = {Neural Networks},
  pages        = {413-424},
  shortjournal = {Neural Netw.},
  title        = {Latent neighborhood-based heterogeneous graph representation},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Return of the normal distribution: Flexible deep continual
learning with variational auto-encoders. <em>NN</em>, <em>154</em>,
397–412. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning continually from sequentially arriving data has been a long standing challenge in machine learning . An emergent body of deep learning literature suggests various solutions, through introduction of significant simplifications to the problem statement. As a consequence of a growing focus on particular tasks and their respective benchmark assumptions, these efforts are thus becoming increasingly tailored to specific settings. Whereas approaches that leverage Variational Bayesian techniques seem to provide a more general perspective of key continual learning mechanisms, they however entail their own caveats. Inspired by prior theoretical work on solving the prevalent mismatch between prior and aggregate posterior in deep generative models , we return to a generic variational auto-encoder based formulation and investigate its utility for continual learning. Specifically, we propose to adapt a two-stage training framework towards a context conditioned variant for continual learning, where we then formulate mechanisms to alleviate catastrophic forgetting through choices of generative rehearsal or well-motivated extraction of data exemplar subsets. Although the proposed generic two-stage variational auto-encoder is not tailored towards a particular task and allows for flexible amounts of supervision, we empirically demonstrate it to surpass task-tailored methods in both supervised classification , as well as unsupervised representation learning .},
  archive      = {J_NN},
  author       = {Yongwon Hong and Martin Mundt and Sungho Park and Yungjung Uh and Hyeran Byun},
  doi          = {10.1016/j.neunet.2022.07.016},
  journal      = {Neural Networks},
  pages        = {397-412},
  shortjournal = {Neural Netw.},
  title        = {Return of the normal distribution: Flexible deep continual learning with variational auto-encoders},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating heading from optic flow: Comparing deep learning
network and human performance. <em>NN</em>, <em>154</em>, 383–396. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have made significant advances over the past decade with visual recognition, matching or exceeding human performance on certain tasks. Visual recognition is subserved by the ventral stream of the visual system, which, remarkably, CNNs also effectively model. Inspired by this connection, we investigated the extent to which CNNs account for human heading perception, an important function of the complementary dorsal stream. Heading refers to the direction of movement during self-motion, which humans judge with high degrees of accuracy from the streaming pattern of motion on the eye known as optic flow. We examined the accuracy with which CNNs estimate heading from optic flow in a range of situations in which human heading perception has been well studied. These scenarios include heading estimation from sparse optic flow, in the presence of moving objects, and in the presence of rotation. We assessed performance under controlled conditions wherein self-motion was simulated through minimal or realistic scenes. We found that the CNN did not capture the accuracy of heading perception. The addition of recurrent processing to the network, however, closed the gap in performance with humans substantially in many situations. Our work highlights important self-motion scenarios in which recurrent processing supports heading estimation that approaches human-like accuracy.},
  archive      = {J_NN},
  author       = {Natalie Maus and Oliver W. Layton},
  doi          = {10.1016/j.neunet.2022.07.007},
  journal      = {Neural Networks},
  pages        = {383-396},
  shortjournal = {Neural Netw.},
  title        = {Estimating heading from optic flow: Comparing deep learning network and human performance},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PDE-READ: Human-readable partial differential equation
discovery using deep learning. <em>NN</em>, <em>154</em>, 360–382. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PDE discovery shows promise for uncovering predictive models of complex physical systems but has difficulty when measurements are noisy and limited. We introduce a new approach for PDE discovery that uses two Rational Neural Networks and a principled sparse regression algorithm to identify the hidden dynamics that govern a system’s response. The first network learns the system response function, while the second learns a hidden PDE describing the system’s evolution. We then use a parameter-free sparse regression algorithm to extract a human-readable form of the hidden PDE from the second network. We implement our approach in an open-source library called PDE-READ . Our approach successfully identifies the governing PDE in six benchmark examples. We demonstrate that our approach is robust to both sparsity and noise and it, therefore, holds promise for application to real-world observational data .},
  archive      = {J_NN},
  author       = {Robert Stephany and Christopher Earls},
  doi          = {10.1016/j.neunet.2022.07.008},
  journal      = {Neural Networks},
  pages        = {360-382},
  shortjournal = {Neural Netw.},
  title        = {PDE-READ: Human-readable partial differential equation discovery using deep learning},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based random forest and contamination model.
<em>NN</em>, <em>154</em>, 346–359. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new approach called ABRF (the attention-based random forest) and its modifications for applying the attention mechanism to the random forest (RF) for regression and classification are proposed. The main idea behind the proposed ABRF models is to assign attention weights with trainable parameters to decision trees in a specific way. The attention weights depend on the distance between an instance, which falls into a corresponding leaf of a tree, and training instances, which fall in the same leaf. This idea stems from representation of the Nadaraya–Watson kernel regression in the form of a RF. Three modifications of the general approach are proposed. The first one is based on applying the Huber’s contamination model and on computing the attention weights by solving quadratic or linear optimization problems . The second and the third modifications use the gradient-based algorithms for computing an extended set of the attention trainable parameters. Numerical experiments with various regression and classification datasets illustrate the proposed method. The code implementing the approach is publicly available.},
  archive      = {J_NN},
  author       = {Lev V. Utkin and Andrei V. Konstantinov},
  doi          = {10.1016/j.neunet.2022.07.029},
  journal      = {Neural Networks},
  pages        = {346-359},
  shortjournal = {Neural Netw.},
  title        = {Attention-based random forest and contamination model},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics guided neural networks for modelling of non-linear
dynamics. <em>NN</em>, <em>154</em>, 333–345. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of the current wave of artificial intelligence can be partly attributed to deep neural networks , which have proven to be very effective in learning complex patterns from large datasets with minimal human intervention. However, it is difficult to train these models on complex dynamical systems from data alone due to their low data efficiency and sensitivity to hyperparameters and initialisation. This work demonstrates that injection of partially known information at an intermediate layer in a DNN can improve model accuracy, reduce model uncertainty, and yield improved convergence during the training. The value of these physics-guided neural networks has been demonstrated by learning the dynamics of a wide variety of nonlinear dynamical systems represented by five well-known equations in nonlinear systems theory: the Lotka–Volterra, Duffing, Van der Pol, Lorenz, and Henon–Heiles systems.},
  archive      = {J_NN},
  author       = {Haakon Robinson and Suraj Pawar and Adil Rasheed and Omer San},
  doi          = {10.1016/j.neunet.2022.07.023},
  journal      = {Neural Networks},
  pages        = {333-345},
  shortjournal = {Neural Netw.},
  title        = {Physics guided neural networks for modelling of non-linear dynamics},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-level group convolution. <em>NN</em>, <em>154</em>,
323–332. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group convolution has been widely used in order to reduce the computation time of convolution , which takes most of the training time of convolutional neural networks . However, it is well known that a large number of groups significantly reduce the performance of group convolution. In this paper, we propose a new convolution methodology called “two-level” group convolution that is robust with respect to the increase of the number of groups and suitable for multi-GPU parallel computation. We first observe that the group convolution can be interpreted as a one-level block Jacobi approximation of the standard convolution, which is a popular notion in the field of numerical analysis . In numerical analysis , there have been numerous studies on the two-level method that introduces an intergroup structure that resolves the performance degradation issue without disturbing parallel computation. Motivated by these, we introduce a coarse-level structure which promotes intergroup communication without being a bottleneck in the group convolution. We show that all the additional work induced by the coarse-level structure can be efficiently processed in a distributed memory system. Numerical results that verify the robustness of the proposed method with respect to the number of groups are presented. Moreover, we compare the proposed method to various approaches for group convolution in order to highlight the superiority of the proposed method in terms of execution time, memory efficiency, and performance.},
  archive      = {J_NN},
  author       = {Youngkyu Lee and Jongho Park and Chang-Ock Lee},
  doi          = {10.1016/j.neunet.2022.07.024},
  journal      = {Neural Networks},
  pages        = {323-332},
  shortjournal = {Neural Netw.},
  title        = {Two-level group convolution},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relational local electroencephalography representations for
sleep scoring. <em>NN</em>, <em>154</em>, 310–322. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational sleep scoring from multimodal neurophysiological time-series (polysomnography PSG) has achieved impressive clinical success. Models that use only a single electroencephalographic (EEG) channel from PSG have not yet received the same clinical recognition, since they lack Rapid Eye Movement (REM) scoring quality. The question whether this lack can be remedied at all remains an important one. We conjecture that predominant Long Short-Term Memory (LSTM) models do not adequately represent distant REM EEG segments (termed epochs), since LSTMs compress these to a fixed-size vector from separate past and future sequences. To this end, we introduce the EEG representation model ENGELBERT (electro En cephalo G raphic E poch L ocal B idirectional E ncoder R epresentations from T ransformer). It jointly attends to multiple EEG epochs from both past and future. Compared to typical token sequences in language, for which attention models have originally been conceived, overnight EEG sequences easily span more than 1000 30 s epochs. Local attention on overlapping windows reduces the critical quadratic computational complexity to linear, enabling versatile sub-one-hour to all-day scoring. ENGELBERT is at least one order of magnitude smaller than established LSTM models and is easy to train from scratch in a single phase. It surpassed state-of-the-art macro F1-scores in 3 single-EEG sleep scoring experiments. REM F1-scores were pushed to at least 86\%. ENGELBERT virtually closed the gap to PSG-based methods from 4–5 percentage points (pp) to less than 1 pp F1-score.},
  archive      = {J_NN},
  author       = {Georg Brandmayr and Manfred Hartmann and Franz Fürbass and Gerald Matz and Matthias Samwald and Tilmann Kluge and Georg Dorffner},
  doi          = {10.1016/j.neunet.2022.07.020},
  journal      = {Neural Networks},
  pages        = {310-322},
  shortjournal = {Neural Netw.},
  title        = {Relational local electroencephalography representations for sleep scoring},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time stability of state-dependent delayed systems and
application to coupled neural networks. <em>NN</em>, <em>154</em>,
303–309. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite-time stability and stabilization problems of state-dependent delayed systems are studied in this paper. Different from discrete delays and time-dependent delays which can be well estimated over time, the information of state-dependent delays is usually hard to be estimated, especially when states are unknown or unmeasurable. To guarantee the stability of state-dependent delayed systems in the framework of finite time, a Razumikhin-type inequality is used, following which estimations on the settling time and the region of attraction are proposed. Moreover, the relationship between the variation speed of state-dependent delays and the size of the region of attraction is proposed. Then as an application of the theoretical result, finite-time stabilization is studied for a set of nonlinear coupled neural networks involving state-dependent transmission delay, where the design of memoryless finite-time controllers is addressed. Two numerical examples are given to show the effectiveness of the proposed results.},
  archive      = {J_NN},
  author       = {Xinyi He and Xiaodi Li and Shiji Song},
  doi          = {10.1016/j.neunet.2022.07.009},
  journal      = {Neural Networks},
  pages        = {303-309},
  shortjournal = {Neural Netw.},
  title        = {Finite-time stability of state-dependent delayed systems and application to coupled neural networks},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain-inspired meta-reinforcement learning cognitive control
in conflictual inhibition decision-making task for artificial agents.
<em>NN</em>, <em>154</em>, 283–302. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conflictual cues and unexpected changes in human real-case scenarios may be detrimental to the execution of tasks by artificial agents, thus affecting their performance. Meta-learning applied to reinforcement learning may enhance the design of control algorithms, where an outer learning system progressively adjusts the operation of an inner learning system, leading to practical benefits for the learning schema. Here, we developed a brain-inspired meta-learning framework for inhibition cognitive control that i) exploits the meta-learning principles in the neuromodulation theory proposed by Doya, ii) relies on a well-established neural architecture that contains distributed learning systems in the human brain, and iii) proposes optimization rules of meta-learning hyperparameters that mimic the dynamics of the major neurotransmitters in the brain. We tested an artificial agent in inhibiting the action command in two well-known tasks described in the literature: NoGo and Stop-Signal Paradigms. After a short learning phase, the artificial agent learned to react to the hold signal, and hence to successfully inhibit the motor command in both tasks, via the continuous adjustment of the learning hyperparameters. We found a significant increase in global accuracy, right inhibition, and a reduction in the latency time required to cancel the action process, i.e., the Stop-signal reaction time. We also performed a sensitivity analysis to evaluate the behavioral effects of the meta-parameters, focusing on the serotoninergic modulation of the dopamine release . We demonstrated that brain-inspired principles can be integrated into artificial agents to achieve more flexible behavior when conflictual inhibitory signals are present in the environment.},
  archive      = {J_NN},
  author       = {Federica Robertazzi and Matteo Vissani and Guido Schillaci and Egidio Falotico},
  doi          = {10.1016/j.neunet.2022.06.020},
  journal      = {Neural Networks},
  pages        = {283-302},
  shortjournal = {Neural Netw.},
  title        = {Brain-inspired meta-reinforcement learning cognitive control in conflictual inhibition decision-making task for artificial agents},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-guided entropy minimization for semi-supervised
domain adaptation. <em>NN</em>, <em>154</em>, 270–282. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Domain Adaptation has been widely studied with various approaches to address domain shift with labeled source-domain data combined with scarcely labeled target-domain data. Model adaptation is becoming promising with a paradigm of source pre-training and target fine-tuning, which eliminates the simultaneous availability of data from both domains and makes for data privacy. Among the model adaptation methods, Entropy Minimization (EM) is popularly incorporated to encourage a low-density separation on target samples. However, EM tends to brutally force models to make over-confident predictions, which could make the models collapse with deteriorated performance. In this paper, we first study the over-confidence of EM with a quantitative analysis, which shows the importance of capturing the dependency among labels. To address this issue, we propose to guide EM via longitudinal self-distillation. Specifically, we produce a dynamic “teacher” label distribution during training by constructing a graph on target data and perform pseudo-label propagation to encourage the “teacher” distribution to capture context category dependency based on a global data structure . Then EM is guided longitudinally by distilling the learned label distribution to combat the brute-force over-confidence. Extensive experiments demonstrate the effectiveness of our methods.},
  archive      = {J_NN},
  author       = {Ning Ma and Jiajun Bu and Lixian Lu and Jun Wen and Sheng Zhou and Zhen Zhang and Jingjun Gu and Haifeng Li and Xifeng Yan},
  doi          = {10.1016/j.neunet.2022.07.011},
  journal      = {Neural Networks},
  pages        = {270-282},
  shortjournal = {Neural Netw.},
  title        = {Context-guided entropy minimization for semi-supervised domain adaptation},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse signal reconstruction via collaborative neurodynamic
optimization. <em>NN</em>, <em>154</em>, 255–269. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we formulate a mixed-integer problem for sparse signal reconstruction and reformulate it as a global optimization problem with a surrogate objective function subject to underdetermined linear equations . We propose a sparse signal reconstruction method based on collaborative neurodynamic optimization with multiple recurrent neural networks for scattered searches and a particle swarm optimization rule for repeated repositioning. We elaborate on experimental results to demonstrate the outperformance of the proposed approach against ten state-of-the-art algorithms for sparse signal reconstruction.},
  archive      = {J_NN},
  author       = {Hangjun Che and Jun Wang and Andrzej Cichocki},
  doi          = {10.1016/j.neunet.2022.07.018},
  journal      = {Neural Networks},
  pages        = {255-269},
  shortjournal = {Neural Netw.},
  title        = {Sparse signal reconstruction via collaborative neurodynamic optimization},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Breaking CAPTCHA with capsule networks. <em>NN</em>,
<em>154</em>, 246–254. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks have achieved state-of-the-art performance in image classification . Their lack of ability to recognise the spatial relationship between features, however, leads to misclassification of the variants of the same image. Capsule Networks were introduced to address this issue by incorporating the spatial information of image features into neural networks. In this paper, we are interested in showcasing the digit recognition task on CAPTCHA images, widely considered a challenge for computers in relation to human capabilities. Our intention is to provide a rigorous empirical regime in which we can compare the competitive performance of Capsule Networks against the Convolutional Neural Networks. Indeed since CAPTCHA distorts images, by adjusting the spatial positioning of features, we aim to demonstrate the advantages and limitations of Capsule Networks architecture . We train the Capsule Networks with Dynamic Routing version and the convolutional-neural-network-based deep-CAPTCHA baseline model to predict the digit sequences on numerical CAPTCHAs, investigate the performance results and propose two improvements to the Capsule Networks model.},
  archive      = {J_NN},
  author       = {Ionela Georgiana Mocanu and Zhenxu Yang and Vaishak Belle},
  doi          = {10.1016/j.neunet.2022.06.041},
  journal      = {Neural Networks},
  pages        = {246-254},
  shortjournal = {Neural Netw.},
  title        = {Breaking CAPTCHA with capsule networks},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MRGAT: Multi-relational graph attention network for
knowledge graph completion. <em>NN</em>, <em>154</em>, 234–245. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most effective ways to solve the problem of knowledge graph completion is embedding-based models. Graph neural networks (GNNs) are popular and promising embedding models which can exploit and use the structural information of neighbors in knowledge graphs. The current GNN-based knowledge graph completion methods assume that all neighbors of a node have equal importance. This assumption which cannot assign different weights to neighbors is pointed out in our study to be unreasonable. In addition, since the knowledge graph is a kind of heterogeneous graph with multiple relations, multiple complex interactions between nodes and neighbors can bring challenges to the effective message passing of GNNs. We then design a multi-relational graph attention network (MRGAT) which can adapt to different cases of heterogeneous multi-relational connections and then calculate the importance of different neighboring nodes through a self-attention layer. The incorporation of self-attention mechanism into the network with different node weights optimizes the network structure, and therefore, significantly results in a promotion of performance. We experimentally validate the rationality of our models on multiple benchmark knowledge graphs, where MRGAT achieves the best performance on various evaluation metrics including MRR score, Hits@ score compared with other state-of-the-art baseline models .},
  archive      = {J_NN},
  author       = {Guoquan Dai and Xizhao Wang and Xiaoying Zou and Chao Liu and Si Cen},
  doi          = {10.1016/j.neunet.2022.07.014},
  journal      = {Neural Networks},
  pages        = {234-245},
  shortjournal = {Neural Netw.},
  title        = {MRGAT: Multi-relational graph attention network for knowledge graph completion},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpolated adversarial training: Achieving robust neural
networks without sacrificing too much accuracy. <em>NN</em>,
<em>154</em>, 218–233. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness has become a central goal in deep learning , both in the theory and the practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how the adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training . On CIFAR-10, adversarial training increases the standard test error ( when there is no adversary) from 4.43\% to 12.32\%, whereas with our Interpolated adversarial training we retain the adversarial robustness while achieving a standard test error of only 6.45\%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1\% to just 45.5\%. Moreover, we provide mathematical analysis of Interpolated Adversarial Training to confirm its efficiencies and demonstrate its advantages in terms of robustness and generalization.},
  archive      = {J_NN},
  author       = {Alex Lamb and Vikas Verma and Kenji Kawaguchi and Alexander Matyasko and Savya Khosla and Juho Kannala and Yoshua Bengio},
  doi          = {10.1016/j.neunet.2022.07.012},
  journal      = {Neural Networks},
  pages        = {218-233},
  shortjournal = {Neural Netw.},
  title        = {Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correntropy based semi-supervised concept factorization with
adaptive neighbors for clustering. <em>NN</em>, <em>154</em>, 203–217.
(<a href="https://doi.org/10.1016/j.neunet.2022.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept factorization (CF) has shown the effectiveness in the field of data clustering . In this paper, a novel and robust semi-supervised CF method , called correntropy based semi-supervised concept factorization with adaptive neighbors (CSCF), is proposed with improved performance in clustering applications. Specifically, on the one hand, the CSCF method adopts correntropy as the cost function to increase the robustness for non-Gaussian noise and outliers, and combines two different types of supervised information simultaneously for obtaining a compact low-dimensional representation of the original data. On the other hand, CSCF assigns the adaptive neighbors for each data point to construct a good data similarity matrix for reducing the sensitiveness of data. Moreover, a generalized version of CSCF is derived for enlarging the clustering application ranges. Analysis is also presented for the relationship of CSCF with several typical CF methods. Experimental results have shown that CSCF has better clustering performance than several state-of-the-art CF methods.},
  archive      = {J_NN},
  author       = {Siyuan Peng and Zhijing Yang and Feiping Nie and Badong Chen and Zhiping Lin},
  doi          = {10.1016/j.neunet.2022.07.021},
  journal      = {Neural Networks},
  pages        = {203-217},
  shortjournal = {Neural Netw.},
  title        = {Correntropy based semi-supervised concept factorization with adaptive neighbors for clustering},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgraph-aware graph structure revision for spatial–temporal
graph modeling. <em>NN</em>, <em>154</em>, 190–202. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial–temporal graph modeling has been widely studied in many fields, such as traffic forecasting and energy analysis, where data has time and space properties. Existing methods focus on capturing stable and dynamic spatial correlations by constructing physical and virtual graphs along with graph convolution and temporal modeling . However, existing methods tending to smooth node features may obscure the spatial–temporal patterns among nodes. Worse, the graph structure is not always available in some fields, while the manually constructed stable or dynamic graphs cannot necessarily reflect the true spatial correlations either. This paper proposes a Subgraph-Aware Graph Structure Revision network (SAGSR) to overcome these limitations. Architecturally, a subgraph-aware structure revision graph convolution module (SASR-GCM) is designed, which revises the learned stable graph to obtain a dynamic one to automatically infer the dynamics of spatial correlations. Each of these two graphs is separated into one homophilic subgraph and one heterophilic subgraph by a subgraph-aware graph convolution mechanism, which aggregates similar nodes in the homophilic subgraph with positive weights, while keeping nodes with dissimilar features in the heterophilic subgraph mutually away with negative aggregation weights to avoid pattern obfuscation. By combining a gated multi-scale temporal convolution module (GMS-TCM) for temporal modeling , SAGSR can efficiently capture the spatial–temporal correlations and extract complex spatial–temporal graph features. Extensive experiments, conducted on two specific tasks: traffic flow forecasting and energy consumption forecasting, indicate the effectiveness and superiority of our proposed approach over several competitive baselines.},
  archive      = {J_NN},
  author       = {Yuhu Wang and Chunxia Zhang and Shiming Xiang and Chunhong Pan},
  doi          = {10.1016/j.neunet.2022.07.017},
  journal      = {Neural Networks},
  pages        = {190-202},
  shortjournal = {Neural Netw.},
  title        = {Subgraph-aware graph structure revision for spatial–temporal graph modeling},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face image-sketch synthesis via generative adversarial
fusion. <em>NN</em>, <em>154</em>, 179–189. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image-sketch synthesis is widely applied in law enforcement and digital entertainment fields. Despite the extensive progression in face image-sketch synthesis, there are few methods focusing on generating a color face image from a sketch. The existing methods pay less attention to learning the illumination or highlight distribution on the face region. However, the illumination is the key factor that makes the generated color face image looks vivid and realistic. Moreover, existing methods tend to employ some image preprocessing technologies and facial region patching approaches to generate high-quality face images, which results in the high complexity and memory consumption in practice. In this paper, we propose a novel end-to-end generative adversarial fusion model, called GAF, which fuses two U-Net generators and a discriminator by jointly learning the content and adversarial loss functions. In particular, we propose a parametric tanh activation function to learn and control illumination highlight distribution over faces, which is integrated between the two U-Net generators by an illumination distribution layer. Additionally, we fuse the attention mechanism into the second U-Net generator of GAF to keep the identity consistency and refine the generated facial details. The qualitative and quantitative experiments on the public benchmark datasets show that the proposed GAF has better performance than existing image-sketch synthesis methods in synthesized face image quality (FSIM) and face recognition accuracy (NLDA). Meanwhile, the good generalization ability of GAF has also been verified. To further demonstrate the reliability and authenticity of face images generated using GAF, we use the generated face image to attack the well-known face recognition system. The result shows that the face images generated by GAF can maintain identity consistency and well maintain everyone’s unique facial characteristics , which can be further used in the benchmark of facial spoofing. Moreover, the experiments are implemented to verify the effectiveness and rationality of the proposed parametric tanh activation function and attention mechanism in GAF.},
  archive      = {J_NN},
  author       = {Jianyuan Sun and Hongchuan Yu and Jian J. Zhang and Junyu Dong and Hui Yu and Guoqiang Zhong},
  doi          = {10.1016/j.neunet.2022.07.013},
  journal      = {Neural Networks},
  pages        = {179-189},
  shortjournal = {Neural Netw.},
  title        = {Face image-sketch synthesis via generative adversarial fusion},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-birth metric learning framework based on binary
constraints. <em>NN</em>, <em>154</em>, 165–178. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-metric learning plays a significant role in improving the generalization of algorithms related to distance metrics since using a single metric is sometimes insufficient to handle complex data. Metric learning can adjust automatically the distance between samples to make the intra-class samples compact while making the inter-class distance as far as possible. To implement this intention better,in this work, we propose a novel multi-metric learning framework based on the pair constraints instead of triple constraints to reduce computational burden. To solve effectively the problem, we first propose a multi-birth metric learning model (termed MBML), where for each class sample, the global metric and a local metric are jointly trained. Both global and local structural information are adapted to better depict sample information. Then two alternating iterative algorithms are developed to optimize the MBML. The convergence of the proposed algorithm and complexity are analyzed theoretically. Moreover, a fast diagonal multi-metric learning method is proposed based on binary constraints , and problem can be reformulated a linear programming, with fast training speed, low the computational burden and the global optimal solutions . Numerical experiments are carried out on different scales and different types of datasets including an artificial data, benchmark datasets and an image database from binary class and multi-class problems. Experiment results confirm the feasibility and effectiveness of the proposed methods.},
  archive      = {J_NN},
  author       = {QiangQiang Ren and Chao Yuan and Yifeng Zhao and Liming Yang},
  doi          = {10.1016/j.neunet.2022.07.004},
  journal      = {Neural Networks},
  pages        = {165-178},
  shortjournal = {Neural Netw.},
  title        = {A multi-birth metric learning framework based on binary constraints},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous neural network approximation for smooth
functions. <em>NN</em>, <em>154</em>, 152–164. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish in this work approximation results of deep neural networks for smooth functions measured in Sobolev norms, motivated by recent development of numerical solvers for partial differential equations using deep neural networks . Our approximation results are nonasymptotic in the sense that the error bounds are explicitly characterized in terms of both the width and depth of the networks simultaneously with all involved constants explicitly determined. Namely, for f ∈ C s ( [ 0 , 1 ] d ) f∈Cs([0,1]d) , we show that deep ReLU networks of width O ( N log N ) O(NlogN) and of depth O ( L log L ) O(LlogL) can achieve a nonasymptotic approximation rate of O ( N − 2 ( s − 1 ) / d L − 2 ( s − 1 ) / d ) O(N−2(s−1)/dL−2(s−1)/d) with respect to the W 1 , p ( [ 0 , 1 ] d ) W1,p([0,1]d) norm for p ∈ [ 1 , ∞ ) p∈[1,∞) . If either the ReLU function or its square is applied as activation functions to construct deep neural networks of width O ( N log N ) O(NlogN) and of depth O ( L log L ) O(LlogL) to approximate f ∈ C s ( [ 0 , 1 ] d ) f∈Cs([0,1]d) , the approximation rate is O ( N − 2 ( s − n ) / d L − 2 ( s − n ) / d ) O(N−2(s−n)/dL−2(s−n)/d) with respect to the W n , p ( [ 0 , 1 ] d ) Wn,p([0,1]d) norm for p ∈ [ 1 , ∞ ) p∈[1,∞) .},
  archive      = {J_NN},
  author       = {Sean Hon and Haizhao Yang},
  doi          = {10.1016/j.neunet.2022.06.040},
  journal      = {Neural Networks},
  pages        = {152-164},
  shortjournal = {Neural Netw.},
  title        = {Simultaneous neural network approximation for smooth functions},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QMEDNet: A quaternion-based multi-order differential
encoder–decoder model for 3D human motion prediction. <em>NN</em>,
<em>154</em>, 141–151. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to deal with the sequence information in the task of 3D human motion prediction effectively, many previous methods seek to predict the motion state of the next moment using the traditional recurrent neural network in Euclidean space . However human motion representation in Euclidean space has high distortion and shows a weak semantic expression when using deep learning models. In this work, we try to process human motion by mapping Euclidean space into a Hypercomplex vector space. We propose a novel model based on quaternion to predict the three-dimensional motion of a human body. The core idea of this study is to use the fusion information to understand and process the human motion state in quaternion space. The multi-order differential information is fused both in the encoder and decoder of feature extraction and mapped to the quaternion space, respectively. The encoder takes graph convolution as the basic unit and the decoder adopts gated recurrent units. Numerous experiments have been carried out to prove that the multi-order information in quaternion space can help build a more reasonable description for 3D human motion. The performance of the proposed QMEDNet is superior to most of the advanced short and long-term motion prediction methods in both public datasets, Human 3.6M and CMU Mocap.},
  archive      = {J_NN},
  author       = {Wenming Cao and Shuangshuang Li and Jianqi Zhong},
  doi          = {10.1016/j.neunet.2022.07.005},
  journal      = {Neural Networks},
  pages        = {141-151},
  shortjournal = {Neural Netw.},
  title        = {QMEDNet: A quaternion-based multi-order differential encoder–decoder model for 3D human motion prediction},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel optimal trajectory tracking for nonlinear affine
systems with an advanced critic learning structure. <em>NN</em>,
<em>154</em>, 131–140. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a critic learning structure based on the novel utility function is developed to solve the optimal tracking control problem with the discount factor of affine nonlinear systems . The utility function is defined as the quadratic form of the error at the next moment, which can not only avoid solving the stable control input, but also effectively eliminate the tracking error. Next, the theoretical derivation of the method under value iteration is given in detail with convergence and stability analysis. Then, the dual heuristic dynamic programming (DHP) algorithm via a single neural network is introduced to reduce the amount of computation. The polynomial is used to approximate the costate function during the DHP implementation. The weighted residual method is used to update the weight matrix . During simulation, the convergence speed of the given strategy is compared with the heuristic dynamic programming (HDP) algorithm. The experiment results display that the convergence speed of the proposed method is faster than the HDP algorithm. Besides, the proposed method is compared with the traditional tracking control approach to verify its tracking performance. The experiment results show that the proposed method can avoid solving the stable control input, and the tracking error is closer to zero than the traditional strategy.},
  archive      = {J_NN},
  author       = {Ding Wang and Huiling Zhao and Mingming Zhao and Jin Ren},
  doi          = {10.1016/j.neunet.2022.07.019},
  journal      = {Neural Networks},
  pages        = {131-140},
  shortjournal = {Neural Netw.},
  title        = {Novel optimal trajectory tracking for nonlinear affine systems with an advanced critic learning structure},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reservoir computing with 3D nanowire networks. <em>NN</em>,
<em>154</em>, 122–130. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks of nanowires are currently being explored for a range of applications in brain-like (or neuromorphic) computing, and especially in reservoir computing (RC). Fabrication of real-world computing devices requires that the nanowires are deposited sequentially, leading to stacking of the wires on top of each other. However, most simulations of computational tasks using these systems treat the nanowires as 1D objects lying in a perfectly 2D plane — the effect of stacking on RC performance has not yet been established. Here we use detailed simulations to compare the performance of perfectly 2D and quasi-3D (stacked) networks of nanowires in two tasks: memory capacity and nonlinear transformation . We also show that our model of the junctions between nanowires is general enough to describe a wide range of memristive networks, and consider the impact of physically realistic electrode configurations on performance. We show that the various networks and configurations have a strikingly similar performance in RC tasks, which is surprising given their radically different topologies. Our results show that networks with an experimentally achievable number of electrodes perform close to the upper bounds achievable when using the information from every wire. However, we also show important differences, in particular that the quasi-3D networks are more resilient to changes in the input parameters, generalizing better to noisy training data. Since previous literature suggests that topology plays an important role in computing performance, these results may have important implications for future applications of nanowire networks in neuromorphic computing.},
  archive      = {J_NN},
  author       = {R.K. Daniels and J.B. Mallinson and Z.E. Heywood and P.J. Bones and M.D. Arnold and S.A. Brown},
  doi          = {10.1016/j.neunet.2022.07.001},
  journal      = {Neural Networks},
  pages        = {122-130},
  shortjournal = {Neural Netw.},
  title        = {Reservoir computing with 3D nanowire networks},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reward prediction errors, not sensory prediction errors,
play a major role in model selection in human reinforcement learning.
<em>NN</em>, <em>154</em>, 109–121. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based reinforcement learning enables an agent to learn in variable environments and tasks by optimizing its actions based on the predicted states and outcomes. This mechanism has also been considered in the brain. However, exactly how the brain selects an appropriate model for confronting environments has remained unclear. Here, we investigated the model selection algorithm in the human brain during a reinforcement learning task. One primary theory of model selection in the brain is based on sensory prediction errors. Here, we compared this theory with an alternative possibility of internal model selection with reward prediction errors. To compare these two theories, we devised a switching experiment from a first-order Markov decision process to a second-order Markov decision process that provides either reward- or sensory prediction error regarding environmental change. We tested two representative computational models driven by different prediction errors. One is the sensory prediction-error-driven Bayesian algorithm, which has been discussed as a representative internal model selection algorithm in the animal reinforcement learning task. The other is the reward-prediction-error-driven policy gradient algorithm. We compared the simulation results of these two computational models with human reinforcement learning behaviors . The model fitting result supports that the policy gradient algorithm is preferable to the Bayesian algorithm. This suggests that the human brain employs the reward prediction error to select an appropriate internal model in the reinforcement learning task.},
  archive      = {J_NN},
  author       = {Yihao Wu and Masahiko Morita and Jun Izawa},
  doi          = {10.1016/j.neunet.2022.07.002},
  journal      = {Neural Networks},
  pages        = {109-121},
  shortjournal = {Neural Netw.},
  title        = {Reward prediction errors, not sensory prediction errors, play a major role in model selection in human reinforcement learning},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural extraction of multiscale essential structure for
network dismantling. <em>NN</em>, <em>154</em>, 99–108. (<a
href="https://doi.org/10.1016/j.neunet.2022.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverse real world systems can be abstracted as complex networks consisting of nodes and edges as functional components. Percolation theory has shown that the failure of a few of nodes could lead to the collapse of a whole network, which brings up the network dismantling problem: How to select the least number of nodes to decompose a network into disconnected components each smaller than a predefined threshold? For its NP-hardness, many heuristic approaches have been proposed to measure and rank each node according to its importance to network structural stability. However, these measures are from a uniscale viewpoint by regarding one complex network as a flatted topology. In this article, we argue that nodes’ structural importance can be measured in different scales of network topologies . Built upon recent deep learning techniques, we propose a self-supervised learning based network dismantling framework (NEES), which can hierarchically merge some compact substructures to convert a network into a coarser one with fewer nodes and edges. During the merging process, we design neural models to extract essential structures and utilize self-attention mechanisms to learn nodes’ importance hierarchy in each scale. Experiments on real world networks and synthetic model networks show that the proposed NEES outperforms the state-of-the-art schemes in most cases in terms of removing the least number of target nodes to dismantle a network. The dismantling effectiveness of our neural extraction framework also highlights the emerging role of multi-scale essential structures.},
  archive      = {J_NN},
  author       = {Qingxia Liu and Bang Wang},
  doi          = {10.1016/j.neunet.2022.07.015},
  journal      = {Neural Networks},
  pages        = {99-108},
  shortjournal = {Neural Netw.},
  title        = {Neural extraction of multiscale essential structure for network dismantling},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AdjointBackMap: Reconstructing effective decision
hypersurfaces from CNN layers using adjoint operators. <em>NN</em>,
<em>154</em>, 78–98. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are several methods in the exploration of Convolutional Neural Networks’ (CNNs’) inner workings. However, in general, finding the inverse of the function performed by CNNs as a whole is an ill-posed problem. In this paper, we propose a method based on adjoint operators to reconstruct, given an arbitrary unit in the CNN (except for the first convolutional layer), its effective hypersurface in the input space. Since the reconstructed hyperplane (each point on the hypersurface) resides in the input space, we can easily visualize it. Our results show that the reconstructed hyperplane , when multiplied by the original input image, would give nearly the exact output value of that unit. We find that the CNN unit’s decision process is largely conditioned on the input, and the corresponding reconstructed hypersurfaces are highly sensitive to adversarial noise, thus providing insights on why CNNs are susceptible to adversarial attack .},
  archive      = {J_NN},
  author       = {Qing Wan and Yoonsuck Choe},
  doi          = {10.1016/j.neunet.2022.06.037},
  journal      = {Neural Networks},
  pages        = {78-98},
  shortjournal = {Neural Netw.},
  title        = {AdjointBackMap: Reconstructing effective decision hypersurfaces from CNN layers using adjoint operators},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BackEISNN: A deep spiking neural network with adaptive
self-feedback and balanced excitatory–inhibitory neurons. <em>NN</em>,
<em>154</em>, 68–77. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) transmit information through discrete spikes that perform well in processing spatial–temporal information. Owing to their nondifferentiable characteristic, difficulties persist in designing SNNs that deliver good performance. SNNs trained with backpropagation have recently exhibited impressive performance by using gradient approximation . However, their performance on complex tasks remains significantly inferior to that of deep neural networks. By taking inspiration from autapses in the brain that connect spiking neurons with a self-feedback connection, we apply adaptive time-delayed self-feedback to the membrane potential to regulate the precision of the spikes. We also strike a balance between the excitatory and inhibitory mechanisms of neurons to dynamically control the output of spiking neurons. By combining these two mechanisms, we propose a deep SNN with adaptive self-feedback and balanced excitatory and inhibitory neurons (BackEISNN). The results of experiments on several standard datasets show that the two modules not only accelerate the convergence of the network but also increase its accuracy. Our model achieved state-of-the-art performance on the MNIST, Fashion-MNIST, and N-MNIST datasets. The proposed BackEISNN also achieved remarkably good performance on the CIFAR10 dataset while using a relatively light structure that competes against state-of-the-art SNNs.},
  archive      = {J_NN},
  author       = {Dongcheng Zhao and Yi Zeng and Yang Li},
  doi          = {10.1016/j.neunet.2022.06.036},
  journal      = {Neural Networks},
  pages        = {68-77},
  shortjournal = {Neural Netw.},
  title        = {BackEISNN: A deep spiking neural network with adaptive self-feedback and balanced excitatory–inhibitory neurons},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning guided graph neural networks for
brain network analysis. <em>NN</em>, <em>154</em>, 56–67. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks’ structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs’ performance in eight brain network disease analysis tasks.},
  archive      = {J_NN},
  author       = {Xusheng Zhao and Jia Wu and Hao Peng and Amin Beheshti and Jessica J.M. Monaghan and David McAlpine and Heivet Hernandez-Perez and Mark Dras and Qiong Dai and Yangyang Li and Philip S. Yu and Lifang He},
  doi          = {10.1016/j.neunet.2022.06.035},
  journal      = {Neural Networks},
  pages        = {56-67},
  shortjournal = {Neural Netw.},
  title        = {Deep reinforcement learning guided graph neural networks for brain network analysis},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Periodic event-triggered adaptive tracking control design
for nonlinear discrete-time systems via reinforcement learning.
<em>NN</em>, <em>154</em>, 43–55. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an event-triggered control scheme with periodic characteristic is developed for nonlinear discrete-time systems under an actor–critic architecture of reinforcement learning (RL). The periodic event-triggered mechanism (ETM) is constructed to decide whether the sampling data are delivered to controllers or not. Meanwhile, the controller is updated only when the event-triggered condition deviates from a prescribed threshold. Compared with traditional continuous ETMs, the proposed periodic ETM can guarantee a minimal lower bound of the inter-event intervals and avoid sampling calculation point-to-point, which means that the partial communication resources can be efficiently economized. The critic and actor neural networks (NNs), consisting of radial basis function neural networks (RBFNNs), aim to approximate the unknown long-term performance index function and the ideal event-triggered controller, respectively. A rigorous stability analysis based on the Lyapunov difference method is provided to substantiate that the closed-loop system can be stabilized. All error signals of the closed-loop system are uniformly ultimately bounded (UUB) under the guidance of the proposed control scheme. Finally, two simulation examples are given to validate the effectiveness of the control design.},
  archive      = {J_NN},
  author       = {Fanghua Tang and Ben Niu and Guangdeng Zong and Xudong Zhao and Ning Xu},
  doi          = {10.1016/j.neunet.2022.06.039},
  journal      = {Neural Networks},
  pages        = {43-55},
  shortjournal = {Neural Netw.},
  title        = {Periodic event-triggered adaptive tracking control design for nonlinear discrete-time systems via reinforcement learning},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How does the brain represent the semantic content of an
image? <em>NN</em>, <em>154</em>, 31–42. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using deep neural networks (DNNs) as models to explore the biological brain is controversial, which is mainly due to the impenetrability of DNNs. Inspired by neural style transfer, we circumvented this problem by using deep features that were given a clear meaning—the representation of the semantic content of an image. Using encoding models and the representational similarity analysis, we quantitatively showed that the deep features which represented the semantic content of an image mainly predicted the activity of voxels in the early visual areas (V1, V2, and V3) and these features were essentially depictive but also propositional. This result is in line with the core viewpoint of the grounded cognition to some extent, which suggested that the representation of information in our brain is essentially depictive and can implement symbolic functions naturally.},
  archive      = {J_NN},
  author       = {Huawei Xu and Ming Liu and Delong Zhang},
  doi          = {10.1016/j.neunet.2022.06.034},
  journal      = {Neural Networks},
  pages        = {31-42},
  shortjournal = {Neural Netw.},
  title        = {How does the brain represent the semantic content of an image?},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active learning of causal structures with deep reinforcement
learning. <em>NN</em>, <em>154</em>, 22–30. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of experiment design to learn causal structures from interventional data. We consider an active learning setting in which the experimenter decides to intervene on one of the variables in the system in each step and uses the results of the intervention to recover further causal relationships among the variables. The goal is to fully identify the causal structures with minimum number of interventions. We present the first deep reinforcement learning based solution for the problem of experiment design . In the proposed method, we embed input graphs to vectors using a graph neural network and feed them to another neural network which outputs a variable for performing intervention in each step. Both networks are trained jointly via a Q-iteration algorithm. Experimental results show that the proposed method achieves competitive performance in recovering causal structures with respect to previous works, while significantly reducing execution time in dense graphs.},
  archive      = {J_NN},
  author       = {Amir Amirinezhad and Saber Salehkaleybar and Matin Hashemi},
  doi          = {10.1016/j.neunet.2022.06.028},
  journal      = {Neural Networks},
  pages        = {22-30},
  shortjournal = {Neural Netw.},
  title        = {Active learning of causal structures with deep reinforcement learning},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compressing speaker extraction model with ultra-low
precision quantization and knowledge distillation. <em>NN</em>,
<em>154</em>, 13–21. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, our proposed speaker extraction model, WASE (learning When to Attend for Speaker Extraction) yielded superior performance over the prior state-of-the-art methods by explicitly modeling onset clue and regarding it as important guidance in speaker extraction tasks. However, it still remains challenging when it comes to the deployments on the resource-constrained devices, where the model must be tiny and fast to perform inference with minimal budget in CPU and memory while keeping the speaker extraction performance. In this work, we utilize model compression techniques to alleviate the problem and propose a lightweight speaker extraction model, TinyWASE, which aims to run on resource-constrained devices. Specifically, we mainly investigate the grouping effects of quantization-aware training and knowledge distillation techniques in the speaker extraction task and propose Distillation-aware Quantization. Experiments on WSJ0-2mix dataset show that our proposed model can achieve comparable performance as the full-precision model while reducing the model size using ultra-low bits (e.g. 3 bits), obtaining 8.97x compression ratio and 2.15 MB model size. We further show that TinyWASE can combine with other model compression techniques, such as parameter sharing, to achieve compression ratio as high as 23.81 with limited performance degradation . Our code is available at https://github.com/aispeech-lab/TinyWASE .},
  archive      = {J_NN},
  author       = {Yating Huang and Yunzhe Hao and Jiaming Xu and Bo Xu},
  doi          = {10.1016/j.neunet.2022.06.026},
  journal      = {Neural Networks},
  pages        = {13-21},
  shortjournal = {Neural Netw.},
  title        = {Compressing speaker extraction model with ultra-low precision quantization and knowledge distillation},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed optimized dynamic event-triggered control for
unknown heterogeneous nonlinear MASs with input-constrained.
<em>NN</em>, <em>154</em>, 1–12. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed optimized dynamic event-triggered controller is investigated for completely unknown heterogeneous nonlinear multi-agent systems (MASs) on a directed graph subject to input-constrained. First, the distributed observer is designed to estimate the information of the leader for each follower, and a network of the augmented system is constructed by employing the dynamics of the followers and the observers. An identifier with a compensator is designed to approximate the unknown augmented system (agent) with an arbitrarily small identifier error. Then, consider that the input-constrained optimal controller , along with Hamilton–Jacobi–Bellman (HJB) equation, is under pressure to execute in certain systems associated with bottlenecks such as communication and computing burdens. A critic–actor-based optimized dynamic event-triggered controller, which tunes the parameters of critic–actor neural networks (NNs) by the dynamic triggering mechanism, is leveraged to determine the rule of aperiodic sampling and maintain the desired synchronization service. In addition, the existence of a positive minimum inter-event time (MIET) between consecutive events is also proved. Finally, the applications in non-identical nonlinear MAS and 2-DOF robots illustrate the availability of the proposed theoretical results.},
  archive      = {J_NN},
  author       = {Lina Xia and Qing Li and Ruizhuo Song and Shuzhi Sam Ge},
  doi          = {10.1016/j.neunet.2022.06.033},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {Distributed optimized dynamic event-triggered control for unknown heterogeneous nonlinear MASs with input-constrained},
  volume       = {154},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>153</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00292-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00292-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Current events. <em>NN</em>, <em>153</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00291-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00291-X},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward reliable designs of data-driven reinforcement
learning tracking control for euler–lagrange systems. <em>NN</em>,
<em>153</em>, 564–575. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses reinforcement learning based, direct signal tracking control with an objective of developing mathematically suitable and practically useful design approaches. Specifically, we aim to provide reliable and easy to implement designs in order to reach reproducible neural network-based solutions. Our proposed new design takes advantage of two control design frameworks: a reinforcement learning based, data-driven approach to provide the needed adaptation and (sub)optimality, and a backstepping based approach to provide closed-loop system stability framework. We develop this work based on an established direct heuristic dynamic programming (dHDP) learning paradigm to perform online learning and adaptation and a backstepping design for a class of important nonlinear dynamics described as Euler–Lagrange systems. We provide a theoretical guarantee for the stability of the overall dynamic system, weight convergence of the approximating nonlinear neural networks, and the Bellman (sub)optimality of the resulted control policy. We use simulations to demonstrate significantly improved design performance of the proposed approach over the original dHDP.},
  archive      = {J_NN},
  author       = {Zhikai Yao and Jianyong Yao},
  doi          = {10.1016/j.neunet.2022.05.017},
  journal      = {Neural Networks},
  pages        = {564-575},
  shortjournal = {Neural Netw.},
  title        = {Toward reliable designs of data-driven reinforcement learning tracking control for Euler–Lagrange systems},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence of deep convolutional neural networks.
<em>NN</em>, <em>153</em>, 553–563. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convergence of deep neural networks as the depth of the networks tends to infinity is fundamental in building the mathematical foundation for deep learning . In a previous study, we investigated this question for deep networks with the Rectified Linear Unit (ReLU) activation function and with a fixed width. This does not cover the important convolutional neural networks where the widths are increased from layer to layer. For this reason, we first study convergence of general ReLU networks with increased widths and then apply the results obtained to deep convolutional neural networks. It turns out the convergence reduces to convergence of infinite products of matrices with increased sizes, which has not been considered in the literature. We establish sufficient conditions for convergence of such infinite products of matrices . Based on the conditions, we present sufficient conditions for pointwise convergence of general deep ReLU networks with increasing widths, and as well as pointwise convergence of deep ReLU convolutional neural networks.},
  archive      = {J_NN},
  author       = {Yuesheng Xu and Haizhang Zhang},
  doi          = {10.1016/j.neunet.2022.06.031},
  journal      = {Neural Networks},
  pages        = {553-563},
  shortjournal = {Neural Netw.},
  title        = {Convergence of deep convolutional neural networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A systematic exploration of reservoir computing for
forecasting complex spatiotemporal dynamics. <em>NN</em>, <em>153</em>,
530–552. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reservoir computer (RC) is a type of recurrent neural network architecture with demonstrated success in the prediction of spatiotemporally chaotic dynamical systems . A further advantage of RC is that it reproduces intrinsic dynamical quantities essential for its incorporation into numerical forecasting routines such as the ensemble Kalman filter—used in numerical weather prediction to compensate for sparse and noisy data. We explore here the architecture and design choices for a “best in class” RC for a number of characteristic dynamical systems . Our analysis points to the importance of large scale parameter optimization. We also note in particular the importance of including input bias in the RC design, which has a significant impact on the forecast skill of the trained RC model. In our tests, the use of a nonlinear readout operator does not affect the forecast time or the stability of the forecast. The effects of the reservoir dimension, spinup time, amount of training data, normalization, noise, and the RC time step are also investigated. Finally, we detail how our investigation leads to optimal design choices for a parallel RC scheme applied to the 40 dimensional spatiotemporally chaotic Lorenz 1996 dynamics.},
  archive      = {J_NN},
  author       = {Jason A. Platt and Stephen G. Penny and Timothy A. Smith and Tse-Chun Chen and Henry D.I. Abarbanel},
  doi          = {10.1016/j.neunet.2022.06.025},
  journal      = {Neural Networks},
  pages        = {530-552},
  shortjournal = {Neural Netw.},
  title        = {A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online action proposal generation using spatio-temporal
attention network. <em>NN</em>, <em>153</em>, 518–529. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation aims to generate temporal boundaries containing action instances. In real-time applications such as surveillance cameras, autonomous driving , and traffic monitoring, the online localization and recognition of human activities occurring in short temporal intervals are important areas of research. Existing approaches of temporal action proposal generation consider only the offline and frame-level feature aggregation along the temporal dimension. Those offline methods also generate many redundant irrelevant proposal regions in the frames as temporal boundaries. This leads to higher computational cost along with slow processing speed which is not suitable for online tasks. In this study, we propose a novel spatio-temporal attention network for online action proposal generation as opposed to existing offline proposal generation methods. Our novel proposed approach incorporates the inter-dependency between the spatial and temporal context information of each incoming video clip to generate more relevant online temporal action proposals. First, we propose a windowed spatial attention module to capture the inter-spatial relationship between the features of incoming frames. The windowed spatial network produces more robust clip-level feature representation and efficiently deals with noisy features such as occlusion or background scenes. Second, we introduce a temporal attention module to capture relevant temporal dynamic information mutually to the localized spatial information to model the long inter-frame temporal relationship since most online real life videos are untrimmed in nature. By applying these two attention modules sequentially, the novel proposed spatio-temporal network model is able to generate precise action boundaries at a particular instant of time. In addition, the model generates fewer discriminative temporal action proposals while maintaining a low computational cost and high processing speed suitable for online settings.},
  archive      = {J_NN},
  author       = {Kanchan Keisham and Amin Jalali and Minho Lee},
  doi          = {10.1016/j.neunet.2022.06.032},
  journal      = {Neural Networks},
  pages        = {518-529},
  shortjournal = {Neural Netw.},
  title        = {Online action proposal generation using spatio-temporal attention network},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Oblique and rotation double random forest. <em>NN</em>,
<em>153</em>, 496–517. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Forest is an ensemble of decision trees based on the bagging and random subspace concepts. As suggested by Breiman, the strength of unstable learners and the diversity among them are the ensemble models’ core strength. In this paper, we propose two approaches known as oblique and rotation double random forests. In the first approach, we propose rotation based double random forest. In rotation based double random forests, transformation or rotation of the feature space is generated at each node. At each node different random feature subspace is chosen for evaluation, hence the transformation at each node is different. Different transformations result in better diversity among the base learners and hence, better generalization performance . With the double random forest as base learner, the data at each node is transformed via two different transformations namely, principal component analysis and linear discriminant analysis . In the second approach, we propose oblique double random forest. Decision trees in random forest and double random forest are univariate, and this results in the generation of axis parallel split which fails to capture the geometric structure of the data. Also, the standard random forest may not grow sufficiently large decision trees resulting in suboptimal performance. To capture the geometric properties and to grow the decision trees of sufficient depth, we propose oblique double random forest. The oblique double random forest models are multivariate decision trees. At each non-leaf node, multisurface proximal support vector machine generates the optimal plane for better generalization performance . Also, different regularization techniques (Tikhonov regularization , axis-parallel split regularization, Null space regularization) are employed for tackling the small sample size problems in the decision trees of oblique double random forest. The proposed ensembles of decision trees produce trees with bigger size compared to the standard ensembles of decision trees as bagging is used at each non-leaf node which results in improved performance. The evaluation of the baseline models and the proposed oblique and rotation double random forest models is performed on benchmark 121 UCI datasets and real-world fisheries datasets. Both statistical analysis and the experimental results demonstrate the efficacy of the proposed oblique and rotation double random forest models compared to the baseline models on the benchmark datasets.},
  archive      = {J_NN},
  author       = {M.A. Ganaie and M. Tanveer and P.N. Suganthan and V. Snasel},
  doi          = {10.1016/j.neunet.2022.06.012},
  journal      = {Neural Networks},
  pages        = {496-517},
  shortjournal = {Neural Netw.},
  title        = {Oblique and rotation double random forest},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modularity-aware graph autoencoders for joint community
detection and link prediction. <em>NN</em>, <em>153</em>, 474–495. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as powerful methods for link prediction. Their performances are less impressive on community detection problems where, according to recent and concurring experimental evaluations, they are often outperformed by simpler alternatives such as the Louvain method. It is currently still unclear to which extent one can improve community detection with GAE and VGAE, especially in the absence of node features. It is moreover uncertain whether one could do so while simultaneously preserving good performances on link prediction. In this paper, we show that jointly addressing these two tasks with high accuracy is possible. For this purpose, we introduce and theoretically study a community-preserving message passing scheme, doping our GAE and VGAE encoders by considering both the initial graph structure and modularity-based prior communities when computing embedding spaces. We also propose novel training and optimization strategies , including the introduction of a modularity-inspired regularizer complementing the existing reconstruction losses for joint link prediction and community detection. We demonstrate the empirical effectiveness of our approach, referred to as Modularity-Aware GAE and VGAE, through in-depth experimental validation on various real-world graphs.},
  archive      = {J_NN},
  author       = {Guillaume Salha-Galvan and Johannes F. Lutzeyer and George Dasoulas and Romain Hennequin and Michalis Vazirgiannis},
  doi          = {10.1016/j.neunet.2022.06.021},
  journal      = {Neural Networks},
  pages        = {474-495},
  shortjournal = {Neural Netw.},
  title        = {Modularity-aware graph autoencoders for joint community detection and link prediction},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-layer vision transformers for more accurate early
exits with less overhead. <em>NN</em>, <em>153</em>, 461–473. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying deep learning models in time-critical applications with limited computational resources, for instance in edge computing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as early exiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture, as well as a fine-tuning strategy that significantly increase the accuracy of early exit branches compared to conventional approaches while introducing less overhead. Through extensive experiments on image and audio classification as well as audiovisual crowd counting, we show that our method works for both classification and regression problems , and in both single- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalities within early exits in audiovisual data analysis, that can lead to a more fine-grained dynamic inference.},
  archive      = {J_NN},
  author       = {Arian Bakhtiarnia and Qi Zhang and Alexandros Iosifidis},
  doi          = {10.1016/j.neunet.2022.06.038},
  journal      = {Neural Networks},
  pages        = {461-473},
  shortjournal = {Neural Netw.},
  title        = {Single-layer vision transformers for more accurate early exits with less overhead},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The minimum regret path problem on stochastic fuzzy
time-varying networks. <em>NN</em>, <em>153</em>, 450–460. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a stochastic fuzzy time-varying minimum regret path problem (SFTMRP), which combines the characteristics of the min–max regret path and maximum probability path as a variant of the stochastic fuzzy time-varying shortest path problem , and its purpose is to find a path with the minimum regret degree in a given stochastic fuzzy time-varying network. To address this problem, we propose a random fuzzy delay neural network (RFDNN) based on novel random fuzzy delay neurons and without any training requirements. The random fuzzy delay neuron consists of six layers: an input layer, receiving layer, status layer, generation layer, sending layer, and output layer. Among them, the input and output layers are the ports of communication between neurons, and the receiving layer, status layer, generate layer, and sending layer are the information processing units of neurons. The information exchange between neurons is characterized by two kinds of signals: the shortest path signal and the maximum probability solution signal. The theoretical analysis of the proposed algorithm is carried out with respect to time-complexity and correctness. The numerical example and experimental results on 25 randomly generated stochastic fuzzy time-varying road networks with different numbers of 1000–5000 nodes show that the performance of the proposed algorithm is significantly better than that of existing algorithms.},
  archive      = {J_NN},
  author       = {Wei Huang and Zhilei Xu and Liehuang Zhu},
  doi          = {10.1016/j.neunet.2022.06.029},
  journal      = {Neural Networks},
  pages        = {450-460},
  shortjournal = {Neural Netw.},
  title        = {The minimum regret path problem on stochastic fuzzy time-varying networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimensionality of the intermediate-level representation of
shape and texture in monkey v4. <em>NN</em>, <em>153</em>, 444–449. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual area V4 has been considered to play a crucial role in the intermediate representation of objects, where low-level image features are transformed into object-level representations. We estimated the intrinsic dimensionality in V4 for the representation of local patches generated from natural scenes. The dimensionality was approximately 40, which is approximately half of that reported in IT for the representation of whole natural objects. The analyses of the estimated dimensionality suggest both common and independent representations that code contour shapes and/or surfaces with textures, implying a relatively complex and mixed representation in the intermediate-level area.},
  archive      = {J_NN},
  author       = {Atsushi Kodama and Kouji Kimura and Ko Sakai},
  doi          = {10.1016/j.neunet.2022.06.027},
  journal      = {Neural Networks},
  pages        = {444-449},
  shortjournal = {Neural Netw.},
  title        = {Dimensionality of the intermediate-level representation of shape and texture in monkey v4},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ME-PLAN: A deep prototypical learning with local attention
network for dynamic micro-expression recognition. <em>NN</em>,
<em>153</em>, 427–443. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the important psychological stress reactions, Micro-expressions (MEs) are spontaneous and subtle facial movements, which usually occur in a high-stake situation and can reveal genuine human feelings and cognition. ME, Recognition (MER) has essential applications in many fields such as lie detection, criminal investigation, and psychological healing. However, due to the challenges of learning discriminative ME features via fleeting facial subtle reactions as well as the shortage of available MEs data, this research topic is still far from well-studied. To this end, in this paper, we propose a deep prototypical learning framework, namely ME-PLAN, with a local attention mechanism for the MER problem. Specifically, ME-PLAN consists of two components, i.e., a 3D residual prototypical network and a local-wise attention module, where the former aims to learn the precise ME feature prototypes through expression-related knowledge transfer and episodic training, and the latter could facilitate the attention to the local facial movements. Furthermore, to alleviate the dilemma that most MER methods need to depend on manually annotated apex frames, we propose an apex frame spotting method with Unimodal Pattern Constrained (UPC) and further extract ME key-frames sequences based on the detected apex frames to train our proposed ME-PLAN in an end-to-end manner. Finally, through extensive experiments and interpretable analysis regarding the apex frame spotting and MER on composite-database, we demonstrate the superiority and effectiveness of the proposed methods.},
  archive      = {J_NN},
  author       = {Sirui Zhao and Huaying Tang and Shifeng Liu and Yangsong Zhang and Hao Wang and Tong Xu and Enhong Chen and Cuntai Guan},
  doi          = {10.1016/j.neunet.2022.06.024},
  journal      = {Neural Networks},
  pages        = {427-443},
  shortjournal = {Neural Netw.},
  title        = {ME-PLAN: A deep prototypical learning with local attention network for dynamic micro-expression recognition},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation rates of DeepONets for learning operators
arising from advection–diffusion equations. <em>NN</em>, <em>153</em>,
411–426. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the analysis of approximation rates of operator learning in Chen and Chen (1995) and Lu et al. (2021), where continuous operators are approximated by a sum of products of branch and trunk networks. In this work, we consider the rates of learning solution operators from both linear and nonlinear advection–diffusion equations with or without reaction. We find that the approximation rates depend on the architecture of branch networks as well as the smoothness of inputs and outputs of solution operators.},
  archive      = {J_NN},
  author       = {Beichuan Deng and Yeonjong Shin and Lu Lu and Zhongqiang Zhang and George Em Karniadakis},
  doi          = {10.1016/j.neunet.2022.06.019},
  journal      = {Neural Networks},
  pages        = {411-426},
  shortjournal = {Neural Netw.},
  title        = {Approximation rates of DeepONets for learning operators arising from advection–diffusion equations},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cardinality-constrained portfolio selection via
two-timescale duplex neurodynamic optimization. <em>NN</em>,
<em>153</em>, 399–410. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses portfolio selection based on neurodynamic optimization. The portfolio selection problem is formulated as a biconvex optimization problem with a variable weight in the Markowitz risk–return framework. In addition, the cardinality-constrained portfolio selection problem is formulated as a mixed-integer optimization problem and reformulated as a biconvex optimization problem. A two-timescale duplex neurodynamic approach is customized and applied for solving the reformulated portfolio optimization problem. In the two-timescale duplex neurodynamic approach, two recurrent neural networks operating at two timescales are employed for local searches, and their neuronal states are reinitialized upon local convergence using a particle swarm optimization rule to escape from local optima toward global ones. Experimental results on four datasets of world stock markets are elaborated to demonstrate the superior performance of the neurodynamic optimization approach to three baselines in terms of two major risk-adjusted performance criteria and portfolio returns.},
  archive      = {J_NN},
  author       = {Man-Fai Leung and Jun Wang and Hangjun Che},
  doi          = {10.1016/j.neunet.2022.06.023},
  journal      = {Neural Networks},
  pages        = {399-410},
  shortjournal = {Neural Netw.},
  title        = {Cardinality-constrained portfolio selection via two-timescale duplex neurodynamic optimization},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIRe-networks: Convolutional neural networks architectural
extension for information preservation via skip/residual connections and
interlaced auto-encoders. <em>NN</em>, <em>153</em>, 386–398. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving existing neural network architectures can involve several design choices such as manipulating the loss functions, employing a diverse learning strategy, exploiting gradient evolution at training time, optimizing the network hyper-parameters, or increasing the architecture depth. The latter approach is a straightforward solution, since it directly enhances the representation capabilities of a network; however, the increased depth generally incurs in the well-known vanishing gradient problem. In this paper, borrowing from different methods addressing this issue, we introduce an interlaced multi-task learning strategy, defined SIRe, to reduce the vanishing gradient in relation to the object classification task . The presented methodology directly improves a convolutional neural network (CNN) by preserving information from the input image through interlaced auto-encoders (AEs), and further refines the base network architecture by means of skip and residual connections. To validate the presented methodology, a simple CNN and various implementations of famous networks are extended via the SIRe strategy and extensively tested on five collections, i.e., MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and Caltech-256; where the SIRe-extended architectures achieve significantly increased performances across all models and datasets, thus confirming the presented approach effectiveness.},
  archive      = {J_NN},
  author       = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti},
  doi          = {10.1016/j.neunet.2022.06.030},
  journal      = {Neural Networks},
  pages        = {386-398},
  shortjournal = {Neural Netw.},
  title        = {SIRe-networks: Convolutional neural networks architectural extension for information preservation via skip/residual connections and interlaced auto-encoders},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image super-resolution with an enhanced group convolutional
neural network. <em>NN</em>, <em>153</em>, 373–385. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CNNs with strong learning abilities are widely chosen to resolve super-resolution problem. However, CNNs depend on deeper network architectures to improve performance of image super-resolution, which may increase computational cost in general. In this paper, we present an enhanced super-resolution group CNN (ESRGCNN) with a shallow architecture by fully fusing deep and wide channel features to extract more accurate low-frequency information in terms of correlations of different channels in single image super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is useful to inherit more long-distance contextual information for resolving long-term dependency. An adaptive up-sampling operation is gathered into a CNN to obtain an image super-resolution model with low-resolution images of different sizes. Extensive experiments report that our ESRGCNN surpasses the state-of-the-arts in terms of SISR performance, complexity, execution speed, image quality evaluation and visual effect in SISR. Code is found at https://github.com/hellloxiaotian/ESRGCNN .},
  archive      = {J_NN},
  author       = {Chunwei Tian and Yixuan Yuan and Shichao Zhang and Chia-Wen Lin and Wangmeng Zuo and David Zhang},
  doi          = {10.1016/j.neunet.2022.06.009},
  journal      = {Neural Networks},
  pages        = {373-385},
  shortjournal = {Neural Netw.},
  title        = {Image super-resolution with an enhanced group convolutional neural network},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational role of exploration noise in error-based de
novo motor learning. <em>NN</em>, <em>153</em>, 349–372. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The redundancy inherent to the human body is a central problem that must be solved by the brain when acquiring new motor skills. The problem of redundancy becomes particularly critical when learning a new motor policy from scratch in a novel environment and task (i.e., de novo learning). It has been proposed that motor variability could be leveraged to explore and identify task-potent motor commands, and recent results indicated a possible role of motor exploration in error-based motor learning, including in de novo learning tasks. However, the precise computational mechanisms underlying this role remain poorly understood. A new controller in a de novo motor task can potentially be learned by first using motor exploration to learn a sensitivity derivative, which can transform observed task errors into motor corrections, enabling the error-based learning of the controller. Although this approach has been discussed, the computational properties of exploration and how this mechanism can explain recent reports of motor exploration in error-based de-novo learning have not been thoroughly examined. Here, we used this approach to simulate the tasks used in several recent studies of human motor learning tasks in which motor exploration was observed, and replicating their main results. Analyses of the proposed learning mechanism using equations and simulations suggested that exploring the entire motor command space leads to the training of an efficient sensitivity derivative, enabling rapid learning of the controller, in visuomotor adaptation and de novo tasks. The successful replication of previous experimental results elucidated the role of motor exploration in motor learning.},
  archive      = {J_NN},
  author       = {Lucas Rebelo Dal’Bello and Jun Izawa},
  doi          = {10.1016/j.neunet.2022.06.011},
  journal      = {Neural Networks},
  pages        = {349-372},
  shortjournal = {Neural Netw.},
  title        = {Computational role of exploration noise in error-based de novo motor learning},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy preserving generative adversarial networks to model
electronic health records. <em>NN</em>, <em>153</em>, 339–348. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospitals and General Practitioner (GP) surgeries within National Health Services (NHS), collect patient information on a routine basis to create personal health records such as family medical history, chronic diseases, medications and dosing. The collected information could be used to build and model various machine learning algorithms , to simplify the task of those working within the NHS. However, such Electronic Health Records are not made publicly available due to privacy concerns. In our paper, we propose a privacy-preserving Generative Adversarial Network (pGAN), which can generate synthetic data of high quality, while preserving the privacy and statistical properties of the source data. pGAN is evaluated on two distinct datasets, one posing as a Classification task , and the other as a Regression task . Privacy score of generated data is calculated using the Nearest Neighbour Adversarial Accuracy. Cosine similarity scores of synthetic data from our proposed model indicate that the data generated is similar in nature, but not identical. Additionally, our proposed model was able to preserve privacy while maintaining high utility. Machine learning models trained on both synthetic data and original data have achieved accuracies of 74.3\% and 74.5\% respectively on the classification dataset; while they have attained an R2-Score of 0.84 and 0.85 on synthetic and original data of the regression task respectively. Our results, therefore, indicate that synthetic data from the proposed model could replace the use of original data for machine learning while preserving privacy.},
  archive      = {J_NN},
  author       = {Rohit Venugopal and Noman Shafqat and Ishwar Venugopal and Benjamin Mark John Tillbury and Harry Demetrios Stafford and Aikaterini Bourazeri},
  doi          = {10.1016/j.neunet.2022.06.022},
  journal      = {Neural Networks},
  pages        = {339-348},
  shortjournal = {Neural Netw.},
  title        = {Privacy preserving generative adversarial networks to model electronic health records},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From YouTube to the brain: Transfer learning can improve
brain-imaging predictions with deep learning. <em>NN</em>, <em>153</em>,
325–338. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently achieved best-in-class performance in several fields, including biomedical domains such as X-ray images. Yet, data scarcity poses a strict limit on training successful deep learning systems in many, if not most, biomedical applications , including those involving brain images. In this study, we translate state-of-the-art transfer learning techniques for single-subject prediction of simpler (sex and age) and more complex phenotypes (number of people in household, household income, fluid intelligence and smoking behavior). We fine-tuned 2D and 3D ResNet-18 convolutional neural networks for target phenotype predictions from brain images of ∼ 40,000 UK Biobank participants, after pretraining on YouTube videos from the Kinetics dataset and natural images from the ImageNet dataset . Transfer learning was effective on several phenotypes, especially sex and age classification. Additionally, transfer learning in particular outperformed deep learning models trained from scratch especially on smaller sample sizes. The out-of-sample performance using transfer learning from previously learned knowledge based on real-world images and videos could unlock the potential in many areas of imaging neuroscience where deep learning solutions are currently infeasible.},
  archive      = {J_NN},
  author       = {Nahiyan Malik and Danilo Bzdok},
  doi          = {10.1016/j.neunet.2022.06.014},
  journal      = {Neural Networks},
  pages        = {325-338},
  shortjournal = {Neural Netw.},
  title        = {From YouTube to the brain: Transfer learning can improve brain-imaging predictions with deep learning},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online subspace learning and imputation by tensor-ring
decomposition. <em>NN</em>, <em>153</em>, 314–324. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the completion problem of a partially observed high-order streaming data , which is cast as an online low-rank tensor completion problem. Though the online low-rank tensor completion problem has drawn lots of attention in recent years, most of them are designed based on the traditional decomposition method , such as CP and Tucker. Inspired by the advantages of Tensor Ring decomposition over the traditional decompositions in expressing high-order data and its superiority in missing values estimation, this paper proposes two online subspace learning and imputation methods based on Tensor Ring decomposition. Specifically, we first propose an online Tensor Ring subspace learning and imputation model by formulating an exponentially weighted least squares with Frobenium norm regularization of TR-cores. Then, two commonly used optimization algorithms , i.e. alternating recursive least squares and stochastic-gradient algorithms, are developed to solve the proposed model. Numerical experiments show that the proposed methods are more effective to exploit the time-varying subspace in comparison with the conventional Tensor Ring completion methods. Besides, the proposed methods are demonstrated to be superior to obtain better results than state-of-the-art online methods in streaming data completion under varying missing ratios and noise.},
  archive      = {J_NN},
  author       = {Jinshi Yu and Tao Zou and Guoxu Zhou},
  doi          = {10.1016/j.neunet.2022.05.023},
  journal      = {Neural Networks},
  pages        = {314-324},
  shortjournal = {Neural Netw.},
  title        = {Online subspace learning and imputation by tensor-ring decomposition},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reframing control methods for parameters optimization in
adversarial image generation. <em>NN</em>, <em>153</em>, 303–313. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training procedures for deep networks require the setting of several hyper-parameters that strongly affect the obtained results. The problem is even worse in adversarial learning strategies used for image generation where a proper balancing of the discriminative and generative networks is fundamental for an effective training. In this work we propose a novel hyper-parameters optimization strategy based on the use of Proportional–Integral (PI) and Proportional–Integral–Derivative (PID) controllers. Both open loop and closed loop schemes for the tuning of a single parameter or of multiple parameters together are proposed allowing an efficient parameter tuning without resorting to computationally demanding trial-and-error schemes. We applied the proposed strategies to the widely used BEGAN and CycleGAN models: They allowed to achieve a more stable training that converges faster. The obtained images are also sharper with a slightly better quality both visually and according to the FID and FCN metrics. Image translation results also showed better background preservation and less color artifacts with respect to CycleGAN.},
  archive      = {J_NN},
  author       = {Qamar Alfalouji and Piergiorgio Sartor and Pietro Zanuttigh},
  doi          = {10.1016/j.neunet.2022.06.015},
  journal      = {Neural Networks},
  pages        = {303-313},
  shortjournal = {Neural Netw.},
  title        = {Reframing control methods for parameters optimization in adversarial image generation},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph regularized spatial–spectral subspace clustering for
hyperspectral band selection. <em>NN</em>, <em>153</em>, 292–302. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral band selection , which aims to select a small number of bands to reduce data redundancy and noisy bands, has attracted widespread attention in recent years. Many effective clustering-based band selection methods have been proposed to accomplish the band selection task and have achieved satisfying performance. However, most of the previous methods reshape the original hyperspectral images (HSIs) into a set of stretched band vectors, which ignore the spatial information of HSIs and the difference between diverse regions. To address these issues, a graph regularized spatial–spectral subspace clustering method for hyperspectral band selection is proposed in this paper, referred to as GRSC. Specifically, the proposed method adopts superpixel segmentation to preserve the spatial information of HSIs by segmenting their first principal component into diverse homogeneous regions . Then the discriminative latent features are generated from each segmented region to represent the whole band, which can mitigate the effect of noise on the band selection. Finally, a self-representation subspace clustering model and an l 2 , 1 l2,1 -norm regularization are utilized to explore the spectral correlation among all bands. In addition, a similarity graph between region-aware latent features is adaptively learned to preserve the spatial structure of HSIs in the latent representation space. Extensive classification experimental results on three public datasets verify the effectiveness of GRSC over several state-of-the-art methods. The demo code of this work is publicly available at https://github.com/WangJun2023/GRSC .},
  archive      = {J_NN},
  author       = {Jun Wang and Chang Tang and Xiao Zheng and Xinwang Liu and Wei Zhang and En Zhu},
  doi          = {10.1016/j.neunet.2022.06.016},
  journal      = {Neural Networks},
  pages        = {292-302},
  shortjournal = {Neural Netw.},
  title        = {Graph regularized spatial–spectral subspace clustering for hyperspectral band selection},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A universal adversarial policy for text classifiers.
<em>NN</em>, <em>153</em>, 282–291. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering the existence of universal adversarial perturbations had large theoretical and practical impacts on the field of adversarial learning. In the text domain, most universal studies focused on adversarial prefixes which are added to all texts. However, unlike the vision domain, adding the same perturbation to different inputs results in noticeably unnatural inputs. Therefore, we introduce a new universal adversarial setup – a universal adversarial policy, which has many advantages of other universal attacks but also results in valid texts – thus making it relevant in practice. We achieve this by learning a single search policy over a predefined set of semantics preserving text alterations, on many texts. This formulation is universal in that the policy is successful in finding adversarial examples on new texts efficiently. Our approach uses text perturbations which were extensively shown to produce natural attacks in the non-universal setup (specific synonym replacements). We suggest a strong baseline approach for this formulation which uses reinforcement learning . Its ability to generalise (from as few as 500 training texts) shows that universal adversarial patterns exist in the text domain as well.},
  archive      = {J_NN},
  author       = {Gallil Maimon and Lior Rokach},
  doi          = {10.1016/j.neunet.2022.06.018},
  journal      = {Neural Networks},
  pages        = {282-291},
  shortjournal = {Neural Netw.},
  title        = {A universal adversarial policy for text classifiers},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Approximation in shift-invariant spaces with deep ReLU
neural networks. <em>NN</em>, <em>153</em>, 269–281. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the expressive power of deep ReLU neural networks for approximating functions in dilated shift-invariant spaces, which are widely used in signal processing, image processing , communications and so on. Approximation error bounds are estimated with respect to the width and depth of neural networks . The network construction is based on the bit extraction and data-fitting capacity of deep neural networks. As applications of our main results, the approximation rates of classical function spaces such as Sobolev spaces and Besov spaces are obtained. We also give lower bounds of the L p ( 1 ≤ p ≤ ∞ ) Lp(1≤p≤∞) approximation error for Sobolev spaces , which show that our construction of neural network is asymptotically optimal up to a logarithmic factor.},
  archive      = {J_NN},
  author       = {Yunfei Yang and Zhen Li and Yang Wang},
  doi          = {10.1016/j.neunet.2022.06.013},
  journal      = {Neural Networks},
  pages        = {269-281},
  shortjournal = {Neural Netw.},
  title        = {Approximation in shift-invariant spaces with deep ReLU neural networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training much deeper spiking neural networks with a small
number of time-steps. <em>NN</em>, <em>153</em>, 254–268. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Network (SNN) is a promising energy-efficient neural architecture when implemented on neuromorphic hardware. The Artificial Neural Network (ANN) to SNN conversion method, which is the most effective SNN training method, has successfully converted moderately deep ANNs to SNNs with satisfactory performance. However, this method requires a large number of time-steps, which hurts the energy efficiency of SNNs. How to effectively covert a very deep ANN (e.g., more than 100 layers) to an SNN with a small number of time-steps remains a difficult task. To tackle this challenge, this paper makes the first attempt to propose a novel error analysis framework that takes both the “quantization error” and the “deviation error” into account, which comes from the discretization of SNN dynamicsthe neuron’s coding scheme and the inconstant input currents at intermediate layers, respectively. Particularly, our theories reveal that the “deviation error” depends on both the spike threshold and the input variance . Based on our theoretical analysis, we further propose the Threshold Tuning and Residual Block Restructuring (TTRBR) method that can convert very deep ANNs (&gt;100 layers) to SNNs with negligible accuracy degradation while requiring only a small number of time-steps. With very deep networks, our TTRBR method achieves state-of-the-art (SOTA) performance on the CIFAR-10, CIFAR-100, and ImageNet classification tasks .},
  archive      = {J_NN},
  author       = {Qingyan Meng and Shen Yan and Mingqing Xiao and Yisen Wang and Zhouchen Lin and Zhi-Quan Luo},
  doi          = {10.1016/j.neunet.2022.06.001},
  journal      = {Neural Networks},
  pages        = {254-268},
  shortjournal = {Neural Netw.},
  title        = {Training much deeper spiking neural networks with a small number of time-steps},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning for motor imagery based brain–computer
interfaces: A tutorial. <em>NN</em>, <em>153</em>, 235–253. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain–computer interface (BCI) enables a user to communicate directly with an external device, e.g., a computer, using brain signals. It can be used to research, map, assist, augment, or repair human cognitive or sensory–motor functions. A closed-loop BCI system performs signal acquisition, temporal filtering , spatial filtering, feature engineering and classification, before sending out the control signal to an external device. Transfer learning (TL) has been widely used in motor imagery (MI) based BCIs to reduce the calibration effort for a new subject, greatly increasing their utility. This tutorial describes how TL can be considered in as many components of a BCI system as possible, and introduces a complete TL pipeline for MI-based BCIs. Examples on two MI datasets demonstrated the advantages of considering TL in multiple components of MI-based BCIs. Especially, integrating data alignment and sophisticated TL approaches can significantly improve the classification performance, and hence greatly reduces the calibration effort.},
  archive      = {J_NN},
  author       = {Dongrui Wu and Xue Jiang and Ruimin Peng},
  doi          = {10.1016/j.neunet.2022.06.008},
  journal      = {Neural Networks},
  pages        = {235-253},
  shortjournal = {Neural Netw.},
  title        = {Transfer learning for motor imagery based brain–computer interfaces: A tutorial},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Riemannian gradient methods for stochastic composition
problems. <em>NN</em>, <em>153</em>, 224–234. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we study a class of novel stochastic composition optimization problems over Riemannian manifold , which have been raised by multiple emerging machine learning applications such as distributionally robust learning in Riemannian manifold setting. To solve these composition problems , we propose an effective Riemannian compositional gradient (RCG) algorithm, which has a sample complexity of O(ϵ−4) for finding an ϵ -stationary point. To further reduce sample complexity, we propose an accelerated momentum-based Riemannian compositional gradient (M-RCG) algorithm. Moreover, we prove that the M-RCG obtains a lower sample complexity of Õ(ϵ−3) without large batches, which achieves the best known sample complexity for its Euclidean counterparts. Extensive numerical experiments on training deep neural networks (DNNs) over Stiefel manifold and learning principal component analysis (PCA) over Grassmann manifold demonstrate effectiveness of our proposed algorithms. To the best of our knowledge, this is the first study of the composition optimization problems over Riemannian manifold.},
  archive      = {J_NN},
  author       = {Feihu Huang and Shangqian Gao},
  doi          = {10.1016/j.neunet.2022.06.004},
  journal      = {Neural Networks},
  pages        = {224-234},
  shortjournal = {Neural Netw.},
  title        = {Riemannian gradient methods for stochastic composition problems},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SepNet: A neural network for directionally correlated data.
<em>NN</em>, <em>153</em>, 215–223. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional tensor data appear in diverse settings, including multichannel signals, spectrograms , and hyperspectral data from remote sensing. In many cases, these data are directionally correlated, i.e. the correlation between variables from different dimensions is significantly weaker than the correlation between variables from the same dimension. Convolutional neural networks are readily applicable to directionally correlated data but are often inefficient, as they impose many unnecessary connections between neurons. Here we propose a novel architecture, SepNet, specifically for directionally correlated datasets. SepNet uses directional operators to extract directional features from each dimension separately, followed by a linear operator along the depth to generate higher-level features from the directional features. Experiments on two representative directionally correlated datasets showed that SepNet improved network efficiency up to 100-fold while maintaining high accuracy comparable with state-of-the-art convolutional neural network models. Furthermore, SepNet can be flexibly constructed with minimal restriction on the output shape of each layer. These results reveal the potential of data-specific architecting of neural networks .},
  archive      = {J_NN},
  author       = {Fuchang Gao and Yiqing Ma and Boyu Zhang and Min Xian},
  doi          = {10.1016/j.neunet.2022.06.005},
  journal      = {Neural Networks},
  pages        = {215-223},
  shortjournal = {Neural Netw.},
  title        = {SepNet: A neural network for directionally correlated data},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MGLNN: Semi-supervised learning via multiple graph
cooperative learning neural networks. <em>NN</em>, <em>153</em>,
204–214. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many machine learning applications, data are coming with multiple graphs, which is known as the multiple graph learning problem. The problem of multiple graph learning is to learn consistent representation by exploiting the complementary information of multiple graphs. Graph Learning Neural Networks (GLNNs) have been demonstrated powerfully for graph data representation and semi-supervised classification tasks. However, Existing GLNNs are mainly developed for single graph data which cannot be utilized for multiple graph data representation. In this paper, we propose a novel learning framework, called Multiple Graph Learning Neural Networks (MGLNN), for multiple graph learning and multi-view semi-supervised classification. The goal of MGLNN is to learn an optimal graph structure from multiple graph structures that best serves GNNs’ learning which integrates multiple graph learning and GNNs’ representation simultaneously. The proposed MGLNN is a general framework which can incorporate any specific GNN model to deal with multiple graphs. A general algorithm has also been developed to optimize/train the proposed MGLNN model. Experimental results on several datasets demonstrate that MGLNN outperforms some other related methods on semi-supervised classification tasks.},
  archive      = {J_NN},
  author       = {Bo Jiang and Si Chen and Beibei Wang and Bin Luo},
  doi          = {10.1016/j.neunet.2022.05.024},
  journal      = {Neural Networks},
  pages        = {204-214},
  shortjournal = {Neural Netw.},
  title        = {MGLNN: Semi-supervised learning via multiple graph cooperative learning neural networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed-time synchronization of discontinuous competitive
neural networks with time-varying delays. <em>NN</em>, <em>153</em>,
192–203. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the fixed-time (FXT) synchronization of discontinuous competitive neural networks (CNNs) involving time-varying delays is investigated. Firstly, two kinds of discontinuous FXT control schemes are proposed and two forms of Lyapunov function are constructed based on p -norm and 1-norm to discuss the FXT synchronization of CNNs. By means of nonsmooth analysis and some inequality techniques, some simple criteria are obtained to achieve FXT synchronization and the upper bound of the settling time with less conservativeness is provided. Furthermore, the effect of time scale on FXT synchronization of CNNs is considered. Lastly, some numerical results for an example are provided to demonstrate the derived theoretical results.},
  archive      = {J_NN},
  author       = {Caicai Zheng and Cheng Hu and Juan Yu and Haijun Jiang},
  doi          = {10.1016/j.neunet.2022.06.002},
  journal      = {Neural Networks},
  pages        = {192-203},
  shortjournal = {Neural Netw.},
  title        = {Fixed-time synchronization of discontinuous competitive neural networks with time-varying delays},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural network interpolation operators optimized by lagrange
polynomial. <em>NN</em>, <em>153</em>, 179–191. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new type of interpolation operators by using Lagrange polynomials of degree r , which can be regarded as feedforward neural networks with four layers. The approximation rate of the new operators can be estimated by the (r+1) -th modulus of smoothness of the objective functions. By adding some smooth assumptions on the activation function , we establish two important inequalities of the derivatives of the operators. With these two inequalities, by using the K -functional and Berens–Lorentz lemma in approximation theory, we establish the converse theorem of approximation . We also give the Voronovskaja-type asymptotic estimation of the operators for smooth functions. Furthermore, we extend our operators to the multivariate case , and investigate their approximation properties for multivariate functions . Finally, some numerical examples are given to demonstrate the validity of the theoretical results obtained and the superiority of the operators.},
  archive      = {J_NN},
  author       = {Guoshun Wang and Dansheng Yu and Ping Zhou},
  doi          = {10.1016/j.neunet.2022.06.007},
  journal      = {Neural Networks},
  pages        = {179-191},
  shortjournal = {Neural Netw.},
  title        = {Neural network interpolation operators optimized by lagrange polynomial},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum neural networks and topological quantum field
theories. <em>NN</em>, <em>153</em>, 164–178. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work intends to show that: (1) Quantum Neural Networks (QNNs) can be mapped onto spin-networks, with the consequence that the level of analysis of their operation can be carried out on the side of Topological Quantum Field Theory (TQFT); (2) A number of Machine Learning (ML) key-concepts can be rephrased by using the terminology of TQFT. Our framework provides as well a working hypothesis for understanding the generalization behavior of DNNs , relating it to the topological features of the graph structures involved.},
  archive      = {J_NN},
  author       = {Antonino Marcianò and Deen Chen and Filippo Fabrocini and Chris Fields and Enrico Greco and Niels Gresnigt and Krid Jinklub and Matteo Lulli and Kostas Terzidis and Emanuele Zappala},
  doi          = {10.1016/j.neunet.2022.05.028},
  journal      = {Neural Networks},
  pages        = {164-178},
  shortjournal = {Neural Netw.},
  title        = {Quantum neural networks and topological quantum field theories},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new predefined-time stability theorem and its application
in the synchronization of memristive complex-valued BAM neural networks.
<em>NN</em>, <em>153</em>, 152–163. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two novel and general predefined-time stability lemmas are given and applied to the predefined-time synchronization problem of memristive complex-valued bidirectional associative memory neural networks (MCVBAMNNs). Firstly, different from the generally fixed-time stability lemma, the setting of an adjustable time parameter in the derived predefined-time stability lemma causes it to be more flexible and more general. Secondly, the model studied in the complex-valued BAM neural networks model, which is different from the previous discussion of the real part and imaginary part respectively. It is more practical to study the complex-valued nonseparation. Thirdly, two effective controllers are designed to realize the synchronization performance of BAM neural networks based on the predefined-time stability, and the analysis is given based on general predefined-time synchronization. Finally, the correctness of the theoretical derivation is verified by numerical simulation. A secure communication scheme based on predefined-time synchronization of MCVBAMNNs is proposed, and the effectiveness and superiority of the results are proved.},
  archive      = {J_NN},
  author       = {Aidi Liu and Hui Zhao and Qingjie Wang and Sijie Niu and Xizhan Gao and Chuan Chen and Lixiang Li},
  doi          = {10.1016/j.neunet.2022.05.031},
  journal      = {Neural Networks},
  pages        = {152-163},
  shortjournal = {Neural Netw.},
  title        = {A new predefined-time stability theorem and its application in the synchronization of memristive complex-valued BAM neural networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boolean matrix factorization based on collaborative
neurodynamic optimization with boltzmann machines. <em>NN</em>,
<em>153</em>, 142–151. (<a
href="https://doi.org/10.1016/j.neunet.2022.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a collaborative neurodynamic approach to Boolean matrix factorization . Based on a binary optimization formulation to minimize the Hamming distance between a given data matrix and its low-rank reconstruction, the proposed approach employs a population of Boltzmann machines operating concurrently for scatter search of factorization solutions. In addition, a particle swarm optimization rule is used to re-initialize the neuronal states of Boltzmann machines upon their local convergence to escape from local minima toward global solutions. Experimental results demonstrate the superior convergence and performance of the proposed approach against six baseline methods on ten benchmark datasets.},
  archive      = {J_NN},
  author       = {Xinqi Li and Jun Wang and Sam Kwong},
  doi          = {10.1016/j.neunet.2022.06.006},
  journal      = {Neural Networks},
  pages        = {142-151},
  shortjournal = {Neural Netw.},
  title        = {Boolean matrix factorization based on collaborative neurodynamic optimization with boltzmann machines},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring phase–amplitude coupling from primary motor
cortex-basal ganglia–thalamus network model. <em>NN</em>, <em>153</em>,
130–141. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to develop a primary motor cortex (M1)-basal ganglia–thalamus model capable of reproducing the physiological phenomenon of exaggerated phase–amplitude coupling (PAC) in Parkinson’s disease and exploring the potential sources of PAC anomalies in M1. The subthalamic nucleus (STN) phase-STN amplitude coupling, STN phase–M1 amplitude coupling, and M1 phase–M1 amplitude coupling are reproduced, where the phase frequencies are distributed in the beta band and the amplitude frequencies are distributed in the broad gamma band. We mainly study the impacts of thalamus → M1 connections and STN ↔ M1 bidirectional synaptic connections . Abnormal beta oscillations generated within the basal ganglia are found to be transmitted to M1 through the STN or thalamus and could be one of the potential sources of PAC-related beta oscillations in M1, thereby interfering with high-frequency signals in the motor cortex . Furthermore, the weakening of M1 → STN leads to a shift of the oscillations of the STN from the high beta band to the low beta band, which is more consistent with pathological experiments, thus supporting the experimental results that the hyper-direct path from M1 to STN drives the beta oscillations of STN. Finally, the suppression effect of STN deep brain stimulation on PAC is investigated. As the stimulation frequency increases, the PAC modulation index within different regions gradually decreases, in general agreement with the trend of synchronization level and beta oscillation energy, indirectly indicating that PAC can be used as a feedback indicator of parkinsonian state.},
  archive      = {J_NN},
  author       = {Ying Yu and Fang Han and Qingyun Wang},
  doi          = {10.1016/j.neunet.2022.05.027},
  journal      = {Neural Networks},
  pages        = {130-141},
  shortjournal = {Neural Netw.},
  title        = {Exploring phase–amplitude coupling from primary motor cortex-basal ganglia–thalamus network model},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing noisy annotations for depression estimation from
facial images. <em>NN</em>, <em>153</em>, 120–129. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression has been considered the most dominant mental disorder over the past few years. To help clinicians effectively and efficiently estimate the severity scale of depression, various automated systems based on deep learning have been proposed. To estimate the severity of depression, i.e., the depression severity score (Beck Depression Inventory-II), various deep architectures have been designed to perform regression using the Euclidean loss. However, they do not consider the label distribution, and they do not learn the relationships between the facial images and BDI-II scores, which can be resulting in the noisy labeling for automatic depression estimation (ADE). To mitigate this problem, we propose an automated deep architecture, namely the self-adaptation network (SAN), to improve this uncertain labeling for ADE. Specifically, the architecture consists of four modules: (1) ResNet-18 and ResNet-50 are adopted in the deep feature extraction module (DFEM) to extract informative deep features; (2) a self-attention module (SAM) is adopted to learn the weights from the mini-batch; (3) a square ranking regularization module (SRRM) to create high partitions and low partitions is proposed; and (4) a re-label module (RM) is used to re-label the uncertain annotations for ADE in the low partitions. We conduct extensive experiments on depression databases (i.e., AVEC2013 and AVEC2014) and obtain a performance comparable to the performances of other ADE methods in assessing the severity of depression. More importantly, the proposed method can learn valuable depression patterns from facial videos and obtain a performance comparable to the performances of other methods for depression recognition.},
  archive      = {J_NN},
  author       = {Lang He and Prayag Tiwari and Chonghua Lv and WenShuai Wu and Liyong Guo},
  doi          = {10.1016/j.neunet.2022.05.025},
  journal      = {Neural Networks},
  pages        = {120-129},
  shortjournal = {Neural Netw.},
  title        = {Reducing noisy annotations for depression estimation from facial images},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph transformer networks: Learning meta-path graphs to
improve GNNs. <em>NN</em>, <em>153</em>, 104–119. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been widely applied to various fields due to their powerful representations of graph-structured data. Despite the success of GNNs, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. To address these limitations, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which preclude noisy connections and include useful connections (e.g., meta-paths) for tasks, while learning effective node representations on the new graphs in an end-to-end fashion. We further propose enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that improve scalability of graph transformations. Compared to GTNs, FastGTNs are up to 230 × × and 150 × × faster in inference and training, and use up to 100 × × and 148 × × less memory while allowing the identical graph transformations as GTNs. In addition, we extend graph transformations to the semantic proximity of nodes allowing non-local operations beyond meta-paths. Extensive experiments on both homogeneous graphs and heterogeneous graphs show that GTNs and FastGTNs with non-local operations achieve the state-of-the-art performance for node classification tasks. The code is available: https://github.com/seongjunyun/Graph_Transformer_Networks},
  archive      = {J_NN},
  author       = {Seongjun Yun and Minbyul Jeong and Sungdong Yoo and Seunghun Lee and Sean S. Yi and Raehyun Kim and Jaewoo Kang and Hyunwoo J. Kim},
  doi          = {10.1016/j.neunet.2022.05.026},
  journal      = {Neural Networks},
  pages        = {104-119},
  shortjournal = {Neural Netw.},
  title        = {Graph transformer networks: Learning meta-path graphs to improve GNNs},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Successfully and efficiently training deep multi-layer
perceptrons with logistic activation function simply requires
initializing the weights with an appropriate negative mean. <em>NN</em>,
<em>153</em>, 87–103. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vanishing gradient problem (i.e., gradients prematurely becoming extremely small during training, thereby effectively preventing a network from learning) is a long-standing obstacle to the training of deep neural networks using sigmoid activation functions when using the standard back-propagation algorithm. In this paper, we found that an important contributor to the problem is weight initialization. We started by developing a simple theoretical model showing how the expected value of gradients is affected by the mean of the initial weights. We then developed a second theoretical model that allowed us to identify a sufficient condition for the vanishing gradient problem to occur. Using these theories we found that initial back-propagation gradients do not vanish if the mean of the initial weights is negative and inversely proportional to the number of neurons in a layer. Numerous experiments with networks with 10 and 15 hidden layers corroborated the theoretical predictions: If we initialized weights as indicated by the theory, the standard back-propagation algorithm was both highly successful and efficient at training deep neural networks using sigmoid activation functions .},
  archive      = {J_NN},
  author       = {Ahmet Yilmaz and Riccardo Poli},
  doi          = {10.1016/j.neunet.2022.05.030},
  journal      = {Neural Networks},
  pages        = {87-103},
  shortjournal = {Neural Netw.},
  title        = {Successfully and efficiently training deep multi-layer perceptrons with logistic activation function simply requires initializing the weights with an appropriate negative mean},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalp EEG functional connection and brain network in infants
with west syndrome. <em>NN</em>, <em>153</em>, 76–86. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The common age-dependent West syndrome can be diagnosed accurately by electroencephalogram (EEG), but its pathogenesis and evolution remain unclear. Existing research mainly aims at the study of West seizure markers in time/frequency domain, while less literature uses a graph-theoretic approach to analyze changes among different brain regions. In this paper, the scalp EEG based functional connectivity (including Correlation, Coherence, Time Frequency Cross Mutual Information, Phase-Locking Value, Phase Lag Index, Weighted Phase Lag Index) and network topology parameters (including Clustering coefficient , Feature path length, Global efficiency, and Local efficiency) are comprehensively studied for the prognostic analysis of the West episode cycle. The scalp EEGs of 15 children with clinically diagnosed string spasticity seizures are used for prospective study, where the signal is divided into pre-seizure, seizure, and post-seizure states in 5 typical brain wave rhythm frequency bands ( δ (1–4 Hz), θ (4–8 Hz), α (8–13 Hz), β (13–30 Hz), and γ (30–80 Hz)) for functional connectivity analysis. The study shows that recurrent West seizures weaken connections between brain regions responsible for cognition and intelligence, while brain regions responsible for information synergy and visual reception have greater variability in connectivity during seizures. It is observed that the changes in β and γ frequency bands of the multiband brain network connectivity patterns calculated by Corr and WPLI can be preliminarily used as judgment of seizure cycle changes in West syndrome.},
  archive      = {J_NN},
  author       = {Runze Zheng and Yuanmeng Feng and Tianlei Wang and Jiuwen Cao and Duanpo Wu and Tiejia Jiang and Feng Gao},
  doi          = {10.1016/j.neunet.2022.05.029},
  journal      = {Neural Networks},
  pages        = {76-86},
  shortjournal = {Neural Netw.},
  title        = {Scalp EEG functional connection and brain network in infants with west syndrome},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent neural networks as kinematics estimator and
controller for redundant manipulators subject to physical constraints.
<em>NN</em>, <em>153</em>, 64–75. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundant manipulators could be efficient tools in industrial production as a result of their dexterity. However, existing kinematic control methods for redundant manipulators have two main disadvantages . On one hand, model uncertainties or unknown kinematic parameters may degrade the performance of existing model-based control methods subject to joint limits. On the other hand, existing model-free control methods ignore the existence of joint limits although they do not need to know kinematic models of redundant manipulators. In this paper, a quadratic programming (QP) scheme is elaborated to achieve the primary tracking control task of redundant manipulators as well as joint limits avoidance task. Besides, a gradient neurodynamics (GND) model is utilized to estimate the kinematics of redundant manipulators. Then, a primal dual neural network , which is employed to solve the QP problem, and the GND model are integrated towards developing a model-free control method constrained by joint angle and velocity limits for redundant manipulators. The visual sensory feedback is fed to the two neural networks. The efficacy of the proposed control method is demonstrated by extensive simulations and experiments, and the merits of the proposed method are also substantiated by comparisons.},
  archive      = {J_NN},
  author       = {Ning Tan and Peng Yu and Shen Liao and Zhenglong Sun},
  doi          = {10.1016/j.neunet.2022.05.021},
  journal      = {Neural Networks},
  pages        = {64-75},
  shortjournal = {Neural Netw.},
  title        = {Recurrent neural networks as kinematics estimator and controller for redundant manipulators subject to physical constraints},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation properties of gaussian-binary restricted
boltzmann machines and gaussian-binary deep belief networks.
<em>NN</em>, <em>153</em>, 49–63. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the successful use of Gaussian-binary restricted Boltzmann machines (GB-RBMs) and Gaussian-binary deep belief networks (GB-DBNs), little is known about their theoretical approximation capabilities to represent distributions of continuous random variables . In this paper, we address the expressive properties of GB-RBMs and GB-DBNs, contributing theoretical insights to the optimal number of hidden variables. We first treat the GB-RBM’s unnormalized log-likelihood as a sum of a special two-layer feedforward neural network and a negative quadratic term . Then, a series of simulation results are established, which can be used to relate GB-RBMs to general two-layer feedforward neural networks whose expressive properties are much better understood. On this basis, we show that a two-layer ReLU network with all weights in the second layer being 1, along with a negative quadratic term , can approximate all continuous functions. In addition, we provide qualified lower bounds for the number of hidden variables of GB-RBMs required to approximate distributions whose log-likelihood are given by some classes of smooth functions. Moreover, we further study the universal approximation of GB-DBNs with two hidden layers by providing a sufficient number of hidden variables O ( ɛ − 2 ) ɛ O(ɛ−2) that are guaranteed to approximate any given strictly positive continuous distribution within a given error ɛ ɛ ɛ . Finally, numerical experiments are carried out to verify some of the proposed theoretical results.},
  archive      = {J_NN},
  author       = {Linyan Gu and Lihua Yang and Feng Zhou},
  doi          = {10.1016/j.neunet.2022.05.020},
  journal      = {Neural Networks},
  pages        = {49-63},
  shortjournal = {Neural Netw.},
  title        = {Approximation properties of gaussian-binary restricted boltzmann machines and gaussian-binary deep belief networks},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive 2-bits-triggered neural control for uncertain
nonlinear multi-agent systems with full state constraints. <em>NN</em>,
<em>153</em>, 37–48. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates an adaptive 2-bits-triggered neural control for a class of uncertain nonlinear multi-agent systems (MASs) with full state constraints. Considering the limitations of practical physical devices and operating conditions, MASs may suffer performance degradation or even crash while the system states are not restricted. With this in mind, combined with barrier Lyapunov function (BLF), an adaptive neural consensus control is developed to guarantee that the state constraints of all followers are not violated. Further, the conversion relationship between the state constraints of MASs and the synchronization error constraints is clarified more precisely, which could improve the synchronization performance of MASs. In addition, considering both trigger threshold setting and control signal transmission bits issues, a 2-bit trigger strategy is proposed to maximize the utilization of MASs bandwidth resources. Theoretical analysis shows that all signals are uniformly ultimately bounded. And the simulation results demonstrate its effectiveness.},
  archive      = {J_NN},
  author       = {Zicong Chen and Jianhui Wang and Tao Zou and Kemao Ma and Qinruo Wang},
  doi          = {10.1016/j.neunet.2022.05.019},
  journal      = {Neural Networks},
  pages        = {37-48},
  shortjournal = {Neural Netw.},
  title        = {Adaptive 2-bits-triggered neural control for uncertain nonlinear multi-agent systems with full state constraints},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey for deep reinforcement learning in markovian
cyber–physical systems: Common problems and solutions. <em>NN</em>,
<em>153</em>, 13–36. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) is increasingly applied in cyber–physical systems for automation tasks. It is important to record the developing trends in DRL’s applications to help researchers overcome common problems using common solutions. This survey investigates trends seen within two applied settings: motor control tasks, and resource allocation tasks. The common problems include intractability of the action space, or state space, as well as hurdles associated with the prohibitive cost of training systems from scratch in the real-world. Real-world training data is sparse and difficult to derive and training in real-world can damage real-world learning systems. Researchers have provided a set of common as well as unique solutions . Tackling the problem of intractability , researchers have succeeded in guiding network training with handcrafted reward functions, auxiliary learning, and by simplifying the state or action spaces before performing transfer learning to more complex systems. Many state-of-the-art algorithms reformulate problems to use multi-agent or hierarchical learning to reduce the intractability of the state or action spaces for a single agent. Common solutions to the prohibitive cost of training include using benchmarks and simulations. This requires a shared feature space common to both simulation and the real world; without that you introduce what is known as the reality gap problem. This is the first survey, to our knowledge, that studies DRL as it is applied in the real world at this scope. It is our hope that the common solutions surveyed become common practice.},
  archive      = {J_NN},
  author       = {Timothy Rupprecht and Yanzhi Wang},
  doi          = {10.1016/j.neunet.2022.05.013},
  journal      = {Neural Networks},
  pages        = {13-36},
  shortjournal = {Neural Netw.},
  title        = {A survey for deep reinforcement learning in markovian cyber–physical systems: Common problems and solutions},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse signal reconstruction via recurrent neural networks
with hyperbolic tangent function. <em>NN</em>, <em>153</em>, 1–12. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, several recurrent neural networks (RNNs) for solving the L 1 -minimization problem are proposed. First, a one-layer RNN based on the hyperbolic tangent function and the projection matrix is designed. In addition, the stability and global convergence of the previously presented RNN are proved by the Lyapunov method . Then, the sliding mode control technique is introduced into the former RNN to design finite-time RNN (FTRNN). Under the condition that the projection matrix satisfies the Restricted Isometry Property (RIP), a suitable Lyapunov function is constructed to prove that the FTRNN is stable in the Lyapunov sense and has the finite-time convergence property . Finally, we make a comparison of the proposed RNN and FTRNN with the existing RNNs. To achieve this, we implement experiments for sparse signal reconstruction and image reconstruction. The results further demonstrate the effectiveness and superior performance of the proposed RNN and FTRNN.},
  archive      = {J_NN},
  author       = {Hongsong Wen and Xing He and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.05.022},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {Sparse signal reconstruction via recurrent neural networks with hyperbolic tangent function},
  volume       = {153},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022f). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>152</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00222-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00222-2},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Current events. <em>NN</em>, <em>152</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00221-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00221-0},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neuroscience-inspired spiking neural network for EEG-based
auditory spatial attention detection. <em>NN</em>, <em>152</em>,
555–565. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that alpha oscillations (8–13 Hz) enable the decoding of auditory spatial attention . Inspired by sparse coding in cortical neurons , we propose a spiking neural network model for auditory spatial attention detection. The proposed model can extract the patterns of recorded EEG of leftward and rightward attention, independently, and uses them to train the network to detect auditory spatial attention. Specifically, our model is composed of three layers, two of which are Integrate and Fire spiking neurons . We formulate a new learning rule that is based on the firing rate of pre- and post-synaptic neurons in the first and second layers of spiking neurons . The third layer has 10 spiking neurons and the pattern of their firing rate is used in the test phase to decode the auditory spatial attention of a given test sample. Moreover, the effects of using low connectivity rates of the layers and specific range of learning parameters of the learning rule are investigated. The proposed model achieves an average accuracy of 90\% with only 10\% of EEG signals as training data. This study also provides new insights into the role of sparse coding in both cortical networks subserving cognitive tasks and brain-inspired machine learning .},
  archive      = {J_NN},
  author       = {Faramarz Faghihi and Siqi Cai and Ahmed A. Moustafa},
  doi          = {10.1016/j.neunet.2022.05.003},
  journal      = {Neural Networks},
  pages        = {555-565},
  shortjournal = {Neural Netw.},
  title        = {A neuroscience-inspired spiking neural network for EEG-based auditory spatial attention detection},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social impact and governance of AI and neurotechnologies.
<em>NN</em>, <em>152</em>, 542–554. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in artificial intelligence (AI) and brain science are going to have a huge impact on society. While technologies based on those advances can provide enormous social benefits, adoption of new technologies poses various risks. This article first reviews the co-evolution of AI and brain science and the benefits of brain-inspired AI in sustainability, healthcare, and scientific discoveries. We then consider possible risks from those technologies, including intentional abuse, autonomous weapons, cognitive enhancement by brain–computer interfaces, insidious effects of social media, inequity, and enfeeblement. We also discuss practical ways to bring ethical principles into practice. One proposal is to stop giving explicit goals to AI agents and to enable them to keep learning human preferences. Another is to learn from democratic mechanisms that evolved in human society to avoid over-consolidation of power. Finally, we emphasize the importance of open discussions not only by experts, but also including a diverse array of lay opinions.},
  archive      = {J_NN},
  author       = {Kenji Doya and Arisa Ema and Hiroaki Kitano and Masamichi Sakagami and Stuart Russell},
  doi          = {10.1016/j.neunet.2022.05.012},
  journal      = {Neural Networks},
  pages        = {542-554},
  shortjournal = {Neural Netw.},
  title        = {Social impact and governance of AI and neurotechnologies},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted incremental–decremental support vector machines for
concept drift with shifting window. <em>NN</em>, <em>152</em>, 528–541.
(<a href="https://doi.org/10.1016/j.neunet.2022.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning the data samples’ distribution as it changes in time. This change, known as concept drift, complicates the task of training a model, as the predictions become less and less accurate. It is known that Support Vector Machines (SVMs) can learn weighted input instances and that they can also be trained online (incremental–decremental learning). Combining these two SVM properties, the open problem is to define an online SVM concept drift model with shifting weighted window. The classic SVM model should be retrained from scratch after each window shift. We introduce the Weighted Incremental–Decremental SVM (WIDSVM), a generalization of the incremental–decremental SVM for shifting windows. WIDSVM is capable of learning from data streams with concept drift, using the weighted shifting window technique. The soft margin constrained optimization problem imposed on the shifting window is reduced to an incremental–decremental SVM. At each window shift, we determine the exact conditions for vector migration during the incremental–decremental process. We perform experiments on artificial and real-world concept drift datasets; they show that the classification accuracy of WIDSVM significantly improves compared to a SVM with no shifting window. The WIDSVM training phase is fast, since it does not retrain from scratch after each window shift.},
  archive      = {J_NN},
  author       = {Honorius Gâlmeanu and Răzvan Andonie},
  doi          = {10.1016/j.neunet.2022.05.018},
  journal      = {Neural Networks},
  pages        = {528-541},
  shortjournal = {Neural Netw.},
  title        = {Weighted Incremental–Decremental support vector machines for concept drift with shifting window},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Corrigendum to “event-centric multi-modal fusion method for
dense video captioning” [neural networks 146 (2022) 120–129].
<em>NN</em>, <em>152</em>, 527. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Zhi Chang and Dexin Zhao and Huilin Chen and Jingdan Li and Pengfei Liu},
  doi          = {10.1016/j.neunet.2022.05.011},
  journal      = {Neural Networks},
  pages        = {527},
  shortjournal = {Neural Netw.},
  title        = {Corrigendum to “Event-centric multi-modal fusion method for dense video captioning” [Neural networks 146 (2022) 120–129]},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-guided auto-labeling for network traffic data: The
GELM approach. <em>NN</em>, <em>152</em>, 510–526. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data labeling is crucial in various areas, including network security , and a prerequisite for applying statistical-based classification and supervised learning techniques. Therefore, developing labeling methods that ensure good performance is important. We propose a human-guided auto-labeling algorithm involving the self-supervised learning concept, with the purpose of labeling data quickly, accurately, and consistently. It consists of three processes: auto-labeling, validation, and update. A labeling scheme is proposed by considering weighted features in the auto-labeling, while the generalized extreme learning machine (GELM) enabling fast training is applied to validate assigned labels. Two different approaches are considered in the update to label new data to investigate labeling speed and accuracy. We experiment to verify the suitability and accuracy of the algorithm for network traffic, applying the algorithm to five traffic datasets, some including distributed denial of service (DDoS), DoS, BruteForce, and PortScan attacks. Numerical results show the algorithm labels unlabeled datasets quickly, accurately, and consistently and the GELM’s learning speed enables labeling data in real-time. It also shows that the performances between auto- and conventional labels are nearly identical on datasets containing only DDoS attacks , which implies the algorithm is quite suitable for such datasets. However, the performance differences between the two labels are not negligible on datasets, including various attacks. Several reasons that require further investigation can be considered, including the selected features and the reliability of conventional labels. Even with this limitation of the current study, the algorithm will provide a criterion for labeling data in real-time occurring in many areas.},
  archive      = {J_NN},
  author       = {Meejoung Kim and Inkyu Lee},
  doi          = {10.1016/j.neunet.2022.05.007},
  journal      = {Neural Networks},
  pages        = {510-526},
  shortjournal = {Neural Netw.},
  title        = {Human-guided auto-labeling for network traffic data: The GELM approach},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multivariate adaptive gradient algorithm with reduced
tuning efforts. <em>NN</em>, <em>152</em>, 499–509. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large neural networks usually perform well for executing machine learning tasks. However, models that achieve state-of-the-art performance involve arbitrarily large number of parameters and therefore their training is very expensive. It is thus desired to implement methods with small per-iteration costs, fast convergence rates, and reduced tuning. This paper proposes a multivariate adaptive gradient descent method that meets the above attributes. The proposed method updates every element of the model parameters separately in a computationally efficient manner using an adaptive vector-form learning rate , resulting in low per-iteration cost. The adaptive learning rate computes the absolute difference of current and previous model parameters over the difference in subgradients of current and previous state estimates. In the deterministic setting, we show that the cost function value converges at a linear rate for smooth and strongly convex cost functions. Whereas in both the deterministic and stochastic setting, we show that the gradient converges in expectation at the order of O ( 1 / k ) O(1/k) for a non-convex cost function with Lipschitz continuous gradient. In addition, we show that after T T iterates, the cost function of the last iterate scales as O ( log ( T ) / T ) O(log(T)/T) for non-smooth strongly convex cost functions. Effectiveness of the proposed method is validated on convex functions , smooth non-convex function, non-smooth convex function, and four image classification data sets, whilst showing that its execution requires hardly any tuning unlike existing popular optimizers that entail relatively large tuning efforts. Our empirical results show that our proposed algorithm provides the best overall performance when comparing it to tuned state-of-the-art optimizers.},
  archive      = {J_NN},
  author       = {Samer Saab Jr and Khaled Saab and Shashi Phoha and Minghui Zhu and Asok Ray},
  doi          = {10.1016/j.neunet.2022.05.016},
  journal      = {Neural Networks},
  pages        = {499-509},
  shortjournal = {Neural Netw.},
  title        = {A multivariate adaptive gradient algorithm with reduced tuning efforts},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GIU-GANs: Global information utilization for generative
adversarial networks. <em>NN</em>, <em>152</em>, 487–498. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the rapid development of artificial intelligence , image generation based on deep learning has advanced significantly. Image generation based on Generative Adversarial Networks (GANs) is a promising study. However, because convolutions are limited by spatial-agnostic and channel-specific, features extracted by conventional GANs based on convolution are constrained. Therefore, GANs cannot capture in-depth details per image. Moreover, straightforwardly stacking of convolutions causes too many parameters and layers in GANs, yielding a high overfitting risk. To overcome the abovementioned limitations, in this study, we propose a GANs called GIU-GANs (where Global Information Utilization: GIU). GIU-GANs leverages a new module called the GIU module, which integrates the squeeze-and-excitation module and involution to focus on global information via the channel attention mechanism , enhancing the generated image quality. Moreover, Batch Normalization (BN) inevitably ignores the representation differences among noise sampled by the generator and thus degrades the generated image quality. Thus, we introduce the representative BN to the GANs’ architecture. The CIFAR-10 and CelebA datasets are employed to demonstrate the effectiveness of the proposed model. Numerous experiments indicate that the proposed model achieves state-of-the-art performance.},
  archive      = {J_NN},
  author       = {Yongqi Tian and Xueyuan Gong and Jialin Tang and Binghua Su and Xiaoxiang Liu and Xinyuan Zhang},
  doi          = {10.1016/j.neunet.2022.05.014},
  journal      = {Neural Networks},
  pages        = {487-498},
  shortjournal = {Neural Netw.},
  title        = {GIU-GANs: Global information utilization for generative adversarial networks},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Set-membership filtering for complex networks with
constraint communication channels. <em>NN</em>, <em>152</em>, 479–486.
(<a href="https://doi.org/10.1016/j.neunet.2022.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The set-membership filtering is studied for a class of multi-rate sampling complex networks with communication capacity constraint. For reducing communication load, the weighted try-once-discard scheduling protocol is utilized to transmit the most needed measurement. To improve the filtering performance, a novel mixed compensation method is proposed to obtain a compensatory measurement that is closer to the actual value. Accordingly, a mixed compensation dependent filter is designed, and a filtering error system is obtained. Sufficient conditions are established to ensure that the filtering error system satisfies P T k PTk -dependent constraint. Then, a new algorithm is designed to obtain the optimized ellipsoid by minimizing the constraint matrix. Finally, an illustrative example is given to demonstrate the validity of the developed filter.},
  archive      = {J_NN},
  author       = {Chang Liu and Lixin Yang and Jie Tao and Yong Xu and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.05.009},
  journal      = {Neural Networks},
  pages        = {479-486},
  shortjournal = {Neural Netw.},
  title        = {Set-membership filtering for complex networks with constraint communication channels},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic consistency learning on manifold for source
data-free unsupervised domain adaptation. <em>NN</em>, <em>152</em>,
467–478. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, source data-free unsupervised domain adaptation (SFUDA) attracts increasing attention. Current work shows that the geometry of the target data is helpful to solving this challenging problem. However, these methods define the geometric structures in Euclidean space . The geometry cannot completely draw the semantic relationship between the target data distributed on a manifold. This article proposed a new SFUDA method, semantic consistency learning on manifold (SCLM), to address this problem. Firstly, we generated pseudo-labels for the target data using a new clustering method , EntMomClustering, that enhanced k-means clustering by fusing the entropy momentum. Secondly, we constructed semantic neighbor topology (SNT) to capture complete geometric information on the manifold. Specifically, in SNT, the global neighbor was detected by a developed collaborative representation-based manifold projection, while the local neighbors were obtained by similarity comparison. Thirdly, we performed a semantic consistency learning on SNT to drive a new kind of deep clustering where SNT was taken as the basic clustering unit. To ensure SNT move as entirety, in the developed objective, the entropy regulator was constructed based on a semantic mixture fused on SNT, while the self-supervised regulator encouraged similar classification on SNT. Experiments on three benchmark datasets show that our method achieves state-of-the-art results. The code is available on https://github.com/tntek/SCLM .},
  archive      = {J_NN},
  author       = {Song Tang and Yan Zou and Zihao Song and Jianzhi Lyu and Lijuan Chen and Mao Ye and Shouming Zhong and Jianwei Zhang},
  doi          = {10.1016/j.neunet.2022.05.015},
  journal      = {Neural Networks},
  pages        = {467-478},
  shortjournal = {Neural Netw.},
  title        = {Semantic consistency learning on manifold for source data-free unsupervised domain adaptation},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Branching time active inference: Empirical study and
complexity class analysis. <em>NN</em>, <em>152</em>, 450–466. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a state-of-the-art framework for modelling the brain that explains a wide range of mechanisms such as habit formation, dopaminergic discharge and curiosity. However, recent implementations suffer from an exponential (space and time) complexity class when computing the prior over all the possible policies up to the time horizon. Fountas et al. (2020) used Monte Carlo tree search to address this problem, leading to very good results in two different tasks. Additionally, Champion et al. (2021a) proposed a tree search approach based on (temporal) structure learning . This was enabled by the development of a variational message passing approach to active inference (Champion, Bowman, Grześ, 2021), which enables compositional construction of Bayesian networks for active inference. However, this message passing tree search approach, which we call branching-time active inference (BTAI), has never been tested empirically. In this paper, we present an experimental study of the approach (Champion, Grześ, Bowman, 2021) in the context of a maze solving agent. In this context, we show that both improved prior preferences and deeper search help mitigate the vulnerability to local minima. Then, we compare BTAI to standard active inference (AcI) on a graph navigation task . We show that for small graphs, both BTAI and AcI successfully solve the task. For larger graphs, AcI exhibits an exponential (space) complexity class, making the approach intractable. However, BTAI explores the space of policies more efficiently, successfully scaling to larger graphs. Then, BTAI was compared to the POMCP algorithm (Silver and Veness, 2010) on the frozen lake environment. The experiments suggest that BTAI and the POMCP algorithm accumulate a similar amount of reward. Also, we describe when BTAI receives more rewards than the POMCP agent, and when the opposite is true. Finally, we compared BTAI to the approach of Fountas et al. (2020) on the dSprites dataset, and we discussed the pros and cons of each approach.},
  archive      = {J_NN},
  author       = {Théophile Champion and Howard Bowman and Marek Grześ},
  doi          = {10.1016/j.neunet.2022.05.010},
  journal      = {Neural Networks},
  pages        = {450-466},
  shortjournal = {Neural Netw.},
  title        = {Branching time active inference: Empirical study and complexity class analysis},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual context learning based on textual knowledge for
image–text retrieval. <em>NN</em>, <em>152</em>, 434–449. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image–text bidirectional retrieval is a significant task within cross-modal learning field. The main issue lies on the jointly embedding learning and accurately measuring image–text matching score. Most prior works make use of either intra-modality methods performing within two separate modalities or inter-modality ones combining two modalities tightly. However, intra-modality methods remain ambiguous when learning visual context due to the existence of redundant messages. And inter-modality methods increase the complexity of retrieval because of unifying two modalities closely when learning modal features. In this research, we propose an eclectic V isual C ontext L earning based on T extual knowledge N etwork (VCLTN), which transfers textual knowledge to visual modality for context learning and decreases the discrepancy of information capacity between two modalities. Specifically, VCLTN merges label semantics into corresponding regional features and employs those labels as intermediaries between images and texts for better modal alignment. Contextual knowledge of those labels learned within textual modality is utilized to guide the visual context learning. Besides, considering the homogeneity within each modality, global features are merged into regional features for assisting in the context learning. In order to alleviate the imbalance of information capacity between images and texts, entities together with relations inside the given caption are extracted and an auxiliary caption is sampled for attaching supplementary messages to textual modality. Experiments performed on Flickr30K and MS-COCO reveal that our model VCLTN achieves best results compared with the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Yuzhuo Qin and Xiaodong Gu and Zhenshan Tan},
  doi          = {10.1016/j.neunet.2022.05.008},
  journal      = {Neural Networks},
  pages        = {434-449},
  shortjournal = {Neural Netw.},
  title        = {Visual context learning based on textual knowledge for image–text retrieval},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampled-data synchronization of complex network based on
periodic self-triggered intermittent control and its application to
image encryption. <em>NN</em>, <em>152</em>, 419–433. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to investigate exponential synchronization issue of time-varying multi-weights network with time delays (TMNTD) via periodic self-triggered intermittent sampled-data control. In particular, it is the first time to combine periodic self-triggered control and intermittent control with sampled-data, which has broader application prospects. Therein, self-triggered scheme is periodic judgment and aimed at intermittent control. And during control intervals in intermittent control, there is periodic sampled-data control. In addition, by applying tools of sampled-data control, intermittent control, event-driven control theory and stability analysis, some sufficient conditions are derived to guarantee exponential synchronization of TMNTD. After that, the theoretical results are utilized to research exponential synchronization issue of time-varying multi-weights Chua’s circuits with time delays . Meantime, numerical simulations are provided to demonstrate the validity of the theoretical results. Finally, an image encryption algorithm is designed as a practical application of the developed results.},
  archive      = {J_NN},
  author       = {Hui Zhou and Zijiang Liu and Dianhui Chu and Wenxue Li},
  doi          = {10.1016/j.neunet.2022.05.004},
  journal      = {Neural Networks},
  pages        = {419-433},
  shortjournal = {Neural Netw.},
  title        = {Sampled-data synchronization of complex network based on periodic self-triggered intermittent control and its application to image encryption},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LAP: Latency-aware automated pruning with dynamic-based
filter selection. <em>NN</em>, <em>152</em>, 407–418. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model pruning is widely used to compress and accelerate convolutional neural networks (CNNs). Conventional pruning techniques only focus on how to remove more parameters while ensuring model accuracy. This work not only covers the optimization of model accuracy, but also optimizes the model latency during pruning. When there are multiple optimization objectives, the difficulty of algorithm design increases exponentially. So latency sensitivity is proposed to effectively guide the determination of layer sparsity in this paper. We present the latency-aware automated pruning (LAP) framework which leverages the reinforcement learning to automatically determine the layer sparsity . Latency sensitivity is used as a prior knowledge and involved into the exploration loop. Rather than relying on a single reward signal such as validation accuracy or floating-point operations (FLOPs), our agent receives the feedback on the accuracy error and latency sensitivity. We also provide a novel filter selection algorithm to accurately distinguish important filters within a layer based on their dynamic changes. Compared to the state-of-the-art compression policies, our framework demonstrated superior performances for VGGNet, ResNet, and MobileNet on CIFAR-10, ImageNet, and Food-101. Our LAP allowed the inference latency of MobileNet-V1 to achieve approximately 1.64 times speedup on the Titan RTX GPU , with no loss of ImageNet Top-1 accuracy. It significantly improved the pareto optimal curve on the accuracy and latency trade-off.},
  archive      = {J_NN},
  author       = {Zailong Chen and Chubo Liu and Wangdong Yang and Kenli Li and Keqin Li},
  doi          = {10.1016/j.neunet.2022.05.002},
  journal      = {Neural Networks},
  pages        = {407-418},
  shortjournal = {Neural Netw.},
  title        = {LAP: Latency-aware automated pruning with dynamic-based filter selection},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-guided multi-task attention network for survival
risk prediction using multi-center computed tomography images.
<em>NN</em>, <em>152</em>, 394–406. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate preoperative prediction of overall survival (OS) risk of human cancers based on CT images is greatly significant for personalized treatment. Deep learning methods have been widely explored to improve automated prediction of OS risk. However, the accuracy of OS risk prediction has been limited by prior existing methods. To facilitate capturing survival-related information, we proposed a novel knowledge-guided multi-task network with tailored attention modules for OS risk prediction and prediction of clinical stages simultaneously. The network exploits useful information contained in multiple learning tasks to improve prediction of OS risk. Three multi-center datasets, including two gastric cancer datasets with 459 patients, and a public American lung cancer dataset with 422 patients, are used to evaluate our proposed network. The results show that our proposed network can boost its performance by capturing and sharing information from other predictions of clinical stages. Our method outperforms the state-of-the-art methods with the highest geometrical metric. Furthermore, our method shows better prognostic value with the highest hazard ratio for stratifying patients into high- and low-risk groups. Therefore, our proposed method may be exploited as a potential tool for the improvement of personalized treatment.},
  archive      = {J_NN},
  author       = {Liwen Zhang and Lianzhen Zhong and Cong Li and Wenjuan Zhang and Chaoen Hu and Di Dong and Zaiyi Liu and Junlin Zhou and Jie Tian},
  doi          = {10.1016/j.neunet.2022.04.027},
  journal      = {Neural Networks},
  pages        = {394-406},
  shortjournal = {Neural Netw.},
  title        = {Knowledge-guided multi-task attention network for survival risk prediction using multi-center computed tomography images},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Replacing pooling functions in convolutional neural networks
by linear combinations of increasing functions. <em>NN</em>,
<em>152</em>, 380–393. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, Convolutional Neural Networks make use of the maximum or arithmetic mean in order to reduce the features extracted by convolutional layers in a downsampling process known as pooling. However, there is no strong argument to settle upon one of the two functions and, in practice, this selection turns to be problem dependent. Further, both of these options ignore possible dependencies among the data. We believe that a combination of both of these functions, as well as of additional ones which may retain different information, can benefit the feature extraction process. In this work, we replace traditional pooling by several alternative functions. In particular, we consider linear combinations of order statistics and generalizations of the Sugeno integral , extending the latter’s domain to the whole real line and setting the theoretical base for their application. We present an alternative pooling layer based on this strategy which we name “CombPool” layer. We replace the pooling layers of three different architectures of increasing complexity by CombPool layers, and empirically prove over multiple datasets that linear combinations outperform traditional pooling functions in most cases. Further, combinations with either the Sugeno integral or one of its generalizations usually yield the best results, proving a strong candidate to apply in most architectures.},
  archive      = {J_NN},
  author       = {Iosu Rodriguez-Martinez and Julio Lafuente and Regivan H.N. Santiago and Graçaliz Pereira Dimuro and Francisco Herrera and Humberto Bustince},
  doi          = {10.1016/j.neunet.2022.04.028},
  journal      = {Neural Networks},
  pages        = {380-393},
  shortjournal = {Neural Netw.},
  title        = {Replacing pooling functions in convolutional neural networks by linear combinations of increasing functions},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative convolution layer for image generation.
<em>NN</em>, <em>152</em>, 370–379. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel convolution method, called generative convolution (GConv), which is simple yet effective for improving the generative adversarial network (GAN) performance. Unlike the standard convolution, GConv first selects useful kernels compatible with the given latent vector, and then linearly combines the selected kernels to make latent-specific kernels. Using the latent-specific kernels, the proposed method produces the latent-specific features which encourage the generator to produce high-quality images. This approach is simple but surprisingly effective. First, the GAN performance is significantly improved with a little additional hardware cost. Second, GConv can be employed to the existing state-of-the-art generators without modifying the network architecture . To reveal the superiority of GConv, this paper provides extensive experiments using various standard datasets including CIFAR-10, CIFAR-100, LSUN-Church, CelebA, and tiny-ImageNet. Quantitative evaluations prove that GConv significantly boosts the performances of the unconditional and conditional GANs in terms of Frechet inception distance (FID) and Inception score (IS). For example, the proposed method improves both FID and IS scores on the tiny-ImageNet dataset from 35.13 to 29.76 and 20.23 to 22.64, respectively.},
  archive      = {J_NN},
  author       = {Seung Park and Yong-Goo Shin},
  doi          = {10.1016/j.neunet.2022.05.006},
  journal      = {Neural Networks},
  pages        = {370-379},
  shortjournal = {Neural Netw.},
  title        = {Generative convolution layer for image generation},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A manifold learning approach for gesture recognition from
micro-doppler radar measurements. <em>NN</em>, <em>152</em>, 353–369.
(<a href="https://doi.org/10.1016/j.neunet.2022.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paper (Mhaskar (2020)) introduces a straightforward and simple kernel based approximation for manifold learning that does not require the knowledge of anything about the manifold, except for its dimension. In this paper, we examine how the pointwise error in approximation using least squares optimization based on similarly localized kernels depends upon the data characteristics and deteriorates as one goes away from the training data. The theory is presented with an abstract localized kernel, which can utilize any prior knowledge about the data being located on an unknown sub-manifold of a known manifold. We demonstrate the performance of our approach using a publicly available micro-Doppler data set, and investigate the use of different preprocessing measures, kernels, and manifold dimensions. Specifically, it is shown that the localized kernel introduced in the above mentioned paper when used with PCA components leads to a near-competitive performance to deep neural networks , and offers significant improvements in training speed and memory requirements. To demonstrate the fact that our methods are agnostic to the domain knowledge, we examine the classification problem in a simple video data set.},
  archive      = {J_NN},
  author       = {E.S. Mason and H.N. Mhaskar and Adam Guo},
  doi          = {10.1016/j.neunet.2022.04.024},
  journal      = {Neural Networks},
  pages        = {353-369},
  shortjournal = {Neural Netw.},
  title        = {A manifold learning approach for gesture recognition from micro-doppler radar measurements},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust kernel principal component analysis with optimal
mean. <em>NN</em>, <em>152</em>, 347–352. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kernel principal component analysis (KPCA) serves as an efficient approach for dimensionality reduction. However, the KPCA method is sensitive to the outliers since the large square errors tend to dominate the loss of KPCA. To strengthen the robustness of KPCA method, we propose a novel robust kernel principal component analysis with optimal mean (RKPCA-OM) method. RKPCA-OM not only possesses stronger robustness for outliers than the conventional KPCA method, but also can eliminate the optimal mean automatically. What is more, the theoretical proof proves the convergence of the algorithm to guarantee that the optimal subspaces and means are obtained. Lastly, exhaustive experimental results verify the superiority of our method.},
  archive      = {J_NN},
  author       = {Pei Li and Wenlin Zhang and Chengjun Lu and Rui Zhang and Xuelong Li},
  doi          = {10.1016/j.neunet.2022.05.005},
  journal      = {Neural Networks},
  pages        = {347-352},
  shortjournal = {Neural Netw.},
  title        = {Robust kernel principal component analysis with optimal mean},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ExSpliNet: An interpretable and expressive spline-based
neural network. <em>NN</em>, <em>152</em>, 332–346. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present ExSpliNet, an interpretable and expressive neural network model. The model combines ideas of Kolmogorov neural networks, ensembles of probabilistic trees, and multivariate B-spline representations. We give a probabilistic interpretation of the model and show its universal approximation properties. We also discuss how it can be efficiently encoded by exploiting B-spline properties. Finally, we test the effectiveness of the proposed model on synthetic approximation problems and classical machine learning benchmark datasets.},
  archive      = {J_NN},
  author       = {Daniele Fakhoury and Emanuele Fakhoury and Hendrik Speleers},
  doi          = {10.1016/j.neunet.2022.04.029},
  journal      = {Neural Networks},
  pages        = {332-346},
  shortjournal = {Neural Netw.},
  title        = {ExSpliNet: An interpretable and expressive spline-based neural network},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embedding graphs on grassmann manifold. <em>NN</em>,
<em>152</em>, 322–331. (<a
href="https://doi.org/10.1016/j.neunet.2022.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning efficient graph representation is the key to favorably addressing downstream tasks on graphs, such as node or graph property prediction. Given the non-Euclidean structural property of graphs, preserving the original graph data’s similarity relationship in the embedded space needs specific tools and a similarity metric. This paper develops a new graph representation learning scheme, namely Egg , which embeds approximated second-order graph characteristics into a Grassmann manifold. The proposed strategy leverages graph convolutions to learn hidden representations of the corresponding subspace of the graph, which is then mapped to a Grassmann point of a low dimensional manifold through truncated singular value decomposition (SVD). The established graph embedding approximates denoised correlationship of node attributes, as implemented in the form of a symmetric matrix space for Euclidean calculation. The effectiveness of Egg is demonstrated using both clustering and classification tasks at the node level and graph level. It outperforms baseline models on various benchmarks.},
  archive      = {J_NN},
  author       = {Bingxin Zhou and Xuebin Zheng and Yu Guang Wang and Ming Li and Junbin Gao},
  doi          = {10.1016/j.neunet.2022.05.001},
  journal      = {Neural Networks},
  pages        = {322-331},
  shortjournal = {Neural Netw.},
  title        = {Embedding graphs on grassmann manifold},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sediment prediction in the great barrier reef using vision
transformer with finite element analysis. <em>NN</em>, <em>152</em>,
311–321. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suspended sediment is a significant threat to the Great Barrier Reef (GBR) ecosystem. This catchment pollutant stems primarily from terrestrial soil erosion. Bulk masses of sediments have potential to propagate from river plumes into the mid-shelf and outer-shelf regions. Existing sediment forecasting methods suffer from the problem of low-resolution predictions, making them unsuitable for wide area coverage. In this paper, a novel sediment distribution prediction model is proposed to augment existing water quality management programs for the GBR. This model is based on the state-of-the-art Transformer network in conjunction with the well-known finite element analysis . For model training, the emerging physics-informed neural network is employed to incorporate both simulated and measured sediment data. Our proposed Finite Element Transformer (FE-Transformer) model offers accurate predictions of sediment across the entire GBR. It provides unblurred outputs, which cannot be achieved with previous next-frame prediction models. This paves a way for accurate forecasting of sediment, which in turn may lead to improved water quality management for the GBR.},
  archive      = {J_NN},
  author       = {Mohammad Jahanbakht and Wei Xiang and Mostafa Rahimi Azghadi},
  doi          = {10.1016/j.neunet.2022.04.022},
  journal      = {Neural Networks},
  pages        = {311-321},
  shortjournal = {Neural Netw.},
  title        = {Sediment prediction in the great barrier reef using vision transformer with finite element analysis},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate time series forecasting method based on
nonlinear spiking neural p systems and non-subsampled shearlet
transform. <em>NN</em>, <em>152</em>, 300–310. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting remains a challenging task because of its nonlinear, non-stationary, high-dimensional, and spatial–temporal characteristics, along with the dependence between variables. To address this limitation, we propose a novel method for multivariate time series forecasting based on nonlinear spiking neural P (NSNP) systems and non-subsampled shearlet transform (NSST). A multivariate time series is first converted into the NSST domain, and then NSNP systems are automatically constructed, trained, and predicted in the NSST domain. Because NSNP systems are used as nonlinear prediction models and work in the NSST domain, the proposed prediction method is essentially a multiscale transform (MST)–based prediction method. Therefore, the proposed prediction method can process nonlinear and non-stationary time series, and the dependence between variables can be characterized by the multiresolution features of the NSST transform. Five real-life multivariate time series were used to compare the proposed prediction method with five state-of-the-art and 28 baseline prediction methods. The comparison results demonstrate the effectiveness of the proposed method for multivariate time-series forecasting.},
  archive      = {J_NN},
  author       = {Lifan Long and Qian Liu and Hong Peng and Jun Wang and Qian Yang},
  doi          = {10.1016/j.neunet.2022.04.030},
  journal      = {Neural Networks},
  pages        = {300-310},
  shortjournal = {Neural Netw.},
  title        = {Multivariate time series forecasting method based on nonlinear spiking neural p systems and non-subsampled shearlet transform},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware dynamic neural computational models for
accurate poly(a) signal prediction. <em>NN</em>, <em>152</em>, 287–299.
(<a href="https://doi.org/10.1016/j.neunet.2022.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting Polyadenylation (Poly(A)) signals isthe key to understand the mechanism of translation regulation and mRNA metabolism. However, existing computational algorithms fail to work well for predicting Poly(A) signals due to the vanishing gradient problem when simply increasing the number of layers. In this work, we devise a spatiotemporal context-aware neural model called ACNet for Poly(A) signal prediction based on co-occurrence embedding. Specifically, genomic sequences of Poly(A) signals are first split into k -mer sequences, and k -mer embeddings are pre-trained based on the co-occurrence matrix information; Then, gated residual networks are devised to fully extract spatial information, which has an excellent ability to control the information flow and ease the problem of vanishing gradients. The gated mechanism generates channel weights by a dilated convolution and aggregates local features by identity connections which are obtained by multi-scale dilated convolutions. Experimental results indicate that our ACNet model outperforms the state-of-the-art prediction methods on various Poly(A) signal data, and an ablation study shows the effectiveness of the design strategy.},
  archive      = {J_NN},
  author       = {Yanbu Guo and Chaoyang Li and Dongming Zhou and Jinde Cao and Hui Liang},
  doi          = {10.1016/j.neunet.2022.04.025},
  journal      = {Neural Networks},
  pages        = {287-299},
  shortjournal = {Neural Netw.},
  title        = {Context-aware dynamic neural computational models for accurate Poly(A) signal prediction},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level landmark-guided deep network for face
super-resolution. <em>NN</em>, <em>152</em>, 276–286. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years deep learning-based methods incorporating facial prior knowledge for face super-resolution (FSR) are advancing and have gained impressive performance. However, some important priors such as facial landmarks are not fully exploited in existing methods, leading to noticeable artifacts in the resultant SR face images especially under large magnification. In this paper, we propose a novel multi-level landmark-guided deep network (MLGDN) for FSR. More specifically, to fully exploit the dependencies between low and high resolution images and to reduce network parameters as well as capture more reliable feature representation, we introduce a recursive back-projection network with a particular feedback mechanism for coarse-to-fine FSR. Furthermore, we incorporate an attention fusion module in the front of backbone network to strengthen face components and a feature modulation module to refine features in the middle of backbone network . By this way, the facial landmarks extracted from face images can be fully shared by the modules in different levels, which benefit to produce more faithful facial details. Both quantitative and qualitative performance evaluations on two benchmark databases demonstrate that the proposed MLGDN can achieve more impressive SR results than other state-of-the-art competitors. Code will be available at https://github.com/zhuangcheng31/MLG_Face.git/},
  archive      = {J_NN},
  author       = {Cheng Zhuang and Minqi Li and Kaibing Zhang and Zheng Li and Jian Lu},
  doi          = {10.1016/j.neunet.2022.04.026},
  journal      = {Neural Networks},
  pages        = {276-286},
  shortjournal = {Neural Netw.},
  title        = {Multi-level landmark-guided deep network for face super-resolution},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning, reinforcement learning, and world models.
<em>NN</em>, <em>152</em>, 267–275. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the “Deep Learning and Reinforcement Learning” session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.},
  archive      = {J_NN},
  author       = {Yutaka Matsuo and Yann LeCun and Maneesh Sahani and Doina Precup and David Silver and Masashi Sugiyama and Eiji Uchibe and Jun Morimoto},
  doi          = {10.1016/j.neunet.2022.03.037},
  journal      = {Neural Networks},
  pages        = {267-275},
  shortjournal = {Neural Netw.},
  title        = {Deep learning, reinforcement learning, and world models},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep networks may capture biological behavior for shallow,
but not deep, empirical characterizations. <em>NN</em>, <em>152</em>,
244–266. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We assess whether deep convolutional networks (DCN) can account for a most fundamental property of human vision: detection/discrimination of elementary image elements (bars) at different contrast levels. The human visual process can be characterized to varying degrees of “depth,” ranging from percentage of correct detection to detailed tuning and operating characteristics of the underlying perceptual mechanism. We challenge deep networks with the same stimuli/tasks used with human observers and apply equivalent characterization of the stimulus–response coupling. In general, we find that popular DCN architectures do not account for signature properties of the human process. For shallow depth of characterization, some variants of network-architecture/training-protocol produce human-like trends; however, more articulate empirical descriptors expose glaring discrepancies. Networks can be coaxed into learning those richer descriptors by shadowing a human surrogate in the form of a tailored circuit perturbed by unstructured input, thus ruling out the possibility that human–model misalignment in standard protocols may be attributable to insufficient representational power. These results urge caution in assessing whether neural networks do or do not capture human behavior : ultimately, our ability to assess “success” in this area can only be as good as afforded by the depth of behavioral characterization against which the network is evaluated. We propose a novel set of metrics/protocols that impose stringent constraints on the evaluation of DCN behavior as an adequate approximation to biological processes.},
  archive      = {J_NN},
  author       = {Peter Neri},
  doi          = {10.1016/j.neunet.2022.04.023},
  journal      = {Neural Networks},
  pages        = {244-266},
  shortjournal = {Neural Netw.},
  title        = {Deep networks may capture biological behavior for shallow, but not deep, empirical characterizations},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Organization of a latent space structure in VAE/GAN trained
by navigation data. <em>NN</em>, <em>152</em>, 234–243. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel artificial cognitive mapping system using generative deep neural networks , called variational autoencoder/generative adversarial network (VAE/GAN), which can map input images to latent vectors and generate temporal sequences internally. The results show that the distance of the predicted image is reflected in the distance of the corresponding latent vector after training. This indicates that the latent space is self-organized to reflect the proximity structure of the dataset and may provide a mechanism through which many aspects of cognition are spatially represented. The present study allows the network to internally generate temporal sequences that are analogous to the hippocampal replay/pre-play ability, where VAE produces only near-accurate replays of past experiences, but by introducing GANs, the generated sequences are coupled with instability and novelty.},
  archive      = {J_NN},
  author       = {Hiroki Kojima and Takashi Ikegami},
  doi          = {10.1016/j.neunet.2022.04.012},
  journal      = {Neural Networks},
  pages        = {234-243},
  shortjournal = {Neural Netw.},
  title        = {Organization of a latent space structure in VAE/GAN trained by navigation data},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attributed graph clustering with multi-task embedding
learning. <em>NN</em>, <em>152</em>, 224–233. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering is challenging as it needs to effectively combine both graph structure and node feature information to accomplish node clustering. Recent studies mostly adopt graph neural networks to learn node embeddings , then apply traditional clustering methods to obtain clusters. However, their node embeddings are not specifically designed for clustering. Moreover, most of their loss functions only rely on either structure or feature information, making both kinds of information not fully retained in node embeddings. In this paper, we propose a multi-task embedding learning method (MTEL) for attributed graph clustering , which constructs two prediction tasks in terms of structure and feature based adjacency matrices respectively. To make the node embeddings helpful for the downstream clustering, in each task, we predict the minimum hop number between each pair of nodes in the adjacency matrix, so that the correlation degrees among nodes can be encoded into node embeddings. To improve the performance of the prediction task, we regularize the model parameters in these two tasks via ℓ 2 , 1 ℓ2,1 norm, through which the model parameters can be jointly learned. Experiments on real attributed graphs show that MTEL is superior for attributed graph clustering over state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Xiaotong Zhang and Han Liu and Xianchao Zhang and Xinyue Liu},
  doi          = {10.1016/j.neunet.2022.04.018},
  journal      = {Neural Networks},
  pages        = {224-233},
  shortjournal = {Neural Netw.},
  title        = {Attributed graph clustering with multi-task embedding learning},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered integral reinforcement learning for
nonzero-sum games with asymmetric input saturation. <em>NN</em>,
<em>152</em>, 212–223. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an event-triggered integral reinforcement learning (IRL) algorithm is developed for the nonzero-sum game problem with asymmetric input saturation. First, for each player, a novel non-quadratic value function with a discount factor is designed, and the coupled Hamilton–Jacobi equation that does not require a complete knowledge of the game is derived by using the idea of IRL. Second, the execution of each player is based on the event-triggered mechanism. In the implementation, an adaptive dynamic programming based learning scheme using a single critic neural network (NN) is developed. Experience replay technique is introduced into the classical gradient descent method to tune the weights of the critic NN. The stability of the system and the elimination of Zeno behavior are proved. Finally, simulation experiments verify the effectiveness of the event-triggered IRL algorithm.},
  archive      = {J_NN},
  author       = {Shan Xue and Biao Luo and Derong Liu and Ying Gao},
  doi          = {10.1016/j.neunet.2022.04.013},
  journal      = {Neural Networks},
  pages        = {212-223},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered integral reinforcement learning for nonzero-sum games with asymmetric input saturation},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-linear perceptual multi-scale network for single image
super-resolution. <em>NN</em>, <em>152</em>, 201–211. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and achieved remarkable progress. However, most of the existing CNN-based SISR networks with a single-stream structure fail to make full use of the multi-scale features of low-resolution (LR) image. While those multi-scale SR models often integrate the information with different receptive fields by means of linear fusion, which leads to the redundant feature extraction and hinders the reconstruction performance of the network. To address both issues, in this paper, we propose a non-linear perceptual multi-scale network (NLPMSNet) to fuse the multi-scale image information in a non-linear manner. Specifically, a novel non-linear perceptual multi-scale module (NLPMSM) is developed to learn more discriminative multi-scale feature correlation by using high-order channel attention mechanism , so as to adaptively extract image features at different scales. Besides, we present a multi-cascade residual nested group (MC-RNG) structure, which uses a global multi-cascade mechanism to organize multiple local residual nested groups (LRNG) to capture sufficient non-local hierarchical context information for reconstructing high-frequency details. LRNG uses a local residual nesting mechanism to stack NLPMSMs, which aims to form a more effective residual learning mechanism and obtain more representative local features . Experimental results show that, compared with the state-of-the-art SISR methods, the proposed NLPMSNet performs well in both quantitative metrics and visual quality with a small number of parameters.},
  archive      = {J_NN},
  author       = {Aiping Yang and Leilei Li and Jinbin Wang and Zhong Ji and Yanwei Pang and Jiale Cao and Zihao Wei},
  doi          = {10.1016/j.neunet.2022.04.020},
  journal      = {Neural Networks},
  pages        = {201-211},
  shortjournal = {Neural Netw.},
  title        = {Non-linear perceptual multi-scale network for single image super-resolution},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distantly supervised relation extraction via recursive
hierarchy-interactive attention and entity-order perception.
<em>NN</em>, <em>152</em>, 191–200. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrong-labeling problem and long-tail relations severely affect the performance of distantly supervised relation extraction task. Many studies mitigate the effect of wrong-labeling through selective attention mechanism and handle long-tail relations by introducing relation hierarchies to share knowledge. However, almost all existing studies ignore the fact that, in a sentence, the appearance order of two entities contributes to the understanding of its semantics. Furthermore, they only utilize each relation level of relation hierarchies separately, but do not exploit the heuristic effect between relation levels, i.e., higher-level relations can give useful information to the lower ones. Based on the above, in this paper, we design a novel Recursive Hierarchy-Interactive Attention network (RHIA) to further handle long-tail relations, which models the heuristic effect between relation levels. From the top down, it passes relation-related information layer by layer, which is the most significant difference from existing models, and generates relation-augmented sentence representations for each relation level in a recursive structure . Besides, we introduce a newfangled training objective, called Entity-Order Perception (EOP), to make the sentence encoder retain more entity appearance information. Substantial experiments on the popular New York Times (NYT) dataset are conducted. Compared to prior baselines, our RHIA-EOP achieves state-of-the-art performance in terms of precision–recall (P–R) curves, AUC, Top-N precision and other evaluation metrics . Insightful analysis also demonstrates the necessity and effectiveness of each component of RHIA-EOP.},
  archive      = {J_NN},
  author       = {Ridong Han and Tao Peng and Jiayu Han and Hai Cui and Lu Liu},
  doi          = {10.1016/j.neunet.2022.04.019},
  journal      = {Neural Networks},
  pages        = {191-200},
  shortjournal = {Neural Netw.},
  title        = {Distantly supervised relation extraction via recursive hierarchy-interactive attention and entity-order perception},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new deep learning framework based on blood pressure range
constraint for continuous cuffless BP estimation. <em>NN</em>,
<em>152</em>, 181–190. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood pressure (BP) is known as an indicator of human health status, and regular measurement is helpful for early detection of cardiovascular diseases. Traditional techniques for measuring BP are either invasive or cuff-based and thus are not suitable for continuous measurement. Aiming at the deficiencies in existing studies, a novel cuffless BP estimation framework of Receptive Field Parallel Attention Shrinkage Network (RFPASN) and BP range constraint is proposed. Firstly, RFPASN uses the multi-scale large receptive field convolution module to capture the long-term dynamics in the photoplethysmography (PPG) signal without using long short-term memory (LSTM). On this basis, the features acquired by the parallel mixed domain attention module are used as thresholds, and the soft threshold function is used to screen the input features to enhance the discriminability and robustness of features, which can significantly improve the prediction accuracy of diastolic blood pressure (DBP) and systolic blood pressure (SBP). Finally, in order to prevent large fluctuations in the prediction results of RFPASN, RFPASN based on BP range constraint is proposed to make the prediction results of RFPASN more accurate and reasonable. The performance of the proposed method is demonstrated on a publically available MIMIC-II database. The database contains normal, hypertensive and hypotensive people. We have achieved MAE of 1.63/1.59 (DBP) and 2.26/2.15 (SBP) mmHg for BP on total population of 1562 subjects. A comparative study shows that the proposed algorithm is more promising than the state-of-the-art.},
  archive      = {J_NN},
  author       = {Yongyi Chen and Dan Zhang and Hamid Reza Karimi and Chao Deng and Wutao Yin},
  doi          = {10.1016/j.neunet.2022.04.017},
  journal      = {Neural Networks},
  pages        = {181-190},
  shortjournal = {Neural Netw.},
  title        = {A new deep learning framework based on blood pressure range constraint for continuous cuffless BP estimation},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimistic reinforcement learning by forward
kullback–leibler divergence optimization. <em>NN</em>, <em>152</em>,
169–180. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a new interpretation of the traditional optimization method in reinforcement learning (RL) as optimization problems using reverse Kullback–Leibler (KL) divergence, and derives a new optimization method using forward KL divergence, instead of reverse KL divergence in the optimization problems . Although RL originally aims to maximize return indirectly through optimization of policy, the recent work by Levine has proposed a different derivation process with explicit consideration of optimality as stochastic variable . This paper follows this concept and formulates the traditional learning laws for both value function and policy as the optimization problems with reverse KL divergence including optimality . Focusing on the asymmetry of KL divergence, the new optimization problems with forward KL divergence are derived. Remarkably, such new optimization problems can be regarded as optimistic RL. That optimism is intuitively specified by a hyperparameter converted from an uncertainty parameter. In addition, it can be enhanced when it is integrated with prioritized experience replay and eligibility traces, both of which accelerate learning. The effects of this expected optimism was investigated through learning tendencies on numerical simulations using Pybullet. As a result, moderate optimism accelerated learning and yielded higher rewards. In a realistic robotic simulation, the proposed method with the moderate optimism outperformed one of the state-of-the-art RL method.},
  archive      = {J_NN},
  author       = {Taisuke Kobayashi},
  doi          = {10.1016/j.neunet.2022.04.021},
  journal      = {Neural Networks},
  pages        = {169-180},
  shortjournal = {Neural Netw.},
  title        = {Optimistic reinforcement learning by forward Kullback–Leibler divergence optimization},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse factorization of square matrices with application to
neural attention modeling. <em>NN</em>, <em>152</em>, 160–168. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Square matrices appear in many machine learning problems and models. Optimization over a large square matrix is expensive in memory and in time. Therefore an economic approximation is needed. Conventional approximation approaches factorize the square matrix into a number matrices of much lower ranks. However, the low-rank constraint is a performance bottleneck if the approximated matrix is intrinsically high-rank or close to full rank. In this paper, we propose to approximate a large square matrix with a product of sparse full-rank matrices. In the approximation, our method needs only N(logN)2 non-zero numbers for an N×N full matrix. Our new method is especially useful for scalable neural attention modeling. Different from the conventional scaled dot-product attention methods, we train neural networks to map input data to the non-zero entries of the factorizing matrices. The sparse factorization method is tested for various square matrices, and the experimental results demonstrate that our method gives a better approximation when the approximated matrix is sparse and high-rank. As an attention module, our new method defeats Transformer and its several variants for long sequences in synthetic data sets and in the Long Range Arena benchmarks. Our code is publicly available 2 .},
  archive      = {J_NN},
  author       = {Ruslan Khalitov and Tong Yu and Lei Cheng and Zhirong Yang},
  doi          = {10.1016/j.neunet.2022.04.014},
  journal      = {Neural Networks},
  pages        = {160-168},
  shortjournal = {Neural Netw.},
  title        = {Sparse factorization of square matrices with application to neural attention modeling},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal CNN with pyramid bottleneck blocks:
Application to eye blinking detection. <em>NN</em>, <em>152</em>,
150–159. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye blink detection is a challenging problem that many researchers are working on because it has the potential to solve many facial analysis tasks, such as face anti-spoofing, driver drowsiness detection , and some health disorders. There have been few attempts to detect blinking in the wild scenario, while most of the work has been done under controlled conditions. Moreover, current learning approaches are designed to process sequences that contain only a single blink ignoring the case of the presence of multiple eye blinks. In this work, we propose a fast framework for eye blink detection and eye blink verification that can effectively extract multiple blinks from image sequences considering several challenges such as lighting changes, variety of poses, and change in appearance. The proposed framework employs fast landmarks detector to extract multiple facial key points including the ones that identify the eye regions. Then, an SVD-based method is proposed to extract the potential eye blinks in a moving time window that is updated with new images every second. Finally, the detected blink candidates are verified using a 2D Pyramidal Bottleneck Block Network (PBBN). We also propose an alternative approach that uses a sequence of frames instead of an image as input and employs a continuous 3D PBBN that follows most of the state-of-the-art approaches schemes. Experimental results show the better performance of the proposed approach compared to the state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {S.E. Bekhouche and I. Kajo and Y. Ruichek and F. Dornaika},
  doi          = {10.1016/j.neunet.2022.04.010},
  journal      = {Neural Networks},
  pages        = {150-159},
  shortjournal = {Neural Netw.},
  title        = {Spatiotemporal CNN with pyramid bottleneck blocks: Application to eye blinking detection},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A dynamical neural network approach for solving stochastic
two-player zero-sum games. <em>NN</em>, <em>152</em>, 140–149. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at solving a stochastic two-player zero-sum Nash game problem studied in Singh and Lisser (2019). The main contribution of our paper is that we model this game problem as a dynamical neural network (DNN for short). In this paper, we show that the saddle point of this game problem is the equilibrium point of the DNN model, and we study the globally asymptotically stable of the DNN model. In our numerical experiments, we present the time-continuous feature of the DNN model and compare it with the state-of-the-art convex solvers, i.e., Splitting conic solver (SCS for short) and Cvxopt. Our numerical results show that our DNN method has two advantages in dealing with this game problem. Firstly, the DNN model can converge to a better optimal point. Secondly, the DNN method can solve all problems, even when the problem size is large.},
  archive      = {J_NN},
  author       = {Dawen Wu and Abdel Lisser},
  doi          = {10.1016/j.neunet.2022.04.006},
  journal      = {Neural Networks},
  pages        = {140-149},
  shortjournal = {Neural Netw.},
  title        = {A dynamical neural network approach for solving stochastic two-player zero-sum games},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DynamicNet: A time-variant ODE network for multi-step wind
speed prediction. <em>NN</em>, <em>152</em>, 118–139. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind power is a new type of green energy. Though it is economical to access and gather such energy, effectively matching the energy with consumers’ demand is difficult, because of the fluctuate, intermittent and chaotic nature of wind speed. Hence, multi-step wind speed prediction becomes an important research topic. In this paper, we propose a novel deep learning method, DyanmicNet, for the problem. DynamicNet follows an encoder–decoder framework. To capture the fluctuate, intermittent and chaotic nature of wind speed, it leverages a time-variant structure to build the decoder, which is different from conventional encoder–decoder methods. In addition, a new neural block (ST-GRU-ODE) is developed, which can model the wind speed in a continuous manner by using the neural ordinary differential equation (ODE). To enhance the prediction performance, a multi-step training procedure is also put forward. Comprehensive experiments have been conducted on two real-world datasets, where wind speed is recorded in the form of two orthogonal components namely U-Wind and V-Wind. Each component can be illustrated as wind speed images. Experimental results demonstrate the effectiveness and superiority of the proposed method over state-of-the-art techniques.},
  archive      = {J_NN},
  author       = {Rui Ye and Xutao Li and Yunming Ye and Baoquan Zhang},
  doi          = {10.1016/j.neunet.2022.04.004},
  journal      = {Neural Networks},
  pages        = {118-139},
  shortjournal = {Neural Netw.},
  title        = {DynamicNet: A time-variant ODE network for multi-step wind speed prediction},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum pulse coupled neural network. <em>NN</em>,
<em>152</em>, 105–117. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural network has been fully developed in recent years, but as the size of the network grows, the required computing power also grows rapidly. In order to take advantage of the parallel computing of quantum computing to solve the difficulties of large computation in neural network , quantum neural network was proposed. In this paper, based on the pulse coupled neural network (PCNN), quantum pulse coupled neural network (QPCNN) is proposed. In this model, the basic quantum logic gates are utilized to form quantum operation modules, such as quantum full adder, quantum multiplier, and quantum comparator . A quantum image convolution operation applicable to QPCNN is designed employing quantum full adders and neighborhood preparation module. And these modules are employed to complete the operations required for QPCNN. And based on QPCNN, an quantum image segmentation is designed. Meanwhile, the effectiveness of QPCNN is proved by simulation experiments, and the complexity analysis shows that QPCNN has exponential speedup compared with classical PCNN.},
  archive      = {J_NN},
  author       = {Zhaobin Wang and Minzhe Xu and Yaonan Zhang},
  doi          = {10.1016/j.neunet.2022.04.007},
  journal      = {Neural Networks},
  pages        = {105-117},
  shortjournal = {Neural Netw.},
  title        = {Quantum pulse coupled neural network},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering diverse solutions in deep reinforcement learning
by maximizing state–action-based mutual information. <em>NN</em>,
<em>152</em>, 90–104. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning algorithms are typically limited to learning a single solution for a specified task, even though diverse solutions often exist. Recent studies showed that learning a set of diverse solutions is beneficial because diversity enables robust few-shot adaptation. Although existing methods learn diverse solutions by using the mutual information as unsupervised rewards, such an approach often suffers from the bias of the gradient estimator induced by value function approximation. In this study, we propose a novel method that can learn diverse solutions without suffering the bias problem. In our method, a policy conditioned on a continuous or discrete latent variable is trained by directly maximizing the variational lower bound of the mutual information, instead of using the mutual information as unsupervised rewards as in previous studies. Through extensive experiments on robot locomotion tasks, we demonstrate that the proposed method successfully learns an infinite set of diverse solutions by learning continuous latent variables, which is more challenging than learning a finite number of solutions. Subsequently, we show that our method enables more effective few-shot adaptation compared with existing methods.},
  archive      = {J_NN},
  author       = {Takayuki Osa and Voot Tangkaratt and Masashi Sugiyama},
  doi          = {10.1016/j.neunet.2022.04.009},
  journal      = {Neural Networks},
  pages        = {90-104},
  shortjournal = {Neural Netw.},
  title        = {Discovering diverse solutions in deep reinforcement learning by maximizing state–action-based mutual information},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistability analysis of delayed recurrent neural networks
with a class of piecewise nonlinear activation functions. <em>NN</em>,
<em>152</em>, 80–89. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the multistability of delayed recurrent neural networks (DRNNs) with a class of piecewise nonlinear activation functions . The coexistence as well as the stability of multiple equilibrium points (EPs) of DRNNs are proved. With the Brouwer’s fixed point theorem as well as the Lagrange mean value theorem , it is obtained that under some conditions, the n -neuron DRNNs with the proposed activation function can have at least 5 n EPs and 3 n of them are locally stable. Compared with the DRNNs with sigmoidal activation functions , DRNNs with this kind of activation function can have more total EPs and more locally stable EPs. It implies that when designing DRNNs with the proposed activation function to apply in associative memory , it can have an even larger storage capacity. Furthermore, it is obtained that there exists a relationship between the number of the total EPs/stable EPs and the frequency of the sinusoidal function in the proposed activation function. Last, the above obtained results are extended to a more general case. It is shown that, DRNNs with the extended activation function can have ( 2 k + 1 ) n EPs, ( k + 1 ) n of which are locally stable, therein k is closely related to the frequency of the sinusoidal function in the extended activation function. Two simulation examples are given to verify the correctness of the theoretical results.},
  archive      = {J_NN},
  author       = {Yang Liu and Zhen Wang and Qian Ma and Hao Shen},
  doi          = {10.1016/j.neunet.2022.04.015},
  journal      = {Neural Networks},
  pages        = {80-89},
  shortjournal = {Neural Netw.},
  title        = {Multistability analysis of delayed recurrent neural networks with a class of piecewise nonlinear activation functions},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context meta-reinforcement learning via neuromodulation.
<em>NN</em>, <em>152</em>, 70–79. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-reinforcement learning (meta-RL) algorithms enable agents to adapt quickly to tasks from few samples in dynamic environments. Such a feat is achieved through dynamic representations in an agent’s policy network (obtained via reasoning about task context, model parameter updates, or both). However, obtaining rich dynamic representations for fast adaptation beyond simple benchmark problems is challenging due to the burden placed on the policy network to accommodate different policies. This paper addresses the challenge by introducing neuromodulation as a modular component to augment a standard policy network that regulates neuronal activities in order to produce efficient dynamic representations for task adaptation. The proposed extension to the policy network is evaluated across multiple discrete and continuous control environments of increasing complexity. To prove the generality and benefits of the extension in meta-RL, the neuromodulated network was applied to two state-of-the-art meta-RL algorithms (CAVIA and PEARL). The result demonstrates that meta-RL augmented with neuromodulation produces significantly better result and richer dynamic representations in comparison to the baselines.},
  archive      = {J_NN},
  author       = {Eseoghene Ben-Iwhiwhu and Jeffery Dick and Nicholas A. Ketz and Praveen K. Pilly and Andrea Soltoggio},
  doi          = {10.1016/j.neunet.2022.04.003},
  journal      = {Neural Networks},
  pages        = {70-79},
  shortjournal = {Neural Netw.},
  title        = {Context meta-reinforcement learning via neuromodulation},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tri-view two-photon microscopic image registration and
deblurring with convolutional neural networks. <em>NN</em>,
<em>152</em>, 57–69. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-photon fluorescence microscopy has enabled the three-dimensional (3D) neural imaging of deep cortical regions . While it can capture the detailed neural structures in the x–y image space, the image quality along the depth direction is lower because of lens blur, which often makes it difficult to identify the neural connectivity. To address this problem, we propose a novel approach for restoring the isotropic image volume by estimating and fusing the intersection regions of the images captured from three orthogonal viewpoints using convolutional neural networks (CNNs). Because convolution on 3D images is computationally complex, the proposed method takes the form of cascaded CNN models consisting of rigid transformation, dense registration, and deblurring networks for more efficient processing. In addition, to enable self-supervised learning, we trained the CNN models with simulated synthetic images by considering the distortions of the microscopic imaging process . Through extensive experiments, the proposed method achieved substantial image quality improvements.},
  archive      = {J_NN},
  author       = {Sehyung Lee and Hideaki Kume and Hidetoshi Urakubo and Haruo Kasai and Shin Ishii},
  doi          = {10.1016/j.neunet.2022.04.011},
  journal      = {Neural Networks},
  pages        = {57-69},
  shortjournal = {Neural Netw.},
  title        = {Tri-view two-photon microscopic image registration and deblurring with convolutional neural networks},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial neural networks with conformable transfer
function for improving the performance in thermal and environmental
processes. <em>NN</em>, <em>152</em>, 44–56. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes a novel transfer function based on the hyperbolic tangent and the Khalil conformable exponential function. The non-integer order transfer function offers a suitable neural network configuration because of its ability to adapt. Consequently, this function was introduced into neural network models for three experimental cases: estimating the annular Nusselt number correlation to a helical double-pipe evaporator , the volumetric mass transfer coefficient in an electrochemical reaction, and the thermal efficiency of a solar parabolic trough collector . We found the new transfer function parameters during the training step of the neural networks. Therefore, weights and biases depend on them. We assessed the models applied to the three cases using the determination coefficient , adjusted determination coefficient , and the slope-intercept test. In addition, the MSE for the training set and the whole database were computed to show that there is no overfitting problem. The best-assessed models showed a relationship of 99\%, 97\%, and 95\% with the experimental data for the first, second, and third cases. This novel proposal made reducing the number of neurons in the hidden layer feasible. Therefore, we show a neural network with a conformable transfer function (ANN-CTF) that learns well enough with less available information from the experimental database during its training.},
  archive      = {J_NN},
  author       = {J.E. Solís-Pérez and J.A. Hernández and A. Parrales and J.F. Gómez-Aguilar and A. Huicochea},
  doi          = {10.1016/j.neunet.2022.04.016},
  journal      = {Neural Networks},
  pages        = {44-56},
  shortjournal = {Neural Netw.},
  title        = {Artificial neural networks with conformable transfer function for improving the performance in thermal and environmental processes},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep unsupervised feature selection by discarding nuisance
and correlated features. <em>NN</em>, <em>152</em>, 34–43. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern datasets often contain large subsets of correlated features and nuisance features, which are not or loosely related to the main underlying structures of the data. Nuisance features can be identified using the Laplacian score criterion, which evaluates the importance of a given feature via its consistency with the Graph Laplacians’ leading eigenvectors . We demonstrate that in the presence of large numbers of nuisance features, the Laplacian must be computed on the subset of selected features rather than on the complete feature set. To do this, we propose a fully differentiable approach for unsupervised feature selection, utilizing the Laplacian score criterion to avoid the selection of nuisance features. We employ an autoencoder architecture to cope with correlated features, trained to reconstruct the data from the subset of selected features. Building on the recently proposed concrete layer that allows controlling for the number of selected features via architectural design , simplifying the optimization process. Experimenting on several real-world datasets, we demonstrate that our proposed approach outperforms similar approaches designed to avoid only correlated or nuisance features, but not both. Several state-of-the-art clustering results are reported. Our code is publically available at https://github.com/jsvir/lscae .},
  archive      = {J_NN},
  author       = {Uri Shaham and Ofir Lindenbaum and Jonathan Svirsky and Yuval Kluger},
  doi          = {10.1016/j.neunet.2022.04.002},
  journal      = {Neural Networks},
  pages        = {34-43},
  shortjournal = {Neural Netw.},
  title        = {Deep unsupervised feature selection by discarding nuisance and correlated features},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-based implementation of COLREGs for autonomous surface
vehicles using deep reinforcement learning. <em>NN</em>, <em>152</em>,
17–33. (<a href="https://doi.org/10.1016/j.neunet.2022.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous systems are becoming ubiquitous and gaining momentum within the marine sector. Since the electrification of transport is happening simultaneously, autonomous marine vessels can reduce environmental impact, lower costs, and increase efficiency. Although close monitoring is still required to ensure safety, the ultimate goal is full autonomy. One major milestone is to develop a control system that is versatile enough to handle any weather and encounter that is also robust and reliable. Additionally, the control system must adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) for successful interaction with human sailors. Since the COLREGs were written for the human mind to interpret, they are written in ambiguous prose and therefore not machine-readable or verifiable. Due to these challenges and the wide variety of situations to be tackled, classical model-based approaches prove complicated to implement and computationally heavy. Within machine learning (ML), deep reinforcement learning (DRL) has shown great potential for a wide range of applications. The model-free and self-learning properties of DRL make it a promising candidate for autonomous vessels. In this work, a subset of the COLREGs is incorporated into a DRL-based path following and obstacle avoidance system using collision risk theory. The resulting autonomous agent dynamically interpolates between path following and COLREG-compliant collision avoidance in the training scenario, isolated encounter situations, and AIS-based simulations of real-world scenarios.},
  archive      = {J_NN},
  author       = {Amalie Heiberg and Thomas Nakken Larsen and Eivind Meyer and Adil Rasheed and Omer San and Damiano Varagnolo},
  doi          = {10.1016/j.neunet.2022.04.008},
  journal      = {Neural Networks},
  pages        = {17-33},
  shortjournal = {Neural Netw.},
  title        = {Risk-based implementation of COLREGs for autonomous surface vehicles using deep reinforcement learning},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic image clustering from projected coordinates of deep
similarity learning. <em>NN</em>, <em>152</em>, 1–16. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonly used clustering algorithms typically require user parameters such as the number of clusters to be divided. Density-based algorithms do not have such requirements but are not suitable for high dimensional data . Recent studies have merged the cluster assignment task with deep similarity learning. In this paper, we propose a novel framework to perform dynamic image clustering without prior knowledge of the cluster count. A deep learning model first learns data similarity from scratch, followed by the use of a coordinate learning model to project high dimensional data onto a two-dimensional space. A new clustering algorithm, raster clustering, is proposed to evaluate and classify the projected data. This mechanism can be applied in high dimensional data clustering like image data, and it allows the prediction of unseen data in a consistent way without the need for consolidating with training data.},
  archive      = {J_NN},
  author       = {Jui-Hung Chang and Yin-Chung Leung},
  doi          = {10.1016/j.neunet.2022.03.030},
  journal      = {Neural Networks},
  pages        = {1-16},
  shortjournal = {Neural Netw.},
  title        = {Dynamic image clustering from projected coordinates of deep similarity learning},
  volume       = {152},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022g). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>151</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00169-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00169-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Current events. <em>NN</em>, <em>151</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00168-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00168-X},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed k-winners-take-all via multiple neural networks
with inertia. <em>NN</em>, <em>151</em>, 385–397. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to solving the k -winners-take-all problem with large-scale input signals in a distributed manner. According to the decomposition of global input signals, a novel dynamical system consisting of multiple coordinated neural networks is proposed for finding the k largest inputs. In the system, each neural network is designed to tackle its available partial inputs only for a local objective ki ( ki≤k ). Simultaneously, a consensus-based approach is adopted to coordinate multiple neural networks for achieving the global objective k . In addition, an inertial term is introduced in each neural network for regulating its transient behavior , which has the potential of accelerating the convergence. By developing a cocoercive operator, we theoretically prove that the multiple neural networks with inertial terms converge asymptotically/exponentially to the k -winners-take-all solution exactly from arbitrary initial states for whatever decomposition of inputs and objective. Furthermore, some extensions to distributed constrained k -winners-take-all are also investigated. Finally, simulation results are presented to substantiate the effectiveness of the proposed system as well as its superior performance over existing distributed networks.},
  archive      = {J_NN},
  author       = {Xiaoxuan Wang and Shaofu Yang and Zhenyuan Guo and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.04.005},
  journal      = {Neural Networks},
  pages        = {385-397},
  shortjournal = {Neural Netw.},
  title        = {Distributed k-winners-take-all via multiple neural networks with inertia},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum support vector machine based on regularized newton
method. <em>NN</em>, <em>151</em>, 376–384. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An elegant quantum version of least-square support vector machine , which is exponentially faster than the classical counterpart, was given by Rebentrost et al. using the matrix inversion algorithm (HHL). However, the application of the HHL algorithm is restricted when the structure of the input matrix is not well. The iteration algorithms such as the Newton method are widespread in training the classical support vector machine . This paper demonstrates a quantum support vector machine based on the regularized Newton method (RN-QSVM), which achieves an exponential speed-up over classical algorithm. At first, the regularized quantum Newton algorithm is proposed to get rid of the constraint of input matrix. Then we train the RN-QSVM by using the regularized quantum Newton algorithm and classify a query sample by constructing the quantum state . Experiments demonstrate that RN-QSVM respectively provides advantages in terms of accuracy, robustness, and complexity compared to QSLS-SVM, LS-QSVM, and the classical method.},
  archive      = {J_NN},
  author       = {Rui Zhang and Jian Wang and Nan Jiang and Hong Li and Zichen Wang},
  doi          = {10.1016/j.neunet.2022.03.043},
  journal      = {Neural Networks},
  pages        = {376-384},
  shortjournal = {Neural Netw.},
  title        = {Quantum support vector machine based on regularized newton method},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of text-to-gesture generation model using
convolutional neural network. <em>NN</em>, <em>151</em>, 365–375. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational gestures have a crucial role in realizing natural interactions with virtual agents and robots. Data-driven approaches, such as deep learning and machine learning , are promising in constructing the gesture generation model, which automatically provides the gesture motion for speech or spoken texts. This study experimentally analyzes a deep learning-based gesture generation model from spoken text using a convolutional neural network . The proposed model takes a sequence of spoken words as the input and outputs a sequence of 2D joint coordinates representing the conversational gesture motion. We prepare a dataset consisting of gesture motions and spoken texts by adding text information to an existing dataset and train the models using specific speaker’s data. The quality of the generated gestures is compared with those from an existing speech-to-gesture generation model through a user perceptual study. The subjective evaluation shows that the model performance is comparable or superior to those by the existing speech-to-gesture generation model. In addition, we investigate the importance of data cleansing and loss function selection in the text-to-gesture generation model. We further examine the model transferability between speakers. The experimental results demonstrate successful model transferability of the proposed model. Finally, we show that the text-to-gesture generation model can produce good quality gestures even when using a transformer architecture.},
  archive      = {J_NN},
  author       = {Eiichi Asakawa and Naoshi Kaneko and Dai Hasegawa and Shinichi Shirakawa},
  doi          = {10.1016/j.neunet.2022.03.041},
  journal      = {Neural Networks},
  pages        = {365-375},
  shortjournal = {Neural Netw.},
  title        = {Evaluation of text-to-gesture generation model using convolutional neural network},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural feedback facilitates rough-to-fine information
retrieval. <em>NN</em>, <em>151</em>, 349–364. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorical relationships between objects are encoded as overlapped neural representations in the brain, where the more similar the objects are, the larger the correlations between their evoked neuronal responses. These representation correlations, however, inevitably incur interference when memories are retrieved. Here, we propose that neural feedback, which is widely observed in the brain but whose function remains largely unknown, contributes to disentangle neural correlations to improve information retrieval. We study a hierarchical neural network storing the hierarchical categorical information of objects, and information retrieval goes from rough-to-fine, aided by the push–pull neural feedback. We elucidate that the push and the pull components of the feedback suppress the interferences due to the representation correlations between objects from different and the same categories, respectively. Our model reproduces the push–pull phenomenon observed in neural data and sheds light on our understanding of the role of feedback in neural information processing .},
  archive      = {J_NN},
  author       = {Xiao Liu and Xiaolong Zou and Zilong Ji and Gengshuo Tian and Yuanyuan Mi and Tiejun Huang and K.Y. Michael Wong and Si Wu},
  doi          = {10.1016/j.neunet.2022.03.042},
  journal      = {Neural Networks},
  pages        = {349-364},
  shortjournal = {Neural Netw.},
  title        = {Neural feedback facilitates rough-to-fine information retrieval},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DGInet: Dynamic graph and interaction-aware convolutional
network for vehicle trajectory prediction. <em>NN</em>, <em>151</em>,
336–348. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates vehicle trajectory prediction problems in real traffic scenarios by fully harnessing the spatio-temporal dependencies between multiple vehicles. The existing GCN-based trajectory predictions are often considered in a single traffic scene without time attributes, complete interaction information, dynamic graph-based model, etc. Time and interaction aware models are more challenging than the existing ones. Despite very well does the graph-based model describe the relationship between driving vehicles, the critical problem in the traffic scene is how to deeply explore the spatio-temporal characteristics of dynamic graphs. Therefore, a novel dynamic graph and interaction-aware neural network model called as DGInet is proposed by combining a semi-global graph mechanism and an M-product based graph convolutional network, which are built into novel dual-network architecture in the entire model. The DGInet is built by exploiting the dynamic interaction in depth between driving vehicles in urban traffic scenarios, and then realized by utilizing semi-global graph convolution operations on the input data cell to capture the basic spatial interaction features of the driving scene. Meanwhile, the dynamic graph is further extracted by a novel M-product approach, in which the embedding of the model is then established along with the embedding of the semi-global network to perform the final embedding. Extensive experiments have been conducted on the two public datasets, named NGSIM and Apollo respectively, to show that our approach outperforms the existing ones with better performance and less computing time. Besides the real-world Shenzhen traffic dataset, China, is also developed to verify the effectiveness of our approach.},
  archive      = {J_NN},
  author       = {Jiyao An and Wei Liu and Qingqin Liu and Liang Guo and Ping Ren and Tao Li},
  doi          = {10.1016/j.neunet.2022.03.038},
  journal      = {Neural Networks},
  pages        = {336-348},
  shortjournal = {Neural Netw.},
  title        = {DGInet: Dynamic graph and interaction-aware convolutional network for vehicle trajectory prediction},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hippocampal formation-inspired probabilistic generative
model. <em>NN</em>, <em>151</em>, 317–335. (<a
href="https://doi.org/10.1016/j.neunet.2022.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In building artificial intelligence (AI) agents, referring to how brains function in real environments can accelerate development by reducing the design space. In this study, we propose a probabilistic generative model (PGM) for navigation in uncertain environments by integrating the neuroscientific knowledge of hippocampal formation (HF) and the engineering knowledge in robotics and AI, namely, simultaneous localization and mapping (SLAM). We follow the approach of brain reference architecture (BRA) (Yamakawa, 2021) to compose the PGM and outline how to verify the model. To this end, we survey and discuss the relationship between the HF findings and SLAM models. The proposed hippocampal formation-inspired probabilistic generative model (HF-PGM) is designed to be highly consistent with the anatomical structure and functions of the HF. By referencing the brain, we elaborate on the importance of integration of egocentric/allocentric information from the entorhinal cortex to the hippocampus and the use of discrete-event queues.},
  archive      = {J_NN},
  author       = {Akira Taniguchi and Ayako Fukawa and Hiroshi Yamakawa},
  doi          = {10.1016/j.neunet.2022.04.001},
  journal      = {Neural Networks},
  pages        = {317-335},
  shortjournal = {Neural Netw.},
  title        = {Hippocampal formation-inspired probabilistic generative model},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Branching time active inference: The theory and its
generality. <em>NN</em>, <em>151</em>, 295–316. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 10 to 15 years, active inference has helped to explain various brain mechanisms from habit formation to dopaminergic discharge and even modelling curiosity. However, the current implementations suffer from an exponential (space and time) complexity class when computing the prior over all the possible policies up to the time-horizon. Fountas et al. (2020) used Monte Carlo tree search to address this problem, leading to impressive results in two different tasks. In this paper, we present an alternative framework that aims to unify tree search and active inference by casting planning as a structure learning problem. Two tree search algorithms are then presented. The first propagates the expected free energy forward in time (i.e., towards the leaves), while the second propagates it backward (i.e., towards the root). Then, we demonstrate that forward and backward propagations are related to active inference and sophisticated inference, respectively, thereby clarifying the differences between those two planning strategies.},
  archive      = {J_NN},
  author       = {Théophile Champion and Lancelot Da Costa and Howard Bowman and Marek Grześ},
  doi          = {10.1016/j.neunet.2022.03.036},
  journal      = {Neural Networks},
  pages        = {295-316},
  shortjournal = {Neural Netw.},
  title        = {Branching time active inference: The theory and its generality},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoding sensorimotor information from superior parietal
lobule of macaque via convolutional neural networks. <em>NN</em>,
<em>151</em>, 276–294. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the well-recognized role of the posterior parietal cortex (PPC) in processing sensory information to guide action, the differential encoding properties of this dynamic processing, as operated by different PPC brain areas, are scarcely known. Within the monkey’s PPC, the superior parietal lobule hosts areas V6A, PEc, and PE included in the dorso-medial visual stream that is specialized in planning and guiding reaching movements. Here, a Convolutional Neural Network (CNN) approach is used to investigate how the information is processed in these areas. We trained two macaque monkeys to perform a delayed reaching task towards 9 positions (distributed on 3 different depth and direction levels) in the 3D peripersonal space. The activity of single cells was recorded from V6A, PEc, PE and fed to convolutional neural networks that were designed and trained to exploit the temporal structure of neuronal activation patterns , to decode the target positions reached by the monkey. Bayesian Optimization was used to define the main CNN hyper-parameters. In addition to discrete positions in space, we used the same network architecture to decode plausible reaching trajectories. We found that data from the most caudal V6A and PEc areas outperformed PE area in the spatial position decoding. In all areas, decoding accuracies started to increase at the time the target to reach was instructed to the monkey, and reached a plateau at movement onset. The results support a dynamic encoding of the different phases and properties of the reaching movement differentially distributed over a network of interconnected areas. This study highlights the usefulness of neurons’ firing rate decoding via CNNs to improve our understanding of how sensorimotor information is encoded in PPC to perform reaching movements. The obtained results may have implications in the perspective of novel neuroprosthetic devices based on the decoding of these rich signals for faithfully carrying out patient’s intentions.},
  archive      = {J_NN},
  author       = {Matteo Filippini and Davide Borra and Mauro Ursino and Elisa Magosso and Patrizia Fattori},
  doi          = {10.1016/j.neunet.2022.03.044},
  journal      = {Neural Networks},
  pages        = {276-294},
  shortjournal = {Neural Netw.},
  title        = {Decoding sensorimotor information from superior parietal lobule of macaque via convolutional neural networks},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Provable training of a ReLU gate with an iterative
non-gradient algorithm. <em>NN</em>, <em>151</em>, 264–275. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we demonstrate provable guarantees on the training of a single ReLU ReLU gate in hitherto unexplored regimes. We give a simple iterative stochastic algorithm that can train a ReLU ReLU gate in the realizable setting in linear time while using significantly milder conditions on the data distribution than previous such results. Leveraging certain additional moment assumptions, we also show a first-of-its-kind approximate recovery of the true label generating parameters under an (online) data-poisoning attack on the true labels, while training a ReLU ReLU gate by the same algorithm. Our guarantee is shown to be nearly optimal in the worst case and its accuracy of recovering the true weight degrades gracefully with increasing probability of attack and its magnitude. For both the realizable and the non-realizable cases as outlined above, our analysis allows for mini-batching and computes how the convergence time scales with the mini-batch size. We corroborate our theorems with simulation results which also bring to light a striking similarity in trajectories between our algorithm and the popular S.G.D. algorithm — for which similar guarantees as here are still unknown.},
  archive      = {J_NN},
  author       = {Sayar Karmakar and Anirbit Mukherjee},
  doi          = {10.1016/j.neunet.2022.03.040},
  journal      = {Neural Networks},
  pages        = {264-275},
  shortjournal = {Neural Netw.},
  title        = {Provable training of a ReLU gate with an iterative non-gradient algorithm},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multigraph classification using learnable integration
network with application to gender fingerprinting. <em>NN</em>,
<em>151</em>, 250–263. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multigraphs with heterogeneous views present one of the most challenging obstacles to classification tasks due to their complexity. Several works based on feature selection have been recently proposed to disentangle the problem of multigraph heterogeneity. However, such techniques have major drawbacks. First , the bulk of such works lies in the vectorization and the flattening operations, failing to preserve and exploit the rich topological properties of the multigraph. Second , they learn the classification process in a dichotomized manner where the cascaded learning steps are pieced in together independently. Hence, such architectures are inherently agnostic to the cumulative estimation error from step to step. To overcome these drawbacks, we introduce MICNet (multigraph integration and classifier network), the first end-to-end graph neural network based model for multigraph classification. First, we learn a single-view graph representation of a heterogeneous multigraph using a GNN based integration model. The integration process in our model helps tease apart the heterogeneity across the different views of the multigraph by generating a subject-specific graph template while preserving its geometrical and topological properties conserving the node-wise information while reducing the size of the graph (i.e., number of views). Second, we classify each integrated template using a geometric deep learning block which enables us to grasp the salient graph features. We train, in end-to-end fashion, these two blocks using a single objective function to optimize the classification performance. We evaluate our MICNet in gender classification using brain multigraphs derived from different cortical measures. We demonstrate that our MICNet significantly outperformed its variants thereby showing its great potential in multigraph classification.},
  archive      = {J_NN},
  author       = {Nada Chaari and Mohammed Amine Gharsallaoui and Hatice Camgöz Akdağ and Islem Rekik},
  doi          = {10.1016/j.neunet.2022.03.035},
  journal      = {Neural Networks},
  pages        = {250-263},
  shortjournal = {Neural Netw.},
  title        = {Multigraph classification using learnable integration network with application to gender fingerprinting},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informative pairs mining based adaptive metric learning for
adversarial domain adaptation. <em>NN</em>, <em>151</em>, 238–249. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial domain adaptation has made remarkable in promoting feature transferability, while recent work reveals that there exists an unexpected degradation of feature discrimination during the procedure of learning transferable features. This paper proposes an informative pairs mining based adaptive metric learning (IPM-AML), where a novel two-triplet-sampling strategy is advanced to select informative positive pairs from the same classes and informative negative pairs from different classes, and a metric loss imposed with special weights is further utilized to adaptively pay more attention to those more informative pairs which can adaptively improve discrimination. Then, we incorporate IPM-AML into popular conditional domain adversarial network (CDAN) to learn feature representation that is transferable and discriminative desirably (IPM-AML-CDAN). To ensure the reliability of pseudo target labels in the whole training process, we select more confident target ones whose predicted scores are higher than a given threshold T T , and also provide theoretical validation for this simple threshold strategy. Extensive experiment results on four cross-domain benchmarks validate that IPM-AML-CDAN can achieve competitive results compared with state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Mengzhu Wang and Paul Li and Li Shen and Ye Wang and Shanshan Wang and Wei Wang and Xiang Zhang and Junyang Chen and Zhigang Luo},
  doi          = {10.1016/j.neunet.2022.03.031},
  journal      = {Neural Networks},
  pages        = {238-249},
  shortjournal = {Neural Netw.},
  title        = {Informative pairs mining based adaptive metric learning for adversarial domain adaptation},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-based tensor subspace analysis system for kinship
verification. <em>NN</em>, <em>151</em>, 222–237. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing automatic kinship verification methods focus on learning the optimal distance metrics between family members. However, learning facial features and kinship features simultaneously may cause the proposed models to be too weak. In this work, we explore the possibility of bridging this gap by developing knowledge-based tensor models based on pre-trained multi-view models. We propose an effective knowledge-based tensor similarity extraction framework for automatic facial kinship verification using four pre-trained networks (i.e., VGG-Face, VGG-F, VGG-M, and VGG-S). Therefore, knowledge-based deep face and general features (such as identity, age, gender, ethnicity, expression, lighting, pose, contour, edges, corners, shape, etc.) were successfully fused by our tensor design to understand the kinship cue. Multiple effective representations are learned for kinship verification statements (children and parents) using a margin maximization learning scheme based on Tensor Cross-view Quadratic Exponential Discriminant Analysis . Through the exponential learning process, the large gap between distributions of the same family can be reduced to the maximum, while the small gap between distributions of different families is simultaneously increased. The WCCN metric successfully reduces the intra-class variability problem caused by deep features. The explanation of black-box models and the problems of ubiquitous face recognition are considered in our system. The extensive experiments on four challenging datasets show that our system performs very well compared to state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {I. Serraoui and O. Laiadi and A. Ouamane and F. Dornaika and A. Taleb-Ahmed},
  doi          = {10.1016/j.neunet.2022.03.020},
  journal      = {Neural Networks},
  pages        = {222-237},
  shortjournal = {Neural Netw.},
  title        = {Knowledge-based tensor subspace analysis system for kinship verification},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural network for a class of sparse optimization with
l0-regularization. <em>NN</em>, <em>151</em>, 211–221. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse optimization involving the L 0 L0 -norm function as the regularization in objective function has a wide application in many fields. In this paper, we propose a projected neural network modeled by a differential equation to solve a class of these optimization problems , in which the objective function is the sum of a nonsmooth convex loss function and the regularization defined by the L 0 L0 -norm function. This optimization problem is not only nonconvex, but also discontinuous. To simplify the structure of the proposed network and let it own better convergence properties , we use the smoothing method, where the new constructed smoothing function for the regularization term plays a key role. We prove that the solution to the proposed network is globally existent and unique, and any accumulation point of it is a critical point of the continuous relaxation model. Except for a special case, which can be easily justified, any critical point is a local minimizer of the considered sparse optimization problem. It is an interesting thing that all critical points own a promising lower bound property, which is satisfied by all global minimizers of the considered problem, but is not by all local minimizers. Finally, we use some numerical experiments to illustrate the efficiency and good performance of the proposed method for solving this class of sparse optimization problems , which include the most widely used models in feature selection of classification learning .},
  archive      = {J_NN},
  author       = {Zhe Wei and Qingfa Li and Jiazhen Wei and Wei Bian},
  doi          = {10.1016/j.neunet.2022.03.033},
  journal      = {Neural Networks},
  pages        = {211-221},
  shortjournal = {Neural Netw.},
  title        = {Neural network for a class of sparse optimization with l0-regularization},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cortical circuits for top-down control of perceptual
grouping. <em>NN</em>, <em>151</em>, 190–210. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental characteristic of human visual perception is the ability to group together disparate elements in a scene and treat them as a single unit. The mechanisms by which humans create such groupings remain unknown, but grouping seems to play an important role in a wide variety of visual phenomena, and a good understanding of these mechanisms might provide guidance for how to improve machine vision algorithms. Here, we build on a proposal that some groupings are the result of connections in cortical area V2 that join disparate elements, thereby allowing them to be selected and segmented together. In previous instantiations of this proposal, connection formation was based on the anatomy (e.g., extent) of receptive fields, which made connection formation obligatory when the stimulus conditions stimulate the corresponding receptive fields. We now propose dynamic circuits that provide greater flexibility in the formation of connections and that allow for top-down control of perceptual grouping. With computer simulations we explain how the circuits work and show how they can account for a wide variety of Gestalt principles of perceptual grouping, texture segmentation tasks, amodal illusory contours, and ratings of perceived groupings. We propose that human observers use such top-down control to implement task-dependent connection strategies that encourage particular groupings of stimulus elements in order to promote performance on various visual tasks.},
  archive      = {J_NN},
  author       = {Maria Kon and Gregory Francis},
  doi          = {10.1016/j.neunet.2022.03.029},
  journal      = {Neural Networks},
  pages        = {190-210},
  shortjournal = {Neural Netw.},
  title        = {Cortical circuits for top-down control of perceptual grouping},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Think positive: An interpretable neural network for image
recognition. <em>NN</em>, <em>151</em>, 178–189. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X X -rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48\%, 0.99, 0.99 and 0.99, respectively.},
  archive      = {J_NN},
  author       = {Gurmail Singh},
  doi          = {10.1016/j.neunet.2022.03.034},
  journal      = {Neural Networks},
  pages        = {178-189},
  shortjournal = {Neural Netw.},
  title        = {Think positive: An interpretable neural network for image recognition},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Double structure scaled simplex representation for
multi-view subspace clustering. <em>NN</em>, <em>151</em>, 168–177. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, there are an increasing number of multisource information data, and multi-view clustering (MVC) algorithms have developed rapidly. However, the affinity matrix learned by most MVC methods is not clean and precise enough and cannot describe the latent structure of multi-view data accurately, which results in poor clustering performance. In this paper, we propose a novel Double Structure Scaled Simplex Representation (DSSSR) method for MVC. Initially, we concatenate the multi-view data into a joint representation. Then, we use the scaled simplex representation (SSR) method on the concatenated data to obtain the affinity matrix . However, the affinity matrix is not clean and precise. Therefore, we use the SSR method again on the obtained affinity matrix to obtain a more accurate and clean affinity matrix. Furthermore, the two-step SSR is integrated into a unified optimization framework, a clean and accurate affinity matrix can be obtained, and the sum of each column vector of the affinity matrix is constrained to be nonnegative and equal to s ( 0 0&amp;lt;s&amp;lt;1 ), which can be adjusted to obtain the best clustering performance. Finally, an efficient optimization algorithm based on the augmented Lagrangian method (ALM) for solving the objective function is also designed. The experimental results on some datasets show that this algorithm has better clustering performance than some state-of-the-art algorithms.},
  archive      = {J_NN},
  author       = {Liang Yao and Gui-Fu Lu},
  doi          = {10.1016/j.neunet.2022.03.039},
  journal      = {Neural Networks},
  pages        = {168-177},
  shortjournal = {Neural Netw.},
  title        = {Double structure scaled simplex representation for multi-view subspace clustering},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive modeling of nonnegative environmental systems based
on projectional differential neural networks observer. <em>NN</em>,
<em>151</em>, 156–167. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new design of a non-parametric adaptive approximate model based on Differential Neural Networks (DNNs) applied for a class of non-negative environmental systems with an uncertain mathematical model is the primary outcome of this study. The approximate model uses an extended state formulation that gathers the dynamics of the DNN and a state projector (pDNN). Implementing a non-differentiable projection operator ensures the positiveness of the identifier states. The extended form allows producing continuous dynamics for the projected model. The design of the learning laws for the weight adjustment of the continuous projected DNN considered the application of a controlled Lyapunov-like function. The stability analysis based on the proposed Lyapunov-like function leads to the characterization of the ultimate boundedness property for the identification error. Applying the Attractive Ellipsoid Method (AEM) yields to analyze the convergence quality of the designed approximate model. The solution to the specific optimization problem using the AEM with matrix inequalities constraints allows us to find the parameters of the considered DNN that minimizes the ultimate bound. The evaluation of two numerical examples confirmed the ability of the proposed pDNN to approximate the positive model in the presence of bounded noises and perturbations in the measured data. The first example corresponds to a catalytic ozonation system that can be used to decompose toxic and recalcitrant contaminants. The second one describes the bacteria growth in aerobic batch regime biodegrading simple organic matter mixture.},
  archive      = {J_NN},
  author       = {Isaac Chairez and Olga Andrianova and Tatyana Poznyak and Alexander Poznyak},
  doi          = {10.1016/j.neunet.2022.03.028},
  journal      = {Neural Networks},
  pages        = {156-167},
  shortjournal = {Neural Netw.},
  title        = {Adaptive modeling of nonnegative environmental systems based on projectional differential neural networks observer},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lag h∞ synchronization of coupled neural networks with
multiple state couplings and multiple delayed state couplings.
<em>NN</em>, <em>151</em>, 143–155. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly focuses on the lag H ∞ H∞ synchronization problem of coupled neural networks with multiple state or delayed state couplings. On one hand, by exploiting state feedback controller and Lyapunov functional , a criterion of lag H ∞ H∞ synchronization for coupled neural networks with multiple state couplings (CNNMSCs) is insured, and lag H ∞ H∞ synchronization problem in CNNMSCs is also coped with based on the adaptive state feedback controller . On the other hand, we explore the lag H ∞ H∞ synchronization for coupled neural networks with multiple delayed state couplings (CNNMDSCs) by utilizing similar control strategies. At last, two numerical examples are presented to verify the effectiveness and correctness of lag H ∞ H∞ synchronization for CNNMSCs and CNNMDSCs.},
  archive      = {J_NN},
  author       = {Yuting Cao and Linhao Zhao and Shiping Wen and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.03.032},
  journal      = {Neural Networks},
  pages        = {143-155},
  shortjournal = {Neural Netw.},
  title        = {Lag h∞ synchronization of coupled neural networks with multiple state couplings and multiple delayed state couplings},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic auxiliary soft labels for decoupled learning.
<em>NN</em>, <em>151</em>, 132–142. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-tailed distribution in the dataset is one of the major challenges of deep learning . Convolutional Neural Networks have poor performance in identifying classes with only a few samples. For this problem, it has been proved that separating the feature learning stage and the classifier learning stage improves the performance of models effectively, which is called decoupled learning. We use soft labels to improve the performance of the decoupled learning framework by proposing a Dynamic Auxiliary Soft Labels (DaSL) method. Specifically, we design a dedicated auxiliary network to generate auxiliary soft labels for the two different training stages. In the feature learning stage, it helps to learn features with smaller variance within the class, and in the classifier learning stage it helps to alleviate the overconfidence of the model prediction. We also introduce a feature-level distillation method for the feature learning, and improve the learning of general features through multi-scale feature fusion . We conduct extensive experiments on three long-tailed recognition benchmark datasets to demonstrate the effectiveness of our DaSL.},
  archive      = {J_NN},
  author       = {Yan Wang and Yongshun Zhang and Furao Shen and Jian Zhao},
  doi          = {10.1016/j.neunet.2022.03.027},
  journal      = {Neural Networks},
  pages        = {132-142},
  shortjournal = {Neural Netw.},
  title        = {Dynamic auxiliary soft labels for decoupled learning},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain-inspired multiple-target tracking using dynamic neural
fields. <em>NN</em>, <em>151</em>, 121–131. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite considerable progress in the field of automatic multi-target tracking, several problems such as data association remained challenging. On the other hand, cognitive studies have reported that humans can robustly track several objects simultaneously. Such circumstances happen regularly in daily life, and humans have evolved to handle the associated problems. Accordingly, using brain-inspired processing principles may contribute to significantly increase the performance of automatic systems able to follow the trajectories of multiple objects. In this paper, we propose a multiple-object tracking algorithm based on dynamic neural field theory which has been proven to provide neuro-plausible processing mechanisms for cognitive functions of the brain. We define several input neural fields responsible for representing previous location and orientation information as well as instantaneous linear and angular speed of the objects in successive video frames. Image processing techniques are applied to extract the critical object features including target location and orientation. Two prediction fields anticipate the objects’ locations and orientations in the upcoming frame after receiving excitatory and inhibitory inputs from the input fields in a feed-forward architecture. This information is used in the data association and labeling process. We tested the proposed algorithm on a zebrafish larvae segmentation and tracking dataset and an ant-tracking dataset containing non-rigid objects with spiky movements and frequently occurring occlusions. The results showed a significant improvement in tracking metrics compared to state-of-the-art algorithms.},
  archive      = {J_NN},
  author       = {Shiva Kamkar and Hamid Abrishami Moghaddam and Reza Lashgari and Wolfram Erlhagen},
  doi          = {10.1016/j.neunet.2022.03.026},
  journal      = {Neural Networks},
  pages        = {121-131},
  shortjournal = {Neural Netw.},
  title        = {Brain-inspired multiple-target tracking using dynamic neural fields},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Golden subject is everyone: A subject transfer neural
network for motor imagery-based brain computer interfaces. <em>NN</em>,
<em>151</em>, 111–120. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalographic measurement of cortical activity subserving motor behavior varies among different individuals, restricting the potential of brain computer interfaces (BCIs) based on motor imagery (MI). How to deal with this variability and thereby improve the accuracy of BCI classification remains a key issue. This paper proposes a deep learning-based approach to transfer the data distribution from BCI-friendly — “golden subjects” to the data from more typical BCI-illiterate users. In this work, we use the perceptual loss to align the dimensionality-reduced BCI-illiterate data with the data of golden subjects in low dimensions, by which a subject transfer neural network (STNN) is proposed. The network consists of two parts: 1) a generator, which generates the transferred BCI-illiterate features, and 2) a CNN classifier, which is used for the classification of the transferred features, thus outperforming traditional classification methods both in terms of accuracy and robustness. Electroencephalography (EEG) signals from 25 healthy subjects performing MI of the right hand and foot were classified with an average accuracy of 88 . 2\% ± 5 . 1\% 88.2\%±5.1\% . The proposed model was further validated on the BCI Competition IV dataset 2b, and was demonstrated to be robust to inter-subject variations. The advantages of STNN allow it to bridge the gap between the golden subjects and the BCI-illiterate ones, paving the way to real-time BCI applications.},
  archive      = {J_NN},
  author       = {Biao Sun and Zexu Wu and Yong Hu and Ting Li},
  doi          = {10.1016/j.neunet.2022.03.025},
  journal      = {Neural Networks},
  pages        = {111-120},
  shortjournal = {Neural Netw.},
  title        = {Golden subject is everyone: A subject transfer neural network for motor imagery-based brain computer interfaces},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning a discriminative SPD manifold neural network for
image set classification. <em>NN</em>, <em>151</em>, 94–110. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing pattern analysis over the symmetric positive definite (SPD) manifold requires specific mathematical computations, characterizing the non-Euclidian property of the involved data points and learning tasks, such as the image set classification problem. Accompanied with the advanced neural networking techniques, several architectures for processing the SPD matrices have recently been studied to obtain fine-grained structured representations. However, existing approaches are challenged by the diversely changing appearance of the data points, begging the question of how to learn invariant representations for improved performance with supportive theories. Therefore, this paper designs two Riemannian operation modules for SPD manifold neural network . Specifically, a Riemannian batch regularization (RBR) layer is firstly proposed for the purpose of training a discriminative manifold-to-manifold transforming network with a novelly-designed metric learning regularization term. The second module realizes the Riemannian pooling operation with geometric computations on the Riemannian manifolds , notably the Riemannian barycenter , metric learning, and Riemannian optimization. Extensive experiments on five benchmarking datasets show the efficacy of the proposed approach.},
  archive      = {J_NN},
  author       = {Rui Wang and Xiao-Jun Wu and Ziheng Chen and Tianyang Xu and Josef Kittler},
  doi          = {10.1016/j.neunet.2022.03.012},
  journal      = {Neural Networks},
  pages        = {94-110},
  shortjournal = {Neural Netw.},
  title        = {Learning a discriminative SPD manifold neural network for image set classification},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards understanding theoretical advantages of
complex-reaction networks. <em>NN</em>, <em>151</em>, 80–93. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-valued neural networks have attracted increasing attention in recent years, while it remains open on the advantages of complex-valued neural networks in comparison with real-valued networks. This work takes one step on this direction by introducing the complex-reaction network with fully-connected feed-forward architecture. We prove the universal approximation property for complex-reaction networks, and show that a class of radial functions can be approximated by a complex-reaction network using the polynomial number of parameters, whereas real-valued networks need at least exponential parameters to reach the same approximation level. For empirical risk minimization , we study the landscape and convergence of complex gradient descents . Our theoretical result shows that the critical point set of complex-reaction networks is a proper subset of that of real-valued networks, which may show some insights on finding the optimal solutions more easily for complex-reaction networks.},
  archive      = {J_NN},
  author       = {Shao-Qun Zhang and Wei Gao and Zhi-Hua Zhou},
  doi          = {10.1016/j.neunet.2022.03.024},
  journal      = {Neural Networks},
  pages        = {80-93},
  shortjournal = {Neural Netw.},
  title        = {Towards understanding theoretical advantages of complex-reaction networks},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GHNN: Graph harmonic neural networks for semi-supervised
graph-level classification. <em>NN</em>, <em>151</em>, 70–79. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification aims to predict the property of the whole graph, which has attracted growing attention in the graph learning community. This problem has been extensively studied in the literature of both graph convolutional networks and graph kernels. Graph convolutional networks can learn effective node representations via message passing to mine graph topology in an implicit way, whereas graph kernels can explicitly utilize graph structural knowledge for classification. Due to the scarcity of labeled data in real-world applications, semi-supervised algorithms are anticipated for this problem. In this paper, we propose Graph Harmonic Neural Network (GHNN) which combines the advantages of both worlds to sufficiently leverage the unlabeled data , and thus overcomes label scarcity in semi-supervised scenarios. Specifically, our GHNN consists of a graph convolutional network (GCN) module and a graph kernel network (GKN) module that explore graph topology information from complementary perspectives. To fully leverage the unlabeled data , we develop a novel harmonic contrastive loss and a harmonic consistency loss to harmonize the training of two modules by giving priority to high-quality unlabeled data, thereby reconciling prediction consistency between both of them. In this manner, the two modules mutually enhance each other to sufficiently explore the graph topology of both labeled and unlabeled data. Extensive experiments on a variety of benchmarks demonstrate the effectiveness of our approach over competitive baselines.},
  archive      = {J_NN},
  author       = {Wei Ju and Xiao Luo and Zeyu Ma and Junwei Yang and Minghua Deng and Ming Zhang},
  doi          = {10.1016/j.neunet.2022.03.018},
  journal      = {Neural Networks},
  pages        = {70-79},
  shortjournal = {Neural Netw.},
  title        = {GHNN: Graph harmonic neural networks for semi-supervised graph-level classification},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guaranteed approximation error estimation of neural networks
and model modification. <em>NN</em>, <em>151</em>, 61–69. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximation error is a key measure in the process of model validation and verification for neural networks . In this paper, the problems of guaranteed error estimation of neural networks and applications to assured system modeling and assured neural network compression are addressed. First, a concept called guaranteed error estimation of feedforward neural networks is proposed, which intends to provide the worst-case approximation error of a trained neural network with respect to a compact input set essentially containing an infinite number of values. Given different prior information about the original system, two approaches including Lipschitz constant analysis and set-valued reachability analysis methods are developed to efficiently compute upper-bounds of approximation errors. Based on the guaranteed approximation error estimation framework, an optimization for obtaining parameter values from data set is proposed. A robotic arm and neural network compression examples are presented to illustrate the effectiveness of our approach.},
  archive      = {J_NN},
  author       = {Yejiang Yang and Tao Wang and Jefferson P. Woolard and Weiming Xiang},
  doi          = {10.1016/j.neunet.2022.03.023},
  journal      = {Neural Networks},
  pages        = {61-69},
  shortjournal = {Neural Netw.},
  title        = {Guaranteed approximation error estimation of neural networks and model modification},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving generalization of deep neural networks by
leveraging margin distribution. <em>NN</em>, <em>151</em>, 48–60. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has used margin theory to analyze the generalization performance for deep neural networks (DNNs). The existed results are almost based on the spectrally-normalized minimum margin. However, optimizing the minimum margin ignores a mass of information about the entire margin distribution, which is crucial to generalization performance . In this paper, we prove a generalization upper bound dominated by the statistics of the entire margin distribution. Compared with the minimum margin bounds, our bound highlights an important measure for controlling the complexity, which is the ratio of the margin standard deviation to the expected margin. We utilize a convex margin distribution loss function on the deep neural networks to validate our theoretical results by optimizing the margin ratio. Experiments and visualizations confirm the effectiveness of our approach and the correlation between generalization gap and margin ratio.},
  archive      = {J_NN},
  author       = {Shen-Huan Lyu and Lu Wang and Zhi-Hua Zhou},
  doi          = {10.1016/j.neunet.2022.03.019},
  journal      = {Neural Networks},
  pages        = {48-60},
  shortjournal = {Neural Netw.},
  title        = {Improving generalization of deep neural networks by leveraging margin distribution},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MoËT: Mixture of expert trees and its application to
verifiable reinforcement learning. <em>NN</em>, <em>151</em>, 34–47. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements in deep learning have led to many recent breakthroughs. While deep learning models achieve superior performance, often statistically better than humans, their adoption into safety-critical settings, such as healthcare or self-driving cars is hindered by their inability to provide safety guarantees or to expose the inner workings of the model in a human understandable form. We present MoËT , a novel model based on Mixture of Experts, consisting of decision tree experts and a generalized linear model gating function. Thanks to such gating function the model is more expressive than the standard decision tree . To support non-differentiable decision trees as experts, we formulate a novel training procedure. In addition, we introduce a hard thresholding version, MoËT h , in which predictions are made solely by a single expert chosen via the gating function. Thanks to that property, MoËT h allows each prediction to be easily decomposed into a set of logical rules in a form which can be easily verified. While MoËT is a general use model, we illustrate its power in the reinforcement learning setting. By training MoËT models using an imitation learning procedure on deep RL agents we outperform the previous state-of-the-art technique based on decision trees while preserving the verifiability of the models. Moreover, we show that MoËT can also be used in real-world supervised problems on which it outperforms other verifiable machine learning models.},
  archive      = {J_NN},
  author       = {Marko Vasić and Andrija Petrović and Kaiyuan Wang and Mladen Nikolić and Rishabh Singh and Sarfraz Khurshid},
  doi          = {10.1016/j.neunet.2022.03.022},
  journal      = {Neural Networks},
  pages        = {34-47},
  shortjournal = {Neural Netw.},
  title        = {MoËT: Mixture of expert trees and its application to verifiable reinforcement learning},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploration in neo-hebbian reinforcement learning:
Computational approaches to the exploration–exploitation balance with
bio-inspired neural networks. <em>NN</em>, <em>151</em>, 16–33. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent theoretical and experimental works have connected Hebbian plasticity with the reinforcement learning (RL) paradigm, producing a class of trial-and-error learning in artificial neural networks known as neo-Hebbian plasticity. Inspired by the role of the neuromodulator dopamine in synaptic modification, neo-Hebbian RL methods extend unsupervised Hebbian learning rules with value-based modulation to selectively reinforce associations. This reinforcement allows for learning exploitative behaviors and produces RL models with strong biological plausibility . The review begins with coverage of fundamental concepts in rate- and spike-coded models. We introduce Hebbian correlation detection as a basis for modification of synaptic weighting and progress to neo-Hebbian RL models guided solely by extrinsic rewards. We then analyze state-of-the-art neo-Hebbian approaches to the exploration–exploitation balance under the RL paradigm, emphasizing works that employ additional mechanics to modulate that dynamic. Our review of neo-Hebbian RL methods in this context indicates substantial potential for novel improvements in exploratory learning, primarily through stronger incorporation of intrinsic motivators. We provide a number of research suggestions for this pursuit by drawing from modern theories and results in neuroscience and psychology. The exploration–exploitation balance is a central issue in RL research, and this review is the first to focus on it under the neo-Hebbian RL framework.},
  archive      = {J_NN},
  author       = {Anthony Triche and Anthony S. Maida and Ashok Kumar},
  doi          = {10.1016/j.neunet.2022.03.021},
  journal      = {Neural Networks},
  pages        = {16-33},
  shortjournal = {Neural Netw.},
  title        = {Exploration in neo-hebbian reinforcement learning: Computational approaches to the exploration–exploitation balance with bio-inspired neural networks},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TSFD-net: Tissue specific feature distillation network for
nuclei segmentation and classification. <em>NN</em>, <em>151</em>, 1–15.
(<a href="https://doi.org/10.1016/j.neunet.2022.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nuclei segmentation and classification of hematoxylin and eosin-stained histology images is a challenging task due to a variety of issues, such as color inconsistency that results from the non-uniform manual staining operations, clustering of nuclei, and blurry and overlapping nuclei boundaries. Existing approaches involve segmenting nuclei by drawing their polygon representations or by measuring the distances between nuclei centroids . In contrast, we leverage the fact that morphological features (appearance, shape, and texture) of nuclei in a tissue vary greatly depending upon the tissue type. We exploit this information by extracting tissue specific (TS) features from raw histopathology images using the proposed tissue specific feature distillation (TSFD) backbone. The bi-directional feature pyramid network (BiFPN) within TSFD-Net generates a robust hierarchical feature pyramid utilizing TS features where the interlinked decoders jointly optimize and fuse these features to generate final predictions. We also propose a novel combinational loss function for joint optimization and faster convergence of our proposed network. Extensive ablation studies are performed to validate the effectiveness of each component of TSFD-Net. The proposed network outperforms state-of-the-art networks such as StarDist, Micro-Net, Mask-RCNN, Hover-Net, and CPP-Net on the PanNuke dataset, which contains 19 different tissue types and 5 clinically important tumor classes, achieving 50.4\% and 63.77\% mean and binary panoptic quality, respectively. The code is available at: https://github.com/Mr-TalhaIlyas/TSFD .},
  archive      = {J_NN},
  author       = {Talha Ilyas and Zubaer Ibna Mannan and Abbas Khan and Sami Azam and Hyongsuk Kim and Friso De Boer},
  doi          = {10.1016/j.neunet.2022.02.020},
  journal      = {Neural Networks},
  pages        = {1-15},
  shortjournal = {Neural Netw.},
  title        = {TSFD-net: Tissue specific feature distillation network for nuclei segmentation and classification},
  volume       = {151},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022h). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>150</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00130-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00130-7},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Current events. <em>NN</em>, <em>150</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00129-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00129-0},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A zeroing neural dynamics based acceleration optimization
approach for optimizers in deep neural networks. <em>NN</em>,
<em>150</em>, 440–461. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first-order optimizers in deep neural networks (DNN) are of pivotal essence for a concrete loss function to reach the local minimum or global one on the loss surface within convergence time. However, each optimizer possesses its own superiority and virtue when encountering a specific application scene and environment. In addition, the existing modified optimizers mostly emphasize a given optimizer without any transfer property. In this paper, a zeroing neural dynamics (ZND) based optimization approach for the first-order optimizers is proposed, which can assist ZND via the activation function to expedite the process of solving gradient information , with lower loss and higher accuracy. To the best of our knowledge, it is the first work to integrate the ZND in control domain with the first-order optimizers in DNN. This generic work is an optimization method for the most commonly-used first-order optimizers to handle different application scenes, rather than developing a brand-new algorithm besides the existing optimizers or their modifications. Furthermore, mathematic derivations concerning the gradient information transformation of the ZND are systematically provided. Finally, comparison experiments are implemented, which demonstrates the effectiveness of the proposed approach with different loss functions and network frameworks on the Reuters, CIFAR, and MNIST data sets.},
  archive      = {J_NN},
  author       = {Shan Liao and Shubin Li and Jiayong Liu and Haoen Huang and Xiuchun Xiao},
  doi          = {10.1016/j.neunet.2022.03.010},
  journal      = {Neural Networks},
  pages        = {440-461},
  shortjournal = {Neural Netw.},
  title        = {A zeroing neural dynamics based acceleration optimization approach for optimizers in deep neural networks},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting progression of alzheimer’s disease using
forward-to-backward bi-directional network with integrative imputation.
<em>NN</em>, <em>150</em>, 422–439. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If left untreated, Alzheimer’s disease (AD) is a leading cause of slowly progressive dementia. Therefore, it is critical to detect AD to prevent its progression. In this study, we propose a bidirectional progressive recurrent network with imputation (BiPro) that uses longitudinal data , including patient demographics and biomarkers of magnetic resonance imaging (MRI), to forecast clinical diagnoses and phenotypic measurements at multiple timepoints. To compensate for missing observations in the longitudinal data, we use an imputation module to inspect both temporal and multivariate relations associated with the mean and forward relations inherent in the time series data . To encode the imputed information, we define a modification of the long short-term memory (LSTM) cell by using a progressive module to compute the progression score of each biomarker between the given timepoint and the baseline through a negative exponential function. These features are used for the prediction task. The proposed system is an end-to-end deep recurrent network that can accomplish multiple tasks at the same time, including (1) imputing missing values, (2) forecasting phenotypic measurements, and (3) predicting the clinical status of a patient based on longitudinal data. We experimented on 1,335 participants from The Alzheimer’s Disease Prediction of Longitudinal Evolution (TADPOLE) challenge cohort. The proposed method achieved a mean area under the receiver-operating characteristic curve (mAUC) of 78\% for predicting the clinical status of patients, a mean absolute error (MAE) of 3 . 5 m l 3.5ml for forecasting MRI biomarkers, and an MAE of 6 . 9 m l 6.9ml for missing value imputation. The results confirm that our proposed model outperforms prevalent approaches, and can be used to minimize the progression of Alzheimer’s disease.},
  archive      = {J_NN},
  author       = {Ngoc-Huynh Ho and Hyung-Jeong Yang and Jahae Kim and Duy-Phuong Dao and Hyuk-Ro Park and Sudarshan Pant},
  doi          = {10.1016/j.neunet.2022.03.016},
  journal      = {Neural Networks},
  pages        = {422-439},
  shortjournal = {Neural Netw.},
  title        = {Predicting progression of alzheimer’s disease using forward-to-backward bi-directional network with integrative imputation},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective particle attention: Rapidly and flexibly selecting
features for deep reinforcement learning. <em>NN</em>, <em>150</em>,
408–421. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (RL) is often criticised for being data inefficient and inflexible to changes in task structure. Part of the reason for these issues is that Deep RL typically learns end-to-end using backpropagation, which results in task-specific representations. One approach for circumventing these problems is to apply Deep RL to existing representations that have been learned in a more task-agnostic fashion. However, this only partially solves the problem as the Deep RL algorithm learns a function of all pre-existing representations and is therefore still susceptible to data inefficiency and a lack of flexibility. Biological agents appear to solve this problem by forming internal representations over many tasks and only selecting a subset of these features for decision-making based on the task at hand; a process commonly referred to as selective attention. We take inspiration from selective attention in biological agents and propose a novel algorithm called Selective Particle Attention (SPA), which selects subsets of existing representations for Deep RL. Crucially, these subsets are not learned through backpropagation, which is slow and prone to overfitting, but instead via a particle filter that rapidly and flexibly identifies key subsets of features using only reward feedback. We evaluate SPA on two tasks that involve raw pixel input and dynamic changes to the task structure, and show that it greatly increases the efficiency and flexibility of downstream Deep RL algorithms.},
  archive      = {J_NN},
  author       = {Sam Blakeman and Denis Mareschal},
  doi          = {10.1016/j.neunet.2022.03.015},
  journal      = {Neural Networks},
  pages        = {408-421},
  shortjournal = {Neural Netw.},
  title        = {Selective particle attention: Rapidly and flexibly selecting features for deep reinforcement learning},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A BERT based dual-channel explainable text emotion
recognition system. <em>NN</em>, <em>150</em>, 392–407. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel dual-channel system for multi-class text emotion recognition has been proposed, and a novel technique to explain its training &amp; predictions has been developed. The architecture of the proposed system contains the embedding module, dual-channel module, emotion classification module, and explainability module. The embedding module extracts the textual features from the input sentences in the form of embedding vectors using the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model. Then the embedding vectors are fed as the inputs to the dual-channel network containing two network channels made up of convolutional neural network (CNN) and bidirectional long short term memory (BiLSTM) network. The intuition behind using CNN and BiLSTM in both the channels was to harness the goodness of the convolutional layer for feature extraction and the BiLSTM layer to extract text’s order and sequence-related information. The outputs of both channels are in the form of embedding vectors which are concatenated and fed to the emotion classification module. The proposed system’s architecture has been determined by thorough ablation studies, and a framework has been developed to discuss its computational cost. The emotion classification module learns and projects the emotion embeddings on a hyperplane in the form of clusters. The proposed explainability technique explains the training and predictions of the proposed system by analyzing the inter &amp; intra-cluster distances and the intersection of these clusters. The proposed approach’s consistent accuracy, precision, recall, and F1 score results for ISEAR, Aman, AffectiveText, and EmotionLines datasets, ensure its applicability to diverse texts.},
  archive      = {J_NN},
  author       = {Puneet Kumar and Balasubramanian Raman},
  doi          = {10.1016/j.neunet.2022.03.017},
  journal      = {Neural Networks},
  pages        = {392-407},
  shortjournal = {Neural Netw.},
  title        = {A BERT based dual-channel explainable text emotion recognition system},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The passive properties of dendrites modulate the propagation
of slowly-varying firing rate in feedforward networks. <em>NN</em>,
<em>150</em>, 377–391. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The propagation of slowly-varying firing rates has been proved significant for the development of the central nervous system . Recent reports have shown that the membrane passive properties of dendrites play a key role in the computation of the single neuron , which is of great importance to the function of neural networks . However, it is still unclear how dendritic passive properties affect the ability of cortical networks to propagate slowly-varying spiking activity. Here, we use two-compartment biophysical models to construct multilayered feedforward neural networks (FFNs) to investigate how dendritic passive properties affect the propagation of the slow-varying inputs. In the two-compartment biophysical models, one compartment represents apical dendrites , and the other compartment describes the soma plus the axon initial segment. Area proportion occupied by somatic compartment and coupling conductance between dendritic and somatic compartments are abstracted to capture the dendritic passive properties. A time-varying signal is injected into the first layer of the FFNs and the fidelity of the signal during propagation is used to qualify the ability of the FFN to transmit wave-like signals. Numerical results reveal an optimal value of coupling conductance between dendritic and somatic compartments to maximize the fidelity of the initial spiking activity. An increase of the dendritic area enhances the initial firing rate of neurons in the first layer by increasing the response of neurons to slow-varying wave-like input, resulting in a delay of attenuation of the firing rate, thus promoting the transmission of signals in FFN. Using a mean-field approach, we examine that changes in area proportion occupied by somatic compartment and coupling conductance between dendritic and somatic compartment affect the signal propagation ability of the FFN by adjusting the input–output transform of a single neuron . With the participation of external noise, a wide range of initial firing rates maintains a unique representation during propagation, which ensures the reliable transmission of slow-varying signals in FFNs. These findings are helpful to understand how passive properties of dendrites participate in the propagation of slowly varying signals in the cerebellum .},
  archive      = {J_NN},
  author       = {Tianshi Gao and Bin Deng and Jixuan Wang and Jiang Wang and Guosheng Yi},
  doi          = {10.1016/j.neunet.2022.03.001},
  journal      = {Neural Networks},
  pages        = {377-391},
  shortjournal = {Neural Netw.},
  title        = {The passive properties of dendrites modulate the propagation of slowly-varying firing rate in feedforward networks},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All by myself: Learning individualized competitive behavior
with a contrastive reinforcement learning optimization. <em>NN</em>,
<em>150</em>, 364–376. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a competitive game scenario, a set of agents have to learn decisions that maximize their goals and minimize their adversaries’ goals at the same time. Besides dealing with the increased dynamics of the scenarios due to the opponents’ actions, they usually have to understand how to overcome the opponent’s strategies. Most of the common solutions, usually based on continual learning or centralized multi-agent experiences, however, do not allow the development of personalized strategies to face individual opponents. In this paper, we propose a novel model composed of three neural layers that learn a representation of a competitive game, learn how to map the strategy of specific opponents, and how to disrupt them. The entire model is trained online, using a composed loss based on a contrastive optimization, to learn competitive and multiplayer games. We evaluate our model on a pokemon duel scenario and the four-player competitive Chef’s Hat card game. Our experiments demonstrate that our model achieves better performance when playing against offline, online, and competitive-specific models, in particular when playing against the same opponent multiple times. We also present a discussion on the impact of our model, in particular on how well it deals with on specific strategy learning for each of the two scenarios.},
  archive      = {J_NN},
  author       = {Pablo Barros and Alessandra Sciutti},
  doi          = {10.1016/j.neunet.2022.03.013},
  journal      = {Neural Networks},
  pages        = {364-376},
  shortjournal = {Neural Netw.},
  title        = {All by myself: Learning individualized competitive behavior with a contrastive reinforcement learning optimization},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compression of deep neural networks based on quantized
tensor decomposition to implement on reconfigurable hardware platforms.
<em>NN</em>, <em>150</em>, 350–363. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have been vastly and successfully employed in various artificial intelligence and machine learning applications (e.g., image processing and natural language processing). As DNNs become deeper and enclose more filters per layer, they incur high computational costs and large memory consumption to preserve their large number of parameters. Moreover, present processing platforms (e.g., CPU , GPU, and FPGA) have not enough internal memory, and hence external memory storage is needed. Hence deploying DNNs on mobile applications is difficult, considering the limited storage space, computation power, energy supply, and real-time processing requirements. In this work, using a method based on tensor decomposition , network parameters were compressed, thereby reducing access to external memory. This compression method decomposes the network layers’ weight tensor into a limited number of principal vectors such that (i) almost all the initial parameters can be retrieved, (ii) the network structure did not change, and (iii) the network quality after reproducing the parameters was almost similar to the original network in terms of detection accuracy. To optimize the realization of this method on FPGA , the tensor decomposition algorithm was modified while its convergence was not affected, and the reproduction of network parameters on FPGA was straightforward. The proposed algorithm reduced the parameters of ResNet50 , VGG16, and VGG19 networks trained with Cifar10 and Cifar100 by almost 10 times.},
  archive      = {J_NN},
  author       = {Amirreza Nekooei and Saeed Safari},
  doi          = {10.1016/j.neunet.2022.02.024},
  journal      = {Neural Networks},
  pages        = {350-363},
  shortjournal = {Neural Netw.},
  title        = {Compression of deep neural networks based on quantized tensor decomposition to implement on reconfigurable hardware platforms},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel projection neurodynamic approaches for constrained
convex optimization. <em>NN</em>, <em>150</em>, 336–349. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider that the constrained convex optimization problems have emerged in a variety of scientific and engineering applications that often require efficient and fast solutions. Inspired by the Nesterov’s accelerated method for solving unconstrained convex and strongly convex optimization problems , in this paper we propose two novel accelerated projection neurodynamic approaches for constrained smooth convex and strongly convex optimization based on the variational approach . First, for smooth, and convex optimization problems , a non-autonomous accelerated projection neurodynamic approach (NAAPNA) is presented and the existence, uniqueness and feasibility of the solution to it are analyzed rigorously. We provide that the NAAPNA has a convergence rate which is inversely proportional to the square of the running time. In addition, we present a novel autonomous accelerated projection neurodynamic approach (AAPNA) for addressing the constrained, smooth, strongly convex optimization problems and prove the existence, uniqueness to the strong global solution of AAPNA based on the Cauchy–Lipschitz–Picard theorem. Furthermore, we also prove the global convergence of AAPNA with different exponential convergence rates for different parameters. Compared with existing projection neurodynamic approaches based on the Brouwer’s fixed point theorem , both NAAPNA and AAPNA use the projection operators of the auxiliary variable to map the primal variables to the constrained feasible region, thus our proposed neurodynamic approaches are easier to realize algorithm’s acceleration. Finally, the effectiveness of NAAPNA and AAPNA is illustrated with several numerical examples.},
  archive      = {J_NN},
  author       = {You Zhao and Xiaofeng Liao and Xing He},
  doi          = {10.1016/j.neunet.2022.03.011},
  journal      = {Neural Networks},
  pages        = {336-349},
  shortjournal = {Neural Netw.},
  title        = {Novel projection neurodynamic approaches for constrained convex optimization},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient learning rate adaptation based on hierarchical
optimization approach. <em>NN</em>, <em>150</em>, 326–335. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new hierarchical approach to learning rate adaptation in gradient methods , called learning rate optimization (LRO). LRO formulates the learning rate adaption problem as a hierarchical optimization problem that minimizes the loss function with respect to the learning rate for current model parameters and gradients. Then, LRO optimizes the learning rate based on the alternating direction method of multipliers (ADMM). In the process of this learning rate optimization, LRO does not require any second-order information and probabilistic model, so it is highly efficient. Furthermore, LRO does not require any additional hyperparameters when compared to the vanilla gradient method with the simple exponential learning rate decay. In the experiments, we integrated LRO with vanilla SGD and Adam. Then, we compared their optimization performance with the state-of-the-art learning rate adaptation methods and also the most commonly-used adaptive gradient methods. The SGD and Adam with LRO outperformed all the competitors on the benchmark datasets in image classification tasks .},
  archive      = {J_NN},
  author       = {Gyoung S. Na},
  doi          = {10.1016/j.neunet.2022.02.014},
  journal      = {Neural Networks},
  pages        = {326-335},
  shortjournal = {Neural Netw.},
  title        = {Efficient learning rate adaptation based on hierarchical optimization approach},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep feature fusion based childhood epilepsy syndrome
classification from electroencephalogram. <em>NN</em>, <em>150</em>,
313–325. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of the children’s epilepsy syndrome is vital to the diagnosis and treatment of epilepsy . But existing literature mainly focuses on seizure detection and few attention has been paid to the children’s epilepsy syndrome classification. In this paper, we present a study on the classification of two most common epilepsy syndromes: the benign childhood epilepsy with centro-temporal spikes (BECT) and the infantile spasms (also known as the WEST syndrome), recorded from the Children’s Hospital, Zhejiang University School of Medicine (CHZU). A novel feature fusion model based on the deep transfer learning and the conventional time–frequency representation of the scalp electroencephalogram (EEG) is developed for the epilepsy syndrome characterization. A fully connected network is constructed for the feature learning and syndrome classification. Experiments on the CHZU database show that the proposed algorithm can offer an average of 92.35\% classification accuracy on the BECT and WEST syndromes and their corresponding normal cases.},
  archive      = {J_NN},
  author       = {Xiaonan Cui and Dinghan Hu and Peng Lin and Jiuwen Cao and Xiaoping Lai and Tianlei Wang and Tiejia Jiang and Feng Gao},
  doi          = {10.1016/j.neunet.2022.03.014},
  journal      = {Neural Networks},
  pages        = {313-325},
  shortjournal = {Neural Netw.},
  title        = {Deep feature fusion based childhood epilepsy syndrome classification from electroencephalogram},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A whole brain probabilistic generative model: Toward
realizing cognitive architectures for developmental robots. <em>NN</em>,
<em>150</em>, 293–312. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building a human-like integrative artificial cognitive system, that is, an artificial general intelligence (AGI), is the holy grail of the artificial intelligence (AI) field. Furthermore, a computational model that enables an artificial system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes an approach to develop a cognitive architecture by integrating elemental cognitive modules to enable the training of the modules as a whole. This approach is based on two ideas: (1) brain-inspired AI, learning human brain architecture to build human-level intelligence, and (2) a probabilistic generative model (PGM)-based cognitive architecture to develop a cognitive system for developmental robots by integrating PGMs. The proposed development framework is called a whole brain PGM (WB-PGM), which differs fundamentally from existing cognitive architectures in that it can learn continuously through a system based on sensory-motor information. In this paper, we describe the rationale for WB-PGM, the current status of PGM-based elemental cognitive modules, their relationship with the human brain, the approach to the integration of the cognitive modules, and future challenges. Our findings can serve as a reference for brain studies. As PGMs describe explicit informational relationships between variables, WB-PGM provides interpretable guidance from computational sciences to brain science. By providing such information, researchers in neuroscience can provide feedback to researchers in AI and robotics on what the current models lack with reference to the brain. Further, it can facilitate collaboration among researchers in neuro-cognitive sciences as well as AI and robotics.},
  archive      = {J_NN},
  author       = {Tadahiro Taniguchi and Hiroshi Yamakawa and Takayuki Nagai and Kenji Doya and Masamichi Sakagami and Masahiro Suzuki and Tomoaki Nakamura and Akira Taniguchi},
  doi          = {10.1016/j.neunet.2022.02.026},
  journal      = {Neural Networks},
  pages        = {293-312},
  shortjournal = {Neural Netw.},
  title        = {A whole brain probabilistic generative model: Toward realizing cognitive architectures for developmental robots},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A self-learning cognitive architecture exploiting causality
from rewards. <em>NN</em>, <em>150</em>, 274–292. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the human vision system and learning, we propose a novel cognitive architecture that understands the content of raw videos in terms of objects without using labels. The architecture achieves four objectives: (1) Decomposing raw frames in objects by exploiting foveal vision and memory. (2) Describing the world by projecting objects on an internal canvas. (3) Extracting relevant objects from the canvas by analyzing the causal relation between objects and rewards. (4) Exploiting the information of relevant objects to facilitate the reinforcement learning (RL) process. In order to speed up learning, and better identify objects that produce rewards, the architecture implements learning by causality from the perspective of Wiener and Granger using object trajectories stored in working memory and the time series of external rewards. A novel non-parametric estimator of directed information using Renyi’s entropy is designed and tested. Experiments on three environments show that our architecture extracts most of relevant objects. It can be thought of as ‘understanding’ the world in an object-oriented way. As a consequence, our architecture outperforms state-of-the-art deep reinforcement learning in terms of training speed and transfer learning .},
  archive      = {J_NN},
  author       = {Hongming Li and Ran Dou and Andreas Keil and Jose C. Principe},
  doi          = {10.1016/j.neunet.2022.02.029},
  journal      = {Neural Networks},
  pages        = {274-292},
  shortjournal = {Neural Netw.},
  title        = {A self-learning cognitive architecture exploiting causality from rewards},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universality of gradient descent neural network training.
<em>NN</em>, <em>150</em>, 259–273. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been observed that design choices of neural networks are often crucial for their successful optimization. In this article, we therefore discuss the question if it is always possible to redesign a neural network so that it trains well with gradient descent . This yields the following universality result: If, for a given network, there is any algorithm that can find good network weights for a classification task , then there exists an extension of this network that reproduces the same forward model by mere gradient descent training. The construction is not intended for practical computations, but it provides some orientation on the possibilities of pre-trained networks in meta-learning and related approaches.},
  archive      = {J_NN},
  author       = {G. Welper},
  doi          = {10.1016/j.neunet.2022.02.016},
  journal      = {Neural Networks},
  pages        = {259-273},
  shortjournal = {Neural Netw.},
  title        = {Universality of gradient descent neural network training},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A differential hebbian framework for biologically-plausible
motor control. <em>NN</em>, <em>150</em>, 237–258. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we explore a neural control architecture that is both biologically plausible, and capable of fully autonomous learning. It consists of feedback controllers that learn to achieve a desired state by selecting the errors that should drive them. This selection happens through a family of differential Hebbian learning rules that, through interaction with the environment, can learn to control systems where the error responds monotonically to the control signal. We next show that in a more general case, neural reinforcement learning can be coupled with a feedback controller to reduce errors that arise non-monotonically from the control signal. The use of feedback control can reduce the complexity of the reinforcement learning problem, because only a desired value must be learned, with the controller handling the details of how it is reached. This makes the function to be learned simpler, potentially allowing learning of more complex actions. We use simple examples to illustrate our approach, and discuss how it could be extended to hierarchical architectures.},
  archive      = {J_NN},
  author       = {Sergio Verduzco-Flores and William Dorrell and Erik De Schutter},
  doi          = {10.1016/j.neunet.2022.03.002},
  journal      = {Neural Networks},
  pages        = {237-258},
  shortjournal = {Neural Netw.},
  title        = {A differential hebbian framework for biologically-plausible motor control},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning online visual invariances for novel objects via
supervised and self-supervised training. <em>NN</em>, <em>150</em>,
222–236. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can identify objects following various spatial transformations such as scale and viewpoint. This extends to novel objects, after a single presentation at a single pose, sometimes referred to as online invariance. CNNs have been proposed as a compelling model of human vision, but their ability to identify objects across transformations is typically tested on held-out samples of trained categories after extensive data augmentation . This paper assesses whether standard CNNs can support human-like online invariance by training models to recognize images of synthetic 3D objects that undergo several transformations: rotation, scaling, translation, brightness, contrast, and viewpoint. Through the analysis of models’ internal representations, we show that standard supervised CNNs trained on transformed objects can acquire strong invariances on novel classes even when trained with as few as 50 objects taken from 10 classes. This extended to a different dataset of photographs of real objects. We also show that these invariances can be acquired in a self-supervised way, through solving the same/different task. We suggest that this latter approach may be similar to how humans acquire invariances.},
  archive      = {J_NN},
  author       = {Valerio Biscione and Jeffrey S. Bowers},
  doi          = {10.1016/j.neunet.2022.02.017},
  journal      = {Neural Networks},
  pages        = {222-236},
  shortjournal = {Neural Netw.},
  title        = {Learning online visual invariances for novel objects via supervised and self-supervised training},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered delayed impulsive control for nonlinear
systems with application to complex neural networks. <em>NN</em>,
<em>150</em>, 213–221. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the Lyapunov stability of nonlinear systems and the synchronization of complex neural networks in the framework of event-triggered delayed impulsive control ( ETDIC ), where the effect of time delays in impulses is fully considered. Based on the Lyapunov-based event-triggered mechanism ( ETM ), some sufficient conditions are presented to avoid Zeno behavior and achieve globally asymptotical stability of the addressed system. In the framework of event-triggered impulse control ( ETIC ), control input is only generated at state-dependent triggered instants and there is no any control input during two consecutive triggered impulse instants, which can greatly reduce resource consumption and control waste. The contributions of this paper can be summarized as follows: Firstly, compared with the classical ETIC , our results not only provide the well-designed ETM to determine the impulse time sequence, but also fully extract the information of time delays in impulses and integrate it into the dynamic analysis of the system. Secondly, it is shown that the time delays in impulses in our results exhibit positive effects, that is, it may contribute to stabilizing a system and achieve better performance. Thirdly, as an application of ETDIC strategies, we apply the proposed theoretical results to synchronization problem of complex neural networks . Some sufficient conditions to ensure the synchronization of complex neural networks are presented, where the information of time delays in impulses is fully fetched in these conditions. Finally, two numerical examples are provided to show the effectiveness and validity of the theoretical results.},
  archive      = {J_NN},
  author       = {Mingzhu Wang and Xiaodi Li and Peiyong Duan},
  doi          = {10.1016/j.neunet.2022.03.007},
  journal      = {Neural Networks},
  pages        = {213-221},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered delayed impulsive control for nonlinear systems with application to complex neural networks},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel ramp loss-based multi-task twin support vector
machine with multi-parameter safe acceleration. <em>NN</em>,
<em>150</em>, 194–212. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct multi-task twin support vector machine (DMTSVM) is an effective algorithm to deal with multi-task classification problems. However, the generated hyperplane may shift to outliers since the hinge loss is used in DMTSVM. Therefore, we propose an improved multi-task model RaMTTSVM based on ramp loss to handle noisy points more effectively. It could limit the maximal loss value distinctly and put definite restrictions on the influences of noises. But RaMTTSVM is non-convex which should be solved by CCCP, then a series of approximate convex problems need to be solved. So, it may be time-consuming. Motivated by the sparse solution of our RaMTTSVM, we further propose a safe acceleration rule MSA to accelerate the solving speed. Based on optimality conditions and convex optimization theory, MSA could delete a lot of inactive samples corresponding to 0 elements in dual solutions before solving the model. Then the computation speed can be accelerated by just solving reduced problems . The rule contains three different parts that correspond to different parameters and different iteration phases of CCCP. It can be used not only for the first approximate convex problem of CCCP but also for the successive problems during the iteration process . More importantly, our MSA is safe in the sense that the reduced problem can derive an identical optimal solution as the original problem, so the prediction accuracy will not be disturbed. Experimental results on one artificial dataset, ten Benchmark datasets, ten Image datasets and one real wine dataset confirm the generalization and acceleration ability of our proposed algorithm.},
  archive      = {J_NN},
  author       = {Xinying Pang and Jiang Zhao and Yitian Xu},
  doi          = {10.1016/j.neunet.2022.03.006},
  journal      = {Neural Networks},
  pages        = {194-212},
  shortjournal = {Neural Netw.},
  title        = {A novel ramp loss-based multi-task twin support vector machine with multi-parameter safe acceleration},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronization and state estimation for discrete-time
coupled delayed complex-valued neural networks with random system
parameters. <em>NN</em>, <em>150</em>, 181–193. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an array of discrete-time coupled complex-valued neural networks (CVNNs) with random system parameters and time-varying delays are introduced. The stochastic fluctuations of system parameters, which are characterized by a set of random variables , are considered in the individual CVNNs. Firstly, the synchronization issue is solved for the considered coupled CVNNs. By the use of the Lyapunov stability theory and the Kronecker product , a synchronization criterion is proposed to guarantee that the coupled CVNNs are asymptotically synchronized in the mean square . Subsequently, the state estimation issue is studied for the identical coupled CVNNs via available measurement output. By establishing a suitable Lyapunov functional , sufficient conditions are derived under which the mean square asymptotic stability of the estimation error system is ensured and the design scheme of desired state estimator is explicitly provided. Finally, two numerical simulation examples are shown for the purpose of illustrating the effectiveness of the proposed theory.},
  archive      = {J_NN},
  author       = {Yufei Liu and Bo Shen and Ping Zhang},
  doi          = {10.1016/j.neunet.2022.02.028},
  journal      = {Neural Networks},
  pages        = {181-193},
  shortjournal = {Neural Netw.},
  title        = {Synchronization and state estimation for discrete-time coupled delayed complex-valued neural networks with random system parameters},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifelong 3D object recognition and grasp synthesis using
dual memory recurrent self-organization networks. <em>NN</em>,
<em>150</em>, 167–180. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans learn to recognize and manipulate new objects in lifelong settings without forgetting the previously gained knowledge under non-stationary and sequential conditions. In autonomous systems , the agents also need to mitigate similar behaviour to continually learn the new object categories and adapt to new environments. In most conventional deep neural networks , this is not possible due to the problem of catastrophic forgetting, where the newly gained knowledge overwrites existing representations. Furthermore, most state-of-the-art models excel either in recognizing the objects or in grasp prediction, while both tasks use visual input. The combined architecture to tackle both tasks is very limited. In this paper, we proposed a hybrid model architecture consists of a dynamically growing dual-memory recurrent neural network (GDM) and an autoencoder to tackle object recognition and grasping simultaneously. The autoencoder network is responsible to extract a compact representation for a given object, which serves as input for the GDM learning, and is responsible to predict pixel-wise antipodal grasp configurations. The GDM part is designed to recognize the object in both instances and categories levels. We address the problem of catastrophic forgetting using the intrinsic memory replay, where the episodic memory periodically replays the neural activation trajectories in the absence of external sensory information. To extensively evaluate the proposed model in a lifelong setting, we generate a synthetic dataset due to lack of sequential 3D objects dataset . Experiment results demonstrated that the proposed model can learn both object representation and grasping simultaneously in continual learning scenarios.},
  archive      = {J_NN},
  author       = {Krishnakumar Santhakumar and Hamidreza Kasaei},
  doi          = {10.1016/j.neunet.2022.02.027},
  journal      = {Neural Networks},
  pages        = {167-180},
  shortjournal = {Neural Netw.},
  title        = {Lifelong 3D object recognition and grasp synthesis using dual memory recurrent self-organization networks},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented graph neural network with hierarchical
global-based residual connections. <em>NN</em>, <em>150</em>, 149–166.
(<a href="https://doi.org/10.1016/j.neunet.2022.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are powerful architectures for learning on graphs. They are efficient for predicting nodes, links and graphs properties. Standard GNN variants follow a message passing schema to update nodes representations using information from higher-order neighborhoods iteratively. Consequently, deeper GNNs make it possible to define high-level nodes representations generated based on local as well as distant neighborhoods. However, deeper networks are prone to suffer from over-smoothing. To build deeper GNN architectures and avoid losing the dependency between lower (the layers closer to the input) and higher (the layers closer to the output) layers, networks can integrate residual connections to connect intermediate layers. We propose the Augmented Graph Neural Network (AGNN) model with hierarchical global-based residual connections. Using the proposed residual connections, the model generates high-level nodes representations without the need for a deeper architecture. We disclose that the nodes representations generated through our proposed AGNN model are able to define an expressive all-encompassing representation of the entire graph. As such, the graph predictions generated through the AGNN model surpass considerably state-of-the-art results. Moreover, we carry out extensive experiments to identify the best global pooling strategy and attention weights to define the adequate hierarchical and global-based residual connections for different graph property prediction tasks. Furthermore, we propose a reversible variant of the AGNN model to address the extensive memory consumption problem that typically arises from training networks on large and dense graph datasets. The proposed Reversible Augmented Graph Neural Network (R-AGNN) only stores the nodes representations acquired from the output layer as opposed to saving all representations from intermediate layers as it is conventionally done when optimizing the parameters of other GNNs. We further refine the definition of the backpropagation algorithm to fit the R-AGNN model. We evaluate the proposed models AGNN and R-AGNN on benchmark Molecular, Bioinformatics and Social Networks datasets for graph classification and achieve state-of-the-art results. For instance the AGNN model realizes improvements of + 39\% +39\% on IMDB-MULTI reaching 91.7\% accuracy and + 16\% +16\% on COLLAB reaching 96.8\% accuracy compared to other GNN variants.},
  archive      = {J_NN},
  author       = {Asmaa Rassil and Hiba Chougrad and Hamid Zouaki},
  doi          = {10.1016/j.neunet.2022.03.008},
  journal      = {Neural Networks},
  pages        = {149-166},
  shortjournal = {Neural Netw.},
  title        = {Augmented graph neural network with hierarchical global-based residual connections},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Connectome of memristive nanowire networks through graph
theory. <em>NN</em>, <em>150</em>, 137–148. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware implementation of neural networks represents a milestone for exploiting the advantages of neuromorphic-type data processing and for making use of the inherent parallelism associated with such structures. In this context, memristive devices with their analogue functionalities are called to be promising building blocks for the hardware realization of artificial neural networks . As an alternative to conventional crossbar architectures where memristive devices are organized with a top-down approach in a grid-like fashion, neuromorphic-type data processing and computing capabilities have been explored in networks realized according to the principle of self-organization similarity found in biological neural networks. Here, we explore structural and functional connectivity of self-organized memristive nanowire (NW) networks within the theoretical framework of graph theory. While graph metrics reveal the link of the graph theoretical approach with geometrical considerations, results show that the interplay between network structure and its capacity to transmit information is related to a phase transition process consistent with percolation theory . Also the concept of memristive distance is introduced to investigate activation patterns and the dynamic evolution of the information flow across the network represented as a memristive graph . In agreement with experimental results, the emergent short-term dynamics reveals the formation of self-selected pathways with enhanced transport characteristics connecting stimulated areas and regulating the trafficking of the information flow. The network capability to process spatio-temporal input signals can be exploited for the implementation of unconventional computing paradigms in memristive graphs that take into advantage the inherent relationship between structure and functionality as in biological systems.},
  archive      = {J_NN},
  author       = {Gianluca Milano and Enrique Miranda and Carlo Ricciardi},
  doi          = {10.1016/j.neunet.2022.02.022},
  journal      = {Neural Networks},
  pages        = {137-148},
  shortjournal = {Neural Netw.},
  title        = {Connectome of memristive nanowire networks through graph theory},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SelfVIO: Self-supervised deep monocular visual–inertial
odometry and depth estimation. <em>NN</em>, <em>150</em>, 119–136. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, numerous supervised deep learning approaches have been proposed for visual–inertial odometry (VIO) and depth map estimation, which require large amounts of labelled data. To overcome the data limitation, self-supervised learning has emerged as a promising alternative that exploits constraints such as geometric and photometric consistency in the scene. In this study, we present a novel self-supervised deep learning-based VIO and depth map recovery approach (SelfVIO) using adversarial training and self-adaptive visual–inertial sensor fusion. SelfVIO learns the joint estimation of 6 degrees-of-freedom (6-DoF) ego-motion and a depth map of the scene from unlabelled monocular RGB image sequences and inertial measurement unit (IMU) readings. The proposed approach is able to perform VIO without requiring IMU intrinsic parameters and/or extrinsic calibration between IMU and the camera. We provide comprehensive quantitative and qualitative evaluations of the proposed framework and compare its performance with state-of-the-art VIO, VO , and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI, EuRoC and Cityscapes datasets. Detailed comparisons prove that SelfVIO outperforms state-of-the-art VIO approaches in terms of pose estimation and depth recovery, making it a promising approach among existing methods in the literature.},
  archive      = {J_NN},
  author       = {Yasin Almalioglu and Mehmet Turan and Muhamad Risqi U. Saputra and Pedro P.B. de Gusmão and Andrew Markham and Niki Trigoni},
  doi          = {10.1016/j.neunet.2022.03.005},
  journal      = {Neural Networks},
  pages        = {119-136},
  shortjournal = {Neural Netw.},
  title        = {SelfVIO: Self-supervised deep monocular Visual–Inertial odometry and depth estimation},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attributes learning network for generalized zero-shot
learning. <em>NN</em>, <em>150</em>, 112–118. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of unseen training data, zero-shot learning algorithms utilize the semantic knowledge shared by the seen and unseen classes to establish the connection between the visual space and the semantic space, so as to realize the recognition of the unseen classes. However, in real applications, the original semantic representation cannot well characterize both the class-specificity structure and discriminative information in dimension space, which leads to unseen classes being easily misclassified into seen classes. To tackle this problem, we propose a Salient Attributes Learning Network (SALN) to generate discriminative and expressive semantic representation under the supervision of the visual features. Meanwhile, ℓ 1 , 2 -norm constraint is employed to make the learned semantic representation well characterize the class-specificity structure and discriminative information in dimension space. Then feature alignment network projects the learned semantic representation into visual space and a relation network is adopted for classification. The performance of the proposed approach has made progress on the five benchmark datasets in generalized zero-shot learning task, and in-depth experiments indicate the effectiveness and excellence of our method.},
  archive      = {J_NN},
  author       = {Yu Yun and Sen Wang and Mingzhen Hou and Quanxue Gao},
  doi          = {10.1016/j.neunet.2022.02.018},
  journal      = {Neural Networks},
  pages        = {112-118},
  shortjournal = {Neural Netw.},
  title        = {Attributes learning network for generalized zero-shot learning},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust multi-view subspace clustering based on consensus
representation and orthogonal diversity. <em>NN</em>, <em>150</em>,
102–111. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of multi-view subspace clustering is to reveal the intrinsic low-dimensional architecture of data points according to their multi-view characteristics. Exploring the potential relationship from views is one of the most essential research focuses of the multi-view task. To better utilize the complementary and consistency information from distinct views, we propose a novel robust subspace clustering approach based on consensus representation and orthogonal diversity (RMSCCO). A novel defined orthogonality term is adopted to improve the diversity and decrease the redundance of learning subspace representation. The consensus representation and subspace learning are integrated into one unified framework to characterize the consistency from views. The grouping-enhanced representation is utilized to maintain the local geometric architecture in the original data space. The ℓ 2 , 1 ℓ2,1 -norm regularizer constraint to the noise is applied to improve the robustness. Finally, an optimization algorithm is exploited to solve RMSCCO with the Alternating Direction Method of Multipliers (ADMM). Extensive experimental results on six challenging datasets demonstrate that our approach has accomplished highly qualified performance.},
  archive      = {J_NN},
  author       = {Nan Zhao and Jie Bu},
  doi          = {10.1016/j.neunet.2022.03.009},
  journal      = {Neural Networks},
  pages        = {102-111},
  shortjournal = {Neural Netw.},
  title        = {Robust multi-view subspace clustering based on consensus representation and orthogonal diversity},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised feature selection via adaptive autoencoder with
redundancy control. <em>NN</em>, <em>150</em>, 87–101. (<a
href="https://doi.org/10.1016/j.neunet.2022.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is one of the efficient approaches to reduce the dimension of unlabeled high-dimensional data. We present a novel adaptive autoencoder with redundancy control (AARC) as an unsupervised feature selector. By adding two Group Lasso penalties to the objective function, AARC integrates unsupervised feature selection and determination of a compact network structure into a single framework. Besides, a penalty based on a measure of dependency between features (such as Pearson correlation, mutual information) is added to the objective function for controlling the level of redundancy in the selected features. To realize the desired effects of different regularizers in different phases of the training, we introduce adaptive parameters which change with iterations. In addition, a smoothing function is utilized to approximate the three penalties since they are not differentiable at the origin. An ablation study is carried out to validate the capabilities of redundancy control and structure optimization of AARC. Subsequently, comparisons with nine state-of-the-art methods illustrate the efficiency of AARC for unsupervised feature selection.},
  archive      = {J_NN},
  author       = {Xiaoling Gong and Ling Yu and Jian Wang and Kai Zhang and Xiao Bai and Nikhil R. Pal},
  doi          = {10.1016/j.neunet.2022.03.004},
  journal      = {Neural Networks},
  pages        = {87-101},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised feature selection via adaptive autoencoder with redundancy control},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-aware hierarchical segment-channel attention
mechanism for reliable and interpretable multichannel signal
classification. <em>NN</em>, <em>150</em>, 68–86. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel signal data analysis has been crucial in various industrial applications, such as human activity recognition , vehicle failure predictions, and manufacturing equipment monitoring. Recently, deep neural networks have come into use for multichannel signal data because of their ability to automatically extract useful features from complex multichannel signals. However, deep neural networks are black-box models whose internal working mechanisms cannot be put in a form readily understood by humans. To address this issue, we have proposed an uncertainty-aware hierarchical segment-channel attention model that consists of a time segment and channel level attentions. The hierarchical attention mechanism enables a neural network to identify important time segments and channels critical for prediction, making the model explainable. In addition, the model uses variational inferences to provide uncertainty information that yields a confidence interval that can be easily explained. We conducted experiments on simulated and real-world datasets to demonstrate the usefulness and applicability of our method. The results confirm that our method can attend to important time segments and sensors while achieving better classification performance.},
  archive      = {J_NN},
  author       = {Jiyoon Lee and Seoung Bum Kim},
  doi          = {10.1016/j.neunet.2022.02.019},
  journal      = {Neural Networks},
  pages        = {68-86},
  shortjournal = {Neural Netw.},
  title        = {Uncertainty-aware hierarchical segment-channel attention mechanism for reliable and interpretable multichannel signal classification},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting the transferability of adversarial examples via
stochastic serial attack. <em>NN</em>, <em>150</em>, 58–67. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial examples , which are crafted by imposing mild perturbation on clean ones. An intriguing property of adversarial examples is that they are efficient among different DNNs. Thus transfer-based attacks against DNNs become an increasing concern. In this scenario, attackers devise adversarial instances based on the local model without feedback information from the target one. Unfortunately, most existing transfer-based attack methods only employ a single local model to generate adversarial examples. It results in poor transferability because of overfitting to the local model. Although several ensemble attacks have been proposed, the transferability of adversarial examples merely have a slight increase. Meanwhile, these methods need high memory cost during the training process. To this end, we propose a novel attack strategy called stochastic serial attack (SSA). It adopts a serial strategy to attack local models, which reduces memory consumption compared to parallel attacks. Moreover, since local models are stochastically selected from a large model set, SSA can ensure that the adversarial examples do not overfit specific weaknesses of local source models. Extensive experiments on the ImageNet dataset and NeurIPS 2017 adversarial competition dataset show the effectiveness of SSA in improving the transferability of adversarial examples and reducing the memory consumption of the training process.},
  archive      = {J_NN},
  author       = {Lingguang Hao and Kuangrong Hao and Bing Wei and Xue-song Tang},
  doi          = {10.1016/j.neunet.2022.02.025},
  journal      = {Neural Networks},
  pages        = {58-67},
  shortjournal = {Neural Netw.},
  title        = {Boosting the transferability of adversarial examples via stochastic serial attack},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quasi-synchronization of fractional-order multi-layer
networks with mismatched parameters via delay-dependent impulsive
feedback control. <em>NN</em>, <em>150</em>, 43–57. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper is devoted to investigating the quasi-synchronization issue of fractional-order multi-layer networks with mismatched parameters under delay-dependent impulsive feedback control . It is worth highlighting that fractional-order multi-layer networks with mismatched parameters, as the extension model for single-layer or two-layer ones, are constructed in this paper. Simultaneously, the intra-layer and inter-layer couplings are taken into consideration, which is more general and rarely considered in discussions of network synchronization . An extended fractional differential inequality with impulsive effects is given to establish the grounded framework and theory on the quasi-synchronization problem under delay-dependent impulsive feedback control . Moreover, in the light of the Lyapunov method and graph theory, two criteria for achieving the quasi-synchronization of fractional-order multi-layer networks with mismatched parameters are derived. Furthermore, exponential convergence rates as well as the bounds of quasi-synchronization errors are successfully deduced. Ultimately, the theoretical results are applied in a practical power system , and some illustrative examples are proposed to show the effectiveness of theoretical analysis.},
  archive      = {J_NN},
  author       = {Yao Xu and Jingjing Liu and Wenxue Li},
  doi          = {10.1016/j.neunet.2022.02.023},
  journal      = {Neural Networks},
  pages        = {43-57},
  shortjournal = {Neural Netw.},
  title        = {Quasi-synchronization of fractional-order multi-layer networks with mismatched parameters via delay-dependent impulsive feedback control},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage streaming keyword detection and localization with
multi-scale depthwise temporal convolution. <em>NN</em>, <em>150</em>,
28–42. (<a href="https://doi.org/10.1016/j.neunet.2022.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A keyword spotting (KWS) system running on smart devices should accurately detect the appearances and predict the locations of predefined keywords from audio streams , with small footprint and high efficiency. To this end, this paper proposes a new two-stage KWS method which combines a novel multi-scale depthwise temporal convolution (MDTC) feature extractor and a two-stage keyword detection and localization module. The MDTC feature extractor learns multi-scale feature representation efficiently with dilated depthwise temporal convolution, modeling both the temporal context and the speech rate variation. We use a region proposal network (RPN) as the first-stage KWS. At each frame, we design multiple time regions, which all take the current frame as the end position but have different start positions. These time regions (or formally anchors ) are used to indicate rough location candidates of keyword. With frame level features from the MDTC feature extractor as inputs, RPN learns to propose keyword region proposals based on the designed anchors. To alleviate the keyword/non-keyword class imbalance problem , we specifically introduce a hard example mining algorithm to select effective negative anchors in RPN training. The keyword region proposals from the first-stage RPN contain keyword location information which is subsequently used to explicitly extract keyword related sequential features to train the second-stage KWS. The second-stage system learns to classify and transform region proposal to keyword IDs and ground-truth keyword region respectively. Experiments on the Google Speech Command dataset show that the proposed MDTC feature extractor surpasses several competitive feature extractors with a new state-of-the-art command classification error rate of 1 . 74\% 1.74\% . With the MDTC feature extractor, we further conduct wake-up word (WuW) detection and localization experiments on a commercial WuW dataset. Compared to a strong baseline, our proposed two-stage method achieves relatively 27–32\% better false rejection rate at one false alarm per hour, while for keyword localization, the two-stage approach achieves more than 0 . 95 0.95 mean intersection-over-union ratio, which is clearly better than the one-stage RPN method.},
  archive      = {J_NN},
  author       = {Jingyong Hou and Lei Xie and Shilei Zhang},
  doi          = {10.1016/j.neunet.2022.03.003},
  journal      = {Neural Networks},
  pages        = {28-42},
  shortjournal = {Neural Netw.},
  title        = {Two-stage streaming keyword detection and localization with multi-scale depthwise temporal convolution},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A class-specific mean vector-based weighted competitive and
collaborative representation method for classification. <em>NN</em>,
<em>150</em>, 12–27. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative representation-based classification (CRC), as a typical kind of linear representation-based classification, has attracted more attention due to the effective and efficient pattern classification performance. However, the existing class-specific representations are not competitively learned from collaborative representation for achieving more informative pattern discrimination among all the classes. With the purpose of enhancing the power of competitive and discriminant representations among all the classes for favorable classification, we propose a novel CRC method called the class-specific mean vector-based weighted competitive and collaborative representation (CMWCCR). The CMWCCR mainly contains three discriminative constraints including the competitive, mean vector and weighted constraints that fully employ the discrimination information in different ways. In the competitive constraint, the representations from any one class and the other classes are adapted for learning competitive representations among all the classes. In the newly designed mean vector constraint, the mean vectors of all the class-specific training samples with the corresponding class-specific representations are taken into account to further enhance the competitive representations. In the devised weighted constraint, the class-specific weights are constrained on the representation coefficients to make the similar classes have more representation contributions to strengthening the discrimination among all the class-specific representations. Thus, these three constraints in the unified CMWCCR model can complement each other for competitively learning the discriminative class-specific representations. To verify the CMWCCR classification performance, the extensive experiments are conducted on twenty-eight data sets in comparisons with the state-of-the-art representation-based classification methods. The experimental results show that the proposed CMWCCR is an effective and robust CRC method with satisfactory performance.},
  archive      = {J_NN},
  author       = {Jianping Gou and Xin He and Junyu Lu and Hongxing Ma and Weihua Ou and Yunhao Yuan},
  doi          = {10.1016/j.neunet.2022.02.021},
  journal      = {Neural Networks},
  pages        = {12-27},
  shortjournal = {Neural Netw.},
  title        = {A class-specific mean vector-based weighted competitive and collaborative representation method for classification},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards improving fast adversarial training in multi-exit
network. <em>NN</em>, <em>150</em>, 1–11. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples are usually generated by adding adversarial perturbations on clean samples, designed to deceive the model to make wrong classifications. Adversarial robustness refers to the ability of a model to resist adversarial attacks . And currently, a mainstream method to enhance adversarial robustness is the Projected Gradient Descent (PGD). However, PGD is often criticized for being time-consuming during constructing adversarial examples. Fast adversarial training can improve the adversarial robustness in shorter time, but it only can train for a limited number of epochs, leading to sub-optimal performance. This paper demonstrates that the multi-exit network can reduce the impact of adversarial perturbations by outputting easily identified samples at early exits. Therefore, we can improve the adversarial robustness. Further, we find that the multi-exit network can prevent catastrophic overfitting existing in single-step adversarial training. Specifically, we find that, in the multi-exit network, (1) the norm of weights at a fully connected layer in a non-overfitted exit is much smaller than that in an overfitted exit; and (2) catastrophic overfitting occurs when the late exits have weight norms larger than the early exits. Based on these findings, we propose an approach to alleviating the catastrophic overfitting of the multi-exit network. Compared to PGD adversarial training, our approach can train a model with decreased time complexity and increased empirical robustness. Extensive experiments have been conducted to evaluate our approach against various adversarial attacks , and the experimental results demonstrate superior robustness accuracies on CIFAR-10, CIFAR-100 and SVHN.},
  archive      = {J_NN},
  author       = {Sihong Chen and Haojing Shen and Ran Wang and Xizhao Wang},
  doi          = {10.1016/j.neunet.2022.02.015},
  journal      = {Neural Networks},
  pages        = {1-11},
  shortjournal = {Neural Netw.},
  title        = {Towards improving fast adversarial training in multi-exit network},
  volume       = {150},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022i). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>149</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00088-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00088-0},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). Current events. <em>NN</em>, <em>149</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00087-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00087-9},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Significance of event related causality (ERC) in eloquent
neural networks. <em>NN</em>, <em>149</em>, 204–216. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural activity emerges and propagates swiftly between brain areas. Investigation of these transient large-scale flows requires sophisticated statistical models. We present a method for assessing the statistical confidence of event-related neural propagation. Furthermore, we propose a criterion for statistical model selection, based on both goodness of fit and width of confidence intervals. We show that event-related causality (ERC) with two-dimensional (2D) moving average, is an efficient estimator of task-related neural propagation and that it can be used to determine how different cognitive task demands affect the strength and directionality of neural propagation across human cortical networks . Using electrodes surgically implanted on the surface of the brain for clinical testing prior to epilepsy surgery, we recorded electrocorticographic (ECoG) signals as subjects performed three naming tasks: naming of ambiguous and unambiguous visual objects, and as a contrast, naming to auditory description. ERC revealed robust and statistically significant patterns of high gamma activity propagation, consistent with models of visually and auditorily cued word production. Interestingly, ambiguous visual stimuli elicited more robust propagation from visual to auditory cortices relative to unambiguous stimuli, whereas naming to auditory description elicited propagation in the opposite direction, consistent with recruitment of modalities other than those of the stimulus during object recognition and naming. The new method introduced here is uniquely suitable to both research and clinical applications and can be used to estimate the statistical significance of neural propagation for both cognitive neuroscientific studies and functional brain mapping prior to resective surgery for epilepsy and brain tumors.},
  archive      = {J_NN},
  author       = {Anna Korzeniewska and Takumi Mitsuhashi and Yujing Wang and Eishi Asano and Piotr J. Franaszczuk and Nathan E. Crone},
  doi          = {10.1016/j.neunet.2022.02.002},
  journal      = {Neural Networks},
  pages        = {204-216},
  shortjournal = {Neural Netw.},
  title        = {Significance of event related causality (ERC) in eloquent neural networks},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sign stochastic gradient descents without bounded gradient
assumption for the finite sum minimization. <em>NN</em>, <em>149</em>,
195–203. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign-based Stochastic Gradient Descents (Sign-based SGDs) use the signs of the stochastic gradients for communication costs reduction. Nevertheless, current convergence results of sign-based SGDs applied to the finite sum optimization are established on the bounded assumption of the gradient, which fails to hold in various cases. This paper presents a convergence framework about sign-based SGDs with the elimination of the bounded gradient assumption. The ergodic convergence rates are provided only with the smooth assumption of the objective functions. The Sign Stochastic Gradient Descent ( signSGD ) and its two variants, including majority vote and zeroth-order version, are developed for different application settings. Our framework also removes the bounded gradient assumption used in the previous analysis of these three algorithms.},
  archive      = {J_NN},
  author       = {Tao Sun and Dongsheng Li},
  doi          = {10.1016/j.neunet.2022.02.012},
  journal      = {Neural Networks},
  pages        = {195-203},
  shortjournal = {Neural Netw.},
  title        = {Sign stochastic gradient descents without bounded gradient assumption for the finite sum minimization},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling learnable electrical synapse for high precision
spatio-temporal recognition. <em>NN</em>, <em>149</em>, 184–194. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bio-inspired recipes are being introduced to artificial neural networks for the efficient processing of spatio-temporal tasks. Among them, Leaky Integrate and Fire (LIF) model is the most remarkable one thanks to its temporal processing capability, lightweight model structure, and well investigated direct training methods. However, most learnable LIF networks generally take neurons as independent individuals that communicate via chemical synapses , leaving electrical synapses all behind. On the contrary, it has been well investigated in biological neural networks that the inter-neuron electrical synapse takes a great effect on the coordination and synchronization of generating action potentials. In this work, we are engaged in modeling such electrical synapses in artificial LIF neurons, where membrane potentials propagate to neighbor neurons via convolution operations, and the refined neural model ECLIF is proposed. We then build deep networks using ECLIF and trained them using a back-propagation-through-time algorithm. We found that the proposed network has great accuracy improvement over traditional LIF on five datasets and achieves high accuracy on them. In conclusion, it reveals that the introduction of the electrical synapse is an important factor for achieving high accuracy on realistic spatio-temporal tasks.},
  archive      = {J_NN},
  author       = {Zhenzhi Wu and Zhihong Zhang and Huanhuan Gao and Jun Qin and Rongzhen Zhao and Guangshe Zhao and Guoqi Li},
  doi          = {10.1016/j.neunet.2022.02.006},
  journal      = {Neural Networks},
  pages        = {184-194},
  shortjournal = {Neural Netw.},
  title        = {Modeling learnable electrical synapse for high precision spatio-temporal recognition},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep adversarial transition learning using cross-grafted
generative stacks. <em>NN</em>, <em>149</em>, 172–183. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a common approach of deep domain adaptation in computer vision , current works have mainly focused on learning domain-invariant features from different domains, achieving limited success in transfer learning . In this paper, we present a novel “deep adversarial transition learning” (DATL) framework that bridges the domain gap by generating some intermediate, transitional spaces between the source and target domains through the employment of adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Specifically, variational auto-encoders (VAEs) are constructed for the domains, and bidirectional transitions are formed by cross-grafting the VAEs’ decoder stacks. Generative adversarial networks are then employed to map the target domain data to the label space of the source domain, which is achieved by aligning the transitions initiated by different domains. This results in a new, effective learning paradigm, where training and testing are carried out in the associated transitional spaces instead of the original domains. Experimental results demonstrate that our method outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.},
  archive      = {J_NN},
  author       = {Jinyong Hou and Xuejie Ding and Jeremiah D. Deng and Stephen Cranefield},
  doi          = {10.1016/j.neunet.2022.02.011},
  journal      = {Neural Networks},
  pages        = {172-183},
  shortjournal = {Neural Netw.},
  title        = {Deep adversarial transition learning using cross-grafted generative stacks},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint learning of multiple granger causal networks via
non-convex regularizations: Inference of group-level brain connectivity.
<em>NN</em>, <em>149</em>, 157–171. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers joint learning of multiple sparse Granger graphical models to discover underlying common and differential Granger causality (GC) structures across multiple time series . This can be applied to drawing group-level brain connectivity inferences from a homogeneous group of subjects or discovering network differences among groups of signals collected under heterogeneous conditions. By recognizing that the GC of a single multivariate time series can be characterized by common zeros of vector autoregressive (VAR) lag coefficients, a group sparse prior is included in joint regularized least-squares estimations of multiple VAR models . Group-norm regularizations based on group- and fused-lasso penalties encourage a decomposition of multiple networks into a common GC structure, with other remaining parts defined in individual-specific networks. Prior information about sparseness and sparsity patterns of desired GC networks are incorporated as relative weights, while a non-convex group norm in the penalty is proposed to enhance the accuracy of network estimation in low-sample settings. Extensive numerical results on simulations illustrated our method’s improvements over existing sparse estimation approaches on GC network sparsity recovery. Our methods were also applied to available resting-state fMRI time series from the ADHD-200 data sets to learn the differences of causality mechanisms, called effective brain connectivity, between adolescents with ADHD and typically developing children. Our analysis revealed that parts of the causality differences between the two groups often resided in the orbitofrontal region and areas associated with the limbic system , which agreed with clinical findings and data-driven results in previous studies.},
  archive      = {J_NN},
  author       = {Parinthorn Manomaisaowapak and Jitkomut Songsiri},
  doi          = {10.1016/j.neunet.2022.02.005},
  journal      = {Neural Networks},
  pages        = {157-171},
  shortjournal = {Neural Netw.},
  title        = {Joint learning of multiple granger causal networks via non-convex regularizations: Inference of group-level brain connectivity},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-hopf bifurcation of a memristive synaptic hopfield
neural network with time delay. <em>NN</em>, <em>149</em>, 146–156. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel memristive synaptic Hopfield neural network (MHNN) with time delay by using a memristor synapse to simulate the electromagnetic induced current caused by the membrane potential difference between two adjacent neurons . First, some sufficient conditions of zero bifurcation and zero-Hopf bifurcation are obtained by choosing time delay and coupling strength of memristor as bifurcation parameters . Then, the third-order normal form of zero-Hopf bifurcation is obtained. By analyzing the obtained normal form, six dynamic regions are found on the plane with coupling strength of memristor and time delay as abscissa and ordinate. There are some interesting dynamics in these areas, i.e., the coupling strength of memristor can affect the number and dynamics of system equilibrium , time delay can contribute to both trivial equilibrium and non-trivial equilibrium losing stability and generating periodic solutions.},
  archive      = {J_NN},
  author       = {Tao Dong and Xiaomei Gong and Tingwen Huang},
  doi          = {10.1016/j.neunet.2022.02.009},
  journal      = {Neural Networks},
  pages        = {146-156},
  shortjournal = {Neural Netw.},
  title        = {Zero-hopf bifurcation of a memristive synaptic hopfield neural network with time delay},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time and sampled-data synchronization of complex
dynamical networks subject to average dwell-time switching signal.
<em>NN</em>, <em>149</em>, 137–145. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study deals with the finite-time synchronization problem of a class of switched complex dynamical networks (CDNs) with distributed coupling delays via sampled-data control. First, the dynamical model is studied with coupling delays in more detail. The sampling system is then converted to a continuous time-delay system using an input delay technique. We obtain some unique and less conservative criteria on exponential stability using the Lyapunov–Krasovskii functional (LKF), which is generated with a Kronecker product , linear matrix inequalities (LMIs), and integral inequality. Furthermore, some sufficient criteria are derived by an average dwell-time method and determine the finite-time boundedness of CDNs with switching signal. The proposed sufficient conditions can be represented in the form of LMIs. Finally, numerical examples are given to show that the suggested strategy is feasible.},
  archive      = {J_NN},
  author       = {Nallappan Gunasekaran and M. Syed Ali and Sabri Arik and H.I. Abdul Ghaffar and Ahmed A. Zaki Diab},
  doi          = {10.1016/j.neunet.2022.02.013},
  journal      = {Neural Networks},
  pages        = {137-145},
  shortjournal = {Neural Netw.},
  title        = {Finite-time and sampled-data synchronization of complex dynamical networks subject to average dwell-time switching signal},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning algorithm for large-scale
semi-supervised ordinal regression. <em>NN</em>, <em>149</em>, 124–136.
(<a href="https://doi.org/10.1016/j.neunet.2022.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special case of multi-classification, ordinal regression (also known as ordinal classification) is a popular method to tackle the multi-class problems with samples marked by a set of ranks. Semi-supervised ordinal regression (SSOR) is especially important for data mining applications because semi-supervised learning can make use of the unlabeled samples to train a high-quality learning model. However, the training of large-scale SSOR is still an open question due to its complicated formulations and non-convexity to the best of our knowledge. To address this challenging problem, in this paper, we propose an incremental learning algorithm for SSOR (IL-SSOR), which can directly update the solution of SSOR based on the KKT conditions . More critically, we analyze the finite convergence of IL-SSOR which guarantees that SSOR can converge to a local minimum based on the framework of concave–convex procedure. To the best of our knowledge, the proposed new algorithm is the first efficient on-line learning algorithm for SSOR with local minimum convergence guarantee. The experimental results show, IL-SSOR can achieve better generalization than other semi-supervised multi-class algorithms. Compared with other semi-supervised ordinal regression algorithms, our experimental results show that IL-SSOR can achieve similar generalization with less running time.},
  archive      = {J_NN},
  author       = {Haiyan Chen and Yizhen Jia and Jiaming Ge and Bin Gu},
  doi          = {10.1016/j.neunet.2022.02.004},
  journal      = {Neural Networks},
  pages        = {124-136},
  shortjournal = {Neural Netw.},
  title        = {Incremental learning algorithm for large-scale semi-supervised ordinal regression},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexibly regularized mixture models and application to image
segmentation. <em>NN</em>, <em>149</em>, 107–123. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic finite mixture models are widely used for unsupervised clustering. These models can often be improved by adapting them to the topology of the data. For instance, in order to classify spatially adjacent data points similarly, it is common to introduce a Laplacian constraint on the posterior probability that each data point belongs to a class. Alternatively, the mixing probabilities can be treated as free parameters, while assuming Gauss–Markov or more complex priors to regularize those mixing probabilities. However, these approaches are constrained by the shape of the prior and often lead to complicated or intractable inference. Here, we propose a new parametrization of the Dirichlet distribution to flexibly regularize the mixing probabilities of over-parametrized mixture distributions. Using the Expectation–Maximization algorithm, we show that our approach allows us to define any linear update rule for the mixing probabilities, including spatial smoothing regularization as a special case. We then show that this flexible design can be extended to share class information between multiple mixture models. We apply our algorithm to artificial and natural image segmentation tasks, and we provide quantitative and qualitative comparison of the performance of Gaussian and Student-t mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to propagate class information across the layers of deep convolutional neural networks in a probabilistically optimal way, suggesting a new interpretation for feedback signals in biological visual systems. Our flexible approach can be easily generalized to adapt probabilistic mixture models to arbitrary data topologies.},
  archive      = {J_NN},
  author       = {Jonathan Vacher and Claire Launay and Ruben Coen-Cagli},
  doi          = {10.1016/j.neunet.2022.02.010},
  journal      = {Neural Networks},
  pages        = {107-123},
  shortjournal = {Neural Netw.},
  title        = {Flexibly regularized mixture models and application to image segmentation},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep bayesian unsupervised lifelong learning. <em>NN</em>,
<em>149</em>, 95–106. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong Learning (LL) refers to the ability to continually learn and solve new problems with incremental available information over time while retaining previous knowledge. Much attention has been given lately to Supervised Lifelong Learning (SLL) with a stream of labelled data. In contrast, we focus on resolving challenges in Unsupervised Lifelong Learning (ULL) with streaming unlabelled data when the data distribution and the unknown class labels evolve over time. Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with new data. We develop a fully Bayesian inference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised Lifelong Learning (DBULL) algorithm, which can progressively discover new clusters without forgetting the past with unlabelled data while learning latent representations. To efficiently maintain past knowledge, we develop a novel knowledge preservation mechanism via sufficient statistics of the latent representation for raw data. To detect the potential new clusters on the fly, we develop an automatic cluster discovery and redundancy removal strategy in our inference inspired by Nonparametric Bayesian statistics techniques. We demonstrate the effectiveness of our approach using image and text corpora benchmark datasets in both LL and batch settings.},
  archive      = {J_NN},
  author       = {Tingting Zhao and Zifeng Wang and Aria Masoomi and Jennifer Dy},
  doi          = {10.1016/j.neunet.2022.02.001},
  journal      = {Neural Networks},
  pages        = {95-106},
  shortjournal = {Neural Netw.},
  title        = {Deep bayesian unsupervised lifelong learning},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain heterogeneous residual network for single image
super-resolution. <em>NN</em>, <em>149</em>, 84–94. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution is an ill-posed problem, whose purpose is to acquire a high-resolution image from its degraded observation. Existing deep learning-based methods are compromised on their performance and speed due to the heavy design (i.e., huge model size) of networks. In this paper, we propose a novel high-performance cross-domain heterogeneous residual network for super-resolved image reconstruction. Our network models heterogeneous residuals between different feature layers by hierarchical residual learning. In outer residual learning, dual-domain enhancement modules extract the frequency-domain information to reinforce the space-domain features of network mapping. In middle residual learning, wide-activated residual-in-residual dense blocks are constructed by concatenating the outputs from previous blocks as the inputs into all subsequent blocks for better parameter efficacy. In inner residual learning, wide-activated residual attention blocks are introduced to capture direction- and location-aware feature maps. The proposed method was evaluated on four benchmark datasets, indicating that it can construct the high-quality super-resolved images and achieve the state-of-the-art performance. Code and pre-trained models are available at https://github.com/zhangyongqin/HRN .},
  archive      = {J_NN},
  author       = {Li Ji and Qinghui Zhu and Yongqin Zhang and Juanjuan Yin and Ruyi Wei and Jinsheng Xiao and Deqiang Xiao and Guoying Zhao},
  doi          = {10.1016/j.neunet.2022.02.008},
  journal      = {Neural Networks},
  pages        = {84-94},
  shortjournal = {Neural Netw.},
  title        = {Cross-domain heterogeneous residual network for single image super-resolution},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Backpropagation neural tree. <em>NN</em>, <em>149</em>,
66–83. (<a href="https://doi.org/10.1016/j.neunet.2022.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm called Backpropagation Neural Tree (BNeuralT), which is a stochastic computational dendritic tree . BNeuralT takes random repeated inputs through its leaves and imposes dendritic nonlinearities through its internal connections like a biological dendritic tree would do. Considering the dendritic-tree like plausible biological properties, BNeuralT is a single neuron neural tree model with its internal sub-trees resembling dendritic nonlinearities. BNeuralT algorithm produces an ad hoc neural tree which is trained using a stochastic gradient descent optimizer like gradient descent (GD), momentum GD, Nesterov accelerated GD, Adagrad, RMSprop , or Adam. BNeuralT training has two phases, each computed in a depth-first search manner: the forward pass computes neural tree’s output in a post-order traversal , while the error backpropagation during the backward pass is performed recursively in a pre-order traversal . A BNeuralT model can be considered a minimal subset of a neural network (NN), meaning it is a “thinned” NN whose complexity is lower than an ordinary NN. Our algorithm produces high-performing and parsimonious models balancing the complexity with descriptive ability on a wide variety of machine learning problems: classification, regression, and pattern recognition.},
  archive      = {J_NN},
  author       = {Varun Ojha and Giuseppe Nicosia},
  doi          = {10.1016/j.neunet.2022.02.003},
  journal      = {Neural Networks},
  pages        = {66-83},
  shortjournal = {Neural Netw.},
  title        = {Backpropagation neural tree},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronization issue of coupled neural networks based on
flexible impulse control. <em>NN</em>, <em>149</em>, 57–65. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global exponential synchronization issue of coupled neural networks with time-delayed impulses is investigated in this paper. On the basis of the characteristics of coupled neural networks and theorems, we have built a novel coupled systems model. In order to fit the real situation, the impulses are flexible and it can beyond the impulsive interval under certain conditions in this paper. Therefore, our results are less restrictive and more practical compared to existing research. Besides, by using average impulsive delay (AID) and average impulsive interval (AII), we investigate two different effects of impulses on synchronization respectively and get a few adequate conditions for different types of synchronization. Finally, there are two examples of numerical simulations presented to illustrate the efficiency of the conclusions.},
  archive      = {J_NN},
  author       = {Ruihong Xiu and Wei Zhang and Zichuan Zhou},
  doi          = {10.1016/j.neunet.2022.01.020},
  journal      = {Neural Networks},
  pages        = {57-65},
  shortjournal = {Neural Netw.},
  title        = {Synchronization issue of coupled neural networks based on flexible impulse control},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Possibilistic classification by support vector networks.
<em>NN</em>, <em>149</em>, 40–56. (<a
href="https://doi.org/10.1016/j.neunet.2022.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world classification problems, the available information is often uncertain. In order to effectively describe the inherent vagueness and improve the classification performance, this paper proposes a novel possibilistic classification algorithm using support vector machines (SVMs). Based on possibility theory, the proposed algorithm aims at finding a maximal-margin fuzzy hyperplane by solving a fuzzy mathematical optimization problem Moreover, the decision function of the proposed approach is generalized such that the values assigned to the data vectors fall within a specified range and indicate the membership grade of these data vectors in the positive class. The proposed algorithm retains the advantages of fuzzy set theory and SVM theory. The proposed approach is more robust for handling data corrupted by outliers. Moreover, the structural risk minimization principle of SVMs enables the proposed approach to effectively classify the unseen data. Furthermore, the proposed algorithm has additional advantage of using vagueness parameter v for controlling the bounds on fractions of support vectors and errors. The extensive experiments performed on benchmark datasets and real applications demonstrate that the proposed algorithm has satisfactory generalization accuracy and better describes the inherent vagueness in the given dataset.},
  archive      = {J_NN},
  author       = {Pei-Yi Hao and Jung-Hsien Chiang and Yu-De Chen},
  doi          = {10.1016/j.neunet.2022.02.007},
  journal      = {Neural Networks},
  pages        = {40-56},
  shortjournal = {Neural Netw.},
  title        = {Possibilistic classification by support vector networks},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting cell assemblies by NMF-based clustering from
calcium imaging data. <em>NN</em>, <em>149</em>, 29–39. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of neurons form cell assemblies that process information in the brain. Recent developments in measurement technology, one of which is calcium imaging , have made it possible to study cell assemblies. In this study, we aim to extract cell assemblies from calcium imaging data. We propose a clustering approach based on non-negative matrix factorization (NMF). The proposed approach first obtains a similarity matrix between neurons by NMF and then performs spectral clustering on it. The application of NMF entails the problem of model selection. The number of bases in NMF affects the result considerably, and a suitable selection method is yet to be established. We attempt to resolve this problem by model averaging with a newly defined estimator based on NMF. Experiments on simulated data suggest that the proposed approach is superior to conventional correlation-based clustering methods over a wide range of sampling rates. We also analyzed calcium imaging data of sleeping/waking mice and the results suggest that the size of the cell assembly depends on the degree and spatial extent of slow wave generation in the cerebral cortex .},
  archive      = {J_NN},
  author       = {Mizuo Nagayama and Toshimitsu Aritake and Hideitsu Hino and Takeshi Kanda and Takehiro Miyazaki and Masashi Yanagisawa and Shotaro Akaho and Noboru Murata},
  doi          = {10.1016/j.neunet.2022.01.023},
  journal      = {Neural Networks},
  pages        = {29-39},
  shortjournal = {Neural Netw.},
  title        = {Detecting cell assemblies by NMF-based clustering from calcium imaging data},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomalous diffusion dynamics of learning in deep neural
networks. <em>NN</em>, <em>149</em>, 18–28. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning in deep neural networks (DNNs) is implemented through minimizing a highly non-convex loss function, typically by a stochastic gradient descent (SGD) method. This learning process can effectively find generalizable solutions at flat minima. In this study, we present a novel account of how such effective deep learning emerges through the interactions of the SGD and the geometrical structure of the loss landscape. We find that the SGD exhibits rich, complex dynamics when navigating through the loss landscape; initially, the SGD exhibits superdiffusion, which attenuates gradually and changes to subdiffusion at long times when approaching a solution. Such learning dynamics happen ubiquitously in different DNN types such as ResNet , VGG-like networks and Vision Transformers ; similar results emerge for various batch size and learning rate settings. The superdiffusion process during the initial learning phase indicates that the motion of SGD along the loss landscape possesses intermittent, big jumps; this non-equilibrium property enables the SGD to effectively explore the loss landscape. By adapting methods developed for studying energy landscapes in complex physical systems, we find that such superdiffusive learning processes are due to the interactions of the SGD and the fractal-like regions of the loss landscape. We further develop a phenomenological model to demonstrate the mechanistic role of the fractal-like loss landscape in enabling the SGD to effectively find flat minima. Our results reveal the effectiveness of SGD in deep learning from a novel perspective and have implications for designing efficient deep neural networks .},
  archive      = {J_NN},
  author       = {Guozhang Chen and Cheng Kevin Qu and Pulin Gong},
  doi          = {10.1016/j.neunet.2022.01.019},
  journal      = {Neural Networks},
  pages        = {18-28},
  shortjournal = {Neural Netw.},
  title        = {Anomalous diffusion dynamics of learning in deep neural networks},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transition dynamics and optogenetic controls of generalized
periodic epileptiform discharges. <em>NN</em>, <em>149</em>, 1–17. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to analyze possible mechanisms underlying the generation of generalized periodic epileptiform discharges (GPEDs), especially to design targeted optogenetic regulation strategies. First and foremost, inspired by existing physiological experiments, we propose a new computational framework by introducing a second inhibitory neuronal population and related synaptic connections into the classic Liley mean field model . The improved model can simulate the basic normal and abnormal brain activities mentioned in previous studies, but much to our relief, it perfectly reproduces some types of GPEDs that match the clinical records. Specifically, results show that disinhibitory synaptic connections between inhibitory interneuronal populations are closely related to the occurrence, transition and termination of GPEDs, including delaying the occurrence of GPEDs caused by the excitatory AMPAergic autapses and regulating the transition process of GPEDs bidirectionally, which support the conjecture that selective changes of synaptic connections can trigger GPEDs. Additionally, we creatively offer six optogenetic strategies with dual targets. They can all control GPEDs well, just as experiments reveal that optogenetic stimulation of inhibitory interneurons can suppress abnormal activities in epilepsy or other brain diseases. More importantly, 1:1 coordinated reset stimulation with one period rest is concluded as the optimal strategy after taking into account the energy consumption and control effect. Hope these results provide feasible references for pathophysiological mechanisms of GPEDs.},
  archive      = {J_NN},
  author       = {Zhuan Shen and Honghui Zhang and Zilu Cao and Luyao Yan and Yuzhi Zhao and Lin Du and Zichen Deng},
  doi          = {10.1016/j.neunet.2022.01.022},
  journal      = {Neural Networks},
  pages        = {1-17},
  shortjournal = {Neural Netw.},
  title        = {Transition dynamics and optogenetic controls of generalized periodic epileptiform discharges},
  volume       = {149},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>148</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00056-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00056-9},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {NN/ENNS/JNNS - membership applic. form},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022f). Current events. <em>NN</em>, <em>148</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00055-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00055-7},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature-based intelligent models for optimisation of
percussive drilling. <em>NN</em>, <em>148</em>, 266–284. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a rotary-percussion system, the vibro-impact drilling (VID) system utilises resonantly induced high frequency periodic impacts alongside existing drill-string rotation to cut through downhole rock layers. Due to the inhomogeneous nature of the rock layers, the system often experiences multi-stability which generates different categories of impact motions as drilling continues downhole. Some impact motions yield better drilling performance in terms of rate of penetration (ROP) and bit life-span when compared to others. As an optimisation strategy, the present study adopts feature-based classification algorithms including multi-layer perceptron, support vector machine and long short-term memory network as intelligent models for categorising impact motions from a one-degree-of-freedom impact oscillator representing the percussive bit-rock impacts of the VID system. This way, high-performance impacts can be easily detected and maintained while undesirable low-performance impacts are well avoided to increase ROP, improve bit life-span and save cost. In this study, scarce and limited classes of experimental impact data are merged with inexhaustibly simulated impact data to train different network models. By means of cross-validation, the trained networks were tested on separate sets of only-simulation and only-experimental data. Results show that extracting appropriate features from raw impact data is essential for optimising the performance of each network model. About 42\% of the feature-based networks yield accuracies greater than 91\% while about 67\% yield accuracies greater than 77\% on both simulation and experimental impact motion data.},
  archive      = {J_NN},
  author       = {Kenneth Omokhagbo Afebu and Yang Liu and Evangelos Papatheou},
  doi          = {10.1016/j.neunet.2022.01.021},
  journal      = {Neural Networks},
  pages        = {266-284},
  shortjournal = {Neural Netw.},
  title        = {Feature-based intelligent models for optimisation of percussive drilling},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying the reproducibility of graph neural networks
using multigraph data representation. <em>NN</em>, <em>148</em>,
254–265. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have witnessed an unprecedented proliferation in tackling several problems in computer vision , computer-aided diagnosis and related fields. While prior studies have focused on boosting the model accuracy, quantifying the reproducibility of the most discriminative features identified by GNNs is still an intact problem that yields concerns about their reliability in clinical applications in particular. Specifically, the reproducibility of biological markers across clinical datasets and distribution shifts across classes (e.g., healthy and disordered brains) is of paramount importance in revealing the underpinning mechanisms of diseases as well as propelling the development of personalized treatment. Motivated by these issues, we propose, for the first time, reproducibility-based GNN selection (RG-Select), a framework for GNN reproducibility assessment via the quantification of the most discriminative features (i.e., biomarkers) shared between different models. To ascertain the soundness of our framework, the reproducibility assessment embraces variations of different factors such as training strategies and data perturbations. Despite these challenges, our framework successfully yielded replicable conclusions across different training strategies and various clinical datasets. Our findings could thus pave the way for the development of biomarker trustworthiness and reliability assessment methods for computer-aided diagnosis and prognosis tasks. RG-Select code is available on GitHub at https://github.com/basiralab/RG-Select .},
  archive      = {J_NN},
  author       = {Ahmed Nebli and Mohammed Amine Gharsallaoui and Zeynep Gürler and Islem Rekik and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.neunet.2022.01.018},
  journal      = {Neural Networks},
  pages        = {254-265},
  shortjournal = {Neural Netw.},
  title        = {Quantifying the reproducibility of graph neural networks using multigraph data representation},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The emergence of a concept in shallow neural networks.
<em>NN</em>, <em>148</em>, 232–253. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider restricted Boltzmann machine (RBMs) trained over an unstructured dataset made of blurred copies of definite but unavailable “archetypes” and we show that there exists a critical sample size beyond which the RBM can learn archetypes, namely the machine can successfully play as a generative model or as a classifier, according to the operational routine. In general, assessing a critical sample size (possibly in relation to the quality of the dataset) is still an open problem in machine learning . Here, restricting to the random theory, where shallow networks suffice and the “grandmother-cell” scenario is correct, we leverage the formal equivalence between RBMs and Hopfield networks , to obtain a phase diagram for both the neural architectures which highlights regions, in the space of the control parameters (i.e., number of archetypes, number of neurons, size and quality of the training set), where learning can be accomplished. Our investigations are led by analytical methods based on the statistical-mechanics of disordered systems and results are further corroborated by extensive Monte Carlo simulations .},
  archive      = {J_NN},
  author       = {Elena Agliari and Francesco Alemanno and Adriano Barra and Giordano De Marzo},
  doi          = {10.1016/j.neunet.2022.01.017},
  journal      = {Neural Networks},
  pages        = {232-253},
  shortjournal = {Neural Netw.},
  title        = {The emergence of a concept in shallow neural networks},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Region-aware network: Model human’s top-down visual
perception mechanism for crowd counting. <em>NN</em>, <em>148</em>,
219–231. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background noise and scale variation are common problems that have been long recognized in crowd counting. Humans glance at a crowd image and instantly know the approximate number of human and where they are through attention the crowd regions and the congestion degree of crowd regions with a global receptive field. Hence, in this paper, we propose a novel feedback network with Region-Aware block called RANet by modeling human’s Top-Down visual perception mechanism. Firstly, we introduce a feedback architecture to generate priority maps that provide prior about candidate crowd regions in input images. The prior enables the RANet pay more attention to crowd regions. Then we design Region-Aware block that could adaptively encode the contextual information into input images through global receptive field. More specifically, we scan the whole input images and its priority maps in the form of column vector to obtain a relevance matrix estimating their similarity. The relevance matrix obtained would be utilized to build global relationships between pixels. Our method outperforms state-of-the-art crowd counting methods on several public datasets.},
  archive      = {J_NN},
  author       = {Yuehai Chen and Jing Yang and Dong Zhang and Kun Zhang and Badong Chen and Shaoyi Du},
  doi          = {10.1016/j.neunet.2022.01.015},
  journal      = {Neural Networks},
  pages        = {219-231},
  shortjournal = {Neural Netw.},
  title        = {Region-aware network: Model human’s top-down visual perception mechanism for crowd counting},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GARAT: Generative adversarial learning for robust and
accurate tracking. <em>NN</em>, <em>148</em>, 206–218. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking by the Siamese network has gained its popularity for its outstanding performance and considerable potential. However, most of the existing Siamese architectures are faced with great difficulties when it comes to the scenes where the target is going through dramatic shape or environmental changes. In this work, we proposed a novel and concise generative adversarial learning method to solve the problem especially when the target is going under drastic changes of appearance, illumination variations and background clutters. We consider the above situations as distractors for tracking and joint a distractor generator into the traditional Siamese network . The component can simulate these distractors , and more robust tracking performance is achieved by eliminating the distractors from the input instance search image. Besides, we use the generalized intersection over union ( GIoU GIoU ) as our training loss. GIoU GIoU is a more strict metric for the bounding box regression compared to the traditional IoU IoU , which can be used as training loss for more accurate tracking results. Experiments on five challenging benchmarks have shown favorable and state-of-the-art results against other trackers in different aspects.},
  archive      = {J_NN},
  author       = {Bowen Yao and Jing Li and Shan Xue and Jia Wu and Huanmei Guan and Jun Chang and Zhiquan Ding},
  doi          = {10.1016/j.neunet.2022.01.010},
  journal      = {Neural Networks},
  pages        = {206-218},
  shortjournal = {Neural Netw.},
  title        = {GARAT: Generative adversarial learning for robust and accurate tracking},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving data augmentation for low resource speech-to-text
translation with diverse paraphrasing. <em>NN</em>, <em>148</em>,
194–205. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High quality end-to-end speech translation model relies on a large scale of speech-to-text training data, which is usually scarce or even unavailable for some low-resource language pairs. To overcome this, we propose a target-side data augmentation method for low-resource language speech translation. In particular, we first generate large-scale target-side paraphrases based on a paraphrase generation model which incorporates several statistical machine translation (SMT) features and the commonly used recurrent neural network (RNN) feature. Then, a filtering model which consists of semantic similarity and speech–word pair co-occurrence was proposed to select the highest scoring source speech–target paraphrase pairs from candidates. Experimental results on English, Arabic, German, Latvian, Estonian, Slovenian and Swedish paraphrase generation show that the proposed method achieves significant and consistent improvements over several strong baseline models on PPDB datasets ( http://paraphrase.org/ ). To introduce the results of paraphrase generation into the low-resource speech translation, we propose two strategies: audio–text pairs recombination and multiple references training. Experimental results show that the speech translation models trained on new audio–text datasets which combines the paraphrase generation results lead to substantial improvements over baselines, especially on low-resource languages.},
  archive      = {J_NN},
  author       = {Chenggang Mi and Lei Xie and Yanning Zhang},
  doi          = {10.1016/j.neunet.2022.01.016},
  journal      = {Neural Networks},
  pages        = {194-205},
  shortjournal = {Neural Netw.},
  title        = {Improving data augmentation for low resource speech-to-text translation with diverse paraphrasing},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep rival penalized competitive learning for low-resolution
face recognition. <em>NN</em>, <em>148</em>, 183–193. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current face recognition tasks are usually carried out on high-quality face images, but in reality, most face images are captured under unconstrained or poor conditions, e.g., by video surveillance. Existing methods are featured by learning data uncertainty to avoid overfitting the noise, or by adding margins to the angle or cosine space of the normalized softmax loss to penalize the target logit, which enforces intra-class compactness and inter-class discrepancy. In this paper, we propose a deep Rival Penalized Competitive Learning (RPCL) for deep face recognition in low-resolution (LR) images. Inspired by the idea of the RPCL, our method further enforces regulation on the rival logit, which is defined as the largest non-target logit for an input image. Different from existing methods that only consider penalization on the target logit, our method not only strengthens the learning towards the target label, but also enforces a reverse direction, i.e., becoming de-learning, away from the rival label. Comprehensive experiments demonstrate that our method improves the existing state-of-the-art methods to be very robust for LR face recognition.},
  archive      = {J_NN},
  author       = {Peiying Li and Shikui Tu and Lei Xu},
  doi          = {10.1016/j.neunet.2022.01.009},
  journal      = {Neural Networks},
  pages        = {183-193},
  shortjournal = {Neural Netw.},
  title        = {Deep rival penalized competitive learning for low-resolution face recognition},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal distribution alignment embedding network for
generalized zero-shot learning. <em>NN</em>, <em>148</em>, 176–182. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many approaches in generalized zero-shot learning (GZSL) rely on cross-modal mapping between the image feature space and the class embedding space, which achieves knowledge transfer from seen to unseen classes. However, these two spaces are completely different space and their manifolds are inconsistent, the existing methods suffer from highly overlapped semantic description of different classes, as in GZSL tasks unseen classes can be easily misclassified into seen classes. To handle these problems, we adopt a novel semantic embedding network which helps to encode more discriminative information from initial semantic attributes to semantic embeddings in visual space. Meanwhile, a distribution alignment constraint is adopted to help keep the distribution of the learned semantic embeddings consistent with the distribution of real image features. Moreover, an auxiliary classifier is adopted to strengthen the quality of the learned semantic embeddings. Finally, a relation network is used to classify the unseen images by computing the relation scores between the semantic embeddings and image features, which is much more flexible than the fixed distance metric functions . Experimental results demonstrate that our proposed method is superior to other state-of-the-arts.},
  archive      = {J_NN},
  author       = {Qin Li and Mingzhen Hou and Hong Lai and Ming Yang},
  doi          = {10.1016/j.neunet.2022.01.007},
  journal      = {Neural Networks},
  pages        = {176-182},
  shortjournal = {Neural Netw.},
  title        = {Cross-modal distribution alignment embedding network for generalized zero-shot learning},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Not every sample is efficient: Analogical generative
adversarial network for unpaired image-to-image translation.
<em>NN</em>, <em>148</em>, 166–175. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation is to learn an effective mapping function that aims to convert an image from a source domain to another target domain. With the proposal and further developments of generative adversarial networks (GANs), the generative models have achieved great breakthroughs. The image-to-image (I2I) translation methods can mainly fall into two categories: Paired and Unpaired . The former paired methods usually require a large amount of input–output sample pairs to perform one-side image translation, which heavily limits its practicability. To address the lack of the paired samples, CycleGAN and its extensions utilize the cycle-consistency loss to provide an elegant and generic solution to perform the unpaired I2I translation between two domains based on unpaired data. This thread of dual learning-based methods usually adopts the random sampling strategy for optimizing and does not consider the content similarity between samples. However, not every sample is efficient and effective for the desired optimization and leads to optimal convergence. Inspired by analogical learning, which is to utilize the relationships and similarities between sample observations, we propose a novel generic metric-based sampling strategy to effectively select samples from different domains for training. Besides, we introduce a novel analogical adversarial loss to force the model to learn from the effective samples and alleviate the influence of the negative samples. Experimental results on various vision tasks have demonstrated the superior performance of the proposed method. The proposed method is also a generic framework that can be easily extended to other I2I translation methods and result in a performance gain.},
  archive      = {J_NN},
  author       = {Ziqiang Zheng and Jie Yang and Zhibin Yu and Yubo Wang and Zhijian Sun and Bing Zheng},
  doi          = {10.1016/j.neunet.2022.01.013},
  journal      = {Neural Networks},
  pages        = {166-175},
  shortjournal = {Neural Netw.},
  title        = {Not every sample is efficient: Analogical generative adversarial network for unpaired image-to-image translation},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-degree term first in ResNet, its variants and the whole
neural network family. <em>NN</em>, <em>148</em>, 155–165. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To explain the working mechanism of ResNet and its variants, this paper proposes a novel argument of shallow subnetwork first (SSF), essentially low-degree term first (LDTF), which also applies to the whole neural network family. A neural network with shortcut connections behaves as an ensemble of a number of subnetworks of differing depths. Among the subnetworks, the shallow subnetworks are trained firstly, having great effects on the performance of the neural network. The shallow subnetworks roughly correspond to low-degree polynomials, while the deep subnetworks are opposite. Based on Taylor expansion , SSF is consistent with LDTF. ResNet is in line with Taylor expansion : shallow subnetworks are trained firstly to keep low-degree terms, avoiding overfitting; deep subnetworks try to maintain high-degree terms, ensuring high description capacity. Experiments on ResNets and DenseNets show that shallow subnetworks are trained firstly and play important roles in the training of the networks. The experiments also reveal the reason why DenseNets outperform ResNets: The subnetworks playing vital roles in the training of the former are shallower than those in the training of the latter. Furthermore, LDTF can also be used to explain the working mechanism of other ResNet variants (SE-ResNets and SK-ResNets), and the common phenomena occurring in many neural networks.},
  archive      = {J_NN},
  author       = {Tongfeng Sun and Shifei Ding and Lili Guo},
  doi          = {10.1016/j.neunet.2022.01.012},
  journal      = {Neural Networks},
  pages        = {155-165},
  shortjournal = {Neural Netw.},
  title        = {Low-degree term first in ResNet, its variants and the whole neural network family},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Signed network representation with novel node proximity
evaluation. <em>NN</em>, <em>148</em>, 142–154. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, signed network representation has been applied to many fields, e.g. , recommendation platforms. A mainstream paradigm of network representation is to map nodes onto a low-dimensional space, such that the node proximity of interest can be preserved. Thus, a key aspect is the node proximity evaluation. Accordingly, three new node proximity metrics were proposed in this study, based on the rigorous theoretical investigation on a new distance metric - signed average first-passage time (SAFT). SAFT derives from a basic random-walk quantity for unsigned networks and can capture high-order network structure and edge signs. We conducted network representation using the proposed proximity metrics and empirically exhibited our advantage in solving two downstream tasks — sign prediction and link prediction. The code is publicly available.},
  archive      = {J_NN},
  author       = {Pinghua Xu and Wenbin Hu and Jia Wu and Weiwei Liu},
  doi          = {10.1016/j.neunet.2022.01.014},
  journal      = {Neural Networks},
  pages        = {142-154},
  shortjournal = {Neural Netw.},
  title        = {Signed network representation with novel node proximity evaluation},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual global enhanced transformer for image captioning.
<em>NN</em>, <em>148</em>, 129–141. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based architectures have shown great success in image captioning, where self-attention module can model source and target interaction (e.g., object-to-object, object-to-word, word-to-word). However, the global information is not explicitly considered in the attention weight calculation, which is essential to understand the scene content. In this paper, we propose Dual Global Enhanced Transformer (DGET) to incorporate global information in the encoding and decoding stages. Concretely, in DGET, we regard the grid feature as the visual global information and adaptively fuse it into region features in each layer by a novel Global Enhanced Encoder (GEE). During decoding, we proposed Global Enhanced Decoder (GED) to explicitly utilize the textual global information. First, we devise the context encoder to encode the existing caption generated by classic captioner as a context vector. Then, we use the context vector to guide the decoder to generate accurate words at each time step. To validate our model, we conduct extensive experiments on the MS COCO image captioning dataset and achieve superior performance over many state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Tiantao Xian and Zhixin Li and Canlong Zhang and Huifang Ma},
  doi          = {10.1016/j.neunet.2022.01.011},
  journal      = {Neural Networks},
  pages        = {129-141},
  shortjournal = {Neural Netw.},
  title        = {Dual global enhanced transformer for image captioning},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On minimal representations of shallow ReLU networks.
<em>NN</em>, <em>148</em>, 121–128. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The realization function of a shallow ReLU network is a continuous and piecewise affine function f:Rd→R , where the domain Rd is partitioned by a set of n hyperplanes into cells on which f is affine. We show that the minimal representation for f uses either n , n+1 or n+2 neurons and we characterize each of the three cases. In the particular case, where the input layer is one-dimensional, minimal representations always use at most n+1 neurons but in all higher dimensional settings there are functions for which n+2 neurons are needed. Then we show that the set of minimal networks representing f forms a C∞ -submanifold M and we derive the dimension and the number of connected components of M . Additionally, we give a criterion for the hyperplanes that guarantees that a continuous, piecewise affine function is the realization function of an appropriate shallow ReLU network.},
  archive      = {J_NN},
  author       = {Steffen Dereich and Sebastian Kassing},
  doi          = {10.1016/j.neunet.2022.01.006},
  journal      = {Neural Networks},
  pages        = {121-128},
  shortjournal = {Neural Netw.},
  title        = {On minimal representations of shallow ReLU networks},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint learning adaptive metric and optimal classification
hyperplane. <em>NN</em>, <em>148</em>, 111–120. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning has attracted a lot of interest in classification tasks due to its efficient performance. Most traditional metric learning methods are based on k-nearest neighbors (kNN) classifiers to make decisions, while the choice k k affects the generalization. In this work, we propose an end-to-end metric learning framework. Specifically, a new linear metric learning (LMML) is first proposed to jointly learn adaptive metrics and the optimal classification hyperplanes , where dissimilar samples are separated by maximizing classification margin. Then a nonlinear metric learning model (called RLMML) is developed based on a bound nonlinear kernel function to extend LMML. The non-convexity of the proposed models makes them difficult to optimize. The half-quadratic optimization algorithms are developed to solve iteratively the problems, by which the optimal classification hyperplane and adaptive metric are alternatively optimized. Moreover, the resulting algorithms are proved to be convergent theoretically. Numerical experiments on different types of data sets show the effectiveness of the proposed algorithms. Finally, the Wilcoxon test shows also the feasibility and effectiveness of the proposed models.},
  archive      = {J_NN},
  author       = {Yidan Wang and Liming Yang},
  doi          = {10.1016/j.neunet.2022.01.002},
  journal      = {Neural Networks},
  pages        = {111-120},
  shortjournal = {Neural Netw.},
  title        = {Joint learning adaptive metric and optimal classification hyperplane},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biological convolutions improve DNN robustness to noise and
generalisation. <em>NN</em>, <em>148</em>, 96–110. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Networks (DNNs) have achieved superhuman accuracy on standard image classification benchmarks. Their success has reignited significant interest in their use as models of the primate visual system, bolstered by claims of their architectural and representational similarities. However, closer scrutiny of these models suggests that they rely on various forms of shortcut learning to achieve their impressive performance, such as using texture rather than shape information. Such superficial solutions to image recognition have been shown to make DNNs brittle in the face of more challenging tests such as noise-perturbed or out-of-distribution images, casting doubt on their similarity to their biological counterparts. In the present work, we demonstrate that adding fixed biological filter banks , in particular banks of Gabor filters , helps to constrain the networks to avoid reliance on shortcuts, making them develop more structured internal representations and more tolerance to noise. Importantly, they also gained around 20–35\% improved accuracy when generalising to our novel out-of-distribution test image sets over standard end-to-end trained architectures. We take these findings to suggest that these properties of the primate visual system should be incorporated into DNNs to make them more able to cope with real-world vision and better capture some of the more impressive aspects of human visual perception such as generalisation.},
  archive      = {J_NN},
  author       = {Benjamin D. Evans and Gaurav Malhotra and Jeffrey S. Bowers},
  doi          = {10.1016/j.neunet.2021.12.005},
  journal      = {Neural Networks},
  pages        = {96-110},
  shortjournal = {Neural Netw.},
  title        = {Biological convolutions improve DNN robustness to noise and generalisation},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time stabilization of complex-valued neural networks
with proportional delays and inertial terms: A non-separation approach.
<em>NN</em>, <em>148</em>, 86–95. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article mainly dedicates on the issue of finite-time stabilization of complex-valued neural networks with proportional delays and inertial terms via directly constructing Lyapunov functions without separating the original complex-valued neural networks into two real-valued subsystems equivalently. First of all, in order to facilitate the analysis of the second-order derivative caused by the inertial term, two intermediate variables are introduced to transfer complex-valued inertial neural networks (CVINNs) into the first-order differential equation form. Then, under the finite-time stability theory, some new criteria with less conservativeness are established to ensure the finite-time stabilizability of CVINNs by a newly designed complex-valued feedback controller . In addition, for reducing expenses of the control, an adaptive control strategy is also proposed to achieve the finite-time stabilization of CVINNs. At last, numerical examples are given to demonstrate the validity of the derived results.},
  archive      = {J_NN},
  author       = {Changqing Long and Guodong Zhang and Zhigang Zeng and Junhao Hu},
  doi          = {10.1016/j.neunet.2022.01.005},
  journal      = {Neural Networks},
  pages        = {86-95},
  shortjournal = {Neural Netw.},
  title        = {Finite-time stabilization of complex-valued neural networks with proportional delays and inertial terms: A non-separation approach},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Corrigendum to “fractional-order discontinuous systems with
indefinite LKFs: An application to fractional-order neural networks with
time delays” [neural networks 145 (2022) 319–330]. <em>NN</em>,
<em>148</em>, 85. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {K. Udhayakumar and Fathalla A. Rihan and R. Rakkiyappan and Jinde Cao},
  doi          = {10.1016/j.neunet.2022.01.008},
  journal      = {Neural Networks},
  pages        = {85},
  shortjournal = {Neural Netw.},
  title        = {Corrigendum to “Fractional-order discontinuous systems with indefinite LKFs: An application to fractional-order neural networks with time delays” [Neural networks 145 (2022) 319–330]},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noise-robust voice conversion with domain adversarial
training. <em>NN</em>, <em>148</em>, 74–84. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice conversion has made great progress in the past few years under the studio-quality test scenario in terms of speech quality and speaker similarity. However, in real applications, test speech from source speaker or target speaker can be corrupted by various environment noises, which seriously degrade the speech quality and speaker similarity. In this paper, we propose a novel encoder–decoder based noise-robust voice conversion framework, which consists of a speaker encoder, a content encoder, a decoder, and two domain adversarial neural networks . Specifically, we integrate disentangling speaker and content representation technique with domain adversarial training technique. Domain adversarial training makes speaker representations and content representations extracted by speaker encoder and content encoder from clean speech and noisy speech in the same space, respectively. In this way, the learned speaker and content representations are noise-invariant. Therefore, the two noise-invariant representations can be taken as input by the decoder to predict the clean converted spectrum. The experimental results demonstrate that our proposed method can synthesize clean converted speech under noisy test scenarios, where the source speech and target speech can be corrupted by seen or unseen noise types during the training process. Additionally, both speech quality and speaker similarity are improved.},
  archive      = {J_NN},
  author       = {Hongqiang Du and Lei Xie and Haizhou Li},
  doi          = {10.1016/j.neunet.2022.01.003},
  journal      = {Neural Networks},
  pages        = {74-84},
  shortjournal = {Neural Netw.},
  title        = {Noise-robust voice conversion with domain adversarial training},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trade off analysis between fixed-time stabilization and
energy consumption of nonlinear neural networks. <em>NN</em>,
<em>148</em>, 66–73. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on trade off analysis between fixed-time stabilization and energy consumption for a type of nonlinear neural networks (NNs). By constructing a compound switching controller and utilizing inequality techniques, a sufficient condition is proposed to ensure the fixed-time stabilization. Then, an estimate of the upper bound of the energy consumed by the controller in the control process is given. Furthermore, the quantitative analysis of the trade-off between the control time and energy consumption is studied. This article reveals that appropriate control parameters can balance the above two indicators to achieve an optimal control state. Finally, the presented theoretical results are verified by two numerical examples.},
  archive      = {J_NN},
  author       = {Yuchun Wang and Song Zhu and Hu Shao and Li Wang and Shiping Wen},
  doi          = {10.1016/j.neunet.2022.01.004},
  journal      = {Neural Networks},
  pages        = {66-73},
  shortjournal = {Neural Netw.},
  title        = {Trade off analysis between fixed-time stabilization and energy consumption of nonlinear neural networks},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering parametric activation functions. <em>NN</em>,
<em>148</em>, 48–65. (<a
href="https://doi.org/10.1016/j.neunet.2022.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that the choice of activation function can significantly affect the performance of deep learning networks. However, the benefits of novel activation functions have been inconsistent and task dependent, and therefore the rectified linear unit (ReLU) is still the most commonly used. This paper proposes a technique for customizing activation functions automatically, resulting in reliable improvements in performance. Evolutionary search is used to discover the general form of the function, and gradient descent to optimize its parameters for different parts of the network and over the learning process. Experiments with four different neural network architectures on the CIFAR-10 and CIFAR-100 image classification datasets show that this approach is effective. It discovers both general activation functions and specialized functions for different architectures, consistently improving accuracy over ReLU and other activation functions by significant margins. The approach can therefore be used as an automated optimization step in applying deep learning to new tasks.},
  archive      = {J_NN},
  author       = {Garrett Bingham and Risto Miikkulainen},
  doi          = {10.1016/j.neunet.2022.01.001},
  journal      = {Neural Networks},
  pages        = {48-65},
  shortjournal = {Neural Netw.},
  title        = {Discovering parametric activation functions},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time synchronization of quaternion-valued neural
networks with delays: A switching control method without decomposition.
<em>NN</em>, <em>148</em>, 37–47. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a class of quaternion-valued neural networks (QVNNs) with discrete and distributed time delays , its finite-time synchronization (FTSYN) is addressed in this paper. Instead of decomposition, a direct analytical method named two-step analysis is proposed. That method can always be used to study FTSYN, under either 1-norm or 2-norm of quaternion. Compared with the decomposing method, the two-step method is also suitable for models that are not easily decomposed. Furthermore, a switching controller based on the two-step method is proposed. In addition, two criteria are given to realize the FTSYN of QVNNs. At last, three numerical examples illustrate the feasibility, effectiveness and practicability of our method.},
  archive      = {J_NN},
  author       = {Tao Peng and Jie Zhong and Zhengwen Tu and Jianquan Lu and Jungang Lou},
  doi          = {10.1016/j.neunet.2021.12.012},
  journal      = {Neural Networks},
  pages        = {37-47},
  shortjournal = {Neural Netw.},
  title        = {Finite-time synchronization of quaternion-valued neural networks with delays: A switching control method without decomposition},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-dimensional conditional mutual information with
application on the EEG signal analysis for spatial cognitive ability
evaluation. <em>NN</em>, <em>148</em>, 23–36. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to explore an effective method to evaluate spatial cognitive ability, which can effectively extract and classify the feature of EEG signals collected from subjects participating in the virtual reality (VR) environment; and evaluate the training effect objectively and quantitatively to ensure the objectivity and accuracy of spatial cognition evaluation, according to the classification results . Therefore, a multi-dimensional conditional mutual information (MCMI) method is proposed, which could calculate the coupling strength of two channels considering the influence of other channels. The coupled characteristics of the multi-frequency combination were transformed into multi-spectral images, and the image data were classified employing the convolutional neural networks (CNN) model. The experimental results showed that the multi-spectral image transform features based on MCMI are better in classification than other methods, and among the classification results of six band combinations, the best classification accuracy of Beta1–Beta2–Gamma combination is 98.3\%. The MCMI characteristics on the Beta1–Beta2–Gamma band combination can be a biological marker for the evaluation of spatial cognition. The proposed feature extraction method based on MCMI provides a new perspective for spatial cognitive ability assessment and analysis.},
  archive      = {J_NN},
  author       = {Dong Wen and Rou Li and Mengmeng Jiang and Jingjing Li and Yijun Liu and Xianling Dong and M. Iqbal Saripan and Haiqing Song and Wei Han and Yanhong Zhou},
  doi          = {10.1016/j.neunet.2021.12.010},
  journal      = {Neural Networks},
  pages        = {23-36},
  shortjournal = {Neural Netw.},
  title        = {Multi-dimensional conditional mutual information with application on the EEG signal analysis for spatial cognitive ability evaluation},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exponential synchronization for variable-order fractional
discontinuous complex dynamical networks with short memory via impulsive
control. <em>NN</em>, <em>148</em>, 13–22. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the exponential synchronization issue for variable-order fractional complex dynamical networks (FCDNs) with short memory and derivative couplings via the impulsive control scheme, where dynamical nodes are modeled to be discontinuous. Firstly, the mathematics model with respect to variable-order fractional systems with short memory is established under the impulsive controller, in which the impulse strength is not only determined by the impulse control gain, but also the order of the control systems. Secondly, the exponential stability criterion for variable-order fractional systems with short memory is developed. Thirdly, the hybrid controller , which consists of the impulsive coupling controller and the discontinuous feedback controller , is designed to realize the synchronization objective. In addition, by constructing Lyapunov functional and applying inequality analysis techniques, the synchronization conditions are achieved in terms of linear matrix inequalities (LMIs). Finally, two simulation examples are performed to verify the effectiveness of the developed synchronization scheme and the theoretical outcomes.},
  archive      = {J_NN},
  author       = {Ruihong Li and Huaiqin Wu and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.12.021},
  journal      = {Neural Networks},
  pages        = {13-22},
  shortjournal = {Neural Netw.},
  title        = {Exponential synchronization for variable-order fractional discontinuous complex dynamical networks with short memory via impulsive control},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolved explainable classifications for lymph node
metastases. <em>NN</em>, <em>148</em>, 1–12. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel evolutionary approach for Explainable Artificial Intelligence is presented: the “Evolved Explanations” model (EvEx). This methodology combines Local Interpretable Model Agnostic Explanations (LIME) with Multi-Objective Genetic Algorithms to allow for automated segmentation parameter tuning in image classification tasks. In this case, the dataset studied is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously. The final explanation is computed as the mean of all explanations generated by Pareto front individuals, evolved by the developed genetic algorithm . To enhance reproducibility and traceability of the explanations, each of them was generated from several different seeds, randomly chosen. The observed results show remarkable agreement between different seeds. Despite the stochastic nature of LIME explanations, regions of high explanation weights proved to have good agreement in the heat maps , as computed by pixel-wise relative standard deviations . The found heat maps coincide with expert medical segmentations, which demonstrates that this methodology can find high quality explanations (according to the evaluation metrics), with the novel advantage of automated parameter fine tuning. These results give additional insight into the inner workings of neural network black box decision making for medical data.},
  archive      = {J_NN},
  author       = {Iam Palatnik de Sousa and Marley M.B.R. Vellasco and Eduardo Costa da Silva},
  doi          = {10.1016/j.neunet.2021.12.014},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {Evolved explainable classifications for lymph node metastases},
  volume       = {148},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022j). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>147</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(22)00020-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00020-X},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022g). Current events. <em>NN</em>, <em>147</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(22)00019-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(22)00019-3},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum to “a model of operant learning based on
chaotically varying synaptic strength” [neural netw. 108 (2018)
114–127]. <em>NN</em>, <em>147</em>, 198. (<a
href="https://doi.org/10.1016/j.neunet.2019.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Tianqi Wei and Barbara Webb},
  doi          = {10.1016/j.neunet.2019.02.003},
  journal      = {Neural Networks},
  pages        = {198},
  shortjournal = {Neural Netw.},
  title        = {Corrigendum to “A model of operant learning based on chaotically varying synaptic strength” [Neural netw. 108 (2018) 114–127]},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HRel: Filter pruning based on high relevance between
activation maps and class labels. <em>NN</em>, <em>147</em>, 186–197.
(<a href="https://doi.org/10.1016/j.neunet.2021.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an Information Bottleneck theory based filter pruning method that uses a statistical measure called Mutual Information (MI). The MI between filters and class labels, also called Relevance , is computed using the filter’s activation maps and the annotations. The filters having High Relevance (HRel) are considered to be more important. Consequently, the least important filters, which have lower Mutual Information with the class labels, are pruned. Unlike the existing MI based pruning methods, the proposed method determines the significance of the filters purely based on their corresponding activation map’s relationship with the class labels. Architectures such as LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50 are utilized to demonstrate the efficacy of the proposed pruning method over MNIST, CIFAR-10 and ImageNet datasets . The proposed method shows the state-of-the-art pruning results for LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50 architectures. In the experiments, we prune 97.98\%, 84.85\%, 76.89\%, 76.95\%, and 63.99\% of Floating Point Operation (FLOP)s from LeNet-5, VGG-16, ResNet-56, ResNet-110, and ResNet-50 respectively. The proposed HRel pruning method outperforms recent state-of-the-art filter pruning methods. Even after pruning the filters from convolutional layers of LeNet-5 drastically ( i . e ., from 20, 50 to 2, 3, respectively), only a small accuracy drop of 0.52\% is observed. Notably, for VGG-16, 94.98\% parameters are reduced, only with a drop of 0.36\% in top-1 accuracy. ResNet-50 has shown a 1.17\% drop in the top-5 accuracy after pruning 66.42\% of the FLOPs. In addition to pruning, the Information Plane dynamics of Information Bottleneck theory is analyzed for various Convolutional Neural Network architectures with the effect of pruning. The code is available at https://github.com/sarvanichinthapalli/HRel .},
  archive      = {J_NN},
  author       = {C.H. Sarvani and Mrinmoy Ghorai and Shiv Ram Dubey and S.H. Shabbeer Basha},
  doi          = {10.1016/j.neunet.2021.12.017},
  journal      = {Neural Networks},
  pages        = {186-197},
  shortjournal = {Neural Netw.},
  title        = {HRel: Filter pruning based on high relevance between activation maps and class labels},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient joint model learning, segmentation and model
updating for visual tracking. <em>NN</em>, <em>147</em>, 175–185. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tracking-by-segmentation framework is widely used in visual tracking to handle severe appearance change such as deformation and occlusion. Tracking-by-segmentation methods first segment the target object from the background, then use the segmentation result to estimate the target state. In existing methods, target segmentation is formulated as a superpixel labeling problem constrained by a target likelihood constraint, a spatial smoothness constraint and a temporal consistency constraint. The target likelihood is calculated by a discriminative part model trained independently from the superpixel labeling framework and updated online using historical tracking results as pseudo-labels. Due to the lack of spatial and temporal constraints and inaccurate pseudo-labels, the discriminative model is unreliable and may lead to tracking failure. This paper addresses the aforementioned problems by integrating the objective function of model training into the target segmentation optimization framework. Thus, during the optimization process, the discriminative model can be constrained by spatial and temporal constraints and provides more accurate target likelihoods for part labeling, and the results produce more reliable pseudo-labels for model learning. Moreover, we also propose a supervision switch mechanism to detect erroneous pseudo-labels caused by a severe change in data distribution and switch the classifier to a semi-supervised setting in such a case. Evaluation results on OTB2013, OTB2015 and TC-128 benchmarks demonstrate the effectiveness of the proposed tracking algorithm.},
  archive      = {J_NN},
  author       = {Wei Han and Chamara Kasun Liyanaarachchi Lekamalage and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2021.12.018},
  journal      = {Neural Networks},
  pages        = {175-185},
  shortjournal = {Neural Netw.},
  title        = {Efficient joint model learning, segmentation and model updating for visual tracking},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symmetric positive definite manifold learning and its
application in fault diagnosis. <em>NN</em>, <em>147</em>, 163–174. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locally linear embedding (LLE) is an effective tool to extract the significant features from a dataset. However, most of the relevant existing algorithms assume that the original dataset resides on a Euclidean space , unfortunately nearly all the original data space is non-Euclidean. In addition, the original LLE does not use the discriminant information of the dataset, which will degrade its performance in feature extraction. To address these problems raised in the conventional LLE, we first employ the original dataset to construct a symmetric positive definite manifold, and then estimate the tangent space of this manifold. Furthermore, the local and global discriminant information are integrated into the LLE, and the improved LLE is operated in the tangent space to extract the important features. We introduce Iris dataset to analyze the capability of the proposed method to extract features. Finally, several experiments are performed on five machinery datasets, and experimental results indicate that our proposed method can extract the excellent low-dimensional representations of the original dataset. Compared with the state-of-the-art methods, the proposed algorithm shows a strong capability for fault diagnosis.},
  archive      = {J_NN},
  author       = {Yuanhong Liu and Zebiao Hu and Yansheng Zhang},
  doi          = {10.1016/j.neunet.2021.12.013},
  journal      = {Neural Networks},
  pages        = {163-174},
  shortjournal = {Neural Netw.},
  title        = {Symmetric positive definite manifold learning and its application in fault diagnosis},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Command-filter-based adaptive neural tracking control for a
class of nonlinear MIMO state-constrained systems with input delay and
saturation. <em>NN</em>, <em>147</em>, 152–162. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of adaptive tracking control for a class of nonlinear multi-input and multi-output (MIMO) state-constrained systems with input delay and saturation. During the process of the control scheme, neural network is employed to approximate the unknown nonlinear uncertainties and the appropriate barrier Lyapunov function is introduced to prevent violation of the constraint. In addition, for the issue of input saturation with time delay , a smooth non-affine approximate function and a novel auxiliary system are utilized, respectively. Moreover, adaptive neural tracking control is developed by combining the command filtering backstepping approach, which effectively avoids the explosion of differentiation and reduces the computation burden. The introduced filtering error compensating system brings a significant improvement for the system tracking performance. Finally, the simulation result is presented to verify the feasibility of the proposed strategy.},
  archive      = {J_NN},
  author       = {Yuhao Zhou and Xin Wang and Rui Xu},
  doi          = {10.1016/j.neunet.2021.12.006},
  journal      = {Neural Networks},
  pages        = {152-162},
  shortjournal = {Neural Netw.},
  title        = {Command-filter-based adaptive neural tracking control for a class of nonlinear MIMO state-constrained systems with input delay and saturation},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully corrective gradient boosting with squared hinge: Fast
learning rates and early stopping. <em>NN</em>, <em>147</em>, 136–151.
(<a href="https://doi.org/10.1016/j.neunet.2021.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient boosting method with theoretical guarantees for binary classification . There are three key ingredients of the proposed boosting method: a fully corrective greedy (FCG) update, a differentiable squared hinge (also called truncated quadratic ) loss function, and an efficient alternating direction method of multipliers (ADMM) solver. Compared with traditional boosting methods, on one hand, the FCG update accelerates the numerical convergence rate, and on the other hand, the squared hinge loss inherits the robustness of the hinge loss for classification and maintains the theoretical benefits of the square loss in regression. The ADMM solver with guaranteed fast convergence then provides an efficient implementation for the proposed boosting method. We conduct both theoretical analysis and numerical verification to show the outperformance of the proposed method. Theoretically, a fast learning rate of order O ( ( m / log m ) − 1 / 2 ) O((m/logm)−1/2) is proved under certain standard assumptions, where m m is the size of sample set. Numerically, a series of toy simulations and real data experiments are carried out to verify the developed theory.},
  archive      = {J_NN},
  author       = {Jinshan Zeng and Min Zhang and Shao-Bo Lin},
  doi          = {10.1016/j.neunet.2021.12.016},
  journal      = {Neural Networks},
  pages        = {136-151},
  shortjournal = {Neural Netw.},
  title        = {Fully corrective gradient boosting with squared hinge: Fast learning rates and early stopping},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based adaptive neural tracking control for a class
of nonlinear systems with prescribed performance and input dead-zone
constraints. <em>NN</em>, <em>147</em>, 126–135. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of output feedback neural network (NN) learning tracking control for nonlinear strict feedback systems subject to prescribed performance and input dead-zone constraints. First, an NN is utilized to approximate the unknown nonlinear functions , then a state observer is developed to estimate the unmeasurable states. Second, based on the command filter method, an output feedback NN learning backstepping control algorithm is established. Third, a prescribed performance function is employed to ensure the transient performance of the closed-loop systems and forces the tracking error to fall within the prescribed performance boundary. It is rigorously proved mathematically that all the signals in the closed-loop systems are semi-globally uniformly ultimately bounded and the tracking error can converge to an arbitrarily small neighborhood of the origin. Finally, a numerical example and an application example of the electromechanical system are given to show effectiveness of the acquired control algorithm.},
  archive      = {J_NN},
  author       = {Guangdeng Zong and Yudi Wang and Hamid Reza Karimi and Kaibo Shi},
  doi          = {10.1016/j.neunet.2021.12.019},
  journal      = {Neural Networks},
  pages        = {126-135},
  shortjournal = {Neural Netw.},
  title        = {Observer-based adaptive neural tracking control for a class of nonlinear systems with prescribed performance and input dead-zone constraints},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient network compression via few-shot slimming.
<em>NN</em>, <em>147</em>, 113–125. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While previous network compression methods achieve great success, most of them rely on the abundant training data which is, unfortunately, often unavailable in practice due to some reasons, e.g., privacy issues, storage constraints, and transmission limitations. A promising way to solve this problem is to perform compression with a few unlabeled data . Proceeding along this way, we propose a novel few-shot network compression framework named Few-Shot Slimming (FSS). FSS follows the student/teacher paradigm, and contains two steps: (1) construct the student by inheriting principal feature maps from the teacher; (2) refine the student feature representation by knowledge distillation with an enhanced mixing data augmentation method called GridMix. Specifically, in the first step, we employ normalized cross correlation to perform the principal feature analysis, and then theoretically construct a new indicator to select the most informative feature maps from the teacher for the student. The indicator is based on the variances of feature maps which can efficiently quantitate the information richness of the input feature maps in a feature-agnostic manner. In the second step, we perform the knowledge distillation for the initialized student in first step with a novel grid-based mixing data augmentation technique which greatly extends the limited sample dataset . In this way, the student is able to refine its feature representation and achieves a better result. Extensive experiments on multiple benchmarks demonstrate the state-of-the-art performance of FSS. For example, by using 0.2\% label-free data of full training set, FSS yields a 60\% FLOPs reduction for DenseNet-40 on CIFAR-10 with only a loss of 0.8\% in top-1 accuracy, achieving a result on par with that obtained by the conventional full-data methods.},
  archive      = {J_NN},
  author       = {Junjie He and Yinzhang Ding and Ming Zhang and Dongxiao Li},
  doi          = {10.1016/j.neunet.2021.12.011},
  journal      = {Neural Networks},
  pages        = {113-125},
  shortjournal = {Neural Netw.},
  title        = {Towards efficient network compression via few-shot slimming},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMPP: Differentiable multi-pruner and predictor for neural
network pruning. <em>NN</em>, <em>147</em>, 103–112. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network pruning can trim the over-parameterized neural networks effectively by removing a number of network parameters. However, the traditional rule-based approaches always depend on manual experience. Existing heuristic search methods in discrete search spaces are usually time consuming and sub-optimal. In this paper, we develop a differentiable multi-pruner and predictor (DMPP) to prune neural networks automatically. The pruner composed of learnable parameters generates the pruning ratios of all convolutional layers as the continuous representation of the network. The neural network-based predictor is employed to predict the performance of different structures, which can accelerate the search process. Pruner and predictor enable us to directly employ gradient-based optimization to find a better structure. In addition, multi-pruner is presented to improve the efficiency of search, and knowledge distillation is leveraged to improve the performance of the pruned network. To evaluate the effectiveness of the proposed method, extensive experiments are performed on CIFAR-10, CIFAR-100, and ImageNet datasets with VGGNet and ResNet . Results show that the present DMPP can achieve a better performance than many previous state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Jiaxin Li and Bo Zhao and Derong Liu},
  doi          = {10.1016/j.neunet.2021.12.020},
  journal      = {Neural Networks},
  pages        = {103-112},
  shortjournal = {Neural Netw.},
  title        = {DMPP: Differentiable multi-pruner and predictor for neural network pruning},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SurvNAM: The machine learning survival model explanation.
<em>NN</em>, <em>147</em>, 81–102. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extension of the Neural Additive Model (NAM) called SurvNAM and its modifications are proposed to explain predictions of a black-box machine learning survival model. The method is based on applying the original NAM to solving the explanation problem in the framework of survival analysis . The basic idea behind SurvNAM is to train the network by means of a specific expected loss function which takes into account peculiarities of the survival model predictions. Moreover, the loss function approximates the black-box model by the extension of the Cox proportional hazards model , which uses the well-known Generalized Additive Model (GAM) in place of the simple linear relationship of covariates . The proposed method SurvNAM allows performing local and global explanations. The global explanation uses the whole training dataset. In contrast to the global explanation, a set of synthetic examples around the explained example are randomly generated for the local explanation. The proposed modifications of SurvNAM are based on using the Lasso-based regularization for functions from GAM and for a special representation of the GAM functions using their weighted linear and non-linear parts, which is implemented as a shortcut connection. Many numerical experiments illustrate efficiency of SurvNAM.},
  archive      = {J_NN},
  author       = {Lev V. Utkin and Egor D. Satyukov and Andrei V. Konstantinov},
  doi          = {10.1016/j.neunet.2021.12.015},
  journal      = {Neural Networks},
  pages        = {81-102},
  shortjournal = {Neural Netw.},
  title        = {SurvNAM: The machine learning survival model explanation},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation capabilities of measure-preserving neural
networks. <em>NN</em>, <em>147</em>, 72–80. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measure-preserving neural networks are well-developed invertible models, however, their approximation capabilities remain unexplored. This paper rigorously analyzes the approximation capabilities of existing measure-preserving neural networks including NICE and RevNets . It is shown that for compact U ⊂ R D U⊂RD with D ≥ 2 D≥2 , the measure-preserving neural networks are able to approximate arbitrary measure-preserving map ψ : U → R D ψ:U→RD which is bounded and injective in the L p Lp -norm. In particular, any continuously differentiable injective map with ± 1 ±1 determinant of Jacobian is measure-preserving, thus can be approximated.},
  archive      = {J_NN},
  author       = {Aiqing Zhu and Pengzhan Jin and Yifa Tang},
  doi          = {10.1016/j.neunet.2021.12.007},
  journal      = {Neural Networks},
  pages        = {72-80},
  shortjournal = {Neural Netw.},
  title        = {Approximation capabilities of measure-preserving neural networks},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NoAS-DS: Neural optimal architecture search for detection of
diverse DNA signals. <em>NN</em>, <em>147</em>, 63–71. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network architectures are high-performing variable models that can solve many learning tasks. Designing architectures manually require substantial time and also prior knowledge and expertise to develop a high-accuracy model. Most of the architecture search methods are developed over the task of image classification resulting in the building of complex architectures intended for large data inputs such as images. Motivated by the applications of DNA computing in Neural Architecture Search (NAS), we propose NoAS-DS which is specifically built for the architecture search of sequence-based classification tasks . Furthermore, NoAS-DS is applied to the task of predicting binding sites. Unlike other methods that implement only Convolution layers, NoAS-DS, specifically combines Convolution and LSTM layers that helps in the process of automatic architecture building. This hybrid approach helped in achieving high accuracy results on TFBS and RBP datasets which outperformed other models in TF-DNA binding prediction tasks. The best architectures generated by the proposed model can be applied to other DNA datasets of similar nature using transfer learning technique that demonstrates its generalization capability. This greatly reduces the effort required to build new architectures for other prediction tasks.},
  archive      = {J_NN},
  author       = {Kaushik Bhargav Sivangi and Chandra Mohan Dasari and Santhosh Amilpur and Raju Bhukya},
  doi          = {10.1016/j.neunet.2021.12.009},
  journal      = {Neural Networks},
  pages        = {63-71},
  shortjournal = {Neural Netw.},
  title        = {NoAS-DS: Neural optimal architecture search for detection of diverse DNA signals},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UTRAD: Anomaly detection and localization with
u-transformer. <em>NN</em>, <em>147</em>, 53–62. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is an active research field in industrial defect detection and medical disease detection. However, previous anomaly detection works suffer from unstable training, or non-universal criteria of evaluating feature distribution. In this paper, we introduce UTRAD, a U - TR ansformer based A nomaly D etection framework. Deep pre-trained features are regarded as dispersed word tokens, and represented with transformer-based autoencoders . With reconstruction on more informative feature distribution instead of raw images, we achieve a more stable training process and a more precise anomaly detection and localization result. In addition, our proposed UTRAD has a multi-scale pyramidal hierarchy with skip connections that help detect both multi-scale structural and non-structural anomalies. As attention layers are decomposed to multi-level patches, UTRAD significantly reduces the computational cost and memory usage compared with the vanilla transformer. Experiments on industrial dataset MVtec AD and medical datasets Retinal-OCT, Brain-MRI, Head-CT have been conducted. Our proposed UTRAD out-performs all other state-of-the-art methods in the above datasets. Code released at https://github.com/gordon-chenmo/UTRAD .},
  archive      = {J_NN},
  author       = {Liyang Chen and Zhiyuan You and Nian Zhang and Juntong Xi and Xinyi Le},
  doi          = {10.1016/j.neunet.2021.12.008},
  journal      = {Neural Networks},
  pages        = {53-62},
  shortjournal = {Neural Netw.},
  title        = {UTRAD: Anomaly detection and localization with U-transformer},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast writer adaptation with style extractor network for
handwritten text recognition. <em>NN</em>, <em>147</em>, 42–52. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writing style is an abstract attribute in handwritten text. It plays an important role in recognition systems and is not easy to define explicitly. Considering the effect of writing style, a writer adaptation method is proposed to transform a writer-independent recognizer toward a particular writer. This transformation has the potential to significantly increase accuracy. In this paper, under the deep learning framework, we propose a general fast writer adaptation solution. Specifically, without depending on other complex skills, a well designed style extractor network (SEN) trained by identification loss (IDL) is introduced to explicitly extract personalized writer information. The architecture of SEN consists of a stack of convolutional layers followed by a recurrent neural network with gated recurrent units to remove semantic context and retain writer information. Then, the outputs of the GRU are further integrated into a one-dimensional vector that is adopted to represent writing style. Finally, the extracted style information is fed into the writer-independent recognizer to achieve adaptation. Validated on offline handwritten text recognition tasks, the proposed fast sentence-level adaptation achieves remarkable improvements in Chinese and English text recognition tasks. Specifically, in the HETR task, a multi-information fusion network that is equipped with a hybrid attention mechanism and that integrates visual features, context features and writing style is proposed. In addition, under the same condition (only one writer-specific text line used as adaptation data), the proposed solution, without consuming extra time, can significantly outperform the previous multiple-pass decoding method. The code is available at https://github.com/Wukong90/Handwritten-Text-Recognition .},
  archive      = {J_NN},
  author       = {Zi-Rui Wang and Jun Du},
  doi          = {10.1016/j.neunet.2021.12.002},
  journal      = {Neural Networks},
  pages        = {42-52},
  shortjournal = {Neural Netw.},
  title        = {Fast writer adaptation with style extractor network for handwritten text recognition},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature correlation-steered capsule network for object
detection. <em>NN</em>, <em>147</em>, 25–41. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite Convolutional Neural Networks (CNNs) based approaches have been successful in objects detection, they predominantly focus on positioning discriminative regions while overlooking the internal holistic part-whole associations within objects. This would ultimately lead to the neglect of feature relationships between object and its parts as well as among those parts, both of which are significantly helpful for detecting discriminative parts. In this paper, we propose to “look insider the objects” by digging into part-whole feature correlations and take the attempts to leverage those correlations endowed by the Capsule Network (CapsNet) for robust object detection. Actually, highly correlated capsules across adjacent layers share high familiarity, which will be more likely to be routed together. In light of this, we take such correlations between different capsules of the preceding training samples as an awareness to constrain the subsequent candidate voting scope during the routing procedure , and a Feature Correlation-Steered CapsNet (FCS-CapsNet) with Locally-Constrained Expectation-Maximum (EM) Routing Agreement (LCEMRA) is proposed. Different from conventional EM routing, LCEMRA stipulates that only those relevant low-level capsules (parts) meeting the requirement of quantified intra-object cohesiveness can be clustered to make up high-level capsules (objects). In doing so, part-object associations can be dug by transformation weighting matrixes between capsules layers during such “part backtracking” procedure. LCEMRA enables low-level capsules to selectively gather projections from a non-spatially-fixed set of high-level capsules. Experiments on VOC2007, VOC2012, HKU-IS, DUTS, and COCO show that FCS-CapsNet can achieve promising object detection effects across multiple evaluation metrics , which are on-par with state-of-the-arts.},
  archive      = {J_NN},
  author       = {Zhongqi Lin and Jingdun Jia and Feng Huang and Wanlin Gao},
  doi          = {10.1016/j.neunet.2021.12.003},
  journal      = {Neural Networks},
  pages        = {25-41},
  shortjournal = {Neural Netw.},
  title        = {Feature correlation-steered capsule network for object detection},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Associative anticipatory learning and control of the
cerebellar cortex based on the spike-timing-dependent plasticity of the
parallel fiber-purkinje cell synapses. <em>NN</em>, <em>147</em>, 10–24.
(<a href="https://doi.org/10.1016/j.neunet.2021.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time delays are inevitable in the neural processing of sensorimotor systems ; small delays can cause severe damage to movement accuracy and stability. It is strongly suggested that the cerebellum compensates for delays in neural signal processing and performs predictive control . Neural computational theories have explored concepts of the internal models of control objects—believed to avoid delays by providing internal feedback information—although there has been no clear relevance to neural processing. The timing-dependent plasticity of parallel fiber-Purkinje cell synapses is well known. The long-term depression of the synapse is observed when parallel fiber activation precedes climbing fiber activation within − − 50–300 ms, and is the greatest within 50–200 ms. This paper presents a theory that this temporal difference of 50–200 ms is the basis for an associative anticipation of as many milliseconds. Associative learning can theoretically connect an input signal to a desired signal; therefore, a 50–200 ms earlier input signal can be connected to a desired output signal through temporary asymmetric plasticity. After learning is completed, an input signal generates a desired output signal that appears 50–200 ms later. For the associative learning of temporally continuous signals, this study integrates the universal function approximation capability of the cerebellar cortex model and temporally asymmetric synaptic plasticity to create the theory of associative anticipatory learning of the cerebellum . The effective motor control of this learning is demonstrated by adaptively stabilizing an inverted pendulum with a delay similar to that done by humans.},
  archive      = {J_NN},
  author       = {Masahiko Fujita},
  doi          = {10.1016/j.neunet.2021.12.004},
  journal      = {Neural Networks},
  pages        = {10-24},
  shortjournal = {Neural Netw.},
  title        = {Associative anticipatory learning and control of the cerebellar cortex based on the spike-timing-dependent plasticity of the parallel fiber-purkinje cell synapses},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A one-layer recurrent neural network for nonsmooth
pseudoconvex optimization with quasiconvex inequality and affine
equality constraints. <em>NN</em>, <em>147</em>, 1–9. (<a
href="https://doi.org/10.1016/j.neunet.2021.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As two important types of generalized convex functions , pseudoconvex and quasiconvex functions appear in many practical optimization problems . The lack of convexity poses some difficulties in solving pseudoconvex optimization with quasiconvex constraint functions. In this paper, we propose a one-layer recurrent neural network for solving such problems. We prove that the state of the proposed neural network is convergent from the feasible region to an optimal solution of the given optimization problem. We show that the proposed neural network has several advantages over the existing neural networks for pseudoconvex optimization. Specifically, the proposed neural network is applicable to optimization problems with quasiconvex inequality constraints as well as affine equality constraints. In addition, parameter matrix inversion is avoided and some assumptions on the objective function and inequality constraints in existing results are relaxed. We demonstrate the superior performance and characteristics of the proposed neural network with simulation results in three numerical examples.},
  archive      = {J_NN},
  author       = {Na Liu and Jun Wang and Sitian Qin},
  doi          = {10.1016/j.neunet.2021.12.001},
  journal      = {Neural Networks},
  pages        = {1-9},
  shortjournal = {Neural Netw.},
  title        = {A one-layer recurrent neural network for nonsmooth pseudoconvex optimization with quasiconvex inequality and affine equality constraints},
  volume       = {147},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022k). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>146</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00502-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00502-5},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022h). Current events. <em>NN</em>, <em>146</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00500-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00500-1},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep semi-supervised learning via dynamic anchor graph
embedding in latent space. <em>NN</em>, <em>146</em>, 350–360. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep semi-supervised graph embedding learning has drawn much attention for its appealing performance on the data with a pre-specified graph structure, which could be predefined or empirically constructed based on given data samples. However, the pre-specified graphs often contain considerable noisy/inaccurate connections and have a huge size for large datasets. Most existing embedding algorithms just take the graph off the shelf during the whole training stage and thus are easy to be misled by the inaccurate graph edges, as well as may result in large model size. In this paper, we attempt to address these issues by proposing a novel deep semi-supervised algorithm for simultaneous graph embedding and node classification , utilizing dynamic graph learning in neural network hidden layer space. Particularly, we construct an anchor graph to summarize the whole dataset using the hidden layer features of a consistency-constrained network. The anchor graph is used for sampling node neighborhood context, which is then presented together with node labels as contextual information to train an embedding network. The outputs of the consistency network and the embedding networks are finally concatenated together to pass a softmax function to perform node classification . The two networks are optimized jointly using both labeled and unlabeled data to minimize a single semi-supervised objective function, including a cross-entropy loss, a consistency loss and an embedding loss. Extensive experimental results on popular image and text datasets have shown that the proposed method is able to improve the performance of existing graph embedding and node classification methods, and outperform many state-of-the-art approaches on both types of datasets.},
  archive      = {J_NN},
  author       = {Enmei Tu and Zihao Wang and Jie Yang and Nikola Kasabov},
  doi          = {10.1016/j.neunet.2021.11.026},
  journal      = {Neural Networks},
  pages        = {350-360},
  shortjournal = {Neural Netw.},
  title        = {Deep semi-supervised learning via dynamic anchor graph embedding in latent space},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed/preassigned-time synchronization of quaternion-valued
neural networks via pure power-law control. <em>NN</em>, <em>146</em>,
341–349. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fixed-time synchronization and preassigned-time synchronization of quaternion-valued neural networks are concerned in this article. By developing fixed-time stability and proposing a pure power-law control scheme, some simple conditions are obtained to realize fixed-time synchronization of quaternion-valued neural networks and the upper bound of the synchronized time is provided. Furthermore, the preassigned-time synchronization of quaternion-valued neural networks is investigated based on pure power-law control design, where the synchronization time is preassigned in advance and the control gains are finite. Note that the designed controllers in this paper are the pure power-law forms, which are simpler and more effective compared with the traditional design composed of the linear part and power-law part. Eventually, an example is given to illustrate the feasibility and validity of the results obtained.},
  archive      = {J_NN},
  author       = {Wanlu Wei and Juan Yu and Leimin Wang and Cheng Hu and Haijun Jiang},
  doi          = {10.1016/j.neunet.2021.11.023},
  journal      = {Neural Networks},
  pages        = {341-349},
  shortjournal = {Neural Netw.},
  title        = {Fixed/Preassigned-time synchronization of quaternion-valued neural networks via pure power-law control},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive accuracy of CNN for cortical oscillatory activity
in an acute rat model of parkinsonism. <em>NN</em>, <em>146</em>,
334–340. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In neurological and neuropsychiatric disorders neuronal oscillatory activity between basal ganglia and cortical circuits are altered, which may be useful as biomarker for adaptive deep brain stimulation . We investigated whether changes in the spectral power of oscillatory activity in the motor cortex (MCtx) and the sensorimotor cortex (SMCtx) of rats after injection of the dopamine (DA) receptor antagonist haloperidol (HALO) would be similar to those observed in Parkinson disease . Thereafter, we tested whether a convolutional neural network (CNN) model would identify brain signal alterations in this acute model of parkinsonism . A sixteen channel surface micro-electrocorticogram (ECoG) recording array was placed under the dura above the MCtx and SMCtx areas of one hemisphere under general anaesthesia in rats. Seven days after surgery, micro ECoG was recorded in individual free moving rats in three conditions: (1) basal activity, (2) after injection of HALO (0.5 mg/kg), and (3) with additional injection of apomorphine (APO) (1 mg/kg). Furthermore, a CNN-based classification consisting of 23,530 parameters was applied on the raw data. HALO injection decreased oscillatory theta band activity (4–8 Hz) and enhanced beta (12–30 Hz) and gamma (30–100 Hz) in MCtx and SMCtx, which was compensated after APO injection (P ¡ 0.001). Evaluation of classification performance of the CNN model provided accuracy of 92\%, sensitivity of 90\% and specificity of 93\% on one-dimensional signals. The CNN proposed model requires a minimum of sensory hardware and may be integrated into future research on therapeutic devices for Parkinson disease , such as adaptive closed loop stimulation, thus contributing to more efficient way of treatment.},
  archive      = {J_NN},
  author       = {Ali Abdul Nabi Ali and Mesbah Alam and Simon C. Klein and Nicolai Behmann and Joachim K. Krauss and Theodor Doll and Holger Blume and Kerstin Schwabe},
  doi          = {10.1016/j.neunet.2021.11.025},
  journal      = {Neural Networks},
  pages        = {334-340},
  shortjournal = {Neural Netw.},
  title        = {Predictive accuracy of CNN for cortical oscillatory activity in an acute rat model of parkinsonism},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational epidemiology study of homeostatic compensation
during sensorimotor aging. <em>NN</em>, <em>146</em>, 316–333. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vestibulo-ocular reflex (VOR) stabilizes vision during head motion. Age-related changes of vestibular neuroanatomical properties predict a linear decay of VOR function. Nonetheless, human epidemiological data show a stable VOR function across the life span. In this study, we model cerebellum-dependent VOR adaptation to relate structural and functional changes throughout aging. We consider three neurosynaptic factors that may codetermine VOR adaptation during aging: the electrical coupling of inferior olive neurons, the long-term spike timing-dependent plasticity at parallel fiber – Purkinje cell synapses and mossy fiber – medial vestibular nuclei synapses, and the intrinsic plasticity of Purkinje cell synapses Our cross-sectional aging analyses suggest that long-term plasticity acts as a global homeostatic mechanism that underpins the stable temporal profile of VOR function. The results also suggest that the intrinsic plasticity of Purkinje cell synapses operates as a local homeostatic mechanism that further sustains the VOR at older ages. Importantly, the computational epidemiology approach presented in this study allows discrepancies among human cross-sectional studies to be understood in terms of interindividual variability in older individuals. Finally, our longitudinal aging simulations show that the amount of residual fibers coding for the peak and trough of the VOR cycle constitutes a predictive hallmark of VOR trajectories over a lifetime.},
  archive      = {J_NN},
  author       = {Niceto R. Luque and Francisco Naveros and Denis Sheynikhovich and Eduardo Ros and Angelo Arleo},
  doi          = {10.1016/j.neunet.2021.11.024},
  journal      = {Neural Networks},
  pages        = {316-333},
  shortjournal = {Neural Netw.},
  title        = {Computational epidemiology study of homeostatic compensation during sensorimotor aging},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep two-way matrix reordering for relational data analysis.
<em>NN</em>, <em>146</em>, 303–315. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix reordering is a task to permute the rows and columns of a given observed matrix such that the resulting reordered matrix shows meaningful or interpretable structural patterns. Most existing matrix reordering techniques share the common processes of extracting some feature representations from an observed matrix in a predefined manner, and applying matrix reordering based on it. However, in some practical cases, we do not always have prior knowledge about the structural pattern of an observed matrix. To address this problem, we propose a new matrix reordering method, called deep two-way matrix reordering (DeepTMR), using a neural network model. The trained network can automatically extract nonlinear row/column features from an observed matrix, which can then be used for matrix reordering. Moreover, the proposed DeepTMR provides the denoised mean matrix of a given observed matrix as an output of the trained network. This denoised mean matrix can be used to visualize the global structure of the reordered observed matrix. We demonstrate the effectiveness of the proposed DeepTMR by applying it to both synthetic and practical datasets.},
  archive      = {J_NN},
  author       = {Chihiro Watanabe and Taiji Suzuki},
  doi          = {10.1016/j.neunet.2021.11.028},
  journal      = {Neural Networks},
  pages        = {303-315},
  shortjournal = {Neural Netw.},
  title        = {Deep two-way matrix reordering for relational data analysis},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient correntropy-based multi-view clustering with
anchor graph embedding. <em>NN</em>, <em>146</em>, 290–302. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multi-view clustering has received widespread attention due to its far superior performance to single-view clustering, it still faces the following issues: (1) high computational cost, considering the introduction of multi-view information, reduces the clustering efficiency greatly; (2) complex noises and outliers, existed in real-world data, pose a huge challenge to the robustness of clustering algorithms . Currently, how to increase the efficiency and robustness has become two important issues of multi-view clustering. To cope with the above issues, an efficient correntropy-based multi-view clustering algorithm (ECMC) is proposed in this paper, which can not only improve clustering efficiency by constructing embedded anchor graph and utilizing nonnegative matrix factorization (NMF), but also enhance the robustness by exploring correntropy to suppress various noises and outliers. To further improve clustering efficiency, one of the factors of NMF is constrained to be an indicator matrix instead of a traditional non-negative matrix, so that the categories of samples can be obtained directly without any extra operation. Subsequently, a novel half-quadratic-based strategy is proposed to optimize the non-convex objective function of ECMC. Finally, extensive experiments on eight real-world datasets and eighteen noisy datasets show that ECMC can guarantee faster speed and better robustness than other state-of-the-art multi-view clustering algorithms .},
  archive      = {J_NN},
  author       = {Ben Yang and Xuetao Zhang and Badong Chen and Feiping Nie and Zhiping Lin and Zhixiong Nan},
  doi          = {10.1016/j.neunet.2021.11.027},
  journal      = {Neural Networks},
  pages        = {290-302},
  shortjournal = {Neural Netw.},
  title        = {Efficient correntropy-based multi-view clustering with anchor graph embedding},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transformers for modeling physical systems. <em>NN</em>,
<em>146</em>, 272–289. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature. 1},
  archive      = {J_NN},
  author       = {Nicholas Geneva and Nicholas Zabaras},
  doi          = {10.1016/j.neunet.2021.11.022},
  journal      = {Neural Networks},
  pages        = {272-289},
  shortjournal = {Neural Netw.},
  title        = {Transformers for modeling physical systems},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using top-down modulation to optimally balance shared versus
separated task representations. <em>NN</em>, <em>146</em>, 256–271. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human adaptive behavior requires continually learning and performing a wide variety of tasks, often with very little practice. To accomplish this, it is crucial to separate neural representations of different tasks in order to avoid interference. At the same time, sharing neural representations supports generalization and allows faster learning. Therefore, a crucial challenge is to find an optimal balance between shared versus separated representations. Typically, models of human cognition employ top-down modulatory signals to separate task representations, but there exist surprisingly little systematic computational investigations of how such modulation is best implemented. We identify and systematically evaluate two crucial features of modulatory signals. First, top-down input can be processed in an additive or multiplicative manner. Second, the modulatory signals can be adaptive (learned) or non-adaptive (random). We cross these two features, resulting in four modulation networks which are tested on a variety of input datasets and tasks with different degrees of stimulus-action mapping overlap. The multiplicative adaptive modulation network outperforms all other networks in terms of accuracy. Moreover, this network develops hidden units that optimally share representations between tasks. Specifically, different than the binary approach of currently popular latent state models, it exploits partial overlap between tasks.},
  archive      = {J_NN},
  author       = {Pieter Verbeke and Tom Verguts},
  doi          = {10.1016/j.neunet.2021.11.030},
  journal      = {Neural Networks},
  pages        = {256-271},
  shortjournal = {Neural Netw.},
  title        = {Using top-down modulation to optimally balance shared versus separated task representations},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging hierarchy in multimodal generative models for
effective cross-modality inference. <em>NN</em>, <em>146</em>, 238–255.
(<a href="https://doi.org/10.1016/j.neunet.2021.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of cross-modality inference (CMI), i.e., inferring missing data of unavailable perceptual modalities (e.g., sound) using data from available perceptual modalities (e.g., image). We overview single-modality variational autoencoder methods and discuss three problems of computational cross-modality inference, arising from recent developments in multimodal generative models . Inspired by neural mechanisms of human recognition, we contribute the Nexus model, a novel hierarchical generative model that can learn a multimodal representation of an arbitrary number of modalities in an unsupervised way. By exploiting hierarchical representation levels, Nexus is able to generate high-quality, coherent data of missing modalities given any subset of available modalities. To evaluate CMI in a natural scenario with a high number of modalities, we contribute the “Multimodal Handwritten Digit” (MHD) dataset, a novel benchmark dataset that combines image, motion, sound and label information from digit handwriting. We access the key role of hierarchy in enabling high-quality samples during cross-modality inference and discuss how a novel training scheme enables Nexus to learn a multimodal representation robust to missing modalities at test time. Our results show that Nexus outperforms current state-of-the-art multimodal generative models in regards to their cross-modality inference capabilities.},
  archive      = {J_NN},
  author       = {Miguel Vasco and Hang Yin and Francisco S. Melo and Ana Paiva},
  doi          = {10.1016/j.neunet.2021.11.019},
  journal      = {Neural Networks},
  pages        = {238-255},
  shortjournal = {Neural Netw.},
  title        = {Leveraging hierarchy in multimodal generative models for effective cross-modality inference},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LOss-based SensiTivity rEgulaRization: Towards deep sparse
neural networks. <em>NN</em>, <em>146</em>, 230–237. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LOBSTER (LOss-Based SensiTivity rEgulaRization) is a method for training neural networks having a sparse topology. Let the sensitivity of a network parameter be the variation of the loss function with respect to the variation of the parameter. Parameters with low sensitivity, i.e. having little impact on the loss when perturbed, are shrunk and then pruned to sparsify the network. Our method allows to train a network from scratch, i.e. without preliminary learning or rewinding. Experiments on multiple architectures and datasets show competitive compression ratios with minimal computational overhead.},
  archive      = {J_NN},
  author       = {Enzo Tartaglione and Andrea Bragagnolo and Attilio Fiandrotti and Marco Grangetto},
  doi          = {10.1016/j.neunet.2021.11.029},
  journal      = {Neural Networks},
  pages        = {230-237},
  shortjournal = {Neural Netw.},
  title        = {LOss-based SensiTivity rEgulaRization: Towards deep sparse neural networks},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feedforward neural networks initialization based on
discriminant learning. <em>NN</em>, <em>146</em>, 220–229. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel data-driven method for weight initialization of Multilayer Perceptrons and Convolutional Neural Networks based on discriminant learning is proposed. The approach relaxes some of the limitations of competing data-driven methods, including unimodality assumptions, limitations on the architectures related to limited maximal dimensionalities of the corresponding projection spaces, as well as limitations related to high computational requirements due to the need of eigendecomposition on high-dimensional data. We also consider assumptions of the method on the data and propose a way to account for them in a form of a new normalization layer. The experiments on three large-scale image datasets show improved accuracy of the trained models compared to competing random-based and data-driven weight initialization methods , as well as better convergence properties in certain cases.},
  archive      = {J_NN},
  author       = {Kateryna Chumachenko and Alexandros Iosifidis and Moncef Gabbouj},
  doi          = {10.1016/j.neunet.2021.11.020},
  journal      = {Neural Networks},
  pages        = {220-229},
  shortjournal = {Neural Netw.},
  title        = {Feedforward neural networks initialization based on discriminant learning},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuroLISP: High-level symbolic programming with attractor
neural networks. <em>NN</em>, <em>146</em>, 200–219. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant improvements in contemporary machine learning , symbolic methods currently outperform artificial neural networks on tasks that involve compositional reasoning, such as goal-directed planning and logical inference. This illustrates a computational explanatory gap between cognitive and neurocomputational algorithms that obscures the neurobiological mechanisms underlying cognition and impedes progress toward human-level artificial intelligence. Because of the strong relationship between cognition and working memory control, we suggest that the cognitive abilities of contemporary neural networks are limited by biologically-implausible working memory systems that rely on persistent activity maintenance and/or temporal nonlocality. Here we present NeuroLISP , an attractor neural network that can represent and execute programs written in the LISP programming language . Unlike previous approaches to high-level programming with neural networks , NeuroLISP features a temporally-local working memory based on itinerant attractor dynamics, top-down gating, and fast associative learning , and implements several high-level programming constructs such as compositional data structures , scoped variable binding, and the ability to manipulate and execute programmatic expressions in working memory (i.e., programs can be treated as data). Our computational experiments demonstrate the correctness of the NeuroLISP interpreter, and show that it can learn non-trivial programs that manipulate complex derived data structures (multiway trees), perform compositional string manipulation operations (PCFG SET task), and implement high-level symbolic AI algorithms (first-order unification). We conclude that NeuroLISP is an effective neurocognitive controller that can replace the symbolic components of hybrid models, and serves as a proof of concept for further development of high-level symbolic programming in neural networks.},
  archive      = {J_NN},
  author       = {Gregory P. Davis and Garrett E. Katz and Rodolphe J. Gentili and James A. Reggia},
  doi          = {10.1016/j.neunet.2021.11.009},
  journal      = {Neural Networks},
  pages        = {200-219},
  shortjournal = {Neural Netw.},
  title        = {NeuroLISP: High-level symbolic programming with attractor neural networks},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network enabled corrective source term approach
to hybrid analysis and modeling. <em>NN</em>, <em>146</em>, 181–199. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce, justify and demonstrate the Corrective Source Term Approach (CoSTA)—a novel approach to Hybrid Analysis and Modeling (HAM). The objective of HAM is to combine physics-based modeling (PBM) and data-driven modeling (DDM) to create generalizable, trustworthy, accurate, computationally efficient and self-evolving models. CoSTA achieves this objective by augmenting the governing equation of a PBM model with a corrective source term generated using a deep neural network . In a series of numerical experiments on one-dimensional heat diffusion , CoSTA is found to outperform comparable DDM and PBM models in terms of accuracy – often reducing predictive errors by several orders of magnitude – while also generalizing better than pure DDM . Due to its flexible but solid theoretical foundation, CoSTA provides a modular framework for leveraging novel developments within both PBM and DDM. Its theoretical foundation also ensures that CoSTA can be used to model any system governed by (deterministic) partial differential equations . Moreover, CoSTA facilitates interpretation of the DNN-generated source term within the context of PBM, which results in improved explainability of the DNN. These factors make CoSTA a potential door-opener for data-driven techniques to enter high-stakes applications previously reserved for pure PBM.},
  archive      = {J_NN},
  author       = {Sindre Stenen Blakseth and Adil Rasheed and Trond Kvamsdal and Omer San},
  doi          = {10.1016/j.neunet.2021.11.021},
  journal      = {Neural Networks},
  pages        = {181-199},
  shortjournal = {Neural Netw.},
  title        = {Deep neural network enabled corrective source term approach to hybrid analysis and modeling},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple-view flexible semi-supervised classification
through consistent graph construction and label propagation.
<em>NN</em>, <em>146</em>, 174–180. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph construction plays an essential role in graph-based label propagation since graphs give some information on the structure of the data manifold. While most graph construction methods rely on predefined distance calculation, recent algorithms merge the task of label propagation and graph construction in a single process. Moreover, the use of several descriptors is proved to outperform a single descriptor in representing the relation between the nodes. In this article, we propose a Multiple-View Consistent Graph construction and Label propagation algorithm (MVCGL) that simultaneously constructs a consistent graph based on several descriptors and performs label propagation over unlabeled samples . Furthermore, it provides a mapping function from the feature space to the label space with which we estimate the label of unseen samples via a linear projection . The constructed graph does not rely on a predefined similarity function and exploits data and label smoothness. Experiments conducted on three face and one handwritten digit databases show that the proposed method can gain better performance compared to other graph construction and label propagation methods.},
  archive      = {J_NN},
  author       = {Najmeh Ziraki and Fadi Dornaika and Alireza Bosaghzadeh},
  doi          = {10.1016/j.neunet.2021.11.015},
  journal      = {Neural Networks},
  pages        = {174-180},
  shortjournal = {Neural Netw.},
  title        = {Multiple-view flexible semi-supervised classification through consistent graph construction and label propagation},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A second-order accelerated neurodynamic approach for
distributed convex optimization. <em>NN</em>, <em>146</em>, 161–173. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the theories of inertial systems, a second-order accelerated neurodynamic approach is designed to solve a distributed convex optimization with inequality and set constraints. Most of the existing approaches for distributed convex optimization problems are usually first-order ones, and it is usually hard to analyze the convergence rate for the state solution of those first-order approaches. Due to the control design for the acceleration, the second-order neurodynamic approaches can often achieve faster convergence rate. Moreover, the existing second-order approaches are mostly designed to solve unconstrained distributed convex optimization problems, and are not suitable for solving constrained distributed convex optimization problems . It is acquired that the state solution of the designed neurodynamic approach in this paper converges to the optimal solution of the considered distributed convex optimization problem. An error function which demonstrates the performance of the designed neurodynamic approach, has a superquadratic convergence. Several numerical examples are provided to show the effectiveness of the presented second-order accelerated neurodynamic approach.},
  archive      = {J_NN},
  author       = {Xinrui Jiang and Sitian Qin and Xiaoping Xue and Xinzhi Liu},
  doi          = {10.1016/j.neunet.2021.11.013},
  journal      = {Neural Networks},
  pages        = {161-173},
  shortjournal = {Neural Netw.},
  title        = {A second-order accelerated neurodynamic approach for distributed convex optimization},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding and mitigating noise in trained deep neural
networks. <em>NN</em>, <em>146</em>, 151–160. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks unlocked a vast range of new applications by solving tasks of which many were previously deemed as reserved to higher human intelligence. One of the developments enabling this success was a boost in computing power provided by special purpose hardware, such as graphic or tensor processing units. However, these do not leverage fundamental features of neural networks like parallelism and analog state variables. Instead, they emulate neural networks relying on binary computing, which results in unsustainable energy consumption and comparatively low speed. Fully parallel and analogue hardware promises to overcome these challenges, yet the impact of analogue neuron noise and its propagation, i.e. accumulation, threatens rendering such approaches inept. Here, we determine for the first time the propagation of noise in deep neural networks comprising noisy nonlinear neurons in trained fully connected layers. We study additive and multiplicative as well as correlated and uncorrelated noise, and develop analytical methods that predict the noise level in any layer of symmetric deep neural networks or deep neural networks trained with back propagation . We find that noise accumulation is generally bound, and adding additional network layers does not worsen the signal to noise ratio beyond a limit. Most importantly, noise accumulation can be suppressed entirely when neuron activation functions have a slope smaller than unity. We therefore developed the framework for noise in fully connected deep neural networks implemented in analog systems, and identify criteria allowing engineers to design noise-resilient novel neural network hardware.},
  archive      = {J_NN},
  author       = {Nadezhda Semenova and Laurent Larger and Daniel Brunner},
  doi          = {10.1016/j.neunet.2021.11.008},
  journal      = {Neural Networks},
  pages        = {151-160},
  shortjournal = {Neural Netw.},
  title        = {Understanding and mitigating noise in trained deep neural networks},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stability and dissipativity criteria for neural networks
with time-varying delays via an augmented zero equality approach.
<em>NN</em>, <em>146</em>, 141–150. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the stability and dissipativity problems for neural networks with time-varying delay. By the construction of new augmented Lyapunov–Krasovskii functionals based on integral inequality and the use of zero equality approach, three improved results are proposed in the forms of linear matrix inequalities . And, based on the stability results, the dissipativity analysis for NNs with time-varying delays was investigated. Through some numerical examples, the superiority and effectiveness of the proposed results are shown by comparing the existing works.},
  archive      = {J_NN},
  author       = {S.H. Lee and M.J. Park and D.H. Ji and O.M. Kwon},
  doi          = {10.1016/j.neunet.2021.11.007},
  journal      = {Neural Networks},
  pages        = {141-150},
  shortjournal = {Neural Netw.},
  title        = {Stability and dissipativity criteria for neural networks with time-varying delays via an augmented zero equality approach},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybridization of distributed policy and heuristic
augmentation for improving federated learning approach. <em>NN</em>,
<em>146</em>, 130–140. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modifying the existing models of classifiers’ operation is primarily aimed at increasing the effectiveness as well as minimizing the training time. An additional advantage is the ability to quickly implement a given solution to the real needs of the market. In this paper, we propose a method that can implement various classifiers using the federated learning concept and taking into account parallelism . Also, an important element is the analysis and selection of the best classifier depending on its reliability found for separated datasets extended by new, augmented samples. The proposed augmentation technique involves image processing techniques, neural architectures, and heuristic methods and improves the operation in federated learning by increasing the role of the server. The proposition has been presented and tested for the fruit image classification problem. The conducted experiments have shown that the described technique can be very useful as an implementation method even in the case of a small database. Obtained results are discussed concerning the advantages and disadvantages in the context of practical application like higher accuracy.},
  archive      = {J_NN},
  author       = {Dawid Połap and Marcin Woźniak},
  doi          = {10.1016/j.neunet.2021.11.018},
  journal      = {Neural Networks},
  pages        = {130-140},
  shortjournal = {Neural Netw.},
  title        = {A hybridization of distributed policy and heuristic augmentation for improving federated learning approach},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Event-centric multi-modal fusion method for dense video
captioning. <em>NN</em>, <em>146</em>, 120–129. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense video captioning aims to automatically describe several events that occur in a given video, which most state-of-the-art models accomplish by locating and describing multiple events in an untrimmed video. Despite much progress in this area, most current approaches only encode visual features in the event location phase and they neglect the relationships between events, which may degrade the consistency of the description in the identical video. Thus, in the present study, we attempted to exploit visual–audio cues to generate event proposals and enhance event-level representations by capturing their temporal and semantic relationships . Furthermore, to compensate for the major limitation of not fully utilizing multimodal information in the description process, we developed an attention-gating mechanism that dynamically fuses and regulates the multi-modal information. In summary, we propose an event-centric multi-modal fusion approach for dense video captioning (EMVC) to capture the relationships between events and effectively fuse multi-modal information. We conducted comprehensive experiments to evaluate the performance of EMVC based on the benchmark ActivityNet Caption and YouCook2 data sets. The experimental results showed that our model achieved impressive performance compared with state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Zhi Chang and Dexin Zhao and Huilin Chen and Jingdan Li and Pengfei Liu},
  doi          = {10.1016/j.neunet.2021.11.017},
  journal      = {Neural Networks},
  pages        = {120-129},
  shortjournal = {Neural Netw.},
  title        = {Event-centric multi-modal fusion method for dense video captioning},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dilated projection correction network based on autoencoder
for hyperspectral image super-resolution. <em>NN</em>, <em>146</em>,
107–119. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on improving the spatial resolution of the hyperspectral image (HSI) by taking the prior information into consideration. In recent years, single HSI super-resolution methods based on deep learning have achieved good performance. However, most of them only simply apply general image super-resolution deep networks to hyperspectral data , thus ignoring some specific characteristics of hyperspectral data itself. In order to make full use of spectral information of the HSI, we transform the HSI SR problem from the image domain into the abundance domain by the dilated projection correction network with an autoencoder , termed as aeDPCN . In particular, we first encode the low-resolution HSI to abundance representation and preserve the spectral information in the decoder network, which could largely reduce the computational complexity . Then, to enhance the spatial resolution of the abundance embedding, we super-resolve the embedding in a coarse-to-fine manner by the dilated projection correction network where the back-projection strategy is introduced to further eliminate spectral distortion . Finally, the predictive images are derived by the same decoder, which increases the stability of our method, even at a large upscaling factor. Extensive experiments on real hyperspectral image scenes demonstrate the superiority of our method over the state-of-the-art, in terms of accuracy and efficiency.},
  archive      = {J_NN},
  author       = {Xinya Wang and Jiayi Ma and Junjun Jiang and Xiao-Ping Zhang},
  doi          = {10.1016/j.neunet.2021.11.014},
  journal      = {Neural Networks},
  pages        = {107-119},
  shortjournal = {Neural Netw.},
  title        = {Dilated projection correction network based on autoencoder for hyperspectral image super-resolution},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inertial neural network approach for robust
time-of-arrival localization considering clock asynchronization.
<em>NN</em>, <em>146</em>, 98–106. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an inertial neural network to solve the source localization optimization problem with l 1 l1 -norm objective function based on the time of arrival (TOA) localization technique. The convergence and stability of the inertial neural network are analyzed by the Lyapunov function method. An inertial neural network iterative approach is further used to find a better solution among the solutions with different inertial parameters . Furthermore, the clock asynchronization is considered in the TOA l 1 l1 -norm model for more general real applications, and the corresponding inertial neural network iterative approach is addressed. The numerical simulations and real data are both considered in the experiments. In the simulation experiments, the noise contains uncorrelated zero-mean Gaussian noise and uniform distributed outliers. In the real experiments, the data is obtained by using the ultra wide band (UWB) technology hardware modules. Whether or not there is clock asynchronization, the results show that the proposed approach always can find a more accurate source position compared with some of the existing algorithms, which implies that the proposed approach is more effective than the compared ones.},
  archive      = {J_NN},
  author       = {Chentao Xu and Qingshan Liu},
  doi          = {10.1016/j.neunet.2021.11.012},
  journal      = {Neural Networks},
  pages        = {98-106},
  shortjournal = {Neural Netw.},
  title        = {An inertial neural network approach for robust time-of-arrival localization considering clock asynchronization},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Functional connectivity inference from fMRI data using
multivariate information measures. <em>NN</em>, <em>146</em>, 85–97. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shannon’s entropy or an extension of Shannon’s entropy can be used to quantify information transmission between or among variables. Mutual information is the pair-wise information that captures nonlinear relationships between variables. It is more robust than linear correlation methods. Beyond mutual information, two generalizations are defined for multivariate distributions : interaction information or co-information and total correlation or multi-mutual information. In comparison to mutual information, interaction information and total correlation are underutilized and poorly studied in applied neuroscience research. Quantifying information flow between brain regions is not explicitly explained in neuroscience by interaction information and total correlation. This article aims to clarify the distinctions between the neuroscience concepts of mutual information, interaction information, and total correlation. Additionally, we proposed a novel method for determining the interaction information between three variables using total correlation and conditional mutual information. On the other hand, how to apply it properly in practical situations. We supplied both simulation experiments and real neural studies to estimate functional connectivity in the brain with the above three higher-order information-theoretic approaches. In order to capture redundancy information for multivariate variables, we discovered that interaction information and total correlation were both robust, and it could be able to capture both well-known and yet-to-be-discovered functional brain connections.},
  archive      = {J_NN},
  author       = {Qiang Li},
  doi          = {10.1016/j.neunet.2021.11.016},
  journal      = {Neural Networks},
  pages        = {85-97},
  shortjournal = {Neural Netw.},
  title        = {Functional connectivity inference from fMRI data using multivariate information measures},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view teacher–student network. <em>NN</em>,
<em>146</em>, 69–84. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning aims to fully exploit the view-consistency and view-discrepancy for performance improvement. Knowledge Distillation (KD), characterized by the so-called “Teacher–Student” (T-S) learning framework, can transfer information learned from one model to another. Inspired by knowledge distillation, we propose a Multi-view Teacher–Student Network (MTS-Net), which combines knowledge distillation and multi-view learning into a unified framework. We first redefine the teacher and student for the multi-view case. Then the MTS-Net is built by optimizing both the view classification loss and the knowledge distillation loss in an end-to-end training manner. We further extend MTS-Net to image recognition tasks and present a multi-view Teacher–Student framework with convolutional neural networks called MTSCNN. To the best of our knowledge, MTS-Net and MTSCNN bring a new insight to extend the Teacher–Student framework to tackle the multi-view learning problem. We theoretically verify the mechanism of MTS-Net and MTSCNN and comprehensive experiments demonstrate the effectiveness of the proposed methods.},
  archive      = {J_NN},
  author       = {Yingjie Tian and Shiding Sun and Jingjing Tang},
  doi          = {10.1016/j.neunet.2021.11.002},
  journal      = {Neural Networks},
  pages        = {69-84},
  shortjournal = {Neural Netw.},
  title        = {Multi-view Teacher–Student network},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ARCNN framework for multimodal infodemic detection.
<em>NN</em>, <em>146</em>, 36–68. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news and misinformation have adopted various propagation media over time, nowadays spreading predominantly through online social networks . During the ongoing COVID-19 pandemic, false information is affecting human life in many spheres The world needs automated detection technology and efforts are being made to meet this requirement with the use of artificial intelligence . Neural network detection mechanisms are robust and durable and hence are used extensively in fake news detection. Deep learning algorithms demonstrate efficiency when they are provided with a large amount of training data. Given the scarcity of relevant fake news datasets, we built the Coronavirus Infodemic Dataset (CovID), which contains fake news posts and articles related to coronavirus. This paper presents a novel framework, the Allied Recurrent and Convolutional Neural Network (ARCNN), to detect fake news based on two different modalities: text and image. Our approach uses recurrent neural networks (RNNs) and convolutional neural networks (CNNs) and combines both streams to generate a final prediction. We present extensive research on various popular RNN and CNN models and their performance on six coronavirus-specific fake news datasets. To exhaustively analyze performance, we present experimentation performed and results obtained by combining both modalities using early fusion and four types of late fusion techniques. The proposed framework is validated by comparisons with state-of-the-art fake news detection mechanisms, and our models outperform each of them.},
  archive      = {J_NN},
  author       = {Chahat Raj and Priyanka Meel},
  doi          = {10.1016/j.neunet.2021.11.006},
  journal      = {Neural Networks},
  pages        = {36-68},
  shortjournal = {Neural Netw.},
  title        = {ARCNN framework for multimodal infodemic detection},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imitation and mirror systems in robots through deep modality
blending networks. <em>NN</em>, <em>146</em>, 22–35. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to interact with the environment not only empowers the agent with manipulation capability but also generates information to facilitate building of action understanding and imitation capabilities. This seems to be a strategy adopted by biological systems, in particular primates, as evidenced by the existence of mirror neurons that seem to be involved in multi-modal action understanding. How to benefit from the interaction experience of the robots to enable understanding actions and goals of other agents is still a challenging question. In this study, we propose a novel method, deep modality blending networks (DMBN), that creates a common latent space from multi-modal experience of a robot by blending multi-modal signals with a stochastic weighting mechanism. We show for the first time that deep learning , when combined with a novel modality blending scheme, can facilitate action recognition and produce structures to sustain anatomical and effect-based imitation capabilities. Our proposed system, which is based on conditional neural processes , can be conditioned on any desired sensory/motor value at any time step, and can generate a complete multi-modal trajectory consistent with the desired conditioning in one-shot by querying the network for all the sampled time points in parallel avoiding the accumulation of prediction errors. Based on simulation experiments with an arm-gripper robot and an RGB camera, we showed that DMBN could make accurate predictions about any missing modality (camera or joint angles) given the available ones outperforming recent multimodal variational autoencoder models in terms of long-horizon high-dimensional trajectory predictions. We further showed that given desired images from different perspectives, i.e. images generated by the observation of other robots placed on different sides of the table, our system could generate image and joint angle sequences that correspond to either anatomical or effect-based imitation behavior . To achieve this mirror-like behavior, our system does not perform a pixel-based template matching but rather benefits from and relies on the common latent space constructed by using both joint and image modalities, as shown by additional experiments. Moreover, we showed that mirror learning (in our system) does not only depend on visual experience and cannot be achieved without proprioceptive experience. Our experiments showed that out of ten training scenarios with different initial configurations , the proposed DMBN model could achieve mirror learning in all of the cases where the model that only uses visual information failed in half of them. Overall, the proposed DMBN architecture not only serves as a computational model for sustaining mirror neuron-like capabilities, but also stands as a powerful machine learning architecture for high-dimensional multi-modal temporal data with robust retrieval capabilities operating with partial information in one or multiple modalities.},
  archive      = {J_NN},
  author       = {M. Yunus Seker and Alper Ahmetoglu and Yukie Nagai and Minoru Asada and Erhan Oztop and Emre Ugur},
  doi          = {10.1016/j.neunet.2021.11.004},
  journal      = {Neural Networks},
  pages        = {22-35},
  shortjournal = {Neural Netw.},
  title        = {Imitation and mirror systems in robots through deep modality blending networks},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSITime: Privacy-preserving human activity recognition using
WiFi channel state information. <em>NN</em>, <em>146</em>, 11–21. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) is an important task in many applications such as smart homes, sports analysis, healthcare services , etc . Popular modalities for human activity recognition involving computer vision and inertial sensors are in the literature for solving HAR, however, they face serious limitations with respect to different illumination, background, clutter, obtrusiveness, and other factors. In recent years, WiFi channel state information (CSI) based activity recognition is gaining momentum due to its many advantages including easy deployability, and cost-effectiveness. This work proposes CSITime, a modified InceptionTime network architecture , a generic architecture for CSI-based human activity recognition. We perceive CSI activity recognition as a multi-variate time series problem. The methodology of CSITime is threefold. First, we pre-process CSI signals followed by data augmentation using two label-mixing strategies — mixup and cutmix to enhance the neural network’s learning. Second, in the basic block of CSITime, features from multiple convolutional kernels are concatenated and passed through a self-attention layer followed by a fully connected layer with Mish activation. CSITime network consists of six such blocks followed by a global average pooling layer and a final fully connected layer for the final classification. Third, in the training of the neural network , instead of adopting general training procedures such as early stopping, we use one-cycle policy and cosine annealing to monitor the learning rate . The proposed model has been tested on publicly available benchmark datasets, i . e . i.e. , ARIL, StanWiFi, and SignFi datasets. The proposed CSITime has achieved accuracy of 98.20\%, 98\%, and 95.42\% on ARIL, StanWiFi, and SignFi datasets, respectively, for WiFi-based activity recognition. This is an improvement on state-of-the-art accuracies by 3.3\%, 0.67\%, and 0.82\% on ARIL, StanWiFi, and SignFi datasets, respectively. In lab-5 users’ scenario of the SignFi dataset, which has the training and testing data from different distributions, our model achieved accuracy was 2.17\% higher than state-of-the-art, which shows the comparative robustness of our model.},
  archive      = {J_NN},
  author       = {Santosh Kumar Yadav and Siva Sai and Akshay Gundewar and Heena Rathore and Kamlesh Tiwari and Hari Mohan Pandey and Mohit Mathur},
  doi          = {10.1016/j.neunet.2021.11.011},
  journal      = {Neural Networks},
  pages        = {11-21},
  shortjournal = {Neural Netw.},
  title        = {CSITime: Privacy-preserving human activity recognition using WiFi channel state information},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-layer information fusion based on graph convolutional
network for knowledge-driven herb recommendation. <em>NN</em>,
<em>146</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prescription of Traditional Chinese Medicine (TCM) is a precious treasure accumulated in the long-term development of TCM. Artificial intelligence (AI) technology is used to build herb recommendation models to deeply understand regularities in prescriptions, which is of great significance to clinical application of TCM and discovery of new prescriptions. Most of herb recommendation models constructed in the past ignored the nature information of herbs, and most of them used statistical models based on bag-of-words for herb recommendation, which makes it difficult for the model to perceive the complex correlation between symptoms and herbs. In this paper, we introduce the properties of herbs as additional auxiliary information by constructing herb knowledge graph, and propose a graph convolution model with multi-layer information fusion to obtain symptom feature representations and herb feature representations with rich information and less noise. We apply the proposed model to the TCM prescription dataset, and the experiment results show that our model outperforms the baseline models in terms of Precision@5 by 6.2\%, Recall@5 by 16.0\% and F1-Score@5 by 12.0\%.},
  archive      = {J_NN},
  author       = {Yun Yang and Yulong Rao and Minghao Yu and Yan Kang},
  doi          = {10.1016/j.neunet.2021.11.010},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Multi-layer information fusion based on graph convolutional network for knowledge-driven herb recommendation},
  volume       = {146},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continual growth and a transition. <em>NN</em>,
<em>145</em>, xx–xxi. (<a
href="https://doi.org/10.1016/S0893-6080(21)00463-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Kenji Doya (departing) ( Co-Editors-in-Chief ) and Taro Toyoizumi (incoming) and DeLiang Wang},
  doi          = {10.1016/S0893-6080(21)00463-9},
  journal      = {Neural Networks},
  pages        = {xx-xxi},
  shortjournal = {Neural Netw.},
  title        = {Continual growth and a transition},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Announcement of the neural networks best paper award.
<em>NN</em>, <em>145</em>, xix. (<a
href="https://doi.org/10.1016/S0893-6080(21)00464-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Kenji Doya ( Co-Editors-in-Chief ) and DeLiang Wang},
  doi          = {10.1016/S0893-6080(21)00464-0},
  journal      = {Neural Networks},
  pages        = {xix},
  shortjournal = {Neural Netw.},
  title        = {Announcement of the neural networks best paper award},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural networks referees in 2021. <em>NN</em>, <em>145</em>,
xi–xviii. (<a
href="https://doi.org/10.1016/S0893-6080(21)00462-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00462-7},
  journal      = {Neural Networks},
  pages        = {xi-xviii},
  shortjournal = {Neural Netw.},
  title        = {Neural networks referees in 2021},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022l). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>145</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00459-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00459-7},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022i). Current events. <em>NN</em>, <em>145</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00458-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00458-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nostradamus: A novel event propagation prediction approach
with spatio-temporal characteristics in non-euclidean space.
<em>NN</em>, <em>145</em>, 386–394. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of event propagation has received extensive attention from the knowledge discovery community for applications such as virus spread analytics, social network analysis , earthquake location prediction, and typhoon tracking. The data describing these phenomena are multidimensional asynchronous event data that affect each other and show complex dynamic patterns in the continuous-time domain. Unlike the discrete characteristics formed by sampling at equal intervals of asynchronous time series, the timestamps of asynchronous events are in the continuous-time field. The study of these dynamic processes and the mining of their potential correlations provide a foundation for applying event propagation prediction, traceability, and causal inference at both the micro and macro levels. Most of the existing methods represent data as being embedded in the Euclidean space . However, when embedding a real-world graph with a tree-likeliness graph, Euclidean space cannot visually represent a graph. Inspired by the characteristics of hyperbolic space , we propose a model called Nostradamus to capture the propagation of the events of interest from historical events in a graph via the hyperbolic graph neural Hawkes process with Spatio-temporal characteristics. The Nostradamus’ core concept is to integrate the Hawkes process’s conditional intensity function with a hyperbolic graph convolutional recurrent neural network .},
  archive      = {J_NN},
  author       = {Haizhou Du and Yan Zhou},
  doi          = {10.1016/j.neunet.2021.11.005},
  journal      = {Neural Networks},
  pages        = {386-394},
  shortjournal = {Neural Netw.},
  title        = {Nostradamus: A novel event propagation prediction approach with spatio-temporal characteristics in non-euclidean space},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-based master–slave synchronization of complex-valued
neural networks via pinning impulsive control. <em>NN</em>,
<em>145</em>, 374–385. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the synchronization problem of complex-valued neural networks via event-triggered pinning impulsive control (ETPIC). A time-delayed pinning impulsive controller is proposed based on three levels of event-triggered conditions. By employing the Lyapunov functional method and differential inequality technique, sufficient delay-dependent synchronization criteria are derived under the proposed ETPIC scheme. The obtained result shows that synchronization of master and slave complex-valued neural networks can be achieved even if the sizes of delays exceed the length of intervals between any two consecutive impulsive instants determined by Lyapunov-based event-triggered conditions in the proposed control strategy. Moreover, the linear matrix inequality approach is utilized to exclude Zeno behavior . Numerical examples are provided to illustrate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Yuan Shen and Xinzhi Liu},
  doi          = {10.1016/j.neunet.2021.10.025},
  journal      = {Neural Networks},
  pages        = {374-385},
  shortjournal = {Neural Netw.},
  title        = {Event-based master–slave synchronization of complex-valued neural networks via pinning impulsive control},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level attention pooling for graph neural networks:
Unifying graph representations with multiple localities. <em>NN</em>,
<em>145</em>, 356–373. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes’ local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks , which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures . In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.},
  archive      = {J_NN},
  author       = {Takeshi D. Itoh and Takatomi Kubo and Kazushi Ikeda},
  doi          = {10.1016/j.neunet.2021.11.001},
  journal      = {Neural Networks},
  pages        = {356-373},
  shortjournal = {Neural Netw.},
  title        = {Multi-level attention pooling for graph neural networks: Unifying graph representations with multiple localities},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FOREX rate prediction improved by elliott waves patterns
based on neural networks. <em>NN</em>, <em>145</em>, 342–355. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial market predictions represent a complex problem. Most prediction systems work with the term time window , which is represented by exchange rate values of a real financial commodity. Such values (time window) provide the base for prediction of future values. Real situations, however, prove that prediction of only a single time-series trend is insufficient. This article aims at suggesting a novelty and unconventional approach based on the use of several neural networks predicting probable courses of a future trend defined in a prediction time window. The basis of the proposed approach is a suitable representation of the training-set input data into the neural networks . It uses selected FFT coefficients as well as robust output indicators based on a histogram of the predicted course of the selected currency pair. At the same time, the given currency pair enters the prediction in a combination with another three mutually interconnected currency pairs. A significant output of the articles is, apart from the proposed methodology, confirmation that the Elliott wave theory is beneficial in the trading environment and provides a substantial profit compared with conventional prediction techniques. That was proved in the performed experimental study.},
  archive      = {J_NN},
  author       = {Robert Jarusek and Eva Volna and Martin Kotyrba},
  doi          = {10.1016/j.neunet.2021.10.024},
  journal      = {Neural Networks},
  pages        = {342-355},
  shortjournal = {Neural Netw.},
  title        = {FOREX rate prediction improved by elliott waves patterns based on neural networks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cycle-consistent adversarial adaptation network and its
application to machine fault diagnosis. <em>NN</em>, <em>145</em>,
331–341. (<a
href="https://doi.org/10.1016/j.neunet.2021.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by industrial big data and intelligent manufacturing, deep learning approaches have flourished and yielded impressive achievements in the community of machine fault diagnosis. Nevertheless, current diagnosis models trained on a specific dataset seldom work well on other datasets due to the domain discrepancy. Recently, adversarial domain adaptation-based approaches have become an emerging and compelling tool to address this issue. Nonetheless, existing methods still have a shortcoming since they cannot guarantee sufficient feature similarity between the source domain and the target domain after adaptation, resulting in unguaranteed performance. To this end, a Cycle-consistent Adversarial Adaptation Network (CAAN) is advanced to realize more effective fault diagnosis of machinery. In CAAN, specifically, an adversarial game formed by the feature extractor and the domain discriminator is constructed to induce transferable feature learning . Meanwhile, the feature translators and discriminators between source and target domains are further designed to build a more comprehensive cycle-consistent generative adversarial constrain, which can more reliably ensure domain-invariant and class-separate characteristics of learned features. Extensive experiments constructed on three datasets from different mechanical systems demonstrate the effectiveness and superiority of CAAN.},
  archive      = {J_NN},
  author       = {Jinyang Jiao and Jing Lin and Ming Zhao and Kaixuan Liang and Chuancang Ding},
  doi          = {10.1016/j.neunet.2021.11.003},
  journal      = {Neural Networks},
  pages        = {331-341},
  shortjournal = {Neural Netw.},
  title        = {Cycle-consistent adversarial adaptation network and its application to machine fault diagnosis},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Fractional-order discontinuous systems with indefinite
LKFs: An application to fractional-order neural networks with time
delays. <em>NN</em>, <em>145</em>, 319–330. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss bipartite fixed-time synchronization for fractional-order signed neural networks with discontinuous activation patterns . The Filippov multi-map is used to convert the fixed-time stability of the fractional-order general solution into the zero solution of the fractional-order differential inclusions . On the Caputo fractional-order derivative, Lyapunov-Krasovskii functional is proved to possess the indefinite fractional derivatives for fixed-time stability of fragmentary discontinuous systems . Furthermore, the fixed-time stability of the fractional-order discontinuous system is achieved as well as an estimate of the new settling time.. The discontinuous controller is designed for the delayed fractional-order discontinuous signed neural networks with antagonistic interactions and new conditions for permanent fixed-time synchronization of these networks with antagonistic interactions are also provided, as well as the settling time for permanent fixed-time synchronization. Two numerical simulation results are presented to demonstrate the effectiveness of the main results},
  archive      = {J_NN},
  author       = {K. Udhayakumar and Fathalla A. Rihan and R. Rakkiyappan and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.10.027},
  journal      = {Neural Networks},
  pages        = {319-330},
  shortjournal = {Neural Netw.},
  title        = {Fractional-order discontinuous systems with indefinite LKFs: An application to fractional-order neural networks with time delays},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimum spanning tree based graph neural network for emotion
classification using EEG. <em>NN</em>, <em>145</em>, 308–318. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion classification based on neurophysiology signals has been a challenging issue in the literature. Recent neuroscience findings suggest that brain network structure underlying the different emotions provides a window in understanding human affection. In this paper, we propose a novel method to capture the distinct minimum spanning tree (MST) topology underpinning the different emotions. Specifically, we propose a hierarchical aggregation-based graph neural network to investigate the MST structure in emotion recognition. Extensive experiments on the public available DEAP dataset demonstrate the superior performance of the model in emotion classification as compared to existing methods. In addition, the results show that the theta, lower beta and gamma frequency band network information are more sensitive to emotions, suggesting a multi-frequency interaction in emotion processing.},
  archive      = {J_NN},
  author       = {Hanjie Liu and Jinren Zhang and Qingshan Liu and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.10.023},
  journal      = {Neural Networks},
  pages        = {308-318},
  shortjournal = {Neural Netw.},
  title        = {Minimum spanning tree based graph neural network for emotion classification using EEG},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence analysis of AdaBound with relaxed bound
functions for non-convex optimization. <em>NN</em>, <em>145</em>,
300–307. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clipping on learning rates in Adam leads to an effective stochastic algorithm—AdaBound. In spite of its effectiveness in practice, convergence analysis of AdaBound has not been fully explored, especially for non-convex optimization. To this end, we address the convergence of the last individual output of AdaBound for non-convex stochastic optimization problems , which is called individual convergence . We prove that, with the iteration of the AdaBound, the cost function converges to a finite value and the corresponding gradient converges to zero. The novelty of this proof is that the convergence conditions on the bound functions and momentum factors are much more relaxed than the existing results, especially when we remove the monotonicity and convergence of the bound functions, and only keep their boundedness . The momentum factors can be fixed to be constant, without the restriction of monotonically decreasing. This provides a new perspective on understanding the bound functions and momentum factors of AdaBound. At last, numerical experiments are provided to corroborate our theory and show that the convergence of AdaBound extends to more general bound functions.},
  archive      = {J_NN},
  author       = {Jinlan Liu and Jun Kong and Dongpo Xu and Miao Qi and Yinghua Lu},
  doi          = {10.1016/j.neunet.2021.10.026},
  journal      = {Neural Networks},
  pages        = {300-307},
  shortjournal = {Neural Netw.},
  title        = {Convergence analysis of AdaBound with relaxed bound functions for non-convex optimization},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure inference of networked system with the synergy of
deep residual network and fully connected layer network. <em>NN</em>,
<em>145</em>, 288–299. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The networked systems are booming in multi-disciplines, including the industrial engineering system , the social system, and so on. The network structure is a prerequisite for the understanding and exploration of networked systems. However, the network structure is always unknown in practice, thus, it is significant yet challenging to investigate the inference of network structure. Although some model-based methods and data-driven methods, such as the phase-space based method and the compressive sensing based method, have investigated the structure inference tasks, they were time-consuming due to the greedy iterative optimization procedure , which makes them difficult to satisfy real-time structure inference requirements. Although the reconstruction time of L1 and other methods is short, the reconstruction accuracy is very low. Inspired by the powerful representation ability and time efficiency for the structure inference with the deep learning framework, a novel synergy method combines the deep residual network and fully connected layer network to solve the network structure inference task efficiently and accurately. This method perfectly solves the problems of long reconstruction time and low accuracy of traditional methods. Moreover, the proposed method can also fulfill the inference task of large scale complex network, which further indicates the scalability of the proposed method.},
  archive      = {J_NN},
  author       = {Keke Huang and Shuo Li and Wenfeng Deng and Zhaofei Yu and Lei Ma},
  doi          = {10.1016/j.neunet.2021.10.016},
  journal      = {Neural Networks},
  pages        = {288-299},
  shortjournal = {Neural Netw.},
  title        = {Structure inference of networked system with the synergy of deep residual network and fully connected layer network},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning and its connections with neuroscience
and psychology. <em>NN</em>, <em>145</em>, 271–287. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning methods have recently been very successful at performing complex sequential tasks like playing Atari games, Go and Poker. These algorithms have outperformed humans in several tasks by learning from scratch, using only scalar rewards obtained through interaction with their environment. While there certainly has been considerable independent innovation to produce such results, many core ideas in reinforcement learning are inspired by phenomena in animal learning, psychology and neuroscience . In this paper, we comprehensively review a large number of findings in both neuroscience and psychology that evidence reinforcement learning as a promising candidate for modeling learning and decision making in the brain. In doing so, we construct a mapping between various classes of modern RL algorithms and specific findings in both neurophysiological and behavioral literature. We then discuss the implications of this observed relationship between RL, neuroscience and psychology and its role in advancing research in both AI and brain science.},
  archive      = {J_NN},
  author       = {Ajay Subramanian and Sharad Chitlangia and Veeky Baths},
  doi          = {10.1016/j.neunet.2021.10.003},
  journal      = {Neural Networks},
  pages        = {271-287},
  shortjournal = {Neural Netw.},
  title        = {Reinforcement learning and its connections with neuroscience and psychology},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Curriculum learning with hindsight experience replay for
sequential object manipulation tasks. <em>NN</em>, <em>145</em>,
260–270. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning complex tasks from scratch is challenging and often impossible for humans as well as for artificial agents. Instead, a curriculum can be used, which decomposes a complex task – the target task – into a sequence of source tasks. Each source task is a simplified version of the next source task with increasing complexity. Learning then occurs gradually by training on each source task while using knowledge from the curriculum’s prior source tasks. In this study, we present a new algorithm that combines curriculum learning with Hindsight Experience Replay (HER), to learn sequential object manipulation tasks for multiple goals and sparse feedback. The algorithm exploits the recurrent structure inherent in many object manipulation tasks and implements the entire learning process in the original simulation without adjusting it to each source task. We test our algorithm on three challenging throwing tasks in simulation and show significant improvements compared to vanilla-HER.},
  archive      = {J_NN},
  author       = {B. Manela and A. Biess},
  doi          = {10.1016/j.neunet.2021.10.011},
  journal      = {Neural Networks},
  pages        = {260-270},
  shortjournal = {Neural Netw.},
  title        = {Curriculum learning with hindsight experience replay for sequential object manipulation tasks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FCL-net: Towards accurate edge detection via fine-scale
corrective learning. <em>NN</em>, <em>145</em>, 248–259. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multi-scale predictions has become a mainstream paradigm in edge detection. However, most existing methods mainly focus on effective feature extraction and multi-scale feature fusion while ignoring the low learning capacity in fine-level branches, limiting the overall fusion performance. In light of this, we propose a novel Fine-scale Corrective Learning Net (FCL-Net) that exploits semantic information from deep layers to facilitate fine-scale feature learning . FCL-Net mainly consists of a Top-down Attentional Guiding (TAG) and a Pixel-level Weighting (PW) module. TAG module adopts semantic attentional cues from coarse-scale prediction into guiding the fine-scale branches by learning a top-down LSTM . PW module treats the contribution of each spatial location independently and promote fine-level branches to detect detailed edges with high confidence. Experiments on three benchmark datasets, i.e. , BSDS500, Multicue, and BIPED, show that our approach significantly outperforms the baseline and achieves a competitive ODS F-measure of 0.826 on the BSDS500 benchmark. The source code and models are publicly available at https://github.com/DREAMXFAR/FCL-Net .},
  archive      = {J_NN},
  author       = {Wenjie Xuan and Shaoli Huang and Juhua Liu and Bo Du},
  doi          = {10.1016/j.neunet.2021.10.022},
  journal      = {Neural Networks},
  pages        = {248-259},
  shortjournal = {Neural Netw.},
  title        = {FCL-net: Towards accurate edge detection via fine-scale corrective learning},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting dynamic spatio-temporal graph convolutional
neural networks for citywide traffic flows prediction. <em>NN</em>,
<em>145</em>, 233–247. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of crowd flows is an important urban computing issue whose purpose is to predict the future number of incoming and outgoing people in regions. Measuring the complicated spatial–temporal dependencies with external factors, such as weather conditions and surrounding point-of-interest (POI) distribution is the most difficult aspect of predicting crowd flows movement. To overcome the above issue, this paper advises a unified dynamic deep spatio-temporal neural network model based on convolutional neural networks and long short-term memory, termed as (DHSTNet) to simultaneously predict crowd flows in every region of a city. The DHSTNet model is made up of four separate components: a recent, daily, weekly, and an external branch component. Our proposed approach simultaneously assigns various weights to different branches and integrates the four properties’ outputs to generate final predictions. Moreover, to verify the generalization and scalability of the proposed model, we apply a Graph Convolutional Network (GCN) based on Long Short Term Memory (LSTM) with the previously published model, termed as GCN-DHSTNet; to capture the spatial patterns and short-term temporal features; and to illustrate its exceptional accomplishment in predicting the traffic crowd flows. The GCN-DHSTNet model not only depicts the spatio-temporal dependencies but also reveals the influence of different time granularity , which are recent, daily, weekly periodicity and external properties, respectively. Finally, a fully connected neural network is utilized to fuse the spatio-temporal features and external properties together. Using two different real-world traffic datasets, our evaluation suggests that the proposed GCN-DHSTNet method is approximately 7.9\%–27.2\% and 11.2\%–11.9\% better than the AAtt-DHSTNet method in terms of RMSE and MAPE metrics, respectively. Furthermore, AAtt-DHSTNet outperforms other state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Ahmad Ali and Yanmin Zhu and Muhammad Zakarya},
  doi          = {10.1016/j.neunet.2021.10.021},
  journal      = {Neural Networks},
  pages        = {233-247},
  shortjournal = {Neural Netw.},
  title        = {Exploiting dynamic spatio-temporal graph convolutional neural networks for citywide traffic flows prediction},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparsity-control ternary weight networks. <em>NN</em>,
<em>145</em>, 221–232. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been widely and successfully applied to various applications, but they require large amounts of memory and computational power. This severely restricts their deployment on resource-limited devices. To address this issue, many efforts have been made on training low-bit weight DNNs. In this paper, we focus on training ternary weight {−1, 0, ＋1} networks which can avoid multiplications and dramatically reduce the memory and computation requirements. A ternary weight network can be considered as a sparser version of the binary weight counterpart by replacing some −1s or 1s in the binary weights with 0s, thus leading to more efficient inference but more memory cost. However, the existing approaches to train ternary weight networks cannot control the sparsity (i.e., percentage of 0s) of the ternary weights, which undermines the advantage of ternary weights. In this paper, we propose to our best knowledge the first sparsity-control approach (SCA) to train ternary weight networks, which is simply achieved by a weight discretization regularizer (WDR). SCA is different from all the existing regularizer-based approaches in that it can control the sparsity of the ternary weights through a controller α α and does not rely on gradient estimators. We theoretically and empirically show that the sparsity of the trained ternary weights is positively related to α α . SCA is extremely simple, easy-to-implement, and is shown to consistently outperform the state-of-the-art approaches significantly over several benchmark datasets and even matches the performances of the full-precision weight counterparts.},
  archive      = {J_NN},
  author       = {Xiang Deng and Zhongfei Zhang},
  doi          = {10.1016/j.neunet.2021.10.018},
  journal      = {Neural Networks},
  pages        = {221-232},
  shortjournal = {Neural Netw.},
  title        = {Sparsity-control ternary weight networks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GuidedStyle: Attribute knowledge guided style manipulation
for semantic face editing. <em>NN</em>, <em>145</em>, 209–220. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although significant progress has been made in synthesizing high-quality and visually realistic face images by unconditional Generative Adversarial Networks (GANs), there is still a lack of control over the generation process in order to achieve semantic face editing. In this paper, we propose a novel learning framework, called GuidedStyle, to achieve semantic face editing on pretrained StyleGAN by guiding the image generation process with a knowledge network. Furthermore, we allow an attention mechanism in StyleGAN generator to adaptively select a single layer for style manipulation. As a result, our method is able to perform disentangled and controllable edits along various attributes, including smiling, eyeglasses, gender, mustache, hair color and attractive. Both qualitative and quantitative results demonstrate the superiority of our method over other competing methods for semantic face editing. Moreover, we show that our model can be also applied to different types of real and artistic face editing, demonstrating strong generalization ability .},
  archive      = {J_NN},
  author       = {Xianxu Hou and Xiaokang Zhang and Hanbang Liang and Linlin Shen and Zhihui Lai and Jun Wan},
  doi          = {10.1016/j.neunet.2021.10.017},
  journal      = {Neural Networks},
  pages        = {209-220},
  shortjournal = {Neural Netw.},
  title        = {GuidedStyle: Attribute knowledge guided style manipulation for semantic face editing},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting out-of-distribution samples via variational
auto-encoder with reliable uncertainty estimation. <em>NN</em>,
<em>145</em>, 199–208. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are influential generative models with rich representation capabilities from the deep neural network architecture and Bayesian method. However, VAE models have a weakness that assign a higher likelihood to out-of-distribution (OOD) inputs than in-distribution (ID) inputs. To address this problem, a reliable uncertainty estimation is considered to be critical for in-depth understanding of OOD inputs. In this study, we propose an improved noise contrastive prior (INCP) to be able to integrate into the encoder of VAEs, called INCPVAE. INCP is scalable, trainable and compatible with VAEs, and it also adopts the merits from the INCP for uncertainty estimation . Experiments on various datasets demonstrate that compared to the standard VAEs, our model is superior in uncertainty estimation for the OOD data and is robust in anomaly detection tasks. The INCPVAE model obtains reliable uncertainty estimation for OOD inputs and solves the OOD problem in VAE models.},
  archive      = {J_NN},
  author       = {Xuming Ran and Mingkun Xu and Lingrui Mei and Qi Xu and Quanying Liu},
  doi          = {10.1016/j.neunet.2021.10.020},
  journal      = {Neural Networks},
  pages        = {199-208},
  shortjournal = {Neural Netw.},
  title        = {Detecting out-of-distribution samples via variational auto-encoder with reliable uncertainty estimation},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Exponential synchronization of coupled neural networks
under stochastic deception attacks. <em>NN</em>, <em>145</em>, 189–198.
(<a href="https://doi.org/10.1016/j.neunet.2021.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the issue of synchronization is investigated for coupled neural networks subject to stochastic deception attacks. Firstly, a general differential inequality with delayed impulses is given. Then, the established differential inequality is further extended to the case of delayed stochastic impulses, in which both the impulsive instants and impulsive intensity are stochastic. Secondly, by modeling the stochastic discrete-time deception attacks as stochastic impulses, synchronization criteria of the coupled neural networks under the corresponding attacks are given. Finally, two numerical examples are provided to demonstrate the correctness of the theoretical results.},
  archive      = {J_NN},
  author       = {Huihui Zhang and Lulu Li and Xiaodi Li},
  doi          = {10.1016/j.neunet.2021.10.015},
  journal      = {Neural Networks},
  pages        = {189-198},
  shortjournal = {Neural Netw.},
  title        = {Exponential synchronization of coupled neural networks under stochastic deception attacks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IC neuron: An efficient unit to construct neural networks.
<em>NN</em>, <em>145</em>, 177–188. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular machine learning method, neural networks can be used to solve many complex tasks. Their strong generalization ability comes from the representation ability of the basic neuron models. The most popular neuron model is the McCulloch–Pitts (MP) neuron, which uses a simple transformation to process the input signal. A common trend is to use the MP neuron to design various neural networks . However, the optimization of the neuron structure is rarely considered. Inspired by the elastic collision model in physics, we propose a new neuron model that can represent more complex distributions. We term it the Inter-layer Collision (IC) neuron which divides the input space into multiple subspaces to represent different linear transformations . Through this operation, the IC neuron enhances the non-linear representation ability and emphasizes useful input features for a given task. We build the IC networks by integrating the IC neurons into the fully-connected, the convolutional, and the recurrent structures. The IC networks outperform the traditional neural networks in a wide range of tasks. Besides, we combine the IC neuron with deep learning models and show the superiority of the IC neuron. Our research proves that the IC neuron can be an effective basic unit to build network structures and make the network performance better.},
  archive      = {J_NN},
  author       = {Junyi An and Fengshan Liu and Furao Shen and Jian Zhao and Ruotong Li and Kepan Gao},
  doi          = {10.1016/j.neunet.2021.10.005},
  journal      = {Neural Networks},
  pages        = {177-188},
  shortjournal = {Neural Netw.},
  title        = {IC neuron: An efficient unit to construct neural networks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Epistemic uncertainty quantification in deep learning
classification by the delta method. <em>NN</em>, <em>145</em>, 164–176.
(<a href="https://doi.org/10.1016/j.neunet.2021.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Delta method is a classical procedure for quantifying epistemic uncertainty in statistical models, but its direct application to deep neural networks is prevented by the large number of parameters P P . We propose a low cost approximation of the Delta method applicable to L 2 L2 -regularized deep neural networks based on the top K K eigenpairs of the Fisher information matrix . We address efficient computation of full-rank approximate eigendecompositions in terms of the exact inverse Hessian, the inverse outer-products of gradients approximation and the so-called Sandwich estimator. Moreover, we provide bounds on the approximation error for the uncertainty of the predictive class probabilities. We show that when the smallest computed eigenvalue of the Fisher information matrix is near the L 2 L2 -regularization rate, the approximation error will be close to zero even when K ≪ P K≪P . A demonstration of the methodology is presented using a TensorFlow implementation, and we show that meaningful rankings of images based on predictive uncertainty can be obtained for two LeNet and ResNet-based neural networks using the MNIST and CIFAR-10 datasets. Further, we observe that false positives have on average a higher predictive epistemic uncertainty than true positives . This suggests that there is supplementing information in the uncertainty measure not captured by the classification alone.},
  archive      = {J_NN},
  author       = {Geir K. Nilsen and Antonella Z. Munthe-Kaas and Hans J. Skaug and Morten Brun},
  doi          = {10.1016/j.neunet.2021.10.014},
  journal      = {Neural Networks},
  pages        = {164-176},
  shortjournal = {Neural Netw.},
  title        = {Epistemic uncertainty quantification in deep learning classification by the delta method},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zenithal isotropic object counting by localization using
adversarial training. <em>NN</em>, <em>145</em>, 155–163. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counting objects in images is a very time-consuming task for humans that yields to errors caused by repetitiveness and boredom. In this paper, we present a novel object counting method that, unlike most of the recent works that focus on the regression of a density map, performs the counting procedure by localizing each single object. This key difference allows us to provide not only an accurate count but the position of every counted object, information that can be critical in some areas such as precision agriculture. The method is designed in two steps: first, a CNN is in charge of mapping arbitrary objects to blob-like structures. Then, using a Laplacian of Gaussian (LoG) filter, we are able to gather the position of all detected objects. We also propose a semi-adversarial training procedure that, combined with the former design, improves the result by a large margin. After evaluating the method on two public benchmarks of isometric objects, we stay on par with the state of the art while being able to provide extra position information.},
  archive      = {J_NN},
  author       = {Javier Rodriguez-Vazquez and Adrian Alvarez-Fernandez and Martin Molina and Pascual Campoy},
  doi          = {10.1016/j.neunet.2021.10.010},
  journal      = {Neural Networks},
  pages        = {155-163},
  shortjournal = {Neural Netw.},
  title        = {Zenithal isotropic object counting by localization using adversarial training},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). On the capacity of deep generative networks for
approximating distributions. <em>NN</em>, <em>145</em>, 144–154. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the efficacy and efficiency of deep generative networks for approximating probability distributions. We prove that neural networks can transform a low-dimensional source distribution to a distribution that is arbitrarily close to a high-dimensional target distribution , when the closeness is measured by Wasserstein distances and maximum mean discrepancy. Upper bounds of the approximation error are obtained in terms of the width and depth of neural network. Furthermore, it is shown that the approximation error in Wasserstein distance grows at most linearly on the ambient dimension and that the approximation order only depends on the intrinsic dimension of the target distribution . On the contrary, when f f -divergences are used as metrics of distributions, the approximation property is different. We show that in order to approximate the target distribution in f f -divergences, the dimension of the source distribution cannot be smaller than the intrinsic dimension of the target distribution.},
  archive      = {J_NN},
  author       = {Yunfei Yang and Zhen Li and Yang Wang},
  doi          = {10.1016/j.neunet.2021.10.012},
  journal      = {Neural Networks},
  pages        = {144-154},
  shortjournal = {Neural Netw.},
  title        = {On the capacity of deep generative networks for approximating distributions},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). New effective approach to quasi synchronization of coupled
heterogeneous complex networks. <em>NN</em>, <em>145</em>, 139–143. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This short paper addresses quasi synchronization of linearly coupled heterogeneous systems . Similarity and difference between the complete synchronization of linearly coupled homogeneous systems and the quasi synchronization of linearly coupled heterogeneous systems will be revealed.},
  archive      = {J_NN},
  author       = {Tianping Chen},
  doi          = {10.1016/j.neunet.2021.10.019},
  journal      = {Neural Networks},
  pages        = {139-143},
  shortjournal = {Neural Netw.},
  title        = {New effective approach to quasi synchronization of coupled heterogeneous complex networks},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-attention-map-based regularization for adversarial
domain adaptation. <em>NN</em>, <em>145</em>, 128–138. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unsupervised domain adaptation (UDA), many efforts are taken to pull the source domain and the target domain closer by adversarial training . Most methods focus on aligning distributions or features between the source domain and the target domain. However, little attention is paid to the interaction between finer-grained levels, such as classes or samples of the two domains. In contrast to UDA, another transfer learning task, i.e., few-shot learning (FSL), takes full advantage of the finer-grained-level alignment. Many FSL methods implement the interaction between samples of support sets and query sets, leading to significant improvements. We wonder whether we can get some inspiration from these methods and bring such ideas of FSL to UDA. To this end, we first take a closer look at the differences between FSL and UDA and bridge the gap between them by high-confidence sample selection (HCSS). Then we propose cross-attention map generation module (CAMGM) to interact samples selected by HCSS. Moreover, we propose a simple but efficient method called cross-attention-map-based regularization (CAMR) to regularize the feature maps generated by the feature extractor. Experiments on three challenging datasets demonstrate that CAMR can bring solid improvements when added to the original objective. More specifically, the proposed CAMR can outperform original methods by 1\% to 2\% in most tasks without bells and whistles.},
  archive      = {J_NN},
  author       = {Jingwei Li and Huanjie Wang and Ke Wu and Chengbao Liu and Jie Tan},
  doi          = {10.1016/j.neunet.2021.10.013},
  journal      = {Neural Networks},
  pages        = {128-138},
  shortjournal = {Neural Netw.},
  title        = {Cross-attention-map-based regularization for adversarial domain adaptation},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning policy scheduling for text augmentation.
<em>NN</em>, <em>145</em>, 121–127. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training deep learning models , data augmentation is an important technique to improve the performance and alleviate overfitting. In natural language processing (NLP), existing augmentation methods often use fixed strategies. However, it might be preferred to use different augmentation policies in different stage of training, and different datasets may require different augmentation policies. In this paper, we take dynamic policy scheduling into consideration. We design a search space over augmentation policies by integrating several common augmentation operations. Then, we adopt a population based training method to search the best augmentation schedule. We conduct extensive experiments on five text classification and two machine translation tasks. The results show that the optimized dynamic augmentation schedules achieve significant improvements against previous methods.},
  archive      = {J_NN},
  author       = {Shuokai Li and Xiang Ao and Feiyang Pan and Qing He},
  doi          = {10.1016/j.neunet.2021.09.028},
  journal      = {Neural Networks},
  pages        = {121-127},
  shortjournal = {Neural Netw.},
  title        = {Learning policy scheduling for text augmentation},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic generative modeling and reinforcement learning
extract the intrinsic features of animal behavior. <em>NN</em>,
<em>145</em>, 107–120. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is one of the ultimate goals of ethology to understand the generative process of animal behavior , and the ability to reproduce and control behavior is an important step in this field. However, it is not easy to achieve this goal in systems with complex and stochastic dynamics such as animal behavior. In this study, we have shown that MDN–RNN,a type of probabilistic deep generative model , is able to reproduce stochastic animal behavior with high accuracy by modeling the behavior of C. elegans . Furthermore, we found that the model learns different dynamics in a disentangled representation as a time-evolving Gaussian mixture . Finally, by combining the model and reinforcement learning , we were able to extract a behavioral policy of goal-directed behavior in silico , and showed that it can be used for regulating the behavior of real animals. This set of methods will be applicable not only to animal behavior but also to broader areas such as neuroscience and robotics.},
  archive      = {J_NN},
  author       = {Keita Mori and Naohiro Yamauchi and Haoyu Wang and Ken Sato and Yu Toyoshima and Yuichi Iino},
  doi          = {10.1016/j.neunet.2021.10.002},
  journal      = {Neural Networks},
  pages        = {107-120},
  shortjournal = {Neural Netw.},
  title        = {Probabilistic generative modeling and reinforcement learning extract the intrinsic features of animal behavior},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpolation consistency training for semi-supervised
learning. <em>NN</em>, <em>145</em>, 90–106. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values.},
  archive      = {J_NN},
  author       = {Vikas Verma and Kenji Kawaguchi and Alex Lamb and Juho Kannala and Arno Solin and Yoshua Bengio and David Lopez-Paz},
  doi          = {10.1016/j.neunet.2021.10.008},
  journal      = {Neural Networks},
  pages        = {90-106},
  shortjournal = {Neural Netw.},
  title        = {Interpolation consistency training for semi-supervised learning},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-learning, social cognition and consciousness in brains
and machines. <em>NN</em>, <em>145</em>, 80–89. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intersection between neuroscience and artificial intelligence (AI) research has created synergistic effects in both fields. While neuroscientific discoveries have inspired the development of AI architectures, new ideas and algorithms from AI research have produced new ways to study brain mechanisms. A well-known example is the case of reinforcement learning (RL), which has stimulated neuroscience research on how animals learn to adjust their behavior to maximize reward. In this review article, we cover recent collaborative work between the two fields in the context of meta-learning and its extension to social cognition and consciousness. Meta-learning refers to the ability to learn how to learn, such as learning to adjust hyperparameters of existing learning algorithms and how to use existing models and knowledge to efficiently solve new tasks. This meta-learning capability is important for making existing AI systems more adaptive and flexible to efficiently solve new tasks. Since this is one of the areas where there is a gap between human performance and current AI systems, successful collaboration should produce new ideas and progress. Starting from the role of RL algorithms in driving neuroscience, we discuss recent developments in deep RL applied to modeling prefrontal cortex functions. Even from a broader perspective, we discuss the similarities and differences between social cognition and meta-learning, and finally conclude with speculations on the potential links between intelligence as endowed by model-based RL and consciousness. For future work we highlight data efficiency, autonomy and intrinsic motivation as key research areas for advancing both fields.},
  archive      = {J_NN},
  author       = {Angela Langdon and Matthew Botvinick and Hiroyuki Nakahara and Keiji Tanaka and Masayuki Matsumoto and Ryota Kanai},
  doi          = {10.1016/j.neunet.2021.10.004},
  journal      = {Neural Networks},
  pages        = {80-89},
  shortjournal = {Neural Netw.},
  title        = {Meta-learning, social cognition and consciousness in brains and machines},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cardinality-constrained portfolio selection based on
collaborative neurodynamic optimization. <em>NN</em>, <em>145</em>,
68–79. (<a href="https://doi.org/10.1016/j.neunet.2021.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio optimization is one of the most important investment strategies in financial markets. It is practically desirable for investors, especially high-frequency traders, to consider cardinality constraints in portfolio selection, to avoid odd lots and excessive costs such as transaction fees. In this paper, a collaborative neurodynamic optimization approach is presented for cardinality-constrained portfolio selection. The expected return and investment risk in the Markowitz framework are scalarized as a weighted Chebyshev function and the cardinality constraints are equivalently represented using introduced binary variables as an upper bound. Then cardinality-constrained portfolio selection is formulated as a mixed-integer optimization problem and solved by means of collaborative neurodynamic optimization with multiple recurrent neural networks repeatedly repositioned using a particle swarm optimization rule. The distribution of resulting Pareto-optimal solutions is also iteratively perfected by optimizing the weights in the scalarized objective functions based on particle swarm optimization . Experimental results with stock data from four major world markets are discussed to substantiate the superior performance of the collaborative neurodynamic approach to several exact and metaheuristic methods.},
  archive      = {J_NN},
  author       = {Man-Fai Leung and Jun Wang},
  doi          = {10.1016/j.neunet.2021.10.007},
  journal      = {Neural Networks},
  pages        = {68-79},
  shortjournal = {Neural Netw.},
  title        = {Cardinality-constrained portfolio selection based on collaborative neurodynamic optimization},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation capabilities of neural networks on unbounded
domains. <em>NN</em>, <em>145</em>, 56–67. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is limited study in the literature on the representability of neural networks on unbounded domains . For some application areas, results in this direction provide additional value in the design of learning systems. Motivated by an old option pricing problem, we are led to the study of this subject. For networks with a single hidden layer, we show that under suitable conditions they are capable of universal approximation in L p ( R × [ 0 , 1 ] n ) Lp(R×[0,1]n) but not in L p ( R 2 × [ 0 , 1 ] n ) Lp(R2×[0,1]n) . For deeper networks, we prove that the ReLU network with two hidden layers is a universal approximator in L p ( R n ) Lp(Rn) .},
  archive      = {J_NN},
  author       = {Ming-Xi Wang and Yang Qu},
  doi          = {10.1016/j.neunet.2021.10.001},
  journal      = {Neural Networks},
  pages        = {56-67},
  shortjournal = {Neural Netw.},
  title        = {Approximation capabilities of neural networks on unbounded domains},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LGN-CNN: A biologically inspired CNN architecture.
<em>NN</em>, <em>145</em>, 42–55. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a biologically inspired Convolutional Neural Network (CNN) architecture called LGN-CNN that has a first convolutional layer composed of a single filter that mimics the role of the Lateral Geniculate Nucleus (LGN). The first layer of the neural network shows a rotational symmetric pattern justified by the structure of the net itself that turns up to be an approximation of a Laplacian of Gaussian (LoG). The latter function is in turn a good approximation of the receptive field profiles (RFPs) of the cells in the LGN . The analogy with the visual system is established, emerging directly from the architecture of the neural network. A proof of rotation invariance of the first layer is given on a fixed LGN-CNN architecture and the computational results are shown. Thus, contrast invariance capability of the LGN-CNN is investigated and a comparison between the Retinex effects of the first layer of LGN-CNN and the Retinex effects of a LoG is provided on different images. A statistical study is done on the filters of the second convolutional layer with respect to biological data. In conclusion, the model we have introduced approximates well the RFPs of both LGN and V1 attaining similar behavior as regards long range connections of LGN cells that show Retinex effects.},
  archive      = {J_NN},
  author       = {Federico Bertoni and Giovanna Citti and Alessandro Sarti},
  doi          = {10.1016/j.neunet.2021.09.024},
  journal      = {Neural Networks},
  pages        = {42-55},
  shortjournal = {Neural Netw.},
  title        = {LGN-CNN: A biologically inspired CNN architecture},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A complementary learning approach for expertise transference
of human-optimized controllers. <em>NN</em>, <em>145</em>, 33–41. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a complementary learning scheme for experience transference of unknown continuous-time linear systems is proposed. The algorithm is inspired in the complementary learning properties that exhibit the hippocampus and neocortex learning systems via the striatum. The hippocampus is modelled as pattern-separated data of a human optimized controller. The neocortex is modelled as a Q-reinforcement learning algorithm which improves the hippocampus control policy. The complementary learning (striatum) is designed as an inverse reinforcement learning algorithm which relates the hippocampus and neocortex learning models to seek and transfer the weights of the hidden expert’s utility function. Convergence of the proposed approach is analysed using Lyapunov recursions. Simulations are given to verify the proposed approach.},
  archive      = {J_NN},
  author       = {Adolfo Perrusquía},
  doi          = {10.1016/j.neunet.2021.10.009},
  journal      = {Neural Networks},
  pages        = {33-41},
  shortjournal = {Neural Netw.},
  title        = {A complementary learning approach for expertise transference of human-optimized controllers},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enriching query semantics for code search with reinforcement
learning. <em>NN</em>, <em>145</em>, 22–32. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code search is a common practice for developers during software implementation. The challenges of accurate code search mainly lie in the knowledge gap between source code and natural language (i.e., queries). Due to the limited code-query pairs and large code-description pairs available, the prior studies based on deep learning techniques focus on learning the semantic matching relation between source code and corresponding description texts for the task, and hypothesize that the semantic gap between descriptions and user queries is marginal. In this work, we found that the code search models trained on code-description pairs may not perform well on user queries, which indicates the semantic distance between queries and code descriptions. To mitigate the semantic distance for more effective code search, we propose QueCos, a Que ry-enriched Co de s earch model. QueCos learns to generate semantic enriched queries to capture the key semantics of given queries with reinforcement learning (RL). With RL, the code search performance is considered as a reward for producing accurate semantic enriched queries. The enriched queries are finally employed for code search. Experiments on the benchmark datasets show that QueCos can significantly outperform the state-of-the-art code search models.},
  archive      = {J_NN},
  author       = {Chaozheng Wang and Zhenhao Nong and Cuiyun Gao and Zongjie Li and Jichuan Zeng and Zhenchang Xing and Yang Liu},
  doi          = {10.1016/j.neunet.2021.09.025},
  journal      = {Neural Networks},
  pages        = {22-32},
  shortjournal = {Neural Netw.},
  title        = {Enriching query semantics for code search with reinforcement learning},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized attention-weighted reinforcement learning.
<em>NN</em>, <em>145</em>, 10–21. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In neuroscience , attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning , despite their popularity, attention mechanisms have seldom been administered to decision-making problems. Here, we leverage a theoretical model from computational neuroscience – the attention-weighted RL (AWRL), defining how humans identify task-relevant features (i.e., that allow value predictions) – to design an applied deep RL paradigm. We formally demonstrate that the conjunction of the self-attention mechanism, widely employed in machine learning, with value function approximation is a general formulation of the AWRL model. To evaluate our agent, we train it on three Atari tasks at different complexity levels, incorporating both task-relevant and irrelevant features. Because the model uses semantic observations, we can uncover not only which features the agent elects to base decisions on, but also how it chooses to compile more complex, relational features from simpler ones. We first show that performance depends in large part on the ability to compile new compound features, rather than mere focus on individual features. In line with neuroscience predictions, self-attention leads to high resiliency to noise (irrelevant features) compared to other benchmark models . Finally, we highlight the importance and separate contributions of both bottom-up and top-down attention in the learning process. Together, these results demonstrate the broader validity of the AWRL framework in complex task scenarios, and illustrate the benefits of a deeper integration between neuroscience-derived models and RL for decision making in machine learning.},
  archive      = {J_NN},
  author       = {Lennart Bramlage and Aurelio Cortese},
  doi          = {10.1016/j.neunet.2021.09.023},
  journal      = {Neural Networks},
  pages        = {10-21},
  shortjournal = {Neural Netw.},
  title        = {Generalized attention-weighted reinforcement learning},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view graph embedding clustering network: Joint
self-supervision and block diagonal representation. <em>NN</em>,
<em>145</em>, 1–9. (<a
href="https://doi.org/10.1016/j.neunet.2021.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has become an active topic in artificial intelligence . Yet, similar investigation for graph-structured data clustering has been absent so far. To fill this gap, we present a Multi-View Graph embedding Clustering network (MVGC). Specifically, unlike traditional multi-view construction methods, which are only suitable to describe Euclidean structure data , we leverage Euler transform to augment the node attribute, as a new view descriptor, for non-Euclidean structure data . Meanwhile, we impose block diagonal representation constraint, which is measured by the ℓ 1 , 2 ℓ1,2 -norm, on self-expression coefficient matrix to well explore the cluster structure. By doing so, the learned view-consensus coefficient matrix well encodes the discriminative information. Moreover, we make use of the learned clustering labels to guide the learnings of node representation and coefficient matrix, where the latter is used in turn to conduct the subsequent clustering. In this way, clustering and representation learning are seamlessly connected, with the aim to achieve better clustering performance. Extensive experimental results indicate that MVGC is superior to 11 state-of-the-art methods on four benchmark datasets. In particular, MVGC achieves an Accuracy of 96.17\% (53.31\%) on the ACM (IMDB) dataset, which is an up to 2.85\% (1.97\%) clustering performance improvement compared with the strongest baseline.},
  archive      = {J_NN},
  author       = {Wei Xia and Sen Wang and Ming Yang and Quanxue Gao and Jungong Han and Xinbo Gao},
  doi          = {10.1016/j.neunet.2021.10.006},
  journal      = {Neural Networks},
  pages        = {1-9},
  shortjournal = {Neural Netw.},
  title        = {Multi-view graph embedding clustering network: Joint self-supervision and block diagonal representation},
  volume       = {145},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
