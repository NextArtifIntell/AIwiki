<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---679">PR - 679</h2>
<ul>
<li><details>
<summary>
(2022). Progressive downsampling and adaptive guidance networks for
dynamic scene deblurring. <em>PR</em>, <em>132</em>, 108988. (<a
href="https://doi.org/10.1016/j.patcog.2022.108988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing learning-based dynamic scene deblurring methods have made good progress to some extent. However, these methods are usually based on multiscale strategy, which has the following shortcomings: (1) The bilinear downsampling operation will cause some loss of important high-frequency information, e.g., strong edges, which also further affects the network learning a better deblurring mapping. (2) Existing methods only use a single activation function , which limits the ability of the network model to fit data and causes the network performance to be easily saturated. Therefore, we propose an end-to-end progressive downsampling and adaptive guidance network called PDAG-Net for solving above problems. The proposed PDAG-Net can retain more strong edges and other high-frequency information of a blurry image so as to make the network learn a more effective deblurring mapping between the input and label images. In the proposed PDAG-Net, we implement a multiscale blended activation residual block called MSBA-ResBlock for learning the nonlinear characteristics of dynamic scene blur, which can also alleviate the performance saturation problem caused by a single activation function and improve multiscale feature extraction ability. Finally, we propose a multisupervision strategy for obtaining more robust and effective features and making the network possess more stable trainging and faster convergence. Extensive experimental results on a public dataset indicate that the proposed network outperforms the state-of-the-art image deblurring methods.},
  archive      = {J_PR},
  author       = {Jinkai Cui and Weihong Li and Wei Guo and Weiguo Gong},
  doi          = {10.1016/j.patcog.2022.108988},
  journal      = {Pattern Recognition},
  pages        = {108988},
  shortjournal = {Pattern Recognition},
  title        = {Progressive downsampling and adaptive guidance networks for dynamic scene deblurring},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive deep non-negative matrix factorization
architecture with graph convolution-based basis image reorganization.
<em>PR</em>, <em>132</em>, 108984. (<a
href="https://doi.org/10.1016/j.patcog.2022.108984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep non-negative matrix factorization is committed to using multi-layer structure to extract underlying parts-based representation. However, the basis images obtained by continuous depth factorization is too sparse, resulting in too fragmented parts reflected by the basis image. This makes the number of factorization layers limited and the underlying local feature representation is inaccurate. Therefore, we propose a novel progressive deep non-negative matrix factorization (PDNMF) architecture that adds a basis image reconstruction step to the successive basis image factorization steps. This helps the basis image in depth factorization to maintain better robustness of feature representation. In the reconstruction step, the attribute similarity graph (ASG) is constructed to describe the semantic expression ability of each basis image. With the help of the ASG, the basis image enhances its own semantic integrity through graph convolution without drastically destroying its representation. The evaluation in image recognition shows that the recognition accuracy of the proposed PDNMF improves with the increase of layers. Our method outperforms the state-of-the-art deep factorization methods in image recognition.},
  archive      = {J_PR},
  author       = {Yang Zhao and Furong Deng and Jihong Pei and Xuan Yang},
  doi          = {10.1016/j.patcog.2022.108984},
  journal      = {Pattern Recognition},
  pages        = {108984},
  shortjournal = {Pattern Recognition},
  title        = {Progressive deep non-negative matrix factorization architecture with graph convolution-based basis image reorganization},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion-blurred image restoration framework based on
parameter estimation and fuzzy radial basis function neural networks.
<em>PR</em>, <em>132</em>, 108983. (<a
href="https://doi.org/10.1016/j.patcog.2022.108983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restoration of motion-blurred images has always been a complex problem in image restoration. The current single blurred image algorithm cannot very well solve the estimation error of motion blur parameters. A comprehensive motion-blurred image restoration framework is proposed, which includes motion-blurred data generation, blur parameter estimation, and image quality assessment of restored images. First, we designed and used four image data sets with different degrees of blurring. We innovatively propose a blur parameter estimation algorithm based on the particle swarm optimization (B-PSO) algorithm. The Naturalness Image Quality Evaluator (NIQE) is used as the fitness function of the PSO algorithm. The framework also introduces a polynomial-based radial basis function neural network (P-RBFNN) as a new image quality assessment (IQA) method , with good image classification performance. Test results from public datasets show that the proposed framework can accurately estimate blur parameters. The peak signal-to-noise ratio (PSNR) reaches 29.976 dB, the structural similarity (SSIM) reaches 0.9044, and the classification rate is 96\%. The proposed restoration framework produces the best image restoration results.},
  archive      = {J_PR},
  author       = {Shengmin Zhao and Sung-Kwun Oh and Jin-Yul Kim and Zunwei Fu and Witold Pedrycz},
  doi          = {10.1016/j.patcog.2022.108983},
  journal      = {Pattern Recognition},
  pages        = {108983},
  shortjournal = {Pattern Recognition},
  title        = {Motion-blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAAN: Context-aware attention network for visual question
answering. <em>PR</em>, <em>132</em>, 108980. (<a
href="https://doi.org/10.1016/j.patcog.2022.108980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding multimodal information is the key to visual question answering (VQA) tasks. Most existing approaches use attention mechanisms to acquire fine-grained information understanding. However, these approaches with merely attention mechanisms do not solve the potential understanding bias problem. Hence, this paper introduces contextual information into VQA for the first time and presents a context-aware attention network (CAAN) to tackle the case. By improving the modular co-attention network (MCAN) framework, CAAN’s main work includes: designing a novel absolute position calculation method based on the coordinates of each image region in the image and the image’s actual size, the position information of all image regions are integrated as contextual information to enhance the visual representation; based on the question itself, several internal contextual information representations are introduced to participate in the modeling of the question words, solving the understanding bias caused by the similarity of the question. Additionally, we also designed two models of different scales, namely CAAN-base and CAAN-large, to explore the effect of the field of view on interaction. Finally, extensive experimental results show that CAAN significantly outperforms MCAN and achieves comparable or even better performance than other state-of-the-art approaches, proving our method can tackle the understanding bias.},
  archive      = {J_PR},
  author       = {Chongqing Chen and Dezhi Han and Chin-Chen Chang},
  doi          = {10.1016/j.patcog.2022.108980},
  journal      = {Pattern Recognition},
  pages        = {108980},
  shortjournal = {Pattern Recognition},
  title        = {CAAN: Context-aware attention network for visual question answering},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein distributional harvesting for highly dense 3D
point clouds. <em>PR</em>, <em>132</em>, 108978. (<a
href="https://doi.org/10.1016/j.patcog.2022.108978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel 3D point cloud harvesting method, which can harvest 3D points from an estimated surface distribution in an unsupervised manner (i.e., an input is a prior distribution). Our method outputs the surface distribution of a 3D object and samples 3D points from the distribution based on the proposed progressive random sampling strategy. The progressive sampling regards a prior distribution itself as a network input and uses a progressively increasing number of latent variables for training, which can diversify the coordinates of 3D points with fast convergence. Subsequently, our stochastic instance normalization transforms the implicit distribution into other distributions, which enables diverse shapes of 3D objects. Experimental results show that our method is competitive with other state-of-the-art methods. Our method can harvest an arbitrary number of 3D points, wherein the 3D object is represented in detail with highly dense 3D points or a part of it is described with partial sampling.},
  archive      = {J_PR},
  author       = {Dong Wook Shu and Sung Woo Park and Junseok Kwon},
  doi          = {10.1016/j.patcog.2022.108978},
  journal      = {Pattern Recognition},
  pages        = {108978},
  shortjournal = {Pattern Recognition},
  title        = {Wasserstein distributional harvesting for highly dense 3D point clouds},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised spectral clustering with exemplar
constraints. <em>PR</em>, <em>132</em>, 108975. (<a
href="https://doi.org/10.1016/j.patcog.2022.108975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a leading graph clustering technique , spectral clustering is one of the most widely used clustering methods that captures complex clusters in data. However, some of its deficiencies, such as the high computational complexity in eigen decomposition and the guidance without supervised information , limit its real applications. To get rid of the deficiencies, we propose a self-supervised spectral clustering algorithm. In this algorithm, we define an exemplar constraint which reflects the relations between objects and exemplars. We provide the related analysis to show that it is more suitable for unsupervised learning . Based on the exemplar constraint, we build an optimization model for self-supervised spectral clustering so that we can simultaneously learn clustering results and exemplar constraints. Furthermore, we propose an iterative method to solve the new optimization problem . Compared to other existing versions of spectral clustering algorithms, the new algorithm can use the low computational costs to discover a high-quality cluster structure of a data set without prior information. Furthermore, we did a number of experiments of algorithm comparison and parameter analysis on benchmark data sets to illustrate that the proposed algorithm is very effective and efficient.},
  archive      = {J_PR},
  author       = {Liang Bai and Yunxiao Zhao and Jiye Liang},
  doi          = {10.1016/j.patcog.2022.108975},
  journal      = {Pattern Recognition},
  pages        = {108975},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised spectral clustering with exemplar constraints},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Online temporal classification of human action using action
inference graph. <em>PR</em>, <em>132</em>, 108972. (<a
href="https://doi.org/10.1016/j.patcog.2022.108972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, deep learning methods have achieved state-of-the-art results in human action recognition . These methods process a full video sequence to recognize an action, which is unnecessary because many frames are similar. Recently, keyframe-based methods are proposed to overcome this issue. Though keyframe based methods have shown competitive performance in action recognition, both methods still process all the required frames of a video clip and average the results of individual clips/frames to recognize the action of the video. We argue that by simply using the average of the results of the video clips, deep models are not using the motion information of the video and thus leads to an inaccurate recognition of the action. To cope with the aforementioned issue, we propose a new online temporal classification model (OTCM) that classifies an action from a video in an online fashion and addresses the issue of averaging by making decision of each frame of a video sequence. As well, we propose a new action inference graph (AIG) that enables early recognition. Hence, the proposed model can recognize an action early before using all the keyframes or the whole video sequence and thus, requires less computation for recognizing human actions. Moreover, our OTCM can perform online action detection. To the best of our knowledge, this is the first time that the OTCM model along with the AIG is proposed. The experimental results of the benchmark datasets show that the proposed OTCM model has achieved and set a new record of the SOTA results, in particular, without using full video sequences.},
  archive      = {J_PR},
  author       = {G M Mashrur E Elahi and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2022.108972},
  journal      = {Pattern Recognition},
  pages        = {108972},
  shortjournal = {Pattern Recognition},
  title        = {Online temporal classification of human action using action inference graph},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic gate-based autoencoder for unsupervised
hyperspectral band selection. <em>PR</em>, <em>132</em>, 108969. (<a
href="https://doi.org/10.1016/j.patcog.2022.108969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its strong feature representation ability, the deep learning (DL)-based method is preferable for the unsupervised band selection task of hyperspectral image (HSI). However, the current DL-based UBS methods have not further investigated the nonlinear relationship between spectral bands , a more robust DL model with effective loss function is desired. To solve the above problem, a novel stochastic gate-based autoencoder (SGAE) has been proposed for the UBS task. With the proposed stochastic gate layer, the desired band subset with learnable parameters can be directly obtained. For obtaining better UBS results, a nonlinear regularization term is added with the loss function to supervise the training process of SGAE. Furthermore, an early stopping criteria with a regularization term-based threshold is developed. Experimental results on four publicly available remote sensing datasets prove the effectiveness of our SGAE.},
  archive      = {J_PR},
  author       = {He Sun and Lei Zhang and Lizhi Wang and Hua Huang},
  doi          = {10.1016/j.patcog.2022.108969},
  journal      = {Pattern Recognition},
  pages        = {108969},
  shortjournal = {Pattern Recognition},
  title        = {Stochastic gate-based autoencoder for unsupervised hyperspectral band selection},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MonoPoly: A practical monocular 3D object detector.
<em>PR</em>, <em>132</em>, 108967. (<a
href="https://doi.org/10.1016/j.patcog.2022.108967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection plays a pivotal role in driver assistance systems and has practical requirements for small storage and fast inference. Monocular 3D detection alternatives abandon the complexity of LiDAR setup and pursues the effectiveness and efficiency of the vision scheme. In this work, we propose a set of anchor-free monocular 3D detectors called MonoPoly based on the keypoint paradigm. Specifically, we design a polynomial feature aggregation sampling module to extract multi-scale context features for auxiliary training and alleviate classification and localization misalignment through an attention-aware loss. Extensive experiments show that the proposed MonoPoly series achieves an excellent trade-off between performance and model size while maintaining real-time efficiency on KITTI and nuScenes datasets.},
  archive      = {J_PR},
  author       = {He Guan and Chunfeng Song and Zhaoxiang Zhang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2022.108967},
  journal      = {Pattern Recognition},
  pages        = {108967},
  shortjournal = {Pattern Recognition},
  title        = {MonoPoly: A practical monocular 3D object detector},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncorrelated feature selection via sparse latent
representation and extended OLSDA. <em>PR</em>, <em>132</em>, 108966.
(<a href="https://doi.org/10.1016/j.patcog.2022.108966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern unsupervised feature selection methods predominantly obtain the cluster structure and pseudo-labels information through spectral clustering. However, the pseudo-labels obtained by spectral clustering are usually mixed between positive and negative. Moreover, the Laplacian matrix in spectral clustering typically affects feature selection. Additionally, spectral clustering does not consider the interconnection information between data. To address these problems, this paper proposes uncorrelated feature selection via sparse latent representation and extended orthogonal least square discriminant analysis (OLSDA), which we term SLREO). Firstly, SLREO retains the interconnection between data by latent representation learning, and preserves the internal information between the data. In order to remove redundant interconnection information, an l 2,1 -norm constraint is applied to the residual matrix of potential representation learning. Secondly, SLREO obtains non-negative pseudo-labels through orthogonal least square discriminant analysis (OLSDA) of embedded non-negative manifold structure. It not only avoids the appearance of negative pseudo-labels, but also eliminates the effect of the Laplacian matrix on feature selection. The manifold information of the data is also preserved. Furthermore, the matrix of the learned latent representation and OLSDA is used as pseudo-labels information. It not only ensures that the generated pseudo-labels are non-negative, but also makes the pseudo-labels closer to the true class labels. Finally, in order to avoid trivial solutions, an uncorrelated constraint and l 2,1 -norm constraint are imposed on the feature transformation matrix . These constraints ensure row sparsity of the feature transformation matrix, select low-redundant and discriminative features , and improve the effect of feature selection. Experimental results show that the Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) of SLREO are significantly improved, as compared with six other published algorithms, tested on 11 benchmark datasets.},
  archive      = {J_PR},
  author       = {Ronghua Shang and Jiarui Kong and Weitong Zhang and Jie Feng and Licheng Jiao and Rustam Stolkin},
  doi          = {10.1016/j.patcog.2022.108966},
  journal      = {Pattern Recognition},
  pages        = {108966},
  shortjournal = {Pattern Recognition},
  title        = {Uncorrelated feature selection via sparse latent representation and extended OLSDA},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JSL3d: Joint subspace learning with implicit structure
supervision for 3D pose estimation. <em>PR</em>, <em>132</em>, 108965.
(<a href="https://doi.org/10.1016/j.patcog.2022.108965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating 3D human poses from a single image is an important task in computer graphics . Most model-based estimation methods represent the labeled/detected 2D poses and the projection of approximated 3D poses using vector representations of body joints. However, such lower-dimensional vector representations fail to maintain the spatial relations of original body joints, because the representations do not consider the inherent structure of body joints. In this paper, we propose JSL3d , a novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model, capturing the latent spatial relations of 2D body joints by an end-to-end autoencoder network. JSL3d jointly combines the learned latent spatial relations and 2D joints as inputs for the standard SR inference frame. The optimization is simultaneously processed via geometric priors in both latent and original feature spaces. We have evaluated JSL3d using four large-scale and well-recognized benchmarks, including Human3.6M , HumanEva-I , CMU MoCap and MPII . The experiment results demonstrate the effectiveness of JSL3d .},
  archive      = {J_PR},
  author       = {Mengxi Jiang and Shihao Zhou and Cuihua Li and Yunqi Lei},
  doi          = {10.1016/j.patcog.2022.108965},
  journal      = {Pattern Recognition},
  pages        = {108965},
  shortjournal = {Pattern Recognition},
  title        = {JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Enhancement of DNN-based multilabel classification by
grouping labels based on data imbalance and label correlation.
<em>PR</em>, <em>132</em>, 108964. (<a
href="https://doi.org/10.1016/j.patcog.2022.108964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel classification (MLC) is a challenging task in real-world applications, such as project document classification which led us to conduct this research. In the past decade, deep neural networks (DNNs) have been explored in MLC due to their flexibility in dealing with annotated data. However, DNN-based MLC still suffers many problems. Two critical problems are data imbalance and label correlation. These two problems will become more prominent when a training dataset is limited and with a large label set. In this study, special neural network configurations were developed to enhance the performance of DNN-based MLC based on data imbalance and label correlation. The classification accuracy of minority labels and users-preferred labels was increased using customized label groups. The proposed method was evaluated using river restoration project documents and other fifteen datasets. The results show that the proposed method generally increases f1-score for minority labels up to 10\%. Adding label dependence into label groups improves the f1-score of user-preferred majority labels up to 5\%. The accuracy increase varies in different datasets.},
  archive      = {J_PR},
  author       = {Ling Chen and Yuhong Wang and Hao Li},
  doi          = {10.1016/j.patcog.2022.108964},
  journal      = {Pattern Recognition},
  pages        = {108964},
  shortjournal = {Pattern Recognition},
  title        = {Enhancement of DNN-based multilabel classification by grouping labels based on data imbalance and label correlation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GFNet: Automatic segmentation of COVID-19 lung infection
regions using CT images based on boundary features. <em>PR</em>,
<em>132</em>, 108963. (<a
href="https://doi.org/10.1016/j.patcog.2022.108963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks , UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics , and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another “never seen” dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at https://github.com/zengzhenhuan/GFNet .},
  archive      = {J_PR},
  author       = {Chaodong Fan and Zhenhuan Zeng and Leyi Xiao and Xilong Qu},
  doi          = {10.1016/j.patcog.2022.108963},
  journal      = {Pattern Recognition},
  pages        = {108963},
  shortjournal = {Pattern Recognition},
  title        = {GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural architecture search via reference point based
multi‐objective evolutionary algorithm. <em>PR</em>, <em>132</em>,
108962. (<a href="https://doi.org/10.1016/j.patcog.2022.108962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For neural architecture search, NSGA-Net has searched a representative neural architecture set of Pareto-optimal solutions to consider both accuracy and computation complexity simultaneously. However, some decision-makers only concentrate on such neural architectures in the subpart regions of Pareto-optimal Frontier that they have interests in. Under the above circumstances, certain uninterested neural architectures may cost many computing resources. In order to consider the preference of decision-makers, we propose the reference point based NSGA-Net (RNSGA-Net) for neural architecture search. The core of RNSGA-Net adopts the reference point approach to guarantee the Pareto-optimal region close to the reference points and also combines the advantage of NSGAII with the fast nondominated sorting approach to split the Pareto front . Moreover, we augment an extra bit value of the original encoding to represent two types of residual block and one type of dense block for residual connection and dense connection in the RNSGA-Net. In order to satisfy the decision-maker preference, the multi-objective is measured to search competitive neural architecture by minimizing an error metric and FLOPs of computational complexity . Experiment results on the CIFAR-10 dataset demonstrate that RNSGA-Net can improve NSGA-Net in terms of the more structured representation space and the preference of decision-makers.},
  archive      = {J_PR},
  author       = {Lyuyang Tong and Bo Du},
  doi          = {10.1016/j.patcog.2022.108962},
  journal      = {Pattern Recognition},
  pages        = {108962},
  shortjournal = {Pattern Recognition},
  title        = {Neural architecture search via reference point based multi‐objective evolutionary algorithm},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Learnable dynamic margin in deep metric learning.
<em>PR</em>, <em>132</em>, 108961. (<a
href="https://doi.org/10.1016/j.patcog.2022.108961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the deepening of deep neural network research, deep metric learning has been further developed and achieved good results in many computer vision tasks. Deep metric learning trains the deep neural network by designing appropriate loss functions, and the deep neural network projects the training samples into an embedding space, where similar samples are very close, while dissimilar samples are far away. In the past two years, the proxy-based loss achieves remarkable improvements, boosts the speed of convergence and is robust against noisy labels and outliers due to the introduction of proxies. In the previous proxy-based losses, fixed margins were used to achieve the goal of metric learning, but the intra-class variance of fine-grained images were not fully considered. In this paper, a new proxy-based loss is proposed, which aims to set a learnable margin for each class, so that the intra-class variance can be better maintained in the final embedding space. Moreover, we also add a loss between proxies, so as to improve the discrimination between classes and further maintain the intra-class distribution. Our method is evaluated on fine-grained image retrieval , person re-identification and remote sensing image retrieval common benchmarks. The standard network trained by our loss achieves state-of-the-art performance. Thus, the possibility of extending our method to different fields of pattern recognition is confirmed.},
  archive      = {J_PR},
  author       = {Yifan Wang and Pingping Liu and Yijun Lang and Qiuzhan Zhou and Xue Shan},
  doi          = {10.1016/j.patcog.2022.108961},
  journal      = {Pattern Recognition},
  pages        = {108961},
  shortjournal = {Pattern Recognition},
  title        = {Learnable dynamic margin in deep metric learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An end-to-end supervised domain adaptation framework for
cross-domain change detection. <em>PR</em>, <em>132</em>, 108960. (<a
href="https://doi.org/10.1016/j.patcog.2022.108960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between pre-event and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bi-temporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34\% and 92.36\% on the CDD and WHU building datasets, respectively. The source code and models are publicly available at https://github.com/Perfect-You/SDACD .},
  archive      = {J_PR},
  author       = {Jia Liu and Wenjie Xuan and Yuhang Gan and Yibing Zhan and Juhua Liu and Bo Du},
  doi          = {10.1016/j.patcog.2022.108960},
  journal      = {Pattern Recognition},
  pages        = {108960},
  shortjournal = {Pattern Recognition},
  title        = {An end-to-end supervised domain adaptation framework for cross-domain change detection},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic self-attention with vision synchronization networks
for video question answering. <em>PR</em>, <em>132</em>, 108959. (<a
href="https://doi.org/10.1016/j.patcog.2022.108959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Question Answering (VideoQA) has gained increasing attention as an important task in understanding the rich spatio-temporal contents, i.e., the appearance and motion in the video. However, existing approaches mainly use the question to learn attentions over all the sampled appearance and motion features separately, which neglect two properties of VideoQA: (1) the answer to the question is often reflected on a few frames and video clips, and most video contents are superfluous; (2) appearance and motion features are usually concomitant and complementary to each other in time series. In this paper, we propose a novel VideoQA model, i.e., Dynamic Self-Attention with Vision Synchronization Networks (DSAVS), to address these problems. Specifically, a gated token selection mechanism is proposed to dynamically select the important tokens from appearance and motion sequences. These chosen tokens are fed into a self-attention mechanism to model the internal dependencies for more effective representation learning . To capture the correlation between the appearance and motion features, a vision synchronization block is proposed to synchronize the two types of vision features at the time slice level. Then, the visual objects can be correlated with their corresponding activities and the performance is further improved. Extensive experiments conducted on three public VideoQA data sets confirm the effectivity and superiority of our model compared with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yun Liu and Xiaoming Zhang and Feiran Huang and Shixun Shen and Peng Tian and Lang Li and Zhoujun Li},
  doi          = {10.1016/j.patcog.2022.108959},
  journal      = {Pattern Recognition},
  pages        = {108959},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic self-attention with vision synchronization networks for video question answering},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Counterfactual explanation based on gradual construction for
deep networks. <em>PR</em>, <em>132</em>, 108958. (<a
href="https://doi.org/10.1016/j.patcog.2022.108958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.},
  archive      = {J_PR},
  author       = {Hong-Gyu Jung and Sin-Han Kang and Hee-Dong Kim and Dong-Ok Won and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2022.108958},
  journal      = {Pattern Recognition},
  pages        = {108958},
  shortjournal = {Pattern Recognition},
  title        = {Counterfactual explanation based on gradual construction for deep networks},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covered style mining via generative adversarial networks for
face anti-spoofing. <em>PR</em>, <em>132</em>, 108957. (<a
href="https://doi.org/10.1016/j.patcog.2022.108957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing, a biometric authentication method , is a central part of automatic face recognition. Recently, two sets of approaches have performed particularly well against presentation attacks: 1) pixel-wise supervision-based methods, which intend to provide fine-grained pixel information to learn specific auxiliary maps; and 2) anomaly detection-based methods, which regard face anti-spoofing as an open-set training task and learn spoof detectors using only bona fide data, where the detectors are shown to generalize well to unknown attacks. However, these approaches depend on handcrafted prior information to control the generation of intermediate difference maps and easily fall into local optima. In this paper, we propose a novel frame-level face anti-spoofing method, Covered Style Mining-GAN (CSM-GAN), which converts face anti-spoofing detection into a style transfer process without any prior information. Specifically, CSM-GAN has four main components: the Covered Style Encoder (CSE), responsible for mining the difference map containing the photography style and discriminative clues; the Auxiliary Style Classifier (ASC), consisting of several stacked Difference Capture Blocks (DCB) responsible for distinguishing bona fide faces from spoofing faces; and the Style Transfer Generator (STG) and Style Adversarial Discriminator (SAD), which form generative adversarial networks to achieve style transfer. Comprehensive experiments on several benchmark datasets show that the proposed method not only outperforms current state-of-the-art but also produces better visual diversity in difference maps.},
  archive      = {J_PR},
  author       = {Yiqiang Wu and Dapeng Tao and Yong Luo and Jun Cheng and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.108957},
  journal      = {Pattern Recognition},
  pages        = {108957},
  shortjournal = {Pattern Recognition},
  title        = {Covered style mining via generative adversarial networks for face anti-spoofing},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory‐augmented neural networks based dynamic complex image
segmentation in digital twins for self‐driving vehicle. <em>PR</em>,
<em>132</em>, 108956. (<a
href="https://doi.org/10.1016/j.patcog.2022.108956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80\%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.},
  archive      = {J_PR},
  author       = {Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli},
  doi          = {10.1016/j.patcog.2022.108956},
  journal      = {Pattern Recognition},
  pages        = {108956},
  shortjournal = {Pattern Recognition},
  title        = {Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain consistency regularization for unsupervised
multi-source domain adaptive classification. <em>PR</em>, <em>132</em>,
108955. (<a href="https://doi.org/10.1016/j.patcog.2022.108955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment . In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.},
  archive      = {J_PR},
  author       = {Zhipeng Luo and Xiaobing Zhang and Shijian Lu and Shuai Yi},
  doi          = {10.1016/j.patcog.2022.108955},
  journal      = {Pattern Recognition},
  pages        = {108955},
  shortjournal = {Pattern Recognition},
  title        = {Domain consistency regularization for unsupervised multi-source domain adaptive classification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards generalizable person re-identification with a
bi-stream generative model. <em>PR</em>, <em>132</em>, 108954. (<a
href="https://doi.org/10.1016/j.patcog.2022.108954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras ( e.g., illumination and resolution differences) or pedestrian misalignments ( e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera ( CC ) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person ( CP ) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature , which contains an encoding network and two stream decoding sub-network. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.},
  archive      = {J_PR},
  author       = {Xin Xu and Wei Liu and Zheng Wang and Ruimin Hu and Qi Tian},
  doi          = {10.1016/j.patcog.2022.108954},
  journal      = {Pattern Recognition},
  pages        = {108954},
  shortjournal = {Pattern Recognition},
  title        = {Towards generalizable person re-identification with a bi-stream generative model},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting shape cues for weakly supervised semantic
segmentation. <em>PR</em>, <em>132</em>, 108953. (<a
href="https://doi.org/10.1016/j.patcog.2022.108953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation maps (CAMs) and use such masks to supervise segmentation networks . However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e ., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.},
  archive      = {J_PR},
  author       = {Sungpil Kho and Pilhyeon Lee and Wonyoung Lee and Minsong Ki and Hyeran Byun},
  doi          = {10.1016/j.patcog.2022.108953},
  journal      = {Pattern Recognition},
  pages        = {108953},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting shape cues for weakly supervised semantic segmentation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-rigid point set registration based on local neighborhood
information support. <em>PR</em>, <em>132</em>, 108952. (<a
href="https://doi.org/10.1016/j.patcog.2022.108952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid point set registration is a crucial task and an unsolved problem in the field of computer vision . One commonly used method for solving the problem is based on the Gaussian mixture model (GMM). In this method, the point set registration is formalized as a probability density estimation problem. Most GMM-based methods achieve registration by maintaining global and local structures of points. However, the previous methods did not filter the neighborhood information in the local structure, and the quality of local neighborhood information directly affects the accuracy of registration. Therefore, extracting effective local neighborhood information is still a challenge. We propose a novel point set registration method based on GMM by extracting local neighborhood information. The two point sets X and Y are regarded as the centroids of GMM and data points produced by GMM, respectively. Our method computes initial correspondences by comparing the feature descriptors of point sets, and the initial correspondences are updated by considering the neighborhood information. Our method then uses the Expectation–Maximization method to solve the GMM. In the experimental results, the efficiency and advantages of our method relative to the current methods are verified by applying five commonly used datasets.},
  archive      = {J_PR},
  author       = {Chuanju Liu and Dongmei Niu and Peng Wang and Xiuyang Zhao and Bo Yang and Caiming Zhang},
  doi          = {10.1016/j.patcog.2022.108952},
  journal      = {Pattern Recognition},
  pages        = {108952},
  shortjournal = {Pattern Recognition},
  title        = {Non-rigid point set registration based on local neighborhood information support},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alleviating the over-smoothing of graph neural computing by
a data augmentation strategy with entropy preservation. <em>PR</em>,
<em>132</em>, 108951. (<a
href="https://doi.org/10.1016/j.patcog.2022.108951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Convolutional Networks (GCN) proposed by Kipf and Welling is an effective model to improve semi-supervised learning of pattern recognition, but faces the obstacle of over-smoothing, which will weaken the representation ability of GCN. Recently some works are proposed to tackle above limitation by randomly perturbing graph topology or feature matrix to generate data augmentations as input for training. However, these operations inevitably do damage to the integrity of information structures and have to sacrifice the smoothness of feature manifold. In this paper, we first introduce a novel graph entropy definition as a measure to quantitatively evaluate the smoothness of a data manifold and then point out that this graph entropy is controlled by triangle motif-based information structures. Considering the preservation of graph entropy, we propose an effective strategy to generate randomly perturbed training data but maintain both graph topology and graph entropy. Extensive experiments have been conducted on real-world datasets and the results verify the effectiveness of our proposed method in improving semi-supervised node classification accuracy compared with a surge of baselines. Beyond that, our proposed approach could significantly enhance the robustness of training process for GCN.},
  archive      = {J_PR},
  author       = {Xue Liu and Dan Sun and Wei Wei},
  doi          = {10.1016/j.patcog.2022.108951},
  journal      = {Pattern Recognition},
  pages        = {108951},
  shortjournal = {Pattern Recognition},
  title        = {Alleviating the over-smoothing of graph neural computing by a data augmentation strategy with entropy preservation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Center prediction loss for re-identification. <em>PR</em>,
<em>132</em>, 108949. (<a
href="https://doi.org/10.1016/j.patcog.2022.108949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e. , the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e. , the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.},
  archive      = {J_PR},
  author       = {Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.108949},
  journal      = {Pattern Recognition},
  pages        = {108949},
  shortjournal = {Pattern Recognition},
  title        = {Center prediction loss for re-identification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatically classifying non-functional requirements using
deep neural network. <em>PR</em>, <em>132</em>, 108948. (<a
href="https://doi.org/10.1016/j.patcog.2022.108948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-functional requirements are property that software products must have in order to meet the user’s business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.},
  archive      = {J_PR},
  author       = {Bing Li and Xiuwen Nong},
  doi          = {10.1016/j.patcog.2022.108948},
  journal      = {Pattern Recognition},
  pages        = {108948},
  shortjournal = {Pattern Recognition},
  title        = {Automatically classifying non-functional requirements using deep neural network},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Siamese networks with an online reweighted example for
imbalanced data learning. <em>PR</em>, <em>132</em>, 108947. (<a
href="https://doi.org/10.1016/j.patcog.2022.108947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key challenging problem in data mining and decision-making is to establish a decision support system based on unbalanced datasets. In this study, we propose a novel algorithm to handle unbalanced learning problems that integrates the advantages of Siamese convolutional neural networks (SCNN) and the online reweighted example (ORE) algorithm into a unified method. First, the SCNN model is established for learning and extracting deep feature features at different levels. Second, the ORE algorithm is used to address the problem of data with a class-imbalanced distribution. Compared with baseline approaches, the experimental results show that our proposed method substantially enhances the performance of both within-project defect prediction and cross-project defect prediction.},
  archive      = {J_PR},
  author       = {Linchang Zhao and Zhaowei Shang and Jin Tan and Mingliang Zhou and Mu Zhang and Dagang Gu and Taiping Zhang and Yuan Yan Tang},
  doi          = {10.1016/j.patcog.2022.108947},
  journal      = {Pattern Recognition},
  pages        = {108947},
  shortjournal = {Pattern Recognition},
  title        = {Siamese networks with an online reweighted example for imbalanced data learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Table structure recognition and form parsing by end-to-end
object detection and relation parsing. <em>PR</em>, <em>132</em>,
108946. (<a href="https://doi.org/10.1016/j.patcog.2022.108946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of two-dimensional structure of tables and forms from document images is a challenge due to the complexity of document structures and the diversity of layouts. In this paper, we propose a graph neural network (GNN) based unified framework named Table Structure Recognition Network (TSRNet) to jointly detect and recognize the structures of various tables and forms. First, a multi-task fully convolutional network (FCN) is used to segment primitive regions such as text segments and ruling lines from document images , then a GNN is used to classify and group these primitive regions into page objects such as tables and cells. At last, the relationships between neighboring page objects are analyzed using another GNN based parsing module. The parameters of all the modules in the system can be trained end-to-end to optimize the overall performance. Experiments of table detection and structure recognition for modern documents on the POD 2017, cTDaR 2019 and PubTabNet datasets and template-free form parsing for historical documents on the NAF dataset show that the proposed method can handle various table/form structures and achieve superior performance.},
  archive      = {J_PR},
  author       = {Xiao-Hui Li and Fei Yin and He-Sen Dai and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2022.108946},
  journal      = {Pattern Recognition},
  pages        = {108946},
  shortjournal = {Pattern Recognition},
  title        = {Table structure recognition and form parsing by end-to-end object detection and relation parsing},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do deep neural networks contribute to multivariate time
series anomaly detection? <em>PR</em>, <em>132</em>, 108945. (<a
href="https://doi.org/10.1016/j.patcog.2022.108945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in time series is a complex task that has been widely studied. In recent years, the ability of unsupervised anomaly detection algorithms has received much attention. This trend has led researchers to compare only learning-based methods in their articles, abandoning some more conventional approaches. As a result, the community in this field has been encouraged to propose increasingly complex learning-based models mainly based on deep neural networks . To our knowledge, there are no comparative studies between conventional, machine learning-based and, deep neural network methods for the detection of anomalies in multivariate time series . In this work, we study the anomaly detection performance of sixteen conventional, machine learning-based and, deep neural network approaches on five real-world open datasets. By analyzing and comparing the performance of each of the sixteen methods, we show that no family of methods outperforms the others. Therefore, we encourage the community to reincorporate the three categories of methods in the anomaly detection in multivariate time series benchmarks.},
  archive      = {J_PR},
  author       = {Julien Audibert and Pietro Michiardi and Frédéric Guyard and Sébastien Marti and Maria A. Zuluaga},
  doi          = {10.1016/j.patcog.2022.108945},
  journal      = {Pattern Recognition},
  pages        = {108945},
  shortjournal = {Pattern Recognition},
  title        = {Do deep neural networks contribute to multivariate time series anomaly detection?},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In the eye of the beholder: A survey of gaze tracking
techniques. <em>PR</em>, <em>132</em>, 108944. (<a
href="https://doi.org/10.1016/j.patcog.2022.108944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features , it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.},
  archive      = {J_PR},
  author       = {Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin},
  doi          = {10.1016/j.patcog.2022.108944},
  journal      = {Pattern Recognition},
  pages        = {108944},
  shortjournal = {Pattern Recognition},
  title        = {In the eye of the beholder: A survey of gaze tracking techniques},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a category-extended object detector with limited
data. <em>PR</em>, <em>132</em>, 108943. (<a
href="https://doi.org/10.1016/j.patcog.2022.108943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detectors are typically learned on fully-annotated training data with fixed predefined categories. However, categories are often required to be increased progressively. Usually, only the original training set annotated with old classes and some new training data labeled with new classes are available in such scenarios. Based on the limited datasets, a unified detector that can handle all categories is strongly needed. We propose a practical scheme to achieve it in this work. A conflict-free loss is designed to avoid label ambiguity, leading to an acceptable detector in one training round. To further improve performance, we propose a retraining phase in which Monte Carlo Dropout is employed to calculate the localization confidence to mine more accurate bounding boxes , and an overlap-weighted method is proposed for making better use of pseudo annotations during retraining. Extensive experiments demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Bowen Zhao and Chen Chen and Xi Xiao and Shutao Xia},
  doi          = {10.1016/j.patcog.2022.108943},
  journal      = {Pattern Recognition},
  pages        = {108943},
  shortjournal = {Pattern Recognition},
  title        = {Towards a category-extended object detector with limited data},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distribution alignment for cross-device palmprint
recognition. <em>PR</em>, <em>132</em>, 108942. (<a
href="https://doi.org/10.1016/j.patcog.2022.108942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of IoT and mobile devices , cross-device palmprint recognition is becoming an emerging research topic in multimedia for its great application potential. Due to the diverse characteristics of different devices, e.g. resolution or artifacts caused by post-processing, cross-device palmprint recognition remains a challenging problem. In this paper, we make efforts to improve cross-device palmprint recognition in two aspects: (1) we put forward a novel distribution-based loss to narrow the representation gap across devices, and (2) we establish a new cross-device benchmark based on existing palmprint recognition datasets. Different from many recent studies that only utilize instance-level or pairwise-level information between devices, the proposed progressive target distribution loss (PTD loss) uses the distributional information. Moreover, we establish a progressive target mechanism that will be dynamically updated during training, making the optimization easier and smoother. The newly established benchmark contains more samples and more types of IoT devices than previous benchmarks, which can facilitate cross-device palmprint research. Extensive comparisons on several benchmarks reveal that: (1) our method outperforms other cross-device biometric recognition approaches significantly; (2) our method presents superior performance compared to SOTA competitors on several general palmprint recognition benchmarks; Code and data are openly available at https://kaizhao.net/palmprint .},
  archive      = {J_PR},
  author       = {Lei Shen and Yingyi Zhang and Kai Zhao and Ruixin Zhang and Wei Shen},
  doi          = {10.1016/j.patcog.2022.108942},
  journal      = {Pattern Recognition},
  pages        = {108942},
  shortjournal = {Pattern Recognition},
  title        = {Distribution alignment for cross-device palmprint recognition},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation in homogeneous distance space
for person re-identification. <em>PR</em>, <em>132</em>, 108941. (<a
href="https://doi.org/10.1016/j.patcog.2022.108941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data distribution alignment and clustering-based self-training are two feasible solutions to tackle unsupervised domain adaptation (UDA) on person re-identification (re-ID). Most existing alignment-based methods solely learn the source domain decision boundaries and align the data distribution of the target domain to the source domain, thus the re-ID performance on the target domain completely depends on the shared decision boundaries and how well the alignment is performed. However, two domains can hardly be precisely aligned because of the label space discrepancy of two domains, resulting in poor target domain re-ID performance. Although clustering-based self-training approaches could learn independent decision boundaries on the pseudo-labelled target domain data , they ignore both the accurate ID-related information of the labelled source domain data and the underlying relations between two domains. To fully exploit the source domain data to learn discriminative target domain ID-related features, in this paper, we propose a novel cross-domain alignment method in the homogeneous distance space, which is constructed by the newly designed stair-stepping alignment (SSA) matcher. Such alignment method can be integrated into both alignment-based framework and clustering-based framework. Extensive experiments validate the effectiveness of our proposed alignment method in these two frameworks. We achieve superior performance when the proposed alignment module is integrated into the clustering-based framework. Codes will be available at: http://github.com/Dingyuan-Zheng/HDS .},
  archive      = {J_PR},
  author       = {Dingyuan Zheng and Jimin Xiao and Yunchao Wei and Qiufeng Wang and Kaizhu Huang and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.108941},
  journal      = {Pattern Recognition},
  pages        = {108941},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation in homogeneous distance space for person re-identification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image based 3D human pose estimation via uncertainty
learning. <em>PR</em>, <em>132</em>, 108934. (<a
href="https://doi.org/10.1016/j.patcog.2022.108934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In monocular image scenes, 3D human pose estimation exhibits inherent ambiguity due to the loss of depth information and occlusions. Simply regressing body joints with high uncertainties will lead to model overfitting and poor generalization. In this paper, we propose an uncertainty-based framework to jointly learn 3D human poses and the uncertainty of each joint . Our proposed joint estimation framework aims to mitigate the adverse effects of training samples with high uncertainties and facilitate the training procedure. To be specific, we model each body joint as a Laplace distribution for uncertainty representation. Since visual joints often exhibit low uncertainties while occluded ones have high uncertainties, we develop an adaptive scaling factor, named the uncertainty-aware scaling factor, to ease the network optimization in accordance with the joint uncertainties. By doing so, our network is able to converge faster and significantly reduce the adverse effects caused by those ambiguous joints. Furthermore, we present an uncertainty-aware graph convolutional network by exploiting the learned joint uncertainties and the relationships among joints to refine the initial joint localization . Extensive experiments on single-person (Human3.6M) and multi-person (MuCo-3DHP &amp; MuPoTS-3D) 3D human pose estimation datasets demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Chuchu Han and Xin Yu and Changxin Gao and Nong Sang and Yi Yang},
  doi          = {10.1016/j.patcog.2022.108934},
  journal      = {Pattern Recognition},
  pages        = {108934},
  shortjournal = {Pattern Recognition},
  title        = {Single image based 3D human pose estimation via uncertainty learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized discriminant analysis via kernel exponential
families. <em>PR</em>, <em>132</em>, 108933. (<a
href="https://doi.org/10.1016/j.patcog.2022.108933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel supervised dimension reduction method for classification and regression problems using reproducing kernel Hilbert spaces. The proposed approach takes advantage of the modeling power of kernel exponential families to extract nonlinear summary statistics of the data that are sufficient to preserve information about the target response. For the special case of finite dimensional exponential family distributions, the proposed method is shown to simplify the known solutions for sufficient dimension reduction. A connection with support vector machines is shown and exploited to obtain efficient estimation procedures. Experiments with simulated and real data illustrate the potential of the proposed approach.},
  archive      = {J_PR},
  author       = {Isaías Ibañez and Liliana Forzani and Diego Tomassi},
  doi          = {10.1016/j.patcog.2022.108933},
  journal      = {Pattern Recognition},
  pages        = {108933},
  shortjournal = {Pattern Recognition},
  title        = {Generalized discriminant analysis via kernel exponential families},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Riemannian dynamic generalized space quantization learning.
<em>PR</em>, <em>132</em>, 108932. (<a
href="https://doi.org/10.1016/j.patcog.2022.108932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing works represent signals by covariance matrices and then develop learning methods on the Riemannian symmetric positive-definite (SPD) manifold to deal with such data. However, they summarize each instance with a single covariance matrix, omitting some potential important information, such as the time evolution of the correlation in signals. In this paper, we represent each instance by a sequence of covariance matrices and develop a novel dynamic generalized learning Riemannian space quantization (DGLRSQ) method to deal with such data representations. The proposed DGLRSQ method incorporates short-term memory mechanism in generalized learning Riemannian space quantization (GLRSQ), which is an extension of Euclidean generalized learning vector quantization to deal with SPD matrix-valued data. The proposed method can capture the temporal evolution of the correlation in signals and thus provides better performance to its the counterpart – GLRSQ, which treats each instance as a signal covariance matrix. Empirical investigations on synthetic data and motor imagery EEG data show the superior performance of the proposed method.},
  archive      = {J_PR},
  author       = {MengLing Fan and Fengzhen Tang and Yinan Guo and Xingang Zhao},
  doi          = {10.1016/j.patcog.2022.108932},
  journal      = {Pattern Recognition},
  pages        = {108932},
  shortjournal = {Pattern Recognition},
  title        = {Riemannian dynamic generalized space quantization learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The familiarity hypothesis: Explaining the behavior of deep
open set methods. <em>PR</em>, <em>132</em>, 108931. (<a
href="https://doi.org/10.1016/j.patcog.2022.108931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such “novel category” objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning . Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. This distinction is important, because familiarity-based detection will fail in many situations where novelty is present. For example when an image contains both a novel object and a familiar one, the familiarity score will be high, so the novel object will not be noticed. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity-based detection is an inevitable consequence of representation learning .},
  archive      = {J_PR},
  author       = {Thomas G. Dietterich and Alex Guyer},
  doi          = {10.1016/j.patcog.2022.108931},
  journal      = {Pattern Recognition},
  pages        = {108931},
  shortjournal = {Pattern Recognition},
  title        = {The familiarity hypothesis: Explaining the behavior of deep open set methods},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ℓp-norm support vector data description. <em>PR</em>,
<em>132</em>, 108930. (<a
href="https://doi.org/10.1016/j.patcog.2022.108930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support vector data description (SVDD) approach serves as a de facto standard for one-class classification where the learning task entails inferring the smallest hyper-sphere to enclose target objects while linearly penalising the errors/slacks via an ℓ 1 ℓ1 -norm penalty term. In this study, we generalise this modelling formalism to a general ℓ p ℓp -norm ( p ≥ 1 p≥1 ) penalty function on slacks. By virtue of an ℓ p ℓp -norm function, in the primal space, the proposed approach enables formulating a non-linear cost for slacks. From a dual problem perspective, the proposed method introduces a dual norm into the objective function, thus, proving a controlling mechanism to tune into the intrinsic sparsity/uniformity of the problem for enhanced descriptive capability. A theoretical analysis based on Rademacher complexities characterises the generalisation performance of the proposed approach while the experimental results on several datasets confirm the merits of the proposed method compared to other alternatives.},
  archive      = {J_PR},
  author       = {Shervin Rahimzadeh Arashloo},
  doi          = {10.1016/j.patcog.2022.108930},
  journal      = {Pattern Recognition},
  pages        = {108930},
  shortjournal = {Pattern Recognition},
  title        = {ℓp-norm support vector data description},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrared and visible image fusion via parallel scene and
texture learning. <em>PR</em>, <em>132</em>, 108929. (<a
href="https://doi.org/10.1016/j.patcog.2022.108929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion plays a pivotal role in numerous high-level computer vision tasks. Existing deep learning-based image fusion methods usually leverage an implicit manner to achieve feature extraction, which would cause some characteristics of source images, e.g. , contrast and structural information, are unable to be fully extracted and integrated into the fused images. In this work, we propose an infrared and visible image fusion method via parallel scene and texture learning. Our key objective is to deploy two branches of deep neural networks , namely the content branch and detail branch, to synchronously extract different characteristics from source images and then reconstruct the fused image. The content branch focuses primarily on coarse-grained information and is deployed to estimate the global content of source images. The detail branch primarily pays attention to fine-grained information, and we design an omni-directional spatially variant recurrent neural networks in this branch to model the internal structure of source images more accurately and extract texture-related features in an explicit manner. Extensive experiments show that our approach achieves significant improvements over state-of-the-arts on qualitative and quantitative evaluations with comparatively less running time consumption. Meanwhile, we also demonstrate the superiority of our fused results in the object detection task. Our code is available at: https://github.com/Melon-Xu/PSTLFusion .},
  archive      = {J_PR},
  author       = {Meilong Xu and Linfeng Tang and Hao Zhang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2022.108929},
  journal      = {Pattern Recognition},
  pages        = {108929},
  shortjournal = {Pattern Recognition},
  title        = {Infrared and visible image fusion via parallel scene and texture learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metric learning via perturbing hard-to-classify instances.
<em>PR</em>, <em>132</em>, 108928. (<a
href="https://doi.org/10.1016/j.patcog.2022.108928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint selection is an effective means to alleviate the problem of a massive amount of constraints in metric learning. However, it is difficult to find and deal with all association constraints with the same hard-to-classify instance (i.e., an instance surrounded by dissimilar instances), negatively affecting metric learning algorithms. To address this problem, we propose a new metric learning algorithm from the perspective of selecting instances, Metric Learning via Perturbing of Hard-to-classify Instances (ML-PHI), which directly perturbs the hard-to-classify instances to reduce over-fitting for the hard-to-classify instances. ML-PHI perturbs hard-to-classify instances to be closer to similar instances while keeping the positions of the remaining instances as constant as possible. As a result, the negative impacts of hard-to-classify instances are effectively reduced. We have conducted extensive experiments on real data sets , and the results show that ML-PHI is effective and outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xinyao Guo and Wei Wei and Jianqing Liang and Chuangyin Dang and Jiye Liang},
  doi          = {10.1016/j.patcog.2022.108928},
  journal      = {Pattern Recognition},
  pages        = {108928},
  shortjournal = {Pattern Recognition},
  title        = {Metric learning via perturbing hard-to-classify instances},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BR-NPA: A non-parametric high-resolution attention model to
improve the interpretability of attention. <em>PR</em>, <em>132</em>,
108927. (<a href="https://doi.org/10.1016/j.patcog.2022.108927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of employing attention mechanisms has brought along concerns about the interpretability of attention distributions. Although it provides insights into how a model is operating, utilizing attention as the explanation of model predictions is still highly dubious. The community is still seeking more interpretable strategies for better identifying local active regions that contribute the most to the final decision. To improve the interpretability of existing attention models, we propose a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant human-interpretable information. The target model is first distilled to have higher-resolution intermediate feature maps. From which, representative features are then grouped based on local pairwise feature similarity, to produce finer-grained, more precise attention maps highlighting task-relevant parts of the input. The obtained attention maps are ranked according to the activity level of the compound feature, which provides information regarding the important level of the highlighted regions. The proposed model can be easily adapted in a wide variety of modern deep models, where classification is involved. Extensive quantitative and qualitative experiments showcase more comprehensive and accurate visual explanations compared to state-of-the-art attention models and visualization methods across multiple tasks including fine-grained image classification , few-shot classification, and person re-identification, without compromising the classification accuracy . The proposed visualization model sheds imperative light on how neural networks ‘pay their attention’ differently in different tasks.},
  archive      = {J_PR},
  author       = {Tristan Gomez and Suiyi Ling and Thomas Fréour and Harold Mouchère},
  doi          = {10.1016/j.patcog.2022.108927},
  journal      = {Pattern Recognition},
  pages        = {108927},
  shortjournal = {Pattern Recognition},
  title        = {BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SWIPENET: Object detection in noisy underwater scenes.
<em>PR</em>, <em>132</em>, 108926. (<a
href="https://doi.org/10.1016/j.patcog.2022.108926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a Sample-WeIghted hyPEr Network (SWIPENET), and a novel training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, inspired by the human education process that drives the learning from easy to hard concepts, we propose the noise-robust CMA training paradigm that learns the clean data first and then move on to learns the diverse noisy data. Experiments on four underwater object detection datasets show that the proposed SWIPENET+CMA framework achieves better or competitive accuracy in object detection against several state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Long Chen and Feixiang Zhou and Shengke Wang and Junyu Dong and Ning Li and Haiping Ma and Xin Wang and Huiyu Zhou},
  doi          = {10.1016/j.patcog.2022.108926},
  journal      = {Pattern Recognition},
  pages        = {108926},
  shortjournal = {Pattern Recognition},
  title        = {SWIPENET: Object detection in noisy underwater scenes},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Learning pseudo labels for semi-and-weakly supervised
semantic segmentation. <em>PR</em>, <em>132</em>, 108925. (<a
href="https://doi.org/10.1016/j.patcog.2022.108925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to tackle semi-and-weakly supervised semantic segmentation (SWSSS), where many image-level classification labels and a few pixel-level annotations are available. We believe the most crucial point for solving SWSSS is to produce high-quality pseudo labels, and our method deals with it from two perspectives. Firstly, we introduce a class-aware cross entropy (CCE) loss for network training. Compared to conventional cross entropy loss, CCE loss encourages the model to distinguish concurrent classes only and simplifies the learning target of pseudo label generation. Secondly, we propose a progressive cross training (PCT) method to build cross supervision between two networks with a dynamic evaluation mechanism, which progressively introduces high-quality predictions as additional supervision for network training. Our method significantly improves the quality of generated pseudo labels in the regime with extremely limited annotations. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods significantly. The code is released for public access 1 .},
  archive      = {J_PR},
  author       = {Yude Wang and Jie Zhang and Meina Kan and Shiguang Shan},
  doi          = {10.1016/j.patcog.2022.108925},
  journal      = {Pattern Recognition},
  pages        = {108925},
  shortjournal = {Pattern Recognition},
  title        = {Learning pseudo labels for semi-and-weakly supervised semantic segmentation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust convolutional neural networks against adversarial
attacks on medical images. <em>PR</em>, <em>132</em>, 108923. (<a
href="https://doi.org/10.1016/j.patcog.2022.108923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90\% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.},
  archive      = {J_PR},
  author       = {Xiaoshuang Shi and Yifan Peng and Qingyu Chen and Tiarnan Keenan and Alisa T. Thavikulwat and Sungwon Lee and Yuxing Tang and Emily Y. Chew and Ronald M. Summers and Zhiyong Lu},
  doi          = {10.1016/j.patcog.2022.108923},
  journal      = {Pattern Recognition},
  pages        = {108923},
  shortjournal = {Pattern Recognition},
  title        = {Robust convolutional neural networks against adversarial attacks on medical images},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel dependence regularizers and gaussian processes with
applications to algorithmic fairness. <em>PR</em>, <em>132</em>, 108922.
(<a href="https://doi.org/10.1016/j.patcog.2022.108922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. We also provide statistical consistency results for both risk and fairness bound for our approach. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process (GP) prior. As a consequence, a GP model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints . Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.},
  archive      = {J_PR},
  author       = {Zhu Li and Adrián Pérez-Suay and Gustau Camps-Valls and Dino Sejdinovic},
  doi          = {10.1016/j.patcog.2022.108922},
  journal      = {Pattern Recognition},
  pages        = {108922},
  shortjournal = {Pattern Recognition},
  title        = {Kernel dependence regularizers and gaussian processes with applications to algorithmic fairness},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privileged multi-task learning for attribute-aware aesthetic
assessment. <em>PR</em>, <em>132</em>, 108921. (<a
href="https://doi.org/10.1016/j.patcog.2022.108921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photo’s aesthetic quality . However, the aesthetic attributes have not been largely and sufficiently exploited for photo aesthetic assessment. In this paper, we propose a novel approach to photo aesthetic assessment with the help of aesthetic attributes. The aesthetic attributes are used as privileged information (PI), which is often available during training phase but unavailable in prediction phase due to the high collection expense. The proposed framework consists of a deep multi-task network as generator and a fully connected network as discriminator . Deep multi-task network learns the aesthetic attributes and score simultaneously to capture their dependencies and extract better feature representations. Specifically, we use ranking constraint in the label space, similarity constraint and prior probabilities loss in the privileged information space to make the output of multi-task network converge to that of ground truth. Adversarial loss is used to identify and distinguish the predicted privileged information of a deep multi-task network from the ground truth PI distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state-of-the-art.},
  archive      = {J_PR},
  author       = {Yangyang Shu and Qian Li and Lingqiao Liu and Guandong Xu},
  doi          = {10.1016/j.patcog.2022.108921},
  journal      = {Pattern Recognition},
  pages        = {108921},
  shortjournal = {Pattern Recognition},
  title        = {Privileged multi-task learning for attribute-aware aesthetic assessment},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local-to-global support vector machines (LGSVMs).
<em>PR</em>, <em>132</em>, 108920. (<a
href="https://doi.org/10.1016/j.patcog.2022.108920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For supervised classification tasks that involve a large number of instances, we propose and study a new efficient tool, namely the Local-to-Global Support Vector Machine (LGSVM) method. Its background somehow lies in the framework of approximation theory and of local kernel-based models, such as the Partition of Unity (PU) method. Indeed, even if the latter needs to be accurately tailored for classification tasks, such as allowing the use of the cosine semi-metric for defining the patches, the LGSVM is a global method constructed by gluing together the local SVM contributions via compactly supported weights. When the number of instances grows, such a construction of a global classifier enables us to significantly reduce the usually high complexity cost of SVMs. This claim is supported by a theoretical analysis of the LGSVM and of its complexity as well as by extensive numerical experiments carried out by considering benchmark datasets.},
  archive      = {J_PR},
  author       = {F. Marchetti and E. Perracchione},
  doi          = {10.1016/j.patcog.2022.108920},
  journal      = {Pattern Recognition},
  pages        = {108920},
  shortjournal = {Pattern Recognition},
  title        = {Local-to-global support vector machines (LGSVMs)},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning of longitudinal mammogram examinations for
breast cancer risk prediction. <em>PR</em>, <em>132</em>, 108919. (<a
href="https://doi.org/10.1016/j.patcog.2022.108919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information in digital mammogram images has been shown to be associated with the risk of developing breast cancer. Longitudinal breast cancer screening mammogram examinations may carry spatiotemporal information that can enhance breast cancer risk prediction. No deep learning models have been designed to capture such spatiotemporal information over multiple examinations to predict the risk. In this study, we propose a novel deep learning structure, LRP-NET, to capture the spatiotemporal changes of breast tissue over multiple negative/benign screening mammogram examinations to predict near-term breast cancer risk in a case-control setting. Specifically, LRP-NET is designed based on clinical knowledge to capture the imaging changes of bilateral breast tissue over four sequential mammogram examinations. We evaluate our proposed model with two ablation studies and compare it to three models/settings, including 1) a “loose” model without explicitly capturing the spatiotemporal changes over longitudinal examinations, 2) LRP-NET but using a varying number (i.e., 1 and 3) of sequential examinations, and 3) a previous model that uses only a single mammogram examination. On a case-control cohort of 200 patients, each with four examinations, our experiments on a total of 3200 images show that the LRP-NET model outperforms the compared models/settings.},
  archive      = {J_PR},
  author       = {Saba Dadsetan and Dooman Arefan and Wendie A. Berg and Margarita L. Zuley and Jules H. Sumkin and Shandong Wu},
  doi          = {10.1016/j.patcog.2022.108919},
  journal      = {Pattern Recognition},
  pages        = {108919},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning of longitudinal mammogram examinations for breast cancer risk prediction},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation with progressive adaptation
of subspaces. <em>PR</em>, <em>132</em>, 108918. (<a
href="https://doi.org/10.1016/j.patcog.2022.108918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Specifically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.},
  archive      = {J_PR},
  author       = {Weikai Li and Songcan Chen},
  doi          = {10.1016/j.patcog.2022.108918},
  journal      = {Pattern Recognition},
  pages        = {108918},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation with progressive adaptation of subspaces},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BSCA-net: Bit slicing context attention network for polyp
segmentation. <em>PR</em>, <em>132</em>, 108917. (<a
href="https://doi.org/10.1016/j.patcog.2022.108917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Bit-Slicing Context Attention Network (BSCA-Net), an end-to-end network, to improve the extraction ability of boundary information for polyp segmentation . The core of BSCA-Net is a new Bit Slice Context Attention (BSCA) module, which exploits the bit-plane slicing information to effectively extract the boundary information between polyps and the surrounding tissue. In addition, we design a novel Split-Squeeze-Bottleneck-Union (SSBU) module, to exploit the geometrical information from different aspects. Also, based on SSBU, we propose an multipath concat attention decoder (MCAD) and an multipath attention concat encoder (MACE), to further improve the network performance for polyp segmentation. Finally, by combining BSCA, SSBU, MCAD and MACE, the proposed BSCA-Net is able to effectively suppress noises in feature maps, and simultaneously improve the ability of feature expression in different levels, for polyp segmentation. Empirical experiments on five benchmark datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300) demonstrate the superior of the proposed BSCA-Net over existing cutting-edge methods.},
  archive      = {J_PR},
  author       = {Yi Lin and Jichun Wu and Guobao Xiao and Junwen Guo and Geng Chen and Jiayi Ma},
  doi          = {10.1016/j.patcog.2022.108917},
  journal      = {Pattern Recognition},
  pages        = {108917},
  shortjournal = {Pattern Recognition},
  title        = {BSCA-net: Bit slicing context attention network for polyp segmentation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-feature sparse similar representation for person
identification. <em>PR</em>, <em>132</em>, 108916. (<a
href="https://doi.org/10.1016/j.patcog.2022.108916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person identification with a single feature (e.g., face recognition, speaker verification, person re-identification, etc.) has been studied extensively for many years, while few works focus on multi-feature person identification. Though promising performance has been achieved by only using the information of facial images , voice, or pedestrian appearance, it is still challenging to recognize a person with only a single feature in some situations (e.g., a person at a distance or occluded by other objects, and a partial person out of view). In this paper, we present a multi-feature sparse similar representation (MFSSR) method to effectively fuse face features, body features, and global image features for the task of person identification. In MFSSR, we designed a reconstructed deep spatial feature for representing the appearance of human body by using the spatial correlation coding of partial deep spatial features. Then we presented a multi-feature sparse similar representation model for jointly using different features, e.g., face, body, and the global image. Besides, considering that the coding coefficients associated with good samples but not outliers should be more similar among different features, we jointly represent different features by imposing a weighted ℓ 1 ℓ1 -norm distance regularization , instead of the conventional ℓ 2 ℓ2 -norm regularization, on the coefficients. Experimental results on several multi-feature person identification databases have clearly shown the superior performance of the proposed model.},
  archive      = {J_PR},
  author       = {Meng Yang and Lei Liao and Kangyin Ke and Guangwei Gao},
  doi          = {10.1016/j.patcog.2022.108916},
  journal      = {Pattern Recognition},
  pages        = {108916},
  shortjournal = {Pattern Recognition},
  title        = {Multi-feature sparse similar representation for person identification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Corrigendum to “LiDAR-based localization using universal
encoding and memory-aware regression” pattern recognition volume 128
(2022) 108685. <em>PR</em>, <em>132</em>, 108915. (<a
href="https://doi.org/10.1016/j.patcog.2022.108915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Shangshu Yu and Cheng Wang and Chenglu Wen and Ming Cheng and Minghao Liu and Zhihong Zhang and Xin Li},
  doi          = {10.1016/j.patcog.2022.108915},
  journal      = {Pattern Recognition},
  pages        = {108915},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “LiDAR-based localization using universal encoding and memory-aware regression” pattern recognition volume 128 (2022) 108685},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted 3D volume reconstruction from series of slice data
using a modified allen–cahn equation. <em>PR</em>, <em>132</em>, 108914.
(<a href="https://doi.org/10.1016/j.patcog.2022.108914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop a fast and accurate computational method for a weighted three-dimensional (3D) volume reconstruction from a series of slice data using a phase-field model. The proposed method is based on a modified Allen–Cahn (AC) equation with a fidelity term. The algorithm automatically generates the necessary slices between the given slices by solving the governing equation. To reconstruct a 3D volume, we first set a source slice and target slice. Next, we set the source slice as the initial condition and the target slice as the fidelity function. Finally, we retain the numerical solutions during an evolution as intermediate slices between the source and target slices. There are two criteria for choosing the intermediate slice: One is based on the area of the symmetric difference between the phase-field solution and the target and the other is based on the change of the phase-field solution relative to the area of the target. We use the weighted average of the two criteria. To validate the efficiency and accuracy of the proposed numerical algorithm, several computational experiments are conducted. Computational test results confirm the superior performance of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Yibao Li and Xin Song and Soobin Kwak and Junseok Kim},
  doi          = {10.1016/j.patcog.2022.108914},
  journal      = {Pattern Recognition},
  pages        = {108914},
  shortjournal = {Pattern Recognition},
  title        = {Weighted 3D volume reconstruction from series of slice data using a modified Allen–Cahn equation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Score-oriented loss (SOL) functions. <em>PR</em>,
<em>132</em>, 108913. (<a
href="https://doi.org/10.1016/j.patcog.2022.108913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss functions engineering and the assessment of prediction performances are two crucial and intertwined aspects of supervised machine learning . This paper focuses on binary classification to introduce a class of loss functions that are defined on probabilistic confusion matrices and that allow an automatic and a priori maximization of the skill scores. These loss functions are tested in various classification experiments, which show that the probability distribution function associated with the confusion matrices significantly impacts the outcome of the score maximization process, and that the proposed functions are competitive with other state-of-the-art probabilistic losses.},
  archive      = {J_PR},
  author       = {F. Marchetti and S. Guastavino and M. Piana and C. Campi},
  doi          = {10.1016/j.patcog.2022.108913},
  journal      = {Pattern Recognition},
  pages        = {108913},
  shortjournal = {Pattern Recognition},
  title        = {Score-oriented loss (SOL) functions},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wrapper feature selection method based differential
evolution and extreme learning machine for intrusion detection system.
<em>PR</em>, <em>132</em>, 108912. (<a
href="https://doi.org/10.1016/j.patcog.2022.108912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intrusion detection system (IDS) has gained a rapid increase of interest due to its widely recognized potential in various security fields, however, it suffers from several challenges. Different network datasets have several redundant and irrelevant features that affect the decision of the IDS classifier. Therefore, it is essential to decrease these features to improve the system performance. In this paper, an efficient wrapper feature selection method is proposed for improving the performance and decreasing the processing time of the IDS. The proposed approach employs a differential evaluation algorithm to select the useful features whilst the extreme learning machine classifier is applied after feature selection to evaluate the selected features. Many experiments are performed using the full NSL-KDD dataset to evaluate the performance of the proposed method. The results prove that the proposed approach can efficiently reduce the features, increase the accuracy, reduce the false alarm rates, and improve the processing time of the IDS in comparison to other recent related works.},
  archive      = {J_PR},
  author       = {Wathiq Laftah Al-Yaseen and Ali Kadhum Idrees and Faezah Hamad Almasoudy},
  doi          = {10.1016/j.patcog.2022.108912},
  journal      = {Pattern Recognition},
  pages        = {108912},
  shortjournal = {Pattern Recognition},
  title        = {Wrapper feature selection method based differential evolution and extreme learning machine for intrusion detection system},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning intra-domain style-invariant representation for
unsupervised domain adaptation of semantic segmentation. <em>PR</em>,
<em>132</em>, 108911. (<a
href="https://doi.org/10.1016/j.patcog.2022.108911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to tackle the problem of unsupervised domain adaptation (UDA) of semantic segmentation and improve the UDA performance with a novel conception of learning intra-domain style-invariant representation. Previous UDA methods focused on reducing the inter-domain inconsistency between the source domain and the target domain. However, due to the different data distributions of the two domains, reducing the inter-domain inconsistency cannot ensure the generalization ability of the trained model in the target domain. Therefore, to improve the UDA performance, we take into consideration the intra-domain diversity of the target domain for the first time in studies on UDA and aim to train the model to generalize well to the diverse intra-domain styles. To achieve this, we propose a self-ensembling method to learn the intra-domain style-invariant representation and we introduce a semantic-aware multimodal image-to-image translation model to obtain images with diversified intra-domain styles. Our method achieves state-of-the-art performance on two synthetic-to-real adaptation benchmarks, and we demonstrate the effectiveness of our method by conducting extensive experiments.},
  archive      = {J_PR},
  author       = {Zongyao Li and Ren Togo and Takahiro Ogawa and Miki Haseyama},
  doi          = {10.1016/j.patcog.2022.108911},
  journal      = {Pattern Recognition},
  pages        = {108911},
  shortjournal = {Pattern Recognition},
  title        = {Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tree-based data augmentation and mutual learning for offline
handwritten mathematical expression recognition. <em>PR</em>,
<em>132</em>, 108910. (<a
href="https://doi.org/10.1016/j.patcog.2022.108910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, thanks to the successful application of the attention-based encoder-decoder framework, handwritten mathematical expression recognition (HMER) has achieved significant improvement. However, HMER is still a challenging task in the handwriting recognition area, which suffers from the ambiguity of handwritten symbols, the two-dimensional structure of mathematical expressions, and the lack of labeled data. In this paper, we attempt to improve the recognition performance and generalization ability of the existing state-of-the-art method from two perspectives: data augmentation and model design. We first propose a tree-based multi-level (including symbol level, sub-expression level, and image level) data augmentation strategy, which can generate many synthetic images . Then, we present a novel encoder-decoder hybrid model via tree-based mutual learning to fully utilize the complementarity between tree decoder and string decoder. Benefitting from our data augmentation strategy, we achieve 58.47\%/57.82\%/62.67\% and 74.45\% expression recognition accuracy respectively on the CROHME14/16/19 competition datasets and the OffRaSHME20 competition dataset. Moreover, tree-based data augmentation is a key technology to our champion system for the OffRaSHME20 competition. Our tree-based mutual learning method further improves the recognition accuracy to 61.63\%/59.81\%/64.38\% and 75.68\% on these datasets. Further quantitative and qualitative analyses also demonstrate the effectiveness and robustness of our proposed methods.},
  archive      = {J_PR},
  author       = {Chen Yang and Jun Du and Jianshu Zhang and Changjie Wu and Mingjun Chen and JiaJia Wu},
  doi          = {10.1016/j.patcog.2022.108910},
  journal      = {Pattern Recognition},
  pages        = {108910},
  shortjournal = {Pattern Recognition},
  title        = {Tree-based data augmentation and mutual learning for offline handwritten mathematical expression recognition},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint operation and attention block search for lightweight
image restoration. <em>PR</em>, <em>132</em>, 108909. (<a
href="https://doi.org/10.1016/j.patcog.2022.108909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, block-based design methods have shown effectiveness in image restoration tasks, which are usually designed in a handcrafted manner and have computation and memory consumption challenges in practice. In this paper, we propose a joint operation and attention block search algorithm for image restoration, which focuses on searching for optimal combinations of operation blocks and attention blocks. Specifically, we first construct two search spaces: operation block search space and attention block search space. The former is used to explore the suitable operation of each layer and aims to construct a lightweight and effective operation search module (OSM). The latter is applied to discover the optimal connection of various attention mechanisms and aims to enhance the feature expression. The searched structure is called the attention search module (ASM). Then we combine OSM and ASM to construct a joint search module (JSM), which serves as the basic module to build the final network. Moreover, we propose a cross-scale fusion module (CSFM) to effectively integrate multiple hierarchical features from JSMs, which helps to mine feature corrections of intermediate layers. Extensive experiments on image super-resolution, gray image denoising , and JPEG image deblocking tasks demonstrate that our proposed network can achieve competitive performance. The source code is available on https://github.com/it-hao/JSNet .},
  archive      = {J_PR},
  author       = {Hao Shen and Zhong-Qiu Zhao and Wenrui Liao and Weidong Tian and De-Shuang Huang},
  doi          = {10.1016/j.patcog.2022.108909},
  journal      = {Pattern Recognition},
  pages        = {108909},
  shortjournal = {Pattern Recognition},
  title        = {Joint operation and attention block search for lightweight image restoration},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-19 contact tracking by group activity trajectory
recovery over camera networks. <em>PR</em>, <em>132</em>, 108908. (<a
href="https://doi.org/10.1016/j.patcog.2022.108908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact tracking plays an important role in the epidemiological investigation of COVID-19, which can effectively reduce the spread of the epidemic. As an excellent alternative method for contact tracking, mobile phone location-based methods are widely used for locating and tracking contacts. However, current inaccurate positioning algorithms that are widely used in contact tracking lead to the inaccurate follow-up of contacts. Aiming to achieve accurate contact tracking for the COVID-19 contact group, we extend the analysis of the GPS data to combine GPS data with video surveillance data and address a novel task named group activity trajectory recovery. Meanwhile, a new dataset called GATR-GPS is constructed to simulate a realistic scenario of COVID-19 contact tracking, and a coordinated optimization algorithm with a spatio-temporal constraint table is further proposed to realize efficient trajectory recovery of pedestrian trajectories. Extensive experiments on the novel collected dataset and commonly used two existing person re-identification datasets are performed, and the results evidently demonstrate that our method achieves competitive results compared to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chao Wang and XiaoChen Wang and Zhongyuan Wang and WenQian Zhu and Ruimin Hu},
  doi          = {10.1016/j.patcog.2022.108908},
  journal      = {Pattern Recognition},
  pages        = {108908},
  shortjournal = {Pattern Recognition},
  title        = {COVID-19 contact tracking by group activity trajectory recovery over camera networks},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-criteria selection of rehearsal samples for continual
learning. <em>PR</em>, <em>132</em>, 108907. (<a
href="https://doi.org/10.1016/j.patcog.2022.108907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retaining a small subset to replay is a direct and effective way to prevent catastrophic forgetting in continual learning. However, due to data complexity and restricted memory, picking a proper subset for rehearsal is challenging and still being explored. In this work, we present a Multi-criteria Subset Selection approach that can stabilize and advance replay-based continual learning. The method picks rehearsal samples by integrating multiple criteria, including distance to prototype, intra-class cluster variation, and classifier loss. By doing so, it maximizes the comprehensive representation power of the sampled subset by ensuring its representativeness, diversity, and discriminability . We empirically find that singular criteria are likely to fail in particular tasks, while multi-criteria minimizes this risk and stabilizes task training throughout the continual learning process. Moreover, our method improves replay-based methods consistently and achieves state-of-the-art performance on both CIFAR100 and Tiny-Imagenet datasets.},
  archive      = {J_PR},
  author       = {Chen Zhuang and Shaoli Huang and Gong Cheng and Jifeng Ning},
  doi          = {10.1016/j.patcog.2022.108907},
  journal      = {Pattern Recognition},
  pages        = {108907},
  shortjournal = {Pattern Recognition},
  title        = {Multi-criteria selection of rehearsal samples for continual learning},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature nonlinear transformation non-negative matrix
factorization with kullback-leibler divergence. <em>PR</em>,
<em>132</em>, 108906. (<a
href="https://doi.org/10.1016/j.patcog.2022.108906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence (FNTNMF-KLD) for extracting the nonlinear features of a matrix in standard NMF. This method uses a nonlinear transformation to act on the feature matrix for constructing a NMF model based on the objective function of Kullback-Leibler Divergence, and the Taylor series expansion and the Newton iteration formula of solving root are used to obtain the iterative update rules of the basis matrix and the feature matrix. Experimental results show that the proposed method obtains the nonlinear features of data matrix in a more efficient way. In object recognition and clustering tasks, better accuracy can be achieved over some typical NMF methods.},
  archive      = {J_PR},
  author       = {Lirui Hu and Ning Wu and Xiao Li},
  doi          = {10.1016/j.patcog.2022.108906},
  journal      = {Pattern Recognition},
  pages        = {108906},
  shortjournal = {Pattern Recognition},
  title        = {Feature nonlinear transformation non-negative matrix factorization with kullback-leibler divergence},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple geometry representations for 6D object pose
estimation in occluded or truncated scenes. <em>PR</em>, <em>132</em>,
108903. (<a href="https://doi.org/10.1016/j.patcog.2022.108903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based 6D object pose estimation methods from a single RGBD image have recently received increasing attention because of their powerful representation learning capabilities. These methods, however, cannot handle severe occlusion and truncation. In this paper, we present a novel 6D object pose estimation method based on multiple geometry representations. Specifically, we introduce a network to fuse the appearance and geometry features extracted from input color and depth images. Then, we utilize these per-point fusion features to estimate keypoint offsets, edge vectors, and dense symmetry correspondences in the canonical coordinate system. Finally, a two-stage pose regression module is applied to compute the 6D pose of an object. Relative to the unitary 3D keypoint-based strategy, such combination of multiple geometry representations provides sufficient and diverse information, especially for occluded or truncated scenes. To show the robustness to occlusion and truncation of the proposed method, we conduct comparative experiments on the Occlusion LineMOD, Truncation LineMOD, and T-LESS datasets. Results reveal that the proposed method outperforms state-of-the-art techniques by a large margin.},
  archive      = {J_PR},
  author       = {Jichun Wang and Lemiao Qiu and Guodong Yi and Shuyou Zhang and Yang Wang},
  doi          = {10.1016/j.patcog.2022.108903},
  journal      = {Pattern Recognition},
  pages        = {108903},
  shortjournal = {Pattern Recognition},
  title        = {Multiple geometry representations for 6D object pose estimation in occluded or truncated scenes},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale-selective and noise-robust extended local binary
pattern for texture classification. <em>PR</em>, <em>132</em>, 108901.
(<a href="https://doi.org/10.1016/j.patcog.2022.108901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most successful local feature descriptors, the local binary pattern (LBP) estimates the texture distribution rule of an image based on the signs of differences between neighboring pixels to obtain intensity- and rotation- invariance. In this paper, we propose a novel image descriptor to address scale transformation and noise interference simultaneously. We name it scale-selective and noise-robust extended LBP (SNELBP). First, each image in training sets is transformed into different scale spaces by a Gaussian filter. Second, noise-robust pattern histograms are obtained from each scale space by using our previously proposed median robust extended LBP (MRELBP). Then, scale-invariant histograms are determined by selecting the maximum among all scale levels for a certain image. Finally, the most informative patterns are selected from the dictionary pretrained by the two-stage compact dominant feature selection method (CDFS), maintaining the descriptor more lightweight with sufficiently low time cost. Extensive experiments on five public databases (Outex_TC_00011, TC_00012, KTH-TIPS, UMD and NEU) and one fresh texture database (JoJo) under two kinds of interferences (Gaussian and salt pepper) indicate that our SNELBP yields more competitive results than thirty classical LPB variants as well as eight typical deep learning methods.},
  archive      = {J_PR},
  author       = {Qiwu Luo and Jiaojiao Su and Chunhua Yang and Olli Silven and Li Liu},
  doi          = {10.1016/j.patcog.2022.108901},
  journal      = {Pattern Recognition},
  pages        = {108901},
  shortjournal = {Pattern Recognition},
  title        = {Scale-selective and noise-robust extended local binary pattern for texture classification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explanation vs. Attention: A two-player game to obtain
attention for VQA and visual dialog. <em>PR</em>, <em>132</em>, 108898.
(<a href="https://doi.org/10.1016/j.patcog.2022.108898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. A generalization of the work is also provided by extending our approach to the task of ‘Visual Dialog’ where the attention is more contextual. Thorough evaluation for this task is also provided. Visualization of the results confirms our hypothesis that attention maps improve using the proposed form of supervision.},
  archive      = {J_PR},
  author       = {Badri N. Patro and Anupriy and Vinay P. Namboodiri},
  doi          = {10.1016/j.patcog.2022.108898},
  journal      = {Pattern Recognition},
  pages        = {108898},
  shortjournal = {Pattern Recognition},
  title        = {Explanation vs. attention: A two-player game to obtain attention for VQA and visual dialog},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional motion in-betweening. <em>PR</em>, <em>132</em>,
108894. (<a href="https://doi.org/10.1016/j.patcog.2022.108894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability. Our code and results are available on our project web page: https://jihoonerd.github.io/Conditional-Motion-In-Betweening .},
  archive      = {J_PR},
  author       = {Jihoon Kim and Taehyun Byun and Seungyoun Shin and Jungdam Won and Sungjoon Choi},
  doi          = {10.1016/j.patcog.2022.108894},
  journal      = {Pattern Recognition},
  pages        = {108894},
  shortjournal = {Pattern Recognition},
  title        = {Conditional motion in-betweening},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighting and pruning based ensemble deep random vector
functional link network for tabular data classification. <em>PR</em>,
<em>132</em>, 108879. (<a
href="https://doi.org/10.1016/j.patcog.2022.108879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first integrate normalization to the Ensemble Deep Random Vector Functional Link network (edRVFL). This re-normalization step can help the network avoid divergence of the hidden features. Then, we propose novel variants of the edRVFL network. Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensemble’s diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art classification methods on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.},
  archive      = {J_PR},
  author       = {Qiushi Shi and Minghui Hu and Ponnuthurai Nagaratnam Suganthan and Rakesh Katuwal},
  doi          = {10.1016/j.patcog.2022.108879},
  journal      = {Pattern Recognition},
  pages        = {108879},
  shortjournal = {Pattern Recognition},
  title        = {Weighting and pruning based ensemble deep random vector functional link network for tabular data classification},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning deep feature correspondence for unsupervised
anomaly detection and segmentation. <em>PR</em>, <em>132</em>, 108874.
(<a href="https://doi.org/10.1016/j.patcog.2022.108874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing machine learning models that can detect and localize the unexpected or anomalous structures within images is very important for numerous computer vision tasks, such as the defect inspection of manufactured products. However, it is challenging especially when there are few or even no anomalous image samples available. In this paper, we propose an unsupervised mechanism, i.e. deep feature correspondence (DFC), which can be effectively leveraged to detect and segment out the anomalies in images solely with the prior knowledge from anomaly-free samples. We develop our DFC in an asymmetric dual network framework that consists of a generic feature extraction network and an elaborated feature estimation network, and detect the possible anomalies within images by modeling and evaluating the associated deep feature correspondence between the two dual network branches. Furthermore, to improve the robustness of the DFC and further boost the detection performance, we specifically propose a self-feature enhancement (SFE) strategy and a multi-context residual learning (MCRL) network module. Extensive experiments have been carried out to validate the effectiveness of our DFC and the proposed SFE and MCRL. Our approach is very effective for detecting and segmenting the anomalies that appear in confined local regions of images, especially the industrial anomalies. It advances the state-of-the-art performances on the benchmark dataset – MVTec AD. Besides, when applied to a real industrial inspection scene, it outperforms the comparatives by a large margin.},
  archive      = {J_PR},
  author       = {Jie Yang and Yong Shi and Zhiquan Qi},
  doi          = {10.1016/j.patcog.2022.108874},
  journal      = {Pattern Recognition},
  pages        = {108874},
  shortjournal = {Pattern Recognition},
  title        = {Learning deep feature correspondence for unsupervised anomaly detection and segmentation},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multimodal relationship interaction for visual
relationship detection. <em>PR</em>, <em>132</em>, 108848. (<a
href="https://doi.org/10.1016/j.patcog.2022.108848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual relationship detection aims to recognize visual relationships in scenes as triplets 〈 〈 subject-predicate-object 〉 〉 . Previous works have shown remarkable progress by introducing multimodal features, external linguistics, scene context, etc. Due to the loss of informative multimodal hyper-relations ( i.e. relations of relationships), the meaningful contexts of relationships are not fully captured yet, which limits the reasoning ability. In this work, we propose a Multimodal Similarity Guided Relationship Interaction Network (MSGRIN) to explicitly model the relations of relationships in graph neural network paradigm. In a visual scene, the MSGRIN takes the visual relationships as nodes to construct an adaptive graph and enhances deep message passing by introducing Entity Appearance Reconstruction, Entity Relevance Filtering and Multimodal Similarity Attention. We have conducted extensive experiments on two datasets: Visual Relationship Detection (VRD) and Visual Genome (VG). The evaluation results demonstrate that the proposed MSGRIN has empirically performed more effectively overall.},
  archive      = {J_PR},
  author       = {Zhixuan Liu and Wei-Shi Zheng},
  doi          = {10.1016/j.patcog.2022.108848},
  journal      = {Pattern Recognition},
  pages        = {108848},
  shortjournal = {Pattern Recognition},
  title        = {Learning multimodal relationship interaction for visual relationship detection},
  volume       = {132},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein approximate bayesian computation for visual
tracking. <em>PR</em>, <em>131</em>, 108905. (<a
href="https://doi.org/10.1016/j.patcog.2022.108905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present novel visual tracking methods based on the Wasserstein approximate Bayesian computation (ABC). For visual tracking, the proposed Wasserstein ABC (WABC) method approximates the likelihood within the Wasserstein space more accurately than the conventional ABC methods by directly measuring the discrepancy between the likelihood distributions. To encode the temporal dependency among time-series likelihood distributions, we extend the WABC method to the time-series WABC (TWABC) method. Subsequently, the proposed Hilbert TWABC (HTWABC) method reduces the computational costs caused by the TWABC method while substituting the original Wasserstein distance with the Hilbert distance. Experimental results demonstrate that the proposed visual trackers outperform other state-of-the-art visual tracking methods quantitatively. Moreover, ablation studies verify the effectiveness of individual components consisting of the proposed method ( e.g., the Wasserstein distance, curve matching, and Hilbert metric).},
  archive      = {J_PR},
  author       = {Jinhee Park and Junseok Kwon},
  doi          = {10.1016/j.patcog.2022.108905},
  journal      = {Pattern Recognition},
  pages        = {108905},
  shortjournal = {Pattern Recognition},
  title        = {Wasserstein approximate bayesian computation for visual tracking},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective full-scale detection for salient object based on
condensing-and-filtering network. <em>PR</em>, <em>131</em>, 108904. (<a
href="https://doi.org/10.1016/j.patcog.2022.108904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning , salient object detection methods have made great progress. However, there are still two challenges: 1) The lack of rich features extracted from multiple perspectives at different encoder levels results in the omission of salient objects with varying scales. 2) The ineffective fusion of multi-level features during decoding dilutes the saliency features, which destroys the purity of the predicted maps. In this paper, we design a Condensing-and-Filtering Network (CFNet), in which a saliency pyramid condensing module (SPCM) and a saliency filtering module (SFM) are proposed to solve the above two problems respectively. Specifically, SPCM introduces pyramid convolution as the basic unit to condense full-scale features from global and local perspectives at each level of the encoder. SFM is equipped with an ingenious ‘funnel’ structure to effectively filter multi-level features and supplement details, which makes the fusion of features more robust. The two modules complement each other, so that the full-scale features can be used effectively to predict salient objects. Extensive experimental results on five benchmark datasets demonstrate that our method performs favourably against the state-of-the-art approaches, and also shows superiority in terms of speed (16.18ms) and FLOPs (21.19G). Meanwhile, we extend our CFNet to the task of RGB-D salient object detection and achieve better results, which further demonstrate its effectiveness. The code will be made available.},
  archive      = {J_PR},
  author       = {Xinyu Yan and Meijun Sun and Yahong Han and Zheng Wang and Qi Tian},
  doi          = {10.1016/j.patcog.2022.108904},
  journal      = {Pattern Recognition},
  pages        = {108904},
  shortjournal = {Pattern Recognition},
  title        = {Effective full-scale detection for salient object based on condensing-and-filtering network},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cubic-cross convolutional attention and count prior
embedding for smoke segmentation. <em>PR</em>, <em>131</em>, 108902. (<a
href="https://doi.org/10.1016/j.patcog.2022.108902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very challenging to accurately segment smoke images because smoke has some adverse properties, such as semi-transparency and blurry boundary. Aiming at solving these problems, we first fuse convolutional results along different axes to equivalently produce a cubic-cross convolutional kernel, which enlarges receptive fields at affordable computational costs for capturing long-range dependency of smoke pixels, and then we propose a Cubic-cross Convolutional Attention (CCA). To embed global category information, we propose a count prior structure to model and supervise the count of smoke pixels. To ensure the network can correctly extract a count prior map, we impose a regression loss on the count prior map and corresponding ideal count map directly calculated from its ground truth. Then we multiply the reshaped input by the count prior map to produce a Count Prior Attention (CPA) map, which is upsampled to generate the final output. A cross entropy loss is used to supervise the final segmentation. Finally, we use ResNet50 for feature encoding, and stack CCA and CPA together to propose a Cubic-cross convolutional attention and Count prior Embedding Network (CCENet) for smoke segmentation. Experiments on both synthetic and real smoke datasets show that our method outperforms existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Feiniu Yuan and Zeshu Dong and Lin Zhang and Xue Xia and Jinting Shi},
  doi          = {10.1016/j.patcog.2022.108902},
  journal      = {Pattern Recognition},
  pages        = {108902},
  shortjournal = {Pattern Recognition},
  title        = {Cubic-cross convolutional attention and count prior embedding for smoke segmentation},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Adaptive weighted guided image filtering for depth
enhancement in shape-from-focus. <em>PR</em>, <em>131</em>, 108900. (<a
href="https://doi.org/10.1016/j.patcog.2022.108900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map estimated by the traditional SFF into base and detail layers. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of our algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details w.r.t. existing methods.},
  archive      = {J_PR},
  author       = {Yuwen Li and Zhengguo Li and Chaobing Zheng and Shiqian Wu},
  doi          = {10.1016/j.patcog.2022.108900},
  journal      = {Pattern Recognition},
  pages        = {108900},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive weighted guided image filtering for depth enhancement in shape-from-focus},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimension-aware attention for efficient mobile networks.
<em>PR</em>, <em>131</em>, 108899. (<a
href="https://doi.org/10.1016/j.patcog.2022.108899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, attention mechanisms have shown great potential in improving the performance of mobile networks. Typically, they involve 2D symmetric convolution operations or generate 2D attention maps. However, such manners usually introduce high computational cost and large memory consumption, increasing the computational burden of mobile networks. To address this problem, we propose a novel lightweight attention mechanism, called Dimension-Aware Attention (DAA) block, by modeling the intra-dependencies of each dimension of the input feature map . Specifically, we factorize the channel and spatial attention by three parallel feature vector encoding branches, where stacked 1D asymmetric convolution operations can be naturally leveraged to capture large receptive fields. In this way, channel-aware, horizontal-aware, and vertical-aware attention vectors are extracted to effectively encode multi-dimensional information and greatly reduce the computational complexity of mobile networks. Experiments on multiple vision tasks demonstrate that our DAA block achieves better accuracy against state-of-the-art attention mechanisms with much lower computational operations. Our code is available at https://github.com/rymo96/DAANet .},
  archive      = {J_PR},
  author       = {Rongyun Mo and Shenqi Lai and Yan Yan and Zhenhua Chai and Xiaolin Wei},
  doi          = {10.1016/j.patcog.2022.108899},
  journal      = {Pattern Recognition},
  pages        = {108899},
  shortjournal = {Pattern Recognition},
  title        = {Dimension-aware attention for efficient mobile networks},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive aggregation-distillation autoencoder for
unsupervised anomaly detection. <em>PR</em>, <em>131</em>, 108897. (<a
href="https://doi.org/10.1016/j.patcog.2022.108897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD) has been receiving great attention as it plays a crucial role in many areas of basic research and industrial applications. However, most existing AD methods not only rely on training on normal data, but also ignore the multi-cluster nature of normal and abnormal patterns. To overcome these limitations, this paper proposes a novel method called Adaptive Aggregation-Distillation AutoEncoder (AADAE) for unsupervised anomaly detection. AADAE is built upon the density-based landmark selection in respect to representing diverse normal patterns. During training, AADAE adaptively updates the location and quantity of landmarks. Then, an aggregation-distillation mechanism is constructed: Firstly, it aggregates the latent representations of normal and anomalous to different landmark-guided regions within the convex polygon with landmarks as vertices, which minimizes the intra-class variation and promotes the separability of normal and abnormal samples. Secondly, the distillation mechanism is applied to obtain reliable detection results when there are anomalies in the training set. The aggregation process motivates AADAE to learn the distribution of multi-cluster normal samples with the help of landmarks, which in turn facilitates the distillation process to differentiate normal from anomalies for training. Extensive empirical studies on ten datasets from different application domains demonstrate the efficiency and generalization ability of the method.},
  archive      = {J_PR},
  author       = {Jiaqi Zhu and Fang Deng and Jiachen Zhao and Jie Chen},
  doi          = {10.1016/j.patcog.2022.108897},
  journal      = {Pattern Recognition},
  pages        = {108897},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive aggregation-distillation autoencoder for unsupervised anomaly detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate multi-layer classifier. <em>PR</em>,
<em>131</em>, 108896. (<a
href="https://doi.org/10.1016/j.patcog.2022.108896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The variance-ratio binary multi-layer classifier (VRBMLC) has been recently proposed and shown to outperform conventional binary decision trees (BDTs). Though effective with better interpretability , the VRBMLC generates deep layers of tree nodes as it employs a one-feature-at-a-time binary split at each layer. To further condense the tree depth and enhance the classification performance, this research proposes a multivariate multi-layer classifier that applies a variance-ratio criterion to enable ternary splits of each tree node and that integrates the oblique discriminant hyperplane in the tree node. We benchmark 16 state-of-the-art univariate and multivariate classifiers on 43 publicly available datasets. The results show that the proposed methods greatly simplify the tree structure and yield a significantly higher average accuracy.},
  archive      = {J_PR},
  author       = {Huanze Zeng and Argon Chen},
  doi          = {10.1016/j.patcog.2022.108896},
  journal      = {Pattern Recognition},
  pages        = {108896},
  shortjournal = {Pattern Recognition},
  title        = {Multivariate multi-layer classifier},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-order conditional mutual information maximization for
dealing with high-order dependencies in feature selection. <em>PR</em>,
<em>131</em>, 108895. (<a
href="https://doi.org/10.1016/j.patcog.2022.108895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel feature selection method based on the conditional mutual information (CMI). The proposed High Order Conditional Mutual Information Maximization (HOCMIM) method incorporates high order dependencies into the feature selection procedure and has a straightforward interpretation due to its bottom-up derivation. The HOCMIM is derived from the CMI’s chain expansion and expressed as a maximization optimization problem . The maximization problem is solved using a greedy search procedure, which speeds up the entire feature selection process. The experiments are run on a set of benchmark datasets (20 in total). The HOCMIM is compared with eighteen state-of-the-art feature selection algorithms , from the results of two supervised learning classifiers (Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best results in terms of accuracy and shows to be faster than high order feature selection counterparts.},
  archive      = {J_PR},
  author       = {Francisco Souza and Cristiano Premebida and Rui Araújo},
  doi          = {10.1016/j.patcog.2022.108895},
  journal      = {Pattern Recognition},
  pages        = {108895},
  shortjournal = {Pattern Recognition},
  title        = {High-order conditional mutual information maximization for dealing with high-order dependencies in feature selection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning deep morphological networks with neural
architecture search. <em>PR</em>, <em>131</em>, 108893. (<a
href="https://doi.org/10.1016/j.patcog.2022.108893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation . Our codes are available at https://nao-morpho.github.io/ .},
  archive      = {J_PR},
  author       = {Yufei Hu and Nacim Belkhir and Jesus Angulo and Angela Yao and Gianni Franchi},
  doi          = {10.1016/j.patcog.2022.108893},
  journal      = {Pattern Recognition},
  pages        = {108893},
  shortjournal = {Pattern Recognition},
  title        = {Learning deep morphological networks with neural architecture search},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving deep learning on point cloud by maximizing mutual
information across layers. <em>PR</em>, <em>131</em>, 108892. (<a
href="https://doi.org/10.1016/j.patcog.2022.108892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a fundamental and vital task to enhance the perception capability of the point cloud learning network in 3D machine vision applications . Most existing methods utilize feature fusion and geometric transformation to improve point cloud learning without paying enough attention to mining further intrinsic information across multiple network layers. Motivated to improve consistency between hierarchical features and strengthen the perception capability of the point cloud network, we propose exploring whether maximizing the mutual information (MI) across shallow and deep layers is beneficial to improve representation learning on point clouds. A novel design of Maximizing Mutual Information (MMI) Module is proposed, which assists the training process of the main network to capture discriminative features of the input point clouds. Specifically, the MMI-based loss function is employed to constrain the differences of semantic information in two hierarchical features extracted from the shallow and deep layers of the network. Extensive experiments show that our method is generally applicable to point cloud tasks, including classification, shape retrieval, indoor scene segmentation, 3D object detection , and completion, and illustrate the efficacy of our proposed method and its advantages over existing ones. Our source code is available at https://github.com/wendydidi/MMI.git .},
  archive      = {J_PR},
  author       = {Di Wang and Lulu Tang and Xu Wang and Luqing Luo and Zhi-Xin Yang},
  doi          = {10.1016/j.patcog.2022.108892},
  journal      = {Pattern Recognition},
  pages        = {108892},
  shortjournal = {Pattern Recognition},
  title        = {Improving deep learning on point cloud by maximizing mutual information across layers},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstruction of fragmented trajectories of collective
motion using hadamard deep autoencoders. <em>PR</em>, <em>131</em>,
108891. (<a href="https://doi.org/10.1016/j.patcog.2022.108891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning dynamics of collectively moving agents such as fish or humans is an essential task in research. Due to phenomena such as occlusion or change of illumination, the multi-object methods tracking such dynamics may lose the tracks of the agents which may result in fragmentations of trajectories. Here, we present an extended deep autoencoder (DA) that we train only on the fully observed segments of the trajectories by defining its loss function as the Hadamard product of a binary indicator matrix with the absolute difference between the outputs and the labels. The trajectory matrix of the agents practicing collective motion is low-rank due to mutual interactions and dependencies between the agents that we utilize as the underlying pattern that our Hadamard deep autoencoder (HDA) codes during its training. The performance of this HDA is compared with that of a low-rank matrix completion scheme in the context of fragmented trajectory reconstruction.},
  archive      = {J_PR},
  author       = {Kelum Gajamannage and Yonggi Park and Randy Paffenroth and Anura P. Jayasumana},
  doi          = {10.1016/j.patcog.2022.108891},
  journal      = {Pattern Recognition},
  pages        = {108891},
  shortjournal = {Pattern Recognition},
  title        = {Reconstruction of fragmented trajectories of collective motion using hadamard deep autoencoders},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). H-ProMed: Ultrasound image segmentation based on the
evolutionary neural network and an improved principal curve.
<em>PR</em>, <em>131</em>, 108890. (<a
href="https://doi.org/10.1016/j.patcog.2022.108890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this work is to develop a method for accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images. These images are difficult to segment due to missing/ambiguous boundary between the prostate and neighboring structures, the presence of shadow artifacts, as well as the large variability in prostate shapes. This paper develops a novel hybrid method for TRUS prostate segmentation by combining an improved principal curve-based method with an evolutionary neural network; the former for achieving the data sequences while and the latter for improving the smoothness of the prostate contour. Both qualitative and quantitative experimental results showed that our proposed method achieved superior segmentation accuracy and robustness as compared to state-of-the-art methods. The average Dice similarity coefficient (DSC), Jaccard similarity coefficient (Ω), and accuracy (ACC) of prostate contours against ground-truths were 96.8\%, 95.7\%, and 96.4\%, and the DSC of around 92\% and 95\% for other deep learning and hybrid methods, respectively.},
  archive      = {J_PR},
  author       = {Tao Peng and Jing Zhao and Yidong Gu and Caishan Wang and Yiyun Wu and Xiuxiu Cheng and Jing Cai},
  doi          = {10.1016/j.patcog.2022.108890},
  journal      = {Pattern Recognition},
  pages        = {108890},
  shortjournal = {Pattern Recognition},
  title        = {H-ProMed: Ultrasound image segmentation based on the evolutionary neural network and an improved principal curve},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of robust adversarial training in pattern
recognition: Fundamental, theory, and methodologies. <em>PR</em>,
<em>131</em>, 108889. (<a
href="https://doi.org/10.1016/j.patcog.2022.108889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved remarkable success in machine learning , computer vision , and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples . Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks . To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.},
  archive      = {J_PR},
  author       = {Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Xu-Yao Zhang},
  doi          = {10.1016/j.patcog.2022.108889},
  journal      = {Pattern Recognition},
  pages        = {108889},
  shortjournal = {Pattern Recognition},
  title        = {A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast algorithms for incremental and decremental
semi-supervised discriminant analysis. <em>PR</em>, <em>131</em>,
108888. (<a href="https://doi.org/10.1016/j.patcog.2022.108888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental and decremental problems are challenging tasks in semi-supervised learning. The incremental semi-supervised discriminant analysis (ISSDA) method proposed by Dhamecha et al. is an efficient method for incremental semi-supervised learning. However, one deficiency of the ISSDA method is that the total scatter matrix remains unchanged during incremental learning, which is impractical in practice. On the other hand, there may be a series of incorrectly artificial labeling in the public data set, and it is interesting to consider the decremental problem in semi-supervised learning. To the best of our knowledge, however, there are few decremental algorithms for semi-supervised discriminant analysis. The contributions of this work are as follows. First, a new incremental semi-supervised discriminant analysis method is proposed, in which we consider updating the total scatter matrix and the between-class scatter matrix simultaneously when new samples are added. Second, we show how to solve the large eigenproblem of the updated total scatter matrix efficiently. Third, we propose two decremental algorithms for semi-supervised discriminant analysis. Numerical experiments demonstrate the superiority of the proposed algorithms over many state-of-the-art algorithms for semi-supervised discriminant analysis.},
  archive      = {J_PR},
  author       = {Wenrao Pang and Gang Wu},
  doi          = {10.1016/j.patcog.2022.108888},
  journal      = {Pattern Recognition},
  pages        = {108888},
  shortjournal = {Pattern Recognition},
  title        = {Fast algorithms for incremental and decremental semi-supervised discriminant analysis},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DFR-ST: Discriminative feature representation with
spatio-temporal cues for vehicle re-identification. <em>PR</em>,
<em>131</em>, 108887. (<a
href="https://doi.org/10.1016/j.patcog.2022.108887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks . It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes in viewpoints and illumination conditions pose grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehicles’ spatio-temporal relationships in different images. This paper proposes a discriminative feature representation with spatio-temporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. The proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information based on this multi-modal information. Experimental results on four public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validates the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Jingzheng Tu and Cailian Chen and Xiaolin Huang and Jianping He and Xinping Guan},
  doi          = {10.1016/j.patcog.2022.108887},
  journal      = {Pattern Recognition},
  pages        = {108887},
  shortjournal = {Pattern Recognition},
  title        = {DFR-ST: Discriminative feature representation with spatio-temporal cues for vehicle re-identification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep rank hashing network for cancellable face
identification. <em>PR</em>, <em>131</em>, 108886. (<a
href="https://doi.org/10.1016/j.patcog.2022.108886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancellable biometrics (CB) is one of the major approaches for biometric template protection. However, almost all the prior arts are designed to work under verification (one-to-one matching). This paper proposes a deep learning-based cancellable biometric scheme for face identification (one-to-many matching). Our scheme comprises two key ingredients: a deep rank hashing (DRH) network and a cancellable identification scheme. The DRH network transforms a raw face image into discriminative yet compact face hash codes based upon the nonlinear subspace ranking notion. The network is designed to be trained for both identification and hashing goals with their respective rich identity-related and rank hashing relevant loss functions. A modified softmax function is utilized to alleviate the hashing quantization error , and a regularization term is designed to encourage hash code balance. The hash code is binarized, compressed, and secured with the randomized lookup table function. Unlike prior CB schemes that require two input factors for verification, the proposed scheme demands no additional input except face images during identification, yet the face template is replaceable whenever needed based upon a one-time XOR cipher notion. The proposed scheme is evaluated on five public unconstrained face datasets in terms of verification, closed-set and open-set identification performance accuracy, computation cost, template protection criteria, and security.},
  archive      = {J_PR},
  author       = {Xingbo Dong and Sangrae Cho and Youngsam Kim and Soohyung Kim and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.patcog.2022.108886},
  journal      = {Pattern Recognition},
  pages        = {108886},
  shortjournal = {Pattern Recognition},
  title        = {Deep rank hashing network for cancellable face identification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning from low-labelled stream data in
open-set video face recognition. <em>PR</em>, <em>131</em>, 108885. (<a
href="https://doi.org/10.1016/j.patcog.2022.108885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15\% F1-score increase respect to non-adaptive state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Eric Lopez-Lopez and Xose M. Pardo and Carlos V. Regueiro},
  doi          = {10.1016/j.patcog.2022.108885},
  journal      = {Pattern Recognition},
  pages        = {108885},
  shortjournal = {Pattern Recognition},
  title        = {Incremental learning from low-labelled stream data in open-set video face recognition},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-view panorama image synthesis with progressive
attention GANs. <em>PR</em>, <em>131</em>, 108884. (<a
href="https://doi.org/10.1016/j.patcog.2022.108884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant progress of conditional image generation , it remains difficult to synthesize a ground-view panorama image from a top-view aerial image . Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map . Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Songsong Wu and Hao Tang and Xiao-Yuan Jing and Jianjun Qian and Nicu Sebe and Yan Yan and Qinghua Zhang},
  doi          = {10.1016/j.patcog.2022.108884},
  journal      = {Pattern Recognition},
  pages        = {108884},
  shortjournal = {Pattern Recognition},
  title        = {Cross-view panorama image synthesis with progressive attention GANs},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the role of question encoder sequence model in robust
visual question answering. <em>PR</em>, <em>131</em>, 108883. (<a
href="https://doi.org/10.1016/j.patcog.2022.108883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing beyond the experiences has a significant role in developing robust and practical machine learning systems . It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct negatively affects the robustness of VQA models and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the OOD performance of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder reduces the over-fit to language biases and improves OOD performance in VQA even without using any additional relatively complex bias-mitigation approaches.},
  archive      = {J_PR},
  author       = {KV Gouthaman and Anurag Mittal},
  doi          = {10.1016/j.patcog.2022.108883},
  journal      = {Pattern Recognition},
  pages        = {108883},
  shortjournal = {Pattern Recognition},
  title        = {On the role of question encoder sequence model in robust visual question answering},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking interactive image segmentation: Feature space
annotation. <em>PR</em>, <em>131</em>, 108882. (<a
href="https://doi.org/10.1016/j.patcog.2022.108882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the progress of interactive image segmentation methods, high-quality pixel-level annotation is still time-consuming and laborious — a bottleneck for several deep learning applications. We take a step back to propose interactive and simultaneous segment annotation from multiple images guided by feature space projection. This strategy is in stark contrast to existing interactive segmentation methodologies, which perform annotation in the image domain. We show that feature space annotation achieves competitive results with state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5\% accuracy in the Cityscapes dataset, being 74.75 times faster than the original annotation procedure. Further, our contribution sheds light on a novel direction for interactive image annotation that can be integrated with existing methodologies. The supplementary material presents video demonstrations. Code available at https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation .},
  archive      = {J_PR},
  author       = {Jordão Bragantini and Alexandre X. Falcão and Laurent Najman},
  doi          = {10.1016/j.patcog.2022.108882},
  journal      = {Pattern Recognition},
  pages        = {108882},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking interactive image segmentation: Feature space annotation},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complementarity-aware cross-modal feature fusion network for
RGB-t semantic segmentation. <em>PR</em>, <em>131</em>, 108881. (<a
href="https://doi.org/10.1016/j.patcog.2022.108881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T semantic segmentation has attracted growing attention because it makes a model robust towards challenging illumination. Most existing methods fuse RGB and thermal information in an equal manner along spatial dimensions, which results in feature redundancy and affects the discriminability of cross-modal features. In this paper, we propose a Complementarity-aware Cross-modal Feature Fusion Network (CCFFNet) including a Complementarity-Aware Encoder (CAE) and a Three-Path Fusion and Supervision (TPFS). The CAE, which consists of cascaded cross-modal fusion modules, can select complementary information from RGB and thermal features via a novel gate and fuse them by a channel-wise weighting mechanism. TPFS not only iteratively performs Three-Path Fusion (TPF) to further enhance cross-modal features, but also supervise the training of CCFFNet along three branches by Three-Supervision (TS). Extensive experiments are carried out and the results demonstrate that our model outperforms the state-of-the-art models by at least 1.6\% mIoU on MFNet dataset and 2.9\% mIoU on PST900 dataset, respectively. And a single-modality-based model can be easily applied to multi-modal semantic segmentation when plugging our CAE.},
  archive      = {J_PR},
  author       = {Wei Wu and Tao Chu and Qiong Liu},
  doi          = {10.1016/j.patcog.2022.108881},
  journal      = {Pattern Recognition},
  pages        = {108881},
  shortjournal = {Pattern Recognition},
  title        = {Complementarity-aware cross-modal feature fusion network for RGB-T semantic segmentation},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-guided information for few-shot classification.
<em>PR</em>, <em>131</em>, 108880. (<a
href="https://doi.org/10.1016/j.patcog.2022.108880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification aims to identify novel categories using only a few labeled samples. Generally, the metric-based few-shot classification methods compare the feature embedding of Query samples (unlabeled samples) with Support samples (labeled samples) in a metric algorithm to predict which category the Query sample belongs to. Obtaining a good feature embedding for each sample in the feature extraction stage can improve the classification accuracy in the metric stage. Based on this, we design the Self-Guided Information Convolution (SGI-Conv), an improved convolution structure, which utilizes the high-level features to guide the network to extract the required discriminative features . To effectively utilize the feature embeddings of samples, we divide the metric network into multiple blocks and build a multi-layer graph convolutional network by sharing adjacent matrices. The multi-layer structure enhances the aggregation ability of graph convolution. Extensive experiments on multiple benchmark datasets demonstrate that our method has achieved competitive results on the few-shot classification tasks .},
  archive      = {J_PR},
  author       = {Zhineng Zhao and Qifan Liu and Wenming Cao and Deliang Lian and Zhihai He},
  doi          = {10.1016/j.patcog.2022.108880},
  journal      = {Pattern Recognition},
  pages        = {108880},
  shortjournal = {Pattern Recognition},
  title        = {Self-guided information for few-shot classification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comprehensive-perception dynamic reasoning for visual
question answering. <em>PR</em>, <em>131</em>, 108878. (<a
href="https://doi.org/10.1016/j.patcog.2022.108878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Visual Question Answering (VQA) is to answer questions based on an image. In the VQA task, reasoning plays an important role in dealing with relations because this task has a high requirement for modeling complex features. In most existing models, the features are only extracted and integrated between adjacent layers. This pattern arguably affects the integrity of information interaction during reasoning. In this paper, we propose a comprehensive-perception dynamic reasoning (CPDR) model to utilize the cross-layer object features for multi-step compound reasoning. It calculates the interactions among the object features from all previous layers and integrates these interactions to generate new object features, iteratively. Finally, the object features of all layers will be used for the final prediction. Empirical results show that our model achieves superior performance among VQA models which are not VLP-based, and incorporating the CPDR module into the VLP models brings considerable performance improvements.},
  archive      = {J_PR},
  author       = {Kai Shuang and Jinyu Guo and Zihan Wang},
  doi          = {10.1016/j.patcog.2022.108878},
  journal      = {Pattern Recognition},
  pages        = {108878},
  shortjournal = {Pattern Recognition},
  title        = {Comprehensive-perception dynamic reasoning for visual question answering},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SO-softmax loss for discriminable embedding learning in
CNNs. <em>PR</em>, <em>131</em>, 108877. (<a
href="https://doi.org/10.1016/j.patcog.2022.108877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs)-based classifiers, trained with the softmax cross-entropy loss, have achieved remarkable success in learning embeddings for pattern recognition. The cosine similarity-based softmax variants further improve the performance by focusing on optimizing the angles between embeddings and class weights. However, embeddings learned by these variants still have significant intra-class variances since these methods only optimize the relative differences between intra- and inter-class cosine similarities. To simultaneously optimize intra- and inter-class cosine similarities, this paper proposes a cosine Similarity Optimization-based softmax (SO-softmax) loss, which is based on a generalized softmax loss formulation that combines both similarities. The proposed loss constrains the intra-class (positive) and inter-class (negative) cosine similarity by quadratic transformations, thus making the embedding representation more compact within classes and more distinguishable between classes. It is verified theoretically that SO-softmax loss can optimize both the similarities simultaneously. Thorough experiments are conducted on typical audio classification, image classification, face verification, image retrieval, and person re-identification tasks, and the results show that SO-softmax loss outperforms the state-of-the-art loss functions in CNNs-based frameworks.},
  archive      = {J_PR},
  author       = {Qiang Zhang and Jibin Yang and Xiongwei Zhang and Tieyong Cao},
  doi          = {10.1016/j.patcog.2022.108877},
  journal      = {Pattern Recognition},
  pages        = {108877},
  shortjournal = {Pattern Recognition},
  title        = {SO-softmax loss for discriminable embedding learning in CNNs},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel explainable neural network for alzheimer’s disease
diagnosis. <em>PR</em>, <em>131</em>, 108876. (<a
href="https://doi.org/10.1016/j.patcog.2022.108876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual classification for medical images has been dominated by convolutional neural networks (CNNs) for years. Though they have shown great performance on accuracy, some of them provide decisions that are hard to explain while others encode information from irrelevant or noisy regions. In this work, we try to close this gap by proposing an explainable framework which consists of a predictor and an explainable tool, so as to provide accurate diagnoses with intuitive visualization maps and prediction basis. Specifically, the predictor is designed by applying attention mechanisms to multi-scale features so as to learn and discover class discriminative latent representations that are close to each brain volume’s label. Meanwhile, to explain our predictor, we propose the novel explainable tool which includes a high-resolution visualization method and a prediction-basis creation and retrieval module. The former effectively integrates the feature maps of intermediate layers as well as the last convolutional layer , which surpasses state-of-the-art visualization approaches in producing high-resolution representations with more accurate localization of discriminative areas. While the latter provides prediction basis evidence via retrieved volumes with similar latent representations which are accessible to neurologists. Extensive experiments show that the proposed framework achieves higher level of accuracy and explainability over other state-of-the-art solutions. More importantly, it localizes crucial brain areas with clearer boundaries, less noises, which matches background knowledge in the neuroscience literature.},
  archive      = {J_PR},
  author       = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping Phoebe Chen and Ruifeng Zhu},
  doi          = {10.1016/j.patcog.2022.108876},
  journal      = {Pattern Recognition},
  pages        = {108876},
  shortjournal = {Pattern Recognition},
  title        = {A novel explainable neural network for alzheimer’s disease diagnosis},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering experience replay for the effective exploitation
in reinforcement learning. <em>PR</em>, <em>131</em>, 108875. (<a
href="https://doi.org/10.1016/j.patcog.2022.108875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning is a useful tool for training an agent to effectively achieve the desired goal in the sequential decision-making problem. It trains the agent to make decision by exploiting the experience in the transitions resulting from the different decisions. To exploit this experience, most reinforcement learning methods replay the explored transitions by uniform sampling. But in this way, it is easy to ignore the last explored transitions. Another way to exploit this experience defines the priority of each transition by the estimation error in training and then replays the transitions according to their priorities. But it only updates the priorities of the transitions replayed at the current training time step, thus the transitions with low priorities will be ignored. In this paper, we propose a clustering experience replay, called CER, to effectively exploit the experience hidden in all explored transitions in the current training. CER clusters and replays the transitions by a divide-and-conquer framework based on time division as follows. Firstly, it divides the whole training process into several periods. Secondly, at the end of each period, it uses k k -means to cluster the transitions explored in this period. Finally, it constructs a conditional probability density function to ensure that all kinds of transitions will be sufficiently replayed in the current training. We construct a new method, TD3 _ _ CER, to implement our clustering experience replay on TD3. Through the theoretical analysis and experiments, we illustrate that our TD3 _ _ CER is more effective than the existing reinforcement learning methods. The source code can be downloaded from https://github.com/grcai/CER-Master .},
  archive      = {J_PR},
  author       = {Min Li and Tianyi Huang and William Zhu},
  doi          = {10.1016/j.patcog.2022.108875},
  journal      = {Pattern Recognition},
  pages        = {108875},
  shortjournal = {Pattern Recognition},
  title        = {Clustering experience replay for the effective exploitation in reinforcement learning},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HCFNN: High-order coverage function neural network for image
classification. <em>PR</em>, <em>131</em>, 108873. (<a
href="https://doi.org/10.1016/j.patcog.2022.108873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks (DNNs) have mainly focused on innovations in network architecture and loss function. In this paper, we introduce a flexible high-order coverage function (HCF) neuron model to replace the fully-connected (FC) layers. The approximation theorem and proof for the HCF are also presented to demonstrate its fitting ability. Unlike the FC layers, which cannot handle high-dimensional data well, the HCF utilizes weight coefficients and hyper-parameters to mine underlying geometries with arbitrary shapes in an n-dimensional space. To explore the power and potential of our HCF neuron model, a high-order coverage function neural network (HCFNN) is proposed, which incorporates the HCF neuron as the building block . Moreover, a novel adaptive optimization method for weights and hyper-parameters is designed to achieve effective network learning. Comprehensive experiments on nine datasets in several domains validate the effectiveness and generalizability of the HCF and HCFNN. The proposed method provides a new perspective for further developments in DNNs and ensures wide application in the field of image classification . The source code is available at https://github.com/Tough2011/HCFNet.git},
  archive      = {J_PR},
  author       = {Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang},
  doi          = {10.1016/j.patcog.2022.108873},
  journal      = {Pattern Recognition},
  pages        = {108873},
  shortjournal = {Pattern Recognition},
  title        = {HCFNN: High-order coverage function neural network for image classification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Alleviating the estimation bias of deep deterministic policy
gradient via co-regularization. <em>PR</em>, <em>131</em>, 108872. (<a
href="https://doi.org/10.1016/j.patcog.2022.108872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overestimation in Deep Deterministic Policy Gradients (DDPG) caused by value approximation error may result in unstable policy training. Twin Delayed Deep Deterministic Policy Gradient (TD3) addresses the overestimation but suffers from the underestimation. In this paper, we propose a Co -Regularization based D eep D eterministic (CoD2) policy gradient method to mitigate the estimation bias. Two learners characterized by overestimated and underestimated biases are trained with Co-regularization to achieve this goal. The overestimated and underestimated values are updated conservatively in CoD2 for policy evaluation. Experimental results show that our method achieves comparable performance compared with other methods.},
  archive      = {J_PR},
  author       = {Yao Li and YuHui Wang and YaoZhong Gan and XiaoYang Tan},
  doi          = {10.1016/j.patcog.2022.108872},
  journal      = {Pattern Recognition},
  pages        = {108872},
  shortjournal = {Pattern Recognition},
  title        = {Alleviating the estimation bias of deep deterministic policy gradient via co-regularization},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2PESNet: Towards online processing of temporal action
localization. <em>PR</em>, <em>131</em>, 108871. (<a
href="https://doi.org/10.1016/j.patcog.2022.108871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online video processing methods such as online action detection focus on a frame-level understanding for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the online settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to further improve the performance. Extensive experiments on THUMOS’14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.},
  archive      = {J_PR},
  author       = {Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim},
  doi          = {10.1016/j.patcog.2022.108871},
  journal      = {Pattern Recognition},
  pages        = {108871},
  shortjournal = {Pattern Recognition},
  title        = {2PESNet: Towards online processing of temporal action localization},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-attention-YOLO (DAY): A comprehensive framework for
mesoscale eddy identification. <em>PR</em>, <em>131</em>, 108870. (<a
href="https://doi.org/10.1016/j.patcog.2022.108870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54\% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51\%.},
  archive      = {J_PR},
  author       = {Xinning Wang and Xuegong Wang and Chong Li and Yuben Zhao and Peng Ren},
  doi          = {10.1016/j.patcog.2022.108870},
  journal      = {Pattern Recognition},
  pages        = {108870},
  shortjournal = {Pattern Recognition},
  title        = {Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel part-level feature extraction method for
fine-grained vehicle recognition. <em>PR</em>, <em>131</em>, 108869. (<a
href="https://doi.org/10.1016/j.patcog.2022.108869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel part-level feature extraction method to enhance the discriminative ability of deep convolutional features for the task of fine-grained vehicle recognition. Generally, the challenges for fine-grained vehicle recognition are mainly caused by the subtle visual differences between part regions of vehicles. Therefore, it is essential to extract discriminative features from part regions. Many existing methods, especially deep convolutional neural networks (D-CNNs), tend to detect the discriminative part regions explicitly or learn the part information implicitly through network restructuring and neglect the abundant part-level information contained in the high-level features generated by CNNs. In light of this, we propose a simple and effective part-level feature extraction method to enhance the representation of part-level features within the global features of target object generated by the backbone networks . The proposed method is built on the deep convolutional layers from which the discriminative part features could be integrated and extracted accordingly. More specifically, a basic feature grouping module is adopted to integrate the feature maps of deep convolutional layers into groups in each of which the related discriminative parts are assembled. The feature grouping process is performed in a multi-stage manner to ensure the integration process. Then a fusion module follows to model the coarse-to-fine relationship of the part features and further ensure the integrity and effectiveness of the part features. We conduct comparison experiments on public datasets, and the results show that the proposed method achieves comparable performance with state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Lei Lu and Ping Wang and Yijie Cao},
  doi          = {10.1016/j.patcog.2022.108869},
  journal      = {Pattern Recognition},
  pages        = {108869},
  shortjournal = {Pattern Recognition},
  title        = {A novel part-level feature extraction method for fine-grained vehicle recognition},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DSLA: Dynamic smooth label assignment for efficient
anchor-free object detection. <em>PR</em>, <em>131</em>, 108868. (<a
href="https://doi.org/10.1016/j.patcog.2022.108868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization . The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation . Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [ 0 , 1 ] [0,1] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https://github.com/YonghaoHe/DSLA .},
  archive      = {J_PR},
  author       = {Hu Su and Yonghao He and Rui Jiang and Jiabin Zhang and Wei Zou and Bin Fan},
  doi          = {10.1016/j.patcog.2022.108868},
  journal      = {Pattern Recognition},
  pages        = {108868},
  shortjournal = {Pattern Recognition},
  title        = {DSLA: Dynamic smooth label assignment for efficient anchor-free object detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shedding light on images: Multi-level image brightness
enhancement guided by arbitrary references. <em>PR</em>, <em>131</em>,
108867. (<a href="https://doi.org/10.1016/j.patcog.2022.108867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-linearity between human perception and image brightness levels results in different definitions of NORMAL-light. Thus, most existing low-light image enhancement methods which produce one-to-one mapping can not meet the aesthetic demand. Other pioneers enhance low-light images guided by a given value. However, the inherent problem of non-linearity will cause poor usability. To this end, we propose a user-friendly neural network for multi-level low-light image enhancement. Inspired by style transfer, our method decomposes an image into content component feature and luminance component feature in the latent space. Then we enhance the image brightness to different levels by concatenating the content components from low-light images and the luminance components from reference images. The network meets various user requirements by selecting different brightness references. Moreover, information except for brightness is preserved to alleviate color distortion. Extensive experiments demonstrate the superiority of our network against existing methods.},
  archive      = {J_PR},
  author       = {Ya’nan Wang and Zhuqing Jiang and Chang Liu and Kai Li and Aidong Men and Haiying Wang and Xiaobo Chen},
  doi          = {10.1016/j.patcog.2022.108867},
  journal      = {Pattern Recognition},
  pages        = {108867},
  shortjournal = {Pattern Recognition},
  title        = {Shedding light on images: Multi-level image brightness enhancement guided by arbitrary references},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting key points supervision and grouped feature fusion
for multiview pedestrian detection. <em>PR</em>, <em>131</em>, 108866.
(<a href="https://doi.org/10.1016/j.patcog.2022.108866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview pedestrian detection detects pedestrians based on the perception of the same environment from multiple perspectives. This task requires feature extraction in a single view with occlusion and aggregation of multiview information. However, existing research is limited by the local occlusion and the multiview feature stitching method, which cannot perform multiview aggregation efficiently. This paper introduces a network that utilizes key points supervision and grouped feature fusion to address these challenges. It uses key points to regress pedestrians in a single view, and augments the pedestrian consistency information in overlapping views by a grouped feature fusion module. Specifically, the proposed key points supervision effectively alleviates false negatives due to occlusion, and the grouped feature fusion module enhances pedestrian location features by computing the similarity and spatial correlation of overlapping views after single view projection to the ground plane, thereby reducing target ambiguity. Quantitative and qualitative results show that the proposed method can reduce false negatives and false positives in multiview pedestrian detection and achieve efficient multiview feature aggregation. Compared to state-of-the-art methods, the proposed model achieves superior performance, achieving the highest MODA of 92.4 and 93.9 on Wildtrack and MultiviewX datasets, respectively. We believe, to the best of our knowledge, that this approach offers a new optimization idea for multiview aggregation.},
  archive      = {J_PR},
  author       = {Xin Gao and Yijin Xiong and Guoying Zhang and Hui Deng and Kangkang Kou},
  doi          = {10.1016/j.patcog.2022.108866},
  journal      = {Pattern Recognition},
  pages        = {108866},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting key points supervision and grouped feature fusion for multiview pedestrian detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inter-attribute awareness for pedestrian attribute
recognition. <em>PR</em>, <em>131</em>, 108865. (<a
href="https://doi.org/10.1016/j.patcog.2022.108865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone ( e.g., ResNet50 or BN-Inception), a more lightweight backbone ( i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability .},
  archive      = {J_PR},
  author       = {Junyi Wu and Yan Huang and Zhipeng Gao and Yating Hong and Jianqiang Zhao and Xinsheng Du},
  doi          = {10.1016/j.patcog.2022.108865},
  journal      = {Pattern Recognition},
  pages        = {108865},
  shortjournal = {Pattern Recognition},
  title        = {Inter-attribute awareness for pedestrian attribute recognition},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DARTSRepair: Core-failure-set guided DARTS for network
robustness to common corruptions. <em>PR</em>, <em>131</em>, 108864. (<a
href="https://doi.org/10.1016/j.patcog.2022.108864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed network’s model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern ( e.g ., noise, blur, etc ..). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples ( e.g ., 10 3 103 examples) to the clean training dataset ( e.g ., 5.0 × 10 4 5.0×104 examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e ., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K K -center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45\% 45\% absolute accuracy improvements.},
  archive      = {J_PR},
  author       = {Xuhong Ren and Jianlang Chen and Felix Juefei-Xu and Wanli Xue and Qing Guo and Lei Ma and Jianjun Zhao and Shengyong Chen},
  doi          = {10.1016/j.patcog.2022.108864},
  journal      = {Pattern Recognition},
  pages        = {108864},
  shortjournal = {Pattern Recognition},
  title        = {DARTSRepair: Core-failure-set guided DARTS for network robustness to common corruptions},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view correlation distillation for incremental object
detection. <em>PR</em>, <em>131</em>, 108863. (<a
href="https://doi.org/10.1016/j.patcog.2022.108863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. Due to the storage burden, data privacy and time consumption, sometimes it is impractical to train the model from scratch with all data of both old and new classes. In this paper, we propose a novel M ulti- V iew C orrelation D istillation (MVCD) based incremental object detection method, which explores the intra-feature correlations in the feature space of the object detector. To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we select the sample-specific discriminative features from channel-wise, point-wise and instance-wise views. Meanwhile, the correlation distillation losses on the selective features are designed to regularize the learning of the incremental object detector. A new metric named Stability-Plasticity-mAP ( SPmAP ) is proposed to evaluate the incremental learning performance as a complementary metric to mAP, which integrates the metrics for the stability on old classes and the plasticity on new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD achieves a better trade-off between stability and plasticity than state-of-the-art first-order distillation-based incremental object detection methods.},
  archive      = {J_PR},
  author       = {Dongbao Yang and Yu Zhou and Aoting Zhang and Xurui Sun and Dayan Wu and Weiping Wang and Qixiang Ye},
  doi          = {10.1016/j.patcog.2022.108863},
  journal      = {Pattern Recognition},
  pages        = {108863},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view correlation distillation for incremental object detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online adaptive kernel learning with random features for
large-scale nonlinear classification. <em>PR</em>, <em>131</em>, 108862.
(<a href="https://doi.org/10.1016/j.patcog.2022.108862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of support vector machines , online random feature map algorithms are very important methods for large-scale nonlinear classification problems. At present, the existing methods have the following shortcomings: (1) If only the hyperplane vector is updated during learning while the random feature components are fixed, there is no guarantee that these online methods can adapt to the change of data distribution shape when the data is coming one by one. (2) When the kernel is selected improperly, the samples mapped to an inappropriate space may not be well classified. In order to overcome these shortcomings, considering the fact that iteratively updating random feature components can make data better fit in the current space and lead to the flexible adjustment of the kernel function , random features based online adaptive kernel learning (RF-OAK) is proposed for large-scale nonlinear classification problems. Theoretical analysis of the proposed algorithm is also provided. The experimental results and the Wilcoxon signed-ranks test show that in terms of test accuracy, the proposed method is significantly better than the state-of-the-art online feature mapping classification methods. Compared with the deep learning algorithms, the training time of RF-OAK is shorter. In terms of test accuracy, RF-OAK is better than online algorithm and comparable with offline algorithms.},
  archive      = {J_PR},
  author       = {Yingying Chen and Xiaowei Yang},
  doi          = {10.1016/j.patcog.2022.108862},
  journal      = {Pattern Recognition},
  pages        = {108862},
  shortjournal = {Pattern Recognition},
  title        = {Online adaptive kernel learning with random features for large-scale nonlinear classification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A modified interval type-2 takagi-sugeno fuzzy neural
network and its convergence analysis. <em>PR</em>, <em>131</em>, 108861.
(<a href="https://doi.org/10.1016/j.patcog.2022.108861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, to compute the firing strength values of type-2 fuzzy models, a soft version of minimum is presented, which endows the fuzzy model with the ability to solve large dimensional problems. In addition, a conjugate gradient method is borrowed to train the designed interval type-2 Takagi-Sugeno fuzzy model. Compared with the existing gradient-based learning strategy, this scheme can efficiently enhance the fuzzy model performance. Last but not least, convergence analysis for this modified interval type-2 Takagi-Sugeno fuzzy neural network (MIT2TSFNN) is conducted in detail, which proves that the gradient of the error function tends to zero with the iteration increasing (weak convergence) and the sequence of model parameters (weights) convergences to a fixed point (strong convergence). To validate the effectiveness of the proposed MIT2TSFNN and its theoretical results, simulation results of six regression and six classification problems are presented.},
  archive      = {J_PR},
  author       = {Tao Gao and Xiao Bai and Chen Wang and Liang Zhang and Jin Zheng and Jian Wang},
  doi          = {10.1016/j.patcog.2022.108861},
  journal      = {Pattern Recognition},
  pages        = {108861},
  shortjournal = {Pattern Recognition},
  title        = {A modified interval type-2 takagi-sugeno fuzzy neural network and its convergence analysis},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering unknowns: Context-enhanced anomaly detection for
curiosity-driven autonomous underwater exploration. <em>PR</em>,
<em>131</em>, 108860. (<a
href="https://doi.org/10.1016/j.patcog.2022.108860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering unknown objects from visual information as curiosity is highly demanded for autonomous exploration in underwater environment. In this research, we propose an end-to-end deep neural network for anomaly detection in the highly dynamic unstructured underwater background faced by a moving robot. A novel patch-level autoencoder combined with a context-enhanced autoregressive network is introduced to differentiate abnormal patterns (unknowns) from normal ones (knowns) in fine-scale regions. The autoencoder and autoregressive network share the same encoder to extract latent features. The autoregressive branch learns semantic dependence based on conditional probability to identify anomaly in a latent feature space. The overall anomaly score is weighted by both image reconstruction loss and feature similarity loss. The model outperforms state-of-the-art anomaly detection , demonstrated on the benchmark dataset CIFAR-10. Average discrimination performance AUROC improved 2.18\%, and inception distance between normal and anomalous classes improved 9.33\% in Z -score. The network has been tested using three underwater datasets from underwater simulation, a real-world undersea video and public SUIM data. The AUROC accuracy improved 6.36\%, 32.45\% and 40.17\% respectively by using the proposed patch learning paradigm. It is the first report on unknown detection as navigation clues for curiosity-driven autonomous underwater exploration.},
  archive      = {J_PR},
  author       = {Yang Zhou and Baihua Li and Jiangtao Wang and Emanuele Rocco and Qinggang Meng},
  doi          = {10.1016/j.patcog.2022.108860},
  journal      = {Pattern Recognition},
  pages        = {108860},
  shortjournal = {Pattern Recognition},
  title        = {Discovering unknowns: Context-enhanced anomaly detection for curiosity-driven autonomous underwater exploration},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal prototype learning for zero-shot handwritten
character recognition. <em>PR</em>, <em>131</em>, 108859. (<a
href="https://doi.org/10.1016/j.patcog.2022.108859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods of handwritten character recognition rely on extensive labeled data. However, humans can generalize to unseen handwritten characters by watching a few printed examples in textbooks. To simulate this ability, we propose a cross-modal prototype learning method (CMPL) to realize zero-shot recognition. For each character class, a prototype is generated by mapping the printed character into a deep neural network feature space. For unseen character class, its prototype can be directly produced from a printed character sample, therefore, not requiring any handwritten samples to realize class-incremental learning. Specifically, CMPL considers different modalities simultaneously - online handwritten trajectories, offline handwritten images, and auxiliary printed character images. The joint learning of the above modalities is achieved through sharing printed prototypes between online and offline data. In zero-shot inference, we feed CMPL the printed samples to obtain corresponding class prototypes, and then the unseen handwritten character can be recognized by the nearest prototype. Our experimental results demonstrate that CMPL outperforms the state-of-the-art methods in both online and offline zero-shot handwritten Chinese character recognition. Moreover, we also show the cross-domain generalization of CMPL from two perspectives: cross-language and modern-to-ancient handwritten character recognition, focusing on the transferability between different languages and different styles (i.e., modern and historical handwritings).},
  archive      = {J_PR},
  author       = {Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2022.108859},
  journal      = {Pattern Recognition},
  pages        = {108859},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal prototype learning for zero-shot handwritten character recognition},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-feature deep information bottleneck network for breast
cancer classification in contrast enhanced spectral mammography.
<em>PR</em>, <em>131</em>, 108858. (<a
href="https://doi.org/10.1016/j.patcog.2022.108858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is considerable variation in the size, shape and location of tumours, which makes it challenging for radiologists to diagnose breast cancer. Automated diagnosis of breast cancer from Contrast Enhanced Spectral Mammography (CESM) can support clinical decision making. However, existing methods fail to obtain an effective representation of the CESM and ignore the relationships between images. In this paper, we investigated for the first time a novel and flexible multimodal representation learning method, multi-feature deep information bottleneck (MDIB), for breast cancer classification in CESM. Specifically, the method incorporated an information bottleneck (IB)-based module to learn the prominent representation that provide concise input while informative for the classification. In addition, we creatively extended IB theory to multi-feature IB, which facilitates the learning of relevant features for classification between CESM images. To validate our method, experiments were conducted on our private and public datasets. The classification results of our method were also compared with those of state-of-the-art methods. The experiment results proved the effectiveness and the efficiency of the proposed method. We release our code at https://github.com/sjq5263/MDIB-for-CESM-classification.},
  archive      = {J_PR},
  author       = {Jingqi Song and Yuanjie Zheng and Jing Wang and Muhammad Zakir Ullah and Xuecheng Li and Zhenxing Zou and Guocheng Ding},
  doi          = {10.1016/j.patcog.2022.108858},
  journal      = {Pattern Recognition},
  pages        = {108858},
  shortjournal = {Pattern Recognition},
  title        = {Multi-feature deep information bottleneck network for breast cancer classification in contrast enhanced spectral mammography},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symbolic sequence representation with markovian state
optimization. <em>PR</em>, <em>131</em>, 108849. (<a
href="https://doi.org/10.1016/j.patcog.2022.108849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.},
  archive      = {J_PR},
  author       = {Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang},
  doi          = {10.1016/j.patcog.2022.108849},
  journal      = {Pattern Recognition},
  pages        = {108849},
  shortjournal = {Pattern Recognition},
  title        = {Symbolic sequence representation with markovian state optimization},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal feature enhancement network with external memory
for live-stream video object detection. <em>PR</em>, <em>131</em>,
108847. (<a href="https://doi.org/10.1016/j.patcog.2022.108847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method exploiting temporal context with an attention mechanism for detecting objects in real-time in a live streaming video. Video object detection is challenging and essential in practical applications such as robotics, smartphones, and surveillance cameras. Although methods have been proposed to improve the accuracy or run-time speed by exploiting temporal information, the trade-off between them tends to be ignored. We thus focus on the trade-off between accuracy and speed, and propose a method to improve the accuracy by aggregating the past information from a lightweight feature extractor with an attention mechanism. Evaluations on the UA-DETRAC and ImageNet VID datasets demonstrate our model’s superior performance to state-of-the-art methods on live streaming real-time object detection.},
  archive      = {J_PR},
  author       = {Masato Fujitake and Akihiro Sugimoto},
  doi          = {10.1016/j.patcog.2022.108847},
  journal      = {Pattern Recognition},
  pages        = {108847},
  shortjournal = {Pattern Recognition},
  title        = {Temporal feature enhancement network with external memory for live-stream video object detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel way to formalize stable graph cores by using
matching-graphs. <em>PR</em>, <em>131</em>, 108846. (<a
href="https://doi.org/10.1016/j.patcog.2022.108846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing amount of data available and the rate at which it is collected leads to rapid developments of systems for intelligent information processing and pattern recognition. Often the underlying data is inherently complex, making it difficult to represent it by linear, vectorial data structures . This is where graphs offer a versatile alternative for formal data representation. Actually, quite an amount of graph-based methods for pattern recognition has been proposed. A considerable part of these methods rely on graph matching . In the present paper, we propose a novel encoding of specific graph matching information. The basic idea is to formalize the stable cores of individual classes of graphs – discovered during intra-class matchings – by means of so called matching-graphs. We evaluate the benefit of these matching-graphs by researching two classification approaches that rely on this novel data structure. The first approach is a distance based classifier focusing on the matching-graphs during dissimilarity computation. For the second approach, we propose to use sets of matching-graphs to embed input graphs into a vector space. The basic idea is to produce hundreds of matching-graphs first, and then represent each graph g as a vector that shows the occurrence of, or the distance to, each matching-graph. In a thorough experimental evaluation on seven real world data sets we empirically confirm that our novel approaches are able to improve the classification accuracy of systems that rely on comparable information as well as state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Mathias Fuchs and Kaspar Riesen},
  doi          = {10.1016/j.patcog.2022.108846},
  journal      = {Pattern Recognition},
  pages        = {108846},
  shortjournal = {Pattern Recognition},
  title        = {A novel way to formalize stable graph cores by using matching-graphs},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative structure transformation and conditional random
field based method for unsupervised multimodal change detection.
<em>PR</em>, <em>131</em>, 108845. (<a
href="https://doi.org/10.1016/j.patcog.2022.108845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection between heterogeneous images has become an increasingly interesting research topic in remote sensing. The different appearances and statistics of heterogeneous images bring great challenges to this task. In this paper, we propose an unsupervised iterative structure transformation and conditional random field (IST-CRF) based multimodal change detection (MCD) method, combining an imaging modality-invariant based structure transformation method with a random filed framework specifically designed for MCD, to acquire an optimal change map within a global probabilistic model. IST-CRF first constructs graphs to represent the structures of the images, and transforms the heterogeneous images to the same differential domain by using graph based forward and backward structure transformations. Then, the change vectors are calculated to distinguish the changed and unchanged areas. Finally, in order to classify the change vectors and compute the binary change map, a CRF model is designed to fully explore the spectral-spatial information, which incorporates the change information, local spatially-adjacent neighbor information, and global spectrally-similar neighbor information with a random field framework. As the changed samples will influence the structure transformation and reduce the quality of change vectors, we use an iterative framework to propagate the CRF segmentation results back to the structure transformation process that removes the changed samples, and thus improve the accuracy of change detection. Experiments conducted on different real data sets show the effectiveness of IST-CRF. Source code of the proposed method will be made available at https://github.com/yulisun/IST-CRF .},
  archive      = {J_PR},
  author       = {Yuli Sun and Lin Lei and Dongdong Guan and Junzheng Wu and Gangyao Kuang},
  doi          = {10.1016/j.patcog.2022.108845},
  journal      = {Pattern Recognition},
  pages        = {108845},
  shortjournal = {Pattern Recognition},
  title        = {Iterative structure transformation and conditional random field based method for unsupervised multimodal change detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locality preserving projection with symmetric graph
embedding for unsupervised dimensionality reduction. <em>PR</em>,
<em>131</em>, 108844. (<a
href="https://doi.org/10.1016/j.patcog.2022.108844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preserving the intrinsic structure of data is very important for unsupervised dimensionality reduction. For structure preserving, graph embedding technique is widely considered. However, most of the existing unsupervised graph embedding based methods cannot effectively preserve the intrinsic structure of data since these methods either use the constant graph or only explore the geometric structure based on the distance information or representation information. To solve this problem, a novel method, called locality preserving projection with symmetric graph embedding (LPP_SGE), is proposed. LPP_SGE introduces a novel adaptive graph learning model and can obtain the intrinsic graph and projection in a unified framework by fully exploring the representation information and distance information of the original data. Different from the existing works which generally introduce no less than two constraints to capture the representation information and distance information, LPP_SGE can simultaneously capture the above two kinds of structure information in one term. Moreover, LPP_SGE introduces an ‘ l 2 , 1 l2,1 ’ norm based projection constraint to select the most discriminative features from the complex data for dimensionality reduction, such that the robustness is enhanced. Experimental results on four databases and two kinds of noisy databases show that LPP_SGE performs better than many well-known methods.},
  archive      = {J_PR},
  author       = {Xiaohuan Lu and Jiang Long and Jie Wen and Lunke Fei and Bob Zhang and Yong Xu},
  doi          = {10.1016/j.patcog.2022.108844},
  journal      = {Pattern Recognition},
  pages        = {108844},
  shortjournal = {Pattern Recognition},
  title        = {Locality preserving projection with symmetric graph embedding for unsupervised dimensionality reduction},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using global information to refine local patterns for
texture representation and classification. <em>PR</em>, <em>131</em>,
108843. (<a href="https://doi.org/10.1016/j.patcog.2022.108843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary pattern (LBP) and its variants have been successfully applied in texture feature extraction. However, it is hard for most LBP-based methods to effectively describe and distinguish the local neighborhoods with similar structures (that is, the calculated feature patterns are identical) but different contrasts or grayscales. To alleviate such problems, we propose a novel global refined local binary pattern (GRLBP) by analyzing the nature of pixel intensity distribution in local neighborhoods. GRLBP consists of two descriptors called magnitude refined local sign binary pattern (MRLBP_S) and center refined local magnitude binary pattern (CRLBP_M). MRLBP_S distinguishes local neighborhoods with contrast differences by using global magnitude anchors to refine local sign patterns. And CRLBP_M identifies local neighborhoods with grayscale differences by employing global central grayscale anchors to refine local magnitude patterns. Finally, frequency histograms of MRLBP_S and CRLBP_M from each image are cascaded to generate the GRLBP. Extensive experimental results on seven benchmark texture databases: Outex, CUReT, KTH-TIPS, UMD, UIUC, KTH-T2b, and DTD demonstrate that the proposed GRLBP can represent the detailed information of texture images. Furthermore, compared with state-of-the-art LBP variants, GRLBP has competitive advantages in classification accuracy, feature dimension, and computational complexity , respectively.},
  archive      = {J_PR},
  author       = {Xin Shu and Hui Pan and Jinlong Shi and Xiaoning Song and Xiao-Jun Wu},
  doi          = {10.1016/j.patcog.2022.108843},
  journal      = {Pattern Recognition},
  pages        = {108843},
  shortjournal = {Pattern Recognition},
  title        = {Using global information to refine local patterns for texture representation and classification},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hiding multiple images into a single image via joint
compressive autoencoders. <em>PR</em>, <em>131</em>, 108842. (<a
href="https://doi.org/10.1016/j.patcog.2022.108842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest in image hiding has been continually growing. Recently, deep learning-based image hiding approaches improve the hidden capacity significantly. However, the major challenges of the existing methods are that they are difficult to balance between the errors of the modified cover image and those of the recovered secret image . To solve this problem, in this paper, we develop an image hiding algorithm based on a joint compressive autoencoder framework. Further, we propose a novel strategy to enlarge the hidden capacity, i.e., hiding multi-images in one container image. Specifically, our approach provides an extremely high image hidden capacity coupled with small reconstruction errors of the secret image. More importantly, we tackle the trade-off problem of earlier approaches by mapping the image representations in the latent spaces of the joint compressive autoencoder models, leading to both high visual quality of the container image and low reconstruction error the secret image. In an extensive set of experiments, we confirm our proposed approach to outperform several state-of-the-art image hiding methods, yielding high imperceptibility and steganalysis resistance of the container images with high recovery quality of the secret images, while improving the image hidden capacity significantly (four times higher than full-image hiding capacity).},
  archive      = {J_PR},
  author       = {Xiyao Liu and Ziping Ma and Zhihong Chen and Fangfang Li and Ming Jiang and Gerald Schaefer and Hui Fang},
  doi          = {10.1016/j.patcog.2022.108842},
  journal      = {Pattern Recognition},
  pages        = {108842},
  shortjournal = {Pattern Recognition},
  title        = {Hiding multiple images into a single image via joint compressive autoencoders},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An entity-weights-based convolutional neural network for
large-sale complex knowledge embedding. <em>PR</em>, <em>131</em>,
108841. (<a href="https://doi.org/10.1016/j.patcog.2022.108841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications ( e.g. , risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing . With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1\% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.},
  archive      = {J_PR},
  author       = {Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar Ul Haq and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis},
  doi          = {10.1016/j.patcog.2022.108841},
  journal      = {Pattern Recognition},
  pages        = {108841},
  shortjournal = {Pattern Recognition},
  title        = {An entity-weights-based convolutional neural network for large-sale complex knowledge embedding},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video summarization with a convolutional attentive
adversarial network. <em>PR</em>, <em>131</em>, 108840. (<a
href="https://doi.org/10.1016/j.patcog.2022.108840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning . However, it is extremely expensive and difficult to construct a large-scale video summarization dataset through human annotation. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network , our overall framework consists of a generator and a discriminator . The former predicts importance scores for all the frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. To capture the global and local temporal relationship of video frames, the generator employs a fully convolutional sequence network to build global representation of a video, and an attention-based network to predict normalized importance scores. To optimize the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.},
  archive      = {J_PR},
  author       = {Guoqiang Liang and Yanbing Lv and Shucheng Li and Shizhou Zhang and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.108840},
  journal      = {Pattern Recognition},
  pages        = {108840},
  shortjournal = {Pattern Recognition},
  title        = {Video summarization with a convolutional attentive adversarial network},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised partial multi-label classification via
consistency learning. <em>PR</em>, <em>131</em>, 108839. (<a
href="https://doi.org/10.1016/j.patcog.2022.108839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning refers to the problem that each instance is associated with a candidate label set involving both relevant and noisy labels. Existing solutions mainly focus on label disambiguation, while ignoring the negative effect of the inconsistency between feature information and label information. Specifically, the existence of completely unlabeled instances makes the estimation of label co-occurrence difficult. To tackle these problems, we propose a novel framework for partial multi-label learning in semi-supervised scenarios by solving the inconsistency between features and labels. In the first stage, the label-level correlation matrix on both labeled and unlabeled instances is derived via Hilbert-Schmidt Independence Criterion (HSIC). The correlation matrix can characterize the label correlation of labeled instances and can propagate the label correlation of unlabeled instances. In the second stage, the proposed framework achieves the training of feature mapping, the recovery of ground-truth labels, and the alleviation of noisy labels in a mutually beneficial manner, and develops an alternative optimization procedure to optimize them. In addition, a nonlinear version is extended by using kernel trick. Experimental studies demonstrate that the proposed methods can achieve competitive superiority against existing well-established methods.},
  archive      = {J_PR},
  author       = {Anhui Tan and Jiye Liang and Wei-Zhi Wu and Jia Zhang},
  doi          = {10.1016/j.patcog.2022.108839},
  journal      = {Pattern Recognition},
  pages        = {108839},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised partial multi-label classification via consistency learning},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial life for segmentation of fusion ultrasound images
of breast abnormalities. <em>PR</em>, <em>131</em>, 108838. (<a
href="https://doi.org/10.1016/j.patcog.2022.108838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of cancerous tumors in ultrasound (US) images of human organs is one of the critical problems in medical imaging . The US images are characterized by low contrast, irregular shapes, high levels of speckle-noise and acoustic shadows, making it difficult to segment the tumor. Yet, US imaging is considered one of the most inexpensive and safe imaging tests available to detect cancer in its early stages. However, an automatic segmentation method applicable to all types of US imagery does not exist. This paper proposes a novel segmentation method that combines image fusion, artificial life (AL) and a genetic algorithm (GA). The new algorithm has been applied to US images of breast cancer. The method is based on tracing agents (TA), which are artificial organisms with memory and the ability to communicate. They live inside a fusion image generated from the US and the elastography (EL) images. The TA can recognize the patterns of strong edges and boundary gaps allowing to outline the tumor. The new model has been tested against six types of segmentation models , i.e., machine learning , active contours , level set models, superpixel models, edge linking models and selected hybrid methods . The experiments include 16 state-of-the-art methods, which outperform 69 recent and classical segmentation routines. The tests were run on 395 breast cancer images from http://onlinemedicalimages.com and https://www.ultrasoundcases.info/ . TA training employs a GA. The model has been verified on “hard” cases (complex shapes, boundary leakage, and noisy edge maps). The proposed algorithm produces more accurate results than the reference methods on high complexity images. A video demo of the algorithm is at http://shorturl.at/htBW9 .},
  archive      = {J_PR},
  author       = {Nalan Karunanayake and Wanrudee Lohitvisate and Stanislav S. Makhanov},
  doi          = {10.1016/j.patcog.2022.108838},
  journal      = {Pattern Recognition},
  pages        = {108838},
  shortjournal = {Pattern Recognition},
  title        = {Artificial life for segmentation of fusion ultrasound images of breast abnormalities},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). To actively initialize active learning. <em>PR</em>,
<em>131</em>, 108836. (<a
href="https://doi.org/10.1016/j.patcog.2022.108836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though much effort has been spent on designing new active learning algorithms, little attention has been paid to the initialization problem of active learning, i.e., how to find a set of labeled samples which contains at least one instance per category. This work identifies the initialization of active learning as a separate and novel research problem, reviews existing methods that can be adapted to be used for this task and, in addition, proposes a new active initialization criterion: the Nearest Neighbor Criterion. Experiments on 16 benchmark datasets verify that the novel method often finds an initialization set with fewer queried samples than other methods do.},
  archive      = {J_PR},
  author       = {Yazhou Yang and Marco Loog},
  doi          = {10.1016/j.patcog.2022.108836},
  journal      = {Pattern Recognition},
  pages        = {108836},
  shortjournal = {Pattern Recognition},
  title        = {To actively initialize active learning},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial feature mapping for 6DoF object pose estimation.
<em>PR</em>, <em>131</em>, 108835. (<a
href="https://doi.org/10.1016/j.patcog.2022.108835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objects’ points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Jianhan Mei and Xudong Jiang and Henghui Ding},
  doi          = {10.1016/j.patcog.2022.108835},
  journal      = {Pattern Recognition},
  pages        = {108835},
  shortjournal = {Pattern Recognition},
  title        = {Spatial feature mapping for 6DoF object pose estimation},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical protection against video data leakage via
universal adversarial head. <em>PR</em>, <em>131</em>, 108834. (<a
href="https://doi.org/10.1016/j.patcog.2022.108834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. A snoop can collect more users’ private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected. We propose universal adversarial head (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be generated only with a few natural videos, and mislead the retrieval system to return irrelevant videos when it is applied to most query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to users’ original videos. Extensive experiments demonstrate the effectiveness of our method in misleading deep video hashing under both white-box and black-box settings.},
  archive      = {J_PR},
  author       = {Jiawang Bai and Bin Chen and Kuofeng Gao and Xuan Wang and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2022.108834},
  journal      = {Pattern Recognition},
  pages        = {108834},
  shortjournal = {Pattern Recognition},
  title        = {Practical protection against video data leakage via universal adversarial head},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cyclical adversarial attack pierces black-box deep neural
networks. <em>PR</em>, <em>131</em>, 108831. (<a
href="https://doi.org/10.1016/j.patcog.2022.108831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have shown vulnerability to adversarial attacks . By exploiting the transferability of adversarial examples , attackers can fool models under black-box settings without accessing the underlying information. However, they often exhibit weak performance when transferring to defenses, which may give a false sense of security. In this paper, we propose Cyclical Adversarial Attack (CA 2 2 ), a general and straightforward method to boost the transferability to break defenders. We first revisit the momentum-based methods from the perspective of optimization and find that they usually suffer from the transferability saturation dilemma. To address this, CA 2 2 performs cyclical optimization algorithm to produce adversarial examples. Unlike the standard momentum policy that accumulates the velocity to continuously update the solution, we divide the generation process into multiple phases and treat the velocity vectors from the previous phase as proper knowledge to guide a new adversarial attack with larger steps. Moreover, CA 2 2 applies a novel and compatible augmentation algorithm at every optimization in a loop manner for enhancing the black-box transferability further, referred to as cyclical augmentation . Extensive experiments conducted on a variety of models not only validate the efficacy of each designed algorithm in CA 2 2 , but also illustrate the superiority of our method compared with the state-of-the-art transferable attacks. Our implemental code is publicly available at https://github.com/mesunhlf/CA2.},
  archive      = {J_PR},
  author       = {Lifeng Huang and Shuxin Wei and Chengying Gao and Ning Liu},
  doi          = {10.1016/j.patcog.2022.108831},
  journal      = {Pattern Recognition},
  pages        = {108831},
  shortjournal = {Pattern Recognition},
  title        = {Cyclical adversarial attack pierces black-box deep neural networks},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous representation learning and matching for
few-shot relation prediction. <em>PR</em>, <em>131</em>, 108830. (<a
href="https://doi.org/10.1016/j.patcog.2022.108830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent explosive development of knowledge graphs (KGs) in artificial intelligence tasks coupled with incomplete or partial information has triggered considerable research interest in relation prediction. However, many challenges still remain unsolved: (i) the previous relation prediction methods require a significant amount of training instances (i.e., head-tail entity pairs) for every relation, which is infeasible in practical scenarios; and (ii) the representation learning of entities and relations always assumes that all local neighbors and their features contribute equally to the embedding, not sufficiently considering the heterogeneity of the information; and (iii) the state-of-the-art methods usually require a lot of training time, resulting in a high cost in real-world applications. To overcome these challenges, we propose a heterogeneous representation learning and matching approach, Multi-metric Feature Extraction Network (MFEN for short), for few-shot relation prediction in KGs. Our method focuses on knowledge graphs to sufficiently explore the topological structure and node content in graphs. Rather than taking the average of the embeddings of all relational neighbors, a heterogeneity-aware representation learning method is proposed to generate high-expressive embeddings, which capture the heterogenous roles of the relational neighbors of given entity and all of their features via a convolutional encoder . To learn the expressive representations efficiently, a single-layer CNN architecture with multi-scale filters is devised. In addition, multiple heuristic metrics are combined to efficiently improve the accuracy of similarity calculation. The proposed MFEN model is evaluated on two representative benchmark datasets NELL and Wiki. Extensive experiments have demonstrated that our method gets more than 5\%\% accuracy improvement and three times speedup to state-of-the-art models. Code is available on https://github.com/summer-funny/MFEN .},
  archive      = {J_PR},
  author       = {Tao Wu and Hongyu Ma and Chao Wang and Shaojie Qiao and Liang Zhang and Shui Yu},
  doi          = {10.1016/j.patcog.2022.108830},
  journal      = {Pattern Recognition},
  pages        = {108830},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous representation learning and matching for few-shot relation prediction},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covid-MANet: Multi-task attention network for explainable
diagnosis and severity assessment of COVID-19 from CXR images.
<em>PR</em>, <em>131</em>, 108826. (<a
href="https://doi.org/10.1016/j.patcog.2022.108826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The devastating outbreak of Coronavirus Disease (COVID-19) cases in early 2020 led the world to face health crises. Subsequently, the exponential reproduction rate of COVID-19 disease can only be reduced by early diagnosis of COVID-19 infection cases correctly. The initial research findings reported that radiological examinations using CT and CXR modality have successfully reduced false negatives by RT-PCR test. This research study aims to develop an explainable diagnosis system for the detection and infection region quantification of COVID-19 disease. The existing research studies successfully explored deep learning approaches with higher performance measures but lacked generalization and interpretability for COVID-19 diagnosis. In this study, we address these issues by the Covid-MANet network, an automated end-to-end multi-task attention network that works for 5 classes in three stages for COVID-19 infection screening. The first stage of the Covid-MANet network localizes attention of the model to the relevant lungs region for disease recognition. The second stage of the Covid-MANet network differentiates COVID-19 cases from bacterial pneumonia, viral pneumonia, normal and tuberculosis cases, respectively. To improve the interpretation and explainability, three experiments have been conducted in exploration of the most coherent and appropriate classification approach. Moreover, the multi-scale attention model MA-DenseNet201 proposed for the classification of COVID-19 cases. The final stage of the Covid-MANet network quantifies the proportion of infection and severity of COVID-19 in the lungs. The COVID-19 cases are graded into more specific severity levels such as mild, moderate, severe, and critical as per the score assigned by the RALE scoring system. The MA-DenseNet201 classification model outperforms eight state-of-the-art CNN models, in terms of sensitivity and interpretation with lung localization network. The COVID-19 infection segmentation by UNet with DenseNet121 encoder achieves dice score of 86.15\% outperforming UNet, UNet++, AttentionUNet, R2UNet, with VGG16, ResNet50 and DenseNet201 encoder. The proposed network not only classifies images based on the predicted label but also highlights the infection by segmentation/localization of model-focused regions to support explainable decisions. MA-DenseNet201 model with a segmentation-based cropping approach achieves maximum interpretation of 96\% with COVID-19 sensitivity of 97.75\%. Finally, based on class-varied sensitivity analysis Covid-MANet ensemble network of MA-DenseNet201, ResNet50 and MobileNet achieve 95.05\% accuracy and 98.75\% COVID-19 sensitivity. The proposed model is externally validated on an unseen dataset, yields 98.17\% COVID-19 sensitivity.},
  archive      = {J_PR},
  author       = {Ajay Sharma and Pramod Kumar Mishra},
  doi          = {10.1016/j.patcog.2022.108826},
  journal      = {Pattern Recognition},
  pages        = {108826},
  shortjournal = {Pattern Recognition},
  title        = {Covid-MANet: Multi-task attention network for explainable diagnosis and severity assessment of COVID-19 from CXR images},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale attention-based pseudo-3D convolution neural
network for alzheimer’s disease diagnosis using structural MRI.
<em>PR</em>, <em>131</em>, 108825. (<a
href="https://doi.org/10.1016/j.patcog.2022.108825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based Computer-Aided Diagnosis methods have been widely utilized due to their highly effective diagnosis of patients. Although Convolutional Neural Networks (CNNs) are capable of extracting the latent structural characteristics of dementia and of capturing the changes of brain anatomy in Magnetic Resonance Imaging (MRI) scans, the high-dimensional input to a deep CNN usually makes the network difficult to train, and affects its diagnostic accuracy . In this paper, a novel method called the hierarchical pseudo-3D convolution neural network based on a kernel attention mechanism with a new global context block, which is abbreviated as “PKG-Net”, is proposed to accurately predict Alzheimer’s disease even when the input features are complex. Specifically, the proposed network first extracts multi-scale features from pre-processed images. Second, the attention mechanism and global context blocks are applied to combine features from different layers to hierarchically transform the MRI into more compact high-level features. Then, a joint loss function is used to train the proposed network to generate more distinguishing features, which improve the generalization performance of the network. In addition, we combine our method with different architectures. Extensive experiments are conducted to analyze the performance of the PKG-Net with different hyper-parameters and architectures. Finally, in order to verify the effectiveness of our method on Alzheimer’s disease diagnosis, we carry out extensive experiments on the ADNI dataset, and compare the results of our method with that of existing methods in terms of accuracy, recall and precision. Furthermore, our network can fully take advantage of the deep 3D convolutional neural network for automatic feature extraction and representation, and thus can avoid the limitation of low processing efficiency caused by the preprocessing procedure in which a specific area needs to be annotated in advance. Finally, we evaluate our proposed framework using two public datasets, ADNI-1 and ADNI-2, and the experimental results show that our proposed framework can achieve superior performance over state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Zhao Pei and Zhiyang Wan and Yanning Zhang and Miao Wang and Chengcai Leng and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2022.108825},
  journal      = {Pattern Recognition},
  pages        = {108825},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale attention-based pseudo-3D convolution neural network for alzheimer’s disease diagnosis using structural MRI},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-granularity episodic contrastive learning for few-shot
learning. <em>PR</em>, <em>131</em>, 108820. (<a
href="https://doi.org/10.1016/j.patcog.2022.108820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims at fast adaptation to novel classes with few training samples. Among FSL methods, meta-learning and transfer learning-based methods are the most powerful ones. However, most of them rely to some extent on cross-entropy loss, which leads to representations that are overly concerned with the classes already seen, and in turn leads to sub-optimal generalization on novel classes. In this study, we are inspired by meta-learning and transfer learning-based methods and believe good feature representations are vital for FSL. To this end, we propose a new multi-granularity episodic contrastive learning method (MGECL) that introduces contrastive learning into the episode training process. In particular, by enforcing our proposed contrastive loss on both class and instance granularities, the model is able to extract category-independent discriminative patterns and learn richer and more transferable feature representations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three popular few-shot benchmarks. Our code is available at https://github.com/z1358/MGECL_PR.},
  archive      = {J_PR},
  author       = {Pengfei Zhu and Zhilin Zhu and Yu Wang and Jinglin Zhang and Shuai Zhao},
  doi          = {10.1016/j.patcog.2022.108820},
  journal      = {Pattern Recognition},
  pages        = {108820},
  shortjournal = {Pattern Recognition},
  title        = {Multi-granularity episodic contrastive learning for few-shot learning},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient federated multi-view learning. <em>PR</em>,
<em>131</em>, 108817. (<a
href="https://doi.org/10.1016/j.patcog.2022.108817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning aims to explore a global common structure shared by different views collected from multiple individual sources. The nascent field of federated learning tries to learn a global model over distributed networks of devices. This paper shows that multi-view learning is naturally suited to address the feature heterogeneity of the federated setting. We propose a novel model, namely robust federated multi-view learning (FedMVL), which is considered in the following formulation: given a dataset with M M views, it is required to train machine learning models while the M M views are distributed across M M devices or nodes. Considering the unique challenges like stragglers and fault tolerance in federated setting, we derive an iterative federated optimization algorithm that allows each node with the flexibility to approximately address its subproblem . To the best of our knowledge, our model for the first time considers the issues including high communication cost, fault tolerance , and stragglers for distributed multi-view learning. The proposed model also achieves encouraging performance on clustering task compared to closely related methods, as we illustrate through simulations on several real-world datasets.},
  archive      = {J_PR},
  author       = {Shudong Huang and Wei Shi and Zenglin Xu and Ivor W. Tsang and Jiancheng Lv},
  doi          = {10.1016/j.patcog.2022.108817},
  journal      = {Pattern Recognition},
  pages        = {108817},
  shortjournal = {Pattern Recognition},
  title        = {Efficient federated multi-view learning},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BRULÈ: Barycenter-regularized unsupervised landmark
extraction. <em>PR</em>, <em>131</em>, 108816. (<a
href="https://doi.org/10.1016/j.patcog.2022.108816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models .},
  archive      = {J_PR},
  author       = {Iaroslav Bespalov and Nazar Buzun and Dmitry V. Dylov},
  doi          = {10.1016/j.patcog.2022.108816},
  journal      = {Pattern Recognition},
  pages        = {108816},
  shortjournal = {Pattern Recognition},
  title        = {BRULÈ: Barycenter-regularized unsupervised landmark extraction},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-layer manifold learning for deep non-negative matrix
factorization-based multi-view clustering. <em>PR</em>, <em>131</em>,
108815. (<a href="https://doi.org/10.1016/j.patcog.2022.108815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data clustering based on Non-negative Matrix Factorization (NMF) has been commonly used for pattern recognition by grouping multi-view high-dimensional data by projecting it to a lower-order dimensional space. However, the NMF framework fails to learn the accurate lower-order representation of the input data if it exhibits complex and non-linear relationships. This paper proposes a deep non-negative matrix factorization-based framework for effective multi-view data clustering by uncovering both the non-linear relationships and the intrinsic components of the data. Both the consensus and complementary information present in multiple views are sufficiently learned in the proposed framework with the effective use of constraints such as normalized cut-type and orthogonal. The optimal manifold of multi-view data is effectively incorporated in all layers of the framework. Extensive experimental results show the proposed method outperforms state-of-the-art multi-view matrix factorization-based methods.},
  archive      = {J_PR},
  author       = {Khanh Luong and Richi Nayak and Thirunavukarasu Balasubramaniam and Md Abul Bashar},
  doi          = {10.1016/j.patcog.2022.108815},
  journal      = {Pattern Recognition},
  pages        = {108815},
  shortjournal = {Pattern Recognition},
  title        = {Multi-layer manifold learning for deep non-negative matrix factorization-based multi-view clustering},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). YOLO-anti: YOLO-based counterattack model for unseen
congested object detection. <em>PR</em>, <em>131</em>, 108814. (<a
href="https://doi.org/10.1016/j.patcog.2022.108814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is advancing rapidly with the development of deep learning solutions and big data dimensions. This paper takes the challenging recognition task as the core work and proposes a novel and efficient network framework dedicated to unseen congestion detection. To guarantee the accuracy as well as the speed of inference, the detector utilizes the advanced You Only Look Once v4 (YOLOv4) as the backbone and agglutinates the four proposed strategies, called YOLO-Anti. Our model mainly consists of three modules: First, an adaptive context module similar to valve control is proposed to obtain contextual information that balances foreground and background features. Second, to solve the problem that the imbalance between feature levels weakens the detection performance, a balanced prediction layer method is developed. Finally, we propose an anti-congestion network to selectively expand the local domain to achieve finer-grained detection. Besides, in the training procedure, a designed heterogeneous cross-entropy loss is utilized to strengthen the detector’s discrimination of similar targets in different categories. Extensive experiments were conducted on the PASCAL VOC, COCO, and UA-DETRAC data sets. The state-of-the-art results were achieved on UA-DETRAC and the leading performance on PASCAL VOC and COCO. Also, compared with baseline YOLOv4, the proposed method brings significant accuracy improvement and negligible time consumption.},
  archive      = {J_PR},
  author       = {Kun Wang and Maozhen Liu},
  doi          = {10.1016/j.patcog.2022.108814},
  journal      = {Pattern Recognition},
  pages        = {108814},
  shortjournal = {Pattern Recognition},
  title        = {YOLO-anti: YOLO-based counterattack model for unseen congested object detection},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple-solutions RANSAC for finding axes of symmetry in
fragments of objects. <em>PR</em>, <em>131</em>, 108805. (<a
href="https://doi.org/10.1016/j.patcog.2022.108805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of “finding best lines passing through a set of straight lines” has appeared in applications such as archaeological pottery analysis, precision manufacturing, and 3D modelling. In these applications, an instance of this problem is finding the symmetry axis of a symmetrical object from a set of its surface normal lines. We show that the mentioned instance of the problem may have two meaningful local minima, one of which is the symmetry axis, a fact that has been neglected in the literature. A multiple-solutions RANSAC algorithm is proposed for finding initial estimates of both local minima in the presence of outliers. Then, a coordinate-descent algorithm is presented that starts from these initial estimates and finds the local minima of the problem. The proposed coordinate-descent method does not involve any line search procedure, and its convergence is guaranteed. We also provide a proof for the rate of the convergence.},
  archive      = {J_PR},
  author       = {Seyed-Mahdi Nasiri and Reshad Hosseini and Hadi Moradi},
  doi          = {10.1016/j.patcog.2022.108805},
  journal      = {Pattern Recognition},
  pages        = {108805},
  shortjournal = {Pattern Recognition},
  title        = {Multiple-solutions RANSAC for finding axes of symmetry in fragments of objects},
  volume       = {131},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal channel-wise attention transformer inspired by
multisensory integration mechanisms of the brain. <em>PR</em>,
<em>130</em>, 108837. (<a
href="https://doi.org/10.1016/j.patcog.2022.108837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisensory integration has attracted intense studies for decades. How to combine visual and auditory information to optimize perception and decision-making is a key question in neuroscience as well as machine learning . Inspired by the mechanisms of multisensory integration in the brain, we propose a multimodal channel-wise attention transformer (MCAT) that performs reliability-weighted integration and revises the weights allocation according to a top-down attention-like mechanism. We apply MCAT on EF-LSTM neural networks for a fine-grained video bird recognition task, and on MulT neural networks for an emotion recognition task. The performance of both models is improved remarkably. Ablation study shows that the attention mechanism is indispensable for effective multisensory integration. Moreover, we found that cross-modal integration models are in accordance with the law of inverse effectiveness of multisensory integration in the brain, which reveals that our model may have mechanisms similar to those in the brain. Taken together, the results demonstrate that the brain-inspired MCAT block is effective for improving multisensory integration, providing useful clues for designing new algorithms and understanding multisensory integration in the brain.},
  archive      = {J_PR},
  author       = {Qianqian Shi and Junsong Fan and Zuoren Wang and Zhaoxiang Zhang},
  doi          = {10.1016/j.patcog.2022.108837},
  journal      = {Pattern Recognition},
  pages        = {108837},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Visual-to-EEG cross-modal knowledge distillation for
continuous emotion recognition. <em>PR</em>, <em>130</em>, 108833. (<a
href="https://doi.org/10.1016/j.patcog.2022.108833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual modality is one of the most dominant modalities for current continuous emotion recognition methods. Compared to which the EEG modality is relatively less sound due to its intrinsic limitation such as subject bias and low spatial resolution. This work attempts to improve the continuous prediction of the EEG modality by using the dark knowledge from the visual modality. The teacher model is built by a cascade convolutional neural network - temporal convolutional network (CNN-TCN) architecture, and the student model is built by TCNs. They are fed by video frames and EEG average band power features, respectively. Two data partitioning schemes are employed, i.e., the trial-level random shuffling (TRS) and the leave-one-subject-out (LOSO). The standalone teacher and student can produce continuous prediction superior to the baseline method , and the employment of the visual-to-EEG cross-modal KD further improves the prediction with statistical significance, i.e., p p -value &amp;lt;0.01 for TRS and p p -value &amp;lt;0.05 for LOSO partitioning. The saliency maps of the trained student model show that the brain areas associated with the active valence state are not located in precise brain areas. Instead, it results from synchronized activity among various brain areas. And the fast beta and gamma waves, with the frequency of 18 − 30 H z 18−30Hz and 30 − 45 H z 30−45Hz , contribute the most to the human emotion process compared to other bands. The code is available at https://github.com/sucv/Visual_to_EEG_Cross_Modal_KD_for_CER .},
  archive      = {J_PR},
  author       = {Su Zhang and Chuangao Tang and Cuntai Guan},
  doi          = {10.1016/j.patcog.2022.108833},
  journal      = {Pattern Recognition},
  pages        = {108833},
  shortjournal = {Pattern Recognition},
  title        = {Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning a deep dual-level network for robust DeepFake
detection. <em>PR</em>, <em>130</em>, 108832. (<a
href="https://doi.org/10.1016/j.patcog.2022.108832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face manipulation techniques, especially DeepFake techniques, are causing severe social concerns and security problems. When faced with skewed data distributions such as those found in the real world, existing DeepFake detection methods exhibit significantly degraded performance, especially the AUC score. In this paper, we focus on DeepFake detection in real-world situations. We propose a dual-level collaborative framework to detect frame-level and video-level forgeries simultaneously with a joint loss function to optimize both the AUC score and error rate at the same time. Our experiments indicate that the AUC loss boosts imbalanced learning performance and outperforms focal loss, a state-of-the-art loss function to address imbalanced data . In addition, our multitask structure enables mutual reinforcement of frame-level and video-level detection and achieves outstanding performance in imbalanced learning. Our proposed method is also more robust to video quality variations and shows better generalization ability in cross-dataset evaluations than existing DeepFake detection methods. Our implementation is available online at https://github.com/PWB97/Deepfake-detection .},
  archive      = {J_PR},
  author       = {Wenbo Pu and Jing Hu and Xin Wang and Yuezun Li and Shu Hu and Bin Zhu and Rui Song and Qi Song and Xi Wu and Siwei Lyu},
  doi          = {10.1016/j.patcog.2022.108832},
  journal      = {Pattern Recognition},
  pages        = {108832},
  shortjournal = {Pattern Recognition},
  title        = {Learning a deep dual-level network for robust DeepFake detection},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CVM-cervix: A hybrid cervical pap-smear image classification
framework using CNN, visual transformer and multilayer perceptron.
<em>PR</em>, <em>130</em>, 108829. (<a
href="https://doi.org/10.1016/j.patcog.2022.108829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is the seventh most common cancer among all the cancers worldwide and the fourth most common cancer among women. Cervical cytopathology image classification is an important method to diagnose cervical cancer. However, manual inspection is very troublesome, and experts are prone to make mistakes. The emergence of the automatic computer-aided diagnosis system solves this problem. This paper proposes a framework called CVM-Cervix based on deep learning to perform cervical cell classification tasks . It can analyze pap slides quickly and accurately. CVM-Cervix first proposes a Convolutional Neural Network module and a Visual Transformer module for local and global feature extraction respectively, then a Multilayer Perceptron module is designed to fuse the local and global features for the final classification. Experimental results show the effectiveness and potential of the proposed CVM-Cervix in the field of cervical Pap smear image classification . In addition, according to the practical needs of clinical work, we perform a lightweight post-processing to compress the model.},
  archive      = {J_PR},
  author       = {Wanli Liu and Chen Li and Ning Xu and Tao Jiang and Md Mamunur Rahaman and Hongzan Sun and Xiangchen Wu and Weiming Hu and Haoyuan Chen and Changhao Sun and Yudong Yao and Marcin Grzegorzek},
  doi          = {10.1016/j.patcog.2022.108829},
  journal      = {Pattern Recognition},
  pages        = {108829},
  shortjournal = {Pattern Recognition},
  title        = {CVM-cervix: A hybrid cervical pap-smear image classification framework using CNN, visual transformer and multilayer perceptron},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification for high-dimension low-sample size data.
<em>PR</em>, <em>130</em>, 108828. (<a
href="https://doi.org/10.1016/j.patcog.2022.108828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimension and low-sample-size (HDLSS) data sets have posed great challenges to many machine learning methods. To deal with practical HDLSS problems, development of new classification techniques is highly desired. After the cause of the over-fitting phenomenon is identified, a new classification criterion for HDLSS data sets, termed tolerance similarity, is proposed to emphasize maximization of within-class variance on the premise of class separability. Leveraging on this criterion, a novel linear binary classifier , termed No-separated Data Maximum Dispersion classifier (NPDMD), is designed. The main idea of the NPDMD is to spread samples of two classes in a large interval in the respective positive or negative space along the projecting direction when the distance between the projection means for two classes is large enough. The salient features of the proposed NPDMD are: (1) The NPDMD operates well on HDLSS data sets; (2) The NPDMD solves the objective function in the entire feature space to avoid the data-piling phenomenon. (3) The NPDMD leverages on the low-rank property of the covariance matrix for HDLSS data sets to accelerate the computation speed. (4) The NPDMD is suitable for different real-word applications. (5) The NPDMD can be implemented readily using Quadratic Programming. Not only theoretical properties of the NPDMD have been derived, but also a series of evaluations have been conducted on one simulated and six real-world benchmark data sets, including face classification and mRNA classification. Experimental results and comprehensive studies demonstrate the superiority of the NPDMD in terms of correct classification rate, mean within-group correct classification rate and the area under the ROC curve.},
  archive      = {J_PR},
  author       = {Liran Shen and Meng Joo Er and Qingbo Yin},
  doi          = {10.1016/j.patcog.2022.108828},
  journal      = {Pattern Recognition},
  pages        = {108828},
  shortjournal = {Pattern Recognition},
  title        = {Classification for high-dimension low-sample size data},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GasHis-transformer: A multi-scale visual transformer
approach for gastric histopathological image detection. <em>PR</em>,
<em>130</em>, 108827. (<a
href="https://doi.org/10.1016/j.patcog.2022.108827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&amp;E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.},
  archive      = {J_PR},
  author       = {Haoyuan Chen and Chen Li and Ge Wang and Xiaoyan Li and Md Mamunur Rahaman and Hongzan Sun and Weiming Hu and Yixin Li and Wanli Liu and Changhao Sun and Shiliang Ai and Marcin Grzegorzek},
  doi          = {10.1016/j.patcog.2022.108827},
  journal      = {Pattern Recognition},
  pages        = {108827},
  shortjournal = {Pattern Recognition},
  title        = {GasHis-transformer: A multi-scale visual transformer approach for gastric histopathological image detection},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model scheduling and sample selection for ensemble
adversarial example attacks. <em>PR</em>, <em>130</em>, 108824. (<a
href="https://doi.org/10.1016/j.patcog.2022.108824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples refer to the malicious inputs that can mislead deep neural networks (DNNs) to falsely classify them. In practice, some adversarial examples are transferable and hence can deceive different target models. In multi-stage ensemble adversarial example attacks, adversaries can generate strongly transferable adversarial examples through iteratively perturbing legitimate examples to attack well-trained source models in a white-box manner. Limited by computational and memory resources (e.g., GPU memory), however, adversaries cannot handle all models and all legitimate examples at a time. This brings an important but never studied research issue: how to optimally schedule source models and appropriately select samples to improve adversarial example transferability and reduce unnecessary computational overheads? To shed light on this problem, we develop a novel multi-stage ensemble adversarial example attack method based on our proposed strategies of model scheduling and sample selection. The first strategy schedules source models to be attacked in every stage, based on the criteria of decision boundary similarity and model diversity. The second selects input samples to be handled by ensemble attacks, according to their sensitivity level for adversarial perturbations. To our knowledge, we are the first to study model scheduling and sample selection for multi-stage ensemble attacks. We conduct extensive experiments on three datasets with a variety of source and target models. Experiments show that our model scheduling based ensemble attack outperforms the all-model ensemble attack and the state-of-the-art ensemble attacks SCES, SMBEA and EnsembleFool in transferability. Moreover, our sample selection strategy improves attack success rate by about 138\% 138\% .},
  archive      = {J_PR},
  author       = {Zichao Hu and Heng Li and Liheng Yuan and Zhang Cheng and Wei Yuan and Ming Zhu},
  doi          = {10.1016/j.patcog.2022.108824},
  journal      = {Pattern Recognition},
  pages        = {108824},
  shortjournal = {Pattern Recognition},
  title        = {Model scheduling and sample selection for ensemble adversarial example attacks},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric cross–modal hashing with high–level semantic
similarity. <em>PR</em>, <em>130</em>, 108823. (<a
href="https://doi.org/10.1016/j.patcog.2022.108823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing aims at using modality content to retrieve semantically relevant objects of different modalities, so cross-modal retrieval has attracted much attention. To effectively exploit the discriminative label information and retain more semantic information in the process of hash learning, we propose a novel cross-modal hashing method , named high-level semantic similarity analysis hashing (HSSAH) for cross-modal retrieval. To reduce time complexity and enhance discriminant ability in hash codes, HSSAH constructs an asymmetric high-level semantic similarity learning framework to replace the binary semantic similarity matrix . Moreover, the developed HSSAH is a two-stage approach, and a semantic-enhanced scheme is proposed in the second stage, which fully leverages the label information to gain more powerful hash functions. We conducted comprehensive experiments on three benchmark datasets to evaluate the performance of HSSAH. Experimental results show that HSSAH can achieve significantly better retrieval precision and outperforms several state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Fan Yang and Yufeng Liu and Xiaojian Ding and Fumin Ma and Jie Cao},
  doi          = {10.1016/j.patcog.2022.108823},
  journal      = {Pattern Recognition},
  pages        = {108823},
  shortjournal = {Pattern Recognition},
  title        = {Asymmetric cross–modal hashing with high–level semantic similarity},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-frame spatio-temporal feature modulation for video
enhancement. <em>PR</em>, <em>130</em>, 108822. (<a
href="https://doi.org/10.1016/j.patcog.2022.108822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current video enhancement approaches have achieved good performance in specific rainy, hazy, foggy, and snowy weather conditions. However, they currently suffer from two important limitations. First, they can only handle degradation caused by single weather. Second, they use large, complex models with 10–50 millions of parameters needing high computing resources. As video enhancement is a pre-processing step for applications like video surveillance, traffic monitoring, autonomous driving , etc. , it is necessary to have a lightweight enhancement module . Therefore, we propose a dual-frame spatio-temporal feature modulation architecture to handle the degradation caused by diverse weather conditions. The proposed architecture combines the concept of spatio-temporal multi-resolution feature modulation with a multi-receptive parallel encoders and domain-based feature filtering modules to learn domain-specific features. Further, the architecture provides temporal consistency with recurrent feature merging, achieved by providing feedback of the previous frame output. The indoor (REVIDE, NYUDepth), synthetically generated outdoor weather degraded video de-hazing, and de-raining with veiling effect databases are used for experimentation. Also, the performance of the proposed method is analyzed for night-time de-hazing and de-raining with veiling effect weather conditions. Experimental results show the superior performance of our framework compared to existing state-of-the-art methods used for video de-hazing ( indoor/outdoor ) and de-raining with veiling effect weather conditions. The code is available at https://github.com/pwp1208/PR2022},
  archive      = {J_PR},
  author       = {Prashant W. Patil and Sunil Gupta and Santu Rana and Svetha Venkatesh},
  doi          = {10.1016/j.patcog.2022.108822},
  journal      = {Pattern Recognition},
  pages        = {108822},
  shortjournal = {Pattern Recognition},
  title        = {Dual-frame spatio-temporal feature modulation for video enhancement},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSRML: A latent space regularization based meta-learning
framework for MR image segmentation. <em>PR</em>, <em>130</em>, 108821.
(<a href="https://doi.org/10.1016/j.patcog.2022.108821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sources for medical image segmentation can be quite extensive, and models trained with data from a source domain may perform poorly on data from the target domain owing to domain shift issues. To overcome the impact of domain shift, we propose a novel meta-learning-based multi-source domain adaptation framework for medical image segmentation . Specifically, we designed a domain discriminator module to produce category prediction over the latent features, and an image reconstruction module to reconstruct the foreground and background of the target domain image separately. Furthermore, we constructed a large-scale multi-modal prostate dataset, which contained 495,902 magnetic resonance images of 419 cases, with prostate and lesion masks, as well as diagnostic descriptions for each patient. We evaluated our proposed method through extensive experiments using the proposed and the benchmark datasets. Experimental results show that our model achieves better segmentation and generalization performance compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Bo Zhang and Yunpeng Tan and Hui Wang and Zheng Zhang and Xiuzhuang Zhou and Jingyun Wu and Yue Mi and Haiwen Huang and Wendong Wang},
  doi          = {10.1016/j.patcog.2022.108821},
  journal      = {Pattern Recognition},
  pages        = {108821},
  shortjournal = {Pattern Recognition},
  title        = {LSRML: A latent space regularization based meta-learning framework for MR image segmentation},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards lifelong object recognition: A dataset and
benchmark. <em>PR</em>, <em>130</em>, 108819. (<a
href="https://doi.org/10.1016/j.patcog.2022.108819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong learning algorithms aim to enable robots to handle open-set and detrimental conditions, and yet there is a lack of adequate datasets with diverse factors for benchmarking. In this work, we constructed and released a lifelong learning robotic vision dataset, OpenLORIS-Object. This dataset was collected by RGB-D camera capturing dynamic environment in daily life scenarios with diverse factors, including illumination, occlusion, object pixel size and clutter, of quantified difficulty levels. To the best of our knowledge, this is an unique real-world dataset for robotic vision with independent and quantifiable environmental factors, which are currently unaccounted for in other lifelong learning datasets such as CORe50 and NICO. We tested 9 state-of-the-art algorithms with 4 evaluation metrics over the dataset in Domain Incremental Learning, Task Incremental Learning, and Class Incremental Learning scenarios. The results demonstrate that these existing algorithms are insufficient to handle lifelong learning task in dynamic environments. Our dataset and benchmarks are now publicly available at this website . 2},
  archive      = {J_PR},
  author       = {Chuanlin Lan and Fan Feng and Qi Liu and Qi She and Qihan Yang and Xinyue Hao and Ivan Mashkin and Ka Shun Kei and Dong Qiang and Vincenzo Lomonaco and Xuesong Shi and Zhengwei Wang and Yao Guo and Yimin Zhang and Fei Qiao and Rosa H.M. Chan},
  doi          = {10.1016/j.patcog.2022.108819},
  journal      = {Pattern Recognition},
  pages        = {108819},
  shortjournal = {Pattern Recognition},
  title        = {Towards lifelong object recognition: A dataset and benchmark},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hippocampus-heuristic character recognition network for
zero-shot learning in chinese character recognition. <em>PR</em>,
<em>130</em>, 108818. (<a
href="https://doi.org/10.1016/j.patcog.2022.108818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93\% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25\% on recognizing unseen characters.},
  archive      = {J_PR},
  author       = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
  doi          = {10.1016/j.patcog.2022.108818},
  journal      = {Pattern Recognition},
  pages        = {108818},
  shortjournal = {Pattern Recognition},
  title        = {Hippocampus-heuristic character recognition network for zero-shot learning in chinese character recognition},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Discriminative and regularized echo state network for time
series classification. <em>PR</em>, <em>130</em>, 108811. (<a
href="https://doi.org/10.1016/j.patcog.2022.108811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An echo State Network (ESN) is a special structure of a recurrent neural network (RNN) in which the recurrent neurons are randomly connected. ESN models which have achieved a high accuracy on time series prediction tasks can be used as time series prediction models in many domains. Nevertheless, in most ESN models, the input weights are randomly generated and the output weights calculated by the least square method are susceptible to outliers, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel discriminative and regularized ESN (DR-ESN) combines discriminative feature aggregation (DFA) and outlier-robust weights (ORW) algorithms are proposed for time series classification. DFA is firstly proposed to replace the random input weights of ESN with the constrained weights generated from sample information. In DFA, weight vectors are selected from the vector space spanned by initial input sequence vectors, then the new generated input weights can adequately represent the data features. Secondly, ORW is employed to enhance the robustness of output weights by constraining the weights assigned to samples with large training errors. The weights evaluation and experiments on a massive set of the synthetic time series data, real-world bearing fault data and UCR benchmarks indicate that the proposed DR-ESN can not only considerably improve the original ESN classifier but also effectively suppress the effect of outliers on classification performance.},
  archive      = {J_PR},
  author       = {Heshan Wang and Yuxi Liu and Dongshu Wang and Yong Luo and Chudong Tong and Zhaomin Lv},
  doi          = {10.1016/j.patcog.2022.108811},
  journal      = {Pattern Recognition},
  pages        = {108811},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative and regularized echo state network for time series classification},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage generative adversarial networks for binarization
of color document images. <em>PR</em>, <em>130</em>, 108810. (<a
href="https://doi.org/10.1016/j.patcog.2022.108810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network (CNN)based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks . In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks , we formulate loss functions between a discriminator and generators having an encoder–decoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/ .},
  archive      = {J_PR},
  author       = {Sungho Suh and Jihun Kim and Paul Lukowicz and Yong Oh Lee},
  doi          = {10.1016/j.patcog.2022.108810},
  journal      = {Pattern Recognition},
  pages        = {108810},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage generative adversarial networks for binarization of color document images},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Directly solving normalized cut for multi-view data.
<em>PR</em>, <em>130</em>, 108809. (<a
href="https://doi.org/10.1016/j.patcog.2022.108809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view clustering, which aims to uncover clusters from multi-view data with graph clustering technique , is one of the most important multi-view clustering methods . Such methods usually perform eigen-decomposition first to solve the relaxed problem and then obtain the final cluster indicator matrix from eigenvectors by k k -means or spectral rotation. However, such a two-step process may result in undesired clustering result since the two steps aim to solve different problems. In this paper, we propose a k k -way normalized cut method for multi-view data, named as the Multi-view Discrete Normalized Cut (MDNC). The new method learns a set of implicit weights for each view to identify its quality, and a novel iterative algorithm is proposed to directly solve the new model without relaxation and post-processing. Moreover, we propose a new method to adjust the distribution of the implicit view weights to obtain better clustering result . Extensive experimental results show that the performance of our approach is superior to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chen Wang and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang},
  doi          = {10.1016/j.patcog.2022.108809},
  journal      = {Pattern Recognition},
  pages        = {108809},
  shortjournal = {Pattern Recognition},
  title        = {Directly solving normalized cut for multi-view data},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FastOPM—a practical method for partial match of time series.
<em>PR</em>, <em>130</em>, 108808. (<a
href="https://doi.org/10.1016/j.patcog.2022.108808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications like stock markets, engineering, medicine, etc., a large amount of time series data has been collected. Interrogating the data for patterns is important for analysis like event prediction and event investigation. A fundamental operation to support such analysis is query processing . In this paper, we aim to efficiently find the optimal match of a query in a timeseries when the match is calculated based on the trend and allows points to be skipped from the middle and ends of the sequences. This problem requires global optimization. The solutions in the literature have prohibitively high time complexities and are not practical for long timeseries. Our method consists of three parts. The first part is an efficiency improvement algorithm called FastOPM which applies the Dijkstra algorithm to get the optimal solution in an efficient manner. The second part derives bounds for optimal solutions. The third part is an algorithm for efficiently searching the target timeseries for the best optimal match of a query. Our experiments show that our method is faster than the baseline method , the bounds are effective, and the search algorithm can identify the best optimal match efficiently. Overall, our algorithm effectively outperforms the state-of-the-art algorithms DTW and MASS in retrieving target segments.},
  archive      = {J_PR},
  author       = {Jixue Liu and Jiuyong Li and Lin Liu},
  doi          = {10.1016/j.patcog.2022.108808},
  journal      = {Pattern Recognition},
  pages        = {108808},
  shortjournal = {Pattern Recognition},
  title        = {FastOPM—A practical method for partial match of time series},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-embedding neural model for incident video retrieval.
<em>PR</em>, <em>130</em>, 108807. (<a
href="https://doi.org/10.1016/j.patcog.2022.108807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many internet search engines have been developed, however, the retrieval of video clips remains a challenge. This paper considers the retrieval of incident videos, which may contain more spatial and temporal semantics . We propose an encoder-decoder ConvLSTM model that explores multiple embeddings of a video to facilitate comparison of similarity between a pair of videos. The model is able to encode a video into an embedding that integrates both its spatial information and temporal semantics. Multiple video embeddings are then generated from coarse- and fine-grained features of a video to capture high- and low-level meanings. Subsequently, a learning-based comparative model is proposed to compare the similarity of two videos based on their embeddings. Extensive evaluations are presented and show that our model outperforms state-of-the-art methods for several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.},
  archive      = {J_PR},
  author       = {Ting-Hui Chiang and Yi-Chun Tseng and Yu-Chee Tseng},
  doi          = {10.1016/j.patcog.2022.108807},
  journal      = {Pattern Recognition},
  pages        = {108807},
  shortjournal = {Pattern Recognition},
  title        = {A multi-embedding neural model for incident video retrieval},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint prediction of monocular depth and structure using
planar and parallax geometry. <em>PR</em>, <em>130</em>, 108806. (<a
href="https://doi.org/10.1016/j.patcog.2022.108806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.},
  archive      = {J_PR},
  author       = {Hao Xing and Yifan Cao and Maximilian Biber and Mingchuan Zhou and Darius Burschka},
  doi          = {10.1016/j.patcog.2022.108806},
  journal      = {Pattern Recognition},
  pages        = {108806},
  shortjournal = {Pattern Recognition},
  title        = {Joint prediction of monocular depth and structure using planar and parallax geometry},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying the key frames: An attention-aware sampling
method for action recognition. <em>PR</em>, <em>130</em>, 108797. (<a
href="https://doi.org/10.1016/j.patcog.2022.108797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based methods have achieved remarkable progress in action recognition. Existing works mainly focus on designing novel deep architectures to learn video representations for action recognition. Most existing methods treat sampled frames equally and average all the frame-level predictions to generate video-level predictions at the testing stage. However, within a video, discriminative actions may occur sparsely in a few frames whereas most other frames are irrelevant to the ground truth which may even lead to wrong results. As a result, we think that the strategy of selecting relevant frames would be a further important key to enhance the existing deep learning based action recognition. In this paper, we propose an attention-aware sampling method for action recognition, which aims to discard the irrelevant and misleading frames and preserve the most discriminative frames. We formulate the process of mining key frames from videos as a Markov decision process and train the attention agent through deep reinforcement learning without extra labels. The agent takes features and predictions from the baseline model as inputs and generates importance scores for all frames. Moreover, our approach is extensible, which can be applied to different existing deep learning based action recognition models. We achieve very competitive action recognition performance on two widely used action recognition datasets.},
  archive      = {J_PR},
  author       = {Wenkai Dong and Zhaoxiang Zhang and Chunfeng Song and Tieniu Tan},
  doi          = {10.1016/j.patcog.2022.108797},
  journal      = {Pattern Recognition},
  pages        = {108797},
  shortjournal = {Pattern Recognition},
  title        = {Identifying the key frames: An attention-aware sampling method for action recognition},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). 3D object detection for autonomous driving: A survey.
<em>PR</em>, <em>130</em>, 108796. (<a
href="https://doi.org/10.1016/j.patcog.2022.108796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning , motion prediction, and collision avoidance etc. . Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.},
  archive      = {J_PR},
  author       = {Rui Qian and Xin Lai and Xirong Li},
  doi          = {10.1016/j.patcog.2022.108796},
  journal      = {Pattern Recognition},
  pages        = {108796},
  shortjournal = {Pattern Recognition},
  title        = {3D object detection for autonomous driving: A survey},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributional barycenter problem through data-driven flows.
<em>PR</em>, <em>130</em>, 108795. (<a
href="https://doi.org/10.1016/j.patcog.2022.108795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method is proposed for the solution of the data-driven optimal transport barycenter problem and of the more general distributional barycenter problem that the article introduces. The distributional barycenter problem provides a conceptual and computational toolbox for central problems in pattern recognition, such as the simulation of conditional distributions , the construction of a representative for a family of distributions indexed by a covariate and a new class of data-based generative models . The method proposed improves on previous approaches based on adversarial games, by slaving the discriminator to the generator and minimizing the need for parameterizations. It applies not only to a discrete family of distributions, but to more general distributions conditioned to factors z z of any cardinality and type. The methodology is applied to numerical examples, including an analysis of the MNIST data set with a new cost function that penalizes non-isometric maps.},
  archive      = {J_PR},
  author       = {Esteban G. Tabak and Giulio Trigila and Wenjun Zhao},
  doi          = {10.1016/j.patcog.2022.108795},
  journal      = {Pattern Recognition},
  pages        = {108795},
  shortjournal = {Pattern Recognition},
  title        = {Distributional barycenter problem through data-driven flows},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The iterative convolution–thresholding method (ICTM) for
image segmentation. <em>PR</em>, <em>130</em>, 108794. (<a
href="https://doi.org/10.1016/j.patcog.2022.108794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational methods, which have been tremendously successful in image segmentation , work by minimizing a given objective functional. The objective functional usually consists of a fidelity term and a regularization term. Because objective functionals may vary from different types of images, developing an efficient, simple, and general numerical method to minimize them has become increasingly vital. However, many existing methods are model-based, converge relatively slowly, or involve complicated techniques. In this paper, we develop a novel iterative convolution–thresholding method (ICTM) that is simple, efficient, and applicable to a wide range of variational models for image segmentation. In ICTM, the interface between two different segment domains is implicitly represented by the characteristic functions of domains. The fidelity term is usually written into a linear functional of the characteristic functions, and the regularization term is approximated by a functional of characteristic functions in terms of heat kernel convolution . This allows us to design an iterative convolution–thresholding method to minimize the approximate energy. The method has the energy-decaying property, and thus the unconditional stability is theoretically guaranteed. Numerical experiments show that the method is simple, easy to implement, robust, and applicable to various image segmentation models.},
  archive      = {J_PR},
  author       = {Dong Wang and Xiao-Ping Wang},
  doi          = {10.1016/j.patcog.2022.108794},
  journal      = {Pattern Recognition},
  pages        = {108794},
  shortjournal = {Pattern Recognition},
  title        = {The iterative convolution–thresholding method (ICTM) for image segmentation},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online multiple object tracking using joint detection and
embedding network. <em>PR</em>, <em>130</em>, 108793. (<a
href="https://doi.org/10.1016/j.patcog.2022.108793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking (MOT) generally employs the paradigm of tracking-by-detection, where object detection and object tracking are executed conventionally using separate systems. Current progress in MOT has focused on detecting and tracking objects by harnessing the representational power of deep learning . Since existing methods always combine two submodules in the same network, it is particularly important that they must be trained effectively together. Therefore, the development of a suitable network architecture for the end-to-end joint training of detection and tracking submodules remains a challenging issue. The present work addresses this issue by proposing a novel architecture denoted as YOLOTracker that performs online MOT by exploiting a joint detection and embedding network. First, an efficient and powerful joint detection and tracking model is constructed to accomplish instance-level embedded training , which can ensure that the proposed tracker achieves highly accurate MOT results with high efficiency. Then, the Path Aggregation Network is employed to combine low-resolution and high-resolution features for integrating textural features and semantic information and mitigating the misalignment of the re-identification features. Experiments are conducted on three challenging and publicly available benchmark datasets and results demonstrate the proposed tracker outperforms other state-of-the-art MOT trackers in terms of accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Sixian Chan and Yangwei Jia and Xiaolong Zhou and Cong Bai and Shengyong Chen and Xiaoqin Zhang},
  doi          = {10.1016/j.patcog.2022.108793},
  journal      = {Pattern Recognition},
  pages        = {108793},
  shortjournal = {Pattern Recognition},
  title        = {Online multiple object tracking using joint detection and embedding network},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning attention-guided pyramidal features for few-shot
fine-grained recognition. <em>PR</em>, <em>130</em>, 108792. (<a
href="https://doi.org/10.1016/j.patcog.2022.108792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot fine-grained recognition (FS-FGR) aims to distinguish several highly similar objects from different sub-categories with limited supervision. However, traditional few-shot learning solutions typically exploit image-level features and are committed to capturing global silhouettes while accidentally ignore to exploring local details, resulting in an inevitable problem of inconspicuous but distinguishable information loss. Thus, how to effectively address the fine-grained recognition issue given limited samples still remains a major challenging. In this article, we tend to propose an effective bidirectional pyramid architecture to enhance internal representations of features to cater to fine-grained image recognition task in the few-shot learning scenario. Specifically, we deploy a multi-scale feature pyramid and a multi-level attention pyramid on the backbone network , and progressively aggregated features from different granular spaces via both of them. We then further present an attention-guided refinement strategy in collaboration with a multi-level attention pyramid to reduce the uncertainty brought by backgrounds conditioned by limited samples. In addition, the proposed method is trained with the meta-learning framework in an end-to-end fashion without any extra supervision. Extensive experimental results on four challenging and widely-used fine-grained benchmarks show that the proposed method performs favorably against state-of-the-arts, especially in the one-shot scenarios.},
  archive      = {J_PR},
  author       = {Hao Tang and Chengcheng Yuan and Zechao Li and Jinhui Tang},
  doi          = {10.1016/j.patcog.2022.108792},
  journal      = {Pattern Recognition},
  pages        = {108792},
  shortjournal = {Pattern Recognition},
  title        = {Learning attention-guided pyramidal features for few-shot fine-grained recognition},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enforced block diagonal subspace clustering with closed form
solution. <em>PR</em>, <em>130</em>, 108791. (<a
href="https://doi.org/10.1016/j.patcog.2022.108791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods , an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix , which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering . Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.},
  archive      = {J_PR},
  author       = {Yalan Qin and Hanzhou Wu and Jian Zhao and Guorui Feng},
  doi          = {10.1016/j.patcog.2022.108791},
  journal      = {Pattern Recognition},
  pages        = {108791},
  shortjournal = {Pattern Recognition},
  title        = {Enforced block diagonal subspace clustering with closed form solution},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The CP‐ABM approach for modelling COVID‐19 infection
dynamics and quantifying the effects of non‐pharmaceutical
interventions. <em>PR</em>, <em>130</em>, 108790. (<a
href="https://doi.org/10.1016/j.patcog.2022.108790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation for this research is to develop an approach that reliably captures the disease dynamics of COVID-19 for an entire population in order to identify the key events driving change in the epidemic through accurate estimation of daily COVID-19 cases. This has been achieved through the new CP-ABM approach which uniquely incorporates C hange P oint detection into an A gent B ased M odel taking advantage of genetic algorithms for calibration and an efficient infection centric procedure for computational efficiency. The CP-ABM is applied to the Northern Ireland population where it successfully captures patterns in COVID-19 infection dynamics over both waves of the pandemic and quantifies the significant effects of non-pharmaceutical interventions (NPI) on a national level for lockdowns and mask wearing. To our knowledge, there is no other approach to date that has captured NPI effectiveness and infection spreading dynamics for both waves of the COVID-19 pandemic for an entire country population.},
  archive      = {J_PR},
  author       = {Aleksandar Novakovic and Adele H. Marshall},
  doi          = {10.1016/j.patcog.2022.108790},
  journal      = {Pattern Recognition},
  pages        = {108790},
  shortjournal = {Pattern Recognition},
  title        = {The CP‐ABM approach for modelling COVID‐19 infection dynamics and quantifying the effects of non‐pharmaceutical interventions},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying group lasso granger causality graph for high
dimensional dynamic system. <em>PR</em>, <em>130</em>, 108789. (<a
href="https://doi.org/10.1016/j.patcog.2022.108789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial preprocessing step in data analysis and machine learning . Since causal relationships imply the underlying mechanism of a system, causality-based feature selection methods have gradually attracted great attentions. For a high dimensional system undergoing dynamic transformation, because of the non-stationarity and sample scarcity, modeling the causal structure among these features is difficult. In this paper, we propose a time-varying Granger causal networks to capture the causal relations underlying high dimensional time-varying vector autoregressive models with high order lagged dependence. A kernel reweighted group lasso method is proposed, which overcomes the limitations of sample scarcity and transforms the problem of Granger causal structural learning into a group variable selection problem. The asymptotic consistency of the proposed algorithm is proved. We apply the time-varying Granger causal networks to simulation experiments and real data in the financial market. The study demonstrates that the method provides an efficient tool to detect changes and analysis characters of causal dependency structure in network evolution.},
  archive      = {J_PR},
  author       = {Wei Gao and Haizhong Yang},
  doi          = {10.1016/j.patcog.2022.108789},
  journal      = {Pattern Recognition},
  pages        = {108789},
  shortjournal = {Pattern Recognition},
  title        = {Time-varying group lasso granger causality graph for high dimensional dynamic system},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel hyperbolic clustering-based band hierarchy (HCBH) for
effective unsupervised band selection of hyperspectral images.
<em>PR</em>, <em>130</em>, 108788. (<a
href="https://doi.org/10.1016/j.patcog.2022.108788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For dimensionality reduction of HSI , many clustering-based unsupervised band selection (UBS) methods have been proposed due to their superiority of reducing the high redundancy between selected bands. However, most of these methods fail to reflect the data structure of HSI , leading to inconsistent results of band selection. To tackle this particular issue, we have proposed a novel hyperbolic clustering-based band hierarchy (HCBH) to fully represent the underlying spectral structure and obtain a more consistent band selection. With the proposed adaptive hyperbolic clustering, the performance can be effectively improved with the aid of geometrical information. By introducing a cluster-centre based ranking metric, the desired band subset can be naturally obtained during the clustering process . Experimental results on three popularly used datasets have validated the superior performance of the proposed approach, which outperforms a few state-of-the-art (SOTA) UBS approaches.},
  archive      = {J_PR},
  author       = {He Sun and Lei Zhang and Jinchang Ren and Hua Huang},
  doi          = {10.1016/j.patcog.2022.108788},
  journal      = {Pattern Recognition},
  pages        = {108788},
  shortjournal = {Pattern Recognition},
  title        = {Novel hyperbolic clustering-based band hierarchy (HCBH) for effective unsupervised band selection of hyperspectral images},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive author-aware text clustering. <em>PR</em>,
<em>130</em>, 108787. (<a
href="https://doi.org/10.1016/j.patcog.2022.108787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning , which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT’s main components.},
  archive      = {J_PR},
  author       = {Xudong Tang and Chao Dong and Wei Zhang},
  doi          = {10.1016/j.patcog.2022.108787},
  journal      = {Pattern Recognition},
  pages        = {108787},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive author-aware text clustering},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modality attentive feature fusion for object detection
in multispectral remote sensing imagery. <em>PR</em>, <em>130</em>,
108786. (<a href="https://doi.org/10.1016/j.patcog.2022.108786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality fusing complementary information of multispectral remote sensing image pairs can improve the perception ability of detection algorithms , making them more robust and reliable for a wider range of applications, such as nighttime detection. Compared with prior methods, we think different features should be processed specifically, the modality-specific features should be retained and enhanced, while the modality-shared features should be cherry-picked from the RGB and thermal IR modalities. Following this idea, a novel and lightweight multispectral feature fusion approach with joint common-modality and differential-modality attentions are proposed, named Cross-Modality Attentive Feature Fusion (CMAFF). Given the intermediate feature maps of RGB and thermal images , our module parallel infers attention maps from two separate modalities, common- and differential-modality, then the attention maps are multiplied to the input feature map respectively for adaptive feature enhancement or selection. Extensive experiments demonstrate that our proposed approach can achieve the state-of-the-art performance at a low computation cost.},
  archive      = {J_PR},
  author       = {Fang Qingyun and Wang Zhaokui},
  doi          = {10.1016/j.patcog.2022.108786},
  journal      = {Pattern Recognition},
  pages        = {108786},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised rigid transformation equivariance for
accurate 3D point cloud registration. <em>PR</em>, <em>130</em>, 108784.
(<a href="https://doi.org/10.1016/j.patcog.2022.108784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformation equivariance has been widely investigated in 3D point cloud representation learning for more informative descriptors, which formulates the change of the representation with respect to the transformation of the input point clouds explicitly. In this paper, we extend this property to the task of 3D point cloud registration and propose a r igid t ransformation e quivariance ( RTE ) for accurate 3D point cloud registration. Specifically, RTE formulates the change of the relative pose explicitly with respect to the rigid transformation of the input point clouds. To exploit RTE , we adopt a Siamese structure network with two shared registration branches. One focuses on the input pair of point clouds, and the other one focuses on the new pair achieved by applying two random rigid transformations to the input point clouds respectively. Since the change of the two output relative poses has been predicted according to RTE , a new additional self-supervised loss is obtained to supervise the training. This general network structure can be integrated with most learning-based point cloud registration frameworks easily to improve the performance. Our method adopts the state-of-the-art virtual point-based pipelines as our shared branches, in which we propose a data-driven matching based on l earned c ost v olume ( LCV ) rather than traditional hand-crafted matching strategies. Experimental evaluations on both synthetic datasets and real datasets validate the effectiveness of our proposed framework. The source code will be made public.},
  archive      = {J_PR},
  author       = {Zhiyuan Zhang and Jiadai Sun and Yuchao Dai and Dingfu Zhou and Xibin Song and Mingyi He},
  doi          = {10.1016/j.patcog.2022.108784},
  journal      = {Pattern Recognition},
  pages        = {108784},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised rigid transformation equivariance for accurate 3D point cloud registration},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully convolutional deep stacked denoising sparse auto
encoder network for partial face reconstruction. <em>PR</em>,
<em>130</em>, 108783. (<a
href="https://doi.org/10.1016/j.patcog.2022.108783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition is one of the most successful applications of image analysis. Since 1960s, automatic face recognition research has been carried out, but the problem is still unresolved. Therefore, in this manuscript, a novel Partial face reconstruction (PFR) algorithm called Self- motivated feature mapping (SMFM) combining a Fully Convolutional Network (FCN) and Deep Stacked Denoising Sparse Autoencoders (DS-DSA) algorithm is proposed to overcome the challenges. The proposed approach focuses on the generation of feature maps from the Fully Convolutional Network and it is used Deep Stacked Denoising Sparse Autoencoders to perform the partial face reconstruction. The spatial maps are generated by extracting the features from Fully Convolutional Network and it is supplied as the input for partial reconstruction and re-identification to the Deep Stacked Denoising Sparse Autoencoders network. The main aim of the proposed work is “to enhance the accuracy during facial reconstruction”. The proposed approach is implemented in MATLAB platform. The performance of the proposed approach attains 23.45\% and 20.41\% accuracy,25.93`\% and 19.43\% sensitivity, 22.21\% and 24.41\% precision and20.21\% and 23.41\% Specificity greater than the existing approaches, like Partial Face Reconstruction using generative adversarial networks (GANs), Partial Face Reconstruction using Deep Recurrent neural network (DRNN).},
  archive      = {J_PR},
  author       = {P.S. Dinesh and M. Manikandan},
  doi          = {10.1016/j.patcog.2022.108783},
  journal      = {Pattern Recognition},
  pages        = {108783},
  shortjournal = {Pattern Recognition},
  title        = {Fully convolutional deep stacked denoising sparse auto encoder network for partial face reconstruction},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Residual objectness for imbalance reduction. <em>PR</em>,
<em>130</em>, 108781. (<a
href="https://doi.org/10.1016/j.patcog.2022.108781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As most object detectors rely on dense candidate samples to cover objects, they have always suffered from the extreme imbalance between very few foreground samples and numerous background samples during training, i.e. , the foreground-background imbalance. Although several resampling and reweighting schemes ( e.g. , OHEM, Focal Loss, GHM) have been proposed to alleviate the imbalance, they are usually heuristic with multiple hyper-parameters, which is difficult to generalize on different object detectors and datasets. In this paper, we propose a novel Residual Objectness (ResObj) mechanism that adaptively learns how to address the foreground-background imbalance problem in object detection. Specifically, we first formulate the imbalance problems on all object classes as an imbalance problem on an “objectness” class. Then, we design multiple cascaded objectness estimators with residual connections for that objectness class to progressively distinguish the foreground samples from background samples. With our residual objectness mechanism, object detectors can learn how to address the foreground-background problem in an end-to-end way, rather than rely on hand-crafted resampling or reweighting schemes. Extensive experiments on the COCO benchmark demonstrate the effectiveness and compatibility of our method for various object detectors: the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3\% ∼ 4\% 3\%∼4\% Average Precision (AP) improvements compared with their vanilla models, respectively.},
  archive      = {J_PR},
  author       = {Joya Chen and Dong Liu and Bin Luo and Xuezheng Peng and Tong Xu and Enhong Chen},
  doi          = {10.1016/j.patcog.2022.108781},
  journal      = {Pattern Recognition},
  pages        = {108781},
  shortjournal = {Pattern Recognition},
  title        = {Residual objectness for imbalance reduction},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMT: Dynamic mutual training for semi-supervised learning.
<em>PR</em>, <em>130</em>, 108777. (<a
href="https://doi.org/10.1016/j.patcog.2022.108777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation . Our codes are released at https://github.com/voldemortX/DST-CBC .},
  archive      = {J_PR},
  author       = {Zhengyang Feng and Qianyu Zhou and Qiqi Gu and Xin Tan and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
  doi          = {10.1016/j.patcog.2022.108777},
  journal      = {Pattern Recognition},
  pages        = {108777},
  shortjournal = {Pattern Recognition},
  title        = {DMT: Dynamic mutual training for semi-supervised learning},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic feature fusion with spatial-temporal context for
robust object tracking. <em>PR</em>, <em>130</em>, 108775. (<a
href="https://doi.org/10.1016/j.patcog.2022.108775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature fusion has been widely used for improving the tracking performance. However, how to effectively analyze the characteristics of different visual features to realize dynamical feature fusion is still a challenging task. In this paper, we propose a spatial-temporal context-based dynamic feature fusion method (STCDFF) with the correlation filters framework for object tracking. The proposed STCDFF method exploits spatial-temporal context to deeply analyze the characteristics of multiple visual features (e.g., HOG, Color-Names and CNN features) to perform feature fusion. On the one hand, spatial context is employed to evaluate the discriminative ability of different features to distinguish the target object from the background. On the other hand, temporal context is utilized to consider the representative ability of different features to capture significant appearance changes of the target object. The weight of a feature is decided by both its discriminative ability and representative ability. By exploring spatial-temporal context for feature fusion, the STCDFF method can fully utilize the strengths of different features to handle complex appearance changes and background clutters to achieve better performance. Extensive experiments on multiple object tracking datasets prove that our STCDFF method performs competitively against several popular tracking methods.},
  archive      = {J_PR},
  author       = {Ke Nai and Zhiyong Li and Haidong Wang},
  doi          = {10.1016/j.patcog.2022.108775},
  journal      = {Pattern Recognition},
  pages        = {108775},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic feature fusion with spatial-temporal context for robust object tracking},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pseudo loss active learning for deep visual tracking.
<em>PR</em>, <em>130</em>, 108773. (<a
href="https://doi.org/10.1016/j.patcog.2022.108773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual tracking tasks, the training data are commonly composed of a large number of video sequences and each frame in the sequences needs to be labeled manually, which is labor-intensive and time-consuming. In addition, considering the similarity among the consecutive frames in the same sequence, there is significant redundancy in the training data. To address these problems, a novel pseudo loss active learning (PLAL) method is developed in this paper. PLAL aims to select the most informative and least redundant data for training to reduce the cost of labeling and maintain competitive tracking results simultaneously. Firstly, the Gaussian distribution based pseudo label is generated for the unlabeled candidates based on the tracking model which is initially trained on a small amount of training data. Then, the pseudo loss based on cross entropy is designed to compute the difference between the pseudo label and the target response map. The pseudo loss measures the uncertainty of the target spatial context which is used as the informativeness criterion of the image frame for selection. Meanwhile, a sampling interval threshold and a temporal penalty are employed for frame selection to avoid drastic variation in target appearance and reduce the redundancy within the consecutive candidate frames. Only the selected frames are labeled by the oracle (human expert) and then added to the training data. Extensive experiments on public benchmarks (OTB2013, OTB2015, VOT2018, UAV123, GOT-10K, TrackingNet, LaSOT, OxUvA and TLP) demonstrate that PLAL method outperforms the baseline and other recent active learning approaches. With only 3\% of labeled data from the training dataset, PLAL reaches competitive performance (98-100\%) compared to the model trained on the entire training dataset.},
  archive      = {J_PR},
  author       = {Zhiyan Cui and Na Lu and Weifeng Wang},
  doi          = {10.1016/j.patcog.2022.108773},
  journal      = {Pattern Recognition},
  pages        = {108773},
  shortjournal = {Pattern Recognition},
  title        = {Pseudo loss active learning for deep visual tracking},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-weighted sample-level fusion with anchors for
incomplete multi-view clustering. <em>PR</em>, <em>130</em>, 108772. (<a
href="https://doi.org/10.1016/j.patcog.2022.108772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at solving the problem of clustering in the multi-view datasets which include samples with information missing in one or more views, incomplete multi-view clustering has received considerable attention. However, most studies can not get satisfying accuracy and efficiency when dealing with datasets in which a considerable number of instances are missing in partial views. To address this problem, a method named Auto-weighted Sample-level Fusion with Anchors for Incomplete Multi-view Clustering (ASA-IC) is proposed in this paper. It designs an auto-weighted sample-level fusion strategy, which realizes the optimized conversion from the individual instance-to-anchor similarity learning to the concensus instance-to-anchor similarity matrix construction. ASA-IC can not only handle incomplete samples and effectively explore the relationship between each instance and anchors, but also deal with various incomplete clustering situations and be applied in large-scale datasets as well. Besides, experiments on 5 complete datasets and 27 incomplete ones illustrate its effectiveness quantitatively and qualitatively.},
  archive      = {J_PR},
  author       = {Xiao Yu and Hui Liu and Yuxiu Lin and Yan Wu and Caiming Zhang},
  doi          = {10.1016/j.patcog.2022.108772},
  journal      = {Pattern Recognition},
  pages        = {108772},
  shortjournal = {Pattern Recognition},
  title        = {Auto-weighted sample-level fusion with anchors for incomplete multi-view clustering},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete curve model for non-elastic shape analysis on shape
manifold. <em>PR</em>, <em>130</em>, 108760. (<a
href="https://doi.org/10.1016/j.patcog.2022.108760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we construct a novel finite dimensional shape manifold for shape analyses. Elements of the shape manifold are a set of discrete, planar, and closed curves, which stand for object boundaries and are represented by direction function. On this manifold, we use a set of N-dimensional Fourier basis to construct the tangent space of the shape manifold as a finite dimensional space. Furthermore, we construct the shape manifold as a Riemannian manifold, in which the Riemannian metric is interpreted as an l 2 l2 metric. Our method improves the performance of bending-only models in the issues of shape analysis including the shape synthesis, comparison, and statistic analysis. We evaluate the performance of the manifold via the following applications: 1) shape interpolation and extrapolation between curves, 2) shape retrieval on the Flavia leaf database, 3) shape synthesis using an estimated probability distribution on the manifold, and 4) a novel application named shape arithmetic. All the above experiments clearly demonstrate our approach achieves superior performance to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Peng Chen and Xutao Li and Changxing Ding and Jianxing Liu and Ligang Wu},
  doi          = {10.1016/j.patcog.2022.108760},
  journal      = {Pattern Recognition},
  pages        = {108760},
  shortjournal = {Pattern Recognition},
  title        = {Discrete curve model for non-elastic shape analysis on shape manifold},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-attention based fine-grained cross-media hybrid
network. <em>PR</em>, <em>130</em>, 108748. (<a
href="https://doi.org/10.1016/j.patcog.2022.108748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the heterogeneity gap, the data representations of different types of media are inconsistent. It is challenging to measure the fine-grained gap between different media. To this end, we propose a self-attention-based hybrid network to learn the common representations of different media data. Specifically, we first utilize a local self-attention layer to learn the common attention space between different media data. Then we propose a similarity concatenation method to understand the content relationship between features. To further improve the robustness of the model, we also learn a local position encoding to capture the spatial relationships between features. Therefore, our proposed approach can effectively reduce the gap between different feature distributions on cross-media retrieval tasks. Extensive experiments and ablation studies demonstrate that our proposed method achieves state-of-the-art performance. The source code and models are publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/SAFGCMHN .},
  archive      = {J_PR},
  author       = {Wei Shan and Dan Huang and Jiangtao Wang and Feng Zou and Suwen Li},
  doi          = {10.1016/j.patcog.2022.108748},
  journal      = {Pattern Recognition},
  pages        = {108748},
  shortjournal = {Pattern Recognition},
  title        = {Self-attention based fine-grained cross-media hybrid network},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot website fingerprinting attack with meta-bias
learning. <em>PR</em>, <em>130</em>, 108739. (<a
href="https://doi.org/10.1016/j.patcog.2022.108739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Website fingerprinting (WF) attack aims to identify which website a user is visiting from the traffic data patterns. Whilst existing methods assume many training samples, we investigate a more realistic and scalable few-shot WF attack with only a few labeled training samples per website. To solve this problem, we introduce a novel Meta-Bias Learning (MBL) method for few-shot WF learning. Taking the meta-learning strategy, MBL simulates and optimizes the target tasks. Moreover, a new model parameter factorization idea is introduced for facilitating meta-training with superior task adaptation. Expensive experiments show that our MBL outperforms significantly existing hand-crafted feature and deep learning based alternatives in both closed-world and open-world attack scenarios, at the absence and presence of defense.},
  archive      = {J_PR},
  author       = {Mantun Chen and Yongjun Wang and Xiatian Zhu},
  doi          = {10.1016/j.patcog.2022.108739},
  journal      = {Pattern Recognition},
  pages        = {108739},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot website fingerprinting attack with meta-bias learning},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General nonconvex total variation and low-rank
regularizations: Model, algorithm and applications. <em>PR</em>,
<em>130</em>, 108692. (<a
href="https://doi.org/10.1016/j.patcog.2022.108692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Total Variation and Low-Rank regularizations have shown significant successes in machine learning , data mining, and image processing in past decades. This paper develops the general nonconvex composite regularized model, which contains previous regularizers and motivates novel ones. Although the classical Alternating Direction Methods of Multiplier (ADMM) algorithm is applicable for this model, the nonconvexity of the problem and the complicacy of choosing the parameters increase the difficulty in the use of ADMM. Thus, by the penalty method, we propose the Alternating Minimization (AM) algorithm, whose convergence results are proved under mild assumptions. The proposed model and algorithm are applied to the image restoration problem . Numerical results demonstrate the efficiency of our model and algorithm.},
  archive      = {J_PR},
  author       = {Tao Sun and Dongsheng Li},
  doi          = {10.1016/j.patcog.2022.108692},
  journal      = {Pattern Recognition},
  pages        = {108692},
  shortjournal = {Pattern Recognition},
  title        = {General nonconvex total variation and low-rank regularizations: Model, algorithm and applications},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample complexity of rank regression using pairwise
comparisons. <em>PR</em>, <em>130</em>, 108688. (<a
href="https://doi.org/10.1016/j.patcog.2022.108688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a rank regression setting, in which a dataset of N samples with features in R d is ranked by an oracle via M pairwise comparisons . Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with ϵ &gt; 0 accuracy, it suffices to conduct M ∈ Ω ( d N log 3 N / ϵ 2 ) comparisons uniformly at random when N is Ω ( d / ϵ 2 ) .},
  archive      = {J_PR},
  author       = {Berkan Kadıoğlu and Peng Tian and Jennifer Dy and Deniz Erdoğmuş and Stratis Ioannidis},
  doi          = {10.1016/j.patcog.2022.108688},
  journal      = {Pattern Recognition},
  pages        = {108688},
  shortjournal = {Pattern Recognition},
  title        = {Sample complexity of rank regression using pairwise comparisons},
  volume       = {130},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HAM: Hybrid attention module in deep convolutional neural
networks for image classification. <em>PR</em>, <em>129</em>, 108785.
(<a href="https://doi.org/10.1016/j.patcog.2022.108785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many researches have demonstrated that the attention mechanism has great potential in improving the performance of deep convolutional neural networks (CNNs). However, the existing methods either ignore the importance of using channel attention and spatial attention mechanisms simultaneously or bring much additional model complexity. In order to achieve a balance between performance and model complexity, we propose the Hybrid Attention Module (HAM), a really lightweight yet efficient attention module. Given an intermediate feature map as the input feature, HAM firstly produces one channel attention map and one channel refined feature through the channel submodule, and then based on the channel attention map, the spatial submodule divides the channel refined feature into two groups along the channel axis to generate a pair of spatial attention descriptors. By applying saptial attention descriptors, the spatial submodule generates the final refined feature which can adaptively emphasize the important regions. Besides, HAM is a simple and general module, it can be embedded into various mainstream deep CNN architectures seamlessly and can be trained with base CNNs in the end-to-end way. We evaluate HAM through abundant of experiments on CIFAR-10, CIFAR-100 and STL-10 datasets. The experimental results show that HAM-integrated networks achieve accuracy improvements and further reduce the negative impact of less training data on deeper networks performance than its counterparts, which proves the effectiveness of HAM.},
  archive      = {J_PR},
  author       = {Guoqiang Li and Qi Fang and Linlin Zha and Xin Gao and Nenggan Zheng},
  doi          = {10.1016/j.patcog.2022.108785},
  journal      = {Pattern Recognition},
  pages        = {108785},
  shortjournal = {Pattern Recognition},
  title        = {HAM: Hybrid attention module in deep convolutional neural networks for image classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Salient object detection with image-level binary
supervision. <em>PR</em>, <em>129</em>, 108782. (<a
href="https://doi.org/10.1016/j.patcog.2022.108782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning based salient object detection (SOD) methods have achieved impressive performance. However, while fully-supervised methods require a large amount of labeled data, weakly-supervised methods still require a considerable human effort. To address this problem, we propose a novel weakly-supervised method for salient object detection based on only binary image tags, which are much cheaper to collect. Our basic idea is to construct a dataset of images that are labeled as either salient (with salient objects) or non-salient (without salient objects), and leverage such binary labels as supervision to learn a salient object detector based on existing unsupervised methods . In particular, we propose a target saliency map hallucinator, which can synthesize pseudo ground truth saliency maps for the salient images in the training data solely from binary labels. We can then use the pseudo ground truth labels to train a salient object detector. Experimental results show that our method performs comparably to the state-of-the-art weakly-supervised methods, but requires considerably less human supervision.},
  archive      = {J_PR},
  author       = {Pengjie Wang and Yuxuan Liu and Ying Cao and Xin Yang and Yu Luo and Huchuan Lu and Zijian Liang and Rynson W.H. Lau},
  doi          = {10.1016/j.patcog.2022.108782},
  journal      = {Pattern Recognition},
  pages        = {108782},
  shortjournal = {Pattern Recognition},
  title        = {Salient object detection with image-level binary supervision},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new distance between multivariate clusters of varying
locations, elliptical shapes, and directions. <em>PR</em>, <em>129</em>,
108780. (<a href="https://doi.org/10.1016/j.patcog.2022.108780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering methods are based on the computations of both the distances between every pair of the n n observations in a multivariate dataset as well as the distances between every pair of clusters in the dataset. The clusters can have different locations and varying elliptical shapes and directions. Numerous methods have been proposed in the literature for computing both of these two types of distances. The contributions of this paper are two folds. First, we propose a new elliptical distance between pairs of clusters in a dataset with different cluster centers and elliptical shapes and directions, Second, we proved analytically that the Ward distance and the Euclidean distance are equivalent. We propose a new classical method for computing the distance between a pair of clusters in the dataset. It is the only distance that does not assume spherical clusters. The proposed classical distances could also be made robust by replacing estimates of location and scale by their respective robust estimators . The proposed distance has a number of advantages including simplicity, interpretability , computational efficiency as well as the ability to accurately capture both the variability of the cluster centers as well as the variability of shapes and directions of their respective covariance matrices . The method is also illustrated by several motivating examples that demonstrate the need of the new proposed distance. The superiority of the proposed method is also demonstrated by application to real-life as well as challenging synthetic data.},
  archive      = {J_PR},
  author       = {Ali S. Hadi},
  doi          = {10.1016/j.patcog.2022.108780},
  journal      = {Pattern Recognition},
  pages        = {108780},
  shortjournal = {Pattern Recognition},
  title        = {A new distance between multivariate clusters of varying locations, elliptical shapes, and directions},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust distance metric optimization driven GEPSVM classifier
for pattern classification. <em>PR</em>, <em>129</em>, 108779. (<a
href="https://doi.org/10.1016/j.patcog.2022.108779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximal support vector machine via generalized eigenvalues (GEPSVM) is one of the most successful methods for classification problems. However, GEPSVM is vulnerable to outliers since it learns classifiers based on the squared L 2 -norm distance without a specific strategy to deal with the outliers. Motivated by existing studies that improve the robustness of GEPSVM via the L 1 -norm distance or not-squared L 2 -norm distance formulation, a novel GEPSVM formulation that minimizes the p -order of L 2 -norm distance is proposed, namely, L 2,p -GEPSVM. This formulation weakens the negative effects of both light and heavy outliers in the data. An iterative algorithm is designed to solve the general L 2,p -norm distance minimization problems and rigorously prove its convergence. In addition, we adjust the parameters of L 2,p -GEPSVM to balance the accuracy and training time. This is especially useful for larger datasets. Extensive results indicate that the L 2,p -GEPSVM improves the classification performance and robustness in various experimental settings.},
  archive      = {J_PR},
  author       = {He Yan and Liyong Fu and Tian&#39;an Zhang and Jun Hu and Qiaolin Ye and Yong Qi and Dong-Jun Yu},
  doi          = {10.1016/j.patcog.2022.108779},
  journal      = {Pattern Recognition},
  pages        = {108779},
  shortjournal = {Pattern Recognition},
  title        = {Robust distance metric optimization driven GEPSVM classifier for pattern classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-weighted learning framework for adaptive locality
discriminant analysis. <em>PR</em>, <em>129</em>, 108778. (<a
href="https://doi.org/10.1016/j.patcog.2022.108778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is one of the most important dimensionality reduction techniques and applied in many areas. However, traditional LDA algorithms aim to capture the global structure from data and ignore the local information. That may lead to the failure of LDA in some real-world datasets which have a complex geometry distribution. Although there are many previous works that focus on preserving the local information, they are all stuck in the same problem that the neighbor relationships of pairwise data points obtained from the original space may not be reliable, especially in the case of heavy noise. Therefore, we proposed a novel self-weighted learning framework, named Self-Weighted Adaptive Locality Discriminant Analysis (SALDA), for locality-aware based dimensionality reduction. The proposed framework can adaptively learn an intrinsic low-dimensional subspace, so that we can explore the better neighbor relationships for samples under the ideal subspace. In addition, our model can automatically learn to assign the weights to data pairwise points within the same class and takes no extra parameters compared to other classical locality-aware methods. At last, the experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness and superiority of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Wei Chang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.108778},
  journal      = {Pattern Recognition},
  pages        = {108778},
  shortjournal = {Pattern Recognition},
  title        = {Self-weighted learning framework for adaptive locality discriminant analysis},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic batch size for adaptive regularization in deep
network optimization. <em>PR</em>, <em>129</em>, 108776. (<a
href="https://doi.org/10.1016/j.patcog.2022.108776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a first-order stochastic optimization algorithm incorporating adaptive regularization for pattern recognition problems in deep learning framework. The adaptive regularization is imposed by stochastic process in determining batch size for each model parameter at each optimization iteration. The stochastic batch size is determined by the update probability of each parameter following a distribution of gradient norms in consideration of their local and global properties in the neural network architecture where the range of gradient norms may vary within and across layers. We empirically demonstrate the effectiveness of our algorithm using an image classification task based on conventional network models applied to commonly used benchmark datasets. The quantitative evaluation indicates that our algorithm outperforms the state-of-the-art optimization algorithms in generalization while providing less sensitivity to the selection of batch size which often plays a critical role in optimization, thus achieving more robustness to the selection of regularity.},
  archive      = {J_PR},
  author       = {Kensuke Nakamura and Stefano Soatto and Byung-Woo Hong},
  doi          = {10.1016/j.patcog.2022.108776},
  journal      = {Pattern Recognition},
  pages        = {108776},
  shortjournal = {Pattern Recognition},
  title        = {Stochastic batch size for adaptive regularization in deep network optimization},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VFMVAC: View-filtering-based multi-view aggregating
convolution for 3D shape recognition and retrieval. <em>PR</em>,
<em>129</em>, 108774. (<a
href="https://doi.org/10.1016/j.patcog.2022.108774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view based 3D shape recognition methods have achieved state-of-the-art performance in 3D shape recognition and retrieval. The main focus of multi-view based approaches is determining how to fuse multi-view features into a compact, descriptive, and robust 3D shape descriptor that can then be utilized for 3D shape recognition and retrieval. This paper proposes a novel multi-view aggregating framework, view-filtering-based multi-view aggregating convolution (VFMVAC) to learn global shape descriptors for 3D shape recognition. The proposed VFMVAC applies a voting-based view filtering strategy to select representative views, also introduces a novel multi-view aggregating module to integrate multi-view features; this substantially improves the descriptiveness of the descriptors, and therefore improves the performance of 3D shape recognition and retrieval. Specifically, all views are fed into a voting-based view filtering module to select the top-k representative views. Subsequently, the features of the top-k views are fed into the multi-view aggregating module, which first conducts cross-view channel shuffle for achieving cross-view information flowing, and the resulted reshaped features are then fed into the aggregating convolution module for feature fusion . Experiments on benchmark datasets demonstrate that the proposed VFMVAC is effective and outperforms several recent techniques with respect to the classification and retrieval performance , robustness and efficiency.},
  archive      = {J_PR},
  author       = {Zehua Liu and Yuhe Zhang and Jian Gao and Shurui Wang},
  doi          = {10.1016/j.patcog.2022.108774},
  journal      = {Pattern Recognition},
  pages        = {108774},
  shortjournal = {Pattern Recognition},
  title        = {VFMVAC: View-filtering-based multi-view aggregating convolution for 3D shape recognition and retrieval},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental multi-target domain adaptation for object
detection with efficient domain transfer. <em>PR</em>, <em>129</em>,
108771. (<a href="https://doi.org/10.1016/j.patcog.2022.108771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation , which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our approach leverages domain discriminators to train a novel Domain Transfer Module (DTM), which only incurs a modest overhead. The DTM transforms source images according to diverse target domains, allowing the model to access a joint representation of previously-learned target domains, and to effectively limit catastrophic forgetting. Our proposed method – called MTDA with DTM (MTDA-DTM) – is compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection. Results indicate that MTDA-DTM achieves the highest level of detection accuracy across multiple target domains, yet requires significantly fewer computational resources. Our code is available. 1},
  archive      = {J_PR},
  author       = {Le Thanh Nguyen-Meidine and Madhu Kiran and Marco Pedersoli and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
  doi          = {10.1016/j.patcog.2022.108771},
  journal      = {Pattern Recognition},
  pages        = {108771},
  shortjournal = {Pattern Recognition},
  title        = {Incremental multi-target domain adaptation for object detection with efficient domain transfer},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bi-level formulation for multiple kernel learning via
self-paced training. <em>PR</em>, <em>129</em>, 108770. (<a
href="https://doi.org/10.1016/j.patcog.2022.108770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel learning (MKL) is a crucial issue which has been widely researched over the last two decades. Although existing MKL algorithms have achieved satisfactory performance in a broad range of applications, these methods do not adequately consider the adverse effects of unreliable or less reliable instances. To handle this shortcoming, we formulate multiple kernel learning in a bi-level learning paradigm consisting of the kernel combination weight learning (KWL) stage and the self-paced learning (SPL) stage, which alternatively negotiate with each other. The KWL stage dynamically absorbs reliable instances into model learning to accurately capture neighborhood relationships and obtains kernel coefficients via maximizing both global and local kernel alignment in a common schema. The SPL stage automatically evaluates the reliability of training samples via self-paced training. The extensive experiments indicate the robustness and superiority of the presented approach in comparison with existing MKL methods.},
  archive      = {J_PR},
  author       = {Fatemeh Alavi and Sattar Hashemi},
  doi          = {10.1016/j.patcog.2022.108770},
  journal      = {Pattern Recognition},
  pages        = {108770},
  shortjournal = {Pattern Recognition},
  title        = {A bi-level formulation for multiple kernel learning via self-paced training},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symmetric binary tree based co-occurrence texture pattern
mining for fine-grained plant leaf image retrieval. <em>PR</em>,
<em>129</em>, 108769. (<a
href="https://doi.org/10.1016/j.patcog.2022.108769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leaf image patterns have been actively researched for plant species recognition. However, as a very challenging fine-grained pattern identification issue, cultivar recognition in which the leaf image patterns usually have very subtle difference among cultivars has not yet received considerable attention in computer vision and pattern recognition community. In this paper, a novel symmetric geometric configuration, named Symmetric Binary Tree (SBT) which has multiple symmetric branch pairs and can change in size, is designed to mine the multiple scale co-occurrence texture patterns. The resulting SBT descriptors encode both shape and texture features which make them more informative than the existing individual descriptors and co-occurrence features. A novel feature fusion scheme, named K-NN Based Handcrafted and Deep Features Fusion (KNN-HDFF) that encodes the neighbouring information of distance measure, is proposed for further boosting the retrieval performance. Extensive experiments conducted on the challenging soybean cultivar leaf image dataset and peanut cultivar leaf image dataset consistently indicate the superiority of the proposed method over the state-of-the-art methods on fine-grained leaf image retrieval . We also conduct extensive experiments of feature fusions using the proposed KNN-HDFF on the benchmark datasets and the experimental results prove its potential for improving the performance of cultivar identification which also indicates that fusing handcrafted and deep features may be the direction to address the challenging fine-grained image recognition problem.},
  archive      = {J_PR},
  author       = {Xin Chen and Bin Wang and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2022.108769},
  journal      = {Pattern Recognition},
  pages        = {108769},
  shortjournal = {Pattern Recognition},
  title        = {Symmetric binary tree based co-occurrence texture pattern mining for fine-grained plant leaf image retrieval},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised discriminative feature learning via finding a
clustering-friendly embedding space. <em>PR</em>, <em>129</em>, 108768.
(<a href="https://doi.org/10.1016/j.patcog.2022.108768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an enhanced deep clustering network (EDCN), which is composed of a Feature Extractor, a Conditional Generator, a Discriminator and a Siamese Network. Specifically, we will utilize two kinds of generated data based on adversarial training , as well as the original data, to train the Feature Extractor for learning effective latent representations. In addition, we adopt the Siamese network to find an embedding space, where a better affinity similarity matrix is obtained as the key to success of spectral clustering in providing reliable pseudo-labels. Particularly, the obtained pseudo-labels will be used to generate realistic data by the Generator. Finally, the discriminator is used to model the real joint distribution of data and corresponding latent representations for Feature Extractor enhancement. To evaluate our proposed EDCN, we conduct extensive experiments on multiple data sets including MNIST, USPS, FRGC, CIFAR-10, STL-10, and Fashion-MNIST by comparing our method with a number of state-of-the-art deep clustering methods , and experimental results demonstrate its effectiveness and superiority.},
  archive      = {J_PR},
  author       = {Wenming Cao and Zhongfan Zhang and Cheng Liu and Rui Li and Qianfen Jiao and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1016/j.patcog.2022.108768},
  journal      = {Pattern Recognition},
  pages        = {108768},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised discriminative feature learning via finding a clustering-friendly embedding space},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SELF-LLP: Self-supervised learning from label proportions
with self-ensemble. <em>PR</em>, <em>129</em>, 108767. (<a
href="https://doi.org/10.1016/j.patcog.2022.108767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem called learning from label proportions (LLP), where the training data is arranged into various bags, with only the proportions of different categories in each bag available. Existing efforts mainly focus on training a model with only the limited proportion information in a weakly supervised manner, thus result in apparent performance gap to supervised learning, as well as computational inefficiency. In this work, we propose a multi-task pipeline called SELF-LLP to make full use of the information contained in the data and model themselves. Specifically, to intensively learn representation from the data, we leverage the self-supervised learning as a plug-in auxiliary task to learn better transferable visual representation. The main insight is to benefit from the self-supervised representation learning with deep model, as well as improving classification performance by a large margin. Meanwhile, in order to better leverage the implicit benefits from the model itself, we incorporate the self-ensemble strategy to guide the training process with an auxiliary supervision information, which is constructed by aggregating multiple previous network predictions. Furthermore, a ramp-up mechanism is further employed to stabilize the training process. In the extensive experiments, our method demonstrates compelling advantages in both accuracy and efficiency over several state-of-the-art LLP approaches.},
  archive      = {J_PR},
  author       = {Jiabin Liu and Zhiquan Qi and Bo Wang and YingJie Tian and Yong Shi},
  doi          = {10.1016/j.patcog.2022.108767},
  journal      = {Pattern Recognition},
  pages        = {108767},
  shortjournal = {Pattern Recognition},
  title        = {SELF-LLP: Self-supervised learning from label proportions with self-ensemble},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pay attention to what you read: Non-recurrent handwritten
text-line recognition. <em>PR</em>, <em>129</em>, 108766. (<a
href="https://doi.org/10.1016/j.patcog.2022.108766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of recurrent neural networks for handwriting recognition marked an important milestone reaching impressive recognition accuracies despite the great variability that we observe across different writing styles. Sequential architectures are a perfect fit to model text lines, not only because of the inherent temporal aspect of text, but also to learn probability distributions over sequences of characters and words. However, using such recurrent paradigms comes at a cost at training stage, since their sequential pipelines prevent parallelization . In this work, we introduce a novel method that bypasses any recurrence during the training process with the use of transformer models. By using multi-head self-attention layers both at the visual and textual stages, we are able to tackle character recognition as well as to learn language-related dependencies of the character sequences to be decoded. Our model is unconstrained to any predefined vocabulary, being able to recognize out-of-vocabulary words, i.e. words that do not appear in the training vocabulary. We significantly advance over prior art and demonstrate that satisfactory recognition accuracies are yielded even in few-shot learning scenarios.},
  archive      = {J_PR},
  author       = {Lei Kang and Pau Riba and Marçal Rusiñol and Alicia Fornés and Mauricio Villegas},
  doi          = {10.1016/j.patcog.2022.108766},
  journal      = {Pattern Recognition},
  pages        = {108766},
  shortjournal = {Pattern Recognition},
  title        = {Pay attention to what you read: Non-recurrent handwritten text-line recognition},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Security and privacy enhanced smartphone-based gait
authentication with random representation learning and digital lockers.
<em>PR</em>, <em>129</em>, 108765. (<a
href="https://doi.org/10.1016/j.patcog.2022.108765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait data captured by inertial sensors of smartphone have demonstrated promising results on user authentication . However, most existing models stored the enrolled gait pattern in plaintext for matching with the pattern being validated, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data captured by smartphone sensors the random keys for user authentication , meanwhile, secures the gait pattern . First , we propose a revocable and random binary string extraction method using deep neural network followed by feature-wise binarization . A novel loss function for network optimization is also designed, to tackle not only the intra-user stability but also the inter-user randomness. Second , we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string a random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. The evaluation showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441\% 5.441\% . In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled the irreversibility and unlinkability requirements.},
  archive      = {J_PR},
  author       = {Lam Tran and Thuc Nguyen and Hyunil Kim and Deokjai Choi},
  doi          = {10.1016/j.patcog.2022.108765},
  journal      = {Pattern Recognition},
  pages        = {108765},
  shortjournal = {Pattern Recognition},
  title        = {Security and privacy enhanced smartphone-based gait authentication with random representation learning and digital lockers},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximization and restoration: Action segmentation through
dilation passing and temporal reconstruction. <em>PR</em>, <em>129</em>,
108764. (<a href="https://doi.org/10.1016/j.patcog.2022.108764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action segmentation aims to split videos into segments of different actions. Recent work focuses on dealing with long-range dependencies of long, untrimmed videos, but still suffers from over-segmentation and performance saturation due to increased model complexity. This paper addresses the aforementioned issues through a divide-and-conquer strategy that first maximizes the frame-wise classification accuracy of the model and then reduces the over-segmentation errors. This strategy is implemented with the Dilation Passing and Reconstruction Network, composed of the Dilation Passing Network, which primarily aims to increase accuracy by propagating information of different dilations, and the Temporal Reconstruction Network, which reduces over-segmentation errors by temporally encoding and decoding the output features from the Dilation Passing Network. We also propose a weighted temporal mean squared error loss that further reduces over-segmentation. Through evaluations on the 50Salads, GTEA, and Breakfast datasets, we show that our model achieves significant results compared to existing state-of-the-art models.},
  archive      = {J_PR},
  author       = {Junyong Park and Daekyum Kim and Sejoon Huh and Sungho Jo},
  doi          = {10.1016/j.patcog.2022.108764},
  journal      = {Pattern Recognition},
  pages        = {108764},
  shortjournal = {Pattern Recognition},
  title        = {Maximization and restoration: Action segmentation through dilation passing and temporal reconstruction},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The devil in the tail: Cluster consolidation plus cluster
adaptive balancing loss for unsupervised person re-identification.
<em>PR</em>, <em>129</em>, 108763. (<a
href="https://doi.org/10.1016/j.patcog.2022.108763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) is to retrieve pedestrians from different camera views without supervision information. State-of-the-art methods are usually built upon training a convolution neural network with pseudo labels generated by clustering. Unfortunately, the pseudo labels are highly unbalanced and heavily noisy, carrying ineffective or even erroneous supervision information. To address these deficiencies, we present an effective clustering and reorganization approach, called Cluster Consolidation, which aims to separate a small proportion of unreliable data points from each cluster. This approach benefits to improve the quality of the pseudo labels, but also yields more tiny clusters. Thus, we further propose a Cluster Adaptive Balancing (CAB) loss to effectively train the network with the imbalance pseudo labels, where our CAB loss is able to automatically balance the importance of each cluster. We conduct extensive experiments on widely used person Re-ID benchmark datasets and demonstrate the effectiveness of our proposals.},
  archive      = {J_PR},
  author       = {Mingkun Li and He Sun and Chaoqun Lin and Chun-Guang Li and Jun Guo},
  doi          = {10.1016/j.patcog.2022.108763},
  journal      = {Pattern Recognition},
  pages        = {108763},
  shortjournal = {Pattern Recognition},
  title        = {The devil in the tail: Cluster consolidation plus cluster adaptive balancing loss for unsupervised person re-identification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D hand pose and shape estimation from RGB images for
keypoint-based hand gesture recognition. <em>PR</em>, <em>129</em>,
108762. (<a href="https://doi.org/10.1016/j.patcog.2022.108762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality , and hand gesture recognition . Currently, reasonable estimations can be computed from single RGB images , especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images , a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.},
  archive      = {J_PR},
  author       = {Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Adriano Fragomeni and Daniele Pannone},
  doi          = {10.1016/j.patcog.2022.108762},
  journal      = {Pattern Recognition},
  pages        = {108762},
  shortjournal = {Pattern Recognition},
  title        = {3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ChildGAN: Face aging and rejuvenation to find missing
children. <em>PR</em>, <em>129</em>, 108761. (<a
href="https://doi.org/10.1016/j.patcog.2022.108761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Child-face aging and rejuvenation have amassed considerable active research interest, owing to their immense impact on a broad range of social and security applications, e.g., digital entertainment, fashion and wellness, and searching for long-lost children using childhood photos. All current face aging approaches based on generative adversarial networks (GANs) focus on adult images or long-term aging. We present a new large-scale longitudinal Indian child (ICD) benchmark dataset to facilitate face age progression and regression, cross-age face recognition, age estimation, gender prediction, and kinship face recognition to alleviate these issues. Furthermore, we propose an automatic child-face age progression and regression model, namely, ChildGAN, that generates visually realistic images for enhanced face-identification accuracy while preserving the identity. Consequently, we have trained state-of-the-art (SOTA) face aging models on ICD for comprehensive qualitative and quantitative evaluations . We also present a multi-racial experiments dataset named Multi-Racial Child Dataset (MRCD) containing 64,965 child face images. The images are selected from publicly available datasets and web crawling. Finally, we investigate the generalization of ChildGAN by experimenting with White, Black, Asian, and Indian races. The experimental results suggest that the proposed ChildGAN and SOTA models can aid in reconnecting young children, who were lost at a young age as victims of child trafficking or abduction, with their families. The model and the MRCD web crawled images are available at https://github.com/praveenkumarchandaliya/ChildGAN_Tamp1/ .},
  archive      = {J_PR},
  author       = {Praveen Kumar Chandaliya and Neeta Nain},
  doi          = {10.1016/j.patcog.2022.108761},
  journal      = {Pattern Recognition},
  pages        = {108761},
  shortjournal = {Pattern Recognition},
  title        = {ChildGAN: Face aging and rejuvenation to find missing children},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-view kernel transfer. <em>PR</em>, <em>129</em>,
108759. (<a href="https://doi.org/10.1016/j.patcog.2022.108759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the kernel completion problem with the presence of multiple views in the data. In this context the data samples can be fully missing in some views, creating missing columns and rows to the kernel matrices that are calculated individually for each view. We propose to solve the problem of completing the kernel matrices with Cross-View Kernel Transfer (CVKT) procedure, in which the features of the other views are transformed to represent the view under consideration. The transformations are learned with kernel alignment to the known part of the kernel matrix, allowing for finding generalizable structures in the kernel matrix under completion. Its missing values can then be predicted with the data available in other views. We illustrate the benefits of our approach with simulated data, multivariate digits dataset and multi-view dataset on gesture classification, as well as with real biological datasets from studies of pattern formation in early Drosophila melanogaster embryogenesis.},
  archive      = {J_PR},
  author       = {Riikka Huusari and Cécile Capponi and Paul Villoutreix and Hachem Kadri},
  doi          = {10.1016/j.patcog.2022.108759},
  journal      = {Pattern Recognition},
  pages        = {108759},
  shortjournal = {Pattern Recognition},
  title        = {Cross-view kernel transfer},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel wavelet-based hierarchical networks for image
compressed sensing. <em>PR</em>, <em>129</em>, 108758. (<a
href="https://doi.org/10.1016/j.patcog.2022.108758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based compressed sensing (CS) algorithms have been reported, which remarkably achieve pleasing reconstruction quality with low computational complexity . However, the sampling process of the common deep learning-based CS methods and the conventional ones cannot sufficiently exploit the structured sparsity within image sequences, especially in preserving finer texture details. In this paper, we propose a novel multilevel wavelet-based hierarchical networks for image compressed sensing (dubbed MWHCS-Net). In particular, MWHCS-Net consists of three modules: a sampling module based on a multilevel wavelet transform, a hierarchical initial reconstruction module and a lightweight deep reconstruction module. Motivated by the fact that a sparser signal is easier to reconstruct accurately, we present the sampling module based on multilevel wavelet transform with hierarchical subspace learning for progressive acquisition of measurements to further optimize sampling efficiency and stability. To enhance the finer texture details, the hierarchical initial reconstruction module is designed as a basic initial reconstruction network plus an enhanced initial reconstruction network, which corresponding to the dominant structure component and the texture detail component of the reconstructed image, respectively. At the same time, we also further explore the impact of the hierarchical initial reconstruction module and prove that the texture detail component branch plays an important role in improving the reconstruction quality. Experimental results demonstrate that the proposed MWHCS-Net achieves the state-of-the-art performance while maintaining an efficient running speed. Furthermore, MWHCS-Net outperforms the existing image CS methods based on deep learning in terms of anti-noise performance in most cases.},
  archive      = {J_PR},
  author       = {Zhu Yin and WuZhen Shi and Zhongcheng Wu and Jun Zhang},
  doi          = {10.1016/j.patcog.2022.108758},
  journal      = {Pattern Recognition},
  pages        = {108758},
  shortjournal = {Pattern Recognition},
  title        = {Multilevel wavelet-based hierarchical networks for image compressed sensing},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EEG-ConvTransformer for single-trial EEG-based visual
stimulus classification. <em>PR</em>, <em>129</em>, 108757. (<a
href="https://doi.org/10.1016/j.patcog.2022.108757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks . Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention.},
  archive      = {J_PR},
  author       = {Subhranil Bagchi and Deepti R. Bathula},
  doi          = {10.1016/j.patcog.2022.108757},
  journal      = {Pattern Recognition},
  pages        = {108757},
  shortjournal = {Pattern Recognition},
  title        = {EEG-ConvTransformer for single-trial EEG-based visual stimulus classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realistic frontal face reconstruction using coupled
complementarity of far-near-sighted face images. <em>PR</em>,
<em>129</em>, 108754. (<a
href="https://doi.org/10.1016/j.patcog.2022.108754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is still a huge gap in the accuracy of face recognition in public video surveillance scenarios. The far-sighted low-resolution (LR) frontal faces have holistic facial profiles but lack sufficient clearness, while the near-sighted high-resolution (HR) tilted faces show rich facial details yet incomplete facial structure suffering from the overhead self-occlusion of the head blocking the face. Following this observation, this paper proposes a dual-branch HR frontal face reconstruction network to explicitly exploit such coupled complementarity hidden in the far-near face images of the same subject, where one branch performs super-resolution (SR) of the LR frontal face and the other branch performs detail fusion and holistic compensation between multiple HR tilted faces as well as the super-resolved frontal result. In particular, we propose a secondary relevance attention mechanism to enhance the embedding of key features, which sequentially performs rough and precise feature matching and embedding, thus enabling coarse-to-fine progressive compensation. Further, scale-entangled densely connected blocks (SEDCB) are used to gradually integrate the relevance information at different scales (due to the different sighting distances) to promote the information interaction between the features of tilted faces. Besides, we also propose a ternary coupled sample pair (LR far-sighted frontal face, HR near-sighted tilted face, normal ground truth clear face) training scheme to supervise the network optimization. Extensive experimental results on two real-world tilt-view face datasets show that our method can not only reconstruct more realistic HR frontal faces but also facilitate the down-stream face identification task compared with the competing counterparts.},
  archive      = {J_PR},
  author       = {Kangli Zeng and Zhongyuan Wang and Tao Lu and Jianyu Chen and Baojin Huang and Zhen Han and Xin Tian},
  doi          = {10.1016/j.patcog.2022.108754},
  journal      = {Pattern Recognition},
  pages        = {108754},
  shortjournal = {Pattern Recognition},
  title        = {Realistic frontal face reconstruction using coupled complementarity of far-near-sighted face images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orthogonal channel attention-based multi-task learning for
multi-view facial expression recognition. <em>PR</em>, <em>129</em>,
108753. (<a href="https://doi.org/10.1016/j.patcog.2022.108753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view facial expression recognition (FER) is a challenging computer vision task due to the large intra-class difference caused by viewpoint variations. This paper presents a novel orthogonal channel attention-based multi-task learning (OCA-MTL) approach for FER. The proposed OCA-MTL approach adopts a Siamese convolutional neural network (CNN) to force the multi-view expression recognition model to learn the same features as the frontal expression recognition model. To further enhance the recognition accuracy of non-frontal expression, the multi-view expression model adopts a multi-task learning framework that regards head pose estimation (HPE) as an auxiliary task. A separated channel attention (SCA) module is embedded in the multi-task learning framework to generate individual attention for FER and HPE. Furthermore, orthogonal channel attention loss is presented to force the model to employ different feature channels to represent the facial expression and head pose, thereby decoupling them. The proposed approach is performed on two public facial expression datasets to evaluate its effectiveness and achieves an average recognition accuracy rate of 88.41\%\% under 13 viewpoints on Multi-PIE and 89.04\% under 5 viewpoints on KDEF, outperforming state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jingying Chen and Lei Yang and Lei Tan and Ruyi Xu},
  doi          = {10.1016/j.patcog.2022.108753},
  journal      = {Pattern Recognition},
  pages        = {108753},
  shortjournal = {Pattern Recognition},
  title        = {Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent flow networks: A recurrent latent variable model
for density estimation of urban mobility. <em>PR</em>, <em>129</em>,
108752. (<a href="https://doi.org/10.1016/j.patcog.2022.108752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). To do so, we argue that predictive models should aim to explicitly disentangle between temporal and spatial variability in the evolution of urban mobility demand. However, current approaches typically ignore this distinction by either treating both sources of variability jointly, or completely ignoring their presence in the first place. In this paper, we propose recurrent flow networks 1 (RFN), where we explore the inclusion of (i) latent random variables in the hidden state of recurrent neural networks to model temporal variability, and (ii) normalizing flows to model the spatial distribution of mobility demand. We demonstrate how predictive models explicitly disentangling between spatial and temporal variability exhibit several desirable properties , and empirically show how this enables the generation of distributions matching potentially complex urban topologies.},
  archive      = {J_PR},
  author       = {Daniele Gammelli and Filipe Rodrigues},
  doi          = {10.1016/j.patcog.2022.108752},
  journal      = {Pattern Recognition},
  pages        = {108752},
  shortjournal = {Pattern Recognition},
  title        = {Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized multi-output gaussian process censored
regression. <em>PR</em>, <em>129</em>, 108751. (<a
href="https://doi.org/10.1016/j.patcog.2022.108751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modelling censored observations (i.e. data in which the value of a measurement or observation is un-observable beyond a given threshold), a typical approach in current regression methods is to use a censored-Gaussian (i.e. Tobit) model to describe the conditional output distribution. In this paper, as in the case of missing data, we argue that exploiting correlations between multiple outputs can enable models to better address the bias introduced by censored data. To do so, we introduce a heteroscedastic multi-output Gaussian process model which combines the non-parametric flexibility of GPs with the ability to leverage information from correlated outputs under input-dependent noise conditions. To address the resulting inference intractability, we further devise a variational bound to the marginal log-likelihood suitable for stochastic optimization . We empirically evaluate our model against other generative models for censored data on both synthetic and real world tasks and further show how it can be generalized to deal with arbitrary likelihood functions. Results show how the added flexibility allows our model to better estimate the underlying non-censored (i.e. true) process under potentially complex censoring dynamics.},
  archive      = {J_PR},
  author       = {Daniele Gammelli and Kasper Pryds Rolsted and Dario Pacino and Filipe Rodrigues},
  doi          = {10.1016/j.patcog.2022.108751},
  journal      = {Pattern Recognition},
  pages        = {108751},
  shortjournal = {Pattern Recognition},
  title        = {Generalized multi-output gaussian process censored regression},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negational symmetry of quantum neural networks for binary
pattern classification. <em>PR</em>, <em>129</em>, 108750. (<a
href="https://doi.org/10.1016/j.patcog.2022.108750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, the behavior of QNNs in binary pattern classification is still underexplored. In this work, we find that QNNs have an Achilles’ heel in binary pattern classification. To illustrate this point, we provide a theoretical insight into the properties of QNNs by presenting and analyzing a new form of symmetry embedded in a family of QNNs with full entanglement , which we term negational symmetry . Due to negational symmetry, QNNs can not differentiate between a quantum binary signal and its negational counterpart. We empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google’s quantum computing framework. Both theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.},
  archive      = {J_PR},
  author       = {Nanqing Dong and Michael Kampffmeyer and Irina Voiculescu and Eric Xing},
  doi          = {10.1016/j.patcog.2022.108750},
  journal      = {Pattern Recognition},
  pages        = {108750},
  shortjournal = {Pattern Recognition},
  title        = {Negational symmetry of quantum neural networks for binary pattern classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance exploitation for learning temporary concepts from
sparsely labeled drifting data streams. <em>PR</em>, <em>129</em>,
108749. (<a href="https://doi.org/10.1016/j.patcog.2022.108749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning from streaming data sources becomes more and more popular due to the increasing number of online tools and systems. Dealing with dynamic and everlasting problems poses new challenges for which traditional batch-based offline algorithms turn out to be insufficient in terms of computational time and predictive performance . One of the most crucial limitations is that we cannot assume having an access to a finite and complete data set – we always have to be ready for new data that may complement our model. This poses a critical problem of providing labels for potentially unbounded streams. In real world, we are forced to deal with very strict budget limitations, therefore, we will most likely face the scarcity of annotated instances, which are essential in supervised learning. In our work, we emphasize this problem and propose a novel instance exploitation technique. We show that when: (i) data is characterized by temporary non-stationary concepts, and (ii) there are very few labels spanned across a long time horizon, it is actually better to risk overfitting and adapt models more aggressively by exploiting the only labeled instances we have, instead of sticking to a standard learning mode and suffering from severe underfitting. We present different strategies and configurations for our methods, as well as an ensemble algorithm that attempts to maintain a sweet spot between risky and normal adaptation. Finally, we conduct a complex in-depth comparative analysis of our methods, using state-of-the-art streaming algorithms relevant for the given problem.},
  archive      = {J_PR},
  author       = {Łukasz Korycki and Bartosz Krawczyk},
  doi          = {10.1016/j.patcog.2022.108749},
  journal      = {Pattern Recognition},
  pages        = {108749},
  shortjournal = {Pattern Recognition},
  title        = {Instance exploitation for learning temporary concepts from sparsely labeled drifting data streams},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Methods for segmenting cracks in 3d images of concrete: A
comparison based on semi-synthetic images. <em>PR</em>, <em>129</em>,
108747. (<a href="https://doi.org/10.1016/j.patcog.2022.108747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast.},
  archive      = {J_PR},
  author       = {Tin Barisin and Christian Jung and Franziska Müsebeck and Claudia Redenbach and Katja Schladitz},
  doi          = {10.1016/j.patcog.2022.108747},
  journal      = {Pattern Recognition},
  pages        = {108747},
  shortjournal = {Pattern Recognition},
  title        = {Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federating recommendations using differentially private
prototypes. <em>PR</em>, <em>129</em>, 108746. (<a
href="https://doi.org/10.1016/j.patcog.2022.108746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods exploit similarities in users’ activity patterns to provide recommendations in applications across a wide range of fields including entertainment, dating, and commerce. However, in domains that demand protection of personally sensitive data, such as medicine or banking, how can we learn recommendation models without accessing the sensitive data and without inadvertently leaking private information? Many situations in the medical field prohibit centralizing the data from different hospitals and thus require learning from information kept in separate databases. We propose a new federated approach to learning global and local private models for recommendation without collecting raw data, user statistics, or information about personal preferences. Our method produces a set of locally learned prototypes that allow us to infer global behavioral patterns while providing differential privacy guarantees for users in any database of the system. By requiring only two rounds of communication, we both reduce the communication costs and avoid excessive privacy loss associated with typical federated learning iterative procedures. We test our framework on synthetic data, real federated medical data, and a federated version of Movielens ratings. We show that local adaptation of the global model allows the proposed method to outperform centralized matrix-factorization-based recommender system models, both in terms of the accuracy of matrix reconstruction and in terms of the relevance of recommendations, while maintaining provable privacy guarantees. We also show that our method is more robust and has smaller variance than individual models learned by independent entities.},
  archive      = {J_PR},
  author       = {Mónica Ribero and Jette Henderson and Sinead Williamson and Haris Vikalo},
  doi          = {10.1016/j.patcog.2022.108746},
  journal      = {Pattern Recognition},
  pages        = {108746},
  shortjournal = {Pattern Recognition},
  title        = {Federating recommendations using differentially private prototypes},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering by centroid drift and boundary shrinkage.
<em>PR</em>, <em>129</em>, 108745. (<a
href="https://doi.org/10.1016/j.patcog.2022.108745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating the centers before assigning clustering labels is a traditional routine of clustering methods, which also limits the development of new clustering ideas. In this paper, we achieve the clustering task by firstly identifying the boundary points in the feature space, and then we shrink the boundary points to allocate the un-clustered points. Concretely, we propose a Centroid Drift (CD) metric and a Boundary Shrinkage (BS) strategy to detect boundary points in the feature space and allocate labels for un-clustered points, respectively. Both the CD and BS are closely related to the pre-computed k-nearest neighbor matrix, contributing to the decrease of algorithm parameters. Moreover, the common problems of noise points and non-uniform density distribution of data points in clustering task can also be alleviated with our proposed large value suppression and normalization of k-nearest neighbor distance techniques. The experiments on synthetic datasets, real-world face image datasets and hyperspectral images demonstrate the superiorities of our proposed clustering framework.},
  archive      = {J_PR},
  author       = {Hui Qv and Tao Ma and Xinyi Tong and Xuhui Huang and Zhe Ma and Jiehong Feng},
  doi          = {10.1016/j.patcog.2022.108745},
  journal      = {Pattern Recognition},
  pages        = {108745},
  shortjournal = {Pattern Recognition},
  title        = {Clustering by centroid drift and boundary shrinkage},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation learning using deep random vector functional
link networks for clustering. <em>PR</em>, <em>129</em>, 108744. (<a
href="https://doi.org/10.1016/j.patcog.2022.108744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method , which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.},
  archive      = {J_PR},
  author       = {Minghui Hu and P.N. Suganthan},
  doi          = {10.1016/j.patcog.2022.108744},
  journal      = {Pattern Recognition},
  pages        = {108744},
  shortjournal = {Pattern Recognition},
  title        = {Representation learning using deep random vector functional link networks for clustering},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Believe the HiPe: Hierarchical perturbation for fast,
robust, and model-agnostic saliency mapping. <em>PR</em>, <em>129</em>,
108743. (<a href="https://doi.org/10.1016/j.patcog.2022.108743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping – a popular visual attribution method – is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints . We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps . Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods – and are over 20 × × faster to compute.},
  archive      = {J_PR},
  author       = {Jessica Cooper and Ognjen Arandjelović and David J Harrison},
  doi          = {10.1016/j.patcog.2022.108743},
  journal      = {Pattern Recognition},
  pages        = {108743},
  shortjournal = {Pattern Recognition},
  title        = {Believe the HiPe: Hierarchical perturbation for fast, robust, and model-agnostic saliency mapping},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation learning with deep sparse auto-encoder for
multi-task learning. <em>PR</em>, <em>129</em>, 108742. (<a
href="https://doi.org/10.1016/j.patcog.2022.108742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate an effective framework to achieve a better performance based on Deep Sparse auto-encoder for Multi-task Learning, called DSML for short. To learn the reconstructed and higher-level features on cross-domain instances for multiple tasks, we combine the labeled and unlabeled data from all tasks to reconstruct the feature representations. Furthermore, we propose the model of Stacked Reconstruction Independence Component Analysis (SRICA for short) for the optimization of feature representations with a large amount of unlabeled data, which can effectively address the redundancy of image data. Our proposed SRICA model is developed from RICA and is based on deep sparse auto-encoder. In addition, we adopt a Semi-Supervised Learning method (SSL for short) based on model parameter regularization to build a unified model for multi-task learning. There are several advantages in our proposed framework as follows: 1) The proposed SRICA makes full use of a large amount of unlabeled data from all tasks. It is used to pursue an optimal sparsity feature representation, which can overcome the over-fitting problem effectively. 2) The deep architecture used in our SRICA model is applied for higher-level and better representation learning , which is designed to train on patches for sphering the input data. 3) Training parameters in our proposed framework has lower computational cost compared to other common deep learning methods such as stacked denoising auto-encoders. Extensive experiments on several real image datasets demonstrate our proposed framework outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yi Zhu and Xindong Wu and Jipeng Qiang and Xuegang Hu and Yuhong Zhang and Peipei Li},
  doi          = {10.1016/j.patcog.2022.108742},
  journal      = {Pattern Recognition},
  pages        = {108742},
  shortjournal = {Pattern Recognition},
  title        = {Representation learning with deep sparse auto-encoder for multi-task learning},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive privileged knowledge distillation for online
action detection. <em>PR</em>, <em>129</em>, 108741. (<a
href="https://doi.org/10.1016/j.patcog.2022.108741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e. , only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture . To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.},
  archive      = {J_PR},
  author       = {Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian},
  doi          = {10.1016/j.patcog.2022.108741},
  journal      = {Pattern Recognition},
  pages        = {108741},
  shortjournal = {Pattern Recognition},
  title        = {Progressive privileged knowledge distillation for online action detection},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Atlanta scaled layouts from non-central panoramas.
<em>PR</em>, <em>129</em>, 108740. (<a
href="https://doi.org/10.1016/j.patcog.2022.108740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a single non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without additional nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas and required algorithms being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover scaled 3D layouts. The results of our experiments show that we improve state-of-the-art methods for layout recovery and line extraction in non-central projection systems. We completely solve the problem both in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors’ knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.},
  archive      = {J_PR},
  author       = {Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero},
  doi          = {10.1016/j.patcog.2022.108740},
  journal      = {Pattern Recognition},
  pages        = {108740},
  shortjournal = {Pattern Recognition},
  title        = {Atlanta scaled layouts from non-central panoramas},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An end-to-end identity association network based on geometry
refinement for multi-object tracking. <em>PR</em>, <em>129</em>, 108738.
(<a href="https://doi.org/10.1016/j.patcog.2022.108738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-target tracking, object interactions and occlusions are two significant factors that affect tracking performance. To settle this, we propose an identity association network (IANet) that integrates the geometry refinement network (GRNet) and the identity verification (IV) module to perform data association and reason the mapping between the detections and tracklets. In our data association process, the object drifts caused by object interactions are suppressed effectively by encoding the direction and velocity of objects to refine the geometric position of tracklets. The tracklets with refined geometric information are further utilized in the IV module to achieve a sufficient encoding of multivariate spatial cues including both appearance and geometry information, which defeats the misleading impacts of interactions and occlusions dramatically in multi-object tracking. The extensive experiments and comparative evaluations have demonstrated that our proposed method can significantly outperform many state-of-the-art methods on benchmarks of 2D MOT2015, MOT16, MOT17, MOT20, and KITTI by using public detection and online settings.},
  archive      = {J_PR},
  author       = {Rui Li and Baopeng Zhang and Zhu Teng and Jianping Fan},
  doi          = {10.1016/j.patcog.2022.108738},
  journal      = {Pattern Recognition},
  pages        = {108738},
  shortjournal = {Pattern Recognition},
  title        = {An end-to-end identity association network based on geometry refinement for multi-object tracking},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the facial expression recognition and its
interpretability via generating expression pattern-map. <em>PR</em>,
<em>129</em>, 108737. (<a
href="https://doi.org/10.1016/j.patcog.2022.108737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition focuses on extracting expression-related features on a face. In this paper, a novel method is proposed for facial expression modeling based on the following two aspects: seeking expression-related regions more accurately, and enhancing expression features more discriminating. To this end, we design a model containing three submodules: the Expression Feature Extractor ( EFE ), the Expression Mask Refiner ( EMR ), and the Expression Pattern-Map Generator ( EPMG ). The EFE module is the backbone that extracts expression features and generates a coarse attention mask which roughly indicates expression-related regions. The EMR module refines the mask to be more precise by modeling the relationship among expression-related regions, and generates the masked features. The EPMG module utilizes the masked features to further model the fusion and extraction process which obtains a compact and discriminating expression-salient embedding for recognition, and generates an expression pattern-map. We propose the concept of the expression pattern-map, which provides a unified visualization of expression features and improves the interpretability of facial expression recognition. Our model is evaluated on four public datasets (CK+, Oulu-CASIA, RAF-DB, AffectNet), and achieves the competitive performance compared with the state-of-the-art.},
  archive      = {J_PR},
  author       = {Jing Zhang and Huimin Yu},
  doi          = {10.1016/j.patcog.2022.108737},
  journal      = {Pattern Recognition},
  pages        = {108737},
  shortjournal = {Pattern Recognition},
  title        = {Improving the facial expression recognition and its interpretability via generating expression pattern-map},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AVPL: Augmented visual perception learning for person
re-identification and beyond. <em>PR</em>, <em>129</em>, 108736. (<a
href="https://doi.org/10.1016/j.patcog.2022.108736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an Augmented Visual Perception Learning (AVPL) method for Person Re-identification (ReID) which is inspired by the two-stream hypothesis theory of Human Visual System (HVS). Deep learning methods dominate ReID and many state-of-the-art performances are achieved from the perspective of optimizing the model of ’what’ visual pathway. It does not blend ’what’ and ’where’ well. Our AVPL method uses the essential mechanism of the ventro-dorsal stream of the ’where’ visual pathway to expand the perception field of the model, and integrates with the ’what’ to complete the information of the visually salient regions . A novel Batch Attention (BA), the key component of our Augmented Visual Perception (AVP) module, is proposed to apply fusion and augmentation into all input feature maps within each batch. Through AVP module, the improved attention-based model attaches more importance to enhancement of salient features , therefore acquiring better perceptual ability of salient regions which provide the most distinguishably distinctions for ReID . Extensive experiments have been carried out on four main stream ReID datasets and two recognition datasets. In terms of ReID, our method achieves rank-1 accuracy of 95.2\% on CUHK03-NP, 98.7\% on Market-1501, 96.0\% on DukeMTMC-reID and 88.8\% on MSMT17-V1, outperforming the state-of-the-art methods by a large margin. Besides, it has been experimentally proven to be applicable and effective in other recognition tasks including facial expression recognition and action recognition with an improved accuracy.},
  archive      = {J_PR},
  author       = {Yewen Huang and Sicheng Lian and Haifeng Hu},
  doi          = {10.1016/j.patcog.2022.108736},
  journal      = {Pattern Recognition},
  pages        = {108736},
  shortjournal = {Pattern Recognition},
  title        = {AVPL: Augmented visual perception learning for person re-identification and beyond},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Versatile, full‐spectrum, and swift network sampling for
model generation. <em>PR</em>, <em>129</em>, 108729. (<a
href="https://doi.org/10.1016/j.patcog.2022.108729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given one task, it is difficult to generate CNN models for many different hardware platforms with extremely diverse computing power for this task. Repeating network pruning or architecture search for each platform is very time-consuming. In this paper, we propose properties that are required for this model generation problem: versatile (fits diverse applications and network structures), full-spectrum (generates models for devices with tiny to gigantic computing power), and swift (total training time for all platforms is short, and generated models have low latency). We show that existing methods do not satisfy these requirements and propose a VFS method (the V/F/S represents Versatile/Full-spectrum/Swift, respectively). VFS uses importance sampling to sample many submodels with versatile structures and with different input image resolutions. We propose new fine-tuning strategies that only need to fine-tune a best candidate submodel for few epochs for each platform. VFS satisfies all three requirements. It generates versatile models with low latency for diverse applications, is suitable for devices with a wide range of computing power differences, and the models which are generated by VFS achieve state-of-the-art accuracy.},
  archive      = {J_PR},
  author       = {Huanyu Wang and Yongshun Zhang and Jianxin Wu},
  doi          = {10.1016/j.patcog.2022.108729},
  journal      = {Pattern Recognition},
  pages        = {108729},
  shortjournal = {Pattern Recognition},
  title        = {Versatile, full‐spectrum, and swift network sampling for model generation},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Query-efficient decision-based attack via sampling
distribution reshaping. <em>PR</em>, <em>129</em>, 108728. (<a
href="https://doi.org/10.1016/j.patcog.2022.108728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a limited query budget and only the final decision of a target model, how to find adversarial examples with low-magnitude distortion has attracted great attention among researchers. Recent solutions to this issue made use of the estimated normal vector at a boundary data point to search for adversarial examples . However, since the sampling independence between two sampling epochs, they still suffer from a prohibitively high query budget, which will get worse when the dimensionality of the attacked samples get increased. To push for further development, in this paper, we pay attention to a query-efficient method to estimate the normal vector for decision-based attack in high-dimensional space. Specifically, we propose a simple yet effective normal vector estimation framework for high-dimension decision-based attack via Sampling Distribution Reshaping, dubbed SDR. Next, SDR is incorporated into general geometric attack framework. Briefly, SDR leverages all the historically sampled noise to build a guiding vector, which will be used to reshape the next sampling distribution. Besides, we also extend SDR to different ℓ p ℓp norms for p = { 2 , ∞ } p={2,∞} and deploy low-frequency constraint to enhance the performance of SDR. Compared to peer decision-based attacks, SDR can reach the competitive ℓ p ℓp norms for p = { 2 , ∞ } p={2,∞} , according to extensive experimental evaluations against both defended and undefended classifiers. Since the simplicity and effectiveness of SDR, we think that reshaping the sampling distribution deserves further research in future works.},
  archive      = {J_PR},
  author       = {Xuxiang Sun and Gong Cheng and Lei Pei and Junwei Han},
  doi          = {10.1016/j.patcog.2022.108728},
  journal      = {Pattern Recognition},
  pages        = {108728},
  shortjournal = {Pattern Recognition},
  title        = {Query-efficient decision-based attack via sampling distribution reshaping},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention regularized semi-supervised learning with
class-ambiguous data for image classification. <em>PR</em>,
<em>129</em>, 108727. (<a
href="https://doi.org/10.1016/j.patcog.2022.108727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation via randomly combining training instances and interpolating the corresponding labels has shown impressive gains in image classification . However, model attention regions are not necessarily meaningful in class semantics, especially for the case of limited supervision. In this paper, we present a semi-supervised classification model based on Class-Ambiguous Data with Attention Regularization , which is referred to as CADAR. Specifically, we adopt a Random Regional Interpolation (RRI) module to construct complex and effective class-ambiguous data, such that the model behavior can be regularized around decision boundaries. By aggregating the parameters of a classification network over training epochs to produce more reliable predictions on unlabeled data , RRI can also be applied to them as well as labeled data. Further, the classifier is enforced to apply consistent attention on the original and constructed data. This is important for inducing the model to learn discriminative features from the class-related regions. The experiment results demonstrate that CADAR significantly benefits from the constructed data and attention regularization, and thus achieves superior performance across multiple standard benchmarks and different amounts of labeled data.},
  archive      = {J_PR},
  author       = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Hau-San Wong},
  doi          = {10.1016/j.patcog.2022.108727},
  journal      = {Pattern Recognition},
  pages        = {108727},
  shortjournal = {Pattern Recognition},
  title        = {Attention regularized semi-supervised learning with class-ambiguous data for image classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Pose error analysis method based on a single circular
feature. <em>PR</em>, <em>129</em>, 108726. (<a
href="https://doi.org/10.1016/j.patcog.2022.108726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The measurement accuracy of pose parameters based on a single circular feature depends not only on the accuracy of camera calibration and feature extraction but also on the relative pose of the feature and camera—different poses correspond to different error transmission coefficients . To obtain the relationship between measurement errors and pose parameters, we propose an error analysis method based on geometric interpretation . The method characterises measurement error by the sensitivity the imaging feature has to the variation of pose parameters. In addition, the method can be extended to the error analysis work of other coplanar features&#39; pose measurement algorithms. We conducted simulations on measurement errors of pose parameters under different poses, and the results show that the error distribution of pose parameters is in good agreement with the theoretical analysis. Moreover, we propose a method for judging and optimising outliers, and experimental results show the feasibility of this method.},
  archive      = {J_PR},
  author       = {Zepeng Wang and Derong Chen and Jiulu Gong},
  doi          = {10.1016/j.patcog.2022.108726},
  journal      = {Pattern Recognition},
  pages        = {108726},
  shortjournal = {Pattern Recognition},
  title        = {Pose error analysis method based on a single circular feature},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual relation network for temporal action localization.
<em>PR</em>, <em>129</em>, 108725. (<a
href="https://doi.org/10.1016/j.patcog.2022.108725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization is a challenging task for video understanding . Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization . To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities . Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Kun Xia and Le Wang and Sanping Zhou and Gang Hua and Wei Tang},
  doi          = {10.1016/j.patcog.2022.108725},
  journal      = {Pattern Recognition},
  pages        = {108725},
  shortjournal = {Pattern Recognition},
  title        = {Dual relation network for temporal action localization},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-resolution rectified gradient-based visual explanations
for weakly supervised segmentation. <em>PR</em>, <em>129</em>, 108724.
(<a href="https://doi.org/10.1016/j.patcog.2022.108724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual explanations for convolutional neural networks (CNNs) act as the backbone for weakly supervised segmentation with image-level labels. This paper proposes a high-resolution rectified gradient-based class activation mapping with bounding box annotations (bbox) to improve the initial seed for weakly supervised segmentation (WSS) tasks. HRCAM extends Grad-CAM by separating the gradient maps from the class activation maps from the shallow layer for higher resolution. Gradient rectified methods are proposed to improve the visualization and WSS score. Experiments and evaluations are conducted to verify the performance of HRCAM-BB on Pascal VOC 2012 and COCO datasets. On Pascal VOC 2012 set, our method achieves outstanding performance with a mean intersection over union (mIOU) of 71.6 with image-level labels and 78.2 with bbox on WSSS, and increases the WSIS mIOU (AP 50 50 ) to 52.1 with image-level labels, and 61.9 with bbox. our method surpasses the previous SOTA approach in the same condition.},
  archive      = {J_PR},
  author       = {Tianyou Zheng and Qiang Wang and Yue Shen and Xiang Ma and Xiaotian Lin},
  doi          = {10.1016/j.patcog.2022.108724},
  journal      = {Pattern Recognition},
  pages        = {108724},
  shortjournal = {Pattern Recognition},
  title        = {High-resolution rectified gradient-based visual explanations for weakly supervised segmentation},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional affinity learning based multi-branch
ensemble network for breast tumor segmentation in MRI. <em>PR</em>,
<em>129</em>, 108723. (<a
href="https://doi.org/10.1016/j.patcog.2022.108723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automatic breast tumor segmentation based on dynamic contrast-enhancement magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer analysis. However, the background parenchymal enhancement and large variations in tumor size, shape or appearance make the task very challenging, and also the segmentation performance is still not satisfactory, especially for non-mass enhancement (NME) and small size tumors ( ≤ 2 cm). To address these challenges, we propose a novel 3D affinity learning based multi-branch ensemble network for accurate breast tumor segmentation. Specifically, two different types of subnetworks are built to form a multi-branch network. The two subnetworks are equipped with effective operation components, i.e., residual connection and channel-wise attention or making use of dense connectivity patterns, which can process the input images in parallel. Second, we propose an end-to-end trainable 3D affinity learning based refinement module by calculating the similarities between features of voxels, which is useful in discovering more pixels belonging to breast tumors. Third, two local affinity matrices are constructed by 3D affinity learning, which are used to refine the segmentation outputs of two subnetworks, respectively. Finally, a novel ensemble module is proposed to combine the information derived from the subnetworks, which can hierarchically merge the local and global affinity matrices derived from subnetworks. A large-scale breast DCE-MR images dataset with 420 subjects are built for evaluation, and comprehensive experiments have been conducted to demonstrate that our proposed method achieves superior performance over state-of-the-art medical image segmentation methods.},
  archive      = {J_PR},
  author       = {Lei Zhou and Shuai Wang and Kun Sun and Tao Zhou and Fuhua Yan and Dinggang Shen},
  doi          = {10.1016/j.patcog.2022.108723},
  journal      = {Pattern Recognition},
  pages        = {108723},
  shortjournal = {Pattern Recognition},
  title        = {Three-dimensional affinity learning based multi-branch ensemble network for breast tumor segmentation in MRI},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Paying attention for adjacent areas: Learning discriminative
features for large-scale 3D scene segmentation. <em>PR</em>,
<em>129</em>, 108722. (<a
href="https://doi.org/10.1016/j.patcog.2022.108722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent improvements in analyzing large-scale 3D point clouds, several problems still exist: (a) segmentation models suffer from intra-class inconsistency and inter-class indistinction; (b) the existing methods ignore the inherent long-tailed class distribution of real-world 3D data. These problems result in unsatisfactory semantic segmentation predictions, especially in object adjacent areas. To handle these problems, this paper proposes a novel Adjacent areas Refinement Network (ARNet). Specifically, an Adjacent areas Refinement (AR) module is designed, which consists of two parallel attention blocks. Besides, our proposed attention blocks can process a large number of points ( N ∼ 10 5 ) with a slight increase in the computational complexity and time cost. Additionally, to deal with the inherent long-tailed class distribution in real-world 3D data, imbalance adjustment loss and occupancy regression loss are introduced. Based on this, the proposed network can handle the classification of both majority and minority classes, which is essential in distinguishing the ambiguous parts in large-scale 3D scenes . The proposed AR module and the loss functions can be easily integrated into the cutting-edge backbone networks , contributing to better performance in modeling semantic inter-dependencies and significantly improving the accuracy of the state-of-the-art semantic segmentation methods on indoor and outdoor scenes.},
  archive      = {J_PR},
  author       = {Mengtian Li and Yuan Xie and Lizhuang Ma},
  doi          = {10.1016/j.patcog.2022.108722},
  journal      = {Pattern Recognition},
  pages        = {108722},
  shortjournal = {Pattern Recognition},
  title        = {Paying attention for adjacent areas: Learning discriminative features for large-scale 3D scene segmentation},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised ensemble learning for genome sequencing.
<em>PR</em>, <em>129</em>, 108721. (<a
href="https://doi.org/10.1016/j.patcog.2022.108721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised ensemble learning refers to methods devised for a particular task that combine data provided by decision learners taking into account their reliability, which is usually inferred from the data. Here, the variant calling step of the next generation sequencing technologies is formulated as an unsupervised ensemble classification problem. A variant calling algorithm based on the expectation-maximization algorithm is further proposed that estimates the maximum-a-posteriori decision among a number of classes larger than the number of different labels provided by the learners. Experimental results with real human DNA sequencing data show that the proposed algorithm is competitive compared to state-of-the-art variant callers as GATK, HTSLIB , and Platypus.},
  archive      = {J_PR},
  author       = {Alba Pagès-Zamora and Idoia Ochoa and Gonzalo Ruiz Cavero and Pol Villalvilla-Ornat},
  doi          = {10.1016/j.patcog.2022.108721},
  journal      = {Pattern Recognition},
  pages        = {108721},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised ensemble learning for genome sequencing},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational cycle-consistent imputation adversarial networks
for general missing patterns. <em>PR</em>, <em>129</em>, 108720. (<a
href="https://doi.org/10.1016/j.patcog.2022.108720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputation of missing data is an important but challenging issue because we do not know the underlying distribution of the missing data. Previous imputation models have addressed this problem by assuming specific kinds of missing distributions. However, in practice, the mechanism of the missing data is unknown, so the most general case of missing pattern needs to be considered for successful imputation. In this paper, we present cycle-consistent imputation adversarial networks to discover the underlying distribution of missing patterns closely under some relaxations. Using adversarial training , our model successfully learns the most general case of missing patterns. Therefore our method can be applied to a wide variety of imputation problems. We empirically evaluated the proposed method with numerical and image data. The result shows that our method yields the state-of-the-art performance quantitatively and qualitatively on standard datasets.},
  archive      = {J_PR},
  author       = {Woojin Lee and Sungyoon Lee and Junyoung Byun and Hoki Kim and Jaewook Lee},
  doi          = {10.1016/j.patcog.2022.108720},
  journal      = {Pattern Recognition},
  pages        = {108720},
  shortjournal = {Pattern Recognition},
  title        = {Variational cycle-consistent imputation adversarial networks for general missing patterns},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised moving object segmentation using background
subtraction and optimal adversarial noise sample search. <em>PR</em>,
<em>129</em>, 108719. (<a
href="https://doi.org/10.1016/j.patcog.2022.108719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving Objects Segmentation (MOS) is a fundamental task in many computer vision applications such as human activity analysis, visual object tracking, content based video search, traffic monitoring, surveillance, and security. MOS becomes challenging due to abrupt illumination variations , dynamic backgrounds, camouflage and scenes with bootstrapping. To address these challenges we propose a MOS algorithm exploiting multiple adversarial regularizations including conventional as well as least squares losses. More specifically, our model is trained on scene background images with the help of cross-entropy loss, least squares adversarial loss and ℓ 1 ℓ1 loss in image space working jointly to learn the dynamic background changes. During testing, our proposed method aims to generate test image background scenes by searching optimal noise samples using joint minimization of ℓ 1 ℓ1 loss in image space, ℓ 1 ℓ1 loss in feature space, and discriminator least squares loss. These loss functions force the generator to synthesize dynamic backgrounds similar to the test sequences which upon subtraction results in moving objects segmentation. Experimental evaluations on five benchmark datasets have shown excellent performance of the proposed algorithm compared to the twenty one existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Maryam Sultana and Arif Mahmood and Soon Ki Jung},
  doi          = {10.1016/j.patcog.2022.108719},
  journal      = {Pattern Recognition},
  pages        = {108719},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised moving object segmentation using background subtraction and optimal adversarial noise sample search},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entropy guided attention network for weakly-supervised
action localization. <em>PR</em>, <em>129</em>, 108718. (<a
href="https://doi.org/10.1016/j.patcog.2022.108718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net.},
  archive      = {J_PR},
  author       = {Yi Cheng and Ying Sun and Hehe Fan and Tao Zhuo and Joo-Hwee Lim and Mohan Kankanhalli},
  doi          = {10.1016/j.patcog.2022.108718},
  journal      = {Pattern Recognition},
  pages        = {108718},
  shortjournal = {Pattern Recognition},
  title        = {Entropy guided attention network for weakly-supervised action localization},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HFA-net: High frequency attention siamese network for
building change detection in VHR remote sensing images. <em>PR</em>,
<em>129</em>, 108717. (<a
href="https://doi.org/10.1016/j.patcog.2022.108717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building change detection (BCD) recently can be handled well under the booming of deep-learning based computer vision techniques. However, segmentation and recognition for objects with sharper boundaries still suffer from the poorly acquired high frequency information, which can result in the deteriorated annotation of building boundaries in BCD. To better obtain the high frequency pattern under the deep learning pipeline, we propose a high frequency attention-guided Siamese network (HFA-Net) in which a novel built-in high frequency attention block (HFAB) is applied. HFA-Net is designed to enhance high frequency information of buildings via HFAB which is composed of two main stages, i.e., the spatial-wise attention (SA) and the high frequency enhancement (HF). The SA firstly guides the model to search and focus on buildings, and HF is employed afterwards to highlight the high frequency information of the input feature maps . With high frequency information of buildings enhanced by HFAB, HFA-Net is able to better detect the edges of changed buildings, so as to improve the performance of BCD. Our proposed method is evaluated on three widely-used public datasets, i.e., WHU-CD, LEVIR-CD, and Google dataset. Remarkable experimental results on these datasets indicate that our proposed method can better detect edges of changed buildings and shows a better performance. The source code will be released at: https://github.com/HaiXing-1998/HFANet .},
  archive      = {J_PR},
  author       = {Hanhong Zheng and Maoguo Gong and Tongfei Liu and Fenlong Jiang and Tao Zhan and Di Lu and Mingyang Zhang},
  doi          = {10.1016/j.patcog.2022.108717},
  journal      = {Pattern Recognition},
  pages        = {108717},
  shortjournal = {Pattern Recognition},
  title        = {HFA-net: High frequency attention siamese network for building change detection in VHR remote sensing images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Example-based color transfer with gaussian mixture modeling.
<em>PR</em>, <em>129</em>, 108716. (<a
href="https://doi.org/10.1016/j.patcog.2022.108716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color transfer, which plays a key role in image editing, has attracted noticeable attention recently. It has remained a challenge to date due to various issues such as time-consuming manual adjustments and prior segmentation issues. In this paper, we propose to model color transfer under a probability framework and cast it as a parameter estimation problem. In particular, we relate the transferred image with the example image under the Gaussian Mixture Model (GMM) and regard the transferred image color as the GMM centroids . We employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for optimization. To better preserve gradient information, we introduce a Laplacian based regularization term to the objective function at the M-step which is solved by deriving a gradient descent algorithm. Given the input of a source image and an example image, our method is able to generate multiple color transfer results with increasing EM iterations. Extensive experiments show that our approach generally outperforms other competitive color transfer methods, both visually and quantitatively.},
  archive      = {J_PR},
  author       = {Chunzhi Gu and Xuequan Lu and Chao Zhang},
  doi          = {10.1016/j.patcog.2022.108716},
  journal      = {Pattern Recognition},
  pages        = {108716},
  shortjournal = {Pattern Recognition},
  title        = {Example-based color transfer with gaussian mixture modeling},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direction of arrival estimation for indoor environments
based on acoustic composition model with a single microphone.
<em>PR</em>, <em>129</em>, 108715. (<a
href="https://doi.org/10.1016/j.patcog.2022.108715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an effective method for multiple talker localisation using only a single microphone in a room. One of the main challenge here is obtaining a model that can be used for estimating the localization parameter. This model must be sensitive to all possible speaker locations and correctly discriminate their positions. The reverberant speech signal in a room environment can be composited by the clean speech and the acoustic transfer function (ATF). The ATF is a useful tool to describe changes of the speech source, and the approaches based on ATF can thus be used to identify talker localizations with a single microphone. This paper presents two methods, referred to as Composite Reverberant Speech (CRS) model and Direct Training Reverberant Speech (DTRS) model, and uses these methods for obtaining the ATF of a room. The approaches based on proposed methods can successfully and accurately process multi-talker localization task with single microphone. Experiments also demonstrate the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Xingchen Guo and Xuexin Xu and Xunquan Chen and Jinhui Chen and Rong Jia and Zhihong Zhang and Tetsuya Takiguchi and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2022.108715},
  journal      = {Pattern Recognition},
  pages        = {108715},
  shortjournal = {Pattern Recognition},
  title        = {Direction of arrival estimation for indoor environments based on acoustic composition model with a single microphone},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-manifold discriminant local spline embedding.
<em>PR</em>, <em>129</em>, 108714. (<a
href="https://doi.org/10.1016/j.patcog.2022.108714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning reveals the intrinsic low-dimensional manifold structure of high-dimensional data and has achieved great success in a wide spectrum of applications. However, traditional manifold learning methods assume that all the data lie on a common manifold, hence fail to capture the complicated geometry structure of the real-world data lying on multiple manifolds. This paper proposes a novel Multi-manifold Discriminant Local Spline Embedding (MDLSE) algorithm for high-dimensional classification, which considers a more realistic scenario where data of the same class lies on the same manifold. On the basis of this assumption, MDLSE seeks to reconstruct multiple manifolds for different classes of data in the embedding and separate them as apart as possible. In order to preserve the geometry structure of all the manifolds, MDLSE employs thin plate splines to align the local patches within each manifold compatibly in the global embedding. Meanwhile, to separate the different manifolds, MDLSE utilizes discriminative information to ensure the neighboring data from different manifolds to be mapped far from each other. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of MDLSE over the other representative manifold learning algorithms. The advantage of MDLSE is often more obvious on smaller size of training data and in lower embedding dimensions.},
  archive      = {J_PR},
  author       = {Ping He and Xiaohua Xu and Xincheng Chang and Jie Ding and Suquan Chen},
  doi          = {10.1016/j.patcog.2022.108714},
  journal      = {Pattern Recognition},
  pages        = {108714},
  shortjournal = {Pattern Recognition},
  title        = {Multi-manifold discriminant local spline embedding},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECDNet: A bilateral lightweight cloud detection network for
remote sensing images. <em>PR</em>, <em>129</em>, 108713. (<a
href="https://doi.org/10.1016/j.patcog.2022.108713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud detection is one of the critical tasks in remote sensing image pre-processing and it has attracted extensive research interest. In recent years, deep neural networks based cloud detection methods have surpassed the traditional methods (threshold-based methods and conventional machine learning-based methods). However, current approaches mainly focus on improving detection accuracy. The computation complexity and large model size are ignored. To tackle this problem, we propose a lightweight deep learning cloud detection model: Efficient Cloud Detection Network (ECDNet). This model is based on the encoder-decoder structure. In the encoder, a two-path architecture is proposed to extract the spatial and semantic information concurrently. One pathway is the detail branch. It is designed to capture low-level detail spatial features with only a few parameters. The other pathway is the semantic branch, which is mainly for capturing context features. In the semantic branch, a proposed dense pyramid module (DPM) is designed for multi-scale contextual information extraction. The number of parameters and calculations in DPM is greatly reduced by features reusing. Besides, a FusionBlock is developed to merge these two kinds of information. Then the extreme lightweight decoder recovers the cloud mask to the same scale as the input image step by step. To improve performance, boost loss is introduced without inference cost increment . We evaluate the proposed method on two public datasets: LandSat8 and MODIS. Extensive experiments demonstrate that the proposed ECDNet achieves comparable accuracy as the state-of-art cloud detection methods, and meantime has a much smaller model size and less computation burden.},
  archive      = {J_PR},
  author       = {Chen Luo and Shanshan Feng and Xutao Li and Yunming Ye and Baoquan Zhang and Zhihao Chen and YingLing Quan},
  doi          = {10.1016/j.patcog.2022.108713},
  journal      = {Pattern Recognition},
  pages        = {108713},
  shortjournal = {Pattern Recognition},
  title        = {ECDNet: A bilateral lightweight cloud detection network for remote sensing images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiameseFuse: A computationally efficient and a not-so-deep
network to fuse visible and infrared images. <em>PR</em>, <em>129</em>,
108712. (<a href="https://doi.org/10.1016/j.patcog.2022.108712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in pattern analysis have motivated many researchers to focus on developing deep learning based solutions in various image processing applications. Fusing multi-modal images has been one such application area where the interest is combining different information coming from different modalities in a more visually meaningful and informative way. For that purpose, it is important to first extract salient features from each modality and then fuse them as efficiently and informatively as possible. Recent literature on fusing multi-modal images reports multiple deep solutions that combine both visible (RGB) and infra-red (IR) images. In this paper, we study the performance of various deep solutions available in the literature while seeking an answer to the question: “Do we really need deeper networks to fuse multi-modal images?” To have an answer for that question, we introduce a novel architecture based on Siamese networks to fuse RGB (visible) images with infrared (IR) images and report the state-of-the-art results. We present an extensive analysis on increasing the layer numbers in the architecture with the above-mentioned question in mind to see if using deeper networks (or adding additional layers) adds significant performance in our proposed solution. We report the state-of-the-art results on visually fusing given visible and IR image pairs in multiple performance metrics, while requiring the least number of trainable parameters. Our experimental results suggest that shallow networks (as in our proposed solutions in this paper) can fuse both visible and IR images as well as the deep networks that were previously proposed in the literature (we were able to reduce the total number of trainable parameters up to 96.5\%, compare 2,625 trainable parameters to the 74,193 trainable parameters).},
  archive      = {J_PR},
  author       = {Sedat Özer and Mert Ege and Mehmet Akif Özkanoglu},
  doi          = {10.1016/j.patcog.2022.108712},
  journal      = {Pattern Recognition},
  pages        = {108712},
  shortjournal = {Pattern Recognition},
  title        = {SiameseFuse: A computationally efficient and a not-so-deep network to fuse visible and infrared images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fovea localization by blood vessel vector in abnormal fundus
images. <em>PR</em>, <em>129</em>, 108711. (<a
href="https://doi.org/10.1016/j.patcog.2022.108711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human eyes, macula is responsible for sharp central vision with fovea in the center. The location of fovea becomes an important landmark in diagnosing the retinal diseases. As macula doesn’t have the clear boundary and obvious shape, deep learning techniques to locate the fovea often fail in complicated lesions and insufficient training samples, and the unsupervised method is incapable for illumination variations . In this paper, a new unsupervised fovea localization method using the retinal raphe and region searching is proposed, and the blood vessel vector (BVV) model is developed. After detecting blood vessels and OD by U-net and probability bubbles, the BVVs are conceived and the retinal raphe is obtained by summating all the BVVs, then the fovea is estimated through the local region searching. Compared with the parabola model, the BVV model does not involve the coordinate transformation and reduces the complexity to the linear time cost O ( N ) O(N) . Two other unsupervised techniques the parabola model and intensity searching and five supervised techniques cGAN, U-net, DRNet, MedTnet and EANet are included and compared. The global feature of retinal vessels is utilized which makes the proposed method more robust to the lesions than the other localization methods. The experiments on public datasets Kaggle, MESSIDOR and IDRiD validate the effectiveness of the proposed method by the student’s t -test, and our method obtains the least average Euclidean distance to the groundtruth on Kaggle and almost least on Base 33 of MESSIDOR.},
  archive      = {J_PR},
  author       = {Yinghua Fu and Ge Zhang and Jiang Li and Dongyan Pan and Yongxiong Wang and Dawei Zhang},
  doi          = {10.1016/j.patcog.2022.108711},
  journal      = {Pattern Recognition},
  pages        = {108711},
  shortjournal = {Pattern Recognition},
  title        = {Fovea localization by blood vessel vector in abnormal fundus images},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MPCCL: Multiview predictive coding with contrastive learning
for person re-identification. <em>PR</em>, <em>129</em>, 108710. (<a
href="https://doi.org/10.1016/j.patcog.2022.108710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a new representation learning approach, termed as Multiview Predictive Coding with Contrastive Learning (MPCCL), for person re-identification (re-ID). Different from the conventional re-ID approaches that focus on learning representations from semantic label , our approach learns the identification of invariant information via representation reconstruction, which explores more fine-grained semantic information in representation space. Specifically, given a chosen identity, the learned representation of its single view can be reconstructed by those of other views. Therefore, kernel density estimation (KDE) is firstly introduced for the adaptive reconstruction of the representation. Then, contrastive learning is adopted to increase the distance between the representations of the same views with different identities. Finally, representation reconstruction and contrastive learning jointly supervise the representation learning process, thus obtaining fine-grained semantic information and appearance-free representations. Extensive experiments on several re-ID datasets demonstrate that the proposed approach yields state-of-the-art results.},
  archive      = {J_PR},
  author       = {Junhui Yin and Jiyang Xie and Zhanyu Ma and Jun Guo},
  doi          = {10.1016/j.patcog.2022.108710},
  journal      = {Pattern Recognition},
  pages        = {108710},
  shortjournal = {Pattern Recognition},
  title        = {MPCCL: Multiview predictive coding with contrastive learning for person re-identification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FocusNet: Classifying better by focusing on confusing
classes. <em>PR</em>, <em>129</em>, 108709. (<a
href="https://doi.org/10.1016/j.patcog.2022.108709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture . Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.},
  archive      = {J_PR},
  author       = {Xue Zhang and Zehua Sheng and Hui-Liang Shen},
  doi          = {10.1016/j.patcog.2022.108709},
  journal      = {Pattern Recognition},
  pages        = {108709},
  shortjournal = {Pattern Recognition},
  title        = {FocusNet: Classifying better by focusing on confusing classes},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sparse graph wavelet convolution neural network for
video-based person re-identification. <em>PR</em>, <em>129</em>, 108708.
(<a href="https://doi.org/10.1016/j.patcog.2022.108708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification (Re-ID) aims to match identical person sequences captured across non-overlapping surveillance areas. It is an essential yet challenging task to effectively embed spatial and temporal information into the video feature representation. For one thing, we observe that different frames in the video can provide complementary information for each other. Also, local features which is lost due to target occlusion or visual ambiguity in one frame can be supplemented by the same pedestrian part in other frames. For another thing, graph neural network enables the contextual interactions between relevant regional features. Therefore, we propose a novel sparse graph wavelet convolution neural network (SGWCNN) for video-based person Re-ID. Distinct from previous graph-based Re-ID methods, we exploit the weighted sparse graph to model the semantic relation among the local patches of pedestrians in the video. Each local patch in one frame can extract supplementary information from highly related patches in other frames. Moreover, to effectively solve the problems of short time occlusion and pedestrian misalignment, the graph wavelet convolution neural network is adopted for feature propagation to refine regional features iteratively. Experiments and evaluation on three challenging benchmarks, that is, MARS, DukeMTMC-VideoReID, and iLIDS-VID, show that the proposed SGWCNN effectively improves the performance of video-based person re-identification.},
  archive      = {J_PR},
  author       = {Yingmao Yao and Xiaoyan Jiang and Hamido Fujita and Zhijun Fang},
  doi          = {10.1016/j.patcog.2022.108708},
  journal      = {Pattern Recognition},
  pages        = {108708},
  shortjournal = {Pattern Recognition},
  title        = {A sparse graph wavelet convolution neural network for video-based person re-identification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Harmonic convolutional networks based on discrete cosine
transform. <em>PR</em>, <em>129</em>, 108707. (<a
href="https://doi.org/10.1016/j.patcog.2022.108707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain . We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification , object detection and semantic segmentation applications.},
  archive      = {J_PR},
  author       = {Matej Ulicny and Vladimir A. Krylov and Rozenn Dahyot},
  doi          = {10.1016/j.patcog.2022.108707},
  journal      = {Pattern Recognition},
  pages        = {108707},
  shortjournal = {Pattern Recognition},
  title        = {Harmonic convolutional networks based on discrete cosine transform},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-aware twin support vector machines. <em>PR</em>,
<em>129</em>, 108706. (<a
href="https://doi.org/10.1016/j.patcog.2022.108706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist uncertain data in the real world due to some factors such as imprecise measurements and noise. Unlike deterministic data, the features of samples in uncertain data are often described by interval numbers or random vectors with probability density functions . In this paper we propose novel twin support vector machines (TSVMs) to handle uncertain data. In the proposed models which are referred to as uncertainty-aware TSVMs, each uncertain sample is modeled as a random vector with Gaussian distributions. To deal with the multi-dimensional integrals in the original models, we derive an interesting and important theorem which helps us transform the original models into the model involving one-dimensional integrals. The simplification of models makes the optimization problem tractable and the simplified models are solved by using the quasi-Newton optimization algorithm . The proposed decision rule allows us to classify uncertain samples with means and covariance matrices . In addition, we extend the proposed models to their kernel versions to capture the nonlinear structure of uncertain data. Experiments on a series of data sets have been performed to demonstrate that the proposed models gain better classification performance than some existing algorithms, especially for representing uncertain cross-plane problems.},
  archive      = {J_PR},
  author       = {Zhizheng Liang and Lei Zhang},
  doi          = {10.1016/j.patcog.2022.108706},
  journal      = {Pattern Recognition},
  pages        = {108706},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty-aware twin support vector machines},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level graph learning network for hyperspectral image
classification. <em>PR</em>, <em>129</em>, 108705. (<a
href="https://doi.org/10.1016/j.patcog.2022.108705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) has emerged as a new technique for hyperspectral image (HSI) classification. However, in current GCN-based methods, the graphs are usually constructed with manual effort and thus is separate from the classification task , which could limit the representation power of GCN. Moreover, the employed graphs often fail to encode the global contextual information in HSI. Hence, we propose a Multi-level Graph Learning Network (MGLN) for HSI classification, where the graph structural information at both local and global levels can be learned in an end-to-end fashion. First, MGLN employs attention mechanism to adaptively characterize the spatial relevance among image regions. Then localized feature representations can be produced and further used to encode the global contextual information. Finally, prediction can be acquired with the help of both local and global contextual information. Experiments on three real-world hyperspectral datasets reveal the superiority of our MGLN when compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Sheng Wan and Shirui Pan and Shengwei Zhong and Jie Yang and Jian Yang and Yibing Zhan and Chen Gong},
  doi          = {10.1016/j.patcog.2022.108705},
  journal      = {Pattern Recognition},
  pages        = {108705},
  shortjournal = {Pattern Recognition},
  title        = {Multi-level graph learning network for hyperspectral image classification},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel forget-update module for few-shot domain
generalization. <em>PR</em>, <em>129</em>, 108704. (<a
href="https://doi.org/10.1016/j.patcog.2022.108704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Few-Shot Learning ( FSL ) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM . The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.},
  archive      = {J_PR},
  author       = {Minglei Yuan and Chunhao Cai and Tong Lu and Yirui Wu and Qian Xu and Shijie Zhou},
  doi          = {10.1016/j.patcog.2022.108704},
  journal      = {Pattern Recognition},
  pages        = {108704},
  shortjournal = {Pattern Recognition},
  title        = {A novel forget-update module for few-shot domain generalization},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised video anomaly detection via normalizing flows
with implicit latent features. <em>PR</em>, <em>129</em>, 108703. (<a
href="https://doi.org/10.1016/j.patcog.2022.108703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos , is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion informationfor example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.},
  archive      = {J_PR},
  author       = {MyeongAh Cho and Taeoh Kim and Woo Jin Kim and Suhwan Cho and Sangyoun Lee},
  doi          = {10.1016/j.patcog.2022.108703},
  journal      = {Pattern Recognition},
  pages        = {108703},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised video anomaly detection via normalizing flows with implicit latent features},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive estimation method with exploration and
exploitation modes for non-stationary environments. <em>PR</em>,
<em>129</em>, 108702. (<a
href="https://doi.org/10.1016/j.patcog.2022.108702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic systems are highly complex and hard to deal with due to their subject- and time-varying nature. The fact that most of the real world systems/events are of dynamic character makes modeling and analysis of such systems inevitable and charmingly useful. One promising estimation method that is capable of unlearning past information to deal with non-stationarity is Stochastic Learning Weak Estimator (SLWE) by Oommen and Rueda (2006). However, due to using a constant learning rate , it faces a trade-off between plasticity and stability. In this paper, we model SLWE as a random walk and provide rigorous theoretical analysis of asymptotic behavior of estimates to obtain a statistical model. Utilizing this model, we detect changes in stationarity to switch between exploratory and exploitative learning modes. Experimental evaluations on both synthetic and real world data show that the proposed method outperforms related algorithms in different types of drifts.},
  archive      = {J_PR},
  author       = {Kutalmış Coşkun and Borahan Tümer},
  doi          = {10.1016/j.patcog.2022.108702},
  journal      = {Pattern Recognition},
  pages        = {108702},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive estimation method with exploration and exploitation modes for non-stationary environments},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cycle-reconstructive subspace learning with class
discriminability for unsupervised domain adaptation. <em>PR</em>,
<em>129</em>, 108700. (<a
href="https://doi.org/10.1016/j.patcog.2022.108700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is used to effectively learn a classifier for data of the unlabeled target domain by utilizing the data of the source domain with sufficient labels but different distributions. In general, a transformation matrix is employed to acquire a common subspace where the distributions of the two domains are aligned, which is easy to lose lots of unique information of the two domains. To better preserve useful information during the transformation process, we propose a novel Cycle-Reconstructive Subspace Learning with Class Discriminability (CRSL) approach that uses two reconstructive matrixes through an iterative strategy to cycle-reconstruct data matrixes and update the common subspace. In this way, we learn the invariant features in the common subspace while better preserving global and local structures of the two original domains. Finally, we implement additional discriminative constraints such as intra-class aggregation and inter-class diffusion on the transformed features to ensure the class discriminability of data of the two domains. Extensive experiment results show that our conventional method outperforms state-of-the-art conventional methods and is comparable with advanced deep methods on four current domain adaptation datasets.},
  archive      = {J_PR},
  author       = {Yayun Xu and Hua Yan},
  doi          = {10.1016/j.patcog.2022.108700},
  journal      = {Pattern Recognition},
  pages        = {108700},
  shortjournal = {Pattern Recognition},
  title        = {Cycle-reconstructive subspace learning with class discriminability for unsupervised domain adaptation},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph regularization multidimensional projection.
<em>PR</em>, <em>129</em>, 108690. (<a
href="https://doi.org/10.1016/j.patcog.2022.108690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel multidimensional projection method of datasets. Our method called Graph Regularization Multidimensional Projection (GRMP) is based on a technique from the graph signal processing theory, the graph regularization. Initially, a similarity graph is built on the high-dimensional space where the dataset lies. A two-dimensional distribution of points is then created in the visual space using a phyllotactic distribution. The similarity graph is copied properly over the phyllotactic distribution and the graph regularization is applied to their coordinates, which are interpreted as graph signals. The graph regularization reorganizes the phyllotactic distribution by bringing together points that represent similar data in the high-dimensional space. We employ synthetic and real datasets to demonstrate the effectiveness of our method. Furthermore, since the solution of the graph regularization can still be approximated using a fast approximation mechanism based on the Chebyshev polynomials , our method is computationally efficient even for large graphs.},
  archive      = {J_PR},
  author       = {Alcebiades Dal Col and Fabiano Petronetto},
  doi          = {10.1016/j.patcog.2022.108690},
  journal      = {Pattern Recognition},
  pages        = {108690},
  shortjournal = {Pattern Recognition},
  title        = {Graph regularization multidimensional projection},
  volume       = {129},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The cobb-douglas learning machine. <em>PR</em>,
<em>128</em>, 108701. (<a
href="https://doi.org/10.1016/j.patcog.2022.108701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel machine learning approach based on robust optimization . Our proposal defines the task of maximizing the two class accuracies of a binary classification problem as a Cobb-Douglas function. This function is well known in production economics and is used to model the relationship between two or more inputs as well as the quantity produced by those inputs. A robust optimization problem is defined to construct the decision function. The goal of the model is to classify each training pattern correctly, up to a given class accuracy, even for the worst possible data distribution. We demonstrate the theoretical advantages of the Cobb-Douglas function in terms of the properties of the resulting second-order cone programming problem. Important extensions are proposed and discussed, including the use of kernel functions and regularization . Experiments performed on several classification datasets confirm these advantages, leading to the best average performance in comparison to various alternative classifiers.},
  archive      = {J_PR},
  author       = {Sebastián Maldonado and Julio López and Miguel Carrasco},
  doi          = {10.1016/j.patcog.2022.108701},
  journal      = {Pattern Recognition},
  pages        = {108701},
  shortjournal = {Pattern Recognition},
  title        = {The cobb-douglas learning machine},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Causal GraphSAGE: A robust graph method for classification
based on causal sampling. <em>PR</em>, <em>128</em>, 108696. (<a
href="https://doi.org/10.1016/j.patcog.2022.108696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GraphSAGE is a widely-used graph neural network for classification, which generates node embeddings in two steps: sampling and aggregation. In this paper, we introduce causal inference into the GraphSAGE sampling stage, and propose Causal GraphSAGE (C-GraphSAGE) to improve the robustness of the classifier. In C-GraphSAGE, we use causal bootstrapping to obtain a weighting between the target node&#39;s neighbors and their label. Then, these weights are used to resample the node&#39;s neighbors to enforce the robustness of the sampling stage. Finally, an aggregation function is trained to integrate the features of the selected neighbors to obtain the embedding of the target node . Experimental results on the Cora, Pubmed, and Citeseer citation datasets show that the classification performance of C-GraphSAGE is equivalent to that of GraphSAGE, GCN, GAT, and RL-GraphSAGE in the case of no perturbation, and outperforms these as the perturbation ratio increases.},
  archive      = {J_PR},
  author       = {Tao Zhang and Hao-Ran Shan and Max A. Little},
  doi          = {10.1016/j.patcog.2022.108696},
  journal      = {Pattern Recognition},
  pages        = {108696},
  shortjournal = {Pattern Recognition},
  title        = {Causal GraphSAGE: A robust graph method for classification based on causal sampling},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpolation-based nonrigid deformation estimation under
manifold regularization constraint. <em>PR</em>, <em>128</em>, 108695.
(<a href="https://doi.org/10.1016/j.patcog.2022.108695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the image/surface deformation problem by estimating interpolation functions pixel by pixel(or voxel by voxel) between control point pairs using labeled control points and unlabeled feature points as input. The labeled control points are usually selected by users and labeled through user operations; the unlabeled feature points are extracted from the source image. We formulate the interpolation function estimation at each pixel as a weighted semi-supervised learning problem. Specially, we employ moving least squares to estimate the nonrigid deformation function according to the weights between each pixel and the labeled control points and exploit manifold regularization to preserve the intrinsic geometric information of the unlabeled feature points contained in the object. Moreover, we define the nonrigid deformation function in a reproducing kernel Hilbert space to derive a closed-form solution. To reduce the computational complexity , we also adopt a sparse approximation to realize a fast implementation. It is worth mentioning that our proposed method is a unified framework with two different basis functions. Both basis-function-based methods are applied to 2D image deformation , 3D surface deformation, and medical image registration. Extensive experiments on the data and the resulting mean opinion score (MOS) on the 2D deformation demonstrate that our methods are superior to state-of-the-art ones.},
  archive      = {J_PR},
  author       = {Huabing Zhou and Zhichao Xu and Yulu Tian and Zhenghong Yu and Yanduo Zhang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2022.108695},
  journal      = {Pattern Recognition},
  pages        = {108695},
  shortjournal = {Pattern Recognition},
  title        = {Interpolation-based nonrigid deformation estimation under manifold regularization constraint},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A categorical data clustering framework on graph
representation. <em>PR</em>, <em>128</em>, 108694. (<a
href="https://doi.org/10.1016/j.patcog.2022.108694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering categorical data is an important task of machine learning , since the type of data widely exists in real world. However, the lack of an inherent order on the domains of categorical features prevents most of classical clustering algorithms from being directly applied for the type of data. Therefore, it is very key issue to learn an appropriate representation of categorical data for the clustering task . In order to address this issue, we develop a categorical data clustering framework based on graph representation . In this framework, a graph-based representation method for categorical data is proposed, which learns the representation of categorical values from their similar graph to provide similar representations for similar categorical values. We compared the proposed framework with other representation methods for categorical data clustering on benchmark data sets. The experiment results illustrate the proposed framework is very effective, compared to other methods.},
  archive      = {J_PR},
  author       = {Liang Bai and Jiye Liang},
  doi          = {10.1016/j.patcog.2022.108694},
  journal      = {Pattern Recognition},
  pages        = {108694},
  shortjournal = {Pattern Recognition},
  title        = {A categorical data clustering framework on graph representation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Expecting individuals’ body reaction to covid-19 based on
statistical naïve bayes technique. <em>PR</em>, <em>128</em>, 108693.
(<a href="https://doi.org/10.1016/j.patcog.2022.108693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covid-19, what a strange, unpredictable mutated virus. It has baffled many scientists, as no firm rule has yet been reached to predict the effect that the virus can inflict on people if they are infected with it. Recently, many researches have been introduced for diagnosing Covid-19; however, none of them pay attention to predict the effect of the virus on the person&#39;s body if the infection occurs but before the infection really takes place. Predicting the extent to which people will be affected if they are infected with the virus allows for some drastic precautions to be taken for those who will suffer from serious complications, while allowing some freedom for those who expect not to be affected badly. This paper introduces Covid-19 Prudential Expectation Strategy (CPES) as a new strategy for predicting the behavior of the person&#39;s body if he has been infected with Covid-19. The CPES composes of three phases called Outlier Rejection Phase (ORP), Feature Selection Phase (FSP), and Classification Phase (CP). For enhancing the classification accuracy in CP, CPES employs two proposed techniques for outlier rejection in ORP and feature selection in FSP, which are called Hybrid Outlier Rejection (HOR) method and Improved Binary Genetic Algorithm (IBGA) method respectively. In ORP, HOR rejects outliers in the training data using a hybrid method that combines standard division and Binary Gray Wolf Optimization (BGWO) method. On the other hand, in FSP, IBGA as a hybrid method selects the most useful features for the prediction process. IBGA includes Fisher Score (F Score ) as a filter method to quickly select the features and BGA as a wrapper method to accurately select the features based on the average accuracy value from several classification models as a fitness function to guarantee the efficiency of the selected subset of features with any classifier. In CP, CPES has the ability to classify people based on their bodies’ reaction to Covid-19 infection, which is built upon a proposed Statistical Naïve Bayes (SNB) classifier after performing the previous two phases. CPES has been compared against recent related strategies in terms of accuracy, error, recall, precision, and run-time using Covid-19 dataset [1] . This dataset contains routine blood tests collected from people before and after their infection with covid-19 through a Web-based form created by us. CPES outperforms the competing methods in experimental results because it provides the best results with values of 0.87, 0.13, 0.84, and 0.79 for accuracy, error, precision, and recall.},
  archive      = {J_PR},
  author       = {Asmaa H. Rabie and Nehal A. Mansour and Ahmed I. Saleh and Ali E. Takieldeen},
  doi          = {10.1016/j.patcog.2022.108693},
  journal      = {Pattern Recognition},
  pages        = {108693},
  shortjournal = {Pattern Recognition},
  title        = {Expecting individuals’ body reaction to covid-19 based on statistical naïve bayes technique},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPARE: Self-supervised part erasing for ultra-fine-grained
visual categorization. <em>PR</em>, <em>128</em>, 108691. (<a
href="https://doi.org/10.1016/j.patcog.2022.108691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents SPARE, a s elf-supervised par t e rasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e. , understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.},
  archive      = {J_PR},
  author       = {Xiaohan Yu and Yang Zhao and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2022.108691},
  journal      = {Pattern Recognition},
  pages        = {108691},
  shortjournal = {Pattern Recognition},
  title        = {SPARE: Self-supervised part erasing for ultra-fine-grained visual categorization},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonconvex clustering via ℓ0 fusion penalized regression.
<em>PR</em>, <em>128</em>, 108689. (<a
href="https://doi.org/10.1016/j.patcog.2022.108689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis has attracted widespread attention in the past several decades. Generally speaking, clustering is considered as an important unsupervised learning method because its goal is to discover unknown subgroups in data without category label information. In this paper, we propose the ℓ 0 ℓ0 fusion penalized clustering model ( ℓ 0 ℓ0 -PClust), which is a novel clustering framework founded on the penalized regression method . Theoretically, we first analyze the existence of the optimal solutions of our model and deduce an upper bound of the tuning parameter. Then we define the Karush-Kuhn-Tucker point and P-stationary point of the ℓ 0 ℓ0 -PClust model, and establish the relationship between them and local optimal solutions . Moreover, based on the P-stationary point of the ℓ 0 ℓ0 -PClust model, we prove that the distances among different cluster centers are greater than a positive threshold. Computationally, we solve the ℓ 0 ℓ0 -PClust model via the famous alternating direction method of multipliers , whose limit point is a P-stationary point and local optimal solution of the model. Finally, we conduct extensive experiments on both synthetic and real data sets . Experimental results show outstanding performance of our method in comparison with several state-of-the-art clustering methods .},
  archive      = {J_PR},
  author       = {Huangyue Chen and Lingchen Kong and Yan Li},
  doi          = {10.1016/j.patcog.2022.108689},
  journal      = {Pattern Recognition},
  pages        = {108689},
  shortjournal = {Pattern Recognition},
  title        = {Nonconvex clustering via ℓ0 fusion penalized regression},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Possibility results for graph clustering: A novel
consistency axiom. <em>PR</em>, <em>128</em>, 108687. (<a
href="https://doi.org/10.1016/j.patcog.2022.108687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm . We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinberg’s Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinberg’s original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinberg’s axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.},
  archive      = {J_PR},
  author       = {Fabio Strazzeri and Rubén J. Sánchez-García},
  doi          = {10.1016/j.patcog.2022.108687},
  journal      = {Pattern Recognition},
  pages        = {108687},
  shortjournal = {Pattern Recognition},
  title        = {Possibility results for graph clustering: A novel consistency axiom},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Total bregman divergence-driven possibilistic fuzzy
clustering with kernel metric and local information for grayscale image
segmentation. <em>PR</em>, <em>128</em>, 108686. (<a
href="https://doi.org/10.1016/j.patcog.2022.108686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel possibilistic fuzzy C-means with local information (KWPFLICM) has important research significance of image segmentation , but it is very sensitive to high noise or outliers. To enhance the segmentation performance of the algorithm, this paper proposes a kernelized total Bregman divergence-driven possibilistic fuzzy clustering with local information (TKWPFLICM). Firstly, a polynomial kernel function is introduced to kernelize total Bregman divergence (TBD), and local neighborhood information of the pixel is used to modify it, which overcomes the shortcomings of Bregman divergence (BD) with rotation variability; Secondly, the modified kernelized TBD and possibilistic typicality are combined to further enhance the anti-noise ability of the algorithm; Finally, the modified kernelized TBD is introduced into the objective function of KWPFLICM algorithm, then a novel robust fuzzy clustering algorithm is derived by optimization theory . Experimental results show that compared with existing fuzzy clustering-related algorithms, the average SA improvement on TKWPFLICM algorithm is in the range of 0.791\% to 33.237\%. Therefore, TKWPFLICM algorithm has better anti-noise robustness and segmentation accuracy .},
  archive      = {J_PR},
  author       = {Chengmao Wu and Xue Zhang},
  doi          = {10.1016/j.patcog.2022.108686},
  journal      = {Pattern Recognition},
  pages        = {108686},
  shortjournal = {Pattern Recognition},
  title        = {Total bregman divergence-driven possibilistic fuzzy clustering with kernel metric and local information for grayscale image segmentation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). LiDAR-based localization using universal encoding and
memory-aware regression. <em>PR</em>, <em>128</em>, 108685. (<a
href="https://doi.org/10.1016/j.patcog.2022.108685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual localization is critical to many robotics and computer vision applications. Absolute pose regression performs localization by encoding scene features followed by pose regression, which has achieved impressive results in localization. It recovers 6-DoF poses from captured scene data alone. However, current methods suffer from being retrained with specific source data whenever the scene changes, resulting in expensive computational costs, data privacy disclosure, and unreliable localization caused by the inability to memorize all data. In this paper, we propose a novel LiDAR-based absolute pose regression network with universal encoding to avoid redundant retraining and the loss of data privacy. Specifically, we propose using universal feature encoding for different scenes. Only the regressor needs to be retrained to achieve higher efficiency, and the training is performed using the encoded features without source data, which preserves data privacy. Then, we propose a memory regressor for memory-aware regression, where the hidden unit numbers in the regressor determine the memorization capacity. It can be used to derive and improve the upper bound of the capacity to enable more reliable localization. Then, it is possible to modify the regressor structure to adapt different memorization capacity requirements for different scene sizes. Extensive experiments on outdoor and indoor datasets validated the above analyses and demonstrated the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Shangshu Yu and Cheng Wang and Chenglu Wen and Ming Cheng and Minghao Liu and Zhihong Zhang and Xin Li},
  doi          = {10.1016/j.patcog.2022.108685},
  journal      = {Pattern Recognition},
  pages        = {108685},
  shortjournal = {Pattern Recognition},
  title        = {LiDAR-based localization using universal encoding and memory-aware regression},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial information enhancement network for 3D object
detection from point cloud. <em>PR</em>, <em>128</em>, 108684. (<a
href="https://doi.org/10.1016/j.patcog.2022.108684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR-based 3D object detection pushes forward an immense influence on autonomous vehicles. Due to the limitation of the intrinsic properties of LiDAR, fewer points are collected at the objects farther away from the sensor. This imbalanced density of point clouds degrades the detection accuracy but is generally neglected by previous works. To address the challenge, we propose a novel two-stage 3D object detection framework, named SIENet. Specifically, we design the Spatial Information Enhancement (SIE) module to predict the spatial shapes of the foreground points within proposals, and extract the structure information to learn the representative features for further box refinement. The predicted spatial shapes are complete and dense point sets, thus the extracted structure information contains more semantic representation . Besides, we design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes multiple branches to learn discriminate features and generate accurate proposals for the SIE module. Extensive experiments on the KITTI 3D object detection benchmark show that our elaborately designed SIENet outperforms the state-of-the-art methods by a large margin. Codes will be publicly available at https://github.com/Liz66666/SIENet .},
  archive      = {J_PR},
  author       = {Ziyu Li and Yuncong Yao and Zhibin Quan and Jin Xie and Wankou Yang},
  doi          = {10.1016/j.patcog.2022.108684},
  journal      = {Pattern Recognition},
  pages        = {108684},
  shortjournal = {Pattern Recognition},
  title        = {Spatial information enhancement network for 3D object detection from point cloud},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MS2GAH: Multi-label semantic supervised graph attention
hashing for robust cross-modal retrieval. <em>PR</em>, <em>128</em>,
108676. (<a href="https://doi.org/10.1016/j.patcog.2022.108676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the strong nonlinear representation capabilities of deep neural networks and the low storage and high efficiency characteristics of hash learning, deep cross-modal hashing has been propelled to the forefront of academics. How to preferably bridge semantic relevance to further bridge the semantic modality gap is the vital bottleneck to improve model performance. Confronting samples with rich semantics, how to comprehensively explore the hidden correlations and establish more precise modality relationships is the primary issue to be solved. In this work, we propose a novel deep hashing method called M ulti-Label S emantic S upervised G raph A ttention H ashing (MS 2 2 GAH), which is an end-to-end framework that integrates graph attention networks (GATs). It constructs graph features through the adjacency of nodes and assigns different weights to adjacent edges to enhance the robustness of the model. Simultaneously, multi-label annotations are utilized to bridge the semantic relevance between modalities in a more fine-grained manner. To make preferable use of rich semantic information, an end-to-end label encoder is designed to mine high-level semantics from multi-label annotations to guide the feature extraction process of specific-modality networks, thereby further narrowing the modality gap. Finally, extensive experiments have been conducted on four datasets, and the results show that MS 2 2 GAH is superior to other baselines and one step forward.},
  archive      = {J_PR},
  author       = {Youxiang Duan and Ning Chen and Peiying Zhang and Neeraj Kumar and Lunjie Chang and Wu Wen},
  doi          = {10.1016/j.patcog.2022.108676},
  journal      = {Pattern Recognition},
  pages        = {108676},
  shortjournal = {Pattern Recognition},
  title        = {MS2GAH: Multi-label semantic supervised graph attention hashing for robust cross-modal retrieval},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nakagami-fuzzy imaging framework for precise lesion
segmentation in MRI. <em>PR</em>, <em>128</em>, 108675. (<a
href="https://doi.org/10.1016/j.patcog.2022.108675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nakagami distribution and related imaging methods are very efficient in diagnostic ultrasonography for visualization and characterization of tissues for years. Abnormalities in tissues are distinguished from surrounding cells by application of the distribution ruled by the Nakagami m-parameter. The potential of discrimination in ultrasonography enables intelligent segmentation of lesions by other diagnostic tools and the imaging technique is very promising in other areas of medicine, like magnetic resonance imaging (MRI) for brain lesion identification, as presented in this paper. Therefore, we propose a novel Nakagami-Fuzzy imaging framework for intelligent and fully automated suspicious region segmentation from axial FLAIR MRI images exhibiting brain tumor characteristics to satisfy ground truth images with different precision levels. The images from MRI data set are processed by applying Nakagami distribution from pre-Rayleigh to post-Rayleigh for adjusting m-parameter. Amorphous and non-homogenous suspicious regions revealed by Nakagami imaging are segmented using customized Fuzzy 2-means to compare with two types of binary ground truths. The framework we propose is an outstanding example of fuzzy-based expert systems providing an average of 92.61\% dice score for the main clinical experiment we conducted using the images and two types of ground truths provided by University of Hospital, Hradec Kralove. We also tested our framework by the BraTS 2012 and BraTS 2020 datasets and achieved an average of 91.88\% and 89.25\% dice scores respectively, which are competitive among the relevant researches.},
  archive      = {J_PR},
  author       = {Orcan Alpar and Rafael Dolezal and Pavel Ryska and Ondrej Krejcar},
  doi          = {10.1016/j.patcog.2022.108675},
  journal      = {Pattern Recognition},
  pages        = {108675},
  shortjournal = {Pattern Recognition},
  title        = {Nakagami-fuzzy imaging framework for precise lesion segmentation in MRI},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HandyPose: Multi-level framework for hand pose estimation.
<em>PR</em>, <em>128</em>, 108674. (<a
href="https://doi.org/10.1016/j.patcog.2022.108674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand pose estimation is a challenging task due to the large number of degrees of freedom and the frequent occlusions of joints . To address these challenges, we propose HandyPose, a single-pass, end-to-end trainable architecture for 2D hand pose estimation using a single RGB image as input. Adopting an encoder-decoder framework with multi-level features, along with a novel multi-level waterfall atrous spatial pooling module for multi-scale representations, our method achieves high accuracy in hand pose while maintaining manageable size complexity and modularity of the network. HandyPose takes a multi-scale approach to representing context by incorporating spatial information at various levels of the network to mitigate the loss of resolution due to pooling. Our advanced multi-level waterfall module leverages the efficiency of progressive cascade filtering while maintaining larger fields-of-view through the concatenation of multi-level features from different levels of the network in the waterfall module. The decoder incorporates both the waterfall and multi-scale features for the generation of accurate joint heatmaps in a single stage. Our results demonstrate state-of-the-art performance on popular datasets and show that HandyPose is a robust and efficient architecture for 2D hand pose estimation.},
  archive      = {J_PR},
  author       = {Divyansh Gupta and Bruno Artacho and Andreas Savakis},
  doi          = {10.1016/j.patcog.2022.108674},
  journal      = {Pattern Recognition},
  pages        = {108674},
  shortjournal = {Pattern Recognition},
  title        = {HandyPose: Multi-level framework for hand pose estimation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFSNet: A multi focus segmentation network for skin lesion
segmentation. <em>PR</em>, <em>128</em>, 108673. (<a
href="https://doi.org/10.1016/j.patcog.2022.108673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: P H 2 PH2 , ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at https://github.com/Rohit-Kundu/MFSNet .},
  archive      = {J_PR},
  author       = {Hritam Basak and Rohit Kundu and Ram Sarkar},
  doi          = {10.1016/j.patcog.2022.108673},
  journal      = {Pattern Recognition},
  pages        = {108673},
  shortjournal = {Pattern Recognition},
  title        = {MFSNet: A multi focus segmentation network for skin lesion segmentation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced task attention with adversarial learning for
dynamic multi-task CNN. <em>PR</em>, <em>128</em>, 108672. (<a
href="https://doi.org/10.1016/j.patcog.2022.108672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task deep learning is promising to solve multi-label multi-instance visual recognition tasks. However, flexible information sharing in the task group might bring performance bottlenecks to an individual task. To tackle this problem, we propose a novel learning framework of multi-task Convolutional Neural Network (CNN) to enhance task attention through conditionally tuning the Task Transfer Connections (TTC) with adversarial learning. For the dynamic multi-task CNN, we set up a shared subnet to extract shared features across multiple tasks and a task discriminator shared by all layers to distinguish features of all subnets. The adversarial training is introduced between the shared subnet and the task discriminator to guide each task subnet to focus on its specific task. To apply adversarial learning to the complex labeling system of multiple tasks, we design an even-label strategy for the multi-task model with a shared subnet to make adversarial learning feasible for the complex labeling system of multiple tasks. As a result, the proposed model can constrain the shared subnet’s learning unbiased to any single task and achieve task attention for all task subnets. Experimental results of the ablation study and the TTC analysis validate the effectiveness of the proposed approach.},
  archive      = {J_PR},
  author       = {Yuchun Fang and Shiwei Xiao and Menglu Zhou and Sirui Cai and Zhaoxiang Zhang},
  doi          = {10.1016/j.patcog.2022.108672},
  journal      = {Pattern Recognition},
  pages        = {108672},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced task attention with adversarial learning for dynamic multi-task CNN},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time series classifier recommendation by a meta-learning
approach. <em>PR</em>, <em>128</em>, 108671. (<a
href="https://doi.org/10.1016/j.patcog.2022.108671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses time series classifier recommendation for the first time in the literature by considering several recommendation forms or meta-targets: classifier accuracies, complete ranking, top-M ranking, best set and best classifier. For this, an ad-hoc set of quick estimators of the accuracies of the candidate classifiers (landmarkers) are designed, which are used as predictors for the recommendation system. The performance of our recommender is compared with the performance of a standard method for non-sequential data and a set of baseline methods , which our method outperforms in 7 of the 9 considered scenarios. Since some meta-targets can be inferred from the predictions of other more fine-grained meta-targets, the last part of the work addresses the hierarchical inference of meta-targets. The experimentation suggests that, in many cases, a single model is sufficient to output many types of meta-targets with competitive results.},
  archive      = {J_PR},
  author       = {A. Abanda and U. Mori and Jose A. Lozano},
  doi          = {10.1016/j.patcog.2022.108671},
  journal      = {Pattern Recognition},
  pages        = {108671},
  shortjournal = {Pattern Recognition},
  title        = {Time series classifier recommendation by a meta-learning approach},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Preserving similarity order for unsupervised clustering.
<em>PR</em>, <em>128</em>, 108670. (<a
href="https://doi.org/10.1016/j.patcog.2022.108670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised clustering categorizes a sample set into several groups, where the samples in the same group share high-level concepts. As the clustering performances are heavily determined by the metric to assess the similarity between sample pairs, we propose to learn a deep similarity score function and use it to capture the correlations between sample pairs for improved clustering. We formulate the learning procedure in a ranking framework and introduce two new supervisory signals to train our model. Specifically, we train the similarity score function to guarantee 1) a sample should have a higher level of similarity with its nearest neighbors than others in order to achieve correct clustering, and 2) the ordering of the similarity between neighboring sample pairs should be preserved in order to achieve robust clustering. To this end, we not only study the relevance between neighboring sample pairs for local structure learning , but also study the relevance between each sample and the boundary samples for global structure learning . Extensive experiments on seven public available datasets validate the effectiveness of our proposed framework, including face image clustering, object image clustering, and real-world image clustering.},
  archive      = {J_PR},
  author       = {Jinghua Wang and Li Wang and Jianmin Jiang},
  doi          = {10.1016/j.patcog.2022.108670},
  journal      = {Pattern Recognition},
  pages        = {108670},
  shortjournal = {Pattern Recognition},
  title        = {Preserving similarity order for unsupervised clustering},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Super u-net: A modularized generalizable architecture.
<em>PR</em>, <em>128</em>, 108669. (<a
href="https://doi.org/10.1016/j.patcog.2022.108669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop and validate a novel convolutional neural network (CNN) termed “Super U-Net” for medical image segmentation . Super U-Net integrates a dynamic receptive field module and a fusion upsampling module into the classical U-Net architecture. The model was developed and tested to segment retinal vessels, gastrointestinal (GI) polyps, skin lesions on several image types (i.e., fundus images, endoscopic images, dermoscopic images). We also trained and tested the traditional U-Net architecture, seven U-Net variants, and two non-U-Net segmentation architectures. K-fold cross-validation was used to evaluate performance. The performance metrics included Dice similarity coefficient (DSC), accuracy, positive predictive value (PPV), and sensitivity. Super U-Net achieved average DSCs of 0.808±0.0210, 0.752±0.019, 0.804±0.239, and 0.877±0.135 for segmenting retinal vessels, pediatric retinal vessels, GI polyps, and skin lesions, respectively. The Super U-net consistently outperformed U-Net, seven U-Net variants, and two non-U-Net segmentation architectures ( p &lt; 0.05). Dynamic receptive fields and fusion upsampling can significantly improve image segmentation performance.},
  archive      = {J_PR},
  author       = {Cameron Beeche and Jatin P Singh and Joseph K Leader and Naciye S Gezer and Amechi P Oruwari and Kunal K Dansingani and Jay Chhablani and Jiantao Pu},
  doi          = {10.1016/j.patcog.2022.108669},
  journal      = {Pattern Recognition},
  pages        = {108669},
  shortjournal = {Pattern Recognition},
  title        = {Super U-net: A modularized generalizable architecture},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Erlang planning network: An iterative model-based
reinforcement learning with multi-perspective. <em>PR</em>,
<em>128</em>, 108668. (<a
href="https://doi.org/10.1016/j.patcog.2022.108668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For model-based reinforcement learning (MBRL), one of the key challenges is modeling error, which cripples the effectiveness of model planning and causes poor robustness during training. In this paper, we propose a bi-level Erlang Planning Network (EPN) architecture, which is composed of an upper-level agent and several multi-scale parallel sub-agents, trained in an iterative way. The proposed method focuses upon the expansion of representation by environment: a multi-perspective over the world model, which presents a varied way to represent an agent’s knowledge about the world that alleviates the problem of falling into local optimal points and enhances robustness during the progress of model planning. Moreover, our experiments evaluate EPN on a range of continuous-control tasks in MuJoCo, the evaluation results show that the proposed framework finds exemplar solutions faster and consistently reaches the state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Jiao Wang and Lemin Zhang and Zhiqiang He and Can Zhu and Zihui Zhao},
  doi          = {10.1016/j.patcog.2022.108668},
  journal      = {Pattern Recognition},
  pages        = {108668},
  shortjournal = {Pattern Recognition},
  title        = {Erlang planning network: An iterative model-based reinforcement learning with multi-perspective},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reflection symmetry detection of shapes based on shape
signatures. <em>PR</em>, <em>128</em>, 108667. (<a
href="https://doi.org/10.1016/j.patcog.2022.108667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two novel shape signature-based reflection symmetry detection methods with their theoretical underpinning and empirical evaluation. LIP-signature and R-signature share similar beneficial properties allowing to detect reflection symmetry directions in a high-performing manner. For the shape signature of a given shape, its merit profile is constructed to detect candidates of symmetry direction. A verification process is utilized to eliminate the false candidates by addressing Radon projections. The proposed methods can effectively deal with compound shapes which are challenging for traditional contour-based methods. To quantify the symmetric efficiency, a new symmetry measure is proposed over the range [0, 1]. Furthermore, we introduce two symmetry shape datasets with a new evaluation protocol and a lost measure for evaluating symmetry detectors. Experimental results using standard and new datasets suggest that the proposed methods prominently perform compared to state of the art.},
  archive      = {J_PR},
  author       = {Thanh Phuong Nguyen and Hung Phuoc Truong and Thanh Tuan Nguyen and Yong-Guk Kim},
  doi          = {10.1016/j.patcog.2022.108667},
  journal      = {Pattern Recognition},
  pages        = {108667},
  shortjournal = {Pattern Recognition},
  title        = {Reflection symmetry detection of shapes based on shape signatures},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Encoder deep interleaved network with multi-scale
aggregation for RGB-d salient object detection. <em>PR</em>,
<em>128</em>, 108666. (<a
href="https://doi.org/10.1016/j.patcog.2022.108666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, RGB-D salient object detection (SOD) has aroused widespread research interest. Existing RGB-D SOD approaches mainly consider the cross-modal information fusion in the decoder. And their multi-modal interaction mainly concentrates on the same level of features between RGB stream and depth stream. They do not deeply explore the coherence of multi-model features at different levels. In this paper, we design a two-stream deep interleaved encoder network to extract RGB and depth information and realize their mixing simultaneously. This network allows us to gradually learn multi-modal representation at different levels from shallow to deep. Moreover, to further fuse multi-modal features in the decoding stage, we propose a cross-modal mutual guidance module and a residual multi-scale aggregation module to implement the global guidance and local refinement of the salient region . Extensive experiments on six benchmark datasets demonstrate that the proposed approach performs favorably against most state-of-the-art methods under different evaluation metrics . During the testing stage, this model can run at a real-time speed of 93 FPS.},
  archive      = {J_PR},
  author       = {Guang Feng and Jinyu Meng and Lihe Zhang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2022.108666},
  journal      = {Pattern Recognition},
  pages        = {108666},
  shortjournal = {Pattern Recognition},
  title        = {Encoder deep interleaved network with multi-scale aggregation for RGB-D salient object detection},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quaternion-based weighted nuclear norm minimization for
color image restoration. <em>PR</em>, <em>128</em>, 108665. (<a
href="https://doi.org/10.1016/j.patcog.2022.108665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color image restoration is one of the basic tasks in pattern recognition. Unlike grayscale image , each color image has three channels in the RGB color space. Due to the inner-relationship within the three channels, color image restoration is usually much more difficult than its grayscale counterpart. Indeed, new problems such as color artifacts could emerge when the grayscale image processing methods are extended to color images directly. Note that one of the most effective gray image restoration methods is the weighted nuclear norm minimization (WNNM) approach. However, when applied to color images, the results of WNNM are usually not as promising as that of grayscale images. In order to solve this problem, in this paper, we propose to restore color images with the quaternion-based WNNM method (QWNNM) since the structure of color channels can be well preserved with quaternion representation. The proposed model can be solved efficiently by the alternating direction method of multipliers (ADMM). The theoretical analysis of the optimal solution is also presented. Numerical experiments are carefully conducted with different kinds of degradation to illustrate the superior performance of our proposed QWNNM over the state-of-the-art methods, including a celebrated deep learning approach, in both visual quality and numerical results.},
  archive      = {J_PR},
  author       = {Chaoyan Huang and Zhi Li and Yubing Liu and Tingting Wu and Tieyong Zeng},
  doi          = {10.1016/j.patcog.2022.108665},
  journal      = {Pattern Recognition},
  pages        = {108665},
  shortjournal = {Pattern Recognition},
  title        = {Quaternion-based weighted nuclear norm minimization for color image restoration},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradient-based refined class activation map for weakly
supervised object localization. <em>PR</em>, <em>128</em>, 108664. (<a
href="https://doi.org/10.1016/j.patcog.2022.108664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization locates objects based on the localization map generated from the classification network. However, most existing methods utilize the information of the target class to locate objects based on the feature map of a single image, which ignores both the relationships of inter-class and intra-class. In this work, we propose a Gradient-based Refined Class Activation Map (GRCAM) approach to achieve more accurate localization. Two kinds of gradients are applied to reveal the relationships of inter-class and intra-class during the testing stage. First, we exploit the gradients of the classification loss function concerning the feature map to enhance class-specific information. The gradients of classification loss reveal the connection among the predicted probabilities of all classes. Second, we design a regression function that refers to the loss between the pseudo-bounding box coordinates containing category consistency and the predicted coordinates generated from the localization map. The predicted coordinates are revised by the gradients of the regression function. The gradients of the regression function reveal the consistency within a class. Despite the apparent simplicity, we demonstrate the advantages of GRCAM on ILSVRC and CUB-200-2011 in extensive experiments. Especially, on ILSVRC dataset, the proposed GRCAM achieves a new state-of-the-art Top-1 localization error of 42.94\%.},
  archive      = {J_PR},
  author       = {Wenjun Hui and Chuangchuang Tan and Guanghua Gu and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.108664},
  journal      = {Pattern Recognition},
  pages        = {108664},
  shortjournal = {Pattern Recognition},
  title        = {Gradient-based refined class activation map for weakly supervised object localization},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end weakly supervised semantic segmentation with
reliable region mining. <em>PR</em>, <em>128</em>, 108663. (<a
href="https://doi.org/10.1016/j.patcog.2022.108663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation is a challenging task that only takes image-level labels as supervision but produces pixel-level predictions for testing. To address such a challenging task, most current approaches generate pseudo pixel masks first that are then fed into a separate semantic segmentation network. However, these two-step approaches suffer from high complexity and being hard to train as a whole. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps . Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into tiny reliable object/background regions. Such reliable regions are then directly served as ground-truth labels for the segmentation branch, where both global information and local information sub-branches are used to generate accurate pixel-level predictions. Furthermore, a new joint loss is proposed that considers both shallow and high-level features. Despite its apparent simplicity, our end-to-end solution achieves competitive mIoU scores ( val : 65.4\%, test : 65.3\%) on Pascal VOC compared with the two-step counterparts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC 2012 dataset( val : 69.3\%, test : 69.2\%). Code is available at: https://github.com/zbf1991/RRM .},
  archive      = {J_PR},
  author       = {Bingfeng Zhang and Jimin Xiao and Yunchao Wei and Kaizhu Huang and Shan Luo and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.108663},
  journal      = {Pattern Recognition},
  pages        = {108663},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end weakly supervised semantic segmentation with reliable region mining},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multi-level weight-centric features for few-shot
learning. <em>PR</em>, <em>128</em>, 108662. (<a
href="https://doi.org/10.1016/j.patcog.2022.108662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is currently enjoying a considerable resurgence of interest, aided by the recent advance of deep learning . Contemporary approaches based on weight-generation scheme delivers a straightforward and flexible solution to the problem. However, they did not fully consider both the representation power for unseen categories and weight generation capacity in feature learning , making it a significant performance bottleneck . This paper proposes a multi-level weight-centric feature learning to give full play to feature extractor’s dual roles in few-shot learning. Our proposed method consists of two essential techniques: a weight-centric training strategy to improve the features’ prototype-ability and a multi-level feature incorporating a mid- and relation-level information. The former increases the feasibility of constructing a discriminative decision boundary based on a few samples. Simultaneously, the latter helps improve the transferability for characterizing novel classes and preserve classification capability for base classes. We extensively evaluate our approach to low-shot classification benchmarks. Experiments demonstrate our proposed method significantly outperforms its counterparts in both standard and generalized settings and using different network backbones .},
  archive      = {J_PR},
  author       = {Mingjiang Liang and Shaoli Huang and Shirui Pan and Mingming Gong and Wei Liu},
  doi          = {10.1016/j.patcog.2022.108662},
  journal      = {Pattern Recognition},
  pages        = {108662},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-level weight-centric features for few-shot learning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Node-feature convolution for graph convolutional networks.
<em>PR</em>, <em>128</em>, 108661. (<a
href="https://doi.org/10.1016/j.patcog.2022.108661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) is an effective neural network model for graph representation learning . However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification .},
  archive      = {J_PR},
  author       = {Li Zhang and Heda Song and Nikolaos Aletras and Haiping Lu},
  doi          = {10.1016/j.patcog.2022.108661},
  journal      = {Pattern Recognition},
  pages        = {108661},
  shortjournal = {Pattern Recognition},
  title        = {Node-feature convolution for graph convolutional networks},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synthetic document generator for annotation-free layout
recognition. <em>PR</em>, <em>128</em>, 108660. (<a
href="https://doi.org/10.1016/j.patcog.2022.108660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions , extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents.},
  archive      = {J_PR},
  author       = {Natraj Raman and Sameena Shah and Manuela Veloso},
  doi          = {10.1016/j.patcog.2022.108660},
  journal      = {Pattern Recognition},
  pages        = {108660},
  shortjournal = {Pattern Recognition},
  title        = {Synthetic document generator for annotation-free layout recognition},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Molecular substructure graph attention network for molecular
property identification in drug discovery. <em>PR</em>, <em>128</em>,
108659. (<a href="https://doi.org/10.1016/j.patcog.2022.108659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular machine learning based on graph neural network has a broad prospect in molecular property identification in drug discovery. Molecules contain many types of substructures that may affect their properties. However, conventional methods based on graph neural networks only consider the interaction information between nodes, which may lead to the oversmoothing problem in the multi-hop operations. These methods may not efficiently express the interacting information between molecular substructures. Hence, We develop a Molecular SubStructure Graph ATtention (MSSGAT) network to capture the interacting substructural information, which constructs a composite molecular representation with multi-substructural feature extraction and processes such features effectively with a nested convolution plus readout scheme. We evaluate the performance of our model on 13 benchmark data sets, in which 9 data sets are from the ChEMBL data base and 4 are the SIDER, BBBP, BACE, and HIV data sets. Extensive experimental results show that MSSGAT achieves the best results on most of the data sets compared with other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xian-bin Ye and Quanlong Guan and Weiqi Luo and Liangda Fang and Zhao-Rong Lai and Jun Wang},
  doi          = {10.1016/j.patcog.2022.108659},
  journal      = {Pattern Recognition},
  pages        = {108659},
  shortjournal = {Pattern Recognition},
  title        = {Molecular substructure graph attention network for molecular property identification in drug discovery},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spatially constrained skew student’s-t mixture model for
brain MR image segmentation and bias field correction. <em>PR</em>,
<em>128</em>, 108658. (<a
href="https://doi.org/10.1016/j.patcog.2022.108658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of brain magnetic resonance images is a key step in quantitative analysis of brain images. Finite mixture model is one of the most widely used methods in brain magnetic resonance image segmentation . However, due to the presence of intensity inhomogeneity artifact and noise, the image histogram distribution of brain MR images may follow a heavy tailed distribution or asymmetric distribution, which makes traditional finite mixture model, such as Gaussian mixture model , hard to achieve accurate segmentation results. To alleviate these problems, a novel spatially constrained finite skew student’s-t mixture model is proposed in this paper. Firstly, we propose anisotropic two-level spatial information, which combines the prior and posterior probabilities , to reduce the impact of noise. The proposed spatial information can preserve rich details, such as edges and corners. Secondly, we couple the anisotropic spatial information into the skew student’s-t distribution to fit the intensity distribution of observation data with heavy tail distribution or asymmetric distribution. Thirdly, we use a linear combination of a set of orthogonal basis functions to model the intensity inhomogeneities. Finally, the objective function integrates both tissue segmentation and the bias field estimation. In the implementation, we used an improved expectation maximization (EM) algorithm to estimate the model parameters. The experimental results of our model on synthetic data and brain magnetic resonance images are better than other state-of-the-art segmentation methods .},
  archive      = {J_PR},
  author       = {Ning Cheng and Chunzheng Cao and Jianwei Yang and Zhichao Zhang and Yunjie Chen},
  doi          = {10.1016/j.patcog.2022.108658},
  journal      = {Pattern Recognition},
  pages        = {108658},
  shortjournal = {Pattern Recognition},
  title        = {A spatially constrained skew student’s-t mixture model for brain MR image segmentation and bias field correction},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive open domain recognition by coarse-to-fine
prototype-based network. <em>PR</em>, <em>128</em>, 108657. (<a
href="https://doi.org/10.1016/j.patcog.2022.108657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open domain recognition has attracted great attention in recent two years, which aims to assign a specific identification for each target sample in the presence of large domain discrepancy both in label space and data distributions. Most existing approaches rely on abundant prior information about the relationship of the label sets between the source and the target domain, which is a great limitation for their applications in practical wild. In this paper, a new Adaptive Open Domain Recognition (AODR) task is introduced, which can generalize to various openness and requires no prior information on the label set. To achieve this adaptive transfer task, a two-stage Progressive Adaptation Network is designed, whose learning process consists of multiple episodes. Each episode is performed to simulate an AODR task. Through training and refining multiple episodes, the basic model has progressively accumulated wealthy experience on predicting unseen categories in the presence of large domain discrepancy, which will well generalize to various openness. More specifically, Fusion Information Guided Feature Prototype Generation module is proposed to synthesize visual feature prototype conditioned on category semantic prototype in training stage. Further, Class-Aware Feature Prototype Alignment module is designed in refining stage to align the global feature prototype for each class between two domains. Experimental results verify that the proposed model not only has superiority on classifying the image instances of known and unknown classes, but also well adapts to various openness.},
  archive      = {J_PR},
  author       = {Yuan Yuan and Xinxing He and Zhiyu Jiang},
  doi          = {10.1016/j.patcog.2022.108657},
  journal      = {Pattern Recognition},
  pages        = {108657},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive open domain recognition by coarse-to-fine prototype-based network},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modality person re-identification via multi-task
learning. <em>PR</em>, <em>128</em>, 108653. (<a
href="https://doi.org/10.1016/j.patcog.2022.108653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its promising preliminary results, existing cross-modality Visible-Infrared Person Re-IDentification (VI-PReID) models incorporating semantic (person) masks simply use these person masks as selection maps to separate person features from background regions. Such models do not dedicate to extracting more modality-invariant person body features in the VI-PReID network itself, thus leading to suboptimal results in VI-PReID. Differently, we aim to better capture person body information in the VI-PReID network itself for VI-PReID by exploiting the inner relations between person mask prediction and VI-PReID. To this end, a novel multi-task learning model is presented in this paper, where person body features obtained by person mask prediction potentially facilitate the extraction of discriminative modality-shared person body information for VI-PReID. On top of that, considering the task difference between person mask prediction and VI-PReID, we propose a novel task translation sub-network to transfer discriminative person body information, extracted by person mask prediction, into VI-PReID. Doing so enables our model to better exploit discriminative and modality-invariant person body information. Thanks to more discriminative modality-shared features, our method outperforms previous state-of-the-arts by a significant margin on several benchmark datasets. Our intriguing findings validate the effectiveness of extracting discriminative person body features for the VI-PReID task.},
  archive      = {J_PR},
  author       = {Nianchang Huang and Kunlong Liu and Yang Liu and Qiang Zhang and Jungong Han},
  doi          = {10.1016/j.patcog.2022.108653},
  journal      = {Pattern Recognition},
  pages        = {108653},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modality person re-identification via multi-task learning},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Searching part-specific neural fabrics for human pose
estimation. <em>PR</em>, <em>128</em>, 108652. (<a
href="https://doi.org/10.1016/j.patcog.2022.108652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has emerged in many domains to jointly learn the architectures and weights of neural networks . The core spirit behind NAS is to automatically search neural architectures for target tasks with better performance-efficiency trade-offs. However, existing approaches emphasize on only searching a single architecture with less human intervention to replace a human-designed neural network, yet making the search process almost independent of the domain knowledge. In this paper, we aim to apply NAS for human pose estimation and we ask: when NAS meets this localization task, can the articulated human body structure help to search better task-specific architectures? To this end, we first design a new neural architecture search space, Cell-based Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, by viewing locating human parts as multiple disentangled prediction sub-tasks, we exploit the compositionality of human body structure as guidance to search multiple part-specific CNFs specialized for different human parts. After the search, all these part-specific neural fabrics have been tailored with distinct micro and macro architecture parameters. The results show that such knowledge-guided NAS-based model outperforms a hand-crafted part-based baseline model , and the resulting multiple part-specific architectures gain significant performance improvement against a single NAS-based architecture for the whole body. The experiments on MPII and COCO datasets show that our models 1 achieve comparable performance against the state-of-the-art methods while being relatively lightweight.},
  archive      = {J_PR},
  author       = {Sen Yang and Wankou Yang and Zhen Cui},
  doi          = {10.1016/j.patcog.2022.108652},
  journal      = {Pattern Recognition},
  pages        = {108652},
  shortjournal = {Pattern Recognition},
  title        = {Searching part-specific neural fabrics for human pose estimation},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-volume preserving-based fusion to group-level emotion
recognition on crowd videos. <em>PR</em>, <em>128</em>, 108646. (<a
href="https://doi.org/10.1016/j.patcog.2022.108646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame.},
  archive      = {J_PR},
  author       = {Kha Gia Quach and Ngan Le and Chi Nhan Duong and Ibsa Jalata and Kaushik Roy and Khoa Luu},
  doi          = {10.1016/j.patcog.2022.108646},
  journal      = {Pattern Recognition},
  pages        = {108646},
  shortjournal = {Pattern Recognition},
  title        = {Non-volume preserving-based fusion to group-level emotion recognition on crowd videos},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human interaction recognition framework based on interacting
body part attention. <em>PR</em>, <em>128</em>, 108645. (<a
href="https://doi.org/10.1016/j.patcog.2022.108645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition in videos has been widely studied and has recently gained significant advances with deep learning approaches; however, it remains a challenging task. In this paper, we propose a novel framework that simultaneously considers both implicit and explicit representations of human interactions by fusing information of local image where the interaction actively occurred, primitive motion with the posture of individual subject’s body parts, and the co-occurrence of overall appearance change. Human interactions change, depending on how the body parts of each human interact with the other. The proposed method captures the subtle difference between different interactions using interacting body part attention. Semantically important body parts that interact with other objects are given more weight during feature representation. The combined feature of interacting body part attention-based individual representation and the co-occurrence descriptor of the full-body appearance change is fed into long short-term memory to model the temporal dynamics over time in a single framework. The experimental results on five widely used public datasets demonstrate the effectiveness of the proposed method to recognize human interactions from videos.},
  archive      = {J_PR},
  author       = {Dong-Gyu Lee and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2022.108645},
  journal      = {Pattern Recognition},
  pages        = {108645},
  shortjournal = {Pattern Recognition},
  title        = {Human interaction recognition framework based on interacting body part attention},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized knowledge-aware recommendation with
collaborative and attentive graph convolutional networks. <em>PR</em>,
<em>128</em>, 108628. (<a
href="https://doi.org/10.1016/j.patcog.2022.108628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are increasingly used to solve the data sparsity and cold start problems of collaborative filtering. Recently, graph neural networks (GNNs) have been applied to build KG-based recommender systems and achieved competitive performance. However, existing GNN-based methods are either limited in their ability to capture fine-grained semantics in a KG, or insufficient in effectively modeling user-item interactions. To address these issues, we propose a novel framework with collaborative and attentive graph convolutional networks for personalized knowledge-aware recommendation. Particularly, we model the user-item graph and the KG separately and simultaneously with an efficient graph convolutional network and a personalized knowledge graph attention network , where the former aims to extract informative collaborative signals, while the latter is designed to capture fine-grained semantics. Collectively, they are able to learn meaningful node representations for predicting user-item interactions. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method compared with state-of-the-arts.},
  archive      = {J_PR},
  author       = {Quanyu Dai and Xiao-Ming Wu and Lu Fan and Qimai Li and Han Liu and Xiaotong Zhang and Dan Wang and Guli Lin and Keping Yang},
  doi          = {10.1016/j.patcog.2022.108628},
  journal      = {Pattern Recognition},
  pages        = {108628},
  shortjournal = {Pattern Recognition},
  title        = {Personalized knowledge-aware recommendation with collaborative and attentive graph convolutional networks},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A feature consistency driven attention erasing network for
fine-grained image retrieval. <em>PR</em>, <em>128</em>, 108618. (<a
href="https://doi.org/10.1016/j.patcog.2022.108618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale fine-grained image retrieval based hashing learning method has two main problems. First, low dimension feature embedding can fasten the retrieval process but bring accuracy decrease due to much information loss. Second, fine-grained images lead to the same category query hash codes mapping into the different cluster in database hash latent space. To handle these issues, we propose a feature consistency driven attention erasing network (FCAENet) for fine-grained image retrieval. For the first issue, we propose an adaptive augmentation module in FCAENet, which is the selective region erasing module (SREM). SREM makes the network more robust on subtle differences of fine-grained task by adaptively covering some regions of raw images. The feature extractor and hash layer can learn more representative hash codes for fine-grained images by SREM. With regard to the second issue, we fully exploit the pair-wise similarity information and add the enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation stabler between the query hash code and database hash code. We conduct extensive experiments on five fine-grained benchmark datasets (CUB2011, Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash codes. The results show that FCAENet achieves the state-of-the-art (SOTA) fine-grained image retrieval performance based on the hashing learning method.},
  archive      = {J_PR},
  author       = {Qi Zhao and Xu Wang and Shuchang Lyu and Binghao Liu and Yifan Yang},
  doi          = {10.1016/j.patcog.2022.108618},
  journal      = {Pattern Recognition},
  pages        = {108618},
  shortjournal = {Pattern Recognition},
  title        = {A feature consistency driven attention erasing network for fine-grained image retrieval},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An attention-based framework for multi-view clustering on
grassmann manifold. <em>PR</em>, <em>128</em>, 108610. (<a
href="https://doi.org/10.1016/j.patcog.2022.108610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key problem of multi-view clustering is to handle the inconsistency among multiple views. This article proposes an attention-based framework for multi-view clustering on Grassmann manifold (AMCGM). To be specific, the proposed AMCGM framework aims to learn a representative element on Grassmann manifold with the following four highlights: 1) AMCGM framework performs an attention-based weighted-learning scheme to capture the difference of views; 2) The clustering results can be directly generated by the structured graph learned via AMCGM, avoiding the randomness caused by traditional label-generation procedures, such as K K -means clustering; 3) AMCGM has high extensibility since it can generate many multi-view clustering models on Grassmann manifold; 4) On Grassmann manifold, the relationship between the projection metric (PM)-based multi-view clustering model and squared projection metric (SPM)-based model is studied. Based on AMCGM framework, we propose some generated models and provide some useful conclusions. Moreover, to solve the optimization problems involved in the proposed AMCGM framework and generated models, we propose an efficiently iterative algorithm and provide rigorous convergence analysis . Extensive experimental results demonstrate the superb performance of our framework.},
  archive      = {J_PR},
  author       = {Danyang Wu and Xia Dong and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.108610},
  journal      = {Pattern Recognition},
  pages        = {108610},
  shortjournal = {Pattern Recognition},
  title        = {An attention-based framework for multi-view clustering on grassmann manifold},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complex shearlets and rotary phase congruence tensor for
corner detection. <em>PR</em>, <em>128</em>, 108606. (<a
href="https://doi.org/10.1016/j.patcog.2022.108606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corner detection algorithms based on multi-scale analysis attract more attention due to their promising performance. However, they only consider amplitude information, neglect phase information and partially utilize multi-scale decomposition coefficients to detect corners. This limits their detection accuracy, repeatability and localization ability. This paper describes a new multi-scale analysis based corner detector. To overcome the problems of bilateral margin responses, edge extension and lack of phase information in traditional shearlets, a novel complex shearlet transform is proposed to better localize distributed discontinuities and especially to extract phase information from geometrical features. Moreover, a new rotary phase congruence tensor is proposed to utilize all amplitude and phase information for corner detection. Its tolerances to noise and ability for corner localization are improved further by screening and normalizing the amplitude information. Experimental results demonstrate that the localization ability and detection accuracy of the proposed method are superior to current detectors, and its repeatability is generally higher than current detectors and recent machine learning based interest point detectors.},
  archive      = {J_PR},
  author       = {Mingzhe Wang and Changming Sun and Arcot Sowmya},
  doi          = {10.1016/j.patcog.2022.108606},
  journal      = {Pattern Recognition},
  pages        = {108606},
  shortjournal = {Pattern Recognition},
  title        = {Complex shearlets and rotary phase congruence tensor for corner detection},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High quality proposal feature generation for crowded
pedestrian detection. <em>PR</em>, <em>128</em>, 108605. (<a
href="https://doi.org/10.1016/j.patcog.2022.108605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is a severe problem for pedestrian detection in crowded scenes. Due to the diversity of pedestrian postures and occlusion forms, leading to false detection and missed detection. In this paper, we propose a high quality proposal feature generation pedestrian detection algorithm to improve detection performance. Firstly, Dual-Region Feature Generation (DRFG) is proposed to generate high quality proposal features. Specifically, visible regions with less occlusion are introduced and low-precision proposals are generated for both the full-body and visible regions respectively. Then, proposals are respectively selected from the two kinds of proposals mentioned above to match in pairs, so as to guarantee a strong correspondence in information between the two proposals. Afterwards, the successfully matched proposal features are fused by Selective Kernel Feature Fusion (SKFF) to generate high quality proposal features. Secondly, Paired Multiple Instance Prediction(PMIP) is performed on the fused features to generate multiple prediction branches, and each prediction branch generates full-body and visible prediction box. Finally, Paired Non-Maximum Suppression(PNMS) is applied to the prediction boxes to reduce the false positives . Experiments have been conducted on CrowdHuman [1] and CityPersons [2] datasets. Comparing with baseline, our methods have achieved 5.9\% A P AP and 1.5\% M R − 2 MR−2 improvement on the above two datasets, sufficiently verifying the effectiveness of our methods in crowded pedestrian detection.},
  archive      = {J_PR},
  author       = {Jing Wang and Cailing Zhao and Zhanqiang Huo and Yingxu Qiao and Haifeng Sima},
  doi          = {10.1016/j.patcog.2022.108605},
  journal      = {Pattern Recognition},
  pages        = {108605},
  shortjournal = {Pattern Recognition},
  title        = {High quality proposal feature generation for crowded pedestrian detection},
  volume       = {128},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AUCO ResNet: An end-to-end network for covid-19
pre-screening from cough and breath. <em>PR</em>, <em>127</em>, 108656.
(<a href="https://doi.org/10.1016/j.patcog.2022.108656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents the Auditory Cortex ResNet (AUCO ResNet), it is a biologically inspired deep neural network especially designed for sound classification and more specifically for Covid-19 recognition from audio tracks of coughs and breaths. Differently from other approaches, it can be trained end-to-end thus optimizing (with gradient descent) all the modules of the learning algorithm: mel-like filter design, feature extraction, feature selection, dimensionality reduction and prediction. This neural network includes three attention mechanisms namely the squeeze and excitation mechanism, the convolutional block attention module, and the novel sinusoidal learnable attention. The attention mechanism is able to merge relevant information from activation maps at various levels of the network. The net takes as input raw audio files and it is able to fine tune also the features extraction phase. In fact, a Mel-like filter is designed during the training, thus adapting filter banks on important frequencies. AUCO ResNet has proved to provide state of art results on many datasets. Firstly, it has been tested on many datasets containing Covid-19 cough and breath. This choice is related to the fact that that cough and breath are language independent, allowing for cross dataset tests with generalization aims. These tests demonstrate that the approach can be adopted as a low cost, fast and remote Covid-19 pre-screening tool. The net has also been tested on the famous UrbanSound 8K dataset, achieving state of the art accuracy without any data preprocessing or data augmentation technique.},
  archive      = {J_PR},
  author       = {Vincenzo Dentamaro and Paolo Giglio and Donato Impedovo and Luigi Moretti and Giuseppe Pirlo},
  doi          = {10.1016/j.patcog.2022.108656},
  journal      = {Pattern Recognition},
  pages        = {108656},
  shortjournal = {Pattern Recognition},
  title        = {AUCO ResNet: An end-to-end network for covid-19 pre-screening from cough and breath},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse matrix factorization with l2,1 norm for matrix
completion. <em>PR</em>, <em>127</em>, 108655. (<a
href="https://doi.org/10.1016/j.patcog.2022.108655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization is a popular matrix completion method, however, it is difficult to determine the ranks of the factor matrices . We propose two new sparse matrix factorization methods with l 2 , 1 l2,1 norm to explicitly force the row sparseness of the factor matrices , where the rank of the factor matrices is adaptively controlled by the regularization coefficient. We further theoretically prove the convergence property of our algorithms. The experimental results on the simulation and the benchmark datasets show that our methods achieve superior performance than its counterparts. Moreover our proposed methods can attain comparable performance with the deep learning-based matrix completion methods.},
  archive      = {J_PR},
  author       = {Xiaobo Jin and Jianyu Miao and Qiufeng Wang and Guanggang Geng and Kaizhu Huang},
  doi          = {10.1016/j.patcog.2022.108655},
  journal      = {Pattern Recognition},
  pages        = {108655},
  shortjournal = {Pattern Recognition},
  title        = {Sparse matrix factorization with l2,1 norm for matrix completion},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making person search enjoy the merits of person
re-identification. <em>PR</em>, <em>127</em>, 108654. (<a
href="https://doi.org/10.1016/j.patcog.2022.108654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search is an extended task of person re-identification (Re-ID). However, most existing one-step person search works do not study how to employ existing Re-ID models to improve the one-step person search. To address this issue, we propose a Teacher-guided Disentangling Network (TDN) to make the one-step person search enjoy the merits of existing Re-ID research. The proposed TDN can significantly boost person search performance by transferring the advanced person Re-ID knowledge to the person search model. In the proposed TDN, for better knowledge transfer from the Re-ID teacher model to the one-step person search model, we design a new one-step person search base framework by partially disentangling the two subtasks. Besides, we propose a Knowledge Transfer Bridge module to bridge the scale gap caused by different input formats between the Re-ID model and the one-step person search model. Moreover, we also propose a Ranking with Context Persons strategy to exploit the context information in panoramic images for better ranking. Experiments on two public person search datasets demonstrate the favorable performance of the proposed method.},
  archive      = {J_PR},
  author       = {Chuang Liu and Hua Yang and Qin Zhou and Shibao Zheng},
  doi          = {10.1016/j.patcog.2022.108654},
  journal      = {Pattern Recognition},
  pages        = {108654},
  shortjournal = {Pattern Recognition},
  title        = {Making person search enjoy the merits of person re-identification},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring interactive attribute reduction via fuzzy
complementary entropy for unlabeled mixed data. <em>PR</em>,
<em>127</em>, 108651. (<a
href="https://doi.org/10.1016/j.patcog.2022.108651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is one of the important applications in fuzzy rough set theory . However, most attribute reduction methods in fuzzy rough theory mainly focus on removing irrelevant or redundant attributes. There are few reports about the method of considering attribute interaction. For this reason, this paper proposes an interactive attribute reduction method for unlabeled mixed data. First, some uncertainty measures based on fuzzy complementary entropy are further defined. Then, based on the proposed uncertainty measure, the attribute evaluation criteria of maximal information, minimal redundancy, and maximal interactivity are developed respectively. As a result, the evaluation index of the attribute importance is established by using the idea of unsupervised maximal information-minimal redundancy-maximal interactivity. Finally, a corresponding algorithm is designed to select attributes. The experimental results show that the proposed algorithm has better performance.},
  archive      = {J_PR},
  author       = {Zhong Yuan and Hongmei Chen and Tianrui Li},
  doi          = {10.1016/j.patcog.2022.108651},
  journal      = {Pattern Recognition},
  pages        = {108651},
  shortjournal = {Pattern Recognition},
  title        = {Exploring interactive attribute reduction via fuzzy complementary entropy for unlabeled mixed data},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative brain tumor retrieval for MR images based on
user’s intention model. <em>PR</em>, <em>127</em>, 108650. (<a
href="https://doi.org/10.1016/j.patcog.2022.108650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, medical content-based image retrieval (CBIR) systems select low-level visual features as image descriptors. However, these descriptors fail to provide clues for understanding the content of medical images in a similar way as a human expert, which makes the retrieval results inconsistent with the user’s intention. To solve this problem, we propose a closed-loop brain tumor retrieval system for MR images with an eye-tracking based relevance feedback mechanism. In our method, we first model the intention of the user by training a convolutional neural network based on the temporal and spatial features extracted from his/her eye-tracking data collected when inspecting the relevance between different images. Upon using visual features as a bridge, the relevancy degree to the query image of any of the database images is computed with our user’s intention model by transferring to it the eye movement data from the most visually similar image amongst images iteratively accumulated in the canvas. Our proposed retrieval system is implemented in an iterative manner. In each round of iteration, user’s eye movement data when inspecting the system returns are collected and the canvas collection of images is also updated by appending to it the user inspected system returns. With the updated canvas collections, the relevancy degree of database images can be recomputed and the system can begin a new round search of the most relevant images. Extensive experiments have been performed on a publicly available T1-weighted contrast-enhanced magnetic resonance image (CE-MRI) dataset that consists of three types of brain tumors (glioma, meningioma, and pituitary tumor) collected from 233 patients with a total of 3064 images across the axial, coronal, and sagittal views. Experimental results of 22 volunteers (11 males and 11 females, with an average age of 24.4 years) from our medical school show that upon implicit involvement of users in the brain tumor retrieving process, our proposed system significantly outperforms state-of-the-art methods and achieves P r e c @ 10 Prec@10 to 99.94\% 99.94\% , m A P mAP to 97.95\% 97.95\% after the third round of iteration.},
  archive      = {J_PR},
  author       = {Mengli Sun and Wei Zou and Nan Hu and Jiajun Wang and Zheru Chi},
  doi          = {10.1016/j.patcog.2022.108650},
  journal      = {Pattern Recognition},
  pages        = {108650},
  shortjournal = {Pattern Recognition},
  title        = {Iterative brain tumor retrieval for MR images based on user’s intention model},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient deep neural network for photo-realistic image
super-resolution. <em>PR</em>, <em>127</em>, 108649. (<a
href="https://doi.org/10.1016/j.patcog.2022.108649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in deep learning-based models has improved photo-realistic (or perceptual) single-image super-resolution significantly. However, despite their powerful performance, many methods are difficult to apply to real-world applications because of the heavy computational requirements. To facilitate the use of a deep model under such demands, we focus on keeping the network efficient while maintaining its performance. In detail, we design an architecture that implements a cascading mechanism on a residual network to boost the performance with limited resources via multi-level feature fusion . In addition, our proposed model adopts group convolution and recursive schemes in order to achieve extreme efficiency. We further improve the perceptual quality of the output by employing the adversarial learning paradigm and a multi-scale discriminator approach. The performance of our method is investigated through extensive internal experiments and benchmarks using various datasets. Our results show that our models outperform the recent methods with similar complexity, for both traditional pixel-based and perception-based tasks.},
  archive      = {J_PR},
  author       = {Namhyuk Ahn and Byungkon Kang and Kyung-Ah Sohn},
  doi          = {10.1016/j.patcog.2022.108649},
  journal      = {Pattern Recognition},
  pages        = {108649},
  shortjournal = {Pattern Recognition},
  title        = {Efficient deep neural network for photo-realistic image super-resolution},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic fine-grained glomerular lesion recognition in
kidney pathology. <em>PR</em>, <em>127</em>, 108648. (<a
href="https://doi.org/10.1016/j.patcog.2022.108648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of glomeruli lesions is the key for diagnosis and treatment planning in kidney pathology; however, the coexisting glomerular structures such as mesangial regions exacerbate the difficulties of this task. In this paper, we introduce a scheme to recognize fine-grained glomeruli lesions from whole slide images. First, a focal instance structural similarity loss is proposed to drive the model to locate all types of glomeruli precisely. Then an Uncertainty Aided Apportionment Network is designed to carry out the fine-grained visual classification without bounding-box annotations. This double branch-shaped structure extracts common features of the child class from the parent class and produces the uncertainty factor for reconstituting the training dataset. Results of slide-wise evaluation illustrate the effectiveness of the entire scheme, with an 8–22\% improvement of the mean Average Precision compared with remarkable detection methods. The comprehensive results clearly demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Yang Nan and Fengyi Li and Peng Tang and Guyue Zhang and Caihong Zeng and Guotong Xie and Zhihong Liu and Guang Yang},
  doi          = {10.1016/j.patcog.2022.108648},
  journal      = {Pattern Recognition},
  pages        = {108648},
  shortjournal = {Pattern Recognition},
  title        = {Automatic fine-grained glomerular lesion recognition in kidney pathology},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CubeNet: X-shape connection for camouflaged object
detection. <em>PR</em>, <em>127</em>, 108644. (<a
href="https://doi.org/10.1016/j.patcog.2022.108644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to detect out-of-attention regions in an image. Current binary segmentation solutions fail to tackle COD easily, since COD is more challenging due to object often accompany with weak boundaries, low contrast, or similar patterns to the background. That is, we need a more efficient scheme to address this problem. In this work, we propose a new COD framework called CubeNet by introducing X X connection to the standard encoder-decoder architecture. Specifically, CubeNet consists of two square fusion decoder (SFD) and a sub edge decoder (SED). The special designed SFD takes full advantage of low-level and high-level features extracted from encoder-decoder blocks, providing more powerful representations at each stage. To explicitly modeling the weak boundaries of the objects, we introduced a SED between the two SFD. With such kind of holistic designs , these three decoder modules resolve the challenging ambiguity of camouflaged object detection. CubeNet significantly advance the cutting-edge model on three challenging COD datasets ( i.e. , COD10K, CAMO, and CHAMELEON), and achieves the real-time (50fps) inference.},
  archive      = {J_PR},
  author       = {Mingchen Zhuge and Xiankai Lu and Yiyou Guo and Zhihua Cai and Shuhan Chen},
  doi          = {10.1016/j.patcog.2022.108644},
  journal      = {Pattern Recognition},
  pages        = {108644},
  shortjournal = {Pattern Recognition},
  title        = {CubeNet: X-shape connection for camouflaged object detection},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BeCAPTCHA-mouse: Synthetic mouse trajectories and improved
bot detection. <em>PR</em>, <em>127</em>, 108643. (<a
href="https://doi.org/10.1016/j.patcog.2022.108643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions , and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93\% 93\% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36\% 36\% , proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.},
  archive      = {J_PR},
  author       = {Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez},
  doi          = {10.1016/j.patcog.2022.108643},
  journal      = {Pattern Recognition},
  pages        = {108643},
  shortjournal = {Pattern Recognition},
  title        = {BeCAPTCHA-mouse: Synthetic mouse trajectories and improved bot detection},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental and compressible kernel null discriminant
analysis. <em>PR</em>, <em>127</em>, 108642. (<a
href="https://doi.org/10.1016/j.patcog.2022.108642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel discriminant analysis (KDA), the nonlinear extension of linear Discriminant Analysis (LDA), is a popular tool for learning one or multiple categories in nonlinear data sets. However, in most modern pattern recognition applications such as video surveillance, data are collected in flow and require sequential processing. In this context, KDA is faced two critical issues: an original formulation unsuited to the dynamic nature of the data and an increasing memory requirement for the kernel matrix storage. Motivated by the state-of-the-art performance reported by the null KDA, we propose in this paper a new solution to solve the null KDA (NKDA) in the context of data streams. Compared to previous works, our contribution is based on three points: first, we develop an exact incremental scheme which guarantees accurate solutions. Secondly, we develop a compression mechanism based on the following observation: rger the size of the training data set more the distances in the null space contract This property of the null space leads to formulate an indicator of redundancy in the training data set. This criterion is the cornerstone of our incremental KNDA because it authorizes incremental learning on large-scale data sets. Third, the problem of novelty detection in multi-class and one-class scenarios is addressed. More precisely, the fact that distances in the null space change over the training period leads us to define adjustable novelty thresholds. Lastly, numerous experiments based on various publicly available data sets and state-of-the-art classifiers show that the proposed method is effective both for multi-class and one-class real applications.},
  archive      = {J_PR},
  author       = {F. Dufrenois},
  doi          = {10.1016/j.patcog.2022.108642},
  journal      = {Pattern Recognition},
  pages        = {108642},
  shortjournal = {Pattern Recognition},
  title        = {Incremental and compressible kernel null discriminant analysis},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Table detection in business document images by message
passing networks. <em>PR</em>, <em>127</em>, 108641. (<a
href="https://doi.org/10.1016/j.patcog.2022.108641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular structures in business documents offer a complementary dimension to the raw textual data. For instance, there is information about the relationships among pieces of information. Nowadays, digital mailroom applications have become a key service for workflow automation . Therefore, the detection and interpretation of tables is crucial. With the recent advances in information extraction, table detection and recognition has gained interest in document image analysis, in particular, with the absence of rule lines and unknown information about rows and columns. However, business documents usually contain sensitive contents limiting the amount of public benchmarking datasets . In this paper, we propose a graph-based approach for detecting tables in document images which do not require the raw content of the document. Hence, the sensitive content can be previously removed and, instead of using the raw image or textual content, we propose a purely structural approach to keep sensitive data anonymous. Our framework uses graph neural networks (GNNs) to describe the local repetitive structures that constitute a table. In particular, our main application domain are business documents. We have carefully validated our approach in two invoice datasets and a modern document benchmark. Our experiments demonstrate that tables can be detected by purely structural approaches.},
  archive      = {J_PR},
  author       = {Pau Riba and Lutz Goldmann and Oriol Ramos Terrades and Diede Rusticus and Alicia Fornés and Josep Lladós},
  doi          = {10.1016/j.patcog.2022.108641},
  journal      = {Pattern Recognition},
  pages        = {108641},
  shortjournal = {Pattern Recognition},
  title        = {Table detection in business document images by message passing networks},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical independence of ECG for biometric
authentication. <em>PR</em>, <em>127</em>, 108640. (<a
href="https://doi.org/10.1016/j.patcog.2022.108640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biometric authentication system using electrocardiogram (ECG) may protect individuals’ privacy and prevent identity frauds. Researchers have demonstrated that ECG is suitable for biometrics use due to its pervasiveness, immutability, measurability , acceptance, and individuality. However, ECG’s statistical independence for biometric authentication has yet to be substantiated. Thereby, this paper proposes a novel model to evaluate the statistical independence of ECG among individuals using heartbeat morphological features. The signal is qualitatively improved and heartbeat features are extracted using signal processing techniques . Three classes of features such as interval, amplitude, and angle are extracted from each heartbeat. The hypothesis estimating the probability of resemblance of interval, amplitude, and angle classes of features is derived. The accumulated effect of these classes of features measure the statistical independence of ECG. Further, the proposed model of statistical independence of ECG biometrics is validated by comparing the statistical performance with the empirical performance of the ECG verification system . The empirical performance is estimated using three different ECG biometric methods, i.e., traditional intraclass-interclass features, artificial neural network , and convolutional neural network . The false resemblance probabilities of heartbeats among individuals computed for four interval class features, five amplitude class features, and five angle class features are found to be 3.4 × 10 − 6 3.4×10−6 , 1.0 × 10 − 7 1.0×10−7 , and 3.9 × 10 − 8 3.9×10−8 , respectively. The cumulative probability of resemblance computed using fourteen heartbeat features of interval, amplitude, and angle classes is found as 1.3 × 10 − 20 1.3×10−20 .},
  archive      = {J_PR},
  author       = {Ranjeet Srivastva and Yogendra Narain Singh and Ashutosh Singh},
  doi          = {10.1016/j.patcog.2022.108640},
  journal      = {Pattern Recognition},
  pages        = {108640},
  shortjournal = {Pattern Recognition},
  title        = {Statistical independence of ECG for biometric authentication},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence analysis of connection center evolution and
faster clustering. <em>PR</em>, <em>127</em>, 108639. (<a
href="https://doi.org/10.1016/j.patcog.2022.108639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a subjective task, that is, several different results can be obtained from a single clustering hierarchy, depending on the observation scale. A local view of the data may necessitate more clusters, whereas a global view requires fewer clusters. It is, therefore, important to provide users with an appropriate clustering hierarchy and let them select the final clustering result based on their own observation scale. Thus, a new clustering method , named connection center evolution (CCE), was recently developed by Geng and Tang (2020). CCE provides gradual clustering results by iteratively merging cluster centers. However, theoretical evidence for its convergence is missing, and the center evolution requires a connectivity matrix of a significantly higher order as iterations proceed , resulting in higher computational costs. Accordingly, we present a convergence analysis of CCE using the properties of ergodic Markov chains and propose a faster algorithm using the enhanced connectivity graph derived from the convergence analysis. Empirical evidence from numerical experiments and theoretical proofs demonstrate the advantages of the proposed method.},
  archive      = {J_PR},
  author       = {Jaemin Lee and Minseok Han and Jong-Seok Lee},
  doi          = {10.1016/j.patcog.2022.108639},
  journal      = {Pattern Recognition},
  pages        = {108639},
  shortjournal = {Pattern Recognition},
  title        = {Convergence analysis of connection center evolution and faster clustering},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation via distilled discriminative
clustering. <em>PR</em>, <em>127</em>, 108638. (<a
href="https://doi.org/10.1016/j.patcog.2022.108638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation addresses the problem of classifying data in an unlabeled target domain, given labeled source domain data that share a common label space but follow a different distribution. Most of the recent methods take the approach of explicitly aligning feature distributions between the two domains. Differently, motivated by the fundamental assumption for domain adaptability, we re-cast the domain adaptation problem as discriminative clustering of target data, given strong privileged information provided by the closely related, labeled source data. Technically, we use clustering objectives based on a robust variant of entropy minimization that adaptively filters target data, a soft Fisher-like criterion, and additionally the cluster ordering via centroid classification. To distill discriminative source information for target clustering, we propose to jointly train the network using parallel, supervised learning objectives over labeled source data. We term our method of distilled discriminative clustering for domain adaptation as DisClusterDA. We also give geometric intuition that illustrates how constituent objectives of DisClusterDA help learn class-wisely pure, compact feature distributions. We conduct careful ablation studies and extensive experiments on five popular benchmark datasets, including a multi-source domain adaptation one. Based on commonly used backbone networks , DisClusterDA outperforms existing methods on these benchmarks. It is also interesting to observe that in our DisClusterDA framework, adding an additional loss term that explicitly learns to align class-level feature distributions across domains does harm to the adaptation performance, though more careful studies in different algorithmic frameworks are to be conducted.},
  archive      = {J_PR},
  author       = {Hui Tang and Yaowei Wang and Kui Jia},
  doi          = {10.1016/j.patcog.2022.108638},
  journal      = {Pattern Recognition},
  pages        = {108638},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation via distilled discriminative clustering},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GA3N: Generative adversarial AutoAugment network.
<em>PR</em>, <em>127</em>, 108637. (<a
href="https://doi.org/10.1016/j.patcog.2022.108637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is beneficial for improving robustness of deep meta-learning. However, data augmentation methods for the recent deep meta-learning are still based on photometric or geometric manipulations or combinations of images. This paper proposes a generative adversarial autoaugment network (GA3N) for enlarging the augmentation search space and improving classification accuracy . To achieve, we first extend the search space of image augmentation by using GANs. However, the main challenge is to generate images suitable for the task. For solution, we find the best policy by optimizing a target and GAN losses alternatively. We then use the manipulated and generated samples determined by the policy network as augmented samples for improving the target tasks. To show the effects of our method, we implement classification networks by combining our GA3N and evaluate them on CIFAR-100 and Tiny-ImageNet datasets. As a result, we achieve better accuracy than the recent AutoAugment methods on each dataset.},
  archive      = {J_PR},
  author       = {Vanchinbal Chinbat and Seung-Hwan Bae},
  doi          = {10.1016/j.patcog.2022.108637},
  journal      = {Pattern Recognition},
  pages        = {108637},
  shortjournal = {Pattern Recognition},
  title        = {GA3N: Generative adversarial AutoAugment network},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EANet: Iterative edge attention network for medical image
segmentation. <em>PR</em>, <em>127</em>, 108636. (<a
href="https://doi.org/10.1016/j.patcog.2022.108636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automatic segmentation of medical images can greatly assist the clinical diagnosis and analysis. However, it remains a challenging task due to (1) the diversity of scale in the medical image targets and (2) the complex context environments of medical images, including ambiguity of structural boundaries, complexity of shapes, and the heterogeneity of textures. To comprehensively tackle these challenges, we propose a novel and effective iterative edge attention network (EANet) for medical image segmentation with steps as follows. First, we propose a dynamic scale-aware context (DSC) module, which dynamically adjusts the receptive fields to extract multi-scale contextual information efficiently. Second, an edge-attention preservation (EAP) module is employed to effectively remove noise and help the edge stream focus on processing only the boundary-related information. Finally, a multi-level pairwise regression (MPR) module is designed to combine the complementary edge and region information for refining the ambiguous structure. This iterative optimization helps to learn better representations and more accurate saliency maps . Extensive experimental results demonstrate that the proposed network achieves superior segmentation performance to state-of-the-art methods in four different challenging medical segmentation tasks , including lung nodule segmentation, COVID-19 infection segmentation, lung segmentation, and thyroid nodule segmentation. The source code of our method is available at https://github.com/DLWK/EANet},
  archive      = {J_PR},
  author       = {Kun Wang and Xiaohong Zhang and Xiangbo Zhang and Yuting Lu and Sheng Huang and Dan Yang},
  doi          = {10.1016/j.patcog.2022.108636},
  journal      = {Pattern Recognition},
  pages        = {108636},
  shortjournal = {Pattern Recognition},
  title        = {EANet: Iterative edge attention network for medical image segmentation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative information restoration and extraction for
weakly supervised low-resolution fine-grained image recognition.
<em>PR</em>, <em>127</em>, 108629. (<a
href="https://doi.org/10.1016/j.patcog.2022.108629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing methods of fine-grained image recognition mainly devote to learning subtle yet discriminative features from the high-resolution input. However, their performance deteriorates significantly when they are used for low quality images because a lot of discriminative details of images are missing. We propose a discriminative information restoration and extraction network, termed as DRE-Net, to address the problem of low-resolution fine-grained image recognition, which has widespread application potential, such as shelf auditing and surveillance scenarios. DRE-Net is the first framework for weakly supervised low-resolution fine-grained image recognition and consists of two sub-networks: (1) fine-grained discriminative information restoration sub-network (FDR) and (2) recognition sub-network with the semantic relation distillation loss (SRD-loss). The first module utilizes the structural characteristic of minimum spanning tree (MST) to establish context information for each pixel by employing the spatial structures between each pixel and other pixels, which can help FDR focus on and restore the critical texture details. The second module employs the SRD-loss to calibrate recognition sub-network by transferring the correct relationships between every two pixels on the feature map. Meanwhile the SRD-loss can further prompt the FDR to recover reliable and accurate fine-grained details and guide the recognition sub-network to perceive the discriminative features from the correct relationships. Extensive experiments on three benchmark datasets and one retail product dataset demonstrate the effectiveness of our proposed framework.},
  archive      = {J_PR},
  author       = {Tiantian Yan and Jian Shi and Haojie Li and Zhongxuan Luo and Zhihui Wang},
  doi          = {10.1016/j.patcog.2022.108629},
  journal      = {Pattern Recognition},
  pages        = {108629},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative information restoration and extraction for weakly supervised low-resolution fine-grained image recognition},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based modelling of superpixels for automatic
identification of empty shelves in supermarkets. <em>PR</em>,
<em>127</em>, 108627. (<a
href="https://doi.org/10.1016/j.patcog.2022.108627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of empty spaces (gaps) between the displayed products as seen in the images of shelves of a supermarket is an interesting image segmentation problem. This paper presents the first known attempt to solve this commercially relevant challenge. The shelf image is first over-segmented into a number of superpixels to construct a graph of superpixels (SG). Subsequently, a graph convolutional network and a Siamese network are built to process the SG. Finally, a structural support vector machine based inference model is formulated based on SG for segmenting the gap and non-gap regions. In order to validate our method, we manually annotate the images of shelves of three benchmark datasets of retail products. We have achieved ∼ ∼ 70 to ∼ ∼ 85\% 85\% segmentation accuracy (in terms of mean intersection-over-union ) on the annotated datasets. A part of the annotated data is released at https://github.com/gapDetection/gapDetectionDatasets .},
  archive      = {J_PR},
  author       = {Bikash Santra and Udita Ghosh and Dipti Prasad Mukherjee},
  doi          = {10.1016/j.patcog.2022.108627},
  journal      = {Pattern Recognition},
  pages        = {108627},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based modelling of superpixels for automatic identification of empty shelves in supermarkets},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rotation invariant point cloud analysis: Where local
geometry meets global topology. <em>PR</em>, <em>127</em>, 108626. (<a
href="https://doi.org/10.1016/j.patcog.2022.108626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud analysis is a fundamental task in 3D computer vision . Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose an approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.},
  archive      = {J_PR},
  author       = {Chen Zhao and Jiaqi Yang and Xin Xiong and Angfan Zhu and Zhiguo Cao and Xin Li},
  doi          = {10.1016/j.patcog.2022.108626},
  journal      = {Pattern Recognition},
  pages        = {108626},
  shortjournal = {Pattern Recognition},
  title        = {Rotation invariant point cloud analysis: Where local geometry meets global topology},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2K-fold-net and feature enhanced 4-fold-net for medical
image segmentation. <em>PR</em>, <em>127</em>, 108625. (<a
href="https://doi.org/10.1016/j.patcog.2022.108625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For segmenting medical images, U-Net has become a popular and effective tool. However, it also has some shortcomings in segmenting fuzzy boundaries and eliminating interferences. Improvements of the original U-Net have been proposed by many authors, resulting in many variants such as MultiResUNet, DoubleU-Net and W-Net. Based on the common characteristics of these structures, we propose in this work a generalized structure by multiplying the folds of a fully convolutional network (FCN) for even more times, and thus name it as “2K-Fold-Net”. The more folds in this structure provide more freedoms to create cross links between the neighboring folds. The influence of the fold-pair number K K on its performance is also studied. The realizations with K K up to 6 are compared to three other variants of cascaded U-Nets using the CVC-ClinicDB dataset. Then the special case “4-Fold-Net” is further empowered with the feature enhancing functionalities recently seen in the attention-aware feature enhancement method. This new net is hence named as “Enhanced-Feature-4-Fold-Net”, abbreviated as “EF 3 3 -Net”. Finally, 2K-Fold-Net and EF 3 3 -Net have been compared with U-Net, SegNet, DoubleU-Net, MultiResUNet and its variants using four challenging medical image datasets. The results have demonstrated that the proposed nets outperform the other variants of U-Net, even with slightly lower amount of parameters. The code is available on: https://github.com/raik7/EF3-Net .},
  archive      = {J_PR},
  author       = {Yunchu Zhang and Jianfei Dong},
  doi          = {10.1016/j.patcog.2022.108625},
  journal      = {Pattern Recognition},
  pages        = {108625},
  shortjournal = {Pattern Recognition},
  title        = {2K-fold-net and feature enhanced 4-fold-net for medical image segmentation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SA-DPNet: Structure-aware dual pyramid network for salient
object detection. <em>PR</em>, <em>127</em>, 108624. (<a
href="https://doi.org/10.1016/j.patcog.2022.108624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection aims at highlighting the most visually distinctive objects in the scene. Previous deep learning based works mainly focus on designing different integration strategies of multi-level features to improve the quality of prediction. However, due to the negligence of spatial structure coherence in predicted saliency maps , they fail to produce satisfactory results in complex scenarios. In this work, we present a structure-aware dual pyramid network (SA-DPNet) for salient object detection. By explicitly formulating spatial location information and spatial covariance features into the self-attention mechanism, a structure-aware spatial non-local block is proposed in SA-DPNet to learn the spatial-sensitive global context. With the proposed edge loss and adversarial loss, the edge structure context and patch-based global structure context are introduced to refine the structural coherence of the predicted results. Comprehensive experimental results on six RGB saliency benchmark datasets and three RGB-D saliency benchmark datasets demonstrate the superiority of proposed SA-DPNet over other state-of-the-art methods, both quantitatively and visually.},
  archive      = {J_PR},
  author       = {Xuemiao Xu and Jiaxing Chen and Huaidong Zhang and Guoqiang Han},
  doi          = {10.1016/j.patcog.2022.108624},
  journal      = {Pattern Recognition},
  pages        = {108624},
  shortjournal = {Pattern Recognition},
  title        = {SA-DPNet: Structure-aware dual pyramid network for salient object detection},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel hybrid model for short-term prediction of wind
speed. <em>PR</em>, <em>127</em>, 108623. (<a
href="https://doi.org/10.1016/j.patcog.2022.108623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the randomness and contingency of wind speed size and direction, it is difficult to predict the wind speed accurately, which seriously affects the stable operation of the power system . To improve the operation stability of power system , the accurate prediction of wind speed is very important. In this paper, a new hybrid model based on gray wolf algorithm (GWO) and support vector machine (SVM) for wind speed prediction is proposed. Firstly, Neo4j(NE) is utilized to identify the data and preprocess the data. Secondly, k-means clustering(KC) is utilized to analyze data and eliminate invalid data. Thirdly, GWO is utilized to optimize the kernel function parameters and penalty factors of SVM to improve the prediction results. Fourthly, The four modules are combined into NE-KC-GWO-SVM model to predict the wind speed accurately. Finally, to verify the effectiveness of the proposed model, the prediction accuracy of the model is experimentally analyzed from two parts. One is to analyze the superiority of the model itself by using the method of single model removed. The results show that the proposed model is the best, and has high accuracy, and can reflect the characteristics of wind speed well and truly. The other one is that models similar to those proposed in the literature are selected for comparative analysis. The experimental results show that compared with the other two models, the proposed model has the best accuracy. At the same time, the proposed model has good prediction stability and acceptable time complexity. Based on all the experimental results, it can be obtained that the proposed model has better prediction effect, which can provide a scientific basis for the macro-control of power system and improve the operation security and stability of power system.},
  archive      = {J_PR},
  author       = {Haize Hu and Yunyi Li and Xiangping Zhang and Mengge Fang},
  doi          = {10.1016/j.patcog.2022.108623},
  journal      = {Pattern Recognition},
  pages        = {108623},
  shortjournal = {Pattern Recognition},
  title        = {A novel hybrid model for short-term prediction of wind speed},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised feature selection via adaptive graph and
dependency score. <em>PR</em>, <em>127</em>, 108622. (<a
href="https://doi.org/10.1016/j.patcog.2022.108622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is an important topic in the fields of machine learning , pattern recognition and data mining. The representation methods include adaptive-graph-based methods and self-representation-based methods. The former methods have a longstanding and undiscovered problem about imbalanced neighbors, and the latter ones do not perform well when features are not linearly dependent . To deal with these problems, a novel unsupervised feature selection method is proposed to ensure k connectivity and eliminate more redundant features based on adaptive graph and dependency score (AGDS). Extensive experiments conducted on 13 benchmark datasets show the effectiveness of AGDS.},
  archive      = {J_PR},
  author       = {Pei Huang and Xiaowei Yang},
  doi          = {10.1016/j.patcog.2022.108622},
  journal      = {Pattern Recognition},
  pages        = {108622},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised feature selection via adaptive graph and dependency score},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human and action recognition using adaptive energy images.
<em>PR</em>, <em>127</em>, 108621. (<a
href="https://doi.org/10.1016/j.patcog.2022.108621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new temporal template approach for action recognition and person identification based on motion sequence information in masked depth video streams obtained from RGB-D data. This new representation creates a membership function that models the change in motion based on the correlation between frames that occur during motion flow. The energy images created with this function emphasize the intervals of motion with more change, while the intervals with less change are suppressed. To understand the distinctive features, the obtained energy images by using the proposed function are given as input to the convolutional neural networks and different handcrafted classifiers. The proposed method was observed on the BodyLogin, NATOPS, and SBU Kinect datasets and compared with the existing temporal templates and recent methods. The results indicate that the proposed method provides both higher performance and better motion representation.},
  archive      = {J_PR},
  author       = {Onur Can Kurban and Nurullah Calik and Tülay Yildirim},
  doi          = {10.1016/j.patcog.2022.108621},
  journal      = {Pattern Recognition},
  pages        = {108621},
  shortjournal = {Pattern Recognition},
  title        = {Human and action recognition using adaptive energy images},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LiTMNet: A deep CNN for efficient HDR image reconstruction
from a single LDR image. <em>PR</em>, <em>127</em>, 108620. (<a
href="https://doi.org/10.1016/j.patcog.2022.108620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods can generate a high dynamic range (HDR) image from a single low dynamic range (LDR) image using convolutional neural networks (CNNs). However, they are too cumbersome to run on mobile devices with limited computational resources. In this work, we design a lightweight CNN, namely LiTMNet which takes a single LDR image as input and recovers the lost information in its saturated regions to reconstruct an HDR image. To avoid trading off the reconstruction quality for efficiency, LiTMNet does not only adapt a lightweight encoder for efficient feature extraction, but also contains newly designed upsampling blocks in the decoder to alleviate artifacts and further accelerate the reconstruction. The final HDR image is produced by nonlinearly blending the network prediction and the original LDR image. Qualitative and quantitative comparisons demonstrate that LiTMNet produces HDR images of high quality comparable with the current state of the art and is 38 × 38× faster as tested on a mobile device. Please refer to the supplementary video for additional visual results.},
  archive      = {J_PR},
  author       = {Guotao Wu and Ran Song and Mingxin Zhang and Xiaolei Li and Paul L. Rosin},
  doi          = {10.1016/j.patcog.2022.108620},
  journal      = {Pattern Recognition},
  pages        = {108620},
  shortjournal = {Pattern Recognition},
  title        = {LiTMNet: A deep CNN for efficient HDR image reconstruction from a single LDR image},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ELSED: Enhanced line SEgment drawing. <em>PR</em>,
<em>127</em>, 108619. (<a
href="https://doi.org/10.1016/j.patcog.2022.108619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting local features , such as corners, segments or blobs, is the first step in the pipeline of many Computer Vision applications. Its speed is crucial for real-time applications. In this paper we present ELSED, the fastest line segment detector in the literature. The key for its efficiency is a local segment growing algorithm that connects gradient-aligned pixels in presence of small discontinuities. The proposed algorithm not only runs in devices with very low end hardware, but may also be parametrized to foster the detection of short or longer segments, depending on the task at hand. We also introduce new metrics to evaluate the accuracy and repeatability of segment detectors. In our experiments with different public benchmarks we prove that our method accounts the highest repeatability and it is the most efficient in the literature. 1 In the experiments we quantify the accuracy traded for such gain.},
  archive      = {J_PR},
  author       = {Iago Suárez and José M. Buenaposada and Luis Baumela},
  doi          = {10.1016/j.patcog.2022.108619},
  journal      = {Pattern Recognition},
  pages        = {108619},
  shortjournal = {Pattern Recognition},
  title        = {ELSED: Enhanced line SEgment drawing},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical feature disentangling network for universal
domain adaptation. <em>PR</em>, <em>127</em>, 108616. (<a
href="https://doi.org/10.1016/j.patcog.2022.108616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal Domain Adaptation (UniDA) aims to address a more practical problem compared with traditional Close-Set Domain Adaptation (CSDA). Besides the domain gap in traditional CSDA, the common and private label sets across domains are unknown in UniDA leading to an additional category gap. Without considering the category gap for domain adversarial training to extract domain-relevant features, existing methods may suffer from the feature misalignment problem and result in negative transfer. This paper proposes a Hierarchical Feature Disentangling Network (HFDN) to disentangle domain-relevant features into domain-specific and category-shift features for latent variables caused by domain gap and category gap, respectively. Domain-specific features are trained to distinguish the source domain from the target one by discovering domain-specific attributes (e.g. illumination, style), and adversarially aligned to bridge the domain gap for knowledge transfer. Category-shift features are extracted to distinguish domains by identifying private classes across domains, so that they can be leveraged to assign larger weights for samples from the common label set. Experiments show that the proposed HFDN surpasses state-of-the-art CSDA, partial DA, open-set DA and UniDA models.},
  archive      = {J_PR},
  author       = {Yuan Gao and Peipeng Chen and Yue Gao and Jinpeng Wang and YoungSun Pan and Andy J. Ma},
  doi          = {10.1016/j.patcog.2022.108616},
  journal      = {Pattern Recognition},
  pages        = {108616},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical feature disentangling network for universal domain adaptation},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft pseudo-label shrinkage for unsupervised domain adaptive
person re-identification. <em>PR</em>, <em>127</em>, 108615. (<a
href="https://doi.org/10.1016/j.patcog.2022.108615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One effective way to tackle unsupervised domain adaptation (UDA) on person re-identification (Re-ID) is to use clustering-based self-training approach, where a model is trained with hard pseudo-labels obtained from a clustering method . Using a hard pseudo-label, a sample is assigned to the cluster with the highest probability, which is sensitive to the incorrect clustering result due to imperfect clustering algorithms . Soft pseudo-labels can mitigate this issue by representing the sample with the full range of class probabilities from all clusters. Specifically, soft pseudo-labels comprise probabilities of full range classes, because they consider both the hard samples and easy samples. This will distract the model from learning more discriminative features in the hard examples. To solve this issue, we propose a coarse-to-fine refinement mechanism to produce robust refined soft pseudo-labels by progressively focusing more on the hard samples while less on the easy samples. The proposed refined soft pseudo-labels can be readily integrated into cross-entropy loss as a strong supervision to guide the model to learn more discriminative features . Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art unsupervised domain adaptation approaches on person Re-ID with a considerable margin. Code will be available at: http://github.com/Dingyuan-Zheng/ctf-UDA .},
  archive      = {J_PR},
  author       = {Dingyuan Zheng and Jimin Xiao and Ke Chen and Xiaowei Huang and Lin Chen and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.108615},
  journal      = {Pattern Recognition},
  pages        = {108615},
  shortjournal = {Pattern Recognition},
  title        = {Soft pseudo-label shrinkage for unsupervised domain adaptive person re-identification},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning residue-aware correlation filters and refining
scale for real-time UAV tracking. <em>PR</em>, <em>127</em>, 108614. (<a
href="https://doi.org/10.1016/j.patcog.2022.108614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-based tracking finds its applications in agriculture, aviation, navigation, transportation and public security, etc and develops rapidly recently. However, due to limitations of computing resources, battery capacity, requirement of low power and maximum load of UAV, the deployment of deep learning-based tracking algorithms in UAV is currently not feasible and therefore discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. But confronted with difficult challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. Inspired by the good optimization properties associated with residue representation, in this paper we exploit the residue nature inherent to videos and propose residue-aware correlation filters which demonstrate better convergence properties in filter learning. In addition, we propose a scale refinement strategy to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, in fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance in UAV tracking.},
  archive      = {J_PR},
  author       = {Shuiwang Li and Yuting Liu and Qijun Zhao and Ziliang Feng},
  doi          = {10.1016/j.patcog.2022.108614},
  journal      = {Pattern Recognition},
  pages        = {108614},
  shortjournal = {Pattern Recognition},
  title        = {Learning residue-aware correlation filters and refining scale for real-time UAV tracking},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From general to specific: Online updating for blind
super-resolution. <em>PR</em>, <em>127</em>, 108613. (<a
href="https://doi.org/10.1016/j.patcog.2022.108613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are trained on samples synthesized by predefined degradations ( e.g .bicubic downsampling), regardless of the domain gap between training and testing data. 2) During testing, they super-resolve all images by the same set of model weights, ignoring the degradation variety. As a result, most previous methods may suffer a performance drop when the degradations of test images are unknown and various ( i.e .the case of blind SR). To address these issues, we propose an online SR (ONSR) method. It does not rely on predefined degradations and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image , and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus get more robust to various degradations. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR.},
  archive      = {J_PR},
  author       = {Shang Li and Guixuan Zhang and Zhengxiong Luo and Jie Liu and Zhi Zeng and Shuwu Zhang},
  doi          = {10.1016/j.patcog.2022.108613},
  journal      = {Pattern Recognition},
  pages        = {108613},
  shortjournal = {Pattern Recognition},
  title        = {From general to specific: Online updating for blind super-resolution},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of metrics on biclustering solution and quality: A
review. <em>PR</em>, <em>127</em>, 108612. (<a
href="https://doi.org/10.1016/j.patcog.2022.108612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To understand how subspace clustering algorithms discover distinct bicluster types and how their effectiveness has been validated, we offer a systematic literature review on available merit functions and how they affect the biclustering task. The covered principles are structured within a methodology to show how evaluation and validation measures/metrics determine the bicluster coherence, ensuring the algorithm effectiveness, and the limitations reported in some selected works. The review did not find any metrics that can be used in a generic way to guarantee the effectiveness of a biclustering algorithm when compared to all others. Therefore, the choice of evaluation metrics must meet to specific objectives of the application. So in this work, we present the measures and metrics in 7 major classes, including metrics based on residues, score thresholding, plaid, and order-preserving constraints, space transforms, correlations, theoretical and probabilistic frames, and set operations.},
  archive      = {J_PR},
  author       = {Marta D.M. Noronha and Rui Henriques and Sara C. Madeira and Luis E. Zárate},
  doi          = {10.1016/j.patcog.2022.108612},
  journal      = {Pattern Recognition},
  pages        = {108612},
  shortjournal = {Pattern Recognition},
  title        = {Impact of metrics on biclustering solution and quality: A review},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved deep convolutional embedded clustering with
re-selectable sample training. <em>PR</em>, <em>127</em>, 108611. (<a
href="https://doi.org/10.1016/j.patcog.2022.108611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep clustering algorithm can learn the latent features of the embedded subspace, and further realize the clustering of samples in the feature space. The existing deep clustering algorithms mostly integrate neural networks and traditional clustering algorithms. However, for sample sets with many noise points, the effect of the clustering remains unsatisfactory. To address this issue, we propose an improved deep convolutional embedded clustering algorithm using reliable samples (IDCEC) in this paper. The algorithm first uses the convolutional autoencoder to extract features and cluster the samples. Then we select reliable samples with pseudo-labels and pass them to the convolutional neural network for training to get a better clustering model. We construct a new loss function for backpropagation training and implement an unsupervised deep clustering method . To verify the performance of the method proposed in this paper, we conducted experimental tests on standard data sets such as MNIST and USPS. Experimental results show that our method has better performance compared to traditional clustering algorithms and the state-of-the-art deep clustering algorithm under four clustering metrics .},
  archive      = {J_PR},
  author       = {Hu Lu and Chao Chen and Hui Wei and Zhongchen Ma and Ke Jiang and Yingquan Wang},
  doi          = {10.1016/j.patcog.2022.108611},
  journal      = {Pattern Recognition},
  pages        = {108611},
  shortjournal = {Pattern Recognition},
  title        = {Improved deep convolutional embedded clustering with re-selectable sample training},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SVC-onGoing: Signature verification competition.
<em>PR</em>, <em>127</em>, 108609. (<a
href="https://doi.org/10.1016/j.patcog.2022.108609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents SVC-onGoing 1 , an on-going competition for on-line signature verification where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases, such as DeepSignDB 2 and SVC2021_EvalDB 3 , and standard experimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021), which has been extended to allow participants anytime. The goal of SVC-onGoing is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC-onGoing prove the high potential of deep learning methods in comparison with traditional methods. In particular, the best signature verification system has obtained Equal Error Rate (EER) values of 3.33\% (Task 1), 7.41\% (Task 2), and 6.04\% (Task 3). Future studies in the field should be oriented to improve the performance of signature verification systems on the challenging mobile scenarios of SVC-onGoing in which several mobile devices and the finger are used during the signature acquisition.},
  archive      = {J_PR},
  author       = {Ruben Tolosana and Ruben Vera-Rodriguez and Carlos Gonzalez-Garcia and Julian Fierrez and Aythami Morales and Javier Ortega-Garcia and Juan Carlos Ruiz-Garcia and Sergio Romero-Tapiador and Santiago Rengifo and Miguel Caruana and Jiajia Jiang and Songxuan Lai and Lianwen Jin and Yecheng Zhu and Javier Galbally and Moises Diaz and Miguel Angel Ferrer and Marta Gomez-Barrero and Ilya Hodashinsky and Konstantin Sarin and Artem Slezkin and Marina Bardamova and Mikhail Svetlakov and Mohammad Saleem and Cintia Lia Szcs and Bence Kovari and Falk Pulsmeyer and Mohamad Wehbi and Dario Zanca and Sumaiya Ahmad and Sarthak Mishra and Suraiya Jabin},
  doi          = {10.1016/j.patcog.2022.108609},
  journal      = {Pattern Recognition},
  pages        = {108609},
  shortjournal = {Pattern Recognition},
  title        = {SVC-onGoing: Signature verification competition},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arbitrarily shaped scene text detection with dynamic
convolution. <em>PR</em>, <em>127</em>, 108608. (<a
href="https://doi.org/10.1016/j.patcog.2022.108608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrarily shaped scene text detection has witnessed great development in recent years, and text detection using segmentation has been proven to an effective approach. However, problems caused by the diverse attributes of text instances, such as shapes, scales, and presentation styles (dense or sparse), persist. In this paper, we propose a novel text detector, termed DText, which can effectively formulate an arbitrarily shaped scene text detection task based on dynamic convolution. Our method can dynamically generate independent text-instance-aware convolutional parameters for each text instance from multi-features thus overcoming some intractable limitations of arbitrary text detection, such as the splitting of similar adjacent text, which poses challenges to fixed instance-shared convolutional parameters-based methods. Unlike standard segmentation methods relying on regions-of-interest bounding boxes , DText focuses on enhancing the flexibility of the network to retain details of instances from diverse resolutions while effectively improving prediction accuracy. Moreover, we propose encoding the shape and position information according to the characteristics of the text instance, termed text-shape sensitive position embedding . Thus, it can provide explicit shape and position information to the generator of the dynamic convolution parameters. Experiments on five benchmarks (Total-Text, SCUT-CTW1500, MSRA-TD500, ICDAR2015, and MLT) showed that our method achieves superior detection performance.},
  archive      = {J_PR},
  author       = {Ying Cai and Yuliang Liu and Chunhua Shen and Lianwen Jin and Yidong Li and Daji Ergu},
  doi          = {10.1016/j.patcog.2022.108608},
  journal      = {Pattern Recognition},
  pages        = {108608},
  shortjournal = {Pattern Recognition},
  title        = {Arbitrarily shaped scene text detection with dynamic convolution},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-separable rotation moment invariants. <em>PR</em>,
<em>127</em>, 108607. (<a
href="https://doi.org/10.1016/j.patcog.2022.108607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce new rotation moment invariants, which are composed of non-separable Appell moments. We prove that Appell polynomials behave under rotation as monomials , which enables easy construction of the invariants. We show by extensive tests that non-separable moments may outperform the separable ones in terms of recognition power and robustness thanks to a better distribution of their zero curves over the image space.},
  archive      = {J_PR},
  author       = {Leonid Bedratyuk and Jan Flusser and Tomáš Suk and Jitka Kostková and Jaroslav Kautsky},
  doi          = {10.1016/j.patcog.2022.108607},
  journal      = {Pattern Recognition},
  pages        = {108607},
  shortjournal = {Pattern Recognition},
  title        = {Non-separable rotation moment invariants},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual explanation of black-box model: Similarity difference
and uniqueness (SIDU) method. <em>PR</em>, <em>127</em>, 108604. (<a
href="https://doi.org/10.1016/j.patcog.2022.108604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of ‘black- box’ models. In this paper, a novel XAI visual explanation algorithm known as the Similarity Difference and Uniqueness (SIDU) method that can effectively localize entire object regions responsible for prediction is presented in full detail. The SIDU algorithm robustness and effectiveness is analyzed through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in the presence of adversarial attack on ’black-box’ models to better understand its performance. Our code is available at: https://github.com/satyamahesh84/SIDU_XAI_CODE .},
  archive      = {J_PR},
  author       = {Satya M. Muddamsetty and Mohammad N.S. Jahromi and Andreea E. Ciontos and Laura M. Fenoy and Thomas B. Moeslund},
  doi          = {10.1016/j.patcog.2022.108604},
  journal      = {Pattern Recognition},
  pages        = {108604},
  shortjournal = {Pattern Recognition},
  title        = {Visual explanation of black-box model: Similarity difference and uniqueness (SIDU) method},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R2CI: Information theoretic-guided feature selection with
multiple correlations. <em>PR</em>, <em>127</em>, 108603. (<a
href="https://doi.org/10.1016/j.patcog.2022.108603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information theoretic-guided feature selection approaches (ITFSs), which exploit the uncertainty of information to measure the correlation of features, aim to select the most informative features. However, most previous approaches suffer from two drawbacks. 1) Complementarity and interaction are not valued, leading to features with potential discriminatory information for learning tasks such as classification not being excavated and affecting the effectiveness of learning. 2) The various correlations that exist between features for the class have not been fully considered, and their differentiation and relationships have not been well reflected. To address the former issue, guided by information theory , the complementarity and interaction between features are studied. For the latter, firstly, some ITFSs are reviewed and analyzed in terms of feature correlation. The analysis reveals that considering feature multi-correlation is absent in the selection process. Motivated by this problem, a feature selection algorithm with class-based relevance, redundancy, complementarity, and interaction (R2CI) is designed for the first time. Moreover, the distinctions and connections among different correlations are also explored. The results of comparisons and hypothesis test against competitive algorithms show that R2CI has significant advantages in most cases.},
  archive      = {J_PR},
  author       = {Jihong Wan and Hongmei Chen and Tianrui Li and Wei Huang and Min Li and Chuan Luo},
  doi          = {10.1016/j.patcog.2022.108603},
  journal      = {Pattern Recognition},
  pages        = {108603},
  shortjournal = {Pattern Recognition},
  title        = {R2CI: Information theoretic-guided feature selection with multiple correlations},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating global and local social networks for group
recommendations. <em>PR</em>, <em>127</em>, 108601. (<a
href="https://doi.org/10.1016/j.patcog.2022.108601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the social nature of human beings, group activities have become an integral part of daily life. This creates the need for an in-depth study of the group-recommendation task: recommending items to a group of users. Unlike individual decision-making, which relies primarily on personal preferences, group decision-making is a process of negotiation and agreement among group members, in which social characteristics are a critical factor in achieving positive recommendation results. Therefore, in this paper, we propose a new model to solve the group recommendation problem from both global and local social networks. In a global network, a user’s social influence spreads through social connections and affects the preferences of others. In a local network, group members may contribute differently to the final decision, forming a dynamic negotiation and consensus process. We propose to model global and local networks with two components: 1) an attentive graph convolutional network based global network diffusion (GND) module to simulate the spread of social influence and capture the social gate of each user, and 2) a multi-channel attention-based local network fusion (LNF) module to learn the complex decision-making process among group members and integrate them into a final representation of the group. Finally, two separate neural collaborative filtering (NCF) modules are presented to model group-item and user-item interactions, respectively, to enhance each other. Extensive experimental results from two real-world datasets show the effectiveness of our proposed model.},
  archive      = {J_PR},
  author       = {Youfang Leng and Li Yu},
  doi          = {10.1016/j.patcog.2022.108601},
  journal      = {Pattern Recognition},
  pages        = {108601},
  shortjournal = {Pattern Recognition},
  title        = {Incorporating global and local social networks for group recommendations},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On computational aspects of high-order dual hahn moments.
<em>PR</em>, <em>127</em>, 108596. (<a
href="https://doi.org/10.1016/j.patcog.2022.108596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present two new algorithms for the fast and stable computation of high-order discrete orthogonal dual Hahn polynomials (DHPs). These algorithms are essentially based on the proposed computation method of the initial values of DHPs following the order n and the variable s . For both algorithms, a single stable value is computed, fully independent of the gamma function that is the source of the numerical overflow, and then the rest of DHPs values are computed recursively via the proposed recurrence scheme. By analyzing the DHPs matrix , we propose a new method, which allows ensuring the numerical stability of high-order DHPs and dual Hahn moments (DHMs) until the last order. This method is based on the use of appropriate stability conditions. The results of simulations and comparisons carried out show on one hand that the second algorithm with the stability condition allows to compute DHPs up to the order n = 17,603 without propagation of numerical error. On the other hand, the performance of analyzing large-size signals and images by high-order DHMs computed by the proposed method significantly exceeds the existing methods in terms of numerical stability, accuracy of reconstruction and in terms of maximum size of the analyzed signals and images. After the acceptance of this paper, the proposed algorithms for high-order DHPs computation will be made publically available at https://github.com/AchrafDaoui/On-Computational-Aspects-of-High-Order-Dual-Hahn-Moments .},
  archive      = {J_PR},
  author       = {Achraf Daoui and Hicham Karmouni and Mohamed Yamni and Mhamed Sayyouri and Hassan Qjidaa},
  doi          = {10.1016/j.patcog.2022.108596},
  journal      = {Pattern Recognition},
  pages        = {108596},
  shortjournal = {Pattern Recognition},
  title        = {On computational aspects of high-order dual hahn moments},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain generalization and adaptation based on second-order
style information. <em>PR</em>, <em>127</em>, 108595. (<a
href="https://doi.org/10.1016/j.patcog.2022.108595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) and unsupervised domain adaptation (UDA) aim to solve the domain-shift problem that arises when the trained model is tested in the domain with different style distribution from the training data. Style Normalization and Restitution(SNR) has solved this problem to a certain extent and achieved the best performance. However, SNR ignores the discriminative information encoded in the appearance style information, which limits the performance of the model. In this paper, we propose Two-level Style Normalization and Restitution(Tl-SNR) to solve this problem. First, we use group whitening to introduce the appearance style information encoded in the second-order statistic into the SNR, which prepares for restituting the task-relevant discriminative information in the appearance information later. Secondly, we defined dynamic affine parameters, which improves the affine parameters in group whitening. It makes the model adjust adaptively according to the characteristics of the sample, so as to better exploit the capabilities of the model. Finally, we designed a Two-level Style Normalization and Restitution module based on the improved group whitening for domain generalization and unsupervised domain adaptation. Extensive experiments show that our method is effective. And our method outperforms state-of-the-art DG and UDA methods on four benchmarks.},
  archive      = {J_PR},
  author       = {Hao Wang and Xiaojun Bi},
  doi          = {10.1016/j.patcog.2022.108595},
  journal      = {Pattern Recognition},
  pages        = {108595},
  shortjournal = {Pattern Recognition},
  title        = {Domain generalization and adaptation based on second-order style information},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Unabridged adjacent modulation for clothing parsing.
<em>PR</em>, <em>127</em>, 108594. (<a
href="https://doi.org/10.1016/j.patcog.2022.108594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clothing parsing has made tremendous progress in the domain of computer vision recently. Most state-of-the-art methods are based on the encoder-decoder architecture. However, the existing methods mainly neglect problems of feature uncalibration within blocks and semantics dilution between blocks. In this work, we propose an unabridged adjacent modulation network (UAM-Net) to aggregate multi-level features for clothing parsing. We first build an unabridged channel attention (UCA) mechanism on feature maps within each block for feature recalibration . We further design a top-down adjacent modulation (TAM) for decoder blocks. By deploying TAM, high-level semantic information and visual contexts can be gradually transferred into lower-level layers without loss. The joint implementation of UCA and TAM ensures that the encoder has an enhanced feature representation ability, and the low-level features of the decoders contain abundant semantic contexts. Quantitative and qualitative experimental results on two challenging benchmarks ( i.e ., colorful fashion parsing and the modified fashion clothing) declare that our proposed UAM-Net can achieve competitive high-accurate performance with the state-of-the-art methods. The source codes are available at: https://github.com/ctzuo/UAM-Net .},
  archive      = {J_PR},
  author       = {Dong Zhang and Chengting Zuo and Qianhao Wu and Liyong Fu and Xinguang Xiang},
  doi          = {10.1016/j.patcog.2022.108594},
  journal      = {Pattern Recognition},
  pages        = {108594},
  shortjournal = {Pattern Recognition},
  title        = {Unabridged adjacent modulation for clothing parsing},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilabel learning based adaptive graph convolutional
network for human parsing. <em>PR</em>, <em>127</em>, 108593. (<a
href="https://doi.org/10.1016/j.patcog.2022.108593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human parsing , graph convolutional networks (GCNs), which naturally model the skeleton of the human body as a fixed graph, have been witnessed to obtain remarkable performance. However, the existing methods perform the fixed graph modeling over all the training samples. This may not be an optimal graph for the diversity of the samples that contain various shapes of human parts, complex body postures, severe occlusions and dense crowd, etc. Focusing on this, we propose a new Multilabel Learning based Adaptive Graph Convolutional Network (ML-AGCN) for human parsing. The ML-AGCN includes three modules: adaptive graph generation module, semantic parts based attention module and label consistency loss. Concretely, to effectively deal with the different sizes and connectivities of the optimal graph for different samples, we first propose an adaptive graph generation module based on multilabel learning that contains graph node adaptation (GNA) and graph connection adaptation (GCA). Then, for a more comprehensive node embedding , we design a semantic parts based attention module to optimally fuse fixed graph embeddings and adaptive graph embeddings. Besides, to further explicitly constraint the consistency between the predicted multilabel and the predicted human parsing results, we propose a label consistency loss that can simultaneously refine the human parsing results and optimize the accuracy of the adaptive graph. Extensive experiments on four challenging datasets, including PASCAL-Person-Part, ATR, LIP and CIHP, well demonstrate the effectiveness of our model, and it outperforms other state-of-the-art methods in human parsing.},
  archive      = {J_PR},
  author       = {Huaqing Hao and Weibin Liu and Weiwei Xing and Shunli Zhang},
  doi          = {10.1016/j.patcog.2022.108593},
  journal      = {Pattern Recognition},
  pages        = {108593},
  shortjournal = {Pattern Recognition},
  title        = {Multilabel learning based adaptive graph convolutional network for human parsing},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian mixture of gaussian processes for data association
problem. <em>PR</em>, <em>127</em>, 108592. (<a
href="https://doi.org/10.1016/j.patcog.2022.108592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.},
  archive      = {J_PR},
  author       = {Younghwan Jeon and Ganguk Hwang},
  doi          = {10.1016/j.patcog.2022.108592},
  journal      = {Pattern Recognition},
  pages        = {108592},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian mixture of gaussian processes for data association problem},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Head pose estimation: An extensive survey on recent
techniques and applications. <em>PR</em>, <em>127</em>, 108591. (<a
href="https://doi.org/10.1016/j.patcog.2022.108591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric based systems are involved in many areas, from surveillance to user authentication , from autonomous systems to human-robot interactions. Head pose estimation (HPE) is the task to support biometric systems in which any of the biometric traits of the head is involved, as face, ear or iris. This particular biometric branch finds its application in driver attention detection, surveillance for recognition, face frontalization, best frame selection and so on. The goal of HPE is to determine the head pose orientation (yaw, pitch, roll). The implemented methods use different techniques depending on the kind of input. In this survey we present an overview of involved datasets, recent techniques and applications. We evaluate and compare the different approaches with respect to their advantages and practical usage. In addition, we propose a technical comparison between training and training-free techniques for the most popular HPE methods.},
  archive      = {J_PR},
  author       = {Andrea F. Abate and Carmen Bisogni and Aniello Castiglione and Michele Nappi},
  doi          = {10.1016/j.patcog.2022.108591},
  journal      = {Pattern Recognition},
  pages        = {108591},
  shortjournal = {Pattern Recognition},
  title        = {Head pose estimation: An extensive survey on recent techniques and applications},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal-adaptive sparse feature aggregation for video
object detection. <em>PR</em>, <em>127</em>, 108587. (<a
href="https://doi.org/10.1016/j.patcog.2022.108587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is a challenging task due to the appearance deterioration in video frames. To enhance feature representation of the deteriorated frames, previous methods usually aggregate features from fixed-density and fixed-length adjacent frames. However, due to the redundancy of videos and irregular object movements over time, temporal information may not be efficiently exploited using the traditional inflexible strategy. Alternatively, we present a temporal-adaptive sparse feature aggregation framework, an accurate and efficient method for video object detection. Instead of adopting a fixed-density and fixed-length window fusion strategy, a temporal-adaptive sparse sampling strategy is proposed using a stride predictor to encode informative frames more efficiently. A collaborative feature aggregation framework, which consists of a pixel-adaptive aggregation module and an object-relational aggregation module, is proposed for feature enhancement. The pixel-adaptive aggregation module enhances pixel-level features on the current frame using corresponding pixel-level features from other frames. Similarly, the object-relational aggregation module further enhances feature representation at proposal level. A graph is constructed to model the relations between different proposals so that the relation features and proposal features are adaptively fused for feature enhancement. Experiments demonstrate that our proposed framework significantly surpasses traditional dense aggregation methods, and comprehensive ablation studies verify the effectiveness of each proposed module in our framework.},
  archive      = {J_PR},
  author       = {Fei He and Qiaozhe Li and Xin Zhao and Kaiqi Huang},
  doi          = {10.1016/j.patcog.2022.108587},
  journal      = {Pattern Recognition},
  pages        = {108587},
  shortjournal = {Pattern Recognition},
  title        = {Temporal-adaptive sparse feature aggregation for video object detection},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nested conformal prediction and quantile out-of-bag ensemble
methods. <em>PR</em>, <em>127</em>, 108496. (<a
href="https://doi.org/10.1016/j.patcog.2021.108496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems , without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets.},
  archive      = {J_PR},
  author       = {Chirag Gupta and Arun K. Kuchibhotla and Aaditya Ramdas},
  doi          = {10.1016/j.patcog.2021.108496},
  journal      = {Pattern Recognition},
  pages        = {108496},
  shortjournal = {Pattern Recognition},
  title        = {Nested conformal prediction and quantile out-of-bag ensemble methods},
  volume       = {127},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep knowledge integration of heterogeneous features for
domain adaptive SAR target recognition. <em>PR</em>, <em>126</em>,
108590. (<a href="https://doi.org/10.1016/j.patcog.2022.108590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to integrate various heterogeneous features for better recognition performance is increasingly critical for automatic target recognition . Existing integration methods present the following drawbacks: (1) most feature integration methods ignore the information, both common and discriminate knowledge, among different types of features; (2) most decision integration methods ignore the fact that different knowledge contributes differently; (3) the feature weights of integration model learned in the source domain cannot perform well in the target domain. To tackle these problems, we propose a deep K nowledge I ntegration framework by combining heterogeneous features for D omain A daptive synthetic aperture radar (SAR) target recognition (KIDA). In the training phase, we implement deep knowledge integration at both feature and decision levels. At the feature level, to exploit the common and discriminative knowledge, multiple heterogeneous features are projected from the feature space into a unified label space by exploring the shared and specific structures simultaneously. The shared structure integrates common information in different features, while the specific structure reserves discriminative information of each type of feature. At the decision level, to reveal the relative importance of different knowledge, a decision integration strategy with feature weights is adopted in the label space. In the online testing phase, to improve the generalization of the model in dynamical environments, we employ online learning with sequential target domain knowledge to update the feature weights, thus achieving domain adaptation . Extensive experiments on different datasets validate the effectiveness and advantages of the proposed KIDA, especially in noisy environments .},
  archive      = {J_PR},
  author       = {Yukun Zhang and Xiansheng Guo and Lin Li and Nirwan Ansari},
  doi          = {10.1016/j.patcog.2022.108590},
  journal      = {Pattern Recognition},
  pages        = {108590},
  shortjournal = {Pattern Recognition},
  title        = {Deep knowledge integration of heterogeneous features for domain adaptive SAR target recognition},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Class-specific discriminative metric learning for scene
recognition. <em>PR</em>, <em>126</em>, 108589. (<a
href="https://doi.org/10.1016/j.patcog.2022.108589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning aims to learn an appropriate distance metric for a given machine learning task. Despite its impressive performance in the field of image recognition, it may still not be discriminative enough for scene recognition because of the high within-class diversity and high between-class similarity of scene images. In this paper, we propose a novel class-specific discriminative metric learning method (CSDML) to alleviate these problems. More specifically, we learn a distinctive linear transformation for each class (or, equivalently, a Mahalanobis distance metric for each class), which allows to project the samples of that class into a corresponding low-dimensional discriminative space. The overall aim is to simultaneously minimize the Euclidean distances between the projections of samples of the same class (or, equivalently, the Mahalanobis distances between these samples) and maximize the Euclidean distances between the projections of samples of different classes. Additionally, we incorporate least squares regression into the optimization problem , rendering class-specific metric learning more flexible and better suited to tackle scene recognition. Experimental results on four benchmark scene datasets demonstrate that the proposed method outperforms most of the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Chen Wang and Guohua Peng and Bernard De Baets},
  doi          = {10.1016/j.patcog.2022.108589},
  journal      = {Pattern Recognition},
  pages        = {108589},
  shortjournal = {Pattern Recognition},
  title        = {Class-specific discriminative metric learning for scene recognition},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust image matching via local graph structure consensus.
<em>PR</em>, <em>126</em>, 108588. (<a
href="https://doi.org/10.1016/j.patcog.2022.108588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching plays a vital role in many computer vision tasks, and this paper focuses on the mismatch removal problem of feature-based matching. We formulate the problem into a general yet effective optimization framework based on graph matching by combining integer quadratic programming with a compensation term for discouraging matches, termed as Local Graph Structure Consensus (LGSC). Considering the local area similarity of those potential true matches, we design a local graph structure for preserving geometric topology, which contains a local indicator vector and a local affinity vector for each correspondence. The local indicator vector is utilized for edge construction, while the local affinity vector represents the match correctness of the nodes and edges between two graphs. In particular, the ranking shift with scale and rotation invariance is exploited to represent the node affinity. Ultimately, we derive a closed-form solution with linearithmic time and linear space complexity. Moreover, a multi-scale and iterative graph construction strategy is proposed to promote the performance of our method in terms of robustness and effectiveness. Extensive experiments on various real image datasets demonstrate that our LGSC can achieve superior performance over current state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Xingyu Jiang and Yifan Xia and Xiao-Ping Zhang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2022.108588},
  journal      = {Pattern Recognition},
  pages        = {108588},
  shortjournal = {Pattern Recognition},
  title        = {Robust image matching via local graph structure consensus},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-seg: A survey of meta-learning for image segmentation.
<em>PR</em>, <em>126</em>, 108586. (<a
href="https://doi.org/10.1016/j.patcog.2022.108586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-performed deep learning model in image segmentation relies on a large number of labeled data. However, it is hard to obtain sufficient high-quality raw data in industrial applications. Meta-learning, one of the most promising research areas, is recognized as a powerful tool for approaching image segmentation. To this end, this paper reviews the state-of-the-art image segmentation methods based on meta-learning. We firstly introduce the background of the image segmentation, including the methods and metrics of image segmentation. Second, we review the timeline of meta-learning and give a more comprehensive definition of meta-learning. The differences between meta-learning and other similar methods are compared comprehensively. Then, we categorize the existing meta-learning methods into model-based, optimization-based, and metric-based. For each categorization, the popular used meta-learning models are discussed in image segmentation. Next, we conduct comprehensive computational experiments to compare these models on two pubic datasets: ISIC-2018 and Covid-19. Finally, the future trends of meta-learning in image segmentation are highlighted.},
  archive      = {J_PR},
  author       = {Shuai Luo and Yujie Li and Pengxiang Gao and Yichuan Wang and Seiichi Serikawa},
  doi          = {10.1016/j.patcog.2022.108586},
  journal      = {Pattern Recognition},
  pages        = {108586},
  shortjournal = {Pattern Recognition},
  title        = {Meta-seg: A survey of meta-learning for image segmentation},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced nuclear norm based matrix regression for occluded
face recognition. <em>PR</em>, <em>126</em>, 108585. (<a
href="https://doi.org/10.1016/j.patcog.2022.108585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective approach for the task of face recognition is proposed in this paper, which formulates the problem as an enhanced nuclear norm based matrix regression model and explores the low-rank property of the reconstructed image. Previous works have already leveraged the nuclear norm to obtain a low-rank representation of the error image and get a promising recognition rate. Motivated by the low-rank property of the reconstructed image through theoretical observation, our model imposes the nuclear norm constraints not only on the representation residual but also on the reconstructed image. The proposed method preserves the 2D structural information of the error images and reconstructs images, which is significant for the face recognition tasks. To further improve the performance of the proposed model, we explore the impact of different regularization terms under various scenarios. Extensive experiments on several benchmark datasets show the efficacy of the proposed model especially in terms of robustness against contiguous occlusion and illumination changes, which achieves superior performance over the most competitive methods.},
  archive      = {J_PR},
  author       = {Qin Li and Huihui He and Hong Lai and Tie Cai and Qianqian Wang and QuanXue Gao},
  doi          = {10.1016/j.patcog.2022.108585},
  journal      = {Pattern Recognition},
  pages        = {108585},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced nuclear norm based matrix regression for occluded face recognition},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MoRE: Multi-output residual embedding for multi-label
classification. <em>PR</em>, <em>126</em>, 108584. (<a
href="https://doi.org/10.1016/j.patcog.2022.108584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification (MLC) is one of the challenging tasks in computer vision , where it confronts high dimensional problem both in output label and input feature spaces. This paper proposed solving MLC through multi-output residual embedding (MoRE), which learns appropriate distance metric by analyzing the residuals between input and output spaces. Unlike traditional MLC paradigms that learn relationships between label space and feature space, our proposed approach further learns a low-rank structure in residuals between input and output spaces. And it encodes such residual projection to achieve dimension reduction in label space, enhancing the performance of the proposed algorithm in processing high dimensional MLC task . Furthermore, considering the label correlations between instances and its neighbors, multiple residuals of instances neighbors are also incorporated into the proposed model to further learn more appropriate distance metric in the same way. Overall, with residual embedding learning from instances and their neighbors, the obtained metric can learn a more appropriate low-rank structure in label space to handle high dimensional problem in MLC. Experimental results on several data sets, such as Cal500, Corel5k, Bibtex, Delicious, Tmc2007, 20ng, Mirflickr and Rcv1s1, demonstrate the excellent predictive performance of MoRE among STOA methods, such as LMMO-kNN, M3MDC, KRAM, SEEM, CPLST, CSSP , FaIE.},
  archive      = {J_PR},
  author       = {Siyu Liu and Xuehua Song and Zhongchen Ma and Ernest Domanaanmwi Ganaa and XiangJun Shen},
  doi          = {10.1016/j.patcog.2022.108584},
  journal      = {Pattern Recognition},
  pages        = {108584},
  shortjournal = {Pattern Recognition},
  title        = {MoRE: Multi-output residual embedding for multi-label classification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bag dissimilarity regularized multi-instance learning.
<em>PR</em>, <em>126</em>, 108583. (<a
href="https://doi.org/10.1016/j.patcog.2022.108583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance learning (MIL) is able to cope with the weakly supervised problems where the training data is represented by labeled bags consisting of multiple unlabeled instances. Due to its practical significance, MIL has recently drawn increasing attention. Introducing bag representations is an attractive way to learn MIL data. However, it is difficult for the existing MIL methods to utilize both implicit and explicit bag representations simultaneously. In this paper, we propose a bag dissimilarity regularized (BDR) framework that incorporates multiple bag representations regardless of explicitness or implicitness. Here, the implicit bag representations are incorporated into a regularization term that contains the intrinsic geometric information provided by the bag dissimilarities. The regularization term can be added to the objective function of supervised classifiers. An effective method for explicit bag embedding is also proposed, which exploits the Fisher score derived from factor analysis. Finally, we propose two specific BDR methods based on support vector machine and broad learning system. The proposed BDR methods are evaluated on 14 datasets, and have achieved competitive results with limited computation consumption. We also discuss the effectiveness and the characteristics of BDR framework.},
  archive      = {J_PR},
  author       = {Shiluo Huang and Zheng Liu and Wei Jin and Ying Mu},
  doi          = {10.1016/j.patcog.2022.108583},
  journal      = {Pattern Recognition},
  pages        = {108583},
  shortjournal = {Pattern Recognition},
  title        = {Bag dissimilarity regularized multi-instance learning},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Class-attribute inconsistency learning for novelty
detection. <em>PR</em>, <em>126</em>, 108582. (<a
href="https://doi.org/10.1016/j.patcog.2022.108582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of novelty detection whose goal is to recognize instances from unseen classes during testing. Our key idea is to leverage the inconsistency between class similarity and (latent) attribute similarity. We are motivated by the observation that a novel class may holistically appear like a certain known class (class-level reference) but often exhibits unique properties similar to others (attribute-level references). That is, the related class- and attribute-level references are often inconsistent for a novel class. A new two-stage Class-Attribute Inconsistency Learning network (CAILNet) is proposed to explore class-attribute inconsistency for novelty detection. Stage one aims to learn both class and attribute features based on the class labels and fake attribute labels, and stage two aims to search for the corresponding references and make fine-grained comparisons for final novelty decision. Empirically we conduct comprehensive experiments on three benchmark datasets, and demonstrate state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Shuaiyuan Du and Chaoyi Hong and Yinpeng Chen and Zhiguo Cao and Ziming Zhang},
  doi          = {10.1016/j.patcog.2022.108582},
  journal      = {Pattern Recognition},
  pages        = {108582},
  shortjournal = {Pattern Recognition},
  title        = {Class-attribute inconsistency learning for novelty detection},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure-aware conditional variational auto-encoder for
constrained molecule optimization. <em>PR</em>, <em>126</em>, 108581.
(<a href="https://doi.org/10.1016/j.patcog.2022.108581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of molecule optimization is to optimize molecular properties by modifying molecule structures. Conditional generative models provide a promising way to transfer the input molecules to the ones with better property. However, molecular properties are highly sensitive to small changes in molecular structures. This leads to an interesting thought that we can improve the property of molecules with limited modification in structure. In this paper, we propose a structure-aware conditional Variational Auto-Encoder, namely SCVAE, which exploits the topology of molecules as structure condition and optimizes the molecular properties with constrained structural modification. SCVAE leverages graph alignment of two-level molecule structures in an unsupervised manner to bind the structure conditions between two molecules. Then, this structure condition facilitates the molecule optimization with limited structural modification, namely, constrained molecule optimization, under a novel variational auto-encoder framework. Extensive experimental evaluations demonstrate that structure-aware CVAE generates new molecules with high similarity to the original ones and better molecular properties.},
  archive      = {J_PR},
  author       = {Junchi Yu and Tingyang Xu and Yu Rong and Junzhou Huang and Ran He},
  doi          = {10.1016/j.patcog.2022.108581},
  journal      = {Pattern Recognition},
  pages        = {108581},
  shortjournal = {Pattern Recognition},
  title        = {Structure-aware conditional variational auto-encoder for constrained molecule optimization},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep face recognition for dim images. <em>PR</em>,
<em>126</em>, 108580. (<a
href="https://doi.org/10.1016/j.patcog.2022.108580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of many state-of-the-art deep face recognition models deteriorates significantly for images captured under low illumination, mainly because the features of dim probe face images cannot match well with those of normal-illumination gallery images. The issue cannot be satisfactorily addressed by enhancing the illumination of face images and performing face recognition on the resulted images alone. We propose a novel deep face recognition framework that consists of a feature restoration network, a feature extraction network , and an embedding matching module. The feature restoration network adopts a two-branch structure based on the convolutional neural network to generate a feature image from the raw image and the illumination-enhanced image. The feature extraction network encodes the feature image into an embedding, which is then used by the embedding matching module for face verification and identification. The overall verification accuracy is improved from 1.1\% to 6.7\% when tested on the Specs on Faces (SoF) dataset. For face identification, the rank-1 identification accuracy is improved by 2.8\%.},
  archive      = {J_PR},
  author       = {Yu-Hsuan Huang and Homer H. Chen},
  doi          = {10.1016/j.patcog.2022.108580},
  journal      = {Pattern Recognition},
  pages        = {108580},
  shortjournal = {Pattern Recognition},
  title        = {Deep face recognition for dim images},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Low-resolution human pose estimation. <em>PR</em>,
<em>126</em>, 108579. (<a
href="https://doi.org/10.1016/j.patcog.2022.108579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation has achieved significant progress on images with high imaging resolution. However, low-resolution imagery data bring nontrivial challenges which are still under-studied. To fill this gap, we start with investigating existing methods and reveal that the most dominant heatmap-based methods would suffer more severe model performance degradation from low-resolution, and offset learning is an effective strategy. Established on this observation, in this work we propose a novel Confidence-Aware Learning (CAL) method which further addresses two fundamental limitations of existing offset learning methods: inconsistent training and testing, decoupled heatmap and offset learning. Specifically, CAL selectively weighs the learning of heatmap and offset with respect to ground-truth and most confident prediction, whilst capturing the statistical importance of model output in mini-batch learning manner. Extensive experiments conducted on the COCO benchmark show that our method outperforms significantly the state-of-the-art methods for low-resolution human pose estimation.},
  archive      = {J_PR},
  author       = {Chen Wang and Feng Zhang and Xiatian Zhu and Shuzhi Sam Ge},
  doi          = {10.1016/j.patcog.2022.108579},
  journal      = {Pattern Recognition},
  pages        = {108579},
  shortjournal = {Pattern Recognition},
  title        = {Low-resolution human pose estimation},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decomposing generation networks with structure prediction
for recipe generation. <em>PR</em>, <em>126</em>, 108578. (<a
href="https://doi.org/10.1016/j.patcog.2022.108578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model, which improves the performance over the state-of-the-art results.},
  archive      = {J_PR},
  author       = {Hao Wang and Guosheng Lin and Steven C.H. Hoi and Chunyan Miao},
  doi          = {10.1016/j.patcog.2022.108578},
  journal      = {Pattern Recognition},
  pages        = {108578},
  shortjournal = {Pattern Recognition},
  title        = {Decomposing generation networks with structure prediction for recipe generation},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video super-resolution via mixed spatial-temporal
convolution and selective fusion. <em>PR</em>, <em>126</em>, 108577. (<a
href="https://doi.org/10.1016/j.patcog.2022.108577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution aims to recover the high-resolution (HR) contents from the low-resolution (LR) observations relying on compositing the spatial-temporal information in the LR frames. It is crucial to model the spatial-temporal information jointly since the video sequences are three-dimensional spatial-temporal signals. Compared with explicitly estimating motions between the 2D frames, 3D convolutional neural networks (CNNs) have been shown its efficiency and effectiveness for video super-resolution (SR), as a natural way of spatial-temporal data modelling . Though promising, the performance of 3D CNNs is still far from satisfactory. The high computational and memory requirements limit the development of more advanced designs to extract and fuse the information from a larger spatial and temporal scale. We thus propose a Mixed Spatial-Temporal Convolution (MSTC) block that simultaneously extracts the spatial information and the supplemented temporal dependency among frames by jointly applying 2D and 3D convolution. To further fuse the learned features corresponding to different frames, we propose a novel similarity-based selective features strategy, unlike precious methods directly stacking the learned features. Additionally, an attention-based motion compensation module is applied to alleviate the influence of misalignment between frames. Experiments on three widely used benchmark datasets and real-world dataset show that, relying on superior feature extraction and fusion ability, the proposed network can outperform previous state-of-the-art methods, especially for recovering the confusing details.},
  archive      = {J_PR},
  author       = {Wei Sun and Dong Gong and Javen Qinfeng Shi and Anton van den Hengel and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.108577},
  journal      = {Pattern Recognition},
  pages        = {108577},
  shortjournal = {Pattern Recognition},
  title        = {Video super-resolution via mixed spatial-temporal convolution and selective fusion},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An attention-enhanced cross-task network to analyse lung
nodule attributes in CT images. <em>PR</em>, <em>126</em>, 108576. (<a
href="https://doi.org/10.1016/j.patcog.2022.108576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate characterization of visual attributes such as spiculation, lobulation, and calcification of lung nodules in computed tomography (CT) images is critical in cancer management. The characterization of these attributes is often subjective, which may lead to high inter- and intra-observer variability. Furthermore, lung nodules are often heterogeneous in the cross-sectional image slices of a 3D volume. Current state-of-the-art methods that score multiple attributes rely on deep learning-based multi-task learning (MTL) schemes. These methods, however, extract shared visual features across attributes and then examine each attribute without explicitly leveraging their inherent intercorrelations. Furthermore, current methods treat each slice with equal importance without considering their relevance or heterogeneity, which limits performance. In this study, we address these challenges with a new convolutional neural network (CNN)-based MTL model that incorporates multiple attention-based learning modules to simultaneously score 9 visual attributes of lung nodules in CT image volumes. Our model processes entire nodule volumes of arbitrary depth and uses a slice attention module to filter out irrelevant slices. We also introduce cross-attribute and attribute specialization attention modules that learn an optimal amalgamation of meaningful representations to leverage relationships between attributes. We demonstrate that our model outperforms previous state-of-the-art methods at scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary nodules from over 1,000 patients. Our model also performs competitively when repurposed for benign-malignant classification. Our attention modules provide easy-to-interpret weights that offer insights into the predictions of the model.},
  archive      = {J_PR},
  author       = {Xiaohang Fu and Lei Bi and Ashnil Kumar and Michael Fulham and Jinman Kim},
  doi          = {10.1016/j.patcog.2022.108576},
  journal      = {Pattern Recognition},
  pages        = {108576},
  shortjournal = {Pattern Recognition},
  title        = {An attention-enhanced cross-task network to analyse lung nodule attributes in CT images},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transferring discriminative knowledge via connective
momentum clustering on person re-identification. <em>PR</em>,
<em>126</em>, 108569. (<a
href="https://doi.org/10.1016/j.patcog.2022.108569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation in person re-identification remains a challenge to learning discriminative representations due to the absence of labels in target domain. Clustering could provide pseudo-labels, but the limitation mainly comes from imperfect clustering and noisy pseudo-labels. To address this drawback, we propose C onnective M omentum C lustering (CMC) framework to build a connection estimator via graph convolutional networks to transfer rich connection knowledge from the annotation space of source data to target domain. It estimates connections from context to reveal relationship between unlabeled data and helps to discover more reliable clusters. With momentum mechanism, stable pseudo-labels are updated iteratively with confidence and refined consistently to encourage more discriminative networks. Meanwhile, we notice that the huge domain gap between source and target domains results in severe pollution in BatchNorm layers. To tackle this problem, we normalize the data stream separately to decouple different distribution and further boost the performance in target domain. We adopt our CMC framework on mainstream tasks and achieves 80.2\% mAP / 91.3\% Rank-1 on Duke → → Market task and 70.4\% mAP / 82.4\% Rank-1 on Market → → Duke task.},
  archive      = {J_PR},
  author       = {Yichen Lu and Weihong Deng},
  doi          = {10.1016/j.patcog.2022.108569},
  journal      = {Pattern Recognition},
  pages        = {108569},
  shortjournal = {Pattern Recognition},
  title        = {Transferring discriminative knowledge via connective momentum clustering on person re-identification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised person re-identification via simultaneous
clustering and mask prediction. <em>PR</em>, <em>126</em>, 108568. (<a
href="https://doi.org/10.1016/j.patcog.2022.108568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting meaningful representation is a key challenge for person re-identification (re-ID) task, especially in the absence of ground truth labels. However, existing unsupervised approaches simply utilize pseudo labels generated from clustering to supervise re-ID model and thus have not yet fully explored the semantic information existing in data itself. This also limits the representation capabilities of learned models. To address the above problem, we propose mask prediction (MaskPre) as a pretext task for unsupervised re-ID, such that the clustering network can capture more semantic information and separate the images into semantic clusters automatically. Specifically, MaskPre masks region-level features with dynamic dropblock layer to generate differently masked views of a single image. To predict the masked regions and bridge the domain gap across views, we design mask prediction head and moving-average model to learn visual consistency from still image and temporal consistency during training process. Meanwhile, we optimize the model by grouping the two masked views into the same cluster, thus enhancing the consistency across views. Experimental results on three public benchmark datasets show that our proposed method outperforms the existing state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Junhui Yin and Siqing Zhang and Jiyang Xie and Zhanyu Ma and Jun Guo},
  doi          = {10.1016/j.patcog.2022.108568},
  journal      = {Pattern Recognition},
  pages        = {108568},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised person re-identification via simultaneous clustering and mask prediction},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Deep attention aware feature learning for person
re-identification. <em>PR</em>, <em>126</em>, 108567. (<a
href="https://doi.org/10.1016/j.patcog.2022.108567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification, however, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the ability of predicting attention maps as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attention maps have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) is proposed to make the feature maps obtained by backbone could focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) is proposed to make the extracted features can be decoupled into several groups that are separately responsible for different body parts, thus increasing the robustness to pose variation and partial occlusion . These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet [1] and Bag of Tricks [2]) and observed significant performance improvement on five widely used datasets.},
  archive      = {J_PR},
  author       = {Yifan Chen and Han Wang and Xiaolu Sun and Bin Fan and Chu Tang and Hui Zeng},
  doi          = {10.1016/j.patcog.2022.108567},
  journal      = {Pattern Recognition},
  pages        = {108567},
  shortjournal = {Pattern Recognition},
  title        = {Deep attention aware feature learning for person re-identification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASMFS: Adaptive-similarity-based multi-modality feature
selection for classification of alzheimer’s disease. <em>PR</em>,
<em>126</em>, 108566. (<a
href="https://doi.org/10.1016/j.patcog.2022.108566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal classification methods using different modalities have great advantages over traditional single-modality-based ones for the diagnosis of Alzheimer&#39;s disease (AD) and its prodromal stage mild cognitive impairment (MCI). With the increasing amount of high-dimensional heterogeneous data to be processed, multi-modality feature selection has become a crucial research direction for AD classification. However, traditional methods usually depict the data structure using pre-defined similarity matrix as a priori, which is difficult to precisely measure the intrinsic relationship across different modalities in high-dimensional space. In this paper, we propose a novel multimodal feature selection method called Adaptive-Similarity-based Multi-modality Feature Selection (ASMFS) which performs adaptive similarity learning and feature selection simultaneously. Specifically, a similarity matrix is learned by jointly considering different modalities and at the same time, an efficient feature selection is conducted by imposing group sparsity-inducing l 2 , 1 l2,1 -norm constraint. Evaluated on the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database with baseline MRI and FDG-PET imaging data collected from 51 AD, 43 MCI converters (MCI-C), 56 MCI non-converters (MCI-NC) and 52 normal controls (NC), we demonstrate the effectiveness and superiority of our proposed method against other state-of-the-art approaches for multi-modality classification of AD/MCI.},
  archive      = {J_PR},
  author       = {Yuang Shi and Chen Zu and Mei Hong and Luping Zhou and Lei Wang and Xi Wu and Jiliu Zhou and Daoqiang Zhang and Yan Wang},
  doi          = {10.1016/j.patcog.2022.108566},
  journal      = {Pattern Recognition},
  pages        = {108566},
  shortjournal = {Pattern Recognition},
  title        = {ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of alzheimer&#39;s disease},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Split, embed and merge: An accurate table structure
recognizer. <em>PR</em>, <em>126</em>, 108565. (<a
href="https://doi.org/10.1016/j.patcog.2022.108565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table structure recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row/column separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and text modalities. Moreover, we achieve a higher precision in our experiments through providing additional textual features. Finally, we process the merging of these basic table grids in a self-regression manner. The corresponding merging results are learned through the attention mechanism . In our experiments, SEM achieves an average F1-Measure of 97.11\% 97.11\% on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing . Extensive experiments on other publicly available datasets further demonstrate the effectiveness of our proposed approach.},
  archive      = {J_PR},
  author       = {Zhenrong Zhang and Jianshu Zhang and Jun Du and Fengren Wang},
  doi          = {10.1016/j.patcog.2022.108565},
  journal      = {Pattern Recognition},
  pages        = {108565},
  shortjournal = {Pattern Recognition},
  title        = {Split, embed and merge: An accurate table structure recognizer},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric imbalanced deep learning with feature scaling and
boundary sample mining. <em>PR</em>, <em>126</em>, 108564. (<a
href="https://doi.org/10.1016/j.patcog.2022.108564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imbalance is a significant factor affecting classification performance in computer vision . In particular, data imbalance is harmful to classification learning and representation learning . To address this issue, this paper proposes a geometric deep learning framework combined with Feature Scaling Module (FSM) and Boundary Samples Mining Module (BSMM). Considering the geometric information in sample distributions of training samples, FSM is proposed to scale the features by hypersphere radius of each class, which improves the representation ability of minority classes. Meanwhile, it is noteworthy that the relationships and information between samples are essential for classification. Therefore, BSMM is proposed to mine the boundary samples by Gabriel Graph that takes the relationships into account. Finally, a loss scheduler is designed to adjust the training process of these two modules. With the scheduler, the model first learns representation and then focuses more on minority classes gradually. Extensive experiments on three benchmark datasets demonstrate the advantages of the proposed learning framework over the state-of-the-art models for solving the imbalance problem.},
  archive      = {J_PR},
  author       = {Zhe Wang and Qida Dong and Wei Guo and Dongdong Li and Jing Zhang and Wenli Du},
  doi          = {10.1016/j.patcog.2022.108564},
  journal      = {Pattern Recognition},
  pages        = {108564},
  shortjournal = {Pattern Recognition},
  title        = {Geometric imbalanced deep learning with feature scaling and boundary sample mining},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GAN for vision, KG for relation: A two-stage network for
zero-shot action recognition. <em>PR</em>, <em>126</em>, 108563. (<a
href="https://doi.org/10.1016/j.patcog.2022.108563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot action recognition can recognize samples of unseen classes that are unavailable in training by exploring common latent semantic representation in samples. However, most methods neglected the connotative relation and extensional relation between the action classes, which leads to the poor generalization ability of the zero-shot learning. Furthermore, the learned classifier inclines to predict the samples of seen class, which leads to poor classification performance. To solve the above problems, we propose a two-stage deep neural network for zero-shot action recognition, which consists of a feature generation sub-network serving as the sampling stage and a graph attention sub-network serving as the classification stage. In the sampling stage, we utilize generative adversarial networks (GAN) trained by action features and word vectors of seen classes to synthesize the action features of unseen classes, which can balance the training sample data of seen classes and unseen classes. In the classification stage, we construct a knowledge graph (KG) based on the relationship between word vectors of action classes and related objects, and propose a graph convolution network (GCN) based on attention mechanism , which dynamically updates the relationship between action classes and objects, and enhances the generalization ability of zero-shot learning. In both stages, we all use word vectors as bridges for feature generation and classifier generalization from seen classes to unseen classes. We compare our method with state-of-the-art methods on UCF101 and HMDB51 datasets. Experimental results show that our proposed method improves the classification performance of the trained classifier and achieves higher accuracy.},
  archive      = {J_PR},
  author       = {Bin Sun and Dehui Kong and Shaofan Wang and Jinghua Li and Baocai Yin and Xiaonan Luo},
  doi          = {10.1016/j.patcog.2022.108563},
  journal      = {Pattern Recognition},
  pages        = {108563},
  shortjournal = {Pattern Recognition},
  title        = {GAN for vision, KG for relation: A two-stage network for zero-shot action recognition},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Super-encoder with cooperative autoencoder networks.
<em>PR</em>, <em>126</em>, 108562. (<a
href="https://doi.org/10.1016/j.patcog.2022.108562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information . With the emergence of deep learning , an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.},
  archive      = {J_PR},
  author       = {Imam Mustafa Kamal and Hyerim Bae},
  doi          = {10.1016/j.patcog.2022.108562},
  journal      = {Pattern Recognition},
  pages        = {108562},
  shortjournal = {Pattern Recognition},
  title        = {Super-encoder with cooperative autoencoder networks},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on conformal and probabilistic prediction with
applications: preface. <em>PR</em>, <em>126</em>, 108561. (<a
href="https://doi.org/10.1016/j.patcog.2022.108561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Alexander Gammerman and Vladimir Vovk and Marco Cristani},
  doi          = {10.1016/j.patcog.2022.108561},
  journal      = {Pattern Recognition},
  pages        = {108561},
  shortjournal = {Pattern Recognition},
  title        = {Special issue on conformal and probabilistic prediction with applications: Preface},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A globally convergent approximate newton method for
non-convex sparse learning. <em>PR</em>, <em>126</em>, 108560. (<a
href="https://doi.org/10.1016/j.patcog.2022.108560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Newton-type greedy pursuit methods have been shown to work favorably for cardinality-constrained sparse learning problems. The appealing sparsity recovery performance of the existing Newton-type greedy pursuit methods, however, is typically guaranteed within a local neighborhood around the target solution. To address this limitation, we present in this paper a novel approximate Newton pursuit method for sparse learning with linear models. The computation procedure of our method iterates between constructing an inexact Newton-type quadratic majorization to the global empirical risk and solving the quadratic approximation via iterative hard thresholding . Provable global guarantees on mean squared prediction error, which is less understood for prior methods, are provided for our method. Numerical evidence is provided to show the advantages of our approach over the prior methods.},
  archive      = {J_PR},
  author       = {Fanfan Ji and Hui Shuai and Xiao-Tong Yuan},
  doi          = {10.1016/j.patcog.2022.108560},
  journal      = {Pattern Recognition},
  pages        = {108560},
  shortjournal = {Pattern Recognition},
  title        = {A globally convergent approximate newton method for non-convex sparse learning},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy minimization for image focus volume in shape from
focus. <em>PR</em>, <em>126</em>, 108559. (<a
href="https://doi.org/10.1016/j.patcog.2022.108559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In shape from focus (SFF) methods, the quality of depth map is mainly dependent on the accuracy level of image focus volume. Most of the SFF techniques optimize focus volume without incorporating any prior or additional structural information about the scene and thus resultant depth maps are deteriorated. We mitigate this deficiency by proposing to optimize focus volume through energy minimization . The proposed energy function contains smoothness and structural similarity along with data term. Smoothness constraint enforces spatial coherence while structural similarity constraint tries to preserve structures which are consistent with image sequence. This results in an optimized focus volume that imitates the underlying scene accurately. For the implementation of our 3D objective function, we employ an efficient technique that decomposes the problem into a sequence of 1D simple sub-problems. Experiments conducted on synthetic and real image sequences from a variety of datasets demonstrate that the proposed method optimizes the focus volume effectively and thus provides improved depth maps.},
  archive      = {J_PR},
  author       = {Usman Ali and Muhammad Tariq Mahmood},
  doi          = {10.1016/j.patcog.2022.108559},
  journal      = {Pattern Recognition},
  pages        = {108559},
  shortjournal = {Pattern Recognition},
  title        = {Energy minimization for image focus volume in shape from focus},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical receptive network oriented to target
recognition in SAR images. <em>PR</em>, <em>126</em>, 108558. (<a
href="https://doi.org/10.1016/j.patcog.2022.108558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent years have witnessed a resurgence on neural network . Many functional layers are stacked hierarchically to learn the high-level representations. Yet the large album of radar image with label information are scarce. The fitting power of deep architectures are therefore limited. Additionally, the coherent imaging mechanism inevitably produce many speckles. They are with the statistical specificity of multiplicative noise , and hence make the image interpretation difficult. To solve the problems, this paper presents a new hierarchical receptive neural network . A signal-wise receptive module is first built by a family of delicate convolutional filters , with which the empirical features and knowledge are encoded. The receptive features are further refined in a patch-wise receptive unit, where some convolutional blocks are configured sequentially. The refined representations are finally used to make the inference. Multiple comparative studies are performed to demonstrate the advantage of proposed strategy.},
  archive      = {J_PR},
  author       = {Ganggang Dong and Hongwei Liu},
  doi          = {10.1016/j.patcog.2022.108558},
  journal      = {Pattern Recognition},
  pages        = {108558},
  shortjournal = {Pattern Recognition},
  title        = {A hierarchical receptive network oriented to target recognition in SAR images},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction with expert advice for a finite number of
experts: A practical introduction. <em>PR</em>, <em>126</em>, 108557.
(<a href="https://doi.org/10.1016/j.patcog.2022.108557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, prediction with expert advice is surveyed focusing on Vovk’s Aggregating Algorithm. The established theory as well as extensions developed in the recent decade are considered. The paper is aimed at practitioners and covers important application scenarios.},
  archive      = {J_PR},
  author       = {Yuri Kalnishkan},
  doi          = {10.1016/j.patcog.2022.108557},
  journal      = {Pattern Recognition},
  pages        = {108557},
  shortjournal = {Pattern Recognition},
  title        = {Prediction with expert advice for a finite number of experts: A practical introduction},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multi-scale synergic discriminative features for
prostate image segmentation. <em>PR</em>, <em>126</em>, 108556. (<a
href="https://doi.org/10.1016/j.patcog.2022.108556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep convolutional neural networks (DCNNs) have been proposed for prostate MR image segmentation , the effectiveness of these methods is often limited by inadequate semantic discrimination and spatial context modeling. To address these issues, we propose a Multi-scale Synergic Discriminative Network (MSD-Net), which includes a shared encoder, a segmentation decoder, and a boundary detection decoder. We further design the cascaded pyramid convolutional block and residual refinement block, and incorporate them and the channel attention block into MSD-Net to exploit the multi-scale spatial contextual information and semantically consistent features of the gland. We also fuse the features from two decoders to boost the segmentation performance , and introduce the synergic multi-task loss to impose the consistence constraint on the joint segmentation and boundary detection. We evaluated MSD-Net against several prostate segmentation methods on three public datasets and achieved an improved accuracy. Our results indicate that the proposed MSD-Net outperforms existing methods with setting the new state-of-the-art for prostate segmentation in magnetic resonance images.},
  archive      = {J_PR},
  author       = {Haozhe Jia and Weidong Cai and Heng Huang and Yong Xia},
  doi          = {10.1016/j.patcog.2022.108556},
  journal      = {Pattern Recognition},
  pages        = {108556},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-scale synergic discriminative features for prostate image segmentation},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical electricity time series prediction with cluster
analysis and sparse penalty. <em>PR</em>, <em>126</em>, 108555. (<a
href="https://doi.org/10.1016/j.patcog.2022.108555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In big data applications , hierarchical time series prediction is an important element of decision-making and concerns the inherent aggregation consistency, which is maintained by reconciliation methods. The paper proposes a novel multiple alternative clustering time series analysis based hierarchical electricity time series prediction method. Instead of adhering the aggregation consistency passively, we first exploit time series mining to construct a hierarchy, and then apply an optimal reconciliation method to improve the prediction accuracy. In particular, k k -means clustering method is employed to cluster time series for many times with different k k so as to make a large number of time series clusters (patterns), and then the clusters (patterns) based hierarchies are constructed respectively. With the large number of clusters hierarchies and the original geographical hierarchy, an optimal aggregation consistency reconciliation based prediction approach is proposed. Furthermore, the sparse penalty is adapted in our method for “ideal” clusters selection to improve the prediction performance. Compared with the state-of-the-art methods on real-life datasets, our method achieves the improvement of 11.13\%\% and 24.07\%\% accurate one-step ahead forecasts on electricity load and solar power data respectively.},
  archive      = {J_PR},
  author       = {Yue Pang and Xiangdong Zhou and Junqi Zhang and Quan Sun and Jianbin Zheng},
  doi          = {10.1016/j.patcog.2022.108555},
  journal      = {Pattern Recognition},
  pages        = {108555},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical electricity time series prediction with cluster analysis and sparse penalty},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rule extraction with guarantees from regression models.
<em>PR</em>, <em>126</em>, 108554. (<a
href="https://doi.org/10.1016/j.patcog.2022.108554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tools for understanding and explaining complex predictive models are critical for user acceptance and trust. One such tool is rule extraction, i.e., approximating opaque models with less powerful but interpretable models. Pedagogical (or black-box) rule extraction, where the interpretable model is induced using the original training instances, but with the predictions from the opaque model as targets, has many advantages compared to the decompositional (white-box) approach. Most importantly, pedagogical methods are agnostic to the kind of opaque model used, and any learning algorithm producing interpretable models can be employed for the learning step. The pedagogical approach has, however, one main problem, clearly limiting its utility. Specifically, while the extracted models are trained to mimic the opaque, there are absolutely no guarantees that this will transfer to novel data. This potentially low test set fidelity must be considered a severe drawback, in particular when the extracted models are used for explanation and analysis. In this paper, a novel approach, solving the problem with test set fidelity by utilizing the conformal prediction framework, is suggested for extracting interpretable regression models from opaque models. The extracted models are standard regression trees, but augmented with valid prediction intervals in the leaves. Depending on the exact setup, the use of conformal prediction guarantees that either the test set fidelity or the test set accuracy will be equal to a preset confidence level, in the long run. In the extensive empirical investigation, using 20 publicly available data sets, the validity of the extracted models is demonstrated. In addition, it is shown how normalization can be used to provide individualized prediction intervals, thus providing highly informative extracted models.},
  archive      = {J_PR},
  author       = {Ulf Johansson and Cecilia Sönströd and Tuwe Löfström and Henrik Boström},
  doi          = {10.1016/j.patcog.2022.108554},
  journal      = {Pattern Recognition},
  pages        = {108554},
  shortjournal = {Pattern Recognition},
  title        = {Rule extraction with guarantees from regression models},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast data reduction by space partitioning via convex hull
and MBR computation. <em>PR</em>, <em>126</em>, 108553. (<a
href="https://doi.org/10.1016/j.patcog.2022.108553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large volumes of training data introduce high computational cost in instance-based classification. Data reduction algorithms select or generate a small (condensing) set of representative training prototypes from the available training data. The Reduction by Space Partitioning algorithm is one of the most well-known prototype generation algorithms that repetitively divides the original training data into subsets. This partitioning process needs to identify the diameter of each subset, i.e., its two farthest instances. This is a costly process since it requires the calculation of all distances between the instances in each subset. The paper introduces two new very fast variations that, instead of computing the actual diameter of a subset, choose a pair of distant-enough instances. The first variation uses instances belonging to an exact 3d convex hull of the subset, while the second one uses instances belonging to the minimum bounding rectangle of the subset. Our experimental study shows that the new variations vastly outperform the original algorithm without a penalty in classification accuracy and reduction rate.},
  archive      = {J_PR},
  author       = {Thomas Giorginis and Stefanos Ougiaroglou and Georgios Evangelidis and Dimitris A. Dervos},
  doi          = {10.1016/j.patcog.2022.108553},
  journal      = {Pattern Recognition},
  pages        = {108553},
  shortjournal = {Pattern Recognition},
  title        = {Fast data reduction by space partitioning via convex hull and MBR computation},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSCNet: Contextual semantic consistency network for
trajectory prediction in crowded spaces. <em>PR</em>, <em>126</em>,
108552. (<a href="https://doi.org/10.1016/j.patcog.2022.108552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction aims to predict the movement trend of the agents like pedestrians, bikers, vehicles. It is helpful to analyze and understand human activities in crowded spaces and widely applied in many areas such as surveillance video analysis and autonomous driving systems. Thanks to the success of deep learning , trajectory prediction has made significant progress. The current methods are dedicated to studying the agents’ future trajectories under the social interaction and the sceneries’ physical constraints. Moreover, how to deal with these factors still catches researchers’ attention. However, they ignore the Semantic Shift Phenomenon when modeling these interactions in various prediction sceneries. There exist several kinds of semantic deviations inner or between social and physical interactions, which we call the “ Gap ”. In this paper, we propose a C ontextual S emantic C onsistency Net work ( CSCNet ) to predict agents’ future activities with powerful and efficient context constraints. We utilize a well-designed context-aware transfer to obtain the intermediate representations from the scene images and trajectories. Then we eliminate the differences between social and physical interactions by aligning activity semantics and scene semantics to cross the Gap. Experiments demonstrate that CSCNet performs better than most of the current methods quantitatively and qualitatively.},
  archive      = {J_PR},
  author       = {Beihao Xia and Conghao Wong and Qinmu Peng and Wei Yuan and Xinge You},
  doi          = {10.1016/j.patcog.2022.108552},
  journal      = {Pattern Recognition},
  pages        = {108552},
  shortjournal = {Pattern Recognition},
  title        = {CSCNet: Contextual semantic consistency network for trajectory prediction in crowded spaces},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Split “n” merge net: A dynamic masking network for
multi-task attention. <em>PR</em>, <em>126</em>, 108551. (<a
href="https://doi.org/10.1016/j.patcog.2022.108551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel Multi-Task Learning (MTL) framework, Split ‘n’ Merge Net. We draw the inspiration from the multi-head attention formulation of Transformers and propose a novel, simple and interpretable pathway to process information captured and exploited by multiple tasks. In particular, we propose a novel splitting network design, which is empowered with multi-head attention, and generates dynamic masks to filter task specific information and task agnostic shared factors from the input. To drive this generation, and to avoid the oversharing of information between the tasks, we propose a novel formulation of the mutual information loss which encourages the generated split embeddings to be distinct as possible. A unique merging network is also introduced to fuse the task specific, and shared information and generate an augmented embedding for the individual downstream tasks in the MTL pipeline. We evaluate the proposed Split ‘n’ Merge Network on two distinct MTL tasks where we achieve state-of-the-art results for both. Our primary, ablation and interpretation evaluations indicate the robustness and flexibility of the propose approach and demonstrates its applicability to numerous, diverse real-world MTL applications.},
  archive      = {J_PR},
  author       = {Tharindu Fernando and Sridha Sridharan and Simon Denman and Clinton Fookes},
  doi          = {10.1016/j.patcog.2022.108551},
  journal      = {Pattern Recognition},
  pages        = {108551},
  shortjournal = {Pattern Recognition},
  title        = {Split ‘n’ merge net: A dynamic masking network for multi-task attention},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive-order proximity learning for graph-based
clustering. <em>PR</em>, <em>126</em>, 108550. (<a
href="https://doi.org/10.1016/j.patcog.2022.108550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, structured proximity matrix learning, which aims to learn a structured proximity matrix with explicit clustering structures from the first-order proximity matrix, has become the mainstream of graph-based clustering. However, the first-order proximity matrix always lacks several must-links compared to the groundtruth in real-world data, which results in a mismatched problem and affects the clustering performance. To alleviate this problem, this work introduces the high-order proximity to structured proximity matrix learning, and explores a novel framework named Adaptive-Order Proximity Learning (AOPL) to learn a consensus structured proximity matrix from the proximities of multiple orders. To be specific, AOPL selects the appropriate orders first, then assigns weights to these selected orders adaptively. In this way, a consensus structured proximity matrix is learned from the proximity matrices of appropriate orders. Based on AOPL framework, two practical models with different properties are derived, namely AOPL-Root and AOPL-Log. Besides, AOPL and the derived models are regarded as the same optimization problem subjected to some slightly different constraints. An efficient algorithm is proposed to solve them and the corresponding theoretical analyses are provided. Extensive experiments on several real-world datasets demonstrate superb performance of our model.},
  archive      = {J_PR},
  author       = {Danyang Wu and Wei Chang and Jitao Lu and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.108550},
  journal      = {Pattern Recognition},
  pages        = {108550},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive-order proximity learning for graph-based clustering},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDUNet: Road extraction via spatial enhanced and densely
connected UNet. <em>PR</em>, <em>126</em>, 108549. (<a
href="https://doi.org/10.1016/j.patcog.2022.108549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting road maps from high-resolution optical remote sensing images has received much attention recently, especially with the rapid development of deep learning methods. However, most of these CNN based approaches simply focused on multi-scale encoder architectures or multiple branches in neural networks , and ignored some inherent characteristics of the road surface. In this paper, we design a novel network for road extraction based on spatial enhanced and densely connected UNet, called SDUNet. SDUNet aggregates both the multi-level features and global prior information of road networks by combining the strengths of spatial CNN-based segmentation and densely connected blocks. To enhance the feature learning about prior information of road surface, a structure preserving model is designed to explore the continuous clues in the spatial level. Experimental results on two benchmark datasets show that the proposed method achieves the state-of-the-art performance, compared with previous approaches for road extraction. Code will be made available on https://github.com/MrStrangerYang/SDUNet .},
  archive      = {J_PR},
  author       = {Mengxing Yang and Yuan Yuan and Ganchao Liu},
  doi          = {10.1016/j.patcog.2022.108549},
  journal      = {Pattern Recognition},
  pages        = {108549},
  shortjournal = {Pattern Recognition},
  title        = {SDUNet: Road extraction via spatial enhanced and densely connected UNet},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MSODANet: A network for multi-scale object detection in
aerial images using hierarchical dilated convolutions. <em>PR</em>,
<em>126</em>, 108548. (<a
href="https://doi.org/10.1016/j.patcog.2022.108548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object detection in aerial images is one of the most commonly used tasks in the wide-range of computer vision applications. However, the object detection is more challenging due to the following issues: (a) the pixel occupancy vary among the different scales of objects, (b) the distribution of objects is not uniform in aerial images, (c) the appearance of an object varies with different view-points and illumination conditions , and (d) the number of objects, even though they belong to same type, vary across the images. To address these issues, we propose a novel network for multi-scale object detection in aerial images using hierarchical dilated convolutions, called as mSODANet. In particular, we probe hierarchical dilated network using parallel dilated convolutions to learn the contextual information of different types of objects at multiple scales and multiple field-of-views. The introduced hierarchical dilated network captures the visual information of aerial image more effectively and enhances the detection capability of the model. Further, the extensive experiments conducted on three challenging publicly available datasets, i.e., Visdrone2019, DOTA (OBB &amp; HBB), NWPU VHR-10, demonstrate the effectiveness of the proposed mSODANet and achieve the state-of-the-art performance on all three datasets.},
  archive      = {J_PR},
  author       = {Vishnu Chalavadi and Prudviraj Jeripothula and Rajeshreddy Datla and Sobhan Babu Ch and Krishna Mohan C},
  doi          = {10.1016/j.patcog.2022.108548},
  journal      = {Pattern Recognition},
  pages        = {108548},
  shortjournal = {Pattern Recognition},
  title        = {MSODANet: A network for multi-scale object detection in aerial images using hierarchical dilated convolutions},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level augmented inpainting network using spatial
similarity. <em>PR</em>, <em>126</em>, 108547. (<a
href="https://doi.org/10.1016/j.patcog.2022.108547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-scale neural networks have shown promising improvements in image inpainting . However, most of them adopt the progressive way, in which the errors on lower scales may be propagated on higher scales. Addressing this issue, we propose a multi-level augmented inpainting network (MLA-Net) to rationally harmonize the inter- and intra-level contexts. Here, a pyramid reconstruction structure (PRS) with three parallel levels is designed to establish the inter-level relationship, which can boost the representation of the features by integrating the texture details into semantics. Then, we propose a novel spatial similarity based attention mechanism (SSA) to ensure the intra-level local continuity between the holes and related available patches. In SSA, in order to focus on the important textures and structures rather than calculating each pixel of the feature equally, a spatial map is utilized to highlight the corresponding spatial locations during the similarity computation. The experiments are evaluated on multiple challenging datasets, which demonstrate that MLA-Net can generate accurate results with better visual quality compared with the state-of-the-art methods. For the 256 × 256 256×256 Places2 dataset, PSNR increases 1.02 dB, while FID decreases 0.075. For the 256 × 256 256×256 CelebA-HQ dataset, there are 0.22 dB and 0.613 improvements in PSNR and FID.},
  archive      = {J_PR},
  author       = {Jia Qin and Huihui Bai and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.108547},
  journal      = {Pattern Recognition},
  pages        = {108547},
  shortjournal = {Pattern Recognition},
  title        = {Multi-level augmented inpainting network using spatial similarity},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QuadNet: Quadruplet loss for multi-view learning in baggage
re-identification. <em>PR</em>, <em>126</em>, 108546. (<a
href="https://doi.org/10.1016/j.patcog.2022.108546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, baggage re-identification (ReID) has become an attractive topic in computer vision because it plays an important role in intelligent surveillance. However, the wide variations in different views of baggage items degrade baggage ReID performance. In this paper, a novel QuadNet is proposed to solve the multi-view problem in baggage ReID at three levels. At the sample level, we propose a multi-view sampling strategy which samples hard examples from multiple identities in multiple views. The sampled baggage items are used to construct quadruplets. At the feature level, view-aware attentional local features are extracted from discriminative regions in each view. These local features are fused with global features to obtain better representations of the quadruplets. At the loss level, a multi-view quadruplet loss operating on the representations of quadruplets is proposed to reduce the intra-class distances caused by view variations and increase the inter-class distances of baggage images captured in the same view. A random local blur data augmentation is proposed to handle the motion blur which is often found in baggage images. The multi-task learning of materials is introduced to obtain discriminative features based on the materials of baggage surfaces. Extensive experiments on three ReID datasets, MVB, Market-1501 and VeRi-776, indicate the remarkable effectiveness and good generalization of the QuadNet model. It has achieved the state-of-the-art performance on the three datasets.},
  archive      = {J_PR},
  author       = {Hao Yang and Xiuxiu Chu and Li Zhang and Yunda Sun and Dong Li and Stephen J. Maybank},
  doi          = {10.1016/j.patcog.2022.108546},
  journal      = {Pattern Recognition},
  pages        = {108546},
  shortjournal = {Pattern Recognition},
  title        = {QuadNet: Quadruplet loss for multi-view learning in baggage re-identification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Human-centric image captioning. <em>PR</em>, <em>126</em>,
108545. (<a href="https://doi.org/10.1016/j.patcog.2022.108545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new topic, Human-Centric Captioning, to mainly describe the human behavior in an image. Human activities and relationships are the primary objectives of visual understanding in daily applications. However, existing image captioning systems cannot differently treat humans and other objects, which limits the ability to understand and describe diverse human activities. As the first explorer of this new task, we build a novel Human-Centric COCO dataset concentrating on humans. Accordingly, we propose a novel Human-Centric Captioning Model (HCCM) that focuses on human-centric feature hierarchization and sentence generation. Specifically, our model first utilizes human body part-level knowledge to hierarchize the image features and then applies a novel three-branch captioning model to process these hierarchical features independently to calibrate the descriptions of human actions. Comprehensive experiments demonstrate that our HCCM achieves the state-of-the-art performance with BLEU-4, CIDEr and SPICE scores of 41.5, 127.3, 23.5 respectively. Dataset and code are publicly available at https://github.com/JohnDreamer/HCCM/.},
  archive      = {J_PR},
  author       = {Zuopeng Yang and Pengbo Wang and Tianshu Chu and Jie Yang},
  doi          = {10.1016/j.patcog.2022.108545},
  journal      = {Pattern Recognition},
  pages        = {108545},
  shortjournal = {Pattern Recognition},
  title        = {Human-centric image captioning},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). D2T: A framework for transferring detection to tracking.
<em>PR</em>, <em>126</em>, 108544. (<a
href="https://doi.org/10.1016/j.patcog.2022.108544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection methods draw increasing attention in deep learning based visual tracking algorithms due to their robust discrimination and powerful regression ability. To further explore the potential of object detection methods in the visual tracking task, there are two gaps that need to be bridged. The first is the difference in object definition. Object detection is class-specific while visual tracking is class-agnostic. Moreover, visual tracking needs to differentiate the target from intra-class distractors . The second is the difference in temporal dimension. Different from object detection which processes still-image, visual tracking concentrates on objects which vary continuously with time. In this paper, we propose a Detection to Tracking (D2T) framework to address the above issues and effectively transfer existing advanced detection methods to visual tracking task. Specifically, to bridge the gap of object definition, we propose a general-to-specific network that separates learning general object features and instance-level features. To make full use of the contextual information while adapting to the appearance variation of targets, we propose a temporal strategy combining short-term constraint and long-term updating. To the best of our knowledge, our D2T framework is the first universal framework which directly transfers deep learning based object detectors to visual tracking task. It provides a novel solution to visual object tracking, and it achieves superior performance in several public datasets.},
  archive      = {J_PR},
  author       = {Huai Qin and Changqian Yu and Changxin Gao and Nong Sang},
  doi          = {10.1016/j.patcog.2022.108544},
  journal      = {Pattern Recognition},
  pages        = {108544},
  shortjournal = {Pattern Recognition},
  title        = {D2T: A framework for transferring detection to tracking},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative feature selection with directional outliers
correcting for data classification. <em>PR</em>, <em>126</em>, 108541.
(<a href="https://doi.org/10.1016/j.patcog.2022.108541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from “misclassified instances” which lead to misclassification . In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification . The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.},
  archive      = {J_PR},
  author       = {Lixin Yuan and Guoqiang Yang and Qian Xu and Tong Lu},
  doi          = {10.1016/j.patcog.2022.108541},
  journal      = {Pattern Recognition},
  pages        = {108541},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative feature selection with directional outliers correcting for data classification},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSDA-net: Seeking reliable correspondences by
channel-spatial difference augment network. <em>PR</em>, <em>126</em>,
108539. (<a href="https://doi.org/10.1016/j.patcog.2022.108539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing reliable correspondences is a fundamental task in computer vision , and it requires rich contextual information. In this paper, we propose a Channel-Spatial Difference Augment Network (CSDA-Net), by selectively aggregating information from spatial and channel aspects, to seek reliable correspondences for feature matching. Specifically, we firstly introduce the spatial and channel attention mechanism to construct a simple yet effective block for discriminately extracting the global context. After that, we design a Overlay Attention block by further exploiting the spatial and channel attention mechanism with different squeeze operations, to gather more comprehensive contextual information. Finally, the proposed CSDA-Net is able to achieve feature maps with a strong representative ability for feature matching due to the integration of the two novel blocks. Extensive experiments on outlier rejection and relative pose estimation have shown better performance improvements of our CSDA-Net over current state-of-the-art methods on both outdoor and indoor datasets.},
  archive      = {J_PR},
  author       = {Shunxing Chen and Linxin Zheng and Guobao Xiao and Zhen Zhong and Jiayi Ma},
  doi          = {10.1016/j.patcog.2022.108539},
  journal      = {Pattern Recognition},
  pages        = {108539},
  shortjournal = {Pattern Recognition},
  title        = {CSDA-net: Seeking reliable correspondences by channel-spatial difference augment network},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universal predictive systems. <em>PR</em>, <em>126</em>,
108536. (<a href="https://doi.org/10.1016/j.patcog.2022.108536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes probability forecasting systems that are universal , or universally consistent , in the sense of being consistent under any data-generating distribution, assuming that the observations are produced independently in the IID fashion. The notion of universal consistency is asymptotic and does not imply any small-sample guarantees of validity. On the other hand, the method of conformal prediction has been recently adapted to producing predictive distributions that satisfy a natural property of small-sample validity, namely they are automatically probabilistically calibrated. The main result of the paper is the existence of universal conformal predictive systems, which output predictive distributions that are both probabilistically calibrated and universally consistent.},
  archive      = {J_PR},
  author       = {Vladimir Vovk},
  doi          = {10.1016/j.patcog.2022.108536},
  journal      = {Pattern Recognition},
  pages        = {108536},
  shortjournal = {Pattern Recognition},
  title        = {Universal predictive systems},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient framework for zero-shot sketch-based image
retrieval. <em>PR</em>, <em>126</em>, 108528. (<a
href="https://doi.org/10.1016/j.patcog.2022.108528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) has recently attracted the attention of the computer vision community due to its real-world applications, and the more realistic and challenging setting that it presents over SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation . The majority of previous studies using deep neural networks have achieved improved results by either projecting sketch and images into a common low-dimensional space, or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements for training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and learns the semantic embedding space from a vision model rather than a language model , as is done by related studies. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN ( i.e. , ResNet50) is fine-tuned with three proposed learning objects : domain-balanced quadruplet loss, semantic classification loss , and semantic knowledge preservation loss . The domain-balanced quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features by considering ZS-SBIR as an object detection and verification problem. To preserve semantic knowledge learned with ImageNet and exploit it for unseen categories, the semantic knowledge preservation loss is proposed. To reduce computational cost and increase the accuracy of the semantic knowledge distillation process, ground-truth semantic knowledge is prepared in a class-oriented fashion prior to training. Extensive experiments are conducted on three challenging ZS-SBIR datasets: Sketchy Extended, TU-Berlin Extended and QuickDraw Extended. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a substantial margin.},
  archive      = {J_PR},
  author       = {Osman Tursun and Simon Denman and Sridha Sridharan and Ethan Goan and Clinton Fookes},
  doi          = {10.1016/j.patcog.2022.108528},
  journal      = {Pattern Recognition},
  pages        = {108528},
  shortjournal = {Pattern Recognition},
  title        = {An efficient framework for zero-shot sketch-based image retrieval},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AccLoc: Anchor-free and two-stage detector for accurate
object localization. <em>PR</em>, <em>126</em>, 108523. (<a
href="https://doi.org/10.1016/j.patcog.2022.108523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current anchor-free object detectors have obtained detection performances comparable to those of anchor-based object detectors while avoiding the weaknesses of anchor designs. However, two challenges limit the localization performance . First, such anchor-free detectors have one stage that predicts the classification and localization results directly. A large regression space reduces the localization performance of such methods. Second, most of the existing detectors extract features which are ineffective for accurate localization. In this paper, for the first challenge, we propose two-stage networks to predict regression results stage by stage, thereby reducing the scope of the prediction space. For the second challenge, we design two novel modules with the aim of extracting effective features for accurate localization. Experimental results validate that each module in our approach is effective and validate that our approach has better object localization performance than previous related and advanced methods.},
  archive      = {J_PR},
  author       = {Zhengquan Piao and Junbo Wang and Linbo Tang and Baojun Zhao and Wenzheng Wang},
  doi          = {10.1016/j.patcog.2022.108523},
  journal      = {Pattern Recognition},
  pages        = {108523},
  shortjournal = {Pattern Recognition},
  title        = {AccLoc: Anchor-free and two-stage detector for accurate object localization},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Learning upper patch attention using dual-branch training
strategy for masked face recognition. <em>PR</em>, <em>126</em>, 108522.
(<a href="https://doi.org/10.1016/j.patcog.2022.108522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of pandemic, COVID-19, recognition of masked face images is a challenging problem, as most of the facial components become invisible. By utilizing prior information that mask-occlusion is located in the lower half of the face, we propose a dual-branch training strategy to guide the model to focus on the upper half of the face to extract robust features for Masked face recognition (MFR). During training, the features learned at the intermediate layers of the global branch are fed to our proposed attention module, named Upper Patch Attention (UPA), which acts as a local branch. Both branches are jointly optimized to enhance the feature extraction from non-occluded regions. We also propose a self-attention module, which integrates into the backbone network to enhance the interaction between the channels and spatial locations in the learning process. Extensive experiments on synthetic and real-masked face datasets demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Yuxuan Zhang and Xin Wang and M. Saad Shakeel and Hao Wan and Wenxiong Kang},
  doi          = {10.1016/j.patcog.2022.108522},
  journal      = {Pattern Recognition},
  pages        = {108522},
  shortjournal = {Pattern Recognition},
  title        = {Learning upper patch attention using dual-branch training strategy for masked face recognition},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Repurposing existing deep networks for caption and
aesthetic-guided image cropping. <em>PR</em>, <em>126</em>, 108485. (<a
href="https://doi.org/10.1016/j.patcog.2021.108485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel optimization framework that crops a given image based on user description and aesthetics. Unlike existing image cropping methods, where one typically trains a deep network to regress to crop parameters or cropping actions, we propose to directly optimize for the cropping parameters by repurposing pre-trained networks on image captioning and aesthetic tasks, without any fine-tuning, thereby avoiding training a separate network. Specifically, we search for the best crop parameters that minimize a combined loss of the initial objectives of these networks. To make the optimization stable, we propose three strategies: (i) multi-scale bilinear sampling, (ii) annealing the scale of the crop region, therefore effectively reducing the parameter space, (iii) aggregation of multiple optimization results. Through various quantitative and qualitative evaluations, we show that our framework can produce crops that are well-aligned to intended user descriptions and aesthetically pleasing.},
  archive      = {J_PR},
  author       = {Nora Horanyi and Kedi Xia and Kwang Moo Yi and Abhishake Kumar Bojja and Aleš Leonardis and Hyung Jin Chang},
  doi          = {10.1016/j.patcog.2021.108485},
  journal      = {Pattern Recognition},
  pages        = {108485},
  shortjournal = {Pattern Recognition},
  title        = {Repurposing existing deep networks for caption and aesthetic-guided image cropping},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable neural architecture learning for efficient
neural networks. <em>PR</em>, <em>126</em>, 108448. (<a
href="https://doi.org/10.1016/j.patcog.2021.108448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient neural networks has received ever-increasing attention with the evolution of convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems to obtaining such efficient neural networks is efficiency, even recent differentiable neural architecture search (DNAS) requires to sample a small number of candidate neural architectures for the selection of the optimal neural architecture. To address this computational efficiency issue, we introduce a novel architecture parameterization based on scaled sigmoid function , and propose a general Differentiable Neural Architecture Learning (DNAL) method to obtain efficient neural networks without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization, which avoids the interaction between the two types of parameters and alleviates the vanishing gradient problem. We address the non-convex optimization problem of efficient neural networks by the continuous scaled sigmoid method instead of the common softmax method. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of efficiency, and adapts to conventional CNNs (e.g., VGG16 and ResNet50), lightweight CNNs (e.g., MobileNetV2) and stochastic supernets (e.g., ProxylessNAS). The optimal neural networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity . Our source code is available at https://github.com/QingbeiGuo/DNAL.git .},
  archive      = {J_PR},
  author       = {Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng},
  doi          = {10.1016/j.patcog.2021.108448},
  journal      = {Pattern Recognition},
  pages        = {108448},
  shortjournal = {Pattern Recognition},
  title        = {Differentiable neural architecture learning for efficient neural networks},
  volume       = {126},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto uning of price prediction models for high-frequency
trading via reinforcement learning. <em>PR</em>, <em>125</em>, 108543.
(<a href="https://doi.org/10.1016/j.patcog.2022.108543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level , thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm . A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.},
  archive      = {J_PR},
  author       = {Weipeng Zhang and Ning Zhang and Junchi Yan and Guofu Li and Xiaokang Yang},
  doi          = {10.1016/j.patcog.2022.108543},
  journal      = {Pattern Recognition},
  pages        = {108543},
  shortjournal = {Pattern Recognition},
  title        = {Auto uning of price prediction models for high-frequency trading via reinforcement learning},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VD-PCR: Improving visual dialog with pronoun coreference
resolution. <em>PR</em>, <em>125</em>, 108540. (<a
href="https://doi.org/10.1016/j.patcog.2022.108540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual dialog task requires an AI agent to interact with humans in multi-round dialogs based on a visual environment. As a common linguistic phenomenon, pronouns are often used in dialogs to improve the communication efficiency. As a result, resolving pronouns (i.e., grounding pronouns to the noun phrases they refer to) is an essential step towards understanding dialogs. In this paper, we propose VD-PCR, a novel framework to improve Visual Dialog understanding with Pronoun Coreference Resolution in both implicit and explicit ways. First, to implicitly help models understand pronouns, we design novel methods to perform the joint training of the pronoun coreference resolution and visual dialog tasks. Second, after observing that the coreference relationship of pronouns and their referents indicates the relevance between dialog rounds, we propose to explicitly prune the irrelevant history rounds in visual dialog models’ input. With pruned input, the models can focus on relevant dialog history and ignore the distraction in the irrelevant one. With the proposed implicit and explicit methods, VD-PCR achieves state-of-the-art experimental results on the VisDial dataset.},
  archive      = {J_PR},
  author       = {Xintong Yu and Hongming Zhang and Ruixin Hong and Yangqiu Song and Changshui Zhang},
  doi          = {10.1016/j.patcog.2022.108540},
  journal      = {Pattern Recognition},
  pages        = {108540},
  shortjournal = {Pattern Recognition},
  title        = {VD-PCR: Improving visual dialog with pronoun coreference resolution},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contour-enhanced attention CNN for CT-based COVID-19
segmentation. <em>PR</em>, <em>125</em>, 108538. (<a
href="https://doi.org/10.1016/j.patcog.2022.108538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of COVID-19 is one of the challenging research topics in today&#39;s healthcare sector to control the coronavirus pandemic. Automatic data-powered insights for COVID-19 localization from medical imaging modality like chest CT scan tremendously augment clinical care assistance. In this research, a Contour-aware Attention Decoder CNN has been proposed to precisely segment COVID-19 infected tissues in a very effective way. It introduces a novel attention scheme to extract boundary, shape cues from CT contours and leverage these features in refining the infected areas. For every decoded pixel, the attention module harvests contextual information in its spatial neighborhood from the contour feature maps. As a result of incorporating such rich structural details into decoding via dense attention, the CNN is able to capture even intricate morphological details. The decoder is also augmented with a Cross Context Attention Fusion Upsampling to robustly reconstruct deep semantic features back to high-resolution segmentation map. It employs a novel pixel-precise attention model that draws relevant encoder features to aid in effective upsampling. The proposed CNN was evaluated on 3D scans from MosMedData and Jun Ma benchmarked datasets. It achieved state-of-the-art performance with a high dice similarity coefficient of 85.43\% and a recall of 88.10\%.},
  archive      = {J_PR},
  author       = {R. Karthik and R. Menaka and Hariharan M and Daehan Won},
  doi          = {10.1016/j.patcog.2022.108538},
  journal      = {Pattern Recognition},
  pages        = {108538},
  shortjournal = {Pattern Recognition},
  title        = {Contour-enhanced attention CNN for CT-based COVID-19 segmentation},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Phase retrieval from incomplete data via weighted nuclear
norm minimization. <em>PR</em>, <em>125</em>, 108537. (<a
href="https://doi.org/10.1016/j.patcog.2022.108537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering an unknown object from the magnitude of its Fourier transform is a phase retrieval problem. Here, we consider a much difficult case, where those observed intensity values are incomplete and contaminated by both salt-and-pepper and random-valued impulse noise. To take advantage of the low-rank property within the image of the object, we use a regularization term which penalizes high weighted nuclear norm values of image patch groups. For outliers (impulse noise) in the observation, the ℓ 1 − 2 ℓ1−2 metric is adopted as the data fidelity term. Then we break down the resulting optimization problem into smaller ones, for example, weighted nuclear norm proximal mapping and ℓ 1 − 2 ℓ1−2 minimization, because the nonconvex and nonsmooth subproblems have available closed-form solutions. The convergence results are also presented, and numerical experiments are provided to demonstrate the superior reconstruction quality of the proposed method.},
  archive      = {J_PR},
  author       = {Zhi Li and Ming Yan and Tieyong Zeng and Guixu Zhang},
  doi          = {10.1016/j.patcog.2022.108537},
  journal      = {Pattern Recognition},
  pages        = {108537},
  shortjournal = {Pattern Recognition},
  title        = {Phase retrieval from incomplete data via weighted nuclear norm minimization},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspherical class prototypes for adversarial robustness.
<em>PR</em>, <em>125</em>, 108527. (<a
href="https://doi.org/10.1016/j.patcog.2022.108527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of adversarial robustness in deep neural network classification from an optimal class boundary estimation perspective. It is argued that increased model robustness to adversarial attacks can be achieved when the feature learning process is monitored by geometrically-inspired optimization criteria. To this end, we propose to learn hyperspherical class prototypes in the neural feature embedding space, along with training the network parameters. Three concurrent optimization functions for the intermediate hidden layer training data activations are devised, requiring items of the same class to be enclosed by the corresponding class prototype boundaries, to have minimum distance from their class prototype vector (i.e., hypersphere center) and to have maximum distance from the remainder hypersphere centers. Our experiments show that training standard classification model architectures with the proposed objectives, significantly increases their robustness to white-box adversarial attacks, without adverse (if not beneficial) effects to their classification accuracy .},
  archive      = {J_PR},
  author       = {Vasileios Mygdalis and Ioannis Pitas},
  doi          = {10.1016/j.patcog.2022.108527},
  journal      = {Pattern Recognition},
  pages        = {108527},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspherical class prototypes for adversarial robustness},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hypergraph matching via game-theoretic hypergraph
clustering. <em>PR</em>, <em>125</em>, 108526. (<a
href="https://doi.org/10.1016/j.patcog.2022.108526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching is used to build correspondences between features in the model and test images. As the extension of graph matching , hypergraph matching is able to encode rich invariance between feature tuples and improve matching accuracy. Different from many existing algorithms based on maximizing the matching score between correspondences, our approach formulates hypergraph matching as a non-cooperative multi-player game and obtains matches by extracting the evolutionary stable strategies (ESS). While this approach generates a high matching accuracy, the number of matches is usually small and it involves a large computation load to obtain more matches. To solve this problem, we extract multiple ESS clusters instead of one single ESS group, thereby transforming hypergraph matching of features to hypergraph clustering of candidate matches. By extracting an appropriate number of clusters, we increase the number of matches efficiently, and improve the matching accuracy by imposing the one-to-one constraint. In experiments with three real datasets, our algorithm is shown to generate a large number of matches efficiently. It also shows significant advantage in matching accuracy in comparison with some other hypergraph matching algorithms .},
  archive      = {J_PR},
  author       = {Jian Hou and Marcello Pelillo and Huaqiang Yuan},
  doi          = {10.1016/j.patcog.2022.108526},
  journal      = {Pattern Recognition},
  pages        = {108526},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph matching via game-theoretic hypergraph clustering},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph label prediction based on local structure
characteristics representation. <em>PR</em>, <em>125</em>, 108525. (<a
href="https://doi.org/10.1016/j.patcog.2022.108525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent study has shown that the real-time anti-noise challenges faced by molecular activity prediction algorithms can be solved by using the part structure features of the molecular graph. However, the sub-structures selected by this method are distributed in a scattered manner such that although they include as many block features as possible, they do not fully consider the connections between these blocks. Therefore, this study was conducted to fully consider the physical interpretation of the betweenness centrality node in the graph, and a sub-structure was obtained by depth-first search (DFS) from this node. This sub-structure not only contains the characteristics of each region but also retains the connections between each region. Then, a cascading multi-layer perception (MLP) model was designed to learn the characteristic representation of the graph from its local structure features. Experiments demonstrated that the performance of our algorithm is superior to that of other algorithms when evaluated on different datasets.},
  archive      = {J_PR},
  author       = {Jingyi Ding and Ruohui Cheng and Jian Song and Xiangrong Zhang and Licheng Jiao and Jianshe Wu},
  doi          = {10.1016/j.patcog.2022.108525},
  journal      = {Pattern Recognition},
  pages        = {108525},
  shortjournal = {Pattern Recognition},
  title        = {Graph label prediction based on local structure characteristics representation},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). BADet: Boundary-aware 3D object detection from point
clouds. <em>PR</em>, <em>125</em>, 108524. (<a
href="https://doi.org/10.1016/j.patcog.2022.108524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, existing state-of-the-art 3D object detectors are in two-stage paradigm. These methods typically comprise two steps: 1) Utilize a region proposal network to propose a handful of high-quality proposals in a bottom-up fashion. 2) Resize and pool the semantic features from the proposed regions to summarize RoI-wise representations for further refinement. Note that these RoI-wise representations in step 2) are considered individually as uncorrelated entries when fed to following detection headers. Nevertheless, we observe these proposals generated by step 1) offset from ground truth somehow, emerging in local neighborhood densely with an underlying probability. Challenges arise in the case where a proposal largely forsakes its boundary information due to coordinate offset while existing networks lack corresponding information compensation mechanism. In this paper, we propose BADet for 3D object detection from point clouds. Specifically, instead of refining each proposal independently as previous works do, we represent each proposal as a node for graph construction within a given cut-off threshold, associating proposals in the form of local neighborhood graph , with boundary correlations of an object being explicitly exploited. Besides, we devise a lightweight Region Feature Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise features with expanding receptive fields for more informative RoI-wise representations. We validate BADet both on widely used KITTI Dataset and highly challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par performance on KITTI 3D detection leaderboard and ranks 1 st on M o d e r a t e Moderate difficulty of C a r Car category on KITTI BEV detection leaderboard. The source code is available at https://github.com/rui-qian/BADet .},
  archive      = {J_PR},
  author       = {Rui Qian and Xin Lai and Xirong Li},
  doi          = {10.1016/j.patcog.2022.108524},
  journal      = {Pattern Recognition},
  pages        = {108524},
  shortjournal = {Pattern Recognition},
  title        = {BADet: Boundary-aware 3D object detection from point clouds},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised person re-identification with multi-label
learning guided self-paced clustering. <em>PR</em>, <em>125</em>,
108521. (<a href="https://doi.org/10.1016/j.patcog.2022.108521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning . Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.},
  archive      = {J_PR},
  author       = {Qing Li and Xiaojiang Peng and Yu Qiao and Qi Hao},
  doi          = {10.1016/j.patcog.2022.108521},
  journal      = {Pattern Recognition},
  pages        = {108521},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised person re-identification with multi-label learning guided self-paced clustering},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symmetry-driven hyper feature GCN for skeleton-based gait
recognition. <em>PR</em>, <em>125</em>, 108520. (<a
href="https://doi.org/10.1016/j.patcog.2022.108520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition , as an attractive task in biometrics , remains challenging due to significant intra-class changes of clothing and pose variations across different cameras. Recent approaches mainly focus on silhouette-based gait mode, which is easy to model in Convolutional Neural Networks (CNNs). Compared with silhouettes, the dynamics of skeletons essentially convey more robust information, which is invariant to view and clothing changes. Conventional approaches for modeling skeletons usually rely on hand-crafted features or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we address the skeleton-based gait recognition task with a novel Symmetry-Driven Hyper Feature Graph Convolutional Network (SDHF-GCN), which goes beyond the limitations of previous approaches by automatically learning multiple dynamic patterns and hierarchical semantic features in a unified Graph Convolutional Network (GCN). This model involves three dynamic patterns: natural connection, temporal correlation and symmetric interaction, which enriches the description of dynamic patterns by exploiting symmetry perceptual principles. Furthermore, a hyper feature network is proposed to aggregate the hierarchical semantic features , including dynamic features at the high level, structured features at the intermediate level, and static features at the low level, which complement each other to enhance the discriminative ability. By integrating different patterns in the hierarchical structure, the model is able to generate versatile and discriminative representations, thus improving the recognition rate. On the CASIA-B and OUMVLP-Pose datasets, the proposed SDHF-GCN renders substantial improvements over mainstream methods, especially in the coat-wearing scenario, with superior robustness to covariate factors.},
  archive      = {J_PR},
  author       = {Xiaokai Liu and Zhaoyang You and Yuxiang He and Sheng Bi and Jie Wang},
  doi          = {10.1016/j.patcog.2022.108520},
  journal      = {Pattern Recognition},
  pages        = {108520},
  shortjournal = {Pattern Recognition},
  title        = {Symmetry-driven hyper feature GCN for skeleton-based gait recognition},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified perspective of classification-based loss and
distance-based loss for cross-view gait recognition. <em>PR</em>,
<em>125</em>, 108519. (<a
href="https://doi.org/10.1016/j.patcog.2021.108519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait can be used to recognize people in an uncooperative and noninvasive manner and it is hard to imitate or counterfeit, which makes it suitable for video surveillance. The current solutions for gait recognition are still not robust to handle the conditions when the view angles of the gallery and query are different. We improve the performance of cross-view gait recognition from the perspective of metric learning. Specifically, we propose to use angular softmax loss to impose an angular margin for extracting separable features. At the same time, we use triplet loss to make the extracted features more discriminative. Additionally, we add a batch-normalization layer after extracting gait features to effectively optimize two different losses. We evaluate our approach on two widely-used gait dataset: CASIA-B dataset and TUM GAID dataset. The experiment results show that our approach outperforms the prior state-of-the-art approaches, which shows the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Feng Han and Xuejian Li and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.patcog.2021.108519},
  journal      = {Pattern Recognition},
  pages        = {108519},
  shortjournal = {Pattern Recognition},
  title        = {A unified perspective of classification-based loss and distance-based loss for cross-view gait recognition},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe incomplete label distribution learning. <em>PR</em>,
<em>125</em>, 108518. (<a
href="https://doi.org/10.1016/j.patcog.2021.108518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance.},
  archive      = {J_PR},
  author       = {Jing Zhang and Hong Tao and Tingjin Luo and Chenping Hou},
  doi          = {10.1016/j.patcog.2021.108518},
  journal      = {Pattern Recognition},
  pages        = {108518},
  shortjournal = {Pattern Recognition},
  title        = {Safe incomplete label distribution learning},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entropy regularization for unsupervised clustering with
adaptive neighbors. <em>PR</em>, <em>125</em>, 108517. (<a
href="https://doi.org/10.1016/j.patcog.2021.108517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based clustering has been considered as an effective kind of method in unsupervised manner to partition various items into several groups, such as Spectral Clustering (SC). However, there are three species of drawbacks in SC: (1) The effects of clustering is sensitive to the affinity matrix that is fixed by original data. (2) The input affinity matrix is simply based on distance measurement, which lacks of clear physical meaning under probabilistic prediction. (3) Additional discretization procedures still need to be operated. To cope with these issues, we propose a new clustering model, which refers to Entropy Regularization for unsupervised Clustering with Adaptive Neighbors (ERCAN), to dynamically and simultaneously update affinity matrix and clustering results . Firstly, the maximized entropy regularization term is introduced in probability model to avoid trivial similarity distributions. Additionally, we newly introduce the Laplacian rank constraint with ℓ 0 ℓ0 -norm to construct adaptive neighbors for sparsity and strength segmentation ability without extra discretization process. Finally, we present a novel monotonic function optimization method, which reveals the consistence between graph sparsity and neighbor assignment, to address the ℓ 0 ℓ0 -norm constraint in alternative optimization process. Comprehensive experiments show the superiority of our method with promising results.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Zhenyu Ma and Feiping Nie and Xuelong Li},
  doi          = {10.1016/j.patcog.2021.108517},
  journal      = {Pattern Recognition},
  pages        = {108517},
  shortjournal = {Pattern Recognition},
  title        = {Entropy regularization for unsupervised clustering with adaptive neighbors},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADR-MVSNet: A cascade network for 3D point cloud
reconstruction with pixel occlusion. <em>PR</em>, <em>125</em>, 108516.
(<a href="https://doi.org/10.1016/j.patcog.2021.108516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud reconstruction is an urgent task in computer vision for environment perception. Nevertheless, the reconstructed scene is inaccurate and incomplete, because the visibility of pixels is not taken into account by existing methods. In this paper, a cascaded network with a multiple cost volume aggregation module named ADR-MVSNet is proposed. Three improvements are presented in ADR-MVSNet. First, to improve the reconstruction accuracy and reduce the time complexity, an adaptive depth reduction module, which adaptively adjusts the depth range of the pixel through the confidence interval, is proposed. Second, to more accurately estimate the depth of occluded pixels in multiview images, a multiple cost volume aggregation module, in which Gini impurity is introduced to measure the confidence of pixel depth prediction, is proposed. Third, a multiscale photometric consistency filter module is proposed, which considers the information in multiple confidence maps at the same time and filters out outliers accurately to remove pixels with low confidence. Therefore, the accuracy of point cloud reconstruction is improved. The experimental results on the DTU and Tanks and Temple datasets demonstrate that ADR-MVSNet achieves highly accurate and highly complete reconstruction compared with state-of-the-art benchmarks.},
  archive      = {J_PR},
  author       = {Ying Li and Zhijie Zhao and Jiahao Fan and Wenyue Li},
  doi          = {10.1016/j.patcog.2021.108516},
  journal      = {Pattern Recognition},
  pages        = {108516},
  shortjournal = {Pattern Recognition},
  title        = {ADR-MVSNet: A cascade network for 3D point cloud reconstruction with pixel occlusion},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative boundary-aware context encoding networks for
error map prediction. <em>PR</em>, <em>125</em>, 108515. (<a
href="https://doi.org/10.1016/j.patcog.2021.108515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately assessing the medical image segmentation quality of the automatically generated predictions is essential for guaranteeing the reliability of the results of computer-assisted diagnosis (CAD). Many researchers have studied segmentation quality estimation without labeled ground truths. Recently, a novel idea is proposed, which transforms segmentation quality assessment (SQA) into the pixel-wise or voxel-wise error map segmentation task . However, the simple application of vanilla segmentation structures in medical domain fails to achieve satisfactory error segmentation results. In this paper, we propose collaborative boundary-aware context encoding networks called EP-Net for error segmentation task. Specifically, we propose a collaborative feature transformation branch for better feature fusion between images and masks, and precise localization of error regions. Further, we propose a context encoding module to utilize the global predictor from the error map to enhance the feature representation and regularize the networks. Extensive experiments on IBSR V2.0 dataset, ACDC dataset and M&amp;Ms dataset demonstrate that EP-Net achieves better error segmentation results compared with the traditional segmentation patterns. Based on error prediction results, we obtain a proxy metric of segmentation quality, which has high Pearson correlation coefficient with the real segmentation accuracy on all datasets.},
  archive      = {J_PR},
  author       = {Zhenxi Zhang and Chunna Tian and Xinbo Gao and Jie Li and Zhicheng Jiao and Cui Wang and Zhusi Zhong},
  doi          = {10.1016/j.patcog.2021.108515},
  journal      = {Pattern Recognition},
  pages        = {108515},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative boundary-aware context encoding networks for error map prediction},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bibliometric analysis of off-line handwritten document
analysis literature (1990–2020). <em>PR</em>, <em>125</em>, 108513. (<a
href="https://doi.org/10.1016/j.patcog.2021.108513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing computers with the ability to process handwriting is both important and challenging, since many difficulties (e.g., different writing styles, alphabets, languages, etc.) need to be overcome for addressing a variety of problems (text recognition, signature verification, writer identification, word spotting, etc.). This paper reviews the growing literature on off-line handwritten document analysis over the last thirty years. A sample of 5389 articles is examined using bibliometric techniques. Using bibliometric techniques, this paper identifies (i) the most influential articles in the area, (ii) the most productive authors and their collaboration networks, (iii) the countries and institutions that have led research on the topic, (iv) the journals and conferences that have published most papers, and (v) the most relevant research topics (and their related tasks and methodologies) and their evolution over the years.},
  archive      = {J_PR},
  author       = {Victoria Ruiz-Parrado and Ruben Heradio and Ernesto Aranda-Escolastico and Ángel Sánchez and José F. Vélez},
  doi          = {10.1016/j.patcog.2021.108513},
  journal      = {Pattern Recognition},
  pages        = {108513},
  shortjournal = {Pattern Recognition},
  title        = {A bibliometric analysis of off-line handwritten document analysis literature (1990–2020)},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). User-based network embedding for opinion spammer detection.
<em>PR</em>, <em>125</em>, 108512. (<a
href="https://doi.org/10.1016/j.patcog.2021.108512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign . The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09\%,12.04\%] and [16.25\%,12.78\%] in terms of AP and AUC, respectively.},
  archive      = {J_PR},
  author       = {Ziyang Wang and Wei Wei and Xian-Ling Mao and Guibing Guo and Pan Zhou and Sheng Jiang},
  doi          = {10.1016/j.patcog.2021.108512},
  journal      = {Pattern Recognition},
  pages        = {108512},
  shortjournal = {Pattern Recognition},
  title        = {User-based network embedding for opinion spammer detection},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attentive occlusion-adaptive deep network for facial
landmark detection. <em>PR</em>, <em>125</em>, 108510. (<a
href="https://doi.org/10.1016/j.patcog.2021.108510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be very specific in this paper, an Attentive Occlusion-adaptive Deep Network, hereafter referred as AODN, is proposed for facial landmark detection, consisting of the geometry-aware module, attention module, and low-rank learning module. Facial Landmark Detection (FLD) is a fundamental pre-processing step of facial related tasks. Occlusion, extreme pose, different expressions and illumination are the main challenges in facial landmark detection related tasks. Convolutional Neural Network (CNN) based FLD methods have attained significant improvement regarding accurate FLD but, to deal with occlusion is still very challenging even for CNN. It is because; probably occlusion misleads CNN on feature representation learning . If faces are partially occluded, the localization accuracy will drop significantly. The role of attention in the human visual system is vital, and researchers proved its significance for the computer vision problem. Taking advantage of geometric relationships among different facial components and attention, we extended our already established Occlusion-adaptive Deep Network (ODN). We introduced the attention module consisting of Channel-wise Attention (CA) and Spatial Attention (SA) to improve its ability to deal with the occlusion and enhance feature representation ability simultaneously. The occlusion probability assists as adaptive weights of high-level features and minimizes the effect of the occlusion and assist in modelling the occlusion. Ablation studies prove the synergistic effect of each module. The summary of our trifold contribution is as follows: i) we introduced attention mechanism in our already established ODN model, to deal with occlusion more precisely, and get the rich feature representation to achieve better performance. ii) As per our best of knowledge, we are the pioneers to introduce CA and SA for FLD to model occlusion. iii) Our proposed methodology reduces the number of entire network parameters, which effectually decreases training time and cost. So, the proposed model is more suitable for scalable data processing. Experimental results prove the better performance of proposed AODN on challenging benchmark datasets.},
  archive      = {J_PR},
  author       = {Muhammad Sadiq and Daming Shi},
  doi          = {10.1016/j.patcog.2021.108510},
  journal      = {Pattern Recognition},
  pages        = {108510},
  shortjournal = {Pattern Recognition},
  title        = {Attentive occlusion-adaptive deep network for facial landmark detection},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deformable CNN-based triplet model for fine-grained
sketch-based image retrieval. <em>PR</em>, <em>125</em>, 108508. (<a
href="https://doi.org/10.1016/j.patcog.2021.108508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of electronic touch-screen and pressure sensing devices, fine-grained sketch based image retrieval (FG-SBIR) has become a research hotspot. In this paper, we stress the core problems of FG-SBIR: a. how to reduce the difference between the non-homogenous of heterogeneous media, and b. how to improve the distinguishability of sketch features. Specifically, a sketch generation model is first proposed to replace the conventional pre-processing of roughly extracting image edges, moreover, this model can alleviate the dilemma of sketch data scarcity. We then construct a novel FG-SBIR model which takes advantage of deformable convolutional neural network while taking into consideration of semantic attributes together. In addition, we build a fine-grained clothing sketch-image dataset, which has rich attribute annotations, for the first time. Extensive experiments exhibit that our proposed model achieves a better performance in improving the retrieval accuracy over the state-of-the-art baselines.},
  archive      = {J_PR},
  author       = {Xianlin Zhang and Mengling Shen and Xueming Li and Fangxiang Feng},
  doi          = {10.1016/j.patcog.2021.108508},
  journal      = {Pattern Recognition},
  pages        = {108508},
  shortjournal = {Pattern Recognition},
  title        = {A deformable CNN-based triplet model for fine-grained sketch-based image retrieval},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint image denoising with gradient direction and
edge-preserving regularization. <em>PR</em>, <em>125</em>, 108506. (<a
href="https://doi.org/10.1016/j.patcog.2021.108506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint image denoising algorithms use the structures of the guidance image as a prior to restore the noisy target image. While the provided guidance images are helpful to improve the denoising performance, the denoised edges are most likely to be blurred especially when the edges of the guidance image are weak or inexistent. To address this weakness, this paper proposes a new gradient-direction-based joint image denoising method in which the absolute cosine value of the angle between two gradient vectors of the guidance image and those of the image to recover is employed as the parallel measurement to ensure that the gradient directions of the denoised image are approximately the same as or opposite to those of the guidance image. Besides, a new edge-preserving regularization term is developed to alleviate the effects of the unreliable prior information from guidance image. To simplify the resultant complex nonconvex and nonlinear fractional model, the logarithm function is employed to convert the multiplication operation into addition operation. Then, we construct the surrogate function for the logarithmic term of l 2 l2 -norm, and separate the variables to transform the objective function into convex one with high numerical stability while retaining high efficiency. Finally, the optimal solutions can be obtained by directly minimizing the convex functions . Experimental results on public datasets and from nine benchmark methods consistently demonstrate the effectiveness of the proposed method both visually and quantitatively.},
  archive      = {J_PR},
  author       = {Pengliang Li and Junli Liang and Miaohua Zhang and Wen Fan and Guoyang Yu},
  doi          = {10.1016/j.patcog.2021.108506},
  journal      = {Pattern Recognition},
  pages        = {108506},
  shortjournal = {Pattern Recognition},
  title        = {Joint image denoising with gradient direction and edge-preserving regularization},
  volume       = {125},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised cross-domain person re-identification by
instance and distribution alignment. <em>PR</em>, <em>124</em>, 108514.
(<a href="https://doi.org/10.1016/j.patcog.2021.108514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing person re-identification (re-id) methods assume supervised model training on a separate large set of training samples from the target domain. While performing well in the training domain, such trained models are seldom generalisable to a new independent unsupervised target domain without further labelled training data from the target domain. To solve this scalability limitation, we develop a novel Hierarchical Unsupervised Domain Adaptation (HUDA) method. It can transfer labelled information of an existing dataset (a source domain) to an unlabelled target domain for unsupervised person re-id. Specifically, HUDA is designed to model jointly global distribution alignment and local instance alignment in a two-level hierarchy for discovering transferable source knowledge in unsupervised domain adaptation. Crucially, this approach aims to overcome the under-constrained learning problem of existing unsupervised domain adaptation methods. Extensive evaluations show the superiority of HUDA for unsupervised cross-domain person re-id over a wide variety of state-of-the-art methods on four re-id benchmarks: Market-1501, DukeMTMC, MSMT17 and CUHK03.},
  archive      = {J_PR},
  author       = {Xu Lan and Xiatian Zhu and Shaogang Gong},
  doi          = {10.1016/j.patcog.2021.108514},
  journal      = {Pattern Recognition},
  pages        = {108514},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised cross-domain person re-identification by instance and distribution alignment},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FW-SMOTE: A feature-weighted oversampling approach for
imbalanced classification. <em>PR</em>, <em>124</em>, 108511. (<a
href="https://doi.org/10.1016/j.patcog.2021.108511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Synthetic Minority Over-sampling Technique (SMOTE) is a well-known resampling strategy that has been successfully used for dealing with the class-imbalance problem, one of the most challenging pattern recognition tasks in the last two decades. In this work, we claim that SMOTE has an important issue when defining the neighborhood in order to create new minority samples: the use of the Euclidean distance may not be suitable in high-dimensional settings. Our hypothesis is that the use of a weighted metric that does not assume that all features are equally important could improve performance in the presence of noisy/redundant variables. In this line, we present a novel SMOTE-like method that uses the weighted Minkowski distance for defining the neighborhood for each example of the minority class. This methodology leads to a better definition of the neighborhood since it prioritizes those features that are more relevant for the classification task . A complementary advantage of the proposal is performing feature selection since attributes can be discarded when their corresponding weights are below a given threshold. Our experiments on 42 class-imbalance datasets show the virtues of the proposed SMOTE variant, achieving the best predictive performance when compared with the traditional SMOTE approach and other recent variants on low- and high-dimensional settings, handling issues such as class overlap and hubness adequately without increasing the complexity of the method.},
  archive      = {J_PR},
  author       = {Sebastián Maldonado and Carla Vairetti and Alberto Fernandez and Francisco Herrera},
  doi          = {10.1016/j.patcog.2021.108511},
  journal      = {Pattern Recognition},
  pages        = {108511},
  shortjournal = {Pattern Recognition},
  title        = {FW-SMOTE: A feature-weighted oversampling approach for imbalanced classification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring semantic segmentation of related subclasses from a
superset of classes. <em>PR</em>, <em>124</em>, 108509. (<a
href="https://doi.org/10.1016/j.patcog.2021.108509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a very important topic in the field of computer vision . We present a method for semantic segmentation of selected stuff classes from a superset of classes. We show that in situations where only select stuff classes are required if we group them as per a strategy then it can attain much higher accuracy than the models trained on the original dataset with all classes intact. The COCO-Stuff Dataset is used for demonstrating the aforesaid strategy. For training purposes, the DeepLabv3+ with Mobilenet-v2 architecture is used. We have achieved an 80.2 percent mean Intersection over Union (mIoU) on these selected classes. We also refine the masks using Learning/Computer Vision (CV) methods and hence obtain better visualization results as compared to the existing DeepLabv3+ results.},
  archive      = {J_PR},
  author       = {Kunjal Shah and Gururaj Bhat},
  doi          = {10.1016/j.patcog.2021.108509},
  journal      = {Pattern Recognition},
  pages        = {108509},
  shortjournal = {Pattern Recognition},
  title        = {Exploring semantic segmentation of related subclasses from a superset of classes},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introduction to conformal predictors. <em>PR</em>,
<em>124</em>, 108507. (<a
href="https://doi.org/10.1016/j.patcog.2021.108507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to provide a compact but accessible introduction to Conformal Predictors (CP), a Machine Learning method with the distinguishing property of producing predictions that exhibit a chosen error rate. This property, referred to as validity, is backed by not only asymptotic, but also finite-sample probabilistic guarantees. CPs differ from the conventional approach to prediction in that they introduce hedging in the form of set-valued predictions. The CP validity guarantees do not require assumptions such as priors, but are of broad applicability as they rely solely on exchangeability . The CP framework is universal in the sense that it operates on top of virtually any Machine Learning method. In addition to the formal definition, this introduction discusses CP variants that can be computed efficiently (Inductive or “split” CP) or that are suitable for imbalanced data sets (class-conditional CP). Finally, a short survey of the field provides references for relevant research and highlights the variety of domains in which CPs have found valuable application.},
  archive      = {J_PR},
  author       = {Paolo Toccaceli},
  doi          = {10.1016/j.patcog.2021.108507},
  journal      = {Pattern Recognition},
  pages        = {108507},
  shortjournal = {Pattern Recognition},
  title        = {Introduction to conformal predictors},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mask encoding: A general instance mask representation for
object segmentation. <em>PR</em>, <em>124</em>, 108505. (<a
href="https://doi.org/10.1016/j.patcog.2021.108505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is one of the most challenging tasks in computer vision , which requires separating each instance in pixels. To date, a low-resolution binary mask is the dominant paradigm for representation of instance mask. For example, the size of the predicted mask in Mask R-CNN is usually 28 × 28 28×28 . Generally, a low-resolution mask can not capture the object details well, while a high-resolution mask dramatically increases the training complexity. In this work, we propose a flexible and effective approach to encode the high-resolution structured mask to the compact representation which shares the advantages of high-quality and low-complexity. The proposed mask representation can be easily integrated into two-stage pipelines such as Mask R-CNN, improving mask AP by 0.9\% on the COCO dataset, 1.4\% on the LVIS dataset, and 2.1\% on the Cityscapes dataset. Moreover, a novel single shot instance segmentation framework can be constructed by extending the existing one-stage detector with a mask branch for this instance representation. Our model shows its superiority over the explicit contour-based pipelines in accuracy with similar computational complexity . We also evaluate our method for video instance segmentation, achieving promising results on YouTube-VIS dataset. Code is available at: https://git.io/AdelaiDet},
  archive      = {J_PR},
  author       = {Rufeng Zhang and Tao Kong and Xinlong Wang and Mingyu You},
  doi          = {10.1016/j.patcog.2021.108505},
  journal      = {Pattern Recognition},
  pages        = {108505},
  shortjournal = {Pattern Recognition},
  title        = {Mask encoding: A general instance mask representation for object segmentation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly-supervised semantic segmentation with superpixel
guided local and global consistency. <em>PR</em>, <em>124</em>, 108504.
(<a href="https://doi.org/10.1016/j.patcog.2021.108504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation task aims to learn a segmentation model with only image-level annotations. Existing methods generally refine the initial seeds to obtain pseudo labels for training a fully supervised model. In recent years, some affinity-based methods perform well in this task. However, most of these methods only focus on the localization information from class activation map, while ignoring rule-based appearance information. In this paper, we find that the superpixel guidance is helpful for mining semantic affinities between pixels because pixels belonging to the same superpixel often have the same class label. As such, we propose a Superpixel Guided Weakly Segmentation framework, which alternately learns two modules to fuse superpixel information and localization information. The semantic segmentation results are more consistent with the image’s local and global consistency through our framework. Experiments show that the proposed method achieves state-of-the-art performance, with mIoU at 70.5\% on the PASCAL VOC 2012 test set and mIoU at 34.4\% on the MS-COCO 2014 val set.},
  archive      = {J_PR},
  author       = {Sheng Yi and Huimin Ma and Xiang Wang and Tianyu Hu and Xi Li and Yu Wang},
  doi          = {10.1016/j.patcog.2021.108504},
  journal      = {Pattern Recognition},
  pages        = {108504},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised semantic segmentation with superpixel guided local and global consistency},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying players in broadcast videos using graph
convolutional network. <em>PR</em>, <em>124</em>, 108503. (<a
href="https://doi.org/10.1016/j.patcog.2021.108503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The person representation problem is a critical bottleneck in the player identification task. However, the current approaches for player identification utilizing the entire image features only are not sufficient to preserve identities due to the reliance on visible visual representations. In this paper, we propose a novel player representation method using a graph-powered pose representation to resolve this bottleneck problem. Our framework consists of three modules: (i.) a novel pose-guided representation module that is able to capture the pose changes dynamically and their associated effects; (ii.) a pose-guided graph embedding module using both the image deep features and the pose structure information for a better player representation inference; (iii.) an identification module as a player classifier. Experiment results on the real-world sport game scenarios demonstrate that our method achieves state-of-the-art identification performance, together with a better player representation.},
  archive      = {J_PR},
  author       = {Tao Feng and Kaifan Ji and Ang Bian and Chang Liu and Jianzhou Zhang},
  doi          = {10.1016/j.patcog.2021.108503},
  journal      = {Pattern Recognition},
  pages        = {108503},
  shortjournal = {Pattern Recognition},
  title        = {Identifying players in broadcast videos using graph convolutional network},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage aware attentional siamese network for visual
tracking. <em>PR</em>, <em>124</em>, 108502. (<a
href="https://doi.org/10.1016/j.patcog.2021.108502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese networks have achieved great success in visual tracking with the advantages of speed and accuracy. However, how to track an object precisely and robustly still remains challenging. One reason is that multiple types of features are required to achieve good precision and robustness, which are unattainable by a single training phase. Moreover, Siamese networks usually struggle with online adaption problem. In this paper, we present a novel two-stage aware attentional Siamese network for tracking (Ta-ASiam). Concretely, we first propose a position-aware and an appearance-aware training strategy to optimize different layers of Siamese network. By introducing diverse training patterns, two types of required features can be captured simultaneously. Then, following the rule of feature distribution, an effective feature selection module is constructed by combining both channel and spatial attention networks to adapt to rapid appearance changes of the object. Extensive experiments on various latest benchmarks have well demonstrated the effectiveness of our method, which significantly outperforms state-of-the-art trackers.},
  archive      = {J_PR},
  author       = {Xinglong Sun and Guangliang Han and Lihong Guo and Hang Yang and Xiaotian Wu and Qingqing Li},
  doi          = {10.1016/j.patcog.2021.108502},
  journal      = {Pattern Recognition},
  pages        = {108502},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage aware attentional siamese network for visual tracking},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Super-resolution semantic segmentation with relation
calibrating network. <em>PR</em>, <em>124</em>, 108501. (<a
href="https://doi.org/10.1016/j.patcog.2021.108501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve high-resolution segmentation results, typical semantic segmentation models often require high-resolution inputs. However, high-resolution inputs inevitably bring high cost on computation, which limits its application seriously in realistic scenarios. To address the problem, we propose to predict a high-resolution semantic segmentation result with a degraded low-resolution image as input, which is called super-resolution semantic segmentation in this paper. We further propose a Relation Calibrating Network (RCNet) for this task. Specifically, we propose two modules, namely Relation Upsampling Module (RUM) and Feature Calibrating Module (FCM). In RUM, the input feature map generates the relation map of pixels in low-resolution, which is then gradually upsampled to high-resolution. Meanwhile, FCM takes the input feature map and the relation map from RUM as inputs, gradually calibrating the feature. Finally, the last FCM outputs the high-resolution segmentation results. We conduct extensive experiments to verify the effectiveness of our method. Specially, we achieve a comparable segmentation result (from 70.01\% to 70.90\%) with only 1/4 of the computational cost (from 1107.57 to 255.72 GFLOPs) based on FCN on Cityscapes dataset.},
  archive      = {J_PR},
  author       = {Jie Jiang and Jing Liu and Jun Fu and Weining Wang and Hanqing Lu},
  doi          = {10.1016/j.patcog.2021.108501},
  journal      = {Pattern Recognition},
  pages        = {108501},
  shortjournal = {Pattern Recognition},
  title        = {Super-resolution semantic segmentation with relation calibrating network},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing a generic framework for anomaly detection.
<em>PR</em>, <em>124</em>, 108500. (<a
href="https://doi.org/10.1016/j.patcog.2021.108500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of one-class classifiers (OCCs) has been shown to exhibit promising performance in a variety of machine learning applications. The ability to assess the similarity or correlation between the output of various OCCs is an important prerequisite for building of a meaningful OCCs ensemble . However, this aspect of the OCC fusion problem has been mostly ignored so far. In this paper, we propose a new method of constructing a fusion of OCCs with three contributions: (a) As a key contribution, enabling an OCC ensemble design using exclusively non anomalous samples, we propose a novel fitness function to evaluate the competency of OCCs without requiring samples from the anomalous class; (b) As a minor, but impactful contribution, we investigate alternative forms of score normalisation of OCCs, and identify a novel two-sided normalisation method as the best in coping with long tail non anomalous data distributions; (c) In the context of building our proposed OCC fusion system based on the weighted averaging approach, we find that the weights optimised using a particle swarm optimisation algorithm produce the most effective solution. We evaluate the merits of the proposed method on 15 benchmarking datasets from different application domains including medical, anti-spam and face spoofing detection. The comparison of the proposed approach with state-of-the-art methods alongside the statistical analysis confirm the effectiveness of the proposed model.},
  archive      = {J_PR},
  author       = {Soroush Fatemifar and Muhammad Awais and Ali Akbari and Josef Kittler},
  doi          = {10.1016/j.patcog.2021.108500},
  journal      = {Pattern Recognition},
  pages        = {108500},
  shortjournal = {Pattern Recognition},
  title        = {Developing a generic framework for anomaly detection},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COVID-MTL: Multitask learning with Shift3D and
random-weighted loss for COVID-19 diagnosis and severity assessment.
<em>PR</em>, <em>124</em>, 108499. (<a
href="https://doi.org/10.1016/j.patcog.2021.108499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an urgent need for automated methods to assist accurate and effective assessment of COVID-19. Radiology and nucleic acid test (NAT) are complementary COVID-19 diagnosis methods. In this paper, we present an end-to-end multitask learning (MTL) framework (COVID-MTL) that is capable of automated and simultaneous detection (against both radiology and NAT) and severity assessment of COVID-19. COVID-MTL learns different COVID-19 tasks in parallel through our novel random-weighted loss function, which assigns learning weights under Dirichlet distribution to prevent task dominance; our new 3D real-time augmentation algorithm (Shift3D) introduces space variances for 3D CNN components by shifting low-level feature representations of volumetric inputs in three dimensions; thereby, the MTL framework is able to accelerate convergence and improve joint learning performance compared to single-task models. By only using chest CT scans, COVID-MTL was trained on 930 CT scans and tested on separate 399 cases. COVID-MTL achieved AUCs of 0.939 and 0.846, and accuracies of 90.23\% and 79.20\% for detection of COVID-19 against radiology and NAT, respectively, which outperformed the state-of-the-art models. Meanwhile, COVID-MTL yielded AUC of 0.800 ± 0.020 and 0.813 ± 0.021 (with transfer learning) for classifying control/suspected, mild/regular, and severe/critically-ill cases. To decipher the recognition mechanism, we also identified high-throughput lung features that were significantly related ( P &lt; 0.001) to the positivity and severity of COVID-19.},
  archive      = {J_PR},
  author       = {Guoqing Bao and Huai Chen and Tongliang Liu and Guanzhong Gong and Yong Yin and Lisheng Wang and Xiuying Wang},
  doi          = {10.1016/j.patcog.2021.108499},
  journal      = {Pattern Recognition},
  pages        = {108499},
  shortjournal = {Pattern Recognition},
  title        = {COVID-MTL: Multitask learning with Shift3D and random-weighted loss for COVID-19 diagnosis and severity assessment},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty estimation for stereo matching based on
evidential deep learning. <em>PR</em>, <em>124</em>, 108498. (<a
href="https://doi.org/10.1016/j.patcog.2021.108498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning-based stereo matching approaches have achieved excellent performance in recent years, it is still a non-trivial task to estimate the uncertainty of the produced disparity map . In this paper, we propose a novel approach to estimate both aleatoric and epistemic uncertainties for stereo matching in an end-to-end way. We introduce an evidential distribution, named Normal Inverse-Gamma (NIG) distribution, whose parameters can be used to calculate the uncertainty. Instead of directly regressed from aggregated features, the uncertainty parameters are predicted for each potential disparity and then averaged via the guidance of matching probability distribution. Furthermore, considering the sparsity of ground truth in real scene datasets, we design two additional losses. The first one tries to enlarge uncertainty on incorrect predictions, so uncertainty becomes more sensitive to erroneous regions. The second one enforces the smoothness of the uncertainty in the regions with smooth disparity. Most stereo matching models, such as PSM-Net, GA-Net, and AA-Net, can be easily integrated with our approach. Experiments on multiple benchmark datasets show that our method improves stereo matching results. We prove that both aleatoric and epistemic uncertainties are well-calibrated with incorrect predictions. Particularly, our method can capture increased epistemic uncertainty on out-of-distribution data, making it effective to prevent a system from potential fatal consequences. Code is available at https://github.com/Dawnstar8411/StereoMatching-Uncertainty .},
  archive      = {J_PR},
  author       = {Chen Wang and Xiang Wang and Jiawei Zhang and Liang Zhang and Xiao Bai and Xin Ning and Jun Zhou and Edwin Hancock},
  doi          = {10.1016/j.patcog.2021.108498},
  journal      = {Pattern Recognition},
  pages        = {108498},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty estimation for stereo matching based on evidential deep learning},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive polarization based reflection removal via
realistic training data generation. <em>PR</em>, <em>124</em>, 108497.
(<a href="https://doi.org/10.1016/j.patcog.2021.108497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reflection effect is unavoidable when taking photos through glasses or other transparent materials, which introduces undesired information into pictures. Hence, removing the influence of reflection becomes a key problem in computer vision . One of the main obstacles of recent learning based approaches is the lacking of realistic training data. To address this issue, we introduce a new dataset synthesis method as well as a novel neural network architecture for single image reflection removal. First, we make use of the polarization characteristics of light into the synthesis of datasets, so as to obtain more realistic and diversified training dataset POL . Then, we design a novel Progressive Polarization based Reflection Removal Network ( P 2 R 2 P2R2 Net), which preliminary estimates the coarse background layer to guide the final reflection removal. We demonstrate that our method performs better than the state-of-the-art single image reflection removal methods through quantitative and qualitative experimental comparisons. Specifically, the average PSNR of our restored images selected from three representative benchmark datesets: “Real20”, “ SI R 2 SIR2 ” and “Nature” is improved at least 0.49 compared with existing methods and reaches to 24.52.},
  archive      = {J_PR},
  author       = {Youxin Pang and Mengke Yuan and Qiang Fu and Peiran Ren and Dong-Ming Yan},
  doi          = {10.1016/j.patcog.2021.108497},
  journal      = {Pattern Recognition},
  pages        = {108497},
  shortjournal = {Pattern Recognition},
  title        = {Progressive polarization based reflection removal via realistic training data generation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive gabor convolutional networks. <em>PR</em>,
<em>124</em>, 108495. (<a
href="https://doi.org/10.1016/j.patcog.2021.108495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great breakthroughs that deep convolutional neural networks (DCNNs) have achieved on image representation learning in recent years, they lack the ability to extract invariant information from images. On the other hand, several traditional feature extractors like Gabor filters are widely used for invariant information learning from images. In this paper, we propose a new class of DCNNs named adaptive Gabor convolutional networks (AGCNs). In the AGCNs, the convolutional kernels are adaptively multiplied by Gabor filters to construct the Gabor convolutional filters (GCFs), while the parameters in the Gabor functions (i.e., scale and orientation) are learned alongside those in the convolutional kernels. In addition, the GCFs can be regenerated after updating the Gabor filters and convolutional kernels. We evaluate the performance of the proposed AGCNs on image classification using five benchmark image datasets, i.e., MNIST and its rotated version, SVHN, CIFAR-10, CINIC-10, and DogsVSCats. Experimental results show that the AGCNs are robust to spatial transformations and have achieved higher accuracy compared with the DCNNs and other state-of-the-art deep networks. Moreover, the GCFs can be easily embedded into any classical DCNN models (e.g., ResNet) and require fewer parameters than the corresponding DCNNs.},
  archive      = {J_PR},
  author       = {Ye Yuan and Li-Na Wang and Guoqiang Zhong and Wei Gao and Wencong Jiao and Junyu Dong and Biao Shen and Dongdong Xia and Wei Xiang},
  doi          = {10.1016/j.patcog.2021.108495},
  journal      = {Pattern Recognition},
  pages        = {108495},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive gabor convolutional networks},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection and rectification of arbitrary shaped scene texts
by using text keypoints and links. <em>PR</em>, <em>124</em>, 108494.
(<a href="https://doi.org/10.1016/j.patcog.2021.108494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and recognition of scene texts of arbitrary shapes remain a grand challenge due to the super-rich text shape variation in text line orientations, lengths, curvatures, etc. This paper presents a mask-guided multi-task network that detects and rectifies scene texts of arbitrary shapes reliably. Three types of keypoints are detected which specify the centre line and so the shape of text instances accurately. In addition, four types of keypoint links are detected of which the horizontal links associate the detected keypoints of each text instance and the vertical links predict a pair of landmark points (for each keypoint) along the upper and lower text boundary, respectively. Scene texts can be located and rectified by linking up the associated landmark points (giving localization polygon boxes) and transforming the polygon boxes via thin plate spline, respectively. Extensive experiments over several public datasets show that the use of text keypoints is tolerant to the variation in text orientations, lengths, and curvatures, and it achieves competitive scene text detection and rectification performance as compared with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chuhui Xue and Shijian Lu and Steven Hoi},
  doi          = {10.1016/j.patcog.2021.108494},
  journal      = {Pattern Recognition},
  pages        = {108494},
  shortjournal = {Pattern Recognition},
  title        = {Detection and rectification of arbitrary shaped scene texts by using text keypoints and links},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An analysis of heuristic metrics for classifier ensemble
pruning based on ordered aggregation. <em>PR</em>, <em>124</em>, 108493.
(<a href="https://doi.org/10.1016/j.patcog.2021.108493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifier ensemble pruning is a strategy through which a subensemble can be identified via optimizing a predefined performance criterion. Choosing the optimum or suboptimum subensemble decreases the initial ensemble size and increases its predictive performance . In this article, a set of heuristic metrics will be analyzed to guide the pruning process. The analyzed metrics are based on modifying the order of the classifiers in the bagging algorithm, with selecting the first set in the queue. Some of these criteria include general accuracy, the complementarity of decisions, ensemble diversity, the margin of samples, minimum redundancy , discriminant classifiers, and margin hybrid diversity. The efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers , and the number of classes. While the efficiency is measured in terms of the computational cost and the memory space requirements. The performance of those metrics is assessed over fifteen binary and fifteen multiclass benchmark classification tasks, respectively. In addition, the behavior of those metrics against randomness is measured in terms of the distribution of their accuracy around the median. Results show that ordered aggregation is an efficient strategy to generate subensembles that improve both predictive performance as well as computational and memory complexities of the whole bagging ensemble.},
  archive      = {J_PR},
  author       = {Amgad M. Mohammed and Enrique Onieva and Michał Woźniak and Gonzalo Martínez-Muñoz},
  doi          = {10.1016/j.patcog.2021.108493},
  journal      = {Pattern Recognition},
  pages        = {108493},
  shortjournal = {Pattern Recognition},
  title        = {An analysis of heuristic metrics for classifier ensemble pruning based on ordered aggregation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised node classification via adaptive graph
smoothing networks. <em>PR</em>, <em>124</em>, 108492. (<a
href="https://doi.org/10.1016/j.patcog.2021.108492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspections on current graph neural networks suggest us to reconsider the computational aspect of the final aggregation. We consider that such aggregations perform a prediction smoothing and impute their potential drawbacks to be the inter-class interference implied by the underlying graphs. We aim at weakening the inter-class connections so that aggregations focus more on intra-class relations and producing smooth predictions according to weakening results. We apply a metric learning module to learn new edge weights and combine entropy losses to ensure the correspondence between the predictions and the learnt distances so that the weights of inter-class edges are reduced and predictions are smoothed according to the modified graph. Experiments on four citation networks and a Wiki network show that in comparison with other state-of-the-art graph neural networks, the proposed algorithm can improve the classification accuracy .},
  archive      = {J_PR},
  author       = {Ruigang Zheng and Weifu Chen and Guocan Feng},
  doi          = {10.1016/j.patcog.2021.108492},
  journal      = {Pattern Recognition},
  pages        = {108492},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised node classification via adaptive graph smoothing networks},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relevance attack on detectors. <em>PR</em>, <em>124</em>,
108491. (<a href="https://doi.org/10.1016/j.patcog.2021.108491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20\%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation , i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.},
  archive      = {J_PR},
  author       = {Sizhe Chen and Fan He and Xiaolin Huang and Kun Zhang},
  doi          = {10.1016/j.patcog.2021.108491},
  journal      = {Pattern Recognition},
  pages        = {108491},
  shortjournal = {Pattern Recognition},
  title        = {Relevance attack on detectors},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TradeBot: Bandit learning for hyper-parameters optimization
of high frequency trading strategy. <em>PR</em>, <em>124</em>, 108490.
(<a href="https://doi.org/10.1016/j.patcog.2021.108490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative trading takes advantage of mathematical functions for automatically making stock or futures trading decisions. Specifically, various trading strategies that proposed by human-experts are associated with weight hyper-parameters to determine the probability of selecting a specific strategy according to market conditions. Prior work manually adjusting the weight hyper-parameters is error-prone, because the essential advantage of quantitative trading, i.e., automation, is lost. In this paper, we propose a dynamic parameter tuning algorithm , i.e., TradeBot, based on bandit learning for quantitative trading. We consider sequentially selecting hyper-parameters of rules for trading as a bandit game, where a set of hyper-parameters of trading rule is considered as an action. A novel reward-agnostic Upper Confidence Bound bandit method is proposed to solve the automatically trading problem with a reward function estimated by inverse reinforcement learning. Experimental results on China Commodity Futures Market Data show state-of-the-art performance. To our best knowledge, this is one of the first work deployed in the online trading system via reinforcement learning, in published literature.},
  archive      = {J_PR},
  author       = {Weipeng Zhang and Lu Wang and Liang Xie and Ke Feng and Xiang Liu},
  doi          = {10.1016/j.patcog.2021.108490},
  journal      = {Pattern Recognition},
  pages        = {108490},
  shortjournal = {Pattern Recognition},
  title        = {TradeBot: Bandit learning for hyper-parameters optimization of high frequency trading strategy},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guided neighborhood affine subspace embedding for feature
matching. <em>PR</em>, <em>124</em>, 108489. (<a
href="https://doi.org/10.1016/j.patcog.2021.108489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching, which refers to determining reliable correspondences between two sets of feature points, is a fundamental component of numerous visual tasks. This paper proposes a novel method, termed as guided neighborhood affine subspace embedding (NASE), to eliminate false matches from the given tentative feature matches. Its essential philosophy is to preserve the underlying intrinsic manifold of potential true matches. Specifically, we aim to approximate the manifold of an inlier with an affine subspace fitted on its neighbors by imposing a motion-consistency constraint. Considering that the “corresponding manifold” of inliers may be biased by gross outliers, we introduce a density-based seed point selection strategy for neighborhood refinement. Based on the above two strategies, we further formulate the general feature matching problem into a mathematical optimization model and deduce a closed-form solution with linearithmic time complexity ( i.e. , O ( N log N ) O(NlogN) ) for mismatch removal. Additionally, we devise a multi-scale strategy for neighborhood construction, making our method more robust to various degradations. Extensive experiments on general feature matching, fundamental matrix estimation, and loop closure detection demonstrate the clear superiority of NASE over the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Zizhuo Li and Yong Ma and Xiaoguang Mei and Jun Huang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2021.108489},
  journal      = {Pattern Recognition},
  pages        = {108489},
  shortjournal = {Pattern Recognition},
  title        = {Guided neighborhood affine subspace embedding for feature matching},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep open-set recognition for silicon wafer production
monitoring. <em>PR</em>, <em>124</em>, 108488. (<a
href="https://doi.org/10.1016/j.patcog.2021.108488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The chips contained in any electronic device are manufactured over circular silicon wafers , which are monitored by inspection machines at different production stages. Inspection machines detect and locate any defect within the wafer and return a Wafer Defect Map (WDM), i.e., a list of the coordinates where defects lie, which can be considered a huge, sparse, and binary image . In normal conditions, wafers exhibit a small number of randomly distributed defects, while defects grouped in specific patterns might indicate known or novel categories of failures in the production line. Needless to say, a primary concern of semiconductor industries is to identify these patterns and intervene as soon as possible to restore normal production conditions. Here we address WDM monitoring as an open-set recognition problem, where the aim is to classify WDM in known categories and promptly detect novel patterns. In particular, we propose a comprehensive pipeline for wafer monitoring based on a Submanifold Sparse Convolutional Network , a deep architecture designed to process sparse data at an arbitrary resolution, which is trained on the known classes. To detect novelties, we define an outlier detector based on a Gaussian Mixture Model fitted on the latent representation of the classifier. Our experiments on a real dataset of WDMs show that directly processing full-resolution WDMs by Submanifold Sparse Convolutions yields superior classification performance on known classes than traditional Convolutional Neural Networks , which require a preliminary binning to reduce the size of the binary images representing WDMs. Moreover, our solution outperforms state-of-the-art open-set recognition solutions in novelty detection.},
  archive      = {J_PR},
  author       = {Luca Frittoli and Diego Carrera and Beatrice Rossi and Pasqualina Fragneto and Giacomo Boracchi},
  doi          = {10.1016/j.patcog.2021.108488},
  journal      = {Pattern Recognition},
  pages        = {108488},
  shortjournal = {Pattern Recognition},
  title        = {Deep open-set recognition for silicon wafer production monitoring},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action transformer: A self-attention model for short-time
pose-based human action recognition. <em>PR</em>, <em>124</em>, 108487.
(<a href="https://doi.org/10.1016/j.patcog.2021.108487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks based purely on attention have been successful across several domains, relying on minimal architectural priors from the designer. In Human Action Recognition (HAR), attention mechanisms have been primarily adopted on top of standard convolutional or recurrent layers, improving the overall generalization capability. In this work, we introduce Action Transformer (AcT), a simple, fully, self-attentional architecture that consistently outperforms more elaborated networks that mix convolutional, recurrent , and attentive layers. In order to limit computational and energy requests, building on previous human action recognition research, the proposed approach exploits 2D pose representations over small temporal windows, providing a low latency solution for accurate and effective real-time performance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as an attempt to build a formal training and evaluation benchmark for real-time, short-time HAR. The proposed methodology was extensively tested on MPOSE2021 and compared to several state-of-the-art architectures, proving the effectiveness of the AcT model and laying the foundations for future work on HAR.},
  archive      = {J_PR},
  author       = {Vittorio Mazzia and Simone Angarano and Francesco Salvetti and Federico Angelini and Marcello Chiaberge},
  doi          = {10.1016/j.patcog.2021.108487},
  journal      = {Pattern Recognition},
  pages        = {108487},
  shortjournal = {Pattern Recognition},
  title        = {Action transformer: A self-attention model for short-time pose-based human action recognition},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse CapsNet with explicit regularizer. <em>PR</em>,
<em>124</em>, 108486. (<a
href="https://doi.org/10.1016/j.patcog.2021.108486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule Network (CapsNet) achieves great improvements in recognizing pose and deformation through a novel encoding mode. However, it carries a large number of parameters, leading to the challenge of heavy memory and computational cost. To solve this problem, we propose sparse CapsNet with an explicit regularizer in this paper. To our knowledge, it’s the first work that utilizes sparse optimization to compress CapsNet. Specifically, to reduce unnecessary weight parameters, we first introduce the component-wise absolute value regularizer into the objective function of CapsNet based on zero-means Laplacian prior. Then, to reduce the computational cost and speed up CapsNet, the weight parameters are further grouped by 2D filters and sparsified by 1-norm regularization . To train our model efficiently, a new stochastic proximal gradient algorithm, which has analytical solutions at each iteration, is presented. Extensive numerical experiments on four commonly used datasets validate the effectiveness and efficiency of the proposed method.},
  archive      = {J_PR},
  author       = {Ruiyang Shi and Lingfeng Niu and Ruizhi Zhou},
  doi          = {10.1016/j.patcog.2021.108486},
  journal      = {Pattern Recognition},
  pages        = {108486},
  shortjournal = {Pattern Recognition},
  title        = {Sparse CapsNet with explicit regularizer},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene-specific crowd counting using synthetic training
images. <em>PR</em>, <em>124</em>, 108484. (<a
href="https://doi.org/10.1016/j.patcog.2021.108484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is a computer vision task on which considerable progress has recently been made thanks to convolutional neural networks . However, it remains a challenging task even in scene-specific settings, in real-world application scenarios where no representative images of the target scene are available, not even unlabelled, for training or fine-tuning a crowd counting model. Inspired by previous work in other computer vision tasks, we propose a simple but effective solution for the above application scenario, which consists of automatically building a scene-specific training set of synthetic images. Our solution does not require from end-users any manual annotation effort nor the collection of representative images of the target scene. Extensive experiments on several benchmark data sets show that the proposed solution can improve the effectiveness of existing crowd counting methods.},
  archive      = {J_PR},
  author       = {Rita Delussu and Lorenzo Putzu and Giorgio Fumera},
  doi          = {10.1016/j.patcog.2021.108484},
  journal      = {Pattern Recognition},
  pages        = {108484},
  shortjournal = {Pattern Recognition},
  title        = {Scene-specific crowd counting using synthetic training images},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning interlaced sparse sinkhorn matching network for
video super-resolution. <em>PR</em>, <em>124</em>, 108475. (<a
href="https://doi.org/10.1016/j.patcog.2021.108475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively fuse inter- and intra-frame spatio-temporal information plays a key role in video super-resolution (VSR). Most existing works rely heavily on the accuracy of motion estimation and compensation for spatio-temporal feature alignment. However, they cannot perform well when suffering from large-scale and complex motions. To this end, this paper introduces an efficient and effective Interlaced Sparse Sinkhorn Matching (ISSM) network for VSR, which aligns supporting frames with the reference one in the feature space by learning optimal matching between image regions across frames. Specifically, the ISSM divides the input dense affinity matrix into two sparse block matrixes : one can match long-distance regions while the other can match short-distance regions, and then we leverage an efficient Sinkhorn method on each block to learn optimal matching. Moreover, we insert a residual atrous spatial pyramid pooling module before the ISSM, which can flexibly generate multi-scale features frame by frame to capture the multi-scale context information in images. The aligned features of each adjacent frame are then fed to a bidirectional temporal fusion module to capture the rich temporal information. Finally, the fused features are sent into a frame-wise dynamic reconstruction network to produce an HR frame. Extensive evaluations on three benchmark datasets demonstrate the superiority of our method over the state-of-the-art methods in terms of PSNR and SSIM.},
  archive      = {J_PR},
  author       = {Huihui Song and Yutong Jin and Yong Cheng and Bo Liu and Dong Liu and Qingshan Liu},
  doi          = {10.1016/j.patcog.2021.108475},
  journal      = {Pattern Recognition},
  pages        = {108475},
  shortjournal = {Pattern Recognition},
  title        = {Learning interlaced sparse sinkhorn matching network for video super-resolution},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated search space and search strategy selection for
AutoML. <em>PR</em>, <em>124</em>, 108474. (<a
href="https://doi.org/10.1016/j.patcog.2021.108474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing works on Automated Machine Learning (AutoML) are mainly based on predefined search space. This paper seeks synergetic automation of two ingredients, i.e., search space and search strategies. Specifically, we formulate the automation of search space and search strategies as a combinatorial optimization problem . Our empirical study on many architecture benchmarks shows that identifying the suitable search space exerts more effect than choosing a sophisticated search strategy. Motivated by this, we attempt to leverage a machine learning method to solve the discrete optimization problem, and thus develop a Layered Architecture Search Tree (LArST) approach to synergize these two components. In addition, we use a probe model-based method to extract dataset-wise features, i.e., meta-features, which is able to facilitate the estimation of proper search space and search strategy for a given task. Experimental results show the efficacy of our approach under different search mechanisms and various datasets and hardware platforms.},
  archive      = {J_PR},
  author       = {Chao Xue and Mengting Hu and Xueqi Huang and Chun-Guang Li},
  doi          = {10.1016/j.patcog.2021.108474},
  journal      = {Pattern Recognition},
  pages        = {108474},
  shortjournal = {Pattern Recognition},
  title        = {Automated search space and search strategy selection for AutoML},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-restrained triplet loss for accurate masked face
recognition. <em>PR</em>, <em>124</em>, 108473. (<a
href="https://doi.org/10.1016/j.patcog.2021.108473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using the face as a biometric identity trait is motivated by the contactless nature of the capture process and the high accuracy of the recognition algorithms . After the current COVID-19 pandemic, wearing a face mask has been imposed in public places to keep the pandemic under control. However, face occlusion due to wearing a mask presents an emerging challenge for face recognition systems. In this paper, we present a solution to improve masked face recognition performance. Specifically, we propose the Embedding Unmasking Model (EUM) operated on top of existing face recognition models. We also propose a novel loss function, the Self-restrained Triplet (SRT), which enabled the EUM to produce embeddings similar to these of unmasked faces of the same identities. The achieved evaluation results on three face recognition models, two real masked datasets, and two synthetically generated masked face datasets proved that our proposed approach significantly improves the performance in most experimental settings.},
  archive      = {J_PR},
  author       = {Fadi Boutros and Naser Damer and Florian Kirchbuchner and Arjan Kuijper},
  doi          = {10.1016/j.patcog.2021.108473},
  journal      = {Pattern Recognition},
  pages        = {108473},
  shortjournal = {Pattern Recognition},
  title        = {Self-restrained triplet loss for accurate masked face recognition},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Semi-supervised robust training with generalized perturbed
neighborhood. <em>PR</em>, <em>124</em>, 108472. (<a
href="https://doi.org/10.1016/j.patcog.2021.108472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have been shown to be a severe threat to deep neural networks (DNNs). One of the most effective adversarial defense methods is adversarial training (AT) through minimizing the adversarial risk R a d v Radv , which encourages both the benign example x x and its adversarially perturbed neighborhoods within the ℓ p ℓp -ball to be predicted as the ground-truth label. In this paper, we propose a novel defense method, the robust training (RT), by jointly minimizing two separated risks ( i . e . i.e. , R s t a n d Rstand and R r o b Rrob ), which are with respect to the benign example and its neighborhoods, respectively. The motivation is to explicitly and jointly enhance the accuracy and the adversarial robustness. We prove that R a d v Radv is upper-bounded by R s t a n d + R r o b Rstand+Rrob , which implies that RT has similar effect as AT. Intuitively, minimizing the standard risk enforces the benign example to be correctly predicted, while the robust risk minimization encourages the predictions of the neighbor examples to be consistent with the prediction of the benign example. Besides, since R r o b Rrob is independent of the ground-truth label, RT is naturally extended to the semi-supervised mode ( i . e . i.e. , SRT), to further enhance its effectiveness. Moreover, we extend the ℓ p ℓp -bounded neighborhood to a general case, which covers different types of perturbations, such as the pixel-wise ( i . e . i.e. , x + δ x+δ ) or the spatial perturbation ( i . e . i.e. , A x + b Ax+b ). Extensive experiments on benchmark datasets not only verify the superiority of the proposed SRT to state-of-the-art methods for defending pixel-wise or spatial perturbations separately but also demonstrate its robustness to both perturbations simultaneously. Our work may shed the light on the understanding of universal model robustness and the potential of unlabeled samples . The code for reproducing main results is available at https://github.com/THUYimingLi/Semi-supervised_Robust_Training .},
  archive      = {J_PR},
  author       = {Yiming Li and Baoyuan Wu and Yan Feng and Yanbo Fan and Yong Jiang and Zhifeng Li and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2021.108472},
  journal      = {Pattern Recognition},
  pages        = {108472},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised robust training with generalized perturbed neighborhood},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Rapid construction of 4D high-quality microstructural image
for cement hydration using partial information registration.
<em>PR</em>, <em>124</em>, 108471. (<a
href="https://doi.org/10.1016/j.patcog.2021.108471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying on the microstructural evolution of cement paste during hydration is of considerable significance for understanding its mechanism and designing such material in cement industry. With the use of microtomography and image registration, the four-dimensional (4D) microstructure of cement paste can be captured, thereby assisting material scientists in studying the hydration process in situ . However, as a challenging task, the construction of high-quality 4D microstructural image is remarkably impeded by image size, isotropy , and homogeneity. This paper proposes an image processing framework to construct 4D high quality microstructural image rapidly for cement hydration. This framework improves and accelerates microstructural image registration and enhancement by using bias field correction , temporal intensity calibration and fast image registration. Additionally, a partial information registration method adopting partial information on the spatial and phased scales, is proposed to improve the registration speed and accuracy. Furthermore, a multi-factor multi-layer particle swarm optimization is proposed to improve the optimization in registration. Experimental results indicate that the 4D high quality microstructural image can be constructed rapidly with promising precision.},
  archive      = {J_PR},
  author       = {Liangliang Zhang and Lin Wang and Bo Yang and Sijie Niu and Yamin Han and Sung-Kwun Oh},
  doi          = {10.1016/j.patcog.2021.108471},
  journal      = {Pattern Recognition},
  pages        = {108471},
  shortjournal = {Pattern Recognition},
  title        = {Rapid construction of 4D high-quality microstructural image for cement hydration using partial information registration},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SibNet: Food instance counting and segmentation.
<em>PR</em>, <em>124</em>, 108470. (<a
href="https://doi.org/10.1016/j.patcog.2021.108470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food computing has recently attracted considerable research attention due to its significance for health risk analysis . In the literature, the majority of research efforts are dedicated to food recognition. Relatively few works are conducted for food counting and segmentation, which are essential for portion size estimation. This paper presents a deep neural network , named SibNet, for simultaneous counting and extraction of food instances from an image. The problem is challenging due to varying size and shape of food as well as arbitrary viewing angle of camera, not to mention that food instances often occlude each other. SibNet is novel for proposal of learning seed map to minimize the overlap between instances. The map facilitates counting and can be completed as an instance segmentation map that depicts the arbitrary shape and size of individual instance under occlusion. To this end, a novel sibling relation sub-network is proposed for pixel connectivity analysis. Along with this paper, three new datasets covering Western, Chinese and Japanese food are also constructed for performance evaluation. The three datasets and SibNet source code are publicly available.},
  archive      = {J_PR},
  author       = {Huu-Thanh Nguyen and Chong-Wah Ngo and Wing-Kwong Chan},
  doi          = {10.1016/j.patcog.2021.108470},
  journal      = {Pattern Recognition},
  pages        = {108470},
  shortjournal = {Pattern Recognition},
  title        = {SibNet: Food instance counting and segmentation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A zero-shot learning framework via cluster-prototype
matching. <em>PR</em>, <em>124</em>, 108469. (<a
href="https://doi.org/10.1016/j.patcog.2021.108469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the descriptions of classes, Zero-Shot Learning (ZSL) aims to recognize unseen samples by learning a projection between the visual features of samples and the semantic descriptions (prototypes) of classes from seen data. However, due to the inherent distribution gap between seen and unseen domains, the learned projection is generally biased to seen classes and may produce misleading relationships between unseen samples and prototypes (sample-prototype relationship). To tackle this problem, we propose a Cluster-Prototype Matching (CPM) framework which exploits the distribution information of samples to explore the cluster structure of samples and then use the robust cluster-prototype relationship to correct the biased sample-prototype relationship. Specifically, we first use an iterative cluster generation module to identify the underlying cluster structure of samples based on their embedding features, which are acquired via a basic ZSL model. Then each identified cluster will be matched with a specific class prototype through the Kuhn-Munkres algorithm, based on which we can export a sharp cluster-prototype similarity. Finally, the cluster-prototype similarity is combined with the sample-prototype similarity to determine the class labels of test samples. We apply CPM to five well-established ZSL methods and the experimental results show that CPM can significantly improve the performance of basic models and enable them achieve or beyond the state-of-the-art.},
  archive      = {J_PR},
  author       = {Jing Zhang and Qingyong Li and YangLi-ao Geng and Wen Wang and Wenju Sun and Chuan Shi and Zhengming Ding},
  doi          = {10.1016/j.patcog.2021.108469},
  journal      = {Pattern Recognition},
  pages        = {108469},
  shortjournal = {Pattern Recognition},
  title        = {A zero-shot learning framework via cluster-prototype matching},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CANet: Co-attention network for RGB-d semantic segmentation.
<em>PR</em>, <em>124</em>, 108468. (<a
href="https://doi.org/10.1016/j.patcog.2021.108468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating the depth (D) information to RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is not trivial due to their inherent physical meaning discrepancy, in which RGB represents RGB information but D depth information. In this paper, we propose a co-attention network (CANet) to build sound interaction between RGB and depth features. The key part in the CANet is the co-attention fusion part. It includes three modules. Specifically, the position and channel co-attention fusion modules adaptively fuse RGB and depth features in spatial and channel dimensions. An additional fusion co-attention module further integrates the outputs of the position and channel co-attention fusion modules to obtain a more representative feature which is used for the final semantic segmentation. Extensive experiments witness the effectiveness of the CANet in fusing RGB and depth features, achieving state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e. , NYUDv2 and SUN-RGBD.},
  archive      = {J_PR},
  author       = {Hao Zhou and Lu Qi and Hai Huang and Xu Yang and Zhaoliang Wan and Xianglong Wen},
  doi          = {10.1016/j.patcog.2021.108468},
  journal      = {Pattern Recognition},
  pages        = {108468},
  shortjournal = {Pattern Recognition},
  title        = {CANet: Co-attention network for RGB-D semantic segmentation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to rectify for robust learning with noisy labels.
<em>PR</em>, <em>124</em>, 108467. (<a
href="https://doi.org/10.1016/j.patcog.2021.108467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise significantly degrades the generalization ability of deep models in applications. Effective strategies and approaches ( e.g. , re-weighting or loss correction) are designed to alleviate the negative impact of label noise when training a neural network . Those existing works usually rely on the pre-specified architecture and manually tuning the additional hyper-parameters. In this paper, we propose warped probabilistic inference (WarPI) to achieve adaptively rectifying the training procedure for the classification network within the meta-learning scenario. In contrast to the deterministic models , WarPI is formulated as a hierarchical probabilistic model by learning an amortization meta-network, which can resolve sample ambiguity and be therefore more robust to serious label noise. Unlike the existing approximated weighting function of directly generating weight values from losses, our meta-network is learned to estimate a rectifying vector from the input of the logits and labels, which has the capability of leveraging sufficient information lying in them. The procedure provides an effective way to rectify the learning procedure for the classification network, demonstrating a significant improvement of the generalization ability . Besides, modeling the rectifying vector as a latent variable and learning the meta-network can be seamlessly integrated into the SGD optimization of the classification network. We evaluate WarPI on four benchmarks of robust learning with noisy labels and achieve the new state-of-the-art under variant noise types. Extensive study and analysis also demonstrate the effectiveness of our model.},
  archive      = {J_PR},
  author       = {Haoliang Sun and Chenhui Guo and Qi Wei and Zhongyi Han and Yilong Yin},
  doi          = {10.1016/j.patcog.2021.108467},
  journal      = {Pattern Recognition},
  pages        = {108467},
  shortjournal = {Pattern Recognition},
  title        = {Learning to rectify for robust learning with noisy labels},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning with credit assignment for
combinatorial optimization. <em>PR</em>, <em>124</em>, 108466. (<a
href="https://doi.org/10.1016/j.patcog.2021.108466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Reinforcement Learning (DRL) demonstrates the potential for solving Combinatorial Optimization (CO) problems. DRL shows advantages over traditional methods both on scalability and computation efficiency. However, the DRL problems transformed from CO problems usually have a huge state space, and the main challenge of solving them has changed from high computation complexity to high sample complexity. Credit assignment determines the contribution of each internal decision to the final success or failure, and it has been shown to be effective in reducing the sample complexity of the training process. In this paper, we resort to a model-based reinforcement learning method to assign credits for model-free DRL methods. Since heuristic methods plays an important role on state-of-the-art solutions for CO problems, we propose using a model to represent those heuristic knowledge and derive the credit assignment from the model. This model-based credit assignment can facilitate the model-free DRL to perform a more effective exploration, and the data collected by the model-free DRL refines the model continuously as the training progresses. Extensive experiments on various CO problems with different settings show that our framework outperforms previous state-of-the-art methods on performance and training efficiency.},
  archive      = {J_PR},
  author       = {Dong Yan and Jiayi Weng and Shiyu Huang and Chongxuan Li and Yichi Zhou and Hang Su and Jun Zhu},
  doi          = {10.1016/j.patcog.2021.108466},
  journal      = {Pattern Recognition},
  pages        = {108466},
  shortjournal = {Pattern Recognition},
  title        = {Deep reinforcement learning with credit assignment for combinatorial optimization},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive attention network with dense field estimation
for face completion. <em>PR</em>, <em>124</em>, 108465. (<a
href="https://doi.org/10.1016/j.patcog.2021.108465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern face completion approaches adopt an autoencoder or its variants to restore missing regions in face images. Encoders are often utilized to learn powerful representations that play an important role in meeting the challenges of sophisticated learning tasks. Specifically, various kinds of masks are often presented in face images in the wild, forming complex patterns, especially in this hard period of COVID-19. It’s difficult for encoders to capture such powerful representations under this complex situation. To address this challenge, we propose a self-supervised Siamese inference network to improve the generalization and robustness of encoders. It can encode contextual semantics from full-resolution images and obtain more discriminative representations. To deal with geometric variations of face images, a dense correspondence field is integrated into the network. We further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine the restored and known regions in an adaptive manner. This multi-scale architecture is beneficial for the decoder to utilize discriminative representations learned from encoders into images. Extensive experiments clearly demonstrate that the proposed approach not only achieves more appealing results compared with state-of-the-art methods but also improves the performance of masked face recognition dramatically.},
  archive      = {J_PR},
  author       = {Xin Ma and Xiaoqiang Zhou and Huaibo Huang and Gengyun Jia and Zhenhua Chai and Xiaolin Wei},
  doi          = {10.1016/j.patcog.2021.108465},
  journal      = {Pattern Recognition},
  pages        = {108465},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive attention network with dense field estimation for face completion},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing ordinal network of time series based on
complexity-entropy curve. <em>PR</em>, <em>124</em>, 108464. (<a
href="https://doi.org/10.1016/j.patcog.2021.108464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing signal dynamics with network approaches have attracted significant attention in nonlinear time series analysis. Among these approaches, ordinal networks have received great interest for their simplicity and computational efficiency. But most studies mainly use the topological structure of ordinal network to characterize time series while the underlying information in the transition probabilities remain insufficiently concerned. In this paper, the authors introduce an ordinal network-based complexity-entropy curve to fill this gap. The numerical results show that this curve has a great discriminating power for signals with different dynamics, outperforming the recently proposed global node entropy. In the empirical application on stock indices, these curves distinguish stock market with different market development and further identify the impact of the 2008 global financial crisis on stock market dynamics. In the analysis of geomagnetic activity, these curves detect the dynamical change in Earths magnetic field caused by the geomagnetic storm.},
  archive      = {J_PR},
  author       = {Kun Peng and Pengjian Shang},
  doi          = {10.1016/j.patcog.2021.108464},
  journal      = {Pattern Recognition},
  pages        = {108464},
  shortjournal = {Pattern Recognition},
  title        = {Characterizing ordinal network of time series based on complexity-entropy curve},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A new framework of designing iterative techniques for image
deblurring. <em>PR</em>, <em>124</em>, 108463. (<a
href="https://doi.org/10.1016/j.patcog.2021.108463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present a framework of designing iterative techniques for image deblurring in inverse problem . The new framework is based on two observations about existing methods. We used Landweber method as the basis to develop and present the new framework but note that the framework is applicable to other iterative techniques. First, we observed that the iterative steps of Landweber method consist of a constant term, which is a low-pass filtered version of the already blurry observation. We proposed a modification to use the observed image directly. Second, we observed that Landweber method uses an estimate of the true image as the starting point. This estimate, however, does not get updated over iterations. We proposed a modification that updates this estimate as the iterative process progresses. We integrated the two modifications into one framework of iteratively deblurring images. Finally, we tested the new method and compared its performance with several existing techniques, including Landweber method, Van Cittert method, GMRES (generalized minimal residual method), and LSQR (least square), to demonstrate its superior performance in image deblurring.},
  archive      = {J_PR},
  author       = {Min Zhang and Geoffrey S. Young and Yanmei Tie and Xianfeng Gu and Xiaoyin Xu},
  doi          = {10.1016/j.patcog.2021.108463},
  journal      = {Pattern Recognition},
  pages        = {108463},
  shortjournal = {Pattern Recognition},
  title        = {A new framework of designing iterative techniques for image deblurring},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-driven features based on image dependencies for
person re-identification. <em>PR</em>, <em>124</em>, 108462. (<a
href="https://doi.org/10.1016/j.patcog.2021.108462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims to search for the same pedestrian in different cameras, which is a crucial research direction in pattern recognition. Recent deep learning methods have advanced the development of Re-ID. However, the existing approaches easily result in performance degradation in the case of larger scene data because they do not adequately consider the spatial dependencies of both the inter-image and the intra-image. The paper proposes a novel Spatial-Driven Network (SDN) to learn particularly discriminative features with abundant semantic information from both the inter-image and the intra-image dependencies for person Re-ID. Firstly, we design a global-correlation attention module to capture the inter-image dependencies among a series of different pedestrian images. Secondly, we present a local-correlation attention module to compute the intra-image dependencies from any pair of pixels within each pedestrian image. Furthermore, we propose a specific network integration mechanism, which carefully combines the above two complementary modules to match well the solution of the spatial dependency problem. We implement numerous experiments to assess the proposed SDN on mainstream person Re-ID databases. The results demonstrate that the proposed SDN outperforms most of the state-of-the-art methods in typical key criteria.},
  archive      = {J_PR},
  author       = {Tongzhen Si and Fazhi He and Haoran Wu and Yansong Duan},
  doi          = {10.1016/j.patcog.2021.108462},
  journal      = {Pattern Recognition},
  pages        = {108462},
  shortjournal = {Pattern Recognition},
  title        = {Spatial-driven features based on image dependencies for person re-identification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Text-instance graph: Exploring the relational semantics for
text-based visual question answering. <em>PR</em>, <em>124</em>, 108455.
(<a href="https://doi.org/10.1016/j.patcog.2021.108455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is time to stop neglecting the text around your world. In VQA, the surrounding text helps humans to understand complete visual scenes and reason question semantics efficiently. Here, we address the challenging Text-based Visual Question Answering (TextVQA) problem, which requires a model to answer the VQA questions with text reading ability. Existing TextVQA methods mainly focus on the latent relationships between detected object instances and scene texts with the given question, but ignore spatial location relationships and complex relational semantics between visual object instances and OCR texts (e.g. the A of B on C). To deal with these challenges, we propose a novel Text-Instance Graph (TIG) network for TextVQA. The TIG builds an OCR-OBJ graph for overlapping relationships modeling, where each node of graph is updated by utilizing relative objects or OCR texts. To deal with the question with complex logic, we propose a dynamic OCR-OBJ graph network to extend the perception space of graph nodes , which grasps the information of non-directly adjacent node features. Considering a scene about “the brand of the computer on the table”, the model would build correlations between “brand” and “table” using “the computer” node as the intermediate node . Extensive experiments on three benchmarks demonstrate the effectiveness and superiority of the proposed method. In addition, our TIG achieves 0.505 ANLS on ST-VQA challenge leaderboard and sets a new state-of-the-art.},
  archive      = {J_PR},
  author       = {Xiangpeng Li and Bo Wu and Jingkuan Song and Lianli Gao and Pengpeng Zeng and Chuang Gan},
  doi          = {10.1016/j.patcog.2021.108455},
  journal      = {Pattern Recognition},
  pages        = {108455},
  shortjournal = {Pattern Recognition},
  title        = {Text-instance graph: Exploring the relational semantics for text-based visual question answering},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sketches by MoSSaRT: Representative selection from manifolds
with gross sparse corruptions. <em>PR</em>, <em>124</em>, 108454. (<a
href="https://doi.org/10.1016/j.patcog.2021.108454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional sampling techniques fall short of selecting representatives that encode the underlying conformation of non-linear manifolds. The problem is exacerbated if the data is contaminated with gross sparse corruptions. In this paper, we present a data selection approach, dubbed MoSSaRT, which draws robust and descriptive sketches of grossly corrupted manifold structures. Built upon an explicit randomized transformation, we obtain a judiciously designed representation of the data relations, which facilitates a versatile selection approach accounting for robustness to gross corruption, descriptiveness and novelty of the chosen representatives, simultaneously. Our model lends itself to a convex formulation with an efficient parallelizable algorithm, which coupled with our randomized matrix structures gives rise to a highly scalable implementation. Theoretical analysis guarantees probabilistic convergence of the approximate function to the desired objective function and reveals insightful geometrical characterization of the chosen representatives. Finally, MoSSaRT substantially outperforms the state-of-the-art algorithms as demonstrated by experiments conducted on both real and synthetic data.},
  archive      = {J_PR},
  author       = {Mahlagha Sedghi and Michael Georgiopoulos and George K. Atia},
  doi          = {10.1016/j.patcog.2021.108454},
  journal      = {Pattern Recognition},
  pages        = {108454},
  shortjournal = {Pattern Recognition},
  title        = {Sketches by MoSSaRT: Representative selection from manifolds with gross sparse corruptions},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GaitSlice: A gait recognition model based on spatio-temporal
slice features. <em>PR</em>, <em>124</em>, 108453. (<a
href="https://doi.org/10.1016/j.patcog.2021.108453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the performance of gait recognition under multiple camera views (i.e., cross-view gait recognition) and various conditions is urgent. From observation, we find that adjacent body parts are inter-related while walking, and each frame in a gait sequence possesses different degrees of semantic information. In this paper, we propose a novel model, GaitSlice, to analyze the human gait based on spatio-temporal slice features. Spatially, we design Slice Extraction Device (SED) to form top-down inter-related slice features. Temporally, we introduce Residual Frame Attention Mechanism (RFAM) to acquire and highlight the key frames. To better simulate reality, GaitSlice combines parallel RFAMs with inter-related slice features to focus on the features’ spatio-temporal information. We evaluate our model on CASIA-B and OU-MVLP gait datasets and compare it with six typical gait recognition models by using rank-1 accuracy. The results show that GaitSlice achieves high accuracy in gait recognition under cross-view and various walking conditions.},
  archive      = {J_PR},
  author       = {Huakang Li and Yidan Qiu and Huimin Zhao and Jin Zhan and Rongjun Chen and Tuanjie Wei and Zhihui Huang},
  doi          = {10.1016/j.patcog.2021.108453},
  journal      = {Pattern Recognition},
  pages        = {108453},
  shortjournal = {Pattern Recognition},
  title        = {GaitSlice: A gait recognition model based on spatio-temporal slice features},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep co-supervision and attention fusion strategy for
automatic COVID-19 lung infection segmentation on CT images.
<em>PR</em>, <em>124</em>, 108452. (<a
href="https://doi.org/10.1016/j.patcog.2021.108452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the irregular shapes,various sizes and indistinguishable boundaries between the normal and infected tissues, it is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images. In this paper, a novel segmentation scheme is proposed for the infections of COVID-19 by enhancing supervised information and fusing multi-scale feature maps of different levels based on the encoder-decoder architecture. To this end, a deep collaborative supervision (Co-supervision) scheme is proposed to guide the network learning the features of edges and semantics. More specifically, an Edge Supervised Module (ESM) is firstly designed to highlight low-level boundary features by incorporating the edge supervised information into the initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised Module (ASSM) is proposed to strengthen high-level semantic information by integrating mask supervised information into the later stage. Then an Attention Fusion Module (AFM) is developed to fuse multiple scale feature maps of different levels by using an attention mechanism to reduce the semantic gaps between high-level and low-level feature maps. Finally, the effectiveness of the proposed scheme is demonstrated on four various COVID-19 CT datasets. The results show that the proposed three modules are all promising. Based on the baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase Dice metric by 1.12\%, 1.95\%,1.63\% in our dataset, while the integration by incorporating three models together can rise 3.97\%. Compared with the existing approaches in various datasets, the proposed method can obtain better segmentation performance in some main metrics, and can achieve the best generalization and comprehensive performance.},
  archive      = {J_PR},
  author       = {Haigen Hu and Leizhao Shen and Qiu Guan and Xiaoxin Li and Qianwei Zhou and Su Ruan},
  doi          = {10.1016/j.patcog.2021.108452},
  journal      = {Pattern Recognition},
  pages        = {108452},
  shortjournal = {Pattern Recognition},
  title        = {Deep co-supervision and attention fusion strategy for automatic COVID-19 lung infection segmentation on CT images},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting appearance transfer and multi-scale context for
efficient person image generation. <em>PR</em>, <em>124</em>, 108451.
(<a href="https://doi.org/10.1016/j.patcog.2021.108451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose . However, convolutional neural networks (CNNs) are inherently limited to geometric transformations due to the fixed geometric structures in their building modules, i.e., convolution, pooling and unpooling, which cannot handle large motion and occlusions caused by large pose transform. This paper introduces a novel two-stream context-aware appearance transfer network to address these challenges. It is a three-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module, a multi-scale context module and two-stream feature fusion modules. The appearance transfer module handles large motion by finding the dense correspondence between the two-stream feature maps and then transferring the appearance information from the source stream to the target stream. The multi-scale context module handles occlusion via contextual modeling, which is achieved by atrous convolutions of different sampling rates. Both quantitative and qualitative results indicate the proposed network can effectively handle challenging cases of large pose transform while retaining the appearance details. Compared with state-of-the-art approaches, it achieves comparable or superior performance using much fewer parameters while being significantly faster.},
  archive      = {J_PR},
  author       = {Chengkang Shen and Peiyan Wang and Wei Tang},
  doi          = {10.1016/j.patcog.2021.108451},
  journal      = {Pattern Recognition},
  pages        = {108451},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting appearance transfer and multi-scale context for efficient person image generation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised dimensionality reduction technology of
generalized discriminant component analysis and its kernelization forms.
<em>PR</em>, <em>124</em>, 108450. (<a
href="https://doi.org/10.1016/j.patcog.2021.108450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised subspace projection technology is a major method for dimensionality reduction in pattern recognition. At present, most supervised subspace projection algorithms are derived from the multi-dimensional extended version of Fisher linear discriminant analysis (FDA), also known as Multi-dimensional Fisher discriminant analysis (MD-FDA). However, MD-FDA needs to be improved further because the projection vectors in the noise-subspace cannot be sorted and the ill-condition of the within-class scatter matrix may cause severe numerical instabilities. Generalized discriminant component analysis (GDCA), the generalization of MD-FDA, together with its kernelization forms are proposed and correspondingly rigorous mathematical proofs are detailed in this paper. By virtue of 5 validation data sets derived from UCI Machine Learning Repository and our laboratory, the theoretical validity and technical advantages of GDCA as well as its kernelization forms are verified, and the effectiveness of the newly proposed method is demonstrated in comparison with 36 kinds of state-of-the-art dimensionality reduction algorithms.},
  archive      = {J_PR},
  author       = {Ruixu Zhou and Wensheng Gao and Dengwei Ding and Weidong Liu},
  doi          = {10.1016/j.patcog.2021.108450},
  journal      = {Pattern Recognition},
  pages        = {108450},
  shortjournal = {Pattern Recognition},
  title        = {Supervised dimensionality reduction technology of generalized discriminant component analysis and its kernelization forms},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep collaborative multi-task network: A human decision
process inspired model for hierarchical image classification.
<em>PR</em>, <em>124</em>, 108449. (<a
href="https://doi.org/10.1016/j.patcog.2021.108449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical classification is significant for big data, where the original task is divided into several sub-tasks to provide multi-granularity predictions based on a tree-shape label structure. Obviously, these sub-tasks are highly correlated: results of the coarser-grained sub-tasks can reduce the candidates for the fine-grained sub-tasks, while results of the fine-grained sub-tasks provide attributes describing the coarser-grained classes. A human can integrate feedbacks from all the related sub-tasks instead of considering each sub-task independently. Therefore, we propose a deep collaborative multi-task network for hierarchical image classification . Specifically, we first extract the relationship matrix between every two sub-tasks defined by the hierarchical label structure. Then, the information of each sub-task is broadcasted to all the related sub-tasks through the relationship matrix . Finally, to combine this information, a novel fusion function based on the task evaluation and the decision uncertainty is designed. Extensive experimental results demonstrate that our model can achieve state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Yu Zhou and Xiaoni Li and Yucan Zhou and Yu Wang and Qinghua Hu and Weiping Wang},
  doi          = {10.1016/j.patcog.2021.108449},
  journal      = {Pattern Recognition},
  pages        = {108449},
  shortjournal = {Pattern Recognition},
  title        = {Deep collaborative multi-task network: A human decision process inspired model for hierarchical image classification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-complementary and unlabeled learning for arbitrary
losses and models. <em>PR</em>, <em>124</em>, 108447. (<a
href="https://doi.org/10.1016/j.patcog.2021.108447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. We also propose a risk correction scheme for alleviating over-fitting caused by negative empirical risk. Finally, the experiments on both linear and deep models show the effectiveness of our proposed methods.},
  archive      = {J_PR},
  author       = {Yuzhou Cao and Shuqi Liu and Yitian Xu},
  doi          = {10.1016/j.patcog.2021.108447},
  journal      = {Pattern Recognition},
  pages        = {108447},
  shortjournal = {Pattern Recognition},
  title        = {Multi-complementary and unlabeled learning for arbitrary losses and models},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face photo-sketch synthesis via full-scale identity
supervision. <em>PR</em>, <em>124</em>, 108446. (<a
href="https://doi.org/10.1016/j.patcog.2021.108446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face photo-sketch synthesis refers transforming a face image between photo domain and sketch domain. It plays a crucial role in law enforcement and digital entertainment. A great deal of efforts have been devoted on face photo-sketch synthesis. However, limited by the weak identity supervision, existing methods mostly yield indistinct details or great deformation, resulting in poor perceptual appearance or low recognition accuracy. In the past several years, face identification achieved great progress, which represents the face images much more precisely than before. Considering the face image translation is also a type of face image re-representation, we attempt to introduce face recognition models to improve the synthesis performance. First, we applied existing synthesis models to augment the training set. Then, we proposed a full-scale identity supervision method to reduce redundant information introduced by these pseudo samples and take the valid information to enhance the intra-class variations. The proposed framework consists of two sub-networks: cross-domain translation (CT) network and intra-domain adaptation (IA) network. The CT network translates the input image from source domain to latent image of target domain, which overcomes the great gap between two domains with less structural deformation. The IA network adapts the perceptual appearance of latent image to target image by adversarial learning. Experimental results on CUHK Face Sketch Database and CUHK Face Sketch FERET Database demonstrate the proposed method preserved best perceptual appearance and more distinct details with less deformation.},
  archive      = {J_PR},
  author       = {Bing Cao and Nannan Wang and Jie Li and Qinghua Hu and Xinbo Gao},
  doi          = {10.1016/j.patcog.2021.108446},
  journal      = {Pattern Recognition},
  pages        = {108446},
  shortjournal = {Pattern Recognition},
  title        = {Face photo-sketch synthesis via full-scale identity supervision},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical domain adaptation with local feature patterns.
<em>PR</em>, <em>124</em>, 108445. (<a
href="https://doi.org/10.1016/j.patcog.2021.108445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is proposed to generalize learning machines and address performance degradation of models that are trained from one specific source domain but applied to novel target domains. Existing domain adaptation methods focus on transferring holistic features whose discriminability is generally tailored to be source-specific and inferiorly generic to be transferable. As a result, standard domain adaptation on holistic features usually damages feature structures, especially local feature statistics, and deteriorates the learned discriminability. To alleviate this issue, we propose to transfer primitive local feature patterns, whose discriminability are shown to be inherently more sharable, and perform hierarchical feature adaptation. Concretely, we first learn a cluster of domain-shared local feature patterns and partition the feature space into cells. Local features are adaptively aggregated inside each cell to obtain cell features, which are further integrated into holistic features. To achieve fine-grained adaptations, we simultaneously perform alignment on local features, cell features and holistic features, within which process the local and cell features are aligned independently inside each cell to maintain the learned local structures and prevent negative transfer . Experimenting on typical one-to-one unsupervised domain adaptation for both image classification and action recognition tasks, partial domain adaptation, and domain-agnostic adaptation, we show that the proposed method achieves more reliable feature transfer by consistently outperforming state-of-the-art models and the learned domain-invariant features generalize well to novel domains.},
  archive      = {J_PR},
  author       = {Jun Wen and Junsong Yuan and Qian Zheng and Risheng Liu and Zhefeng Gong and Nenggan Zheng},
  doi          = {10.1016/j.patcog.2021.108445},
  journal      = {Pattern Recognition},
  pages        = {108445},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical domain adaptation with local feature patterns},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust gaussian process regression with a bias model.
<em>PR</em>, <em>124</em>, 108444. (<a
href="https://doi.org/10.1016/j.patcog.2021.108444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach to a robust Gaussian process regression, creating a non-parametric Bayesian regression estimate robust to outliers. Most existing approaches replace an outlier-prone Gaussian likelihood with a non-Gaussian likelihood induced from a heavy tail distribution, such as the Laplace distribution and Student-t distribution. However, the use of a non-Gaussian likelihood would incur the need for a computationally expensive Bayesian approximate computation in the posterior inferences. The proposed approach models an outlier as a noisy and biased observation of an unknown regression function , and accordingly, the likelihood contains bias terms to explain the degree of deviations from the regression function. We introduce two bias models that handle the bias terms differently, treating a bias as an unknown and fixed quantity or treating a bias as a random quantity. We entail how the biases can be estimated accurately with other hyperparameters by a regularized maximum likelihood estimation . Conditioned on the bias estimates, the robust GP regression can be reduced to a standard GP regression problem with analytical forms of the predictive mean and variance estimates. Therefore, the proposed approach is simple and very computationally attractive. It also gives a very robust and accurate GP estimate for many tested scenarios. For the numerical evaluation , we perform a comprehensive simulation study to evaluate the proposed approach with the comparison to the existing robust GP approaches under various simulated scenarios of different outlier proportions and different noise levels. The approach is applied to data from two measurement systems, where the predictors are based on robust environmental parameter measurements and the response variables utilize more complex chemical sensing methods that contain a certain percentage of outliers. The utility of the measurement systems and value of the environmental data are improved through the computationally efficient GP regression and bias model.},
  archive      = {J_PR},
  author       = {Chiwoo Park and David J. Borth and Nicholas S. Wilson and Chad N. Hunter and Fritz J. Friedersdorf},
  doi          = {10.1016/j.patcog.2021.108444},
  journal      = {Pattern Recognition},
  pages        = {108444},
  shortjournal = {Pattern Recognition},
  title        = {Robust gaussian process regression with a bias model},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Poisson kernel: Avoiding self-smoothing in graph
convolutional networks. <em>PR</em>, <em>124</em>, 108443. (<a
href="https://doi.org/10.1016/j.patcog.2021.108443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network is now an effective tool to deal with non-Euclidean data, such as social behavior analysis, molecular structure analysis, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in graph convolutional networks to extract nodes’ feature, and some variants of it have achieved highly satisfactory performance theoretically and experimentally. However, there was limited research about how exactly different graph structures influence the performance of these kernels. Some existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not explore the internal reasons. In this paper, we start from theoretical analysis of the spectral graph and study the properties of existing graph convolutional kernels, revealing the self-smoothing phenomenon and its effect in specific structured graphs. After that, we propose the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark datasets where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets .},
  archive      = {J_PR},
  author       = {Ziqing Yang and Shoudong Han and Jun Zhao},
  doi          = {10.1016/j.patcog.2021.108443},
  journal      = {Pattern Recognition},
  pages        = {108443},
  shortjournal = {Pattern Recognition},
  title        = {Poisson kernel: Avoiding self-smoothing in graph convolutional networks},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global models for time series forecasting: A simulation
study. <em>PR</em>, <em>124</em>, 108441. (<a
href="https://doi.org/10.1016/j.patcog.2021.108441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances in Big Data have opened up the opportunity to develop competitive Global Forecasting Models (GFM) that simultaneously learn from many time series. Although, the concept of series relatedness has been heavily exploited with GFMs to explain their superiority over local statistical benchmarks, this concept remains largely under-investigated in an empirical setting. Hence, this study attempts to explore the factors that affect GFM performance, by simulating a number of datasets having controllable characteristics. The factors being controlled are along the homogeneity/heterogeneity of series, the complexity of patterns in the series, the complexity of forecasting models, and the lengths/number of series. We simulate time series from simple Data Generating Processes (DGP), such as Auto Regressive (AR), Seasonal AR and Fourier Terms to complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold Auto-Regressive and Mackey-Glass Equations. We perform experiments on these datasets using Recurrent Neural Networks (RNN), Feed-Forward Neural Networks, Pooled Regression models and Light Gradient Boosting Models (LGBM) built as GFMs, and compare their performance against standard statistical forecasting techniques. Our experiments demonstrate that with respect to GFM performance, relatedness is closely associated with other factors such as the availability of data, complexity of data and the complexity of the forecasting technique used. Also, techniques such as RNNs and LGBMs having complex non-linear modelling capabilities, when built as GFMs are competitive methods under challenging forecasting scenarios such as short series, heterogeneous series and having minimal prior knowledge of the data patterns.},
  archive      = {J_PR},
  author       = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
  doi          = {10.1016/j.patcog.2021.108441},
  journal      = {Pattern Recognition},
  pages        = {108441},
  shortjournal = {Pattern Recognition},
  title        = {Global models for time series forecasting: A simulation study},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic clustering based deduction learning for image
recognition and classification. <em>PR</em>, <em>124</em>, 108440. (<a
href="https://doi.org/10.1016/j.patcog.2021.108440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a semantic clustering based deduction learning by mimicking the learning and thinking process of human brains. Human beings can make judgments based on experience and cognition, and as a result, no one would recognize an unknown animal as a car. Inspired by this observation, we propose to train deep learning models using the clustering prior that can guide the models to learn with the ability of semantic deducing and summarizing from classification attributes, such as a cat belonging to animals while a car pertaining to vehicles. The proposed approach realizes the high-level clustering in the semantic space, enabling the model to deduce the relations among various classes during the learning process. In addition, the paper introduces a semantic prior based random search for the opposite labels to ensure the smooth distribution of the clustering and the robustness of the classifiers. The proposed approach is supported theoretically and empirically through extensive experiments. We compare the performance across state-of-the-art classifiers on popular benchmarks, and the generalization ability is verified by adding noisy labeling to the datasets. Experimental results demonstrate the superiority of the proposed approach.},
  archive      = {J_PR},
  author       = {Wenchi Ma and Xuemin Tu and Bo Luo and Guanghui Wang},
  doi          = {10.1016/j.patcog.2021.108440},
  journal      = {Pattern Recognition},
  pages        = {108440},
  shortjournal = {Pattern Recognition},
  title        = {Semantic clustering based deduction learning for image recognition and classification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D pose estimation and future motion prediction from 2D
images. <em>PR</em>, <em>124</em>, 108439. (<a
href="https://doi.org/10.1016/j.patcog.2021.108439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences. Based on Lie algebra pose representation, a novel self-projection mechanism is proposed that naturally preserves human motion kinematics. This is further facilitated by a sequence-to-sequence multi-task architecture based on an encoder-decoder topology, which enables us to tap into the common ground shared by both tasks. Finally, a global refinement module is proposed to boost the performance of our framework. The effectiveness of our approach, called PoseMoNet, is demonstrated by ablation tests and empirical evaluations on Human3.6M and HumanEva-I benchmark, where competitive performance is obtained comparing to the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Ji Yang and Youdong Ma and Xinxin Zuo and Sen Wang and Minglun Gong and Li Cheng},
  doi          = {10.1016/j.patcog.2021.108439},
  journal      = {Pattern Recognition},
  pages        = {108439},
  shortjournal = {Pattern Recognition},
  title        = {3D pose estimation and future motion prediction from 2D images},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human object interaction detection using two-direction
spatial enhancement and exclusive object prior. <em>PR</em>,
<em>124</em>, 108438. (<a
href="https://doi.org/10.1016/j.patcog.2021.108438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection aims to detect visual relations between humans and objects in images. One significant problem of HOI detection is that non-interactive human-object pair can be easily mis-grouped and misclassified as an action, especially when the humans are close and performing similar actions in the scene. To address the mis-grouping problem, we propose a spatial enhancement approach to enforce fine-level spatial constraints in two directions between human body parts and object parts. At inference, we propose a human-object regrouping approach for object-exclusive actions by considering the object-exclusive property of the interactive object, where the target object should not be shared by more than one human. By suppressing non-interactive pairs, our approach can decrease the false positives . Experiments on V-COCO and HICO-DET datasets demonstrate our approach is more robust compared to the existing methods under the presence of multiple humans and objects in the scene.},
  archive      = {J_PR},
  author       = {Lu Liu and Robby T. Tan},
  doi          = {10.1016/j.patcog.2021.108438},
  journal      = {Pattern Recognition},
  pages        = {108438},
  shortjournal = {Pattern Recognition},
  title        = {Human object interaction detection using two-direction spatial enhancement and exclusive object prior},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive region-aware feature enhancement for object
detection. <em>PR</em>, <em>124</em>, 108437. (<a
href="https://doi.org/10.1016/j.patcog.2021.108437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing object detectors reveal the importance of feature representation in improving detection performance. Currently, feature enhancement mainly focuses on Feature Pyramid Network (FPN) as well as Region-of-Interest (RoI) feature fusion in two-stage object detectors. Based on this, we propose Adaptive Region-aware Feature Enhancement method including Adaptive Region-aware FPN (AR-FPN) and Adaptive Region-aware RoI Feature Fusion (AR-RFF) modules. Specifically, AR-FPN aims to capture position-sensitive map for each level to enhance the pixel-wise interest degree and make the differences among levels more distinctive. AR-RFF focuses on obtaining distinguishable RoI features by introducing adaptive region information and eliminating scale inconsistency between the refined and original features. Extensive experiments show that our method acquires 1.7\% AP higher at least and strong generalization capability compared to others.},
  archive      = {J_PR},
  author       = {Zhongjie Fan and Qiong Liu},
  doi          = {10.1016/j.patcog.2021.108437},
  journal      = {Pattern Recognition},
  pages        = {108437},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive region-aware feature enhancement for object detection},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Source data-free domain adaptation for a faster r-CNN.
<em>PR</em>, <em>124</em>, 108436. (<a
href="https://doi.org/10.1016/j.patcog.2021.108436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing domain adaptive object detection methods often need to carry a large number of source domain samples for domain adaptation , which is not realistic due to GPU limitations, privacy and physical memory in practical applications. To solve this problem, we propose a source data-free domain adaptive object detection method. Only unlabeled target domain data is used to optimize the source domain model so that it can work better in the target domain. Our method takes Faster R-CNN as baseline. Specifically, we first construct global class prototypes which will be updated in batch iteratively. Then based on the global class prototypes, more accurate pseudo-labels are generated for training the target model. In this way, the source and target domains are also implicitly aligned. Our contributions are 1) a prototype guided domain adaptation method which uses prototypes to mine the semantic category information without accessing the source dataset ; 2) a scheme of iteratively updating global class prototype which can handle the class and sample imbalances in the training procedure and 3) a more accurate pseudo-label generation method combining semantic information and image information. On multiple public domain adaptive scenarios, our method achieves the state-of-the-art results in terms of accuracy compared with the Faster R-CNN model and some domain adaptive methods with source datasets.},
  archive      = {J_PR},
  author       = {Lin Xiong and Mao Ye and Dan Zhang and Yan Gan and Yiguang Liu},
  doi          = {10.1016/j.patcog.2021.108436},
  journal      = {Pattern Recognition},
  pages        = {108436},
  shortjournal = {Pattern Recognition},
  title        = {Source data-free domain adaptation for a faster R-CNN},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative deep attributes for generalized zero-shot
learning. <em>PR</em>, <em>124</em>, 108435. (<a
href="https://doi.org/10.1016/j.patcog.2021.108435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We indirectly predict a class by deriving user-defined (i.e., existing) attributes (UA) from an image in generalized zero-shot learning (GZSL). High-quality attributes are essential for GZSL, but the existing UAs are sometimes not discriminative. We observe that the hidden units at each layer in a convolutional neural network (CNN) contain highly discriminative semantic information across a range of objects, parts, scenes, textures, materials, and color. The semantic information in CNN features is similar to the attributes that can distinguish each class. Motivated by this observation, we employ CNN features like novel class representative semantic data, i.e., deep attribute (DA). Precisely, we propose three objective functions (e.g., compatible, discriminative, and intra-independent) to inject the fundamental properties into the generated DA. We substantially outperform the state-of-the-art approaches on four challenging GZSL datasets, including CUB, FLO, AWA1, and SUN. Furthermore, the existing UA and our proposed DA are complementary and can be combined to enhance performance further.},
  archive      = {J_PR},
  author       = {Hoseong Kim and Jewook Lee and Hyeran Byun},
  doi          = {10.1016/j.patcog.2021.108435},
  journal      = {Pattern Recognition},
  pages        = {108435},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative deep attributes for generalized zero-shot learning},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brain tumor segmentation based on the dual-path network of
multi-modal MRI images. <em>PR</em>, <em>124</em>, 108434. (<a
href="https://doi.org/10.1016/j.patcog.2021.108434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the tumor with infiltrative growth, the glioma boundary is usually fused with the brain tissue, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dual-path network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.},
  archive      = {J_PR},
  author       = {Lingling Fang and Xin Wang},
  doi          = {10.1016/j.patcog.2021.108434},
  journal      = {Pattern Recognition},
  pages        = {108434},
  shortjournal = {Pattern Recognition},
  title        = {Brain tumor segmentation based on the dual-path network of multi-modal MRI images},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining embedding-based and symbol-based methods for
entity alignment. <em>PR</em>, <em>124</em>, 108433. (<a
href="https://doi.org/10.1016/j.patcog.2021.108433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence , pp. 162–175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining e mbedding-based and s ymbol-based methods for e ntity a lignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol-based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding-based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.},
  archive      = {J_PR},
  author       = {Tingting Jiang and Chenyang Bu and Yi Zhu and Xindong Wu},
  doi          = {10.1016/j.patcog.2021.108433},
  journal      = {Pattern Recognition},
  pages        = {108433},
  shortjournal = {Pattern Recognition},
  title        = {Combining embedding-based and symbol-based methods for entity alignment},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loss function search for person re-identification.
<em>PR</em>, <em>124</em>, 108432. (<a
href="https://doi.org/10.1016/j.patcog.2021.108432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, person re-identification, which learns discriminative features for the specific person retrieval problem across non-overlapping cameras, has attracted extensive attention. One of the main challenges in person re-identification with deep neural networks is the design of the loss function, which plays a vital role in improving the discrimination of the learned features. However, most existing models utilize the hand-designed loss functions, which are usually sub-optimal and time-consuming. The search spaces of the two existing AutoML-based methods are either too complicated or too simple to include various forms of loss functions. In order to solve the irrationality of the above search spaces, in this paper, we propose a method of AutoML for loss function search named LFS-ReID for person ReID in the framework of the margin-based softmax loss function. Specifically, we first analyze the margin-based softmax loss function and conclude four key properties. Then we carefully design a sampling distribution based on the non-independent truncated Gaussian distributions to sample the loss function, which conforms to the above four properties. Finally, a method based on reinforcement learning is adopted to optimize the sampling distribution dynamically. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on four commonly used datasets.},
  archive      = {J_PR},
  author       = {Hongyang Gu and Jianmin Li and Guangyuan Fu and Min Yue and Jun Zhu},
  doi          = {10.1016/j.patcog.2021.108432},
  journal      = {Pattern Recognition},
  pages        = {108432},
  shortjournal = {Pattern Recognition},
  title        = {Loss function search for person re-identification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach to boundary detection for 3D point clouds based
on DBSCAN clustering. <em>PR</em>, <em>124</em>, 108431. (<a
href="https://doi.org/10.1016/j.patcog.2021.108431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new DBSCAN-based method for boundary detection and plane segmentation for 3D point clouds. The proposed method is based on candidate samples selection in 3D space and plane validity detection via revising the classical DBSCAN clustering algorithm to obtain a valid fitting plane. Technically, a coplanar threshold is designed as an additional clustering condition to group 3D points whose distances to the fitting plane satisfy the constraint of the threshold as one cluster. The threshold value is automatically adjusted to fit the local distribution of samples in the input dataset, which is free of parameter tuning. Planar objects can be detected by the proposed method since a cluster contains only data points belonging to one plane, and the boundaries among different planes can be correctly detected. Experimental evaluations are performed on both synthetic and real point cloud datasets. Results show that the proposed approach is effective for planar segmentation and high-quality segmentation of intersection boundaries.},
  archive      = {J_PR},
  author       = {Hui Chen and Man Liang and Wanquan Liu and Weina Wang and Peter Xiaoping Liu},
  doi          = {10.1016/j.patcog.2021.108431},
  journal      = {Pattern Recognition},
  pages        = {108431},
  shortjournal = {Pattern Recognition},
  title        = {An approach to boundary detection for 3D point clouds based on DBSCAN clustering},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-way alignment approach for unsupervised multi-source
domain adaptation. <em>PR</em>, <em>124</em>, 108430. (<a
href="https://doi.org/10.1016/j.patcog.2021.108430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims at transferring knowledge from labeled source domain to unlabeled target domain. Current advances primarily concern single source domain and neglect the setting of multiple source domains. Previous unsupervised multi-source domain adaptation (MDA) algorithms only consider domain-level alignment, while neglecting the category-level information among multiple domains and the instance variations inside each domain. This paper introduces a Two-Way alignment framework for MDA (TWMDA), which considers both domain-level and category-level alignments, and addresses the instance variations. We first align the target and multiple sources on the domain-level by an adversarial learning process. To circumvent the drawbacks of adversarial learning, we further reduce the domain gap on the category-level by minimizing the distance between the category prototypes and unlabeled target instances. To address the instance variations, we design an instance weighting strategy for diverse source instances. The effectiveness of TWMDA is demonstrated on three benchmark datasets for image classification .},
  archive      = {J_PR},
  author       = {Yong-Hui Liu and Chuan-Xian Ren},
  doi          = {10.1016/j.patcog.2021.108430},
  journal      = {Pattern Recognition},
  pages        = {108430},
  shortjournal = {Pattern Recognition},
  title        = {A two-way alignment approach for unsupervised multi-source domain adaptation},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-weighting multi-view spectral clustering based on
nuclear norm. <em>PR</em>, <em>124</em>, 108429. (<a
href="https://doi.org/10.1016/j.patcog.2021.108429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering attracts more and more attention due to the fact that it can utilize the complementary and compatible information from multi-view data sets. In many graph-based multi-view clustering approaches , the graph quality is important since it influences the following clustering performance. Therefore, learning a high quality similarity graph is desired. In this paper, we propose a novel clustering method which is named as Self-weighting Multi-view Spectral Clustering based on Nuclear Norm (SMSC_NN). Specifically, to fully utilize the multiple view features, the common consensus representation is learned. Moreover, to capture the principal components from various view features, the nuclear norm is introduced which can make the view-specific information be well explored. Further, due to the fact that each view feature denotes a sort of specific property, the adaptive weights are assigned instead of equal view weights. In order to verify the effectiveness of the proposed method, four multi-view data sets are used to conduct the clustering experiments. Extensive experimental results demonstrate the superiority of the proposed method comparing with state-of-the-art multi-view clustering approaches . In addition, the proposed approach is experimented on the Cal101-20 data set with ”salt and pepper” noises, and experimental results verify that the proposed SMSC_NN method can remain robust to noises.},
  archive      = {J_PR},
  author       = {Shaojun Shi and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2021.108429},
  journal      = {Pattern Recognition},
  pages        = {108429},
  shortjournal = {Pattern Recognition},
  title        = {Self-weighting multi-view spectral clustering based on nuclear norm},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted clustering ensemble: A review. <em>PR</em>,
<em>124</em>, 108428. (<a
href="https://doi.org/10.1016/j.patcog.2021.108428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods . Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.},
  archive      = {J_PR},
  author       = {Mimi Zhang},
  doi          = {10.1016/j.patcog.2021.108428},
  journal      = {Pattern Recognition},
  pages        = {108428},
  shortjournal = {Pattern Recognition},
  title        = {Weighted clustering ensemble: A review},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmentation information with attention integration for
classification of breast tumor in ultrasound image. <em>PR</em>,
<em>124</em>, 108427. (<a
href="https://doi.org/10.1016/j.patcog.2021.108427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the most common forms of cancer among women worldwide. The development of computer-aided diagnosis (CAD) technology based on ultrasound imaging to promote the diagnosis of breast lesions has attracted the attention of researchers and deep learning is a popular and effective method. However, most of the deep learning based CAD methods neglect the relationship between two vision tasks tumor region segmentation and classification. In this paper, taking into account some prior knowledges of medicine, we propose a novel segmentation-to-classification scheme by adding the segmentation-based attention (SBA) information to the deep convolution network (DCNN) for breast tumors classification. A segmentation network is trained to generate tumor segmentation enhancement images. Then two parallel networks extract features for the original images and segmentation enhanced images and one channel attention based feature aggregation network is to automatically integrate the features extracted from two feature networks to improve the performance of recognizing malignant tumors in the breast ultrasound images. To validate our method, experiments have been conducted on breast ultrasound datasets. The classification results of our method have been compared with those obtained by eleven existing approaches. The experimental results show that the proposed method achieves the highest Accuracy (90.78\%), Sensitivity (91.18\%), Specificity (90.44\%), F1-score (91.46\%), and AUC (0.9549).},
  archive      = {J_PR},
  author       = {Yaozhong Luo and Qinghua Huang and Xuelong Li},
  doi          = {10.1016/j.patcog.2021.108427},
  journal      = {Pattern Recognition},
  pages        = {108427},
  shortjournal = {Pattern Recognition},
  title        = {Segmentation information with attention integration for classification of breast tumor in ultrasound image},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Signature barcodes for online verification. <em>PR</em>,
<em>124</em>, 108426. (<a
href="https://doi.org/10.1016/j.patcog.2021.108426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a sub-branch of behavioral biometrics, online signature verification systems deal with unique signing characteristics, which could be better differentiated by extraction of habitual singing styles instead of geometric features in case of perfect forgery. Even if the signatures are geometrically identical, speed and frequency components of the signing process might significantly vary. Therefore, a novel framework is introduced as a new signature verification protocol for touchscreen devices using barcodes containing the dominant frequency component of the speed signals. A special interface is designed as signature tracker to extract the displacement data sampled from the signing process. The speed signals are interpolated from the displacement data and the frequency components of the signals are computed by scalograms analysis governed by continuous wavelet transformations (CWT). The signature barcodes are generated as 4-scale scalograms and classified by support vector machines (SVM). Among several compatible wavelets, Gaussian derivative wavelet is selected for generating scalograms and the results of the process are calculated as 2.25\% FAR , 2.75\% FRR and 2.81\%EER for our dataset. The framework is also tested with SVC2004 data that we achieved 0\% FAR, 9.33\% FRR and 8\%EER, also with SUSIG-Visual, SUSIG-Blind, MOBISIG databases and we reached between 1.22\%-3.62\% average EERs, which are competitive among the relevant results. Given the promising outcomes, the signature barcoding is very reliable method which could be executed by a simple touchscreen interface collecting the barcodes for storing and benchmarking when needed.},
  archive      = {J_PR},
  author       = {Orcan Alpar},
  doi          = {10.1016/j.patcog.2021.108426},
  journal      = {Pattern Recognition},
  pages        = {108426},
  shortjournal = {Pattern Recognition},
  title        = {Signature barcodes for online verification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MTCNet: Multi-task collaboration network for
rotation-invariance face detection. <em>PR</em>, <em>124</em>, 108425.
(<a href="https://doi.org/10.1016/j.patcog.2021.108425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting rotated faces is a challenging task with images from uncontrolled environments. The use of deep convolutional neural networks have greatly improved detection performance, but these methods still do not fully exploit face structure information. This leaves faces with more extreme rotation angles undetectable. In this paper, we present a novel Multi-Task Collaboration Network (MTCNet) for rotation-invariance face detection that fully uses facial landmarks to improve the detection performance by means of collaboration between face detection and face alignment. Differing from previous methods that predict rotation angles in a single step, MTCNet employs a cascaded architecture with three stages to predict faces with gradually decreasing rotation-in-plane ranges in a coarse-to-fine process. Accurate facial landmarks further facilitate face detection. We also introduce a new training loss by integrating the geometric angle into the penalization process, which is much more reasonable than measuring the differences of training samples roughly. Our approach also explores contextual information to distinguish challenging faces from unconstrained scenarios. Extensive experimental results were conducted to demonstrate the effectiveness of MTCNet on both the multiple orientation and rotation datasets. Empirical studies show that MTCNet achieves results competitive with state-of-the-art face detectors while being time-efficient.},
  archive      = {J_PR},
  author       = {Lifang Zhou and Hui Zhao and Jiaxu Leng},
  doi          = {10.1016/j.patcog.2021.108425},
  journal      = {Pattern Recognition},
  pages        = {108425},
  shortjournal = {Pattern Recognition},
  title        = {MTCNet: Multi-task collaboration network for rotation-invariance face detection},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Velocity-to-velocity human motion forecasting. <em>PR</em>,
<em>124</em>, 108424. (<a
href="https://doi.org/10.1016/j.patcog.2021.108424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting human motion from a sequence of human poses is an important problem in the fields of computer vision and robotics. Most previous approaches merely consider learning the temporal dynamics of body joints or joint angles, while neglect derivatives of body joints (i.e., pose velocities) which could reasonably reduce noise impact and improve stability. To exploit the benefits of pose velocities, we propose the velocity-to-velocity learning paradigm for human motion prediction which attempts to directly build the sequence-to-sequence model in the velocity space. Two variant architectures based on recurrent encoder-decoder networks are introduced under this paradigm. Considering human motion as kinematics of rigid bodies, joint angles which denote transformation are the computations of inverse kinematics . Accordingly, a novel loss function in terms of rotation matrices is designed during training for human motion prediction through a rotation matrix transformation (RMT) layer. Finally, we present an effective training algorithm which exploits sequence transformation to improve model generalization. Our approaches substantially outperform state-of-the-art approaches on two large-scale datasets, Human3.6M and CMU Motion Capture, for both short-term prediction and long-term prediction. In particular, our model can competently forecast human-like and meaningful poses up to 1000 milliseconds. The code is available on GitHub: https://github.com/hongsong-wang/RNN_based_human_motion_prediction .},
  archive      = {J_PR},
  author       = {Hongsong Wang and Liang Wang and Jiashi Feng and Daquan Zhou},
  doi          = {10.1016/j.patcog.2021.108424},
  journal      = {Pattern Recognition},
  pages        = {108424},
  shortjournal = {Pattern Recognition},
  title        = {Velocity-to-velocity human motion forecasting},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved time series clustering based on new geometric
frameworks. <em>PR</em>, <em>124</em>, 108423. (<a
href="https://doi.org/10.1016/j.patcog.2021.108423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods for time series clustering rely on distances calculated from the entire raw data using the Euclidean distance or Dynamic Time Warping distance. In this work, we propose to embed the time series onto higher-dimensional spaces to obtain geometric representations of the time series themselves. Particularly, the embedding on R n × p Rn×p , on the Stiefel manifold and on the unit Sphere are analyzed for their performances with respect to several yet well-known clustering algorithms . The gain brought by the geometrical representation for the time series clustering is illustrated through a large benchmark of databases. We particularly exhibit that, firstly, the embedding of the time series on higher dimensional spaces gives better results than classical approaches and, secondly, that the embedding on the Stiefel manifold - in conjunction with UMAP and HDBSCAN clustering algorithms - is the recommended framework for time series clustering.},
  archive      = {J_PR},
  author       = {Clément Péalat and Guillaume Bouleux and Vincent Cheutet},
  doi          = {10.1016/j.patcog.2021.108423},
  journal      = {Pattern Recognition},
  pages        = {108423},
  shortjournal = {Pattern Recognition},
  title        = {Improved time series clustering based on new geometric frameworks},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cascaded nested network for 3T brain MR image segmentation
guided by 7T labeling. <em>PR</em>, <em>124</em>, 108420. (<a
href="https://doi.org/10.1016/j.patcog.2021.108420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of the brain into gray matter, white matter, and cerebrospinal fluid using magnetic resonance (MR) imaging is critical for visualization and quantification of brain anatomy. Compared to 3T MR images, 7T MR images exhibit higher tissue contrast that is contributive to accurate tissue delineation for training segmentation models . In this paper, we propose a cascaded nested network (CaNes-Net) for segmentation of 3T brain MR images, trained by tissue labels delineated from the corresponding 7T images. We first train a nested network (Nes-Net) for a rough segmentation. The second Nes-Net uses tissue-specific geodesic distance maps as contextual information to refine the segmentation. This process is iterated to build CaNes-Net with a cascade of Nes-Net modules to gradually refine the segmentation. To alleviate the misalignment between 3T and corresponding 7T MR images, we incorporate a correlation coefficient map to allow well-aligned voxels to play a more important role in supervising the training process. We compared CaNes-Net with SPM and FSL tools, as well as four deep learning models on 18 adult subjects and the ADNI dataset. Our results indicate that CaNes-Net reduces segmentation errors caused by the misalignment and improves segmentation accuracy substantially over the competing methods.},
  archive      = {J_PR},
  author       = {Jie Wei and Zhengwang Wu and Li Wang and Toan Duc Bui and Liangqiong Qu and Pew-Thian Yap and Yong Xia and Gang Li and Dinggang Shen},
  doi          = {10.1016/j.patcog.2021.108420},
  journal      = {Pattern Recognition},
  pages        = {108420},
  shortjournal = {Pattern Recognition},
  title        = {A cascaded nested network for 3T brain MR image segmentation guided by 7T labeling},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse attention block: Aggregating contextual information
for object detection. <em>PR</em>, <em>124</em>, 108418. (<a
href="https://doi.org/10.1016/j.patcog.2021.108418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well recognized that the contextual information of surrounding objects is beneficial for object detection. Such contextual information can often be obtained from long-range dependencies. This paper proposes a sparse attention block to capture long-range dependencies in an efficient way. Unlike the conventional non-local block, which generates a dense attention map to characterize the dependency between any two positions of the input feature map , our sparse attention block samples the most representative positions for contextual information aggregation. After searching for local peaks in a heat map of the given input feature map , it adaptively selects a sparse set of positions to represent the relationship between query and key elements. With the obtained sparse positions, our sparse attention block can well model long-range dependencies, and greatly improve the object detection performance at the additional cost of &amp;lt; 2\% GPU memory and computation of the conventional non-local block. This sparse attention block can be easily plugged into various object detection frameworks, such as Faster R-CNN, RetinaNet and Mask R-CNN. Experiments on COCO benchmark confirm that our sparse attention block can boost the detection accuracy with significant gains ranging from 1.4\% to 1.9\% and negligible overhead of computation and memory usage.},
  archive      = {J_PR},
  author       = {Chunlin Chen and Jun Yu and Qiang Ling},
  doi          = {10.1016/j.patcog.2021.108418},
  journal      = {Pattern Recognition},
  pages        = {108418},
  shortjournal = {Pattern Recognition},
  title        = {Sparse attention block: Aggregating contextual information for object detection},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tri-attention fusion guided multi-modal segmentation
network. <em>PR</em>, <em>124</em>, 108417. (<a
href="https://doi.org/10.1016/j.patcog.2021.108417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multimodal segmentation, the correlation between different modalities can be considered for improving the segmentation results. Considering the correlation between different MR modalities, in this paper, we propose a multi-modality segmentation network guided by a novel tri-attention fusion. Our network includes N model-independent encoding paths with N image sources, a tri-attention fusion block, a dual-attention fusion block, and a decoding path. The model independent encoding paths can capture modality-specific features from the N modalities. Considering that not all the features extracted from the encoders are useful for segmentation, we propose to use dual attention based fusion to re-weight the features along the modality and space paths, which can suppress less informative features and emphasize the useful ones for each modality at different positions. Since there exists a strong correlation between different modalities, based on the dual attention fusion block, we propose a correlation attention module to form the tri-attention fusion block. In the correlation attention module, a correlation description block is first used to learn the correlation between modalities and then a constraint based on the correlation is used to guide the network to learn the latent correlated features which are more relevant for segmentation. Finally, the obtained fused feature representation is projected by the decoder to obtain the segmentation results. Our experiment results tested on BraTS 2018 dataset for brain tumor segmentation demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Tongxue Zhou and Su Ruan and Pierre Vera and Stéphane Canu},
  doi          = {10.1016/j.patcog.2021.108417},
  journal      = {Pattern Recognition},
  pages        = {108417},
  shortjournal = {Pattern Recognition},
  title        = {A tri-attention fusion guided multi-modal segmentation network},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). DE-GAN: Domain embedded GAN for high quality face image
inpainting. <em>PR</em>, <em>124</em>, 108415. (<a
href="https://doi.org/10.1016/j.patcog.2021.108415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain knowledge of face shapes and structures plays an important role in face inpainting. However, general inpainting methods focus mainly on the resolution of generated images without considering the particular structure of human faces and generally produce inharmonious facial parts. Existing face-inpainting methods incorporate only one type of facial feature for face completion, and their results are still undesirable. To improve face inpainting quality, we propose a Domain Embedded Generative Adversarial Network (DE-GAN) for face inpainting. DE-GAN embeds three types of face domain knowledge (i.e., face mask, face part, and landmark image) via a hierarchical variational auto-encoder (HVAE) into a latent variable space to guide face completion. Two adversarial discriminators , a global discriminator and a patch discriminator, are used to judge whether the generated distribution is close to the real distribution or not. Experiments on two public face datasets demonstrate that our proposed method generates higher quality inpainting results with consistent and harmonious facial structures and appearance than existing methods and achieves the state-of-the-art performance, esp. for inpainting under-pose variations.},
  archive      = {J_PR},
  author       = {Xian Zhang and Xin Wang and Canghong Shi and Zhe Yan and Xiaojie Li and Bin Kong and Siwei Lyu and Bin Zhu and Jiancheng Lv and Youbing Yin and Qi Song and Xi Wu and Imran Mumtaz},
  doi          = {10.1016/j.patcog.2021.108415},
  journal      = {Pattern Recognition},
  pages        = {108415},
  shortjournal = {Pattern Recognition},
  title        = {DE-GAN: Domain embedded GAN for high quality face image inpainting},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PRNU registration under scale and rotation transform based
on convolutional neural networks. <em>PR</em>, <em>124</em>, 108413. (<a
href="https://doi.org/10.1016/j.patcog.2021.108413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing if an image comes from a specific device is fundamental in many application scenarios. The most promising techniques to solve this problem rely on the Photo Response Non Uniformity (PRNU), a unique trace left during image acquisition. A PRNU fingerprint is computed from several images of a given device, then it is compared with the probe residual noise by means of correlation. However, such a comparison requires that PRNUs are synchronized: even small image transformations can spoil this task. Most of the attempts to solve the registration problem rely on time consuming brute-force search, which is prone to missing detections and false positives . In this paper, the problem is addressed from a computer vision perspective, exploiting recent image registration techniques based on deep learning , and focusing on scaling and rotation transformations. Experiments show that the proposed method is both more accurate and faster than state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Marco Fanfani and Alessandro Piva and Carlo Colombo},
  doi          = {10.1016/j.patcog.2021.108413},
  journal      = {Pattern Recognition},
  pages        = {108413},
  shortjournal = {Pattern Recognition},
  title        = {PRNU registration under scale and rotation transform based on convolutional neural networks},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CR-GAN: Automatic craniofacial reconstruction for personal
identification. <em>PR</em>, <em>124</em>, 108400. (<a
href="https://doi.org/10.1016/j.patcog.2021.108400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Craniofacial reconstruction is applied to identify human remains in the absence of determination data (e.g., fingerprinting, dental records, radiological materials, or DNA), by predicting the likeness of the unidentified remains based on the internal relationship between the skull and face. Conventional 3D methods are usually based on statistical models with poor capacity, which limit the description of such complex relationship. Moreover, the required high-quality data are difficult to collect. In this study, we present a novel craniofacial reconstruction paradigm that synthesize craniofacial images from 2D computed tomography scan of skull data. The key idea is to recast craniofacial reconstruction as an image translation task, with the goal of generating corresponding craniofacial images from 2D skull images. To this end, we design an automatic skull-to-face transformation system based on deep generative adversarial nets. The system was trained on 4551 paired skull-face images obtained from 1780 CT head scans of the Han Chinese population. To the best of our knowledge, this is the only database of this magnitude in the literature. Finally, to accurately evaluate the performance of the model, a face recognition task employing five existing deep learning algorithms, —FaceNet, —SphereFace, —CosFace, —ArcFace, and —MagFace, was tested on 102 reconstruction cases in a face pool composed of 1744 CT-scan face images. The experimental results demonstrate that the proposed method can be used as an effective forensic tool .},
  archive      = {J_PR},
  author       = {Yuan Li and Jian Wang and Weibo Liang and Hui Xue and Zhenan He and Jiancheng Lv and Lin Zhang},
  doi          = {10.1016/j.patcog.2021.108400},
  journal      = {Pattern Recognition},
  pages        = {108400},
  shortjournal = {Pattern Recognition},
  title        = {CR-GAN: Automatic craniofacial reconstruction for personal identification},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infinite-dimensional feature aggregation via a factorized
bilinear model. <em>PR</em>, <em>124</em>, 108397. (<a
href="https://doi.org/10.1016/j.patcog.2021.108397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregating infinite-dimensional features has demonstrated superiority compared with their finite-dimensional counterparts. However, most existing methods approximate infinite-dimensional features with finite-dimensional representations, which inevitably results in approximation error and inferior performance. In this paper, we propose a non-approximate aggregation method that directly aggregates infinite-dimensional features rather than relying on approximation strategies. Specifically, since infinite-dimensional features are infeasible to store, represent and compute explicitly, we introduce a factorized bilinear model to capture pairwise second-order statistics of infinite-dimensional features as a global descriptor. It enables the resulting aggregation formulation to only involve the inner product in an infinite-dimensional space. The factorized bilinear model is calculated by a Sigmoid kernel to generate informative features containing infinite order statistics. Experiments on four visual tasks including the fine-grained, indoor scene, texture, and material classification, demonstrate that our method consistently achieves the state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Jindou Dai and Yuwei Wu and Zhi Gao and Yunde Jia},
  doi          = {10.1016/j.patcog.2021.108397},
  journal      = {Pattern Recognition},
  pages        = {108397},
  shortjournal = {Pattern Recognition},
  title        = {Infinite-dimensional feature aggregation via a factorized bilinear model},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPCA++: An efficient method for contrastive feature
learning. <em>PR</em>, <em>124</em>, 108378. (<a
href="https://doi.org/10.1016/j.patcog.2021.108378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new data visualization and clustering technique for discovering discriminative structures in high-dimensional data. This technique, referred to as cPCA++, is motivated by the fact that the interesting features of a “target” dataset may be obscured by high variance components during traditional PCA. By analyzing what is referred to as a “background” dataset (i.e., one that exhibits the high variance principal components but not the interesting structures), our technique is capable of efficiently highlighting the structures that are unique to the “target” dataset. Similar to another recently proposed algorithm called “contrastive PCA” (cPCA), the proposed cPCA++ method identifies important dataset-specific patterns that are not detected by traditional PCA in a wide variety of settings. However, unlike cPCA, the proposed cPCA++ method does not require a parameter sweep, and as a result, it is significantly more efficient. Several experiments were conducted in order to compare the proposed method to state-of-the-art methods. These experiments show that the proposed method achieves performance that is similar to or better than that of the other methods, while being more efficient.},
  archive      = {J_PR},
  author       = {Ronald Salloum and C.-C. Jay Kuo},
  doi          = {10.1016/j.patcog.2021.108378},
  journal      = {Pattern Recognition},
  pages        = {108378},
  shortjournal = {Pattern Recognition},
  title        = {CPCA++: An efficient method for contrastive feature learning},
  volume       = {124},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neighborhood linear discriminant analysis. <em>PR</em>,
<em>123</em>, 108422. (<a
href="https://doi.org/10.1016/j.patcog.2021.108422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Discriminant Analysis (LDA) assumes that all samples from the same class are independently and identically distributed (i.i.d.). LDA may fail in the cases where the assumption does not hold. Particularly when a class contains several clusters (or subclasses), LDA cannot correctly depict the internal structure as the scatter matrices that LDA relies on are defined at the class level. In order to mitigate the problem, this paper proposes a neighborhood linear discriminant analysis (nLDA) in which the scatter matrices are defined on a neighborhood consisting of reverse nearest neighbors. Thus, the new discriminator does not need an i.i.d. assumption. In addition, the neighborhood can be naturally regarded as the smallest subclass, for which it is easier to be obtained than subclass without resorting to any clustering algorithms . The projected directions are sought to make sure that the within-neighborhood scatter as small as possible and the between-neighborhood scatter as large as possible, simultaneously. The experimental results show that nLDA performs significantly better than previous discriminators, such as LDA, LFDA , ccLDA, LM-NNDA, and l 2 , 1 l2,1 -RLDA.},
  archive      = {J_PR},
  author       = {Fa Zhu and Junbin Gao and Jian Yang and Ning Ye},
  doi          = {10.1016/j.patcog.2021.108422},
  journal      = {Pattern Recognition},
  pages        = {108422},
  shortjournal = {Pattern Recognition},
  title        = {Neighborhood linear discriminant analysis},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driving behavior explanation with multi-level fusion.
<em>PR</em>, <em>123</em>, 108421. (<a
href="https://doi.org/10.1016/j.patcog.2021.108421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.},
  archive      = {J_PR},
  author       = {Hédi Ben-Younes and Éloi Zablocki and Patrick Pérez and Matthieu Cord},
  doi          = {10.1016/j.patcog.2021.108421},
  journal      = {Pattern Recognition},
  pages        = {108421},
  shortjournal = {Pattern Recognition},
  title        = {Driving behavior explanation with multi-level fusion},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orthogonal least squares based fast feature selection for
linear classification. <em>PR</em>, <em>123</em>, 108419. (<a
href="https://doi.org/10.1016/j.patcog.2021.108419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Orthogonal Least Squares (OLS) based feature selection method is proposed for both binomial and multinomial classification. The novel Squared Orthogonal Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR) in OLS and used as the feature ranking criterion. The equivalence between the canonical correlation coefficient, Fisher’s criterion, and the sum of the SOCCs is revealed, which unveils the statistical implication of ERR in OLS for the first time. It is also shown that the OLS based feature selection method has speed advantages when applied for greedy search. The proposed method is comprehensively compared with the mutual information based feature selection methods and the embedded methods using both synthetic and real world datasets. The results show that the proposed method is always in the top 5 among the 12 candidate methods. Besides, the proposed method can be directly applied to continuous features without discretisation , which is another significant advantage over mutual information based methods.},
  archive      = {J_PR},
  author       = {Sikai Zhang and Zi-Qiang Lang},
  doi          = {10.1016/j.patcog.2021.108419},
  journal      = {Pattern Recognition},
  pages        = {108419},
  shortjournal = {Pattern Recognition},
  title        = {Orthogonal least squares based fast feature selection for linear classification},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Query pixel guided stroke extraction with model-based
matching for offline handwritten chinese characters. <em>PR</em>,
<em>123</em>, 108416. (<a
href="https://doi.org/10.1016/j.patcog.2021.108416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke extraction and matching are critical for structural interpretation based applications of handwritten Chinese characters, such as Chinese character education and calligraphy analysis. Stroke extraction from offline handwritten Chinese characters is difficult because of the missing of temporal information, the multi-stroke structures and the distortion of handwritten shapes. In this paper, we propose a comprehensive scheme for solving the stroke extraction problem for handwritten Chinese characters. The method consists of three main steps: (1) fully convolutional network (FCN) based skeletonization ; (2) query pixel guided stroke extraction; (3) model-based stroke matching. Specifically, based on a recently proposed architecture of FCN, both the stroke skeletons and cross regions are firstly extracted from the character image by the proposed SkeNet and CrossNet, respectively. Stroke extraction is solved by simulating the human perception that once given a certain pixel from non-cross region of a stroke, the whole stroke containing the pixel can be traced. To realize this idea, we formulate stroke extraction as a problem of pairing and connecting skeleton-wise stroke segments which are adjacent to the same cross region, where the pairing consistency between stroke segments is measured using a PathNet [1]. To reduce the ambiguity of stroke extraction, the extracted candidate strokes are matched with a character model consisting of standard strokes by tree search to identify the correct strokes. For verifying the effectiveness of the proposed method, we train and test our models on character images with stroke segmentation annotations generated from the online handwriting datasets CASIA-OLHWDB and ICDAR13-Online, as well as a dataset of R egularly- W ritten online handwritten characters (RW-OLHWDB). The experimental results demonstrate the effectiveness of the proposed method and provide several benchmarks. Particularly, the precisions of stroke extraction for ICDAR13-Online and RW-OLHWDB are 89.0\% and 94.9\%, respectively.},
  archive      = {J_PR},
  author       = {Tie-Qiang Wang and Xiaoyi Jiang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2021.108416},
  journal      = {Pattern Recognition},
  pages        = {108416},
  shortjournal = {Pattern Recognition},
  title        = {Query pixel guided stroke extraction with model-based matching for offline handwritten chinese characters},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast camouflaged object detection via edge-based reversible
re-calibration network. <em>PR</em>, <em>123</em>, 108414. (<a
href="https://doi.org/10.1016/j.patcog.2021.108414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) aims to detect objects with similar patterns ( e.g. , texture, intensity, colour, etc ) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet . Our model is characterized by two innovative designs, namely Selective Edge Aggregation ( SEA ) and Reversible Re-calibration Unit ( RRU ), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by ∼ ∼ 6\% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task.},
  archive      = {J_PR},
  author       = {Ge-Peng Ji and Lei Zhu and Mingchen Zhuge and Keren Fu},
  doi          = {10.1016/j.patcog.2021.108414},
  journal      = {Pattern Recognition},
  pages        = {108414},
  shortjournal = {Pattern Recognition},
  title        = {Fast camouflaged object detection via edge-based reversible re-calibration network},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multiview nonnegative representation learning
with multiple graphs. <em>PR</em>, <em>123</em>, 108412. (<a
href="https://doi.org/10.1016/j.patcog.2021.108412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has become an important research topic during the past decade. However, partial views of many data instances are missing in some realistic multiview learning scenarios. To handle this problem, we develop an effective incomplete multiview nonnegative representation learning (IMNRL) framework, which is suitable for incomplete multiview clustering in various situations. The IMNRL framework performs matrix factorization on multiple incomplete graphs and decomposes these incomplete graphs into a consensus nonnegative representation and view-specific spectral representations , which integrates the advantages of multiview nonnegative representation learning and graph learning. The proposed framework has the following merits: (1) it learns a consensus nonnegative embedding and view-specific embeddings simultaneously; (2) the nonnegative embedding satisfies the neighbor constraint on each incomplete view, which directly reveals the multiview clustering results. Experimental results show that the proposed framework outperforms other state-of-the-art incomplete multiview clustering algorithms .},
  archive      = {J_PR},
  author       = {Nan Zhang and Shiliang Sun},
  doi          = {10.1016/j.patcog.2021.108412},
  journal      = {Pattern Recognition},
  pages        = {108412},
  shortjournal = {Pattern Recognition},
  title        = {Incomplete multiview nonnegative representation learning with multiple graphs},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual vs internal attention mechanisms in deep neural
networks for image classification and object detection. <em>PR</em>,
<em>123</em>, 108411. (<a
href="https://doi.org/10.1016/j.patcog.2021.108411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The so-called “attention mechanisms” in Deep Neural Networks (DNNs) denote an automatic adaptation of DNNs to capture representative features given a specific classification task and related data. Such attention mechanisms perform both globally by reinforcing feature channels and locally by stressing features in each feature map. Channel and feature importance are learnt in the global end-to-end DNNs training process. In this paper, we present a study and propose a method with a different approach, adding supplementary visual data next to training images. We use human visual attention maps obtained independently with psycho-visual experiments, both in task-driven or in free viewing conditions, or powerful models for prediction of visual attention maps. We add visual attention maps as new data alongside images, thus introducing human visual attention into the DNNs training and compare it with both global and local automatic attention mechanisms . Experimental results show that known attention mechanisms in DNNs work pretty much as human visual attention, but still the proposed approach allows a faster convergence and better performance in image classification tasks.},
  archive      = {J_PR},
  author       = {Abraham Montoya Obeso and Jenny Benois-Pineau and Mireya Saraí García Vázquez and Alejandro Álvaro Ramírez Acosta},
  doi          = {10.1016/j.patcog.2021.108411},
  journal      = {Pattern Recognition},
  pages        = {108411},
  shortjournal = {Pattern Recognition},
  title        = {Visual vs internal attention mechanisms in deep neural networks for image classification and object detection},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Description and recognition of complex spatial
configurations of object pairs with force banner 2D features.
<em>PR</em>, <em>123</em>, 108410. (<a
href="https://doi.org/10.1016/j.patcog.2021.108410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in scene understanding is the handling of spatial relations between objects or object parts. Several descriptors dedicated to this task already exist, such as the force histogram which is a typical example of relative position descriptor. By computing the interaction between two objects for a given force in all the directions, it gives a good overview of the configuration, and it has useful properties that can make it invariant to the 2D viewpoint. Considering that using complementary forces (negative for repulsion, positive for attraction) should improve the description of complex spatial configurations , we propose to extend the force histogram to a panel of forces so as to make it a more complete descriptor. This gives a 2D descriptor that we called “ (discrete) Force Banner ” and which can be used as input of a classical Convolutional Neural Network (CNN), benefiting from their powerful performances, and reduced into more compact spatial features to use them in another system. As an illustration of its ability to describe spatial configurations, we used it to solve a classification problem aiming to discriminate simple spatial relations, but with variable configuration complexities . Experimental results obtained on datasets of synthetic and natural images with various shapes highlight the interest of this approach, in particular for complex spatial configurations.},
  archive      = {J_PR},
  author       = {Robin Deléarde and Camille Kurtz and Laurent Wendling},
  doi          = {10.1016/j.patcog.2021.108410},
  journal      = {Pattern Recognition},
  pages        = {108410},
  shortjournal = {Pattern Recognition},
  title        = {Description and recognition of complex spatial configurations of object pairs with force banner 2D features},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust and efficient fingerprint image restoration method
based on a phase-field model. <em>PR</em>, <em>123</em>, 108405. (<a
href="https://doi.org/10.1016/j.patcog.2021.108405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a robust and efficient fingerprint image restoration algorithm using the nonlocal Cahn–Hilliard (CH) equation, which was proposed for modeling the microphase separation of diblock copolymers. We take a small local region embedding the damaged domain and solve the nonlocal CH equation to restore the fingerprint image. A Gauss–Seidel type iterative method, which is efficient and simple to implement, is used. The proposed method has the advantage in that the pixel values in the damaged fingerprint domain can be obtained using the image information from the outside of the damaged fingerprint region. Fingerprint restoration based on adjacent pixel information can ensure the accuracy of the fingerprint information with a low computational cost. Computational experiments demonstrated the superior performance of the proposed fingerprint restoration algorithm .},
  archive      = {J_PR},
  author       = {Yibao Li and Qing Xia and Chaeyoung Lee and Sangkwon Kim and Junseok Kim},
  doi          = {10.1016/j.patcog.2021.108405},
  journal      = {Pattern Recognition},
  pages        = {108405},
  shortjournal = {Pattern Recognition},
  title        = {A robust and efficient fingerprint image restoration method based on a phase-field model},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Big data directed acyclic graph model for real-time COVID-19
twitter stream detection. <em>PR</em>, <em>123</em>, 108404. (<a
href="https://doi.org/10.1016/j.patcog.2021.108404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every day, large-scale data are continuously generated on social media as streams, such as Twitter, which inform us about all events around the world in real-time. Notably, Twitter is one of the effective platforms to update countries leaders and scientists during the coronavirus (COVID-19) pandemic. Other people have also used this platform to post their concerns about the spread of this virus and a rapid increase of death cases globally. The aim of this work is to detect anomalous events associated with COVID-19 from Twitter. To this end, we propose a distributed Directed Acyclic Graph topology framework to aggregate and process large-scale real-time tweets related to COVID-19. The core of our system is a novel lightweight algorithm that can automatically detect anomaly events. In addition, our system can also identify, cluster, and visualize important keywords in tweets. On 18 August 2020, our model detected the highest anomaly since many tweets mentioned the casualties’ updates and the debates on the pandemic that day. We obtained the three most commonly listed terms on Twitter: “covid”, “death”, and “Trump” (21,566, 11,779, and 4761 occurrences, respectively), with the highest TF-IDF score for these terms: “people” (0.63637), “school” (0.5921407) and “virus” (0.57385). From our clustering result , the word “death”, “corona”, and “case” are grouped into one cluster, where the word “pandemic”, “school”, and “president” are grouped as another cluster. These terms were located near each other on vector space so that they were clustered, indicating people’s most concerned topics on Twitter.},
  archive      = {J_PR},
  author       = {Bakhtiar Amen and Syahirul Faiz and Thanh-Toan Do},
  doi          = {10.1016/j.patcog.2021.108404},
  journal      = {Pattern Recognition},
  pages        = {108404},
  shortjournal = {Pattern Recognition},
  title        = {Big data directed acyclic graph model for real-time COVID-19 twitter stream detection},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fitbeat: COVID-19 estimation based on wristband heart rate
using a contrastive convolutional auto-encoder. <em>PR</em>,
<em>123</em>, 108403. (<a
href="https://doi.org/10.1016/j.patcog.2021.108403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth research project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symptoms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95.3\% 95.3\% , a sensitivity of 100\% 100\% and a specificity of 90.6\% 90.6\% , an area under the receiver operating characteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate .},
  archive      = {J_PR},
  author       = {Shuo Liu and Jing Han and Estela Laporta Puyal and Spyridon Kontaxis and Shaoxiong Sun and Patrick Locatelli and Judith Dineley and Florian B. Pokorny and Gloria Dalla Costa and Letizia Leocani and Ana Isabel Guerrero and Carlos Nos and Ana Zabalza and Per Soelberg Sørensen and Mathias Buron and Melinda Magyari and Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Callum Stewart and Amos A Folarin and Richard JB Dobson and Raquel Bailón and Srinivasan Vairavan and Nicholas Cummins and Vaibhav A Narayan and Matthew Hotopf and Giancarlo Comi and Björn Schuller and RADAR-CNS Consortium},
  doi          = {10.1016/j.patcog.2021.108403},
  journal      = {Pattern Recognition},
  pages        = {108403},
  shortjournal = {Pattern Recognition},
  title        = {Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AdaNFF: A new method for adaptive nonnegative multi-feature
fusion to scene classification. <em>PR</em>, <em>123</em>, 108402. (<a
href="https://doi.org/10.1016/j.patcog.2021.108402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification is an important basis for many modern intelligent applications, however the performance of pattern recognition or deep learning-based methods are still not sufficient since complicated structure and context of scene images. In this paper, we propose a novel fusion framework of adaptive nonnegative feature fusion (AdaNFF) for scene classification. The AdaNFF integrates nonnegative matrix factorization , adaptive feature fusion and feature fusion boosting into an end-to-end process. Firstly, feature fusion is known as a general strategy to strengthen weak features, and we observe that pixel values and most hand-craft features of the scene image are naturally nonnegative. Therefore we are motivated to build a fusion method based on nonnegative matrix factorization , which can preserve features nonnegative properties and improve their representation performance. Secondly, with the results of fused single or multiple features fusion, we develop an adaptive feature fusion and boosting algorithm to improve the efficiency of image features . Finally, a normalized l 2 l2 -norm classifier and a deep-learning like multilayer perceptron (MLP) classifier are trained to predict label of scene image. Under this framework, there are two versions of the proposed feature fusion method for nonnegative single-feature fusion and multi-feature fusion. All methods were validated on scene classification benchmarks. Experiment results suggest that the proposed methods can deal with multi-class scene problems and achieve remarkable classification performance.},
  archive      = {J_PR},
  author       = {Zhiyuan Zou and Weibin Liu and Weiwei Xing},
  doi          = {10.1016/j.patcog.2021.108402},
  journal      = {Pattern Recognition},
  pages        = {108402},
  shortjournal = {Pattern Recognition},
  title        = {AdaNFF: A new method for adaptive nonnegative multi-feature fusion to scene classification},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-attentive multi-task convolutional neural network for
facial expression recognition. <em>PR</em>, <em>123</em>, 108401. (<a
href="https://doi.org/10.1016/j.patcog.2021.108401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research on Facial Expression Recognition (FER) assisted by facial landmarks mainly focused on single-task learning or hard-parameter sharing based multi-task learning. However, soft-parameter sharing based methods have not been explored in this area. Therefore, this paper adopts Facial Landmark Detection (FLD) as the auxiliary task and explores new multi-task learning strategies for FER. First, three classical multi-task structures, including Hard-Parameter Sharing (HPS), Cross-Stitch Network (CSN), and Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), are used to verify the advantages of multi-task learning for FER. Then, we propose a new end-to-end Co-attentive Multi-task Convolutional Neural Network (CMCNN), which is composed of the Channel Co-Attention Module (CCAM) and the Spatial Co-Attention Module (SCAM). Functionally, the CCAM generates the channel co-attention scores by capturing the inter-dependencies of different channels between FER and FLD tasks. The SCAM combines the max- and average-pooling operations to formulate the spatial co-attention scores. Finally, we conduct extensive experiments on four widely used benchmark facial expression databases, including RAF, SFEW2, CK+, and Oulu-CASIA. Extensive experimental results show that our approach achieves better performance than single-task and multi-task baselines, fully validating multi-task learning’s effectiveness and generalizability 1 .},
  archive      = {J_PR},
  author       = {Wenmeng Yu and Hua Xu},
  doi          = {10.1016/j.patcog.2021.108401},
  journal      = {Pattern Recognition},
  pages        = {108401},
  shortjournal = {Pattern Recognition},
  title        = {Co-attentive multi-task convolutional neural network for facial expression recognition},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernelized supervised laplacian eigenmap for visualization
and classification of multi-label data. <em>PR</em>, <em>123</em>,
108399. (<a href="https://doi.org/10.1016/j.patcog.2021.108399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We had previously proposed a supervised Laplacian eigenmap for visualization (SLE-ML) that can handle multi-label data. In addition, SLE-ML can control the trade-off between the class separability and local structure by a single trade-off parameter. However, SLE-ML cannot transform new data, that is, it has the “out-of-sample” problem. In this paper, we show that this problem is solvable, that is, it is possible to simulate the same transformation perfectly using a set of linear sums of reproducing kernels (KSLE-ML) with a nonsingular Gram matrix. We experimentally showed that the difference between training and testing is not large; thus, a high separability of classes in a low-dimensional space is realizable with KSLE-ML by assigning an appropriate value to the trade-off parameter. This offers the possibility of separability-guided feature extraction for classification. In addition, to optimize the performance of KSLE-ML, we conducted both kernel selection and parameter selection. As a result, it is shown that parameter selection is more important than kernel selection. We experimentally demonstrated the advantage of using KSLE-ML for visualization and for feature extraction compared with a few typical algorithms.},
  archive      = {J_PR},
  author       = {Mariko Tai and Mineichi Kudo and Akira Tanaka and Hideyuki Imai and Keigo Kimura},
  doi          = {10.1016/j.patcog.2021.108399},
  journal      = {Pattern Recognition},
  pages        = {108399},
  shortjournal = {Pattern Recognition},
  title        = {Kernelized supervised laplacian eigenmap for visualization and classification of multi-label data},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real masks and spoof faces: On the masked face presentation
attack detection. <em>PR</em>, <em>123</em>, 108398. (<a
href="https://doi.org/10.1016/j.patcog.2021.108398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition (FR) a challenging task because masks hide several discriminative features of faces. Moreover, face presentation attack detection (PAD) is crucial to ensure the security of FR systems. In contrast to the growing number of masked FR studies, the impact of face masked attacks on PAD has not been explored. Therefore, we present novel attacks with real face masks placed on presentations and attacks with subjects wearing masks to reflect the current real-world situation. Furthermore, this study investigates the effect of masked attacks on PAD performance by using seven state-of-the-art PAD algorithms under different experimental settings. We also evaluate the vulnerability of FR systems to masked attacks. The experiments show that real masked attacks pose a serious threat to the operation and security of FR systems.},
  archive      = {J_PR},
  author       = {Meiling Fang and Naser Damer and Florian Kirchbuchner and Arjan Kuijper},
  doi          = {10.1016/j.patcog.2021.108398},
  journal      = {Pattern Recognition},
  pages        = {108398},
  shortjournal = {Pattern Recognition},
  title        = {Real masks and spoof faces: On the masked face presentation attack detection},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defect attention template generation cycleGAN for weakly
supervised surface defect segmentation. <em>PR</em>, <em>123</em>,
108396. (<a href="https://doi.org/10.1016/j.patcog.2021.108396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect segmentation is very important for the quality inspection of industrial production and is an important pattern recognition problem. Although deep learning (DL) has achieved remarkable results in surface defect segmentation, most of these results have been obtained by using massive images with pixel-level annotations, which are difficult to obtain at industrial sites. This paper proposes a weakly supervised defect segmentation method based on the dynamic templates generated by an improved cycle-consistent generative adversarial network (CycleGAN) trained by image-level annotations. To generate better templates for defects with weak signals, we propose a defect attention module by applying the defect residual for the discriminator to strengthen the elimination of defect regions and suppress changes in the background. A defect cycle-consistent loss is designed by adding structural similarity (SSIM) to the original L1 loss to include the grayscale and structural features; the proposed loss can better model the inner structure of defects. After obtaining the defect-free template, a defect segmentation map can easily be obtained through a simple image comparison and threshold segmentation. Experiments show that the proposed method is both efficient and effective, significantly outperforms other weakly supervised methods, and achieves performance that is comparable or even superior to that of supervised methods on three industrial datasets (intersection over union (IoU) on the DAGM 2007, KSD and CCSD datasets of 78.28\%, 59.43\%,and 68.83\%, respectively). The proposed method can also be employed as a semiautomatic annotation tool combined with active learning.},
  archive      = {J_PR},
  author       = {Shuanlong Niu and Bin Li and Xinggang Wang and Songping He and Yaru Peng},
  doi          = {10.1016/j.patcog.2021.108396},
  journal      = {Pattern Recognition},
  pages        = {108396},
  shortjournal = {Pattern Recognition},
  title        = {Defect attention template generation cycleGAN for weakly supervised surface defect segmentation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian evaluation framework for subjectively annotated
visual recognition tasks. <em>PR</em>, <em>123</em>, 108395. (<a
href="https://doi.org/10.1016/j.patcog.2021.108395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators , i.e., what would the average person’s judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor’s model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictor’s internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance . The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.},
  archive      = {J_PR},
  author       = {Derek S. Prijatelj and Mel McCurrie and Samuel E. Anthony and Walter J. Scheirer},
  doi          = {10.1016/j.patcog.2021.108395},
  journal      = {Pattern Recognition},
  pages        = {108395},
  shortjournal = {Pattern Recognition},
  title        = {A bayesian evaluation framework for subjectively annotated visual recognition tasks},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning directly from synthetic point clouds for
“in-the-wild” 3D face recognition. <em>PR</em>, <em>123</em>, 108394.
(<a href="https://doi.org/10.1016/j.patcog.2021.108394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds-based networks have achieved great attention in 3D object classification, segmentation, and indoor scene semantic parsing , but its application to 3D face recognition is still underdeveloped owing to two main reasons: lack of large-scale 3D facial data and absence of deep neural network that can directly extract discriminative face representations from point clouds. To address these two problems, a PointNet++ based network is proposed in this paper to extract face features directly from point clouds facial scans and a statistical 3D Morphable Model based 3D face synthesizing strategy is established to generate large-scale unreal facial scans to train the proposed network from scratch. A curvature-aware point sampling technique is proposed to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate discriminative facial features deeply. In addition, a novel 3D face transfer learning method is proposed to ease the domain discrepancy between synthetic and ‘in-the-wild’ faces. Experimental results on two public 3D face benchmarks show that the network trained only on synthesized data can also be well generalized to ‘in-the-wild’ 3D face recognition. Our method achieves the state-of-the-art results by achieving an overall rank-1 identification rate of 99.46\% and 99.65\% on FRGCv2 and Bosphorus, respectively. Further, we evaluate on a self-collected dataset to demonstrate the robustness and application potential of our method.},
  archive      = {J_PR},
  author       = {Ziyu Zhang and Feipeng Da and Yi Yu},
  doi          = {10.1016/j.patcog.2021.108394},
  journal      = {Pattern Recognition},
  pages        = {108394},
  shortjournal = {Pattern Recognition},
  title        = {Learning directly from synthetic point clouds for “in-the-wild” 3D face recognition},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised extensions of multi-task tree ensembles.
<em>PR</em>, <em>123</em>, 108393. (<a
href="https://doi.org/10.1016/j.patcog.2021.108393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scale inconsistency is a widely encountered issue in multi-output learning problems. Specifically, target sets with multiple real valued or a mixture of categorical and real valued variables require addressing the scale differences to obtain predictive models with sufficiently good performance. Data transformation techniques are often employed to solve that problem. However, these operations are susceptible to different shortcomings such as changing the statistical properties of the data and increase the computational burden. Scale differences also pose problem in semi-supervised learning (SSL) models as they require processing of unsupervised information where distance measures are commonly employed. Classical distance metrics can be criticized as they lose efficiency when variables exhibit type or scale differences, too. Besides, in higher dimensions distance metrics cause problems due to loss of discriminative power . This paper introduces alternative semi-supervised tree-based strategies that are robust to scale differences both in terms of feature and target variables. We propose use of a scale-invariant proximity measure by means of tree-based ensembles to preserve the original characteristics of the data. We update classical tree derivation procedure to a multi-criteria form to resolve scale inconsistencies. We define proximity based clustering indicators and extend the supervised model with unsupervised criteria. Our experiments show that proposed method significantly outperforms its benchmark learning model that is predictive clustering trees.},
  archive      = {J_PR},
  author       = {Esra Adıyeke and Mustafa Gökçe Baydoğan},
  doi          = {10.1016/j.patcog.2021.108393},
  journal      = {Pattern Recognition},
  pages        = {108393},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised extensions of multi-task tree ensembles},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised deep clustering via contractive feature
representation and focal loss. <em>PR</em>, <em>123</em>, 108386. (<a
href="https://doi.org/10.1016/j.patcog.2021.108386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering aims to promote clustering tasks by combining deep learning and clustering together to learn the clustering-oriented representation, and many approaches have shown their validity. However, the feature learning modules in existing methods hardly learn a discriminative representation. In addition, the label assignment mechanism becomes inefficient when dealing with some hard samples. To address these issues, a new joint optimization clustering framework is proposed through introducing the contractive representation in feature learning and utilizing focal loss in the clustering layer. The contractive penalty term added in feature learning would cause the local feature space contraction, resulting in learning more discriminative features . To our certain knowledge, this is also the first work to utilize the focal loss to improve the label assignment in deep clustering method . Moreover, the construction of the joint optimization framework enables the proposed method to learn feature representation and label assignment simultaneously in an end-to-end way. Finally, we comprehensively compare with some state-of-the-art clustering approaches on several clustering tasks to demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Jinyu Cai and Shiping Wang and Chaoyang Xu and Wenzhong Guo},
  doi          = {10.1016/j.patcog.2021.108386},
  journal      = {Pattern Recognition},
  pages        = {108386},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised deep clustering via contractive feature representation and focal loss},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale signed recurrence plot based time series
classification using inception architectural networks. <em>PR</em>,
<em>123</em>, 108385. (<a
href="https://doi.org/10.1016/j.patcog.2021.108385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the great success of deep neural networks in image classification, recent works use Recurrence Plots (RP) to encode time series as images for classification. RP provide rich texture information and construct long-term time correlations, which are effective supplements to the networks. However, RP cannot handle the scale and length variability of sequences. Moreover, RP have serious tendency confusion problem. They cannot represent the upward and downward trends of sequences effectively. In addition to the defects of RP, existing time series classification (TSC) networks cannot adapt to the various scales of discriminative regions of time series effectively. To tackle these problems, this paper proposes a method, named MSRP-IFCN. It is composed of two submodules, the Multi-scale Signed RP (MSRP) and the Inception Fully Convolutional Network (IFCN). MSRP are proposed to handle the defects of RP. They comprise three components, namely the multi-scale RP, the asymmetric RP and the signed RP. We first use the multi-scale RP to enrich the scales of images. Then, the asymmetric RP are constructed to represent long sequences. Finally, the signed RP images are obtained by multiplying the designed sign masks to remove the tendency confusion. Besides, IFCN is proposed to enhance the existing TSC networks in multi-scale feature extraction. By introducing the modified Inception modules, IFCN obtains extensive receptive fields and better extracts multi-scale features from the MSRP images. Experimental results on 85 UCR datasets indicate the superior performance of MSRP-IFCN. The visualization results further demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Ye Zhang and Yi Hou and Kewei OuYang and Shilin Zhou},
  doi          = {10.1016/j.patcog.2021.108385},
  journal      = {Pattern Recognition},
  pages        = {108385},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale signed recurrence plot based time series classification using inception architectural networks},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level adversarial network for domain adaptive semantic
segmentation. <em>PR</em>, <em>123</em>, 108384. (<a
href="https://doi.org/10.1016/j.patcog.2021.108384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progresses in domain adaptive semantic segmentation demonstrate the effectiveness of adversarial learning (AL) in unsupervised domain adaptation . However, most adversarial learning based methods align source and target distributions at a global image level but neglect the inconsistency around local image regions. This paper presents a novel multi-level adversarial network (MLAN) that aims to address inter-domain inconsistency at both global image level and local region level optimally. MLAN has two novel designs, namely, region-level adversarial learning (RL-AL) and co-regularized adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional context-relations explicitly in the feature space of a labelled source domain and transfers them to an unlabelled target domain via adversarial learning. CR-AL fuses region-level AL and image-level AL optimally via mutual regularization . In addition, we design a multi-level consistency map that can guide domain adaptation in both input space (i.e., image-to-image translation) and output space (i.e., self-training) effectively. Extensive experiments show that MLAN outperforms the state-of-the-art with a large margin consistently across multiple datasets.},
  archive      = {J_PR},
  author       = {Jiaxing Huang and Dayan Guan and Aoran Xiao and Shijian Lu},
  doi          = {10.1016/j.patcog.2021.108384},
  journal      = {Pattern Recognition},
  pages        = {108384},
  shortjournal = {Pattern Recognition},
  title        = {Multi-level adversarial network for domain adaptive semantic segmentation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Majorities help minorities: Hierarchical structure guided
transfer learning for few-shot fault recognition. <em>PR</em>,
<em>123</em>, 108383. (<a
href="https://doi.org/10.1016/j.patcog.2021.108383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the operational safety and reliability, fault recognition of complex systems is becoming an essential process in industrial systems. However, the existing recognition methods mainly focus on common faults with enough data, which ignore that many faults are lack of samples in engineering practice. Transfer learning can be helpful, but irrelevant knowledge transfer can cause performance degradation , especially in complex systems. To address the above problem, a hierarchy guided transfer learning framework (HGTL) is proposed in this paper for fault recognition with few-shot samples. Firstly, we fuse domain knowledge, label semantics and inter-class distance to calculate the affinity between categories, based on which a category hierarchical tree is constructed by hierarchical clustering . Then, guided by the hierarchical structure, the samples in most similar majority classes are selected from the source domain to pre-train the hierarchical feature learning network (HFN) and extract the transferable fault information. For the fault knowledge extracted from the child nodes of one parent node are similar and can be transferred with each other, so the trained HFN can extract better features of few samples classes with the help of the information from similar faults, and used to address few-shot fault recognition problems. Finally, a dataset of a nuclear power system with 65 categories and the widely used Tennessee Eastman dataset are analyzed respectively via the proposed method, as well as state-of-the-art recognition methods for comparison. The experimental results demonstrate the effectiveness and superiority of the proposed method in fault recognition with few-shot problem.},
  archive      = {J_PR},
  author       = {Hao Chen and Ruonan Liu and Zongxia Xie and Qinghua Hu and Jianhua Dai and Junhai Zhai},
  doi          = {10.1016/j.patcog.2021.108383},
  journal      = {Pattern Recognition},
  pages        = {108383},
  shortjournal = {Pattern Recognition},
  title        = {Majorities help minorities: Hierarchical structure guided transfer learning for few-shot fault recognition},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel bio-inspired texture descriptor based on
biodiversity and taxonomic measures. <em>PR</em>, <em>123</em>, 108382.
(<a href="https://doi.org/10.1016/j.patcog.2021.108382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture can be defined as the change of image intensity that forms repetitive patterns resulting from the physical properties of an object’s roughness or differences in a reflection on the surface. Considering that texture forms a system of patterns in a non-deterministic way, biodiversity concepts can help its characterization from an image. This paper proposes a novel approach to quantify such a complex system of diverse patterns through species diversity, richness, and taxonomic distinctiveness. The proposed approach considers each image channel as a species ecosystem and computes species diversity and richness as well as taxonomic measures to describe the texture. Furthermore, the proposed approach takes advantage of ecological patterns’ invariance characteristics to build a permutation, rotation, and translation invariant descriptor. Experimental results on three datasets of natural texture images and two datasets of histopathological images have shown that the proposed texture descriptor has advantages over several texture descriptors and deep methods.},
  archive      = {J_PR},
  author       = {Steve Tsham Mpinda Ataky and Alessandro Lameiras Koerich},
  doi          = {10.1016/j.patcog.2021.108382},
  journal      = {Pattern Recognition},
  pages        = {108382},
  shortjournal = {Pattern Recognition},
  title        = {A novel bio-inspired texture descriptor based on biodiversity and taxonomic measures},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rain-component-aware capsule-GAN for single image
de-raining. <em>PR</em>, <em>123</em>, 108377. (<a
href="https://doi.org/10.1016/j.patcog.2021.108377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images taken in the rain usually have poor visual quality, which may cause difficulties for vision-based analysis systems. The research aims to recover clean image content from a single rainy image by removing rain components without introducing any artifacts. Existing rain removal methods often model the rain component as noise, but it obviously has clear patterns instead of random noise. Motivated by this, we raise the idea to build modules to capture rain patterns for de-raining. A Rain-Component-Aware ( R C A RCA ) network is proposed to capture the characteristics of the rain. We then integrate it into an image-conditioned generative adversarial network (image-cGAN) as a R C A RCA loss to guide the generation of rainless images. This results in the proposed two-branch cGAN, where one branch aims at improving the image visual quality after de-raining, and the other aims at extracting rain patterns so that the rain could be effectively removed. To better capture the spatial relationship of different objects within an image, we incorporate the capsule structure in both generator and discriminator of cGAN, which further improves the quality of generated images. The proposed approach is hence named as RCA-cGAN. Benefited by the RCA loss based two-branch optimization and the capsule structure, RCA-cGAN achieves good de-raining effect. Extensive experimental results on several benchmark datasets show that the R C A RCA network is effective to capture rain patterns and the proposed approach could produce much better de-raining images in terms of both subjective visual quality inspection and objective quantitative assessment .},
  archive      = {J_PR},
  author       = {Fei Yang and Jianfeng Ren and Zheng Lu and Jialu Zhang and Qian Zhang},
  doi          = {10.1016/j.patcog.2021.108377},
  journal      = {Pattern Recognition},
  pages        = {108377},
  shortjournal = {Pattern Recognition},
  title        = {Rain-component-aware capsule-GAN for single image de-raining},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust multi-feature collective non-negative matrix
factorization for ECG biometrics. <em>PR</em>, <em>123</em>, 108376. (<a
href="https://doi.org/10.1016/j.patcog.2021.108376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of electrocardiogram (ECG) biometrics has received considerable attention in recent years. Although some promising methods have been proposed, it is challenging to design a robust and precise method to improve the recognition performance of ECG signals with noise and sample variation. While the advantage of improved local binary pattern (LBP) for establishing identities has been widely recognized, extracting the latent semantics from multiple LBP features has attracted little attention. We propose a robust multi-feature collective non-negative matrix factorization (RMCNMF) model to handle noise and sample variation in ECG Biometrics. We extract multiple LBP histograms as feature descriptors from segmented ECG signals, and propose a multi-feature learning framework that learns unified representations in the shared latent semantic space via collective non-negative matrix factorization. To further enhance the discrimination of learned representations, we integrate label information and multiple norms in the proposed model, which not only preserves intra- and inter-subject similarities but also mitigates the influence of noise and sample variation. RMCNMF can be solved by an efficient iteration method, for which we provide a convergence analysis in detail. Extensive experiments on four ECG databases show that it performs competitively with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yuwen Huang and Gongping Yang and Kuikui Wang and Haiying Liu and Yilong Yin},
  doi          = {10.1016/j.patcog.2021.108376},
  journal      = {Pattern Recognition},
  pages        = {108376},
  shortjournal = {Pattern Recognition},
  title        = {Robust multi-feature collective non-negative matrix factorization for ECG biometrics},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GL-GAN: Adaptive global and local bilevel optimization for
generative adversarial network. <em>PR</em>, <em>123</em>, 108375. (<a
href="https://doi.org/10.1016/j.patcog.2021.108375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Generative Adversarial Networks (GAN) have shown remarkable performance in image generation , there exist some challenges in instability and convergence speed. During the training, the results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model (GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. Based on DCGAN, GL-GAN is able to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, through feature map cues from discriminator output, we propose the adaptive local and global optimization method (Ada-OP) for interactive optimization and observe that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, Oxford Flowers, CelebA-HQ and LSUN datasets.},
  archive      = {J_PR},
  author       = {Ying Liu and Heng Fan and Xiaohui Yuan and Jinhai Xiang},
  doi          = {10.1016/j.patcog.2021.108375},
  journal      = {Pattern Recognition},
  pages        = {108375},
  shortjournal = {Pattern Recognition},
  title        = {GL-GAN: Adaptive global and local bilevel optimization for generative adversarial network},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A coarse-to-fine approach for dynamic-to-static image
translation. <em>PR</em>, <em>123</em>, 108373. (<a
href="https://doi.org/10.1016/j.patcog.2021.108373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic-to-static image translation aims to convert the dynamic scene into static so that dynamic elements are eliminated from the image. Recent works typically see the problem as an image-to-image translation task, and perform the learned feature mapping over the whole dynamic image to synthesize the static image, which leads to unnecessary detail loss in original static regions. To that end, we delicately formulate it as an image inpainting-like problem to fill the missing static pixels in dynamic regions while retaining original static regions. We achieve this by proposing a coarse-to-fine framework. At coarse stage, we utilize a simple encoder-decoder network to rough out the static image. Using the coarse predicted image, we explicitly infer a more accurate dynamic mask to identify both dynamic objects and their shadows, so that the task could be effectively converted to an image inpainting problem. At fine stage, we recover the missing static pixels in the estimated dynamic regions on the basis of their coarse predictions. We enhance the coarse predicted contents by proposing a mutual texture-structure attention module, which enables the dynamic regions to borrow textures and structures separately from distant locations based on contextual similarity. Several losses are combined as the training objective function to generate excellent results with global consistency and fine details. Qualitative and quantitative experiments verify the superiority of our method in restoring high-quality static contents over state-of-the-art models. In addition, we evaluate the usefulness of the recovered static images by using them as query images to improve visual place recognition in dynamic scenes.},
  archive      = {J_PR},
  author       = {Teng Wang and Lin Wu and Changyin Sun},
  doi          = {10.1016/j.patcog.2021.108373},
  journal      = {Pattern Recognition},
  pages        = {108373},
  shortjournal = {Pattern Recognition},
  title        = {A coarse-to-fine approach for dynamic-to-static image translation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DLA-net: Learning dual local attention features for semantic
segmentation of large-scale building facade point clouds. <em>PR</em>,
<em>123</em>, 108372. (<a
href="https://doi.org/10.1016/j.patcog.2021.108372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of building facades is critical for various construction applications, such as urban building reconstruction and damage assessments. As there is a lack of 3D point cloud datasets related to fine-grained building facades , in this work we construct the first large-scale point cloud benchmark dataset for building facade semantic segmentation. In terms of the characteristics of building facade dataset, the existing methods of semantic segmentation cannot fully mine the local neighborhood information of point clouds; therefore, we propose an attention module that learns Dual Local Attention features, called DLA in this paper. The proposed DLA module consists of two blocks, a self-attention block and an attentive pooling block, which both embed an enhanced position encoding block. The DLA module can be easily embedded into various network architectures for point cloud segmentation , naturally resulting in a new 3D semantic segmentation network with an encoder-decoder architecture; we called this network the DLA-Net. Extensive experimental results on our constructed building facade dataset demonstrate that the proposed DLA-Net achieves better performance than the state-of-the-art methods for semantic segmentation.},
  archive      = {J_PR},
  author       = {Yanfei Su and Weiquan Liu and Zhimin Yuan and Ming Cheng and Zhihong Zhang and Xuelun Shen and Cheng Wang},
  doi          = {10.1016/j.patcog.2021.108372},
  journal      = {Pattern Recognition},
  pages        = {108372},
  shortjournal = {Pattern Recognition},
  title        = {DLA-net: Learning dual local attention features for semantic segmentation of large-scale building facade point clouds},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incomplete multi-view clustering with cosine similarity.
<em>PR</em>, <em>123</em>, 108371. (<a
href="https://doi.org/10.1016/j.patcog.2021.108371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering partitions multi-view data suffering from missing views, for which matrix factorization approaches seek the latent representation of incomplete multi-view data and constitute one effective category of methods. To exploit data properties further, manifold structure preserving is also incorporated into matrix factorization . However, previous methods optimized the data similarity matrix in the manifold structure preserving term as an unknown variable, which is not guaranteed to faithfully represent the similarities of the original multi-view data and also increases the computational difficulty. To overcome these drawbacks, in this paper, we propose Incomplete Multi-view Clustering with Cosine Similarity (IMCCS). In IMCCS, we directly calculate the cosine similarity in the original multi-view space to strengthen the ability of preserving the manifold structure of the original multi-view data. There is no need to introduce the additional variable. The manifold structure preserving term with cosine similarity and the matrix factorization term are integrated into a unified objective function. An iterative algorithm with gradient descent is designed to solve this objective. Extensive experiments on multi-view datasets show that IMCCS outperforms state-of-the-art incomplete multi-view clustering methods .},
  archive      = {J_PR},
  author       = {Jun Yin and Shiliang Sun},
  doi          = {10.1016/j.patcog.2021.108371},
  journal      = {Pattern Recognition},
  pages        = {108371},
  shortjournal = {Pattern Recognition},
  title        = {Incomplete multi-view clustering with cosine similarity},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhance to read better: A multi-task adversarial network for
handwritten document image enhancement. <em>PR</em>, <em>123</em>,
108370. (<a href="https://doi.org/10.1016/j.patcog.2021.108370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten document images can be highly affected by degradation for different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.), bad scanning process and so on. These artifacts raise many readability issues for current Handwritten Text Recognition (HTR) algorithms and severely devalue their efficiency. In this paper, we propose an end to end architecture based on Generative Adversarial Networks (GANs) to recover the degraded documents into a c l e a n clean and r e a d a b l e readable form. Unlike the most well-known document binarization methods, which try to improve the visual quality of the degraded document, the proposed architecture integrates a handwritten text recognizer that promotes the generated document image to be more readable. To the best of our knowledge, this is the first work to use the text information while binarizing handwritten documents. Extensive experiments conducted on degraded Arabic and Latin handwritten documents demonstrate the usefulness of integrating the recognizer within the GAN architecture, which improves both the visual quality and the readability of the degraded document images . Moreover, we outperform the state of the art in H-DIBCO challenges, after fine tuning our pre-trained model with synthetically degraded Latin handwritten images, on this task.},
  archive      = {J_PR},
  author       = {Sana Khamekhem Jemni and Mohamed Ali Souibgui and Yousri Kessentini and Alicia Fornés},
  doi          = {10.1016/j.patcog.2021.108370},
  journal      = {Pattern Recognition},
  pages        = {108370},
  shortjournal = {Pattern Recognition},
  title        = {Enhance to read better: A multi-task adversarial network for handwritten document image enhancement},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADCNN: Towards learning adaptive dilation for convolutional
neural networks. <em>PR</em>, <em>123</em>, 108369. (<a
href="https://doi.org/10.1016/j.patcog.2021.108369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dilated convolution kernels are constrained by their shared dilation, keeping them from being aware of diverse spatial contents at different locations. We address such limitations by formulating the dilation as trainable weights with respect to individual positions. We propose Adaptive Dilation Convolutional Neural Networks (ADCNN), a light-weighted extension that allows convolutional kernels to adjust their dilation value based on different contents at the pixel level . Unlike previous content-adaptive models, ADCNN dynamically infers pixel-wise dilation via modeling feed-forward inter-patterns, which provides a new perspective for developing adaptive network structures other than sampling kernel spaces. Our evaluation results indicate ADCNNs can be easily integrated into various backbone networks and consistently outperform their regular counterparts on various visual tasks.},
  archive      = {J_PR},
  author       = {Jie Yao and Dongdong Wang and Hao Hu and Weiwei Xing and Liqiang Wang},
  doi          = {10.1016/j.patcog.2021.108369},
  journal      = {Pattern Recognition},
  pages        = {108369},
  shortjournal = {Pattern Recognition},
  title        = {ADCNN: Towards learning adaptive dilation for convolutional neural networks},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Discrete embedding for attributed graphs. <em>PR</em>,
<em>123</em>, 108368. (<a
href="https://doi.org/10.1016/j.patcog.2021.108368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graphs refer to graphs where both node links and node attributes are observable for analysis. Attributed graph embedding enables joint representation learning of node links and node attributes. Different from classical graph embedding methods such as Deepwalk and node2vec that first project node links into low-dimensional vectors which are then linearly concatenated with node attribute vectors as node representation, attributed graph embedding fully explores data dependence between node links and attributes by either using node attributes as class labels to supervise structure learning from node links, or reversely using node links to supervise the learning from node attributes. However, existing attributed graph embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. In this paper, we study a new problem of discrete embedding for attributed graphs that can learn succinct node representations. Specifically, we present a Binarized Attributed Network Embedding model ( BANE for short) to learn binary node representation by factorizing a Weisfeiler-Lehman proximity matrix under the constraint of binary node representation. Furthermore, based on BANE, we propose a new Low-bit Quantization for Attributed Network Representation learning model (LQANR for short) to learn even more compact node representation of bit-width values. Theoretical analysis and empirical studies on real-world datasets show that the new discrete embedding models outperform benchmark methods.},
  archive      = {J_PR},
  author       = {Hong Yang and Ling Chen and Shirui Pan and Haishuai Wang and Peng Zhang},
  doi          = {10.1016/j.patcog.2021.108368},
  journal      = {Pattern Recognition},
  pages        = {108368},
  shortjournal = {Pattern Recognition},
  title        = {Discrete embedding for attributed graphs},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified deep sparse graph attention network for scene
graph generation. <em>PR</em>, <em>123</em>, 108367. (<a
href="https://doi.org/10.1016/j.patcog.2021.108367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) plays an important role in deep understanding of the visual scene. Despite the empirical success of traditional methods in many applications, they still have several challenges in the high computational complexity of dense graph and the inaccurate pruning of sparse graph. To tackle these problems, we propose a novel deep sparse graph attention network to mine the rich contextual clues and simultaneously preserve the statistical co-occurrence knowledge of SGG. Specifically, our Relationship Measurement Network (RelMN) is adapted to first classify all object pairs in dense graph as the foreground and background categories to filter the false relationships and then construct a sparse graph efficiently. Meanwhile, we design a novel feature aggregation and update method via graphical message passing to jointly learn the node and edge features for object recognition and relationship classification in the graph attention network . Extensive experimental results on the large scale VG and VRD datasets demonstrate our proposed method outperforms several state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Hao Zhou and Yazhou Yang and Tingjin Luo and Jun Zhang and Shuohao Li},
  doi          = {10.1016/j.patcog.2021.108367},
  journal      = {Pattern Recognition},
  pages        = {108367},
  shortjournal = {Pattern Recognition},
  title        = {A unified deep sparse graph attention network for scene graph generation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A polarization fusion network with geometric feature
embedding for SAR ship classification. <em>PR</em>, <em>123</em>,
108365. (<a href="https://doi.org/10.1016/j.patcog.2021.108365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current synthetic aperture radar (SAR) ship classifiers using convolutional neural networks (CNNs) offer state-of-the-art performance. Yet, they still have two defects potentially hindering accuracy progress – polarization insufficient utilization and traditional feature abandonment. Therefore, we propose a polarization fusion network with geometric feature embedding (PFGFE-Net) to solve them. PFGFE-Net achieves the polarization fusion (PF) from the input data, feature-level, and decision-level. Moreover, the geometric feature embedding (GFE) enriches expert experience. Results on OpenSARShip reveal PFGFE-Net&#39;s excellent performance.},
  archive      = {J_PR},
  author       = {Tianwen Zhang and Xiaoling Zhang},
  doi          = {10.1016/j.patcog.2021.108365},
  journal      = {Pattern Recognition},
  pages        = {108365},
  shortjournal = {Pattern Recognition},
  title        = {A polarization fusion network with geometric feature embedding for SAR ship classification},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised active salient object detection.
<em>PR</em>, <em>123</em>, 108364. (<a
href="https://doi.org/10.1016/j.patcog.2021.108364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel semi-supervised active salient object detection (SOD) method that actively acquires a small subset of the most discriminative and representative samples for labeling. Two main contributions have been made to prevent the method from being overwhelmed by labeling similar distributed samples. First, we design a saliency encoder-decoder with adversarial discriminator to generate a confidence map, representing the network uncertainty on the current prediction. Then, we select the least confident (discriminative) samples from the unlabeled pool to form the “candidate labeled pool”. Second, we train a Variational Auto-Encoder (VAE) to select and add the most representative data from the “candidate labeled pool” into the labeled pool by comparing their corresponding features in the latent space. Within our framework, these two networks are optimized conditioned on the states of each other progressively. Experimental results on six benchmarking SOD datasets demonstrate that our annotation-efficient learning based salient object detection method, reaching to 14\% labeling budget, can be on par with the state-of-the-art fully-supervised deep SOD models. The source code is publicly available via our project page: https://github.com/JingZhang617/Semi-sup-active-self-sup-Learning .},
  archive      = {J_PR},
  author       = {Yunqiu Lv and Bowen Liu and Jing Zhang and Yuchao Dai and Aixuan Li and Tong Zhang},
  doi          = {10.1016/j.patcog.2021.108364},
  journal      = {Pattern Recognition},
  pages        = {108364},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised active salient object detection},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain structure preserving projection for
heterogeneous domain adaptation. <em>PR</em>, <em>123</em>, 108362. (<a
href="https://doi.org/10.1016/j.patcog.2021.108362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem . It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65 vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Qian Wang and Toby P. Breckon},
  doi          = {10.1016/j.patcog.2021.108362},
  journal      = {Pattern Recognition},
  pages        = {108362},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain structure preserving projection for heterogeneous domain adaptation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to select cuts for efficient mixed-integer
programming. <em>PR</em>, <em>123</em>, 108353. (<a
href="https://doi.org/10.1016/j.patcog.2021.108353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cutting plane methods play a significant role in modern solvers for tackling mixed-integer programming (MIP) problems. Proper selection of cuts would remove infeasible solutions in the early stage, thus largely reducing the computational burden without hurting the solution accuracy. However, the major cut selection approaches heavily rely on heuristics, which strongly depend on the specific problem at hand and thus limit their generalization capability. In this paper, we propose a data-driven and generalizable cut selection approach, named Cut Ranking , in the settings of multiple instance learning . To measure the quality of the candidate cuts, a scoring function, which takes the instance-specific cut features as inputs, is trained and applied in cut ranking and selection. In order to evaluate our method, we conduct extensive experiments on both synthetic datasets and real-world datasets. Compared with commonly used heuristics for cut selection, the learning-based policy has shown to be more effective, and is capable of generalizing over multiple problems with different properties. Cut Ranking has been deployed in an industrial solver for large-scale MIPs. In the online A/B testing of the product planning problems with more than 10 7 107 variables and constraints daily, Cut Ranking has achieved the average speedup ratio of 12.42\% over the production solver without any accuracy loss of solution.},
  archive      = {J_PR},
  author       = {Zeren Huang and Kerong Wang and Furui Liu and Hui-Ling Zhen and Weinan Zhang and Mingxuan Yuan and Jianye Hao and Yong Yu and Jun Wang},
  doi          = {10.1016/j.patcog.2021.108353},
  journal      = {Pattern Recognition},
  pages        = {108353},
  shortjournal = {Pattern Recognition},
  title        = {Learning to select cuts for efficient mixed-integer programming},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified curiosity-driven learning with smoothed intrinsic
reward estimation. <em>PR</em>, <em>123</em>, 108352. (<a
href="https://doi.org/10.1016/j.patcog.2021.108352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.},
  archive      = {J_PR},
  author       = {Fuxian Huang and Weichao Li and Jiabao Cui and Yongjian Fu and Xi Li},
  doi          = {10.1016/j.patcog.2021.108352},
  journal      = {Pattern Recognition},
  pages        = {108352},
  shortjournal = {Pattern Recognition},
  title        = {Unified curiosity-driven learning with smoothed intrinsic reward estimation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank inter-class sparsity based semi-flexible target
least squares regression for feature representation. <em>PR</em>,
<em>123</em>, 108346. (<a
href="https://doi.org/10.1016/j.patcog.2021.108346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least squares regression (LSR) is an important machine learning method for feature extraction, feature selection, and image classification . For the training samples, there are correlations among samples from the same class. Therefore, many LSR-based methods utilize this property to pursue discriminative representation. However, if the training samples contain noise or outliers, it will be hard to obtain the exact inter-class correlation. To address this problem, in this paper, a novel LSR-based method is proposed, named low-rank inter-class sparsity based semi-flexible target least squares regression (LIS_StLSR). Firstly, the low-rank representation method is utilized to achieve the intrinsic characteristics of the training samples. Afterwards, the low-rank inter-class sparsity constraint is used to force the projected data to have an exact common sparsity structure in each class, which will be robust to noise and outliers in the training samples. This step can also reduce margins of samples from the same class and enlarge margins of samples from different classes to make the projection matrix discriminative. The low-rank representation and the discriminative projection matrix are jointly learned such that they can be boosted mutually. Moreover, a semi-flexible regression target matrix is introduced to measure the regression error more accurately, thus the regression performance can be enhanced to improve the classification accuracy . Experiments are implemented on the different databases of Yale B, AR, LFW, CASIA NIR-VIS, 15-Scene SPF, COIL-20, and Caltech 101, illustrating that the proposed LIS_StLSR outperforms many state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Shuping Zhao and Jigang Wu and Bob Zhang and Lunke Fei},
  doi          = {10.1016/j.patcog.2021.108346},
  journal      = {Pattern Recognition},
  pages        = {108346},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank inter-class sparsity based semi-flexible target least squares regression for feature representation},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic generation of moment invariant bases for 2D and
3D tensor fields. <em>PR</em>, <em>123</em>, 108313. (<a
href="https://doi.org/10.1016/j.patcog.2021.108313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moment invariants have been successfully applied to pattern detection tasks in 2D and 3D scalar, vector, and matrix valued data. However so far no flexible basis of invariants exists, i.e., no set that is optimal in the sense that it is complete and independent for every input pattern. In this paper, we prove that a basis of moment invariants can be generated that consists of tensor contractions of not more than two different moment tensors each under the conjecture of the set of all possible tensor contractions to be complete. This result allows us to derive the first generator algorithm that produces flexible bases of moment invariants with respect to orthogonal transformations by selecting a single non-zero moment to pair with all others in these two-factor products. Since at least one non-zero moment can be found in every non-zero pattern, this approach always generates a complete set of descriptors.},
  archive      = {J_PR},
  author       = {Roxana Bujack and Xinhua Zhang and Tomáš Suk and David Rogers},
  doi          = {10.1016/j.patcog.2021.108313},
  journal      = {Pattern Recognition},
  pages        = {108313},
  shortjournal = {Pattern Recognition},
  title        = {Systematic generation of moment invariant bases for 2D and 3D tensor fields},
  volume       = {123},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face mask recognition from audio: The MASC database and an
overview on the mask challenge. <em>PR</em>, <em>122</em>, 108361. (<a
href="https://doi.org/10.1016/j.patcog.2021.108361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden outbreak of COVID-19 has resulted in tough challenges for the field of biometrics due to its spread via physical contact, and the regulations of wearing face masks. Given these constraints, voice biometrics can offer a suitable contact-less biometric solution; they can benefit from models that classify whether a speaker is wearing a mask or not. This article reviews the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 COMputational PARalinguistics challengE (ComParE), which focused on the following classification task : Given an audio chunk of a speaker, classify whether the speaker is wearing a mask or not. First, we report the collection of the Mask Augsburg Speech Corpus (MASC) and the baseline approaches used to solve the problem, achieving a performance of 71.8\% 71.8\% Unweighted Average Recall (UAR). We then summarise the methodologies explored in the submitted and accepted papers that mainly used two common patterns: (i) phonetic-based audio features, or (ii) spectrogram representations of audio combined with Convolutional Neural Networks (CNNs) typically used in image processing . Most approaches enhance their models by adapting ensembles of different models and attempting to increase the size of the training data using various techniques. We review and discuss the results of the participants of this sub-challenge, where the winner scored a UAR of 80.1\% 80.1\% . Moreover, we present the results of fusing the approaches, leading to a UAR of 82.6\% 82.6\% . Finally, we present a smartphone app that can be used as a proof of concept demonstration to detect in real-time whether users are wearing a face mask; we also benchmark the run-time of the best models.},
  archive      = {J_PR},
  author       = {Mostafa M. Mohamed and Mina A. Nessiem and Anton Batliner and Christian Bergler and Simone Hantke and Maximilian Schmitt and Alice Baird and Adria Mallol-Ragolta and Vincent Karas and Shahin Amiriparian and Björn W. Schuller},
  doi          = {10.1016/j.patcog.2021.108361},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {108361},
  shortjournal = {Pattern Recognition},
  title        = {Face mask recognition from audio: The MASC database and an overview on the mask challenge},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reasoning structural relation for occlusion-robust facial
landmark localization. <em>PR</em>, <em>122</em>, 108325. (<a
href="https://doi.org/10.1016/j.patcog.2021.108325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In facial landmark localization tasks, various occlusions heavily degrade the localization accuracy due to the partial observability of facial features . This paper proposes a structural relation network (SRN) for occlusion-robust landmark localization. Unlike most existing methods that simply exploit the shape constraint, the proposed SRN aims to capture the structural relations among different facial components. These relations can be considered a more powerful shape constraint against occlusion. To achieve this, a hierarchical structural relation module (HSRM) is designed to hierarchically reason the structural relations that represent both long- and short-distance spatial dependencies. Compared with existing network architectures ,the HSRM can efficiently model the spatial relations by leveraging its geometry-aware network architecture, which reduces the semantic ambiguity caused by occlusion. Moreover, the SRN augments the training data by synthesizing occluded faces. To further extend our SRN for occluded video data, we formulate the occluded face synthesis as a Markov decision process (MDP). Specifically, it plans the movement of the dynamic occlusion based on an accumulated reward associated with the performance degradation of the pre-trained SRN. This procedure augments hard samples for robust facial landmark tracking. Extensive experimental results indicate that the proposed method achieves outstanding performance on occluded and masked faces. Code is available at https://github.com/zhuccly/SRN},
  archive      = {J_PR},
  author       = {Congcong Zhu and Xiaoqiang Li and Jide Li and Songmin Dai and Weiqin Tong},
  doi          = {10.1016/j.patcog.2021.108325},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {108325},
  shortjournal = {Pattern Recognition},
  title        = {Reasoning structural relation for occlusion-robust facial landmark localization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An empirical study of the impact of masks on face
recognition. <em>PR</em>, <em>122</em>, 108308. (<a
href="https://doi.org/10.1016/j.patcog.2021.108308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has a wide range of applications like video surveillance, security, access control, etc. Over the past decade, the field of face recognition has matured and grown at par with the latest advancements in technology, particularly deep learning. Convolution Neural Networks have surpassed human accuracy in Face Recognition on popular evaluation tests such as LFW. However, most existing models evaluate their performance with an assumption of the availability of full facial information. The COVID-19 pandemic has laid forth challenges to this assumption, and to the performance of existing methods and leading-edge algorithms in the field of face recognition. This is in the wake of an explosive increase in the number of people wearing face masks. The reduced amount of facial information available to a recognition system from a masked face impacts their discrimination ability. In this context, we design and conduct a series of experiments comparing the masked face recognition performances of CNN architectures available in literature and exploring possible alterations in loss functions, architectures, and training methods that can enable existing methods to fully extract and leverage the limited facial information available in a masked face. We evaluate existing CNN-based face recognition systems for their performance against datasets composed entirely of masked faces, in contrast to the existing standard evaluations where masked or occluded faces are a rare occurrence. The study also presents evidence denoting an increased impact of network depth on performance compared to standard face recognition. Our observations indicate that substantial performance gains can be achieved by the introduction of masked faces in the training set. The study also inferred that various parameter settings determined suitable for standard face recognition are not ideal for masked face recognition. Through empirical analysis we derived new value recommendations for these parameters and settings.},
  archive      = {J_PR},
  author       = {Govind Jeevan and Geevar C. Zacharias and Madhu S. Nair and Jeny Rajan},
  doi          = {10.1016/j.patcog.2021.108308},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {108308},
  shortjournal = {Pattern Recognition},
  title        = {An empirical study of the impact of masks on face recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based stock correlation and prediction for
high-frequency trading systems. <em>PR</em>, <em>122</em>, 108209. (<a
href="https://doi.org/10.1016/j.patcog.2021.108209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71\% and the standard deviation of daily returns is only 0.42\% in three months of operation. Only 1 week loss in 13 weeks of running time.},
  archive      = {J_PR},
  author       = {Tao Yin and Chenzhengyi Liu and Fangyu Ding and Ziming Feng and Bo Yuan and Ning Zhang},
  doi          = {10.1016/j.patcog.2021.108209},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {108209},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based stock correlation and prediction for high-frequency trading systems},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based resilience guarantee for multi-UAV
collaborative QoS management. <em>PR</em>, <em>122</em>, 108166. (<a
href="https://doi.org/10.1016/j.patcog.2021.108166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned and intelligent technologies are the future development trend in the business field. It is of great significance for the connotation analysis and application characterization of massive interactive data. Particularly, during major epidemics or disasters, how to provide business services safely and securely is crucial. Specifically, providing users with resilient and guaranteed communication services is a challenging business task when the communication facilities are damaged. Unmanned aerial vehicles (UAVs), with flexible deployment and high maneuverability , can be used to serve as aerial base stations (BSs) to establish emergency networks . However, it is challenging to control multiple UAVs to provide efficient and fair communication quality of service (QoS) to users due to their limited communication service capabilities. In this paper, we propose a learning-based resilience guarantee framework for multi-UAV collaborative QoS management. We formulate this problem as a partial observable Markov decision process and solve it with proximal policy optimization (PPO), which is a policy-based deep reinforcement learning method. A centralized training and decentralized execution paradigm is used, where the experience collected by all UAVs is used to train the shared control policy. Each UAV takes actions based on the partial environment information it observes. In addition, the design of the reward function considers the average and variance of the communication QoS of all users. Extensive simulations are conducted for performance evaluation. The simulation results indicate that (1) the trained policies can adapt to different scenarios and provide resilient and guaranteed communication QoS to users, (2) increasing the number of UAVs can compensate for the lack of service capabilities of UAVs, (3) when UAVs have local communication service capabilities, the policies trained with PPO have better performance compared with the policies trained with other algorithms.},
  archive      = {J_PR},
  author       = {Chengchao Bai and Peng Yan and Xiaoqiang Yu and Jifeng Guo},
  doi          = {10.1016/j.patcog.2021.108166},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {108166},
  shortjournal = {Pattern Recognition},
  title        = {Learning-based resilience guarantee for multi-UAV collaborative QoS management},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end supermask pruning: Learning to prune image
captioning models. <em>PR</em>, <em>122</em>, 108366. (<a
href="https://doi.org/10.1016/j.patcog.2021.108366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of deep models, research work on image captioning has led to a remarkable gain in raw performance over the last decade, along with increasing model complexity and computational cost. However, surprisingly works on compression of deep networks for image captioning task has received little to no attention. For the first time in image captioning research, we provide an extensive comparison of various unstructured weight pruning methods on three different popular image captioning architectures, namely Soft-Attention, Up-Down and Object Relation Transformer . Following this, we propose a novel end-to-end weight pruning method that performs gradual sparsification based on weight sensitivity to the training loss. The pruning schemes are then extended with encoder pruning, where we show that conducting both decoder pruning and training simultaneously prior to the encoder pruning provides good overall performance. Empirically, we show that an 80\% to 95\% sparse network (up to 75\% reduction in model size) can either match or outperform its dense counterpart. The code and pre-trained models for Up-Down and Object Relation Transformer that are capable of achieving CIDEr scores &gt; &amp;gt; 120 on the MS-COCO dataset but with only 8.7 MB and 14.5 MB in model size (size reduction of 96\% and 94\% respectively against dense versions) are publicly available at https://github.com/jiahuei/sparse-image-captioning .},
  archive      = {J_PR},
  author       = {Jia Huei Tan and Chee Seng Chan and Joon Huang Chuah},
  doi          = {10.1016/j.patcog.2021.108366},
  journal      = {Pattern Recognition},
  pages        = {108366},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end supermask pruning: Learning to prune image captioning models},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new bayesian poisson denoising algorithm based on nonlocal
means and stochastic distances. <em>PR</em>, <em>122</em>, 108363. (<a
href="https://doi.org/10.1016/j.patcog.2021.108363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poisson noise is the main cause of degradation of many imaging modalities . However, many of the proposed methods for reducing noise in images lack a formal approach. Our work develops a new, general, formal and computationally efficient bayesian Poisson denoising algorithm, based on the Nonlocal Means framework and replacing the euclidean distance by stochastic distances, which are more appropriate for the denoising problem . It takes advantage of the conjugacy of Poisson and gamma distributions to obtain its computational efficiency. When dealing with low dose CT images, the algorithm operates on the sinogram, modeling the rates of the Poisson noise by the Gamma distribution. Based on the Bayesian formulation and the conjugacy property, the likelihood follows the Poisson distribution, while the a posteriori distribution is also described by the Gamma distribution. The derived algorithm is applied to simulated and real low-dose CT images and compared to several algorithms proposed in the literature, with competitive results.},
  archive      = {J_PR},
  author       = {Rodrigo C. Evangelista and Denis H.P. Salvadeo and Nelson D.A. Mascarenhas},
  doi          = {10.1016/j.patcog.2021.108363},
  journal      = {Pattern Recognition},
  pages        = {108363},
  shortjournal = {Pattern Recognition},
  title        = {A new bayesian poisson denoising algorithm based on nonlocal means and stochastic distances},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Skeleton-based relational reasoning for group activity
analysis. <em>PR</em>, <em>122</em>, 108360. (<a
href="https://doi.org/10.1016/j.patcog.2021.108360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism , that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.},
  archive      = {J_PR},
  author       = {Mauricio Perez and Jun Liu and Alex C. Kot},
  doi          = {10.1016/j.patcog.2021.108360},
  journal      = {Pattern Recognition},
  pages        = {108360},
  shortjournal = {Pattern Recognition},
  title        = {Skeleton-based relational reasoning for group activity analysis},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative unimodal feature selection and fusion for
RGB-d salient object detection. <em>PR</em>, <em>122</em>, 108359. (<a
href="https://doi.org/10.1016/j.patcog.2021.108359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detectors make use of the complementary information of RGB-D images to overcome the challenging scenarios, e.g., low contrast, clutter backgrounds. However, these models generally neglect the fact that one of the input images may be poor in quality. This will adversely affect the discriminative ability of cross-modal features when the two channels are fused directly. To address this issue, a novel end-to-end RGB-D salient object detection model is proposed in this paper. At the core of our model is a Semantic-Guided Modality-Weight Map Generation (SG-MWMG) sub-network, producing modality-weight maps to indicate which regions on both modalities are high-quality regions, given input RGB-D images and the guidance of their semantic information. Based on it, a Bi-directional Multi-scale Cross-modal Feature Fusion (Bi-MCFF) module is presented, where the interactions of the features across different modalities and scales are exploited by using a novel bi-directional structure for better capturing cross-scale and cross-modal complementary information. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Nianchang Huang and Yongjiang Luo and Qiang Zhang and Jungong Han},
  doi          = {10.1016/j.patcog.2021.108359},
  journal      = {Pattern Recognition},
  pages        = {108359},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative unimodal feature selection and fusion for RGB-D salient object detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revisiting image captioning via maximum discrepancy
competition. <em>PR</em>, <em>122</em>, 108358. (<a
href="https://doi.org/10.1016/j.patcog.2021.108358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is a hot research topic bridging computer vision and natural language processing during the past several decades. It has achieved great progress with the help of large-scale datasets and deep learning techniques. Though the variety of image captioning models (ICMs), the performance of ICMs have got stuck in a bottleneck judging from the publicly published results. Considering the marginal performance gains brought by recent ICMs, we raise the following question: “what about the performances of the recent ICMs achieve on in-the-wild images? To clarify this question, we compare existing ICMs by evaluating their generalization ability. Specifically, we propose a novel method based on maximum discrepancy competition to diagnose existing ICMs. Firstly, we establish a new test set containing only informative images selected by adopting maximum discrepancy competition on the existing ICMs, from an arbitrary large-scale raw image set. Secondly, a small-scale and low-cost subjective annotation experiment is conducted on the new test set. Thirdly, we rank the generalization ability of the existing ICMs by comparing their performances on the new test set. Finally, the keys of different ICMs are demonstrated based on a detailed analysis of experimental results. Our analysis yields several interesting findings, including that 1) Using simultaneously low- and high-level object features may be an effective tool to boost the generalization ability for the Transformer based ICMs. 2) Self-attention mechanism may provide better modelling ability for inter- and intra-modal data than other attention-based mechanisms. 3) Constructing an ICM with a multistage language decoder may be a promising way to improve its performance.},
  archive      = {J_PR},
  author       = {Boyang Wan and Wenhui Jiang and Yu-Ming Fang and Minwei Zhu and Qin Li and Yang Liu},
  doi          = {10.1016/j.patcog.2021.108358},
  journal      = {Pattern Recognition},
  pages        = {108358},
  shortjournal = {Pattern Recognition},
  title        = {Revisiting image captioning via maximum discrepancy competition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous conditional random field convolution for point
cloud segmentation. <em>PR</em>, <em>122</em>, 108357. (<a
href="https://doi.org/10.1016/j.patcog.2021.108357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud segmentation is the foundation of 3D environmental perception for modern intelligent systems. To solve this problem and image segmentation , conditional random fields (CRFs) are usually formulated as discrete models in label space to encourage label consistency, which is actually a kind of postprocessing. In this paper, we reconsider the CRF in feature space for point cloud segmentation because it can capture the structure of features well to improve the representation ability of features rather than simply smoothing. Therefore, we first model the point cloud features with a continuous quadratic energy model and formulate its solution process as a message-passing graph convolution, by which it can be easily integrated into a deep network. We theoretically demonstrate that the message passing in the graph convolution is equivalent to the mean-field approximation of a continuous CRF model. Furthermore, we build an encoder-decoder network based on the proposed continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in the decoding layers can restore the details of high-level features that were lost in the encoding stage to enhance the location ability of the network, thereby benefiting segmentation. Analogous to the CRFConv, we show that the classical discrete CRF can also work collaboratively with the proposed network via another graph convolution to further improve the segmentation results. Experiments on various point cloud benchmarks demonstrate the effectiveness and robustness of the proposed method. Compared with the state-of-the-art methods, the proposed method can also achieve competitive segmentation performance .},
  archive      = {J_PR},
  author       = {Fei Yang and Franck Davoine and Huan Wang and Zhong Jin},
  doi          = {10.1016/j.patcog.2021.108357},
  journal      = {Pattern Recognition},
  pages        = {108357},
  shortjournal = {Pattern Recognition},
  title        = {Continuous conditional random field convolution for point cloud segmentation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient k-nearest neighbor search based on clustering and
adaptive k values. <em>PR</em>, <em>122</em>, 108356. (<a
href="https://doi.org/10.1016/j.patcog.2021.108356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k k -Nearest Neighbor ( k k NN) algorithm is widely used in the supervised learning field and, particularly, in search and classification tasks , owing to its simplicity, competitive performance, and good statistical properties. However, its inherent inefficiency prevents its use in most modern applications due to the vast amount of data that the current technological evolution generates, being thus the optimization of k k NN-based search strategies of particular interest. This paper introduces the caKD+ algorithm, which tackles this limitation by combining the use of feature learning techniques, clustering methods , adaptive search parameters per cluster, and the use of pre-calculated K-Dimensional Tree structures, and results in a highly efficient search method. This proposal has been evaluated using 10 datasets and the results show that caKD+ significantly outperforms 16 state-of-the-art efficient search methods while still depicting such an accurate performance as the one by the exhaustive k k NN search.},
  archive      = {J_PR},
  author       = {Antonio Javier Gallego and Juan Ramón Rico-Juan and Jose J. Valero-Mas},
  doi          = {10.1016/j.patcog.2021.108356},
  journal      = {Pattern Recognition},
  pages        = {108356},
  shortjournal = {Pattern Recognition},
  title        = {Efficient k-nearest neighbor search based on clustering and adaptive k values},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GeoConv: Geodesic guided convolution for facial action unit
recognition. <em>PR</em>, <em>122</em>, 108355. (<a
href="https://doi.org/10.1016/j.patcog.2021.108355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D morphable face model . Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that our approach significantly outperforms the state-of-the-art AU recognition methods.},
  archive      = {J_PR},
  author       = {Yuedong Chen and Guoxian Song and Zhiwen Shao and Jianfei Cai and Tat-Jen Cham and Jianmin Zheng},
  doi          = {10.1016/j.patcog.2021.108355},
  journal      = {Pattern Recognition},
  pages        = {108355},
  shortjournal = {Pattern Recognition},
  title        = {GeoConv: Geodesic guided convolution for facial action unit recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gradient-aligned convolution neural network. <em>PR</em>,
<em>122</em>, 108354. (<a
href="https://doi.org/10.1016/j.patcog.2021.108354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation , such as medical images, microscopic images , remote sensing images and astronomical images. In this paper, we propose a novel convolution operation , called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation , feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360 ∘ 360∘ rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.},
  archive      = {J_PR},
  author       = {You Hao and Ping Hu and Shirui Li and Jayaram K. Udupa and Yubing Tong and Hua Li},
  doi          = {10.1016/j.patcog.2021.108354},
  journal      = {Pattern Recognition},
  pages        = {108354},
  shortjournal = {Pattern Recognition},
  title        = {Gradient-aligned convolution neural network},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loss functions for pose guided person image generation.
<em>PR</em>, <em>122</em>, 108351. (<a
href="https://doi.org/10.1016/j.patcog.2021.108351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose guided person image generation aims to transform a source person image to a target pose . It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and show promising performance. However, they simply adopt loss functions widely used in generic image generation tasks, e.g. , adversarial loss, L1-norm loss, perceptual loss, and style loss, which fail to consider the unique structural patterns of a person. In addition, it remains unclear how each individual loss and their combinations impact the generated person images. The goal of this paper is to have a comprehensive study of loss functions for pose guided person image generation. After revisiting these generic loss functions, we consider the structural similarity (SSIM) index as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. In addition, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or texture, we extend the SSIM loss into a novel Part-based SSIM (PSSIM) loss to explicitly account for the articulated body structure. A new PSSIM metric is then proposed naturally to access the quality of generated person images. In order to have a deep investigation of loss functions, we conduct extensive experiments including single-loss analysis, multi-loss combination analysis, optimal loss combination search, and comparison with state-of-the-art methods. Both quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images, (2) the combination of adversarial loss, perceptual loss, and PSSIM loss is the optimal choice for person image generation, and (3) the proposed PSSIM loss is complementary to prior losses and helps improve the performance of state-of-the art methods. We have made the source code publicly available at https://github.com/shyern/Pose-Transfer-pSSIM.git .},
  archive      = {J_PR},
  author       = {Haoyue Shi and Le Wang and Nanning Zheng and Gang Hua and Wei Tang},
  doi          = {10.1016/j.patcog.2021.108351},
  journal      = {Pattern Recognition},
  pages        = {108351},
  shortjournal = {Pattern Recognition},
  title        = {Loss functions for pose guided person image generation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi‐frame based adversarial learning approach for video
surveillance. <em>PR</em>, <em>122</em>, 108350. (<a
href="https://doi.org/10.1016/j.patcog.2021.108350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground-background segmentation (FBS) is one of the prime tasks for automated video-based applications like traffic analysis and surveillance. The different practical scenarios like weather degraded videos, irregular moving objects, dynamic background, etc., make FBS a challenging task. The existing FBS algorithms mainly depend on one of the three different factors, namely (1) complicated training process, (2) additionally trained modules for other applications, or (3) neglect the inter-frame spatio-temporal structural dependencies. In this paper, a novel multi-frame-based adversarial learning network is proposed with multi-scale inception and residual module for FBS. As, FBS is a temporal enlightenment-based problem, a temporal encoding mechanism with decreasing variable intervals is proposed for the input frame selection. The proposed network comprises multi-scale inception and residual connection-based dense modules to learn prominent features of the foreground object(s). Also, feedback of the estimated foreground map of previous frame is utilized to exhibit more temporal consistency. Learning of the network is concentrated in different ways like cross-data, disjoint, and global training-testing for FBS. The qualitative and quantitative experimental analysis of the proposed approach is done on three benchmark datasets for FBS. Experimental analysis on three benchmark datasets proves the significance of the proposed approach as compared to state-of-the-art FBS approaches.},
  archive      = {J_PR},
  author       = {Prashant W. Patil and Akshay Dudhane and Sachin Chaudhary and Subrahmanyam Murala},
  doi          = {10.1016/j.patcog.2021.108350},
  journal      = {Pattern Recognition},
  pages        = {108350},
  shortjournal = {Pattern Recognition},
  title        = {Multi‐frame based adversarial learning approach for video surveillance},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-attention augmented network for single image
super-resolution. <em>PR</em>, <em>122</em>, 108349. (<a
href="https://doi.org/10.1016/j.patcog.2021.108349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to improve the representational power of visual features extracted by deep convolutional neural networks is of crucial importance for high-quality image super-resolution. To address this issue, we propose a multi-attention augmented network, which mainly consists of content-, orientation- and position-aware modules. Specifically, we develop an attention augmented U-net structure to form the content-aware module in order to learn and combine multi-scale informative features within a large receptive field. To better reconstruct image details in different directions, we design a set of pre-defined sparse kernels to construct the orientation-aware module, which can extract more representative multi-orientation features and enhance the discriminative capacity in stacked convolutional stages. Then these extracted features are adaptively fused through channel attention mechanism . In upscale stage, the position-aware module adopts a novel self-attention to reweight the element-wise value of final low-resolution feature maps, for further suppressing the possible artifacts. Experimental results demonstrate that our method obtains better reconstruction accuracy and perceptual quality against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Rui Chen and Heng Zhang and Jixin Liu},
  doi          = {10.1016/j.patcog.2021.108349},
  journal      = {Pattern Recognition},
  pages        = {108349},
  shortjournal = {Pattern Recognition},
  title        = {Multi-attention augmented network for single image super-resolution},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale spatial-spectral fusion based on multi-input
fusion calculation and coordinate attention for hyperspectral image
classification. <em>PR</em>, <em>122</em>, 108348. (<a
href="https://doi.org/10.1016/j.patcog.2021.108348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the deep learning method that integrates image features has gradually become a hot development trend in hyperspectral image classification. However, these studies did not fully consider the fusion of image features , and did not remove the interference to the classification process caused by the difference in the size of the objects. These factors hinder the further improvement of the classification effect. To eliminate these drawbacks, this paper proposes a more effective fusion scheme (MSF-MIF), which realizes the fusion from the perspective of location characteristics and channel characteristics through 3D convolution and spatial feature concatenation. In view of the size discrepancy of the objects to be classified, this method extracts features from several input patches of different scales and uses the novel calculation method proposed to fuse them, which minimizes the interference caused by size differences. In addition, this research also tried to quote the coordinate attention structure for the first time that combines spatial and spectral attention features to further improve the classification performance. Experimental results on three commonly used data sets prove that this framework has achieved a breakthrough in classification accuracy .},
  archive      = {J_PR},
  author       = {Lina Yang and Fengqi Zhang and Patrick Shen-Pei Wang and Xichun Li and Zuqiang Meng},
  doi          = {10.1016/j.patcog.2021.108348},
  journal      = {Pattern Recognition},
  pages        = {108348},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coarse-to-fine-grained method for image splicing region
detection. <em>PR</em>, <em>122</em>, 108347. (<a
href="https://doi.org/10.1016/j.patcog.2021.108347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we aim to improve the accuracy of image splicing detection. We propose a progressive image splicing detection method that can detect the position and shape of spliced region. Because image splicing is likely to destroy or change the consistent correlation pattern introduced by color filter array (CFA) interpolation process, we first used a covariance matrix to reconstruct the R, G and B channels of image and utilized the inconsistencies of the CFA interpolation pattern to extract forensics feature. Then, these forensics features were used to perform coarse-grained detection, and texture strength features were used to perform fine-grained detection. Finally, an edge smoothing method was applied to realize precise localization . As compared to the state-of-the-art CFA-based image splicing detection methods, the proposed method has a high-level detection accuracy and strong robustness against content-preserving manipulations and JPEG compression.},
  archive      = {J_PR},
  author       = {Xiaofeng Wang and Yan Wang and Jinjin Lei and Bin Li and Qin Wang and Jianru Xue},
  doi          = {10.1016/j.patcog.2021.108347},
  journal      = {Pattern Recognition},
  pages        = {108347},
  shortjournal = {Pattern Recognition},
  title        = {Coarse-to-fine-grained method for image splicing region detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive decision forest: An incremental machine learning
framework. <em>PR</em>, <em>122</em>, 108345. (<a
href="https://doi.org/10.1016/j.patcog.2021.108345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present an incremental machine learning framework called Adaptive Decision Forest (ADF), which produces a decision forest to classify new records. Based on our two novel theorems, we introduce a new splitting strategy called iSAT, which allows ADF to classify new records even if they are associated with previously unseen classes. ADF is capable of identifying and handling concept drift; it, however, does not forget previously gained knowledge. Moreover, ADF is capable of handling big data if the data can be divided into batches. We evaluate ADF on nine publicly available natural datasets and one synthetic dataset , and compare the performance of ADF against the performance of eight state-of-the-art techniques. We also examine the effectiveness of ADF in some challenging situations. Our experimental results, including statistical sign test and Nemenyi test analyses, indicate a clear superiority of the proposed framework over the state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Md Geaur Rahman and Md Zahidul Islam},
  doi          = {10.1016/j.patcog.2021.108345},
  journal      = {Pattern Recognition},
  pages        = {108345},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive decision forest: An incremental machine learning framework},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging local and global descriptors in parallel to
search correspondences for visual localization. <em>PR</em>,
<em>122</em>, 108344. (<a
href="https://doi.org/10.1016/j.patcog.2021.108344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual localization to compute 6DoF camera pose from a given image has wide applications. Both local and global descriptors are crucial for visual localization. Most of the existing visual localization methods adopt a two-stage strategy: image retrieval first is performed by global descriptors, and then 2D-3D correspondences are made by local descriptors from 2D query image points and its nearest neighbor candidates which are the 3D points visible by these retrieved images. The above two stages are serially performed in these methods. However, due to the fact that 3D points obtained from the retrieval feedback are only rely on global descriptors, these methods cannot fully take the advantages of both local and global descriptors. In this paper, we propose a novel parallel search framework, which fully leverages advantages of both local and global descriptors to get nearest neighbor candidates of a 2D query image point. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the 2D query image point. We propose a new probability model and a new deep learning based local descriptor when constructing the random trees. In addition, a weighted Hamming regularization term to keep discriminativeness after binarization is given in loss function for the proposed local descriptor. The loss function co-trains both real and binary local descriptors of which the results are integrated into the random trees. Experiments on challenging benchmarks show that the proposed localization method can significantly improve the robustness and accuracy compared with the ones which get nearest neighbor candidates of a query local feature just based on either local or global descriptors.},
  archive      = {J_PR},
  author       = {Pengju Zhang and Chaofan Zhang and Bingxi Liu and Yihong Wu},
  doi          = {10.1016/j.patcog.2021.108344},
  journal      = {Pattern Recognition},
  pages        = {108344},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging local and global descriptors in parallel to search correspondences for visual localization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Robust and discrete matrix factorization hashing for
cross-modal retrieval. <em>PR</em>, <em>122</em>, 108343. (<a
href="https://doi.org/10.1016/j.patcog.2021.108343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing based methods have gained great success for cross-modal similarity search, due to its fast query speed and low storage cost. However, there are some challenging problems that need to be further solved: 1) Many approaches are sensitive to noises and outliers, because ℓ 2 ℓ2 norm is utilized in the objective function, the error may be amplified. 2) Most existing methods take relaxation or rounding scheme to generate binary codes , causing a large quantization loss. 3) Many supervised cross-media algorithms usually take a large n × n n×n matrix to preserve the similarity relationship, leading to large calculation and making them unscalable. To mitigate these challenges, we develop a novel cross-media search algorithm, i.e., robust and discrete matrix factorization hashing, dubbed RDMH. The method takes a two-step strategy. In the first phase, the ℓ 2 , 1 ℓ2,1 norm is utilized to improve the robustness, which makes our model not sensitive to noises and outliers. We can learn the hash codes directly by the proposed discrete optimization method instead of relaxation scheme, avoiding the large quantization loss. Moreover, RDMH correlates the hash codes and semantic labels directly instead of manipulating the large similarity matrix . In the second phase, we propose an autoencoder strategy to learn the hash functions, more valuable information can be preserved and making the hash functions more powerful. Comprehensive experiments on several databases demonstrate the superior performance and efficacy of the developed RDMH.},
  archive      = {J_PR},
  author       = {Donglin Zhang and Xiao-Jun Wu},
  doi          = {10.1016/j.patcog.2021.108343},
  journal      = {Pattern Recognition},
  pages        = {108343},
  shortjournal = {Pattern Recognition},
  title        = {Robust and discrete matrix factorization hashing for cross-modal retrieval},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High dynamic range imaging via gradient-aware context
aggregation network. <em>PR</em>, <em>122</em>, 108342. (<a
href="https://doi.org/10.1016/j.patcog.2021.108342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining a high dynamic range (HDR) image from multiple low dynamic range images with different exposures is an important step in various computer vision tasks. One of the ongoing challenges in the field is to generate HDR images without ghosting artifacts. Motivated by an observation that such artifacts are particularly noticeable in the gradient domain, in this paper, we propose an HDR imaging approach that aggregates the information from multiple LDR images with guidance from image gradient domain. The proposed method generates artifact-free images by integrating the image gradient information and the image context information in the pixel domain . The context information in a large area helps to reconstruct the contents contaminated by saturation and misalignments. Specifically, an additional gradient stream and the supervision in the gradient domain are applied to incorporate the gradient information in HDR imaging. To use the context information captured from a large area while preserving spatial resolution, we adopt dilated convolutions to extract multi-scale features with rich context information. Moreover, we build a new dataset containing 40 groups of real-world images from diverse scenes with ground truth to validate the proposed model. The samples in the proposed dataset include more challenging moving objects inducing misalignments. Extensive experimental results demonstrate that our proposed model outperforms previous methods on different datasets in terms of both quantitative measure and visual perception quality.},
  archive      = {J_PR},
  author       = {Qingsen Yan and Dong Gong and Javen Qinfeng Shi and Anton van den Hengel and Jinqiu Sun and Yu Zhu and Yanning Zhang},
  doi          = {10.1016/j.patcog.2021.108342},
  journal      = {Pattern Recognition},
  pages        = {108342},
  shortjournal = {Pattern Recognition},
  title        = {High dynamic range imaging via gradient-aware context aggregation network},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised segmentation of COVID19 infection with
scribble annotation on CT images. <em>PR</em>, <em>122</em>, 108341. (<a
href="https://doi.org/10.1016/j.patcog.2021.108341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.},
  archive      = {J_PR},
  author       = {Xiaoming Liu and Quan Yuan and Yaozong Gao and Kelei He and Shuo Wang and Xiao Tang and Jinshan Tang and Dinggang Shen},
  doi          = {10.1016/j.patcog.2021.108341},
  journal      = {Pattern Recognition},
  pages        = {108341},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised segmentation of COVID19 infection with scribble annotation on CT images},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-stationary, online variational bayesian learning, with
circular variables. <em>PR</em>, <em>122</em>, 108340. (<a
href="https://doi.org/10.1016/j.patcog.2021.108340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an online variational Bayesian model for tracking changes in a non-stationary, multivariate, temporal signal , using as an example the changing frequency and amplitude of a noisy sinusoidal signal over time. The model incorporates each observation as it arrives and then discards it, and places priors over precision hyperparameters to ensure that (i) the posterior probability distributions do not become overly tight, which would impede its ability to recognise and track changes, and (ii) no values in the system are able to continuously increase and hence exceed the numerical representation of the programming language . It is thus able to perform truly online processing for an infinitely long set of observations. Only a single round of updates in the variational Bayesian scheme per observation is used, and the complexity of the algorithm is constant in time. The proposed method is demonstrated on a large number of synthetic datasets , comparing the results from the full model (with precision hyperparameters as variables with priors) with those from the base model where the precision hyperparameters are fixed values. The full model is also demonstrated on a set of real climate data.},
  archive      = {J_PR},
  author       = {J. Christmas},
  doi          = {10.1016/j.patcog.2021.108340},
  journal      = {Pattern Recognition},
  pages        = {108340},
  shortjournal = {Pattern Recognition},
  title        = {Non-stationary, online variational bayesian learning, with circular variables},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating tukey depth using incremental quantile
estimators. <em>PR</em>, <em>122</em>, 108339. (<a
href="https://doi.org/10.1016/j.patcog.2021.108339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measures of distance or how data points are positioned relative to each other are fundamental in pattern recognition. The concept of depth measures how deep an arbitrary point is positioned in a dataset, and is an interesting concept in this regard. However, while this concept has received a lot of attention in the statistical literature, its application within pattern recognition is still limited. To increase the applicability of the depth concept in pattern recognition, we address the well-known computational challenges associated with the depth concept, by suggesting to estimate depth using incremental quantile estimators . The suggested algorithm can not only estimate depth when the dataset is known in advance, but can also track depth for dynamically varying data streams by using recursive updates . The tracking ability of the algorithm was demonstrated based on a real-life application associated with detecting changes in human activity from real-time accelerometer observations. Given the flexibility of the suggested approach, it can detect virtually any kind of changes in the distributional patterns of the observations, and thus outperforms detection approaches based on the Mahalanobis distance .},
  archive      = {J_PR},
  author       = {Hugo L. Hammer and Anis Yazidi and Håvard Rue},
  doi          = {10.1016/j.patcog.2021.108339},
  journal      = {Pattern Recognition},
  pages        = {108339},
  shortjournal = {Pattern Recognition},
  title        = {Estimating tukey depth using incremental quantile estimators},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From soccer video to ball possession statistics.
<em>PR</em>, <em>122</em>, 108338. (<a
href="https://doi.org/10.1016/j.patcog.2021.108338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ball possession statistics in a soccer match is evaluated by counting the number of valid passes by both teams. The valid passes are determined by monitoring the start and end of a ball passing event initiated by a player. In this work, we map pass detection as detection of split and merge of nodes of a flow network. The players and ball represent nodes in the network. A group is formed by the objects (ball and players) which are spatially close to each other. Objects belonging to the same group are allowed to split or merge. We use this group relation to check if the objects split or merge in the sequence of frames. A constraint is added to the network to make sure that two objects can split only if the objects were previously merged. Flow through the split or merge node of the network denotes a ball pass event. Additional nodes like appear and disappear are added to the network to map the possibility that new objects could appear or old objects may disappear to and from the frame. The minimum cost path in the flow network provides the solution for valid pass events. Experimental evaluation shows that our proposal is at least 4\% better in estimating ball possession statistics and 8\% better in pass detection of a soccer match seen in a broadcast video than that of competitive methods.},
  archive      = {J_PR},
  author       = {Saikat Sarkar and Dipti Prasad Mukherjee and Amlan Chakrabarti},
  doi          = {10.1016/j.patcog.2021.108338},
  journal      = {Pattern Recognition},
  pages        = {108338},
  shortjournal = {Pattern Recognition},
  title        = {From soccer video to ball possession statistics},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernelized support tensor train machines. <em>PR</em>,
<em>122</em>, 108337. (<a
href="https://doi.org/10.1016/j.patcog.2021.108337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for high-dimensional image classification with very small number of training samples. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. This reduces the storage and computation complexity of kernel matrix construction from exponential to polynomial. The validity proof and computation complexity of the proposed TT-based kernel functions are provided elaborately. Extensive experiments are performed on high-dimensional fMRI and color images datasets, which demonstrates the superiority of the proposed scheme compared with the state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Cong Chen and Kim Batselier and Wenjian Yu and Ngai Wong},
  doi          = {10.1016/j.patcog.2021.108337},
  journal      = {Pattern Recognition},
  pages        = {108337},
  shortjournal = {Pattern Recognition},
  title        = {Kernelized support tensor train machines},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cascade reconstruction model with generalization ability
evaluation for anomaly detection in videos. <em>PR</em>, <em>122</em>,
108336. (<a href="https://doi.org/10.1016/j.patcog.2021.108336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection plays an important role in surveillance video since it maintains public safety efficiently with low cost. In current works, anomaly detection methods based on reconstruction with deep learning has been extensively studied for the powerful representation capacity. These methods use convolutional neural networks to learn model for describing normality at training and detect anomalies according to reconstruction error at testing. However, excessive representation capacity of neural networks will also bring disadvantages to anomaly detection when it is powerful enough to reconstruct abnormal information. For this reason, we proposed two solutions; firstly, a cascade model which conducts pixel reconstruction followed by optical flow prediction is designed. The conversion from frame to optical flow learns the correlation between object appearance and motion, while pixel reconstruction enlarges the optical flow prediction error to conduct effective anomaly detection. Secondly, the generalization ability evaluation based on pseudo-anomaly is proposed, which is used to evaluate the ability of model to represent anomaly, thus selecting an optimal model for anomaly detection. The selected model achieves AUC 88.9\% on Avenue, 82.6\% on Ped1, 97.7\% on Ped2, and 70.7\% on ShanghaiTech datasets. Extensive ablation experiments have verified the effectiveness of our method. Code will be released at https://github.com/Xia-Chen/Cascade_Reconstruction.},
  archive      = {J_PR},
  author       = {Yuanhong Zhong and Xia Chen and Jinyang Jiang and Fan Ren},
  doi          = {10.1016/j.patcog.2021.108336},
  journal      = {Pattern Recognition},
  pages        = {108336},
  shortjournal = {Pattern Recognition},
  title        = {A cascade reconstruction model with generalization ability evaluation for anomaly detection in videos},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neighborhood preserving embedding on grassmann manifold for
image-set analysis. <em>PR</em>, <em>122</em>, 108335. (<a
href="https://doi.org/10.1016/j.patcog.2021.108335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling image sets as points on Grassmann manifold has attracted increasing interests in computer vision community and has been applied to many applications. However, such approaches have suffered from the limitation that high computational cost on Grassmann manifold must be involved, especially high-dimensional ones. In this paper, we propose an unsupervised robust dimensionality reduction algorithm for Grassmann manifold based on Neighborhood Preserving Embedding (GNPE). We first introduce two strategies to construct the coefficients-based similarity graph to eliminate the effects of errors. Then, a projection is learned from the high-dimensional Grassmann manifold to the relative low-dimensional one with more discriminative capability, where the local neighborhood structure is well preserved. To address the issue that the estimated similarity graph is unreliable with noise and outliers, we further propose a unified learning framework which performs similarity learning and projection learning simultaneously. By leveraging the interactions between these two essential tasks, we can capture accurate structures and learn discriminative projections. The proposed method can be optimized by an efficient iterative algorithm . Experiments on various image set classification and clustering tasks clearly show that our model achieves consistent improvements in terms of both effectiveness and efficiency.},
  archive      = {J_PR},
  author       = {Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao and Zhenwen Ren},
  doi          = {10.1016/j.patcog.2021.108335},
  journal      = {Pattern Recognition},
  pages        = {108335},
  shortjournal = {Pattern Recognition},
  title        = {Neighborhood preserving embedding on grassmann manifold for image-set analysis},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph clustering via variational graph embedding.
<em>PR</em>, <em>122</em>, 108334. (<a
href="https://doi.org/10.1016/j.patcog.2021.108334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lin Guo and Qun Dai},
  doi          = {10.1016/j.patcog.2021.108334},
  journal      = {Pattern Recognition},
  pages        = {108334},
  shortjournal = {Pattern Recognition},
  title        = {Graph clustering via variational graph embedding},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective deep network using target vector update modules
for image restoration. <em>PR</em>, <em>122</em>, 108333. (<a
href="https://doi.org/10.1016/j.patcog.2021.108333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration (IR) has been widely used in many computer vision applications. The model-based IR methods have clear theoretical bases. However, numerous hyper-parameters need to be set empirically, which is often challenging and time-consuming. Because of the powerful nonlinear fitting ability, deep convolutional neural networks (CNNs) have been widely used in IR tasks in recent years. However, it is challenging to design new network architecture to further significantly improve the IR performance. Inspired by the plug and play (P&amp;P) methods, we first decouple the original IR problem into two subproblems with the variable splitting technique . Then, derived from the model-based methods, a novel deep CNN framework in the transformation domain is proposed to mimic the optimization process of the two subproblems . The proposed framework is driven effectively by the target vector update (TVU) module. Extensive experiments demonstrate the effectiveness of our proposed method over other state-of-the-art IR methods.},
  archive      = {J_PR},
  author       = {Sen Zhai and Chao Ren and Zhengyong Wang and Xiaohai He and Linbo Qing},
  doi          = {10.1016/j.patcog.2021.108333},
  journal      = {Pattern Recognition},
  pages        = {108333},
  shortjournal = {Pattern Recognition},
  title        = {An effective deep network using target vector update modules for image restoration},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian-guided feature alignment for unsupervised
cross-subject adaptation. <em>PR</em>, <em>122</em>, 108332. (<a
href="https://doi.org/10.1016/j.patcog.2021.108332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activities recognition (HAR) and human intent recognition (HIR) are important for medical diagnosis and human-robot interaction. HAR and HIR usually rely on the signals of some wearable sensors , such as inertial measurement unit (IMU), but these signals may be user-dependent, which degrades the performance of the recognition algorithm on new subjects. Traditional supervised learning methods require labeling signals and training specific classifiers for each new subject, which is burdensome. To deal with this problem, this paper proposes a novel non-adversarial cross-subject adaptation method called Gaussian-guided feature alignment (GFA). The proposed GFA metric quantifies the discrepancy between the labeled features of source subjects and the unlabeled features of target subjects so that minimizing the GFA metric leads to the alignment of the source and target features. The GFA metric is estimated by calculating the divergence between the feature distribution and Gaussian distribution, as well as the mean squared error of the mean and variance between source and target features. This paper analytically proves the effect of the GFA metric and validates its performance using three public human activity datasets. Experimental results show that the proposed GFA achieves 1\% higher target classification accuracy and 0.5\% lower variance than state-of-the-art methods in case of cross-subject validation. These results indicate that the proposed GFA is feasible for improving the generalization of the HAR and HIR.},
  archive      = {J_PR},
  author       = {Kuangen Zhang and Jiahong Chen and Jing Wang and Yuquan Leng and Clarence W. de Silva and Chenglong Fu},
  doi          = {10.1016/j.patcog.2021.108332},
  journal      = {Pattern Recognition},
  pages        = {108332},
  shortjournal = {Pattern Recognition},
  title        = {Gaussian-guided feature alignment for unsupervised cross-subject adaptation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multinomial random forest. <em>PR</em>, <em>122</em>,
108331. (<a href="https://doi.org/10.1016/j.patcog.2021.108331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of random forests (RF), its theoretical properties have not been thoroughly understood. In this paper, we propose a novel RF framework, dubbed multinomial random forest (MRF), to analyze its consistency and privacy-preservation . Instead of deterministic greedy split rule or with simple randomness, the MRF adopts two impurity-based multinomial distributions to randomly select a splitting feature and a splitting value, respectively. Theoretically, we prove the consistency of MRF and analyze its privacy-preservation within the framework of differential privacy . We also demonstrate with multiple datasets that its performance is on par with the standard RF. To the best of our knowledge, MRF is the first consistent RF variant that has comparable performance to the standard RF. The code is available at https://github.com/jiawangbai/Multinomial-Random-Forest .},
  archive      = {J_PR},
  author       = {Jiawang Bai and Yiming Li and Jiawei Li and Xue Yang and Yong Jiang and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2021.108331},
  journal      = {Pattern Recognition},
  pages        = {108331},
  shortjournal = {Pattern Recognition},
  title        = {Multinomial random forest},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Support structure representation learning for sequential
data clustering. <em>PR</em>, <em>122</em>, 108326. (<a
href="https://doi.org/10.1016/j.patcog.2021.108326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential data clustering is a challenging task in data mining (e.g., motion recognition and video segmentation). For good performance in dealing with complex local correlation and high-dimensional structure of sequential data, representation based methods have become one of the hot topics for sequential data clustering, in which subspace clustering is a representative tool. Subspace clustering methods divide the sequence into disjoint segments according to a locally continuous and connected representation of raw data. Although the subspace clustering methods maintain the successive property of sequential data well, there exist redundant connections in the intersection of two subsequences, which will destroy the integrity of a cluster and easily cause the chained partition of the sequence. So it is necessary to learn a more specific structure representation of a sequence to preserves both sequential information and efficient connections. Besides, the representation that conducive to clustering should have sparsity and connectivity under some assumptions. To this end, we propose a novel method to learn the support structure representation of sequence, which can extract sufficient information about instances and get the compact structure of sequential data. Furthermore, a new subspace clustering method is proposed based on the representation based method. Theoretical analysis and experimental results show the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Xiumei Wang and Dingning Guo and Peitao Cheng},
  doi          = {10.1016/j.patcog.2021.108326},
  journal      = {Pattern Recognition},
  pages        = {108326},
  shortjournal = {Pattern Recognition},
  title        = {Support structure representation learning for sequential data clustering},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-step domain adaptation for underwater image enhancement.
<em>PR</em>, <em>122</em>, 108324. (<a
href="https://doi.org/10.1016/j.patcog.2021.108324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, underwater image enhancement methods based on deep learning have achieved remarkable results. Since the images obtained in complex underwater scenarios lack a ground truth, these algorithms mainly train models on underwater images synthesized from in-air images. Synthesized underwater images are different from real-world underwater images; this difference leads to the limited generalizability of the training model when enhancing real-world underwater images. In this work, we present an underwater image enhancement method that does not require training on synthetic underwater images and eliminates the dependence on underwater ground-truth images. Specifically, a novel domain adaptation framework for real-world underwater image enhancement inspired by transfer learning is presented; it transfers in-air image dehazing to real-world underwater image enhancement. The experimental results on different real-world underwater scenes indicate that the proposed method produces visually satisfactory results.},
  archive      = {J_PR},
  author       = {Qun Jiang and Yunfeng Zhang and Fangxun Bao and Xiuyang Zhao and Caiming Zhang and Peide Liu},
  doi          = {10.1016/j.patcog.2021.108324},
  journal      = {Pattern Recognition},
  pages        = {108324},
  shortjournal = {Pattern Recognition},
  title        = {Two-step domain adaptation for underwater image enhancement},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature flow: In-network feature flow estimation for video
object detection. <em>PR</em>, <em>122</em>, 108323. (<a
href="https://doi.org/10.1016/j.patcog.2021.108323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical flow, which expresses pixel displacement , is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network , recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent with the pixel displacement , a common approach is to forward optical flow to a neural network and fine-tune this network on the task dataset. With this method, they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink about this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an I n-network F eature F low estimation module (IFF module) for video object detection. Without resorting to pre-training on any additional dataset, our IFF module is able to directly produce feature flow which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on self-supervision , which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and achieves new state-of-the-art performance on ImageNet VID.},
  archive      = {J_PR},
  author       = {Ruibing Jin and Guosheng Lin and Changyun Wen and Jianliang Wang and Fayao Liu},
  doi          = {10.1016/j.patcog.2021.108323},
  journal      = {Pattern Recognition},
  pages        = {108323},
  shortjournal = {Pattern Recognition},
  title        = {Feature flow: In-network feature flow estimation for video object detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainable scale distillation for hyperspectral image
classification. <em>PR</em>, <em>122</em>, 108316. (<a
href="https://doi.org/10.1016/j.patcog.2021.108316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability . The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.},
  archive      = {J_PR},
  author       = {Cheng Shi and Li Fang and Zhiyong Lv and Minghua Zhao},
  doi          = {10.1016/j.patcog.2021.108316},
  journal      = {Pattern Recognition},
  pages        = {108316},
  shortjournal = {Pattern Recognition},
  title        = {Explainable scale distillation for hyperspectral image classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DDBN: Dual detection branch network for semantic diversity
predictions. <em>PR</em>, <em>122</em>, 108315. (<a
href="https://doi.org/10.1016/j.patcog.2021.108315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that detail features and context semantics are conducive to improving object detection performance. However, the current single-prediction detectors do not well incorporate these two types of information together. To alleviate the limitation of single-prediction on the use of multiple types of information, we propose a dual detection branch network (DDBN) with adjacent feature compensation and customized training strategy for semantic diversity predictions. Different from the conventional single-prediction models, our DDBN is in the form of a single model with dual different semantic predictions. In particular, two types of adjacent feature compensations are designed to extract detail and context information from different perspectives. Also, a specialized training strategy is customized for our DDBN to well explore the diversity of predictions for improving the performance of object detection. We conduct extensive experiments on three datasets, i.e. , DOTA, MS-COCO, and Pascal-VOC, and the experimental results strongly demonstrate the efficacy of our proposed model.},
  archive      = {J_PR},
  author       = {Qifeng Lin and Chengjiang Long and Jianhui Zhao and Gang Fu and Zhiyong Yuan},
  doi          = {10.1016/j.patcog.2021.108315},
  journal      = {Pattern Recognition},
  pages        = {108315},
  shortjournal = {Pattern Recognition},
  title        = {DDBN: Dual detection branch network for semantic diversity predictions},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image-to-video person re-identification using
three-dimensional semantic appearance alignment and cross-modal
interactive learning. <em>PR</em>, <em>122</em>, 108314. (<a
href="https://doi.org/10.1016/j.patcog.2021.108314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-video person re-identification (I2V ReID), which aims to retrieve human targets between image-based queries and video-based galleries, has recently become a new research focus. However, the appearance misalignment and modality misalignment in both images and videos caused by pose variations, camera views, misdetections , and different data types , make I2V ReID still challenging. To this end, we propose a deep I2V ReID pipeline based on three-dimensional semantic appearance alignment (3D-SAA) and cross-modal interactive learning (CMIL) to address the aforementioned two challenges. Specifically, in the 3D-SAA module, the aligned local appearance images extracted by dense 3D human appearance estimation are in conjunction with global image and video embedding streams to learn more fine-grained identity features. The aligned local appearance images are further semantically aggregated by the proposed multi-branch aggregation network to weaken the negligible body parts. Moreover, to overcome the influence of modality misalignment, a CMIL module enables the communication between global image and video streams by interactively propagating the temporal information in videos to the channels of image feature maps. Extensive experiments on challenging MARS, DukeMTMC-VideoReID and iLIDS-VID datasets, show the superiority of our approach.},
  archive      = {J_PR},
  author       = {Wei Shi and Hong Liu and Mengyuan Liu},
  doi          = {10.1016/j.patcog.2021.108314},
  journal      = {Pattern Recognition},
  pages        = {108314},
  shortjournal = {Pattern Recognition},
  title        = {Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multiscale hierarchical attention for video
summarization. <em>PR</em>, <em>122</em>, 108312. (<a
href="https://doi.org/10.1016/j.patcog.2021.108312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a multiscale hierarchical attention approach for supervised video summarization. Different from most existing supervised methods which employ bidirectional long short-term memory networks, our method exploits the underlying hierarchical structure of video sequences and learns both the short-range and long-range temporal representations via a intra-block and a inter-block attention. Specifically, we first separate each video sequence into blocks of equal length and employ the intra-block and inter-block attention to learn local and global information, respectively. Then, we integrate the frame-level, block-level, and video-level representations for the frame-level importance score prediction. Next, we conduct shot segmentation and compute shot-level importance scores. Finally, we perform key shot selection to produce video summaries. Moreover, we extend our method into a two-stream framework, where appearance and motion information is leveraged. Experimental results on the SumMe and TVSum datasets validate the effectiveness of our method against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wencheng Zhu and Jiwen Lu and Yucheng Han and Jie Zhou},
  doi          = {10.1016/j.patcog.2021.108312},
  journal      = {Pattern Recognition},
  pages        = {108312},
  shortjournal = {Pattern Recognition},
  title        = {Learning multiscale hierarchical attention for video summarization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonconvex 3D array image data recovery and pattern
recognition under tensor framework. <em>PR</em>, <em>122</em>, 108311.
(<a href="https://doi.org/10.1016/j.patcog.2021.108311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a weighted tensor Schatten- p p quasi-norm ( 0 0&amp;lt;p&amp;lt;1 ) regularizer for 3D array datasets in order to recover the low-rank part and the sparse part, respectively. Corresponding algorithms associated with augmented Lagrangian multipliers are established and the constructed sequence converges to the desirable Karush-Kuhn-Tucker (KKT) point, which is mathematically validated in detail. Although the proposed weighted tensor Schatten- p p quasi-norm is non-convex, it appears not only to less penalize the singular values but also to be effective in capturing the low-rank property. The main findings in this paper are the appropriate choice of p p depends on specific tasks: low-rank data set recovery usually requires relatively large value of p p , while sparse data set recovery needs relatively small value of p p . And the weights chosen in our tensor Schatten- p p quasi-norm are inversely to the singular values exponentially for promoting the sensitivity to different singular values. Experimental results for video inpainting (tensor completion), image recovery and salient object detection (tensor robust principal component analysis) have been shown that the proposed approach outperforms various latest approaches in literature.},
  archive      = {J_PR},
  author       = {Ming Yang and Qilun Luo and Wen Li and Mingqing Xiao},
  doi          = {10.1016/j.patcog.2021.108311},
  journal      = {Pattern Recognition},
  pages        = {108311},
  shortjournal = {Pattern Recognition},
  title        = {Nonconvex 3D array image data recovery and pattern recognition under tensor framework},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unimodal regularisation based on beta distribution for deep
ordinal regression. <em>PR</em>, <em>122</em>, 108310. (<a
href="https://doi.org/10.1016/j.patcog.2021.108310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the use of deep learning for solving ordinal classification problems, where categories follow a natural order, has not received much attention. In this paper, we propose an unimodal regularisation based on the beta distribution applied to the cross-entropy loss. This regularisation encourages the distribution of the labels to be a soft unimodal distribution, more appropriate for ordinal problems. Given that the beta distribution has two parameters that must be adjusted, a method to automatically determine them is proposed. The regularised loss function is used to train a deep neural network model with an ordinal scheme in the output layer. The results obtained are statistically analysed and show that the combination of these methods increases the performance in ordinal problems. Moreover, the proposed beta distribution performs better than other distributions proposed in previous works, achieving also a reduced computational cost.},
  archive      = {J_PR},
  author       = {Víctor Manuel Vargas and Pedro Antonio Gutiérrez and César Hervás-Martínez},
  doi          = {10.1016/j.patcog.2021.108310},
  journal      = {Pattern Recognition},
  pages        = {108310},
  shortjournal = {Pattern Recognition},
  title        = {Unimodal regularisation based on beta distribution for deep ordinal regression},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ProCAN: Progressive growing channel attentive non-local
network for lung nodule classification. <em>PR</em>, <em>122</em>,
108309. (<a href="https://doi.org/10.1016/j.patcog.2021.108309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05\% and an accuracy of 95.28\% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method.},
  archive      = {J_PR},
  author       = {Mundher Al-Shabi and Kelvin Shak and Maxine Tan},
  doi          = {10.1016/j.patcog.2021.108309},
  journal      = {Pattern Recognition},
  pages        = {108309},
  shortjournal = {Pattern Recognition},
  title        = {ProCAN: Progressive growing channel attentive non-local network for lung nodule classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature wise normalization: An effective way of normalizing
data. <em>PR</em>, <em>122</em>, 108307. (<a
href="https://doi.org/10.1016/j.patcog.2021.108307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Feature Wise Normalization approach for the effective normalization of data. In this approach, each feature is normalized independently with one of the methods from the pool of normalization methods. It is in contrast to the conventional approach which normalizes the data with one method only and as a result, yields suboptimal performance. Additionally, generalization and superiority among normalization methods are also not ensured owing to different machine learning mechanisms for solving classification tasks . The proposed approach benefits from the collective response of multiple methods to normalize the data better as individual features become a normalization unit. The selection of methods is a combinatorial problem that can be solved with optimization algorithms . For this purpose, Antlion optimization is considered that combines the search of methods with the fine-tuning of classifier parameters. Twelve methods are used to create the pool beside the original scale, and the obtained data is evaluated on four learning algorithms. Experiments are performed on 18 benchmark datasets to show the efficacy of the proposed approach in contrast to conventional normalization.},
  archive      = {J_PR},
  author       = {Dalwinder Singh and Birmohan Singh},
  doi          = {10.1016/j.patcog.2021.108307},
  journal      = {Pattern Recognition},
  pages        = {108307},
  shortjournal = {Pattern Recognition},
  title        = {Feature wise normalization: An effective way of normalizing data},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A black-box adversarial attack for poisoning clustering.
<em>PR</em>, <em>122</em>, 108306. (<a
href="https://doi.org/10.1016/j.patcog.2021.108306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms play a fundamental role as tools in decision-making and sensible automation processes. Due to the widespread use of these applications, a robustness analysis of this family of algorithms against adversarial noise has become imperative. To the best of our knowledge, however, only a few works have currently addressed this problem. In an attempt to fill this gap, in this work, we propose a black-box adversarial attack for crafting adversarial samples to test the robustness of clustering algorithms. We formulate the problem as a constrained minimization program, general in its structure and customizable by the attacker according to her capability constraints. We do not assume any information about the internal structure of the victim clustering algorithm, and we allow the attacker to query it as a service only. In the absence of any derivative information, we perform the optimization with a custom approach inspired by the Abstract Genetic Algorithm (AGA). In the experimental part, we demonstrate the sensibility of different single and ensemble clustering algorithms against our crafted adversarial samples on different scenarios. Furthermore, we perform a comparison of our algorithm with a state-of-the-art approach showing that we are able to reach or even outperform its performance. Finally, to highlight the general nature of the generated noise, we show that our attacks are transferable even against supervised algorithms such as SVMs, random forests and neural networks .},
  archive      = {J_PR},
  author       = {Antonio Emanuele Cinà and Alessandro Torcinovich and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2021.108306},
  journal      = {Pattern Recognition},
  pages        = {108306},
  shortjournal = {Pattern Recognition},
  title        = {A black-box adversarial attack for poisoning clustering},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Divide well to merge better: A novel clustering algorithm.
<em>PR</em>, <em>122</em>, 108305. (<a
href="https://doi.org/10.1016/j.patcog.2021.108305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel non-parametric clustering algorithm which is based on the concept of divide-and-merge is proposed. The proposed algorithm is based on two primary phases, after data cleaning: (i) the Division phase and (ii) the Merging phase. In the initial phase of division, the data is divided into an optimized number of small sub-clusters utilizing all the dimensions of the data. In the second phase of merging, the small sub-clusters obtained as a result of division are merged according to an advanced statistical metric to form the actual clusters in the data. The proposed algorithm has the following merits: (i) ability to discover both convex and non-convex shaped clusters, (ii) ability to discover clusters different in densities, (iii) ability to detect and remove outliers/noise in the data (iv) easily tunable or fixed hyperparameters (v) and its usability for high dimensional data. The proposed algorithm is extensively tested on 20 benchmark datasets including both, the synthetic and the real datasets and is found better/competing to the existing state-of-the-art parametric and non-parametric clustering algorithms.},
  archive      = {J_PR},
  author       = {Atiq Ur Rehman and Samir Brahim Belhaouari},
  doi          = {10.1016/j.patcog.2021.108305},
  journal      = {Pattern Recognition},
  pages        = {108305},
  shortjournal = {Pattern Recognition},
  title        = {Divide well to merge better: A novel clustering algorithm},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised descriptor selection based meta-learning
networks for few-shot classification. <em>PR</em>, <em>122</em>, 108304.
(<a href="https://doi.org/10.1016/j.patcog.2021.108304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning aims to train a classifier on collections of tasks, such that it can recognize new classes given few samples from each. However, current approaches encounter overfitting and poor generalization since the internal representation learning is obstructed by backgrounds and noises in limited samples. To alleviate those issues, we propose the Unsupervised Descriptor Selection (UDS) to tackle few-shot learning tasks. Specifically, a descriptor selection module is proposed to localize and select semantic meaningful regions in feature maps without supervision. The selected features are then mapped into novel vectors by a task-related aggregation module to enhance internal representations. With a simple network structure, UDS makes adaptation between tasks more efficient, and improves the performance in few-shot learning. Extensive experiments with various backbones are conducted on Caltech-UCSD Bird and mini ImageNet, indicate that UDS achieves the comparable performance to state-of-the-art methods, and improves the performance of prior meta-learning methods.},
  archive      = {J_PR},
  author       = {Zhengping Hu and Zijun Li and Xueyu Wang and Saiyue Zheng},
  doi          = {10.1016/j.patcog.2021.108304},
  journal      = {Pattern Recognition},
  pages        = {108304},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised descriptor selection based meta-learning networks for few-shot classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). (AD)2: Adversarial domain adaptation to defense with
adversarial perturbation removal. <em>PR</em>, <em>122</em>, 108303. (<a
href="https://doi.org/10.1016/j.patcog.2021.108303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are demonstrated to be vulnerable to adversarial examples , which are crafted by adding adversarial perturbations to the legitimate examples. To address this issue, some defense methods have been proposed. Among them, the adversarial training (AT) is a popular method to improve the robustness of DNNs. However, theory analysis has shown that in the adversarial training framework, the improvement of the robustness will lead to a decline of standard accuracy. In this paper, we propose a modularized defense framework, namely Adversarial Domain Adaptation to Defense ((AD) 2 2 ). Different from all adversarial training methods, (AD) 2 2 detects adversarial example using a generative algorithm and applies the adversarial domain adaptation method to remove adversarial perturbation. Experimental results show that (AD) 2 2 is effective to remove the adversarial perturbation and mitigate the odds between the robustness and standard accuracy for DNNs.},
  archive      = {J_PR},
  author       = {Keji Han and Bin Xia and Yun Li},
  doi          = {10.1016/j.patcog.2021.108303},
  journal      = {Pattern Recognition},
  pages        = {108303},
  shortjournal = {Pattern Recognition},
  title        = {(AD)2: Adversarial domain adaptation to defense with adversarial perturbation removal},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative feature generation for classification of
imbalanced data. <em>PR</em>, <em>122</em>, 108302. (<a
href="https://doi.org/10.1016/j.patcog.2021.108302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data imbalance problem is a frequent bottleneck in the classification performance of neural networks . In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator , feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism , the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.},
  archive      = {J_PR},
  author       = {Sungho Suh and Paul Lukowicz and Yong Oh Lee},
  doi          = {10.1016/j.patcog.2021.108302},
  journal      = {Pattern Recognition},
  pages        = {108302},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative feature generation for classification of imbalanced data},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-aware supervised classification: An informative
subspace based multi-objective approach. <em>PR</em>, <em>122</em>,
108301. (<a href="https://doi.org/10.1016/j.patcog.2021.108301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharing the raw or an abstract representation of a labelled dataset on cloud platforms can potentially expose sensitive information of the data to an adversary, e.g., in the case of an emotion classification task from text, an adversary-agnostic abstract representation of the text data may eventually lead an adversary to identify the demographics of the authors, such as their gender and age. In this paper, we propose a universal defense mechanism against such malicious attempts of stealing sensitive information from data shared on cloud platforms. More specifically, our proposed method employs an informative subspace based multi-objective approach to obtain a sensitive information aware encoding of the data representation. A number of experiments conducted on both standard text and image datasets demonstrate that our proposed approach is able to reduce the effectiveness of the adversarial task (i.e., in other words is able to better protect the sensitive information of the data) without significantly reducing the effectiveness of the primary task itself.},
  archive      = {J_PR},
  author       = {Chandan Biswas and Debasis Ganguly and Partha Sarathi Mukherjee and Ujjwal Bhattacharya and Yufang Hou},
  doi          = {10.1016/j.patcog.2021.108301},
  journal      = {Pattern Recognition},
  pages        = {108301},
  shortjournal = {Pattern Recognition},
  title        = {Privacy-aware supervised classification: An informative subspace based multi-objective approach},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Atom correlation based graph propagation for scene graph
generation. <em>PR</em>, <em>122</em>, 108300. (<a
href="https://doi.org/10.1016/j.patcog.2021.108300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed distribution in the dataset is one of the major problems of the scene graph generation task. Previous methods attempt to alleviate this by introducing human commonsense knowledge in the form of statistical correlations between object pairs. However, the reasoning path they used is usually composable and the prior knowledge they employed is generally image-specific, making the knowledge learning less flexible, stable and holistic. In this paper, we propose Atom Correlation Based Graph Propagation (AC-GP) for the scene graph generation task. Specifically, diverse atom correlations between objects and their relationships are explored by separating relationships to form new semantic nodes and decomposing the compound reasoning paths. Based on these atom correlations, the knowledge graphs are introduced for the feature enhancement by information propagating in the global category space. By exploiting atom correlations, the introduced prior knowledge can be more common and easy to learn. Moreover, propagating the knowledge in the global category space enables the model aware of more comprehensive and holistic knowledge. As a result, the model capacity and stability can be effectively improved to mine infrequent and missed relationships. Experimental results on two benchmark datasets: Visual Relation Detection (VRD) and Visual Genome (VG) show the superiority of the proposed AC-GP over strong baseline methods .},
  archive      = {J_PR},
  author       = {Bingqian Lin and Yi Zhu and Xiaodan Liang},
  doi          = {10.1016/j.patcog.2021.108300},
  journal      = {Pattern Recognition},
  pages        = {108300},
  shortjournal = {Pattern Recognition},
  title        = {Atom correlation based graph propagation for scene graph generation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph regularized locally linear embedding for unsupervised
feature selection. <em>PR</em>, <em>122</em>, 108299. (<a
href="https://doi.org/10.1016/j.patcog.2021.108299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the important dimensionality reduction techniques , unsupervised feature selection (UFS) has enjoyed amounts of popularity over the last few decades, which can not only improve learning performance, but also enhance interpretability and reduce computational costs. The existing UFS methods often model the data in the original feature space, which cannot fully exploit the discriminative information. In this paper, to address this issue, we investigate how to strengthen the relationship between UFS and the feature subspace, so as to select relevant features more straightforwardly and effectively. Methodologically, a novel UFS approach, referred to as Graph Regularized Local Linear Embedding (GLLE), is proposed by integrating local linear embedding (LLE) and manifold regularization constrained in feature subspace into a unified framework. To be more specific, we explicitly define a feature selection matrix composed of 0 and 1, which can realize the process of UFS. For the purpose of modelling the feature selection matrix, we propose to preserve the local linear reconstruction relationship among neighboring data points in the feature subspace, which corresponds to LLE constrained in the feature subspace. To make the feature selection matrix more accurate, we propose to use manifold regularization as an assistant of LLE to find the relevant and representative features such that the selected features can make each sample under the feature subspace be accordance with the manifold assumption. A tailored iterative algorithm based on Alternative Direction Method of Multipliers (ADMM) is designed to solve the proposed optimization problem . Extensive experiments on twelve real-world benchmark datasets are conducted, and the more promising results are achieved compared with the state-of-the-arts approaches.},
  archive      = {J_PR},
  author       = {Jianyu Miao and Tiejun Yang and Lijun Sun and Xuan Fei and Lingfeng Niu and Yong Shi},
  doi          = {10.1016/j.patcog.2021.108299},
  journal      = {Pattern Recognition},
  pages        = {108299},
  shortjournal = {Pattern Recognition},
  title        = {Graph regularized locally linear embedding for unsupervised feature selection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel robust low-rank multi-view diversity optimization
model with adaptive-weighting based manifold learning. <em>PR</em>,
<em>122</em>, 108298. (<a
href="https://doi.org/10.1016/j.patcog.2021.108298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has become a hot yet challenging topic, due mainly to the independence of and information complementarity between different views. Although good results are achieved to a certain extent from typical methods including multi-view based k k -means clustering, sparse cooperative representation clustering and subspace clustering, they still suffer from several drawbacks or limitations: (1) When each view is sparse decomposed, it still contains some hidden information for mining, such as the structure of samples, the intra-class similarity measure, and the inter-class diversity discrimination, etc. (2) Most of the existing multi-view methods only consider the local features within each view, but fail to effectively balance the importance of and combine information among different views in a diversified way. To tackle these issues, we propose a novel multi-view diversity learning model based on robust bilinear error decomposition (BED). The BED term with a low rank sparse constraint is an improved non-negative matrix factorization (NMF), which is used to extract the hidden structure information in sparse decomposition and useful diversity discrimination information in error matrix. The preservation of local features and selection of important views are achieved by adaptive weighted manifold learning. Furthermore, the Hilbert Schmidt independence criterion is used as a diversity learning term for mutual learning and fusion among views. Finally, the proposed robust low-rank multi-view diversity learning spectral clustering method is evaluated and benchmarked with eight state-of-the-art methods. Experiments in six real datasets have fully validated the significantly improved accuracy and efficiency of the proposed methodology for effective clustering of multi-view images.},
  archive      = {J_PR},
  author       = {Junpeng Tan and Zhijing Yang and Jinchang Ren and Bing Wang and Yongqiang Cheng and Wing-Kuen Ling},
  doi          = {10.1016/j.patcog.2021.108298},
  journal      = {Pattern Recognition},
  pages        = {108298},
  shortjournal = {Pattern Recognition},
  title        = {A novel robust low-rank multi-view diversity optimization model with adaptive-weighting based manifold learning},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust face alignment by dual-attentional spatial-aware
capsule networks. <em>PR</em>, <em>122</em>, 108297. (<a
href="https://doi.org/10.1016/j.patcog.2021.108297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face alignment in-the-wild still faces great challenges due to that i) partial occlusion blurs the inter-features spatial relations of faces and ii) traditional CNN makes the network more difficult to capture the spatial positional relations between landmarks. To address the issues above, we propose a face alignment algorithm named Dual-attentional Spatial-aware Capsule Network (DSCN). Firstly, the spatial-aware module builds a more accurate inter-features spatial constrained model with the hourglass capsule network (HGCaps) as the backbone, which can effectively enhance its robustness against occlusions. Then, two sorts of attention mechanisms , namely capsule attention and spatial attention , are added to the attention-guided module to make the network focus more on the advantageous features and suppress other unrelated ones for more effective feature recalibration . Our method achieves 1.08\% failure rate on the COFW dataset, which is much lower than the current state-of-the-art algorithms. The mean error under 300W dataset and WFLW dataset are respectively 3.91\% and 5.66\%, which shows that DSCN is more robust to occlusion and outperforms state-of-the-art methods in the literature.},
  archive      = {J_PR},
  author       = {Jinyan Ma and Jing Li and Bo Du and Jia Wu and Jun Wan and Yafu Xiao},
  doi          = {10.1016/j.patcog.2021.108297},
  journal      = {Pattern Recognition},
  pages        = {108297},
  shortjournal = {Pattern Recognition},
  title        = {Robust face alignment by dual-attentional spatial-aware capsule networks},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coarse-to-fine pseudo supervision guided meta-task
optimization for few-shot object classification. <em>PR</em>,
<em>122</em>, 108296. (<a
href="https://doi.org/10.1016/j.patcog.2021.108296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Learning (FSL) is a challenging and practical learning pattern, aiming to solve a target task which has only a few labeled examples. Currently, the field of FSL has made great progress, but largely in the supervised setting, where a large auxiliary labeled dataset is required for offline training . However, the unsupervised FSL (UFSL) problem where the auxiliary dataset is fully unlabeled has been seldom investigated despite of its significant value. This paper focuses on the more general and challenging UFSL problem and presents a novel method named Coarse-to-Fine Pseudo Supervision-guided Meta-Learning (C2FPS-ML) for unsupervised few-shot object classification. It first obtains prior knowledge from an unlabeled auxiliary dataset during unsupervised meta-training, and then use the prior knowledge to assist the downstream few-shot classification task . Coarse-to-Fine Pseudo Supervisions in C2FPS-ML aim to optimize meta-task sampling process in unsupervised meta-training stage which is one of the dominant factors for improving the performance of meta-learning based FSL algorithms. Human can learn new concepts progressively or hierarchically following the coarse-to-fine manners. By simulating this human’s behaviour, we develop two versions of C2FPS-ML for two different scenarios: one is natural object dataset and another one is other kinds of dataset ( e.g. , handwritten character dataset). For natural object dataset scenario, we propose to exploit the potential hierarchical semantics of the unlabeled auxiliary dataset to build a tree-like structure of visual concepts. For another scenario, progressive pseudo supervision is obtained by forming clusters in different similarity aspects and is represented by a pyramid-like structure. The obtained structure is applied as the supervision to construct meta-tasks in meta-training stage, and prior knowledge from the unlabeled auxiliary dataset is learned from the coarse-grained level to the fine-grained level. The proposed method sets the new state of the art on the gold-standard mini ImageNet and achieves remarkable results on Omniglot while simultaneously increases efficiency.},
  archive      = {J_PR},
  author       = {Yawen Cui and Qing Liao and Dewen Hu and Wei An and Li Liu},
  doi          = {10.1016/j.patcog.2021.108296},
  journal      = {Pattern Recognition},
  pages        = {108296},
  shortjournal = {Pattern Recognition},
  title        = {Coarse-to-fine pseudo supervision guided meta-task optimization for few-shot object classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Exploring rich intermediate representations for
reconstructing 3D shapes from 2D images. <em>PR</em>, <em>122</em>,
108295. (<a href="https://doi.org/10.1016/j.patcog.2021.108295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering 3D voxelized shapes with fine details from single-view 2D images is an extremely challenging and ill-conditioned problem. Most of the existing methods learn the 3D reconstruction process by encoding the 3D shapes and the 2D images into the same low-dimensional latent vector, which lacks the capacity to capture detailed features in the surface of the 3D object shapes. To address this issue, we propose to explore rich intermediate representation for 3D shape reconstruction by using a newly designed network architecture . We first use a two-steam network to infer the depth map and the topology-specific mean shape from the given 2D image, which forms the intermediate representation prediction branch. The intermediate representations capture the global spatial structure and the visible surface geometric structure, which are important for reconstructing high-quality 3D shapes. Based on the obtained intermediate representation, a novel shape transformation network is then proposed to reconstruct the fine details of the whole 3D object shapes. The experimental results on the challenging ShapeNet and Pix3D datasets show that our approach outperforms the existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yang Yang and Junwei Han and Dingwen Zhang and Qi Tian},
  doi          = {10.1016/j.patcog.2021.108295},
  journal      = {Pattern Recognition},
  pages        = {108295},
  shortjournal = {Pattern Recognition},
  title        = {Exploring rich intermediate representations for reconstructing 3D shapes from 2D images},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-label sampling based on local label imbalance.
<em>PR</em>, <em>122</em>, 108294. (<a
href="https://doi.org/10.1016/j.patcog.2021.108294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is an inherent characteristic of multi-label data that hinders most multi-label learning methods. One efficient and flexible strategy to deal with this problem is to employ sampling techniques before training a multi-label learning model. Although existing multi-label sampling approaches alleviate the global imbalance of multi-label datasets, it is actually the imbalance level within the local neighbourhood of minority class examples that plays a key role in performance degradation . To address this issue, we propose a novel measure to assess the local label imbalance of multi-label datasets, as well as two multi-label sampling approaches, namely Multi-Label Synthetic Oversampling based on Local label imbalance (MLSOL) and Multi-Label Undersampling based on Local label imbalance (MLUL). By considering all informative labels, MLSOL creates more diverse and better labeled synthetic instances for difficult examples, while MLUL eliminates instances that are harmful to their local region. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed measure and sampling approaches for a variety of evaluation metrics , particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.},
  archive      = {J_PR},
  author       = {Bin Liu and Konstantinos Blekas and Grigorios Tsoumakas},
  doi          = {10.1016/j.patcog.2021.108294},
  journal      = {Pattern Recognition},
  pages        = {108294},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label sampling based on local label imbalance},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Who is closer: A computational method for domain gap
evaluation. <em>PR</em>, <em>122</em>, 108293. (<a
href="https://doi.org/10.1016/j.patcog.2021.108293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain gaps between different datasets limit the generalization ability of CNN models. Precise evaluation on the domain gap has potential to assist the promotion of CNN generalization ability . This paper proposes a computational framework to evaluate gaps between different domains, e.g. , judging which one of source domains is closer to the target domain. Our model is based on the observation that, given a well-trained classifier on the source domain, the entropy of its classification scores of the output layer can be used as an indicator of the domain gap. For instance, smaller domain gap generally corresponds to smaller entropy of classification scores. To further boost the discriminative power in distinguishing domain gaps, a novel training strategy is proposed to supervise the model to produce smaller entropy on one source domain and larger entropy on other source domains. This supervision leads to an efficient and discriminative domain gap evaluation model. Extensive experiments on multiple datasets including faces, vehicles, fashions, and persons, etc . show that our method can reasonably measure domain gaps. We further conduct experiments on domain adaptive person ReID task and our method is adopted to pre-trained model selection, pre-trained model fusion, source dataset fusion, and source dataset selection. As shown in the experiments, our method substantially boosts the ReID accuracy. To the best of our knowledge, this is an original work focusing on computational domain gap evaluation. Our code is available at https://github.com/liu-xb/DomainGapEvaluation .},
  archive      = {J_PR},
  author       = {Xiaobin Liu and Shiliang Zhang},
  doi          = {10.1016/j.patcog.2021.108293},
  journal      = {Pattern Recognition},
  pages        = {108293},
  shortjournal = {Pattern Recognition},
  title        = {Who is closer: A computational method for domain gap evaluation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalizable model-agnostic semantic segmentation via
target-specific normalization. <em>PR</em>, <em>122</em>, 108292. (<a
href="https://doi.org/10.1016/j.patcog.2021.108292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation in a supervised learning manner has achieved significant progress in recent years. However, its performance usually drops dramatically due to the data-distribution discrepancy between seen and unseen domains when we directly deploy the trained model to segment the images of unseen (or new coming) domains. To this end, we propose a novel domain generalization framework for the generalizable semantic segmentation task, which enhances the generalization ability of the model from two different views, including the training paradigm and the test strategy. Concretely, we exploit the model-agnostic learning to simulate the domain shift problem, which deals with the domain generalization from the training scheme perspective. Besides, considering the data-distribution discrepancy between seen source and unseen target domains, we develop the target-specific normalization scheme to enhance the generalization ability . Furthermore, when images come one by one in the test stage, we design the image-based memory bank (Image Bank in short) with style-based selection policy to select similar images to obtain more accurate statistics of normalization. Extensive experiments highlight that the proposed method produces state-of-the-art performance for the domain generalization of semantic segmentation on multiple benchmark segmentation datasets, i.e. , Cityscapes, Mapillary.},
  archive      = {J_PR},
  author       = {Jian Zhang and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1016/j.patcog.2021.108292},
  journal      = {Pattern Recognition},
  pages        = {108292},
  shortjournal = {Pattern Recognition},
  title        = {Generalizable model-agnostic semantic segmentation via target-specific normalization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AE-net: Fine-grained sketch-based image retrieval via
attention-enhanced network. <em>PR</em>, <em>122</em>, 108291. (<a
href="https://doi.org/10.1016/j.patcog.2021.108291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the task of Fine-grained Sketch-based Image Retrieval (FG-SBIR), which uses hand-drawn sketches as input queries to retrieve the relevant images at the fine-grained instance level. The sketches and images come from different modalities, thus the similarity computation needs to consider both fine-grained and cross-modal characteristics. Existing solutions only focus on fine-grained details or spatial contexts, while ignoring the channel context and spatial sequence information. To mitigate such challenging problems, we propose a novel deep FG-SBIR model, which aims at inferring attention maps along channel dimension and spatial dimension, improving modules of channel attention and spatial attention , and exploring Transformer to enhance the model’s ability for constructing and understanding spatial sequence information. We focus not only on the correlation information between two modalities of sketch and image, but also on the discrimination information inside the single modality. Mutual Loss is especially proposed to enhance the traditional triplet loss, and promote the internal discrimination ability of the model on a single modality. Extensive experiments show that our AE-Net obtains promising results on Sketchy , which is the largest public dataset available for FG-SBIR at present.},
  archive      = {J_PR},
  author       = {Yangdong Chen and Zhaolong Zhang and Yanfei Wang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan},
  doi          = {10.1016/j.patcog.2021.108291},
  journal      = {Pattern Recognition},
  pages        = {108291},
  shortjournal = {Pattern Recognition},
  title        = {AE-net: Fine-grained sketch-based image retrieval via attention-enhanced network},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contextual ensemble network for semantic segmentation.
<em>PR</em>, <em>122</em>, 108290. (<a
href="https://doi.org/10.1016/j.patcog.2021.108290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, exploring features from different layers in fully convolutional networks (FCNs) has gained substantial attention to capture context information for semantic segmentation . This paper presents a novel encoder-decoder architecture, called contextual ensemble network (CENet), for semantic segmentation, where the contextual cues are aggregated via densely usampling the convolutional features of deep layer to the shallow deconvolutional layers. The proposed CENet is trained in terms of end-to-end segmentation to match the resolution of input image, and allows us to fully explore contextual features through ensemble of dense deconvolutions . We evaluate our CENet on two widely-used semantic segmentation datasets: PASCAL VOC 2012 and CityScapes. The experimental results demonstrate our CENet achieves superior performance with respect to recent state-of-the-art results. Furthermore, we also evaluate CENet on MS COCO dataset and ISBI 2012 dataset for the task of instance segmentation and biological segmentation, respectively. The experimental results show that CENet obtains promising results on these two datasets.},
  archive      = {J_PR},
  author       = {Quan Zhou and Xiaofu Wu and Suofei Zhang and Bin Kang and Zongyuan Ge and Longin Jan Latecki},
  doi          = {10.1016/j.patcog.2021.108290},
  journal      = {Pattern Recognition},
  pages        = {108290},
  shortjournal = {Pattern Recognition},
  title        = {Contextual ensemble network for semantic segmentation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI-based human audio processing for COVID-19: A
comprehensive overview. <em>PR</em>, <em>122</em>, 108289. (<a
href="https://doi.org/10.1016/j.patcog.2021.108289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patients’ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using ‘Artificial Intelligence’ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.},
  archive      = {J_PR},
  author       = {Gauri Deshpande and Anton Batliner and Björn W. Schuller},
  doi          = {10.1016/j.patcog.2021.108289},
  journal      = {Pattern Recognition},
  pages        = {108289},
  shortjournal = {Pattern Recognition},
  title        = {AI-based human audio processing for COVID-19: A comprehensive overview},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmentation of handwritten arabic graphemes using a
directed convolutional neural network and mathematical morphology
operations. <em>PR</em>, <em>122</em>, 108288. (<a
href="https://doi.org/10.1016/j.patcog.2021.108288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nature of Arabic handwriting, segmenting words into characters/graphemes is the most difficult and critical task of the recognition system. The present paper proposes an approach to segment handwritten Arabic words into graphemes based on a directed Convolutional Neural Network (CNN) and Mathematical Morphology Operations (MMO). Arabic script is cursive, which means that almost all graphemes are connected via horizontal links; therefore, a technique to remove links will facilitate the segmentation of graphemes. In general, an MMO such as erosion seems suitable for getting the job done, but since Arabic handwriting is difficult, MMOs cause information loss and suffer from many issues such as diacritics and over-traces, which lead to over/under/bad segmentations. To overcome limitations, the present paper addresses these issues in the following order: the over-traces issue is addressed for the first time in the literature; a robust algorithm for diacritics extraction is provided; and finally, the main segmentation algorithm adopts a strategy based on a Partial Dilation (PD)-Global Erosion (GE) technique to combat the information loss issue. The PD phase amplifies important regions, while GE eliminates links between graphemes. The complementarity between PD and GE facilitates the extraction of graphemes and creates resistance against information loss. To properly tackle these difficult problems, this article exploits the robustness of CNNs, so a new directed CNN model is suggested. The idea is to draw the model&#39;s attention to certain targeted features, which are selected according to the nature of the problem addressed. The proposed directed CNN is used in all phases of the segmentation process. The experimental results are very encouraging and show that the proposed directed CNN model outperformed basic CNN in many experiments. The results also reveal that the followed strategy improved the ability of MMOs to perform segmentation and to compete with other approaches in this research area.},
  archive      = {J_PR},
  author       = {Mohsine Elkhayati and Youssfi Elkettani and Mohammed Mourchid},
  doi          = {10.1016/j.patcog.2021.108288},
  journal      = {Pattern Recognition},
  pages        = {108288},
  shortjournal = {Pattern Recognition},
  title        = {Segmentation of handwritten arabic graphemes using a directed convolutional neural network and mathematical morphology operations},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal association rule based deep annotation-free
clustering (STAR-DAC) for unsupervised person re-identification.
<em>PR</em>, <em>122</em>, 108287. (<a
href="https://doi.org/10.1016/j.patcog.2021.108287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-camera video surveillance environment has a variety of emerging research problems among, which person re-identification is the premier one. Unsupervised person re-identification has been explored less in literature than the supervised approach. Images acquired from the video surveillance systems are unlabeled, which denotes that it is naturally an unsupervised learning problem. The state-of-the-art unsupervised methods seek external annotations support such as incorporating transfer learning techniques, partial labeling of train images, etc., which makes them not purely unsupervised and unsuitable for practical real-world surveillance settings. Identity mismatch happens due to the similar costumes and complex environmental factors. To resolve this issue, we introduce a new framework named Spatio-Temporal Association Rule based Deep Annotation-free Clustering (STAR-DAC) which incrementally clusters the unlabeled person re-identification images based on visual features and performs cluster fine-tuning through the mined spatio-temporal association rules. STAR formulations leveraged upto 75\% of images for reliable sample selection through cluster fine-tuning. STAR based fine-tune algorithm aims to attain ground-truth labels of an unlabeled dataset and eliminate cluster outliers to stabilize the evaluation. Experiments are performed on image and video-based benchmark person re-identification datasets such as DukeMTMC re-ID, Market1501, MSMT17, CUHK03, GRID and Dukevideo re-ID, iLIDSVid, ViPer respectively. Experimental results clearly show that the proposed STAR-DAC framework outperforms the state-of-the-art methods in case of large scale datasets with multiple cameras.},
  archive      = {J_PR},
  author       = {Sridhar Raj S and Munaga V.N.K. Prasad and Ramadoss Balakrishnan},
  doi          = {10.1016/j.patcog.2021.108287},
  journal      = {Pattern Recognition},
  pages        = {108287},
  shortjournal = {Pattern Recognition},
  title        = {Spatio-temporal association rule based deep annotation-free clustering (STAR-DAC) for unsupervised person re-identification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective and efficient pixel-level detection for diverse
video copy-move forgery types. <em>PR</em>, <em>122</em>, 108286. (<a
href="https://doi.org/10.1016/j.patcog.2021.108286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video copy-move forgery detection (VCMFD) is a significant and greatly challenging task due to a variety of difficulties, including a huge amount of video information, diverse forgery types, rich forgery objects, and homogenous forgery sources. These difficulties raise four unresolved key challenges in VCMFD: i) ineffective detection in some popular forgery cases; ii) inefficient matching in processing numerous video pixels with hundred-dimensional features under dozens of matching iterations; iii) high false positive ( F P ) in detecting forgery videos; iv) low trade-off of efficiency and effectiveness in filling forgery region, and even failing in indicating forgeries at the pixel level. In this paper, a novel VCMFD method is proposed to address these issues: i) an innovatively improved SIFT structure that can address the thorough feature extraction in all video copy-move forgery cases; ii) a novel fast keypoint-label matching (FKLM) algorithm is proposed that creates some keypoint-label groups so that every high-dimensional feature is assigned into one of these groups. As a result, matching of video pixels can be directly done on a small number of keypoint-label groups only, leading to a nearly 500\% raise in matching efficiency; iii) a new coarse-to-fine filtering relying on intrinsic attributes of exact keypoint-matches is designed to more effectively reduce the false keypoint-matches; iv) the adaptive block filling relying on true keypoint-matches contributes to the accurate and efficient suspicious region filling, even at the pixel level. Finally, the suspicious region locations with the forgery vision persistence concept indicate forgery videos. Compared to the state-of-art methods, the experiments show that our proposed method achieves the best detection accuracy, lowest F P , and improved at least 16\% and 8\% of F 1 scores on the GRIP 2.0 dataset and a combination of SULFA 2.0 &amp; REWIND datasets. Furthermore, the proposed method is with low computational time (4.45 s/Mpixels), which is about 1/2-1/3 times of the latest DFMI-BM (8.02 s/Mpixels) and PM-2D (13.1 s/Mpixels) methods.},
  archive      = {J_PR},
  author       = {Jun-Liu Zhong and Yan-Fen Gan and Chi-Man Vong and Ji-Xiang Yang and Jing-Hong Zhao and Jia-Hua Luo},
  doi          = {10.1016/j.patcog.2021.108286},
  journal      = {Pattern Recognition},
  pages        = {108286},
  shortjournal = {Pattern Recognition},
  title        = {Effective and efficient pixel-level detection for diverse video copy-move forgery types},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Protect, show, attend and tell: Empowering image captioning
models with ownership protection. <em>PR</em>, <em>122</em>, 108285. (<a
href="https://doi.org/10.1016/j.patcog.2021.108285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By and large, existing Intellectual Property (IP) protection on deep neural networks typically i) focus on image classification task only, and ii) follow a standard digital watermarking framework that was conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that the current digital watermarking framework is insufficient to protect image captioning tasks that are often regarded as one of the frontiers AI problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect the image captioning model. From empirical points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose of infringement. To the best of our knowledge, this work is the first to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks. Code is available at https://github.com/jianhanlim/ipr-imagecaptioning},
  archive      = {J_PR},
  author       = {Jian Han Lim and Chee Seng Chan and Kam Woh Ng and Lixin Fan and Qiang Yang},
  doi          = {10.1016/j.patcog.2021.108285},
  journal      = {Pattern Recognition},
  pages        = {108285},
  shortjournal = {Pattern Recognition},
  title        = {Protect, show, attend and tell: Empowering image captioning models with ownership protection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context extraction module for deep convolutional neural
networks. <em>PR</em>, <em>122</em>, 108284. (<a
href="https://doi.org/10.1016/j.patcog.2021.108284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional layers convolve the input feature maps to generate valuable output features, and they help deep learning methods significantly in solving complex problems. In order to tackle problems efficiently, deep learning solutions should ensure that the parameters of the model do not increase significantly with the complexity of the problem. Pointwise convolutions are primarily used for parameter reduction in many deep learning architectures. They are convolutional filters of kernel size 1 × 1 1×1 . The pointwise convolution, however, ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of pointwise convolution has a significant impact on network performance. We propose a novel alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our approach extracts spatial context information from the input at two scales and further refines the extracted context based on the channel importance. Finally, we add the refined context to the output of the pointwise convolution. This is the first work that improves pointwise convolution by incorporating context information. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We perform experiments on coarse/fine-grained image classification , few-shot fine-grained classification, and on object detection. We further perform various ablation experiments to validate the significance of the different components used in our design. Lastly, we show experimentally that our proposed technique can be combined with existing state-of-the-art network performance improvement approaches to further improve the network performance.},
  archive      = {J_PR},
  author       = {Pravendra Singh and Pratik Mazumder and Vinay P. Namboodiri},
  doi          = {10.1016/j.patcog.2021.108284},
  journal      = {Pattern Recognition},
  pages        = {108284},
  shortjournal = {Pattern Recognition},
  title        = {Context extraction module for deep convolutional neural networks},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SetMargin loss applied to deep keystroke biometrics with
circle packing interpretation. <em>PR</em>, <em>122</em>, 108283. (<a
href="https://doi.org/10.1016/j.patcog.2021.108283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a new deep learning approach for keystroke biometrics based on a novel Distance Metric Learning method (DML). DML maps input data into a learned representation space that reveals a “semantic” structure based on distances. In this work, we propose a novel DML method specifically designed to address the challenges associated to free-text keystroke identification where the classes used in learning and inference are disjoint. The proposed SetMargin Loss (SM-L) extends traditional DML approaches with a learning process guided by pairs of sets instead of pairs of samples, as done traditionally. The proposed learning strategy allows to enlarge inter-class distances while maintaining the intra-class structure of keystroke dynamics. We analyze the resulting representation space using the mathematical problem known as Circle Packing, which provides neighbourhood structures with a theoretical maximum inter-class distance. We finally prove experimentally the effectiveness of the proposed approach on a challenging task: keystroke biometric identification over a large set of 78,000 subjects. Our method achieves state-of-the-art accuracy on a comparison performed with the best existing approaches.},
  archive      = {J_PR},
  author       = {Aythami Morales and Julian Fierrez and Alejandro Acien and Ruben Tolosana and Ignacio Serna},
  doi          = {10.1016/j.patcog.2021.108283},
  journal      = {Pattern Recognition},
  pages        = {108283},
  shortjournal = {Pattern Recognition},
  title        = {SetMargin loss applied to deep keystroke biometrics with circle packing interpretation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained action recognition using dynamic kernels.
<em>PR</em>, <em>122</em>, 108282. (<a
href="https://doi.org/10.1016/j.patcog.2021.108282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained action recognition involves comparison of similar actions of variable-length size consisting of subtle interactions between human and specific objects. Hence, we propose a dynamic kernel-based approach to handle the variable-length patterns for effective recognition of fine-grained actions. Initially, we extract local spatio-temporal features for each video to capture appearance and motion information effectively. An action-independent Gaussian mixture model (AIGMM) is trained on the extracted features of all fine-grained actions to analyze spatio-temporal information and preserve the local similarities among fine-grained actions. Then, the statistics of AIGMM, namely, mean, covariance, and posteriors are used to build the kernels for finding the similarity between any two fine-grained actions by mapping statistics to kernel feature space. We demonstrate the effectiveness of proposed approach using three dynamic kernels i.e., GMM mean interval kernel, supervector kernel, intermediate matching kernel on four varieties of fine-grained action datasets, namely, MERL , JIGSAWS, KSCGR , and MPII cooking2},
  archive      = {J_PR},
  author       = {Sravani Yenduri and Nazil Perveen and Vishnu Chalavadi and Krishna Mohan C},
  doi          = {10.1016/j.patcog.2021.108282},
  journal      = {Pattern Recognition},
  pages        = {108282},
  shortjournal = {Pattern Recognition},
  title        = {Fine-grained action recognition using dynamic kernels},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel quasi-newton method for composite convex
minimization. <em>PR</em>, <em>122</em>, 108281. (<a
href="https://doi.org/10.1016/j.patcog.2021.108281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fast parallelable Jacobi iteration type optimization method for non-smooth convex composite optimization is presented. Traditional gradient-based techniques cannot solve the problem. Smooth approximate functions are attempted to be used as a replacement of those non-smooth terms without compromising the accuracy. Recently, proximal mapping concept has been introduced into this field. Techniques which utilize proximal average based proximal gradient have been used to solve the problem. The state-of-art methods only utilize first-order information of the smooth approximate function. We integrate both first and second-order techniques to use both first and second-order information to boost the convergence speed. A convergence rate with a lower bound of O ( 1 k 2 ) O(1k2) is achieved by the proposed method and a super-linear convergence is enjoyed when there is proper second-order information. In experiments, the proposed method converges significantly better than the state of art methods which enjoy O ( 1 k ) O(1k) convergence.},
  archive      = {J_PR},
  author       = {W.H. Chai and S.S. Ho and H.C. Quek},
  doi          = {10.1016/j.patcog.2021.108281},
  journal      = {Pattern Recognition},
  pages        = {108281},
  shortjournal = {Pattern Recognition},
  title        = {A novel quasi-newton method for composite convex minimization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspectral super-resolution via coupled tensor ring
factorization. <em>PR</em>, <em>122</em>, 108280. (<a
href="https://doi.org/10.1016/j.patcog.2021.108280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) to obtain a high-resolution HSI (HR-HSI). In this paper, we propose a new model called coupled tensor ring factorization (CTRF) for HSR. The proposed CTRF approach simultaneously learns the tensor ring core tensors of the HR-HSI from a pair of HSI and MSI. The CTRF model can separately exploit the low-rank property of each class (Section 3.3), which has not been explored in previous coupled tensor models. Meanwhile, the model inherits the simple representation of coupled matrix/canonical polyadic factorization and flexible low-rank exploration of coupled Tucker factorization. We further introduce spectral nuclear norm regularization to explore the global spectral low-rank property. The experiments demonstrated the advantage of the proposed nuclear norm regularized CTRF model compared to previous matrix/tensor and deep learning methods.},
  archive      = {J_PR},
  author       = {Wei He and Yong Chen and Naoto Yokoya and Chao Li and Qibin Zhao},
  doi          = {10.1016/j.patcog.2021.108280},
  journal      = {Pattern Recognition},
  pages        = {108280},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral super-resolution via coupled tensor ring factorization},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A black-box adversarial attack strategy with adjustable
sparsity and generalizability for deep image classifiers. <em>PR</em>,
<em>122</em>, 108279. (<a
href="https://doi.org/10.1016/j.patcog.2021.108279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing adversarial perturbations for deep neural networks is an important direction of research. Crafting image-dependent adversarial perturbations using white-box feedback has hitherto been the norm for such adversarial attacks . However, black-box attacks are much more practical for real-world applications. Universal perturbations applicable across multiple images are gaining popularity due to their innate generalizability . There have also been efforts to restrict the perturbations to a few pixels in the image. This helps to retain visual similarity with the original images making such attacks hard to detect. This paper marks an important step that combines all these directions of research. We propose the DEceit algorithm for constructing effective universal pixel-restricted perturbations using only black-box feedback from the target network. We conduct empirical investigations using the ImageNet validation set on the state-of-the-art deep neural classifiers by varying the number of pixels to be perturbed from a meager 10 pixels to as high as all pixels in the image. We find that perturbing only about 10\% of the pixels in an image using DEceit achieves a commendable and highly transferable Fooling Rate while retaining the visual quality. We further demonstrate that DEceit can be successfully applied to image-dependent attacks as well. In both sets of experiments, we outperform several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Arka Ghosh and Sankha Subhra Mullick and Shounak Datta and Swagatam Das and Asit Kr. Das and Rammohan Mallipeddi},
  doi          = {10.1016/j.patcog.2021.108279},
  journal      = {Pattern Recognition},
  pages        = {108279},
  shortjournal = {Pattern Recognition},
  title        = {A black-box adversarial attack strategy with adjustable sparsity and generalizability for deep image classifiers},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature refinement: An expression-specific feature learning
and fusion method for micro-expression recognition. <em>PR</em>,
<em>122</em>, 108275. (<a
href="https://doi.org/10.1016/j.patcog.2021.108275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches have proposed various expression-shared features algorithms for micro-expression recognition. However, these approaches do not reveal the specific discriminative characteristics, which leads to sub-optimal performance. This paper proposes a novel Feature Refinement ( FeatRef ) with expression-specific feature learning and fusion for micro-expression recognition that aims to obtain salient and discriminative features for specific expressions and predicts expressions by fusing expression-specific features. FeatRef consists of an expression proposal module with an attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, to extract salient and discriminative features for specific expressions, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, category labels are predicted via a fusion of expression-specific features. Experiments on three publicly available databases validate the effectiveness of FeatRef under different protocols. The results on public benchmarks demonstrate that FeatRef provides salient and discriminative information for micro-expression recognition. The results also show that FeatRef achieves better or competitive performance with existing state-of-the-art methods on micro-expression recognition.},
  archive      = {J_PR},
  author       = {Ling Zhou and Qirong Mao and Xiaohua Huang and Feifei Zhang and Zhihong Zhang},
  doi          = {10.1016/j.patcog.2021.108275},
  journal      = {Pattern Recognition},
  pages        = {108275},
  shortjournal = {Pattern Recognition},
  title        = {Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GraphXCOVID: Explainable deep graph diffusion
pseudo-labelling for identifying COVID-19 on chest x-rays. <em>PR</em>,
<em>122</em>, 108274. (<a
href="https://doi.org/10.1016/j.patcog.2021.108274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing automatic techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data . We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist’s mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.},
  archive      = {J_PR},
  author       = {Angelica I. Aviles-Rivero and Philip Sellars and Carola-Bibiane Schönlieb and Nicolas Papadakis},
  doi          = {10.1016/j.patcog.2021.108274},
  journal      = {Pattern Recognition},
  pages        = {108274},
  shortjournal = {Pattern Recognition},
  title        = {GraphXCOVID: Explainable deep graph diffusion pseudo-labelling for identifying COVID-19 on chest X-rays},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Online learnable keyframe extraction in videos and its
application with semantic word vector in action recognition.
<em>PR</em>, <em>122</em>, 108273. (<a
href="https://doi.org/10.1016/j.patcog.2021.108273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though many of the frames in the video sequence are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in recognizing an action but also can reduce the processing time of each video sequence in classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion. Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key shots in video and thus, can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed. The experimental results on many commonly used datasets in video summarization and in action recognition have demonstrated the effectiveness of the proposed module.},
  archive      = {J_PR},
  author       = {G M Mashrur E Elahi and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2021.108273},
  journal      = {Pattern Recognition},
  pages        = {108273},
  shortjournal = {Pattern Recognition},
  title        = {Online learnable keyframe extraction in videos and its application with semantic word vector in action recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The UU-test for statistical modeling of unimodal data.
<em>PR</em>, <em>122</em>, 108272. (<a
href="https://doi.org/10.1016/j.patcog.2021.108272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciding on the unimodality of a dataset is an important problem in data analysis and statistical modeling. It allows to obtain knowledge about the structure of the dataset, i.e. whether data points have been generated by a probability distribution with a single or more than one peaks. Such knowledge is very useful for several data analysis problems, such as for deciding on the number of clusters and determining unimodal projections. We propose a technique called UU-test (Unimodal Uniform test) to decide on the unimodality of a one-dimensional dataset. The method operates on the empirical cumulative density function (ecdf) of the dataset. It attempts to build a piecewise linear approximation of the ecdf that is unimodal and models the data sufficiently in the sense that the data corresponding to each linear segment follows the uniform distribution. A unique feature of this approach is that in the case of unimodality, it also provides a statistical model of the data in the form of a Uniform Mixture Model. We present experimental results in order to assess the ability of the method to decide on unimodality and perform comparisons with the well-known dip-test approach. In addition, in the case of unimodal datasets we evaluate the Uniform Mixture Models provided by the proposed method using the test set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.},
  archive      = {J_PR},
  author       = {Paraskevi Chasani and Aristidis Likas},
  doi          = {10.1016/j.patcog.2021.108272},
  journal      = {Pattern Recognition},
  pages        = {108272},
  shortjournal = {Pattern Recognition},
  title        = {The UU-test for statistical modeling of unimodal data},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Well-calibrated confidence measures for multi-label text
classification with a large number of labels. <em>PR</em>, <em>122</em>,
108271. (<a href="https://doi.org/10.1016/j.patcog.2021.108271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend our previous work on Inductive Conformal Prediction (ICP) for multi-label text classification and present a novel approach for addressing the computational inefficiency of the Label Powerset (LP) ICP, arrising when dealing with a high number of unique labels. We present experimental results using the original and the proposed efficient LP-ICP on two English and one Czech language data-sets. Specifically, we apply the LP-ICP on three deep Artificial Neural Network (ANN) classifiers of two types: one based on contextualised (bert) and two on non-contextualised (word2vec) word-embeddings. In the LP-ICP setting we assign nonconformity scores to label-sets from which the corresponding p p -values and prediction-sets are determined. Our approach deals with the increased computational burden of LP by eliminating from consideration a significant number of label-sets that will surely have p p -values below the specified significance level. This reduces dramatically the computational complexity of the approach while fully respecting the standard CP guarantees. Our experimental results show that the contextualised-based classifier surpasses the non-contextualised-based ones and obtains state-of-the-art performance for all data-sets examined. The good performance of the underlying classifiers is carried on to their ICP counterparts without any significant accuracy loss, but with the added benefits of ICP, i.e. the confidence information encapsulated in the prediction sets. We experimentally demonstrate that the resulting prediction sets can be tight enough to be practically useful even though the set of all possible label-sets contains more than 1 e + 16 1e+16 combinations. Additionally, the empirical error rates of the obtained prediction-sets confirm that our outputs are well-calibrated.},
  archive      = {J_PR},
  author       = {Lysimachos Maltoudoglou and Andreas Paisios and Ladislav Lenc and Jiří Martínek and Pavel Král and Harris Papadopoulos},
  doi          = {10.1016/j.patcog.2021.108271},
  journal      = {Pattern Recognition},
  pages        = {108271},
  shortjournal = {Pattern Recognition},
  title        = {Well-calibrated confidence measures for multi-label text classification with a large number of labels},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep momentum uncertainty hashing. <em>PR</em>,
<em>122</em>, 108264. (<a
href="https://doi.org/10.1016/j.patcog.2021.108264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial optimization (CO) has been a hot research topic because of its theoretic and practical importance. As a classic CO problem , deep hashing aims to find an optimal code for each data from finite discrete possibilities, while the discrete nature brings a big challenge to the optimization process. Previous methods usually mitigate this challenge by binary approximation , substituting binary codes for real-values via activation functions or regularizations . However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance . In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty . It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin.},
  archive      = {J_PR},
  author       = {Chaoyou Fu and Guoli Wang and Xiang Wu and Qian Zhang and Ran He},
  doi          = {10.1016/j.patcog.2021.108264},
  journal      = {Pattern Recognition},
  pages        = {108264},
  shortjournal = {Pattern Recognition},
  title        = {Deep momentum uncertainty hashing},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep and interpretable regression models for ordinal
outcomes. <em>PR</em>, <em>122</em>, 108263. (<a
href="https://doi.org/10.1016/j.patcog.2021.108263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcomes with a natural order commonly occur in prediction problems and often the available input data are a mixture of complex data like images and tabular predictors. Deep Learning (DL) models are state-of-the-art for image classification tasks but frequently treat ordinal outcomes as unordered and lack interpretability . In contrast, classical ordinal regression models consider the outcome’s order and yield interpretable predictor effects but are limited to tabular data. We present ordinal neural network transformation models ( ontram s), which unite DL with classical ordinal regression approaches. ontram s are a special case of transformation models and trade off flexibility and interpretability by additively decomposing the transformation function into terms for image and tabular data using jointly trained neural networks . The performance of the most flexible ontram is by definition equivalent to a standard multi-class DL model trained with cross-entropy while being faster in training when facing ordinal outcomes. Lastly, we discuss how to interpret model components for both tabular and image data on two publicly available datasets.},
  archive      = {J_PR},
  author       = {Lucas Kook and Lisa Herzog and Torsten Hothorn and Oliver Dürr and Beate Sick},
  doi          = {10.1016/j.patcog.2021.108263},
  journal      = {Pattern Recognition},
  pages        = {108263},
  shortjournal = {Pattern Recognition},
  title        = {Deep and interpretable regression models for ordinal outcomes},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete online cross-modal hashing. <em>PR</em>,
<em>122</em>, 108262. (<a
href="https://doi.org/10.1016/j.patcog.2021.108262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of multimedia content on the Web which usually continuously comes in a stream fashion, online cross-modal hashing methods have attracted extensive interest in recent years. However, most online hashing methods adopt a relaxation strategy or real-valued auxiliary variable strategy to avoid complex optimization of hash codes, leading to large quantization errors . In this paper, based on Discrete Latent Factor model-based cross-modal Hashing (DLFH), we propose a novel cross-modal online hashing method, i.e., Discrete Online Cross-modal Hashing (DOCH). To generate uniform high-quality hash codes of different modal, DOCH not only directly exploits the similarity between newly coming data and old existing data in the Hamming space, but also utilizes the fine-grained semantic information by label embedding. Moreover, DOCH can discretely learn hash codes by an efficient optimization algorithm . Extensive experiments conducted on two real-world datasets demonstrate the superiority of DOCH.},
  archive      = {J_PR},
  author       = {Yu-Wei Zhan and Yongxin Wang and Yu Sun and Xiao-Ming Wu and Xin Luo and Xin-Shun Xu},
  doi          = {10.1016/j.patcog.2021.108262},
  journal      = {Pattern Recognition},
  pages        = {108262},
  shortjournal = {Pattern Recognition},
  title        = {Discrete online cross-modal hashing},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting foreground and background separation for
prohibited item detection in overlapping x-ray images. <em>PR</em>,
<em>122</em>, 108261. (<a
href="https://doi.org/10.1016/j.patcog.2021.108261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray imagery security screening is an essential component of transportation and logistics. In recent years, some researchers have used computer vision algorithms to replace inefficient and tedious manual baggage inspection. However, X-ray images are complicated, and objects overlap with one another in a semi-transparent state, which underperforms the existing object detection frameworks. To solve the severe overlapping problem of X-ray images, we propose a foreground and background separation (FBS) X-ray prohibited item detection framework, which separates prohibited items from other items to exclude irrelevant information. First, we design a target foreground and use recursive training to adaptively approximate the real foreground. Thereafter, with the constraints of X-ray imaging characteristics, a decoder is employed to separate the prohibited items from other irrelevant items to obtain the foreground and background (FB). Finally, we use the attention module to make the detection framework focus more on the foreground. Our method is evaluated on a synthetic dataset with FB ground truth and two public datasets with only bounding box annotations. Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art solutions. Furthermore, experiments are performed in the case where only a small number of images contain the FB ground truth. The results indicate that our method requires only a small number of FB ground truths to obtain a performance equivalent to that of all FB ground truths.},
  archive      = {J_PR},
  author       = {Fangtao Shao and Jing Liu and Peng Wu and Zhiwei Yang and Zhaoyang Wu},
  doi          = {10.1016/j.patcog.2021.108261},
  journal      = {Pattern Recognition},
  pages        = {108261},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting foreground and background separation for prohibited item detection in overlapping X-ray images},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Bayesian compression for dynamically expandable networks.
<em>PR</em>, <em>122</em>, 108260. (<a
href="https://doi.org/10.1016/j.patcog.2021.108260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops Bayesian Compression for Dynamically Expandable Network (BCDEN), which can learn a compact model structure with preserving the accuracy in a continual learning scenarios. Dynamically Expandable Network (DEN) is efficiently trained by performing selective retraining, dynamically expands network capacity with only the necessary number of units, and effectively prevents semantic drift by duplicating and timestamping units in an online manner. Overcoming conventional DEN only giving point estimates, we providing the Bayesian inference under the principle framework. We validate our BCDEN on multiple public datasets under continual learning setting, on which it can outperform existing continual learning methods on a variety of tasks, and with the state-of-the-art compression results, while still maintaining comparable performance.},
  archive      = {J_PR},
  author       = {Yang Yang and Bo Chen and Hongwei Liu},
  doi          = {10.1016/j.patcog.2021.108260},
  journal      = {Pattern Recognition},
  pages        = {108260},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian compression for dynamically expandable networks},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balanced single-shot object detection using cross-context
attention-guided network. <em>PR</em>, <em>122</em>, 108258. (<a
href="https://doi.org/10.1016/j.patcog.2021.108258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world application scenarios, object detection usually encounters two technical challenges, i.e., high accuracy and high speed. Although the latest detection frameworks based on anchor-free detection have achieved outstanding performance, they cannot be widely used in real-world scenarios due to their model complexity and slow speed. In this paper, inspired by cross-context attention mechanism of human visual systems , we propose a light but effective single-shot detection framework using Cross-context Attention-guided Network (CCAGNet) to balance the accuracy and speed. CCAGNet uses attention-guided mechanism to highlight the interaction of object-synergy regions, and suppresses non-object-synergy regions by combining Cross-context Attention Mechanism (CCAM), Receptive Field Attention Mechanism (RFAM), and Semantic Fusion Attention Mechanism (SFAM). The main contribution of our work includes establishing a novel attention mechanism that takes the context information of channel, spatial, cross- and adjacent-regions into consideration simultaneously. Extensive experiments demonstrate the feasibility and effectiveness of our method on the public benchmark datasets. To the best of our knowledge, CCAGNet obtains the state-of-the-art performance on both PascalVOC and MSCOCO with the excellent trade-off between accuracy and speed among single-shot detectors. Especially, the Average Precision (AP) metric is significantly improved by 17.0\% on small object detection on MSCOCO .},
  archive      = {J_PR},
  author       = {Shuyu Miao and Shanshan Du and Rui Feng and Yuejie Zhang and Huayu Li and Tianbi Liu and Lin Zheng and Weiguo Fan},
  doi          = {10.1016/j.patcog.2021.108258},
  journal      = {Pattern Recognition},
  pages        = {108258},
  shortjournal = {Pattern Recognition},
  title        = {Balanced single-shot object detection using cross-context attention-guided network},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SARS-net: COVID-19 detection from chest x-rays by combining
graph convolutional network and convolutional neural network.
<em>PR</em>, <em>122</em>, 108255. (<a
href="https://doi.org/10.1016/j.patcog.2021.108255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has emerged as one of the deadliest pandemics that has ever crept on humanity. Screening tests are currently the most reliable and accurate steps in detecting severe acute respiratory syndrome coronavirus in a patient, and the most used is RT-PCR testing. Various researchers and early studies implied that visual indicators (abnormalities) in a patient&#39;s Chest X-Ray (CXR) or computed tomography (CT) imaging were a valuable characteristic of a COVID-19 patient that can be leveraged to find out virus in a vast population. Motivated by various contributions to open-source community to tackle COVID-19 pandemic, we introduce SARS-Net, a CADx system combining Graph Convolutional Networks and Convolutional Neural Networks for detecting abnormalities in a patient&#39;s CXR images for presence of COVID-19 infection in a patient. In this paper, we introduce and evaluate the performance of a custom-made deep learning architecture SARS-Net, to classify and detect the Chest X-ray images for COVID-19 diagnosis. Quantitative analysis shows that the proposed model achieves more accuracy than previously mentioned state-of-the-art methods. It was found that our proposed model achieved an accuracy of 97.60\% and a sensitivity of 92.90\% on the validation set.},
  archive      = {J_PR},
  author       = {Aayush Kumar and Ayush R Tripathi and Suresh Chandra Satapathy and Yu-Dong Zhang},
  doi          = {10.1016/j.patcog.2021.108255},
  journal      = {Pattern Recognition},
  pages        = {108255},
  shortjournal = {Pattern Recognition},
  title        = {SARS-net: COVID-19 detection from chest x-rays by combining graph convolutional network and convolutional neural network},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient COVID-19 testing via contextual model based
compressive sensing. <em>PR</em>, <em>122</em>, 108253. (<a
href="https://doi.org/10.1016/j.patcog.2021.108253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic is threatening billions of people&#39;s life all over the world. As of March 6, 2021, covid-19 has confirmed in 115,653,459 people worldwide. It has also a devastating effect on businesses and social activities. Since there is still no definite cure for this disease, extensive testing is the most critical issue to determine the trend of illness, appropriate medical treatment, and make social distancing policies. Besides, testing more people in a shorter time helps to contain the contagion. The PCR-based methods are the most popular tests which take about an hour to make the output result. Obviously, it makes the number of tests highly limited and consequently, hurts the efficiency of pandemic control. In this paper, we propose a new approach to identify affected individuals with a considerably reduced No. of tests. Intuitively, saving time and resources is the main advantage of our approach. We use contextual information to make a graph-based model to be used in model-based compressive sensing (CS). Our proposed model makes the testing with fewer tests required compared to traditional testing methods and even group testing. We embed contextual information such as age, underlying disease, symptoms (i.e. cough, fever, fatigue, loss of consciousness), and social contacts into a graph-based model. This model is used in model-based CS to minimize the required test. We take advantage of Discrete Graph Signal Processing on Graph (DSP G ) to generate the model. Our contextual model makes CS more efficient in both the number of samples and the recovery quality. Moreover, it can be applied in the case that group testing is not applicable due to its severe dependency on sparsity. Experimental results show that the overall testing speed (individuals per test ratio) increases more than 15 times compared to the individual testing with the error of less than 5\% which is dramatically lower than that of traditional compressive sensing.},
  archive      = {J_PR},
  author       = {Mehdi Hasaninasab and Mohammad Khansari},
  doi          = {10.1016/j.patcog.2021.108253},
  journal      = {Pattern Recognition},
  pages        = {108253},
  shortjournal = {Pattern Recognition},
  title        = {Efficient COVID-19 testing via contextual model based compressive sensing},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JSPNet: Learning joint semantic &amp; instance segmentation
of point clouds via feature self-similarity and cross-task probability.
<em>PR</em>, <em>122</em>, 108250. (<a
href="https://doi.org/10.1016/j.patcog.2021.108250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method named JSPNet, to segment 3D point cloud in semantic and instance simultaneously. First, we analyze the problem in addressing joint semantic and instance segmentation, including the common ground of cooperation of two tasks, conflict of two tasks, quadruplet relation between semantic and instance distributions, and ignorance of existing works. Then we introduce our method to reinforce mutual cooperation and alleviate the essential conflict. Our method has a shared encoder and two decoders to address two tasks. Specifically, to maintain discriminative features and characterize inconspicuous content, a similarity-based feature fusion module is designed to locate the inconspicuous area in the feature of current branch and then select related features from the other branch to compensate for the unclear content. Furthermore, given the salient semantic feature and the salient instance feature, a cross-task probability-based feature fusion module is developed to establish the probabilistic correlation between semantic and instance features. This module could transform features from one branch and further fuse them with the other branch by multiplying probabilistic matrix. Experimental results on a large-scale 3D indoor point cloud dataset S3DIS and a part-segmentation dataset ShapeNet have demonstrated the superiority of our method over existing state-of-the-arts in both semantic and instance segmentation. The proposed method outperforms PointNet with 12\% and 26\% improvements and outperforms ASIS with 2.7\% and 4.3\% improvements in terms of mIoU and mPre. Code of this work has been made available at https://github.com/Chenfeng1271/JSPNet .},
  archive      = {J_PR},
  author       = {Feng Chen and Fei Wu and Guangwei Gao and Yimu Ji and Jing Xu and Guo-Ping Jiang and Xiao-Yuan Jing},
  doi          = {10.1016/j.patcog.2021.108250},
  journal      = {Pattern Recognition},
  pages        = {108250},
  shortjournal = {Pattern Recognition},
  title        = {JSPNet: Learning joint semantic &amp; instance segmentation of point clouds via feature self-similarity and cross-task probability},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep image prior based defense against adversarial examples.
<em>PR</em>, <em>122</em>, 108249. (<a
href="https://doi.org/10.1016/j.patcog.2021.108249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks (DNNs) have shown serious vulnerability to adversarial examples with imperceptible perturbation to clean images. To counter this issue, many powerful defensive methods ( e.g. , ComDefend) focus on rectifying the adversarial examples with well-trained models from a large training dataset ( e.g. , clean-adversarial image pairs). However, such methods rely heavily on the learned external priors from an external large training dataset, while neglecting the rich image internal priors of the input itself, thus limiting the generalization of the defense models against the adversarial examples with biased image statistics from the external training dataset. Motivated by deep image prior that can capture rich image statistics from a single image, we propose an effective Deep Image Prior Driven Defense (DIPDefend) method against adversarial examples. With a DIP generator to fit the target/adversarial input, we find that our image reconstruction exhibits quite interesting learning preference from a feature learning perspectives, i.e. , the early stage primarily learns the robust features resistant to adversarial perturbation, followed by learning non-robust features that are sensitive to adversarial perturbation. Besides, we develop an adaptive stopping strategy that adapts our method to diverse images. In this way, the proposed model obtains a unique defender for each individual adversarial input, thus being robust to various attackers. Experimental results demonstrate the superiority of our method over the state-of-the-art defense methods against white-box and black-box adversarial attacks .},
  archive      = {J_PR},
  author       = {Tao Dai and Yan Feng and Bin Chen and Jian Lu and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2021.108249},
  journal      = {Pattern Recognition},
  pages        = {108249},
  shortjournal = {Pattern Recognition},
  title        = {Deep image prior based defense against adversarial examples},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated generalized zero-shot learning for fine-grained
classification. <em>PR</em>, <em>122</em>, 108246. (<a
href="https://doi.org/10.1016/j.patcog.2021.108246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding learning (EL) and feature synthesizing (FS) are two of the popular categories of fine-grained GZSL methods. EL or FS using global features cannot discriminate fine details in the absence of local features . On the other hand, EL or FS methods exploiting local features either neglect direct attribute guidance or global information. Consequently, neither method performs well. In this paper, we propose to explore global and direct attribute-supervised local visual features for both EL and FS categories in an integrated manner for fine-grained GZSL. The proposed integrated network has an EL sub-network and a FS sub-network. Consequently, the proposed integrated network can be tested in two ways. We propose a novel two-step dense attention mechanism to discover attribute-guided local visual features. We introduce new mutual learning between the sub-networks to exploit mutually beneficial information for optimization. Moreover, we propose to compute source-target class similarity based on mutual information and transfer-learn the target classes to reduce bias towards the source domain during testing. We demonstrate that our proposed method outperforms contemporary methods on benchmark datasets.},
  archive      = {J_PR},
  author       = {Tasfia Shermin and Shyh Wei Teng and Ferdous Sohel and Manzur Murshed and Guojun Lu},
  doi          = {10.1016/j.patcog.2021.108246},
  journal      = {Pattern Recognition},
  pages        = {108246},
  shortjournal = {Pattern Recognition},
  title        = {Integrated generalized zero-shot learning for fine-grained classification},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards automatic threat detection: A survey of advances of
deep learning within x-ray security imaging. <em>PR</em>, <em>122</em>,
108245. (<a href="https://doi.org/10.1016/j.patcog.2021.108245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray security screening is widely used to maintain aviation/transport security, and its significance poses a particular interest in automated screening systems. This paper aims to review computerised X-ray security imaging algorithms by taxonomising the field into conventional machine learning and contemporary deep learning applications. The first part briefly discusses the classical machine learning approaches utilised within X-ray security imaging, while the latter part thoroughly investigates the use of modern deep learning algorithms. The proposed taxonomy sub-categorises the use of deep learning approaches into supervised and unsupervised learning , with a particular focus on object classification, detection, segmentation and anomaly detection tasks. The paper further explores well-established X-ray datasets and provides a performance benchmark. Based on the current and future trends in deep learning, the paper finally presents a discussion and future directions for X-ray security imagery.},
  archive      = {J_PR},
  author       = {Samet Akcay and Toby Breckon},
  doi          = {10.1016/j.patcog.2021.108245},
  journal      = {Pattern Recognition},
  pages        = {108245},
  shortjournal = {Pattern Recognition},
  title        = {Towards automatic threat detection: A survey of advances of deep learning within X-ray security imaging},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring DeshuffleGANs in self-supervised generative
adversarial networks. <em>PR</em>, <em>122</em>, 108244. (<a
href="https://doi.org/10.1016/j.patcog.2021.108244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation . Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN .},
  archive      = {J_PR},
  author       = {Gulcin Baykal and Furkan Ozcelik and Gozde Unal},
  doi          = {10.1016/j.patcog.2021.108244},
  journal      = {Pattern Recognition},
  pages        = {108244},
  shortjournal = {Pattern Recognition},
  title        = {Exploring DeshuffleGANs in self-supervised generative adversarial networks},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task driven explainable diagnosis of COVID-19 using
chest x-ray images. <em>PR</em>, <em>122</em>, 108243. (<a
href="https://doi.org/10.1016/j.patcog.2021.108243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network (COMiT-Net) which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions and semantic segmentation of COVID19 symptoms in CXRs taken from the ChestXray-14, CheXpert, and a consolidated COVID-19 dataset. These annotations will be released to the research community. Experiments performed with more than 2500 frontal CXR images show that at 90\% specificity, the proposed COMiT-Net yields 96.80\% sensitivity.},
  archive      = {J_PR},
  author       = {Aakarsh Malhotra and Surbhi Mittal and Puspita Majumdar and Saheb Chhabra and Kartik Thakral and Mayank Vatsa and Richa Singh and Santanu Chaudhury and Ashwin Pudrod and Anjali Agrawal},
  doi          = {10.1016/j.patcog.2021.108243},
  journal      = {Pattern Recognition},
  pages        = {108243},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task driven explainable diagnosis of COVID-19 using chest X-ray images},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data synthesis method preserving correlation of features.
<em>PR</em>, <em>122</em>, 108241. (<a
href="https://doi.org/10.1016/j.patcog.2021.108241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abundant data are essential for improving the performance of machine learning algorithms. Thus, if only limited data are available, data synthesis can be used to enlarge datasets. Data synthesis methods based on the covariance matrix are useful because of their fast data synthesis capabilities. However, artificial datasets generated via classical techniques show statistical discrepancies when compared to original datasets. To address this problem, we developed a new data synthesis method that preserves the correlation (between features) observed in the original dataset. This preservation was realized by considering not only the correlation but also the random noises used in data synthesis process. This method was applied to various biosignals (i.e., electrocortiography, electromyogram, and electrocardiogram), wherein data points are insufficient. Several classifiers (i.e., convolutional neural network, support vector machine, and k -nearest neighbor) were used to verify that the classification accuracy can be improved by the proposed data synthesis method.},
  archive      = {J_PR},
  author       = {Wonseok Yang and Woochul Nam},
  doi          = {10.1016/j.patcog.2021.108241},
  journal      = {Pattern Recognition},
  pages        = {108241},
  shortjournal = {Pattern Recognition},
  title        = {Data synthesis method preserving correlation of features},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning panoptic segmentation through feature
discriminability. <em>PR</em>, <em>122</em>, 108240. (<a
href="https://doi.org/10.1016/j.patcog.2021.108240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoptic segmentation has attracted increasing attention as a joint task of semantic and instance segmentation. However, previous works have not noticed that the different requirements for semantic and instance segmentation can lead to conflict of feature discriminability. Instance segmentation mainly focuses on the central area of each instance in things regions, while semantic segmentation focuses on the whole region of a specific class. To resolve it, we propose: 1) a Dual-FPN framework which separates the shared Feature Pyramid Network (FPN) in previous works to reduce the conflict of receptive field and meet different requirements of the two tasks; 2) a Region Refinement Module which leverages the prediction of semantic segmentation to refine the result of instance segmentation and resolves the conflict between the things regions and the stuff regions. Experimental results on Cityscapes dataset and Mapillary Vistas dataset show that our proposed method can improve the result of both things and stuff and obtain state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Tao Chu and Wenjie Cai and Qiong Liu},
  doi          = {10.1016/j.patcog.2021.108240},
  journal      = {Pattern Recognition},
  pages        = {108240},
  shortjournal = {Pattern Recognition},
  title        = {Learning panoptic segmentation through feature discriminability},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot learning via a specific rank-controlled semantic
autoencoder. <em>PR</em>, <em>122</em>, 108237. (<a
href="https://doi.org/10.1016/j.patcog.2021.108237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing embedding zero-shot learning models usually learn a projection function from the visual feature space to the semantic embedding space, e.g. attribute space or word vector space. However, the projection learned based on seen samples may not generalize well to unseen classes, which is known as the projection domain shift problem in ZSL. To address this issue, we propose a method named Low-rank Semantic Autoencoder (LSA) to consider the low-rank structure of seen samples to maintain the sparse feature of reconstruction error, which can further improve zero-shot learning capability. Moreover, to obtain a more robust projection for unseen classes, we propose a Specific Rank-controlled Semantic Autoencoder (SRSA) to accurately control of the projection’s rank. Extensive experiments on six benchmarks demonstrate the superiority of the proposed models over most existing embedding ZSL models under the standard zero-shot setting and the more realistic generalized zero-shot setting.},
  archive      = {J_PR},
  author       = {Yang Liu and Xinbo Gao and Jungong Han and Li Liu and Ling Shao},
  doi          = {10.1016/j.patcog.2021.108237},
  journal      = {Pattern Recognition},
  pages        = {108237},
  shortjournal = {Pattern Recognition},
  title        = {Zero-shot learning via a specific rank-controlled semantic autoencoder},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factored latent-dynamic conditional random fields for single
and multi-label sequence modeling. <em>PR</em>, <em>122</em>, 108236.
(<a href="https://doi.org/10.1016/j.patcog.2021.108236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional Random Fields (CRF) are frequently applied for labeling and segmenting sequence data. Morency et al. (2007) introduced hidden state variables in a labeled CRF structure in order to model the latent dynamics within class labels, thus improving the labeling performance. Such a model is known as Latent-Dynamic CRF (LDCRF). We present Factored LDCRF (FLDCRF), a structure that allows multiple latent dynamics of the class labels to interact with each other. Including such latent-dynamic interactions leads to improved labeling performance on single-label and multi-label sequence modeling experiments across two different datasets, viz., UCI gesture phase data and UCI opportunity data. FLDCRF outperforms all state-of-the-art sequence models, viz., CRF, LDCRF, LSTM, LSTM-CRF, Factorial CRF, Coupled CRF and a multi-label LSTM model across experiments in this paper. In addition, FLDCRF offers easier model selection and is more consistent across validation and test data than LSTM models. FLDCRF is also much faster to train compared to LSTM, even without a GPU . FLDCRF outshines the best LSTM model by ∼ ∼ 4\% on a single-label task on the UCI gesture phase data and outperforms LSTM models by ∼ ∼ 2\% on average on the multi-label sequence tagging experiment on the UCI opportunity data.},
  archive      = {J_PR},
  author       = {Satyajit Neogi and Justin Dauwels},
  doi          = {10.1016/j.patcog.2021.108236},
  journal      = {Pattern Recognition},
  pages        = {108236},
  shortjournal = {Pattern Recognition},
  title        = {Factored latent-dynamic conditional random fields for single and multi-label sequence modeling},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrepant multiple instance learning for weakly supervised
object detection. <em>PR</em>, <em>122</em>, 108233. (<a
href="https://doi.org/10.1016/j.patcog.2021.108233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Instance Learning (MIL) is a fundamental method for weakly supervised object detection (WSOD), but experiences difficulty in excluding local optimal solutions and may miss objects or falsely localize object parts. In this paper, we introduce discrepantly collaborative modules into MIL and thereby create discrepant multiple instance learning (D-MIL), pursuing optimal solutions in a simple-yet-effective way. D-MIL adopts multiple MIL learners to pursue discrepant yet complementary solutions indicating object parts, which are fused with a collaboration module for precise object localization . D-MIL implements a new “teachers-students” model, where MIL learners act as “teachers” and object detectors as “students”. Multiple teachers provide rich yet complementary information, which are absorbed by students and transferred back to reinforce the performance of teachers. Experiments show that D-MIL significantly improves the baseline while achieves state-of-the-art performance on the challenging MS-COCO object detection benchmark.},
  archive      = {J_PR},
  author       = {Wei Gao and Fang Wan and Jun Yue and Songcen Xu and Qixiang Ye},
  doi          = {10.1016/j.patcog.2021.108233},
  journal      = {Pattern Recognition},
  pages        = {108233},
  shortjournal = {Pattern Recognition},
  title        = {Discrepant multiple instance learning for weakly supervised object detection},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neighbor-aware embedding for node clustering in
attributed graphs. <em>PR</em>, <em>122</em>, 108230. (<a
href="https://doi.org/10.1016/j.patcog.2021.108230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k k -means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task . In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering ( DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder , which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results . The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.},
  archive      = {J_PR},
  author       = {Chun Wang and Shirui Pan and Celina P. Yu and Ruiqi Hu and Guodong Long and Chengqi Zhang},
  doi          = {10.1016/j.patcog.2021.108230},
  journal      = {Pattern Recognition},
  pages        = {108230},
  shortjournal = {Pattern Recognition},
  title        = {Deep neighbor-aware embedding for node clustering in attributed graphs},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph matching based on fast normalized cut and
multiplicative update mapping. <em>PR</em>, <em>122</em>, 108228. (<a
href="https://doi.org/10.1016/j.patcog.2021.108228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point correspondence is a fundamental problem in pattern recognition and computer vision , which can be tackled by graph matching . Since graph matching is basically an NP-complete problem, some approximate methods are proposed to solve it. Continuous relaxation offers an effective approximate method for graph matching problem. However, the discrete constraint is not taken into consideration in the optimization step. In this paper, a fast normalized cut based graph matching method is proposed, where the discrete constraint is introduced into the optimization step. Specifically, first a semidefinite positive affinity matrix based form objective function is constructed by introducing a regularization term which is related to the discrete constraint. Then the fast normalized cut algorithm is utilized to find the continuous solution. Last, the discrete solution of graph matching is obtained by a multiplicative update algorithm. Experiments on both synthetic points and real-world images validate the effectiveness of the proposed method by comparing it with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jing Yang and Xu Yang and Zhang-Bing Zhou and Zhi-Yong Liu},
  doi          = {10.1016/j.patcog.2021.108228},
  journal      = {Pattern Recognition},
  pages        = {108228},
  shortjournal = {Pattern Recognition},
  title        = {Graph matching based on fast normalized cut and multiplicative update mapping},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Multi-task framework based on feature separation and
reconstruction for cross-modal retrieval. <em>PR</em>, <em>122</em>,
108217. (<a href="https://doi.org/10.1016/j.patcog.2021.108217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval.},
  archive      = {J_PR},
  author       = {Li Zhang and Xiangqian Wu},
  doi          = {10.1016/j.patcog.2021.108217},
  journal      = {Pattern Recognition},
  pages        = {108217},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task framework based on feature separation and reconstruction for cross-modal retrieval},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multimodal attention fusion network with a dynamic
vocabulary for TextVQA. <em>PR</em>, <em>122</em>, 108214. (<a
href="https://doi.org/10.1016/j.patcog.2021.108214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) is a well-known problem in computer vision . Recently, Text-based VQA tasks are getting more and more attention because text information is very important for image understanding. The key to this task is to make good use of text information in the image. In this work, we propose an attention-based encoder-decoder network that combines the multimodal information of visual, linguistic, and location features together. By using the attention mechanism to focus on key features to the question, our multimodal feature fusion can provide more accurate information to improve the performance. Furthermore, we present a decoder with attention map loss, which can not only predict complex answers but also deal with a dynamic vocabulary to reduce the decoding space. Compared with softmax-based cross entropy loss which can only handle a fixed-length vocabulary, the attention map loss significantly improves the accuracy and efficiency. Our method achieved the first place of all three tasks in the ICDAR2019 robust reading challenge on scene text visual question answering (ST-VQA).},
  archive      = {J_PR},
  author       = {Jiajia Wu and Jun Du and Fengren Wang and Chen Yang and Xinzhe Jiang and Jinshui Hu and Bing Yin and Jianshu Zhang and Lirong Dai},
  doi          = {10.1016/j.patcog.2021.108214},
  journal      = {Pattern Recognition},
  pages        = {108214},
  shortjournal = {Pattern Recognition},
  title        = {A multimodal attention fusion network with a dynamic vocabulary for TextVQA},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video anomaly detection with spatio-temporal dissociation.
<em>PR</em>, <em>122</em>, 108213. (<a
href="https://doi.org/10.1016/j.patcog.2021.108213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in videos remains a challenging task due to the ambiguous definition of anomaly and the complexity of visual scenes from real video data. Different from the previous work which utilizes reconstruction or prediction as an auxiliary task to learn the temporal regularity , in this work, we explore a novel convolution autoencoder architecture that can dissociate the spatio-temporal representation to separately capture the spatial and the temporal information, since abnormal events are usually different from the normality in appearance and/or motion behavior . Specifically, the spatial autoencoder models the normality on the appearance feature space by learning to reconstruct the input of the first individual frame (FIF), while the temporal part takes the first four consecutive frames as the input and the RGB difference as the output to simulate the motion of optical flow in an efficient way. The abnormal events, which are irregular in appearance or in motion behavior , lead to a large reconstruction error. To improve detection performance on fast moving outliers, we exploit a variance-based attention module and insert it into the motion autoencoder to highlight large movement areas. In addition, we propose a deep K-means cluster strategy to force the spatial and the motion encoder to extract a compact representation . Extensive experiments on some publicly available datasets have demonstrated the effectiveness of our method which achieves the state-of-the-art performance. The code is publicly released at the link 1 .},
  archive      = {J_PR},
  author       = {Yunpeng Chang and Zhigang Tu and Wei Xie and Bin Luo and Shifu Zhang and Haigang Sui and Junsong Yuan},
  doi          = {10.1016/j.patcog.2021.108213},
  journal      = {Pattern Recognition},
  pages        = {108213},
  shortjournal = {Pattern Recognition},
  title        = {Video anomaly detection with spatio-temporal dissociation},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biological eagle eye-based method for change detection in
water scenes. <em>PR</em>, <em>122</em>, 108203. (<a
href="https://doi.org/10.1016/j.patcog.2021.108203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) is an important vision task for autonomous landing of unmanned aerial vehicles (UAV) on water. High-density photoreceptors and lateral inhibition mechanisms have inspired a novel biologic computational method based on structure and properties in eagle eyes as proposed for change detection. We call this method “STabCD,” which ensures spatiotemporal distribution consistency to achieve foreground acquisition, noise reduction, and background adaptability. Therefore, our proposed model responds strongly to object information and suppresses noise and wave textures. Then, we present a cloning method to simulate water scenes and collect a new synthetic dataset (called “Synthetic Boat Sequence”) for UAV vision research. Besides, we utilize synthetic datasets and corresponding real datasets to conduct change detection experiments. The experimental results indicate that: 1) the STabCD model achieves the best results in real or synthetic water landing scenes; and 2) change detection models for UAV can be quantitatively analyzed and tested under challenging synthetic scenarios.},
  archive      = {J_PR},
  author       = {Xuan Li and Haibin Duan and Jingchun Li and Yimin Deng and Fei-Yue Wang},
  doi          = {10.1016/j.patcog.2021.108203},
  journal      = {Pattern Recognition},
  pages        = {108203},
  shortjournal = {Pattern Recognition},
  title        = {Biological eagle eye-based method for change detection in water scenes},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A near effective and efficient model in recognition.
<em>PR</em>, <em>122</em>, 108173. (<a
href="https://doi.org/10.1016/j.patcog.2021.108173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuro-fuzzy models have been applied in various domains, in which the issue of long time-consumption for optimizing parameters and less innovation in fuzzy method for feature extraction remains to be solved. Here, we present a novel cycle reinforce hierarchical model (CRHM) for effective and efficient recognition. The innovative strategies of CRHM consist of the hierarchical structure, the groups of fuzzy subsystems and the cycle mechanism. The hierarchical structure is innovatively built to extract features and transform the low-level features into advanced ones semantically, in which we adopt the groups of fuzzy subsystems as feature extraction units in each hidden layer, which ensures the diversity of features, avoids the fuzzy rules explosion, and reduces the time for clustering. The cycle mechanism is first proposed to connect the hierarchical structure and the output layer directly, transferring the tuned parameters again and again, to reinforce features gradually. To demonstrate the performance of CRHM, we have conducted extensive comparison with several state-of-the-art algorithms on benchmark 1D and 2D datasets. The experimental results show that the recognition rate of CRHM is higher than convolutional neural network (CNN), while the training time is only 5\% of CNN&#39;s, which confirms that our approach provides a novel model for recognition, which can simultaneously improve the effectiveness and efficiency without the need of advanced equipment. In addition, the analysis results about the contribution of the core strategies to CRHM performance indicates that the contribution of the hierarchical structure is greater than that of the groups of fuzzy subsystems, which is superior than that of the cycle mechanism.},
  archive      = {J_PR},
  author       = {Hongjun Li and Ze Zhou and Chaobo Li and Ching Y. Suen},
  doi          = {10.1016/j.patcog.2021.108173},
  journal      = {Pattern Recognition},
  pages        = {108173},
  shortjournal = {Pattern Recognition},
  title        = {A near effective and efficient model in recognition},
  volume       = {122},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Learning common and label-specific features for multi-label
classification with correlation information. <em>PR</em>, <em>121</em>,
108259. (<a href="https://doi.org/10.1016/j.patcog.2021.108259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label classification, many existing works only pay attention to the label-specific features and label correlation while they ignore the common features and instance correlation, which are also essential for building a competitive classifier. Besides, existing works usually depend on the assumption that they tend to have the similar label-specific features if two labels are correlated. However, this assumption cannot always hold in some cases. Therefore, in this paper, we propose a new approach of learning common and label-specific features for multi-label classification using the correlation information from labels and instances. First, we introduce l 2 , 1 l2,1 -norm and l 1 l1 -norm regularizers to learn common and label-specific features simultaneously. Second, we use a regularizer to constrain label correlations on label outputs instead of coefficient matrix . Finally, instance correlations are also considered through the k-nearest neighbor mechanism. Comprehensive experiments manifest the superiority of our proposed approach against other well-established multi-label learning algorithms for label-specific features.},
  archive      = {J_PR},
  author       = {Junlong Li and Peipei Li and Xuegang Hu and Kui Yu},
  doi          = {10.1016/j.patcog.2021.108259},
  journal      = {Pattern Recognition},
  pages        = {108259},
  shortjournal = {Pattern Recognition},
  title        = {Learning common and label-specific features for multi-label classification with correlation information},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Part-based annotation-free fine-grained classification of
images of retail products. <em>PR</em>, <em>121</em>, 108257. (<a
href="https://doi.org/10.1016/j.patcog.2021.108257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel solution that classifies very similar images (fine-grained classification) of variants of retail products displayed on the racks of supermarkets. The proposed scheme simultaneously captures object-level and part-level cues of the product images. The object-level cues of the product images are captured with our novel reconstruction-classification network (RC-Net). For annotation-free modeling of part-level cues, the discriminatory parts of the product images are identified around the keypoints . The ordered sequences of these discriminatory parts, encoded using convolutional LSTM , describe the products uniquely. Finally, the part-level and object-level models jointly determine the products explicitly explaining coarse to finer descriptions of the products. This bi-level architecture is embedded in R-CNN for recognizing variants of retail products on the rack. We perform extensive experiments on one In-house and three benchmark datasets. The proposed scheme outperforms competing methods in almost all the evaluations.},
  archive      = {J_PR},
  author       = {Bikash Santra and Avishek Kumar Shaw and Dipti Prasad Mukherjee},
  doi          = {10.1016/j.patcog.2021.108257},
  journal      = {Pattern Recognition},
  pages        = {108257},
  shortjournal = {Pattern Recognition},
  title        = {Part-based annotation-free fine-grained classification of images of retail products},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical model for learning to understand head gesture
videos. <em>PR</em>, <em>121</em>, 108256. (<a
href="https://doi.org/10.1016/j.patcog.2021.108256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head gesture videos recorded of a person bear rich information about the individual. Automatically understanding these videos can empower many useful human-centered applications in areas such as smart health, education, work safety and security. To understand a video’s content, low-level head gesture signals carried in the video that capture characteristics of both human postures and motions need to be translated into high-level semantic labels. To meet this aim, we propose a hierarchical model for learning to understand head gesture videos. Given a head gesture video of an arbitrary length, the model first segments the full-length video into multiple short clips for clip-based feature extraction. Multiple base feature extraction procedures are then independently tuned via a set of peripheral learning tasks without consuming any labels of the goal task. These independently derived base features are subsequently aggregated through a multi-task learning framework, coupled with a feature dimensionality reduction module, to optimally learn to accomplish the end video understanding task in an weakly supervised manner, utilizing the limited amount of video labels available of the goal task. Experimental results show that the hierarchical model is superior to multiple state-of-the-art peer methods in tackling versatile video understanding tasks.},
  archive      = {J_PR},
  author       = {Jiachen Li and Songhua Xu and Xueying Qin},
  doi          = {10.1016/j.patcog.2021.108256},
  journal      = {Pattern Recognition},
  pages        = {108256},
  shortjournal = {Pattern Recognition},
  title        = {A hierarchical model for learning to understand head gesture videos},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to schedule multi-NUMA virtual machines via
reinforcement learning. <em>PR</em>, <em>121</em>, 108254. (<a
href="https://doi.org/10.1016/j.patcog.2021.108254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing , the importance of dynamic virtual machine scheduling is increasing. Existing works formulate the VM scheduling as a bin-packing problem and design greedy methods to solve it. However, cloud service providers widely adopt multi-NUMA architecture servers in recent years, and existing methods do not consider the architecture. This paper formulates the multi-NUMA VM scheduling into a novel structured combinatorial optimization and transforms it into a reinforcement learning problem. We propose a reinforcement learning algorithm called SchedRL with a delta reward scheme and an episodic guided sampling strategy to solve the problem efficiently. Evaluating on a public dataset of Azure under two different scenarios, our SchedRL outperforms FirstFit and BestFit on the fulfill number and allocation rate.},
  archive      = {J_PR},
  author       = {Junjie Sheng and Yiqiu Hu and Wenli Zhou and Lei Zhu and Bo Jin and Jun Wang and Xiangfeng Wang},
  doi          = {10.1016/j.patcog.2021.108254},
  journal      = {Pattern Recognition},
  pages        = {108254},
  shortjournal = {Pattern Recognition},
  title        = {Learning to schedule multi-NUMA virtual machines via reinforcement learning},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pedestrian trajectory prediction with convolutional neural
networks. <em>PR</em>, <em>121</em>, 108252. (<a
href="https://doi.org/10.1016/j.patcog.2021.108252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving . In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks . In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.},
  archive      = {J_PR},
  author       = {Simone Zamboni and Zekarias Tilahun Kefato and Sarunas Girdzijauskas and Christoffer Norén and Laura Dal Col},
  doi          = {10.1016/j.patcog.2021.108252},
  journal      = {Pattern Recognition},
  pages        = {108252},
  shortjournal = {Pattern Recognition},
  title        = {Pedestrian trajectory prediction with convolutional neural networks},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel GCN-based point cloud classification model robust to
pose variances. <em>PR</em>, <em>121</em>, 108251. (<a
href="https://doi.org/10.1016/j.patcog.2021.108251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud data can be produced by many depth sensors, such as Light Detection and Ranging (LIDAR) and RGB-D cameras, and they are widely used in broad applications of robotic navigation and remote-sensing for the understanding of environment. Hence, new techniques for object representation and classification based on 3D point cloud are becoming increasingly in high demand. Due to the irregularity of the object shape, the point cloud-based object recognition is a very challenging task, especially the pose variances of a point cloud will impose many difficulties. In this paper, we tackle the challenge of pose variances in object classification based on point cloud by developing a novel end-to-end pose robust graph convolutional network . Technically, we first represent the point cloud using the spherical system instead of the traditional Cartesian system for simplicity of computation and representation. Then a pose auxiliary network is constructed with an aim to estimate the pose changes in terms of rotation angles . Finally, a graph convolutional network is constructed for object classification against the pose variations of point cloud. The experimental results show the new model outperforms the existing approaches (such as PointNet and PointNet++) on the classification task when conducting experiments on both the ModelNet40 and the ShapeNetCore dataset with a series of random rotations of a 3D point cloud. Specifically, we obtain 73.02\% accuracy for classification task on the ModelNet40 with delaunay triangulation algorithm, which is much better than the state of the art algorithms, such as PointNet and PointCNN.},
  archive      = {J_PR},
  author       = {Huafeng Wang and Yaming Zhang and Wanquan Liu and Xianfeng Gu and Xin Jing and Zicheng Liu},
  doi          = {10.1016/j.patcog.2021.108251},
  journal      = {Pattern Recognition},
  pages        = {108251},
  shortjournal = {Pattern Recognition},
  title        = {A novel GCN-based point cloud classification model robust to pose variances},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preference prediction based on a photo gallery analysis with
scene recognition and object detection. <em>PR</em>, <em>121</em>,
108248. (<a href="https://doi.org/10.1016/j.patcog.2021.108248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a user modeling task is examined by processing mobile device gallery of photos and videos. We propose a novel engine for preferences prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered, and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep sophisticated neural networks . The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single descriptor in the neural attention unit. The proposed pipeline is implemented in mobile Android application . Experimental results for the Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion data demonstrate the possibility to efficiently process images without significant accuracy degradation.},
  archive      = {J_PR},
  author       = {A.V. Savchenko and K.V. Demochkin and I.S. Grechikhin},
  doi          = {10.1016/j.patcog.2021.108248},
  journal      = {Pattern Recognition},
  pages        = {108248},
  shortjournal = {Pattern Recognition},
  title        = {Preference prediction based on a photo gallery analysis with scene recognition and object detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards open-set touchless palmprint recognition via
weight-based meta metric learning. <em>PR</em>, <em>121</em>, 108247.
(<a href="https://doi.org/10.1016/j.patcog.2021.108247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Touchless biometrics has become significant in the wake of novel coronavirus 2019 (COVID-19). Due to the convenience, user-friendly, and high-accuracy, touchless palmprint recognition shows great potential when the hygiene issues are considered during COVID-19. However, previous palmprint recognition methods are mainly focused on close-set scenario. In this paper, a novel Weight-based Meta Metric Learning (W2ML) method is proposed for accurate open-set touchless palmprint recognition, where only a part of categories is seen during training. Deep metric learning-based feature extractor is learned in a meta way to improve the generalization ability. Multiple sets are sampled randomly to define support and query sets, which are further combined into meta sets to constrain the set-based distances. Particularly, hard sample mining and weighting are adopted to select informative meta sets to improve the efficiency. Finally, embeddings with obvious inter-class and intra-class differences are obtained as features for palmprint identification and verification. Experiments are conducted on four palmprint benchmarks including fourteen constrained and unconstrained palmprint datasets. The results show that our W2ML method is more robust and efficient in dealing with open-set palmprint recognition issue as compared to the state-of-the-arts, where the accuracy is increased by up to 9.11\% and the Equal Error Rate (EER) is decreased by up to 2.97\%.},
  archive      = {J_PR},
  author       = {Huikai Shao and Dexing Zhong},
  doi          = {10.1016/j.patcog.2021.108247},
  journal      = {Pattern Recognition},
  pages        = {108247},
  shortjournal = {Pattern Recognition},
  title        = {Towards open-set touchless palmprint recognition via weight-based meta metric learning},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pareto optimization of deep networks for COVID-19 diagnosis
from chest x-rays. <em>PR</em>, <em>121</em>, 108242. (<a
href="https://doi.org/10.1016/j.patcog.2021.108242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limitation, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of experiments on several publicly available datasets, accounting for more than 92,000 images, shows that the proposed approach provides average recognition rates up to 93.54\% when tested on external datasets.},
  archive      = {J_PR},
  author       = {Valerio Guarrasi and Natascha Claudia D’Amico and Rosa Sicilia and Ermanno Cordelli and Paolo Soda},
  doi          = {10.1016/j.patcog.2021.108242},
  journal      = {Pattern Recognition},
  pages        = {108242},
  shortjournal = {Pattern Recognition},
  title        = {Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning hybrid ranking representation for person
re-identification. <em>PR</em>, <em>121</em>, 108239. (<a
href="https://doi.org/10.1016/j.patcog.2021.108239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary person re-identification (re-id) methods mostly compute independently a feature representation of each person image in the query set and the gallery set. This strategy fails to consider any ranking context information of each probe image in the query set represented implicitly by the whole gallery set. Some recent re-ranking re-id methods therefore propose to take a post-processing strategy to exploit such contextual information for improving re-id matching performance. However, post-processing is independent of model training without jointly optimising the re-id feature and the ranking context information for better compatibility. In this work, for the first time, we show that the appearance feature and the ranking context information can be jointly optimised for learning more discriminative representations and achieving superior matching accuracy. Specifically, we propose to learn a hybrid ranking representation for person re-id with a two-stream architecture: (1) In the external stream, we use the ranking list of each probe image to learn plausible visual variations among the top ranks from the gallery as the external ranking information; (2) In the internal stream, we employ the part-based fine-grained feature as the internal ranking information, which mitigates the harm of incorrect matches in the ranking list. Assembling these two streams generates a hybrid ranking representation for person matching. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods on four large-scale re-id benchmarks (Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17), under both supervised and unsupervised settings.},
  archive      = {J_PR},
  author       = {Guile Wu and Xiatian Zhu and Shaogang Gong},
  doi          = {10.1016/j.patcog.2021.108239},
  journal      = {Pattern Recognition},
  pages        = {108239},
  shortjournal = {Pattern Recognition},
  title        = {Learning hybrid ranking representation for person re-identification},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universal multi-source domain adaptation for image
classification. <em>PR</em>, <em>121</em>, 108238. (<a
href="https://doi.org/10.1016/j.patcog.2021.108238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (DA) enables intelligent models to learn transferable knowledge from a labeled source domain and adapt to a similar but unlabeled target domain. Studies showed that knowledge could be transferred from one source domain to another unknown target domain, called Universal DA (UDA). However, there is often more than one source domain in the real-world application to be exploited for DA . In this paper, we formally propose a more general domain adaptation setting for image classification , universal multi-source DA (UMDA), where the label sets of multiple source domains can be different, and the label set of the target domain is completely unknown. The main challenge in UMDA is to identify the common label set among each source and target domain and keep the model scalable as the number of source domains increases. In the face of this challenge, we propose a universal multi-source adaptation network (UMAN) to solve the DA problem without increasing the complexity of the model in various UMDA settings. In UMAN, the reliability of each known class belonging to the common label set is estimated via a novel pseudo-margin vector and its weighted form, which helps adversarial training better align the distributions of multiple source domains and target domain. Moreover, the theoretical guarantee for UMAN is also provided. Massive experimental results show that existing UDA and multi-source DA (MDA) methods cannot be directly deployed to UMDA, and the proposed UMAN achieves the state-of-the-art performance in various UMDA settings.},
  archive      = {J_PR},
  author       = {Yueming Yin and Zhen Yang and Haifeng Hu and Xiaofu Wu},
  doi          = {10.1016/j.patcog.2021.108238},
  journal      = {Pattern Recognition},
  pages        = {108238},
  shortjournal = {Pattern Recognition},
  title        = {Universal multi-source domain adaptation for image classification},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep anomaly detection with self-supervised learning and
adversarial training. <em>PR</em>, <em>121</em>, 108234. (<a
href="https://doi.org/10.1016/j.patcog.2021.108234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep anomaly detection, which utilizes neural networks to discover anomalies, is a vital research topic in pattern recognition. With the burgeoning of inference mechanism, inference-based methods show the promising performance. However, inference-based methods have two limitations: (1) they use an adversarial training way to learn data features . Such training way fails to learn task-specific features which can be conducive to capture the difference between normal and anomaly data. (2) The structure of detection network cannot capture the marginal distributions of normal data and corresponding features, which influences on the performance of anomaly detection. To overcome these limitations, this paper proposes a deep adversarial anomaly detection (DAAD) method. Specifically, an auxiliary task with self-supervised learning is first designed to learn task-specific features. Then a deep adversarial training (DAT) model is constructed to capture marginal distributions of normal data in different spaces. In addition, a majority voting strategy is applied to obtain reliable detection results. Experimental results on image and sequence datasets show that proposed method performs significantly better than many strong baselines.},
  archive      = {J_PR},
  author       = {Xianchao Zhang and Jie Mu and Xiaotong Zhang and Han Liu and Linlin Zong and Yuangang Li},
  doi          = {10.1016/j.patcog.2021.108234},
  journal      = {Pattern Recognition},
  pages        = {108234},
  shortjournal = {Pattern Recognition},
  title        = {Deep anomaly detection with self-supervised learning and adversarial training},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal consistency-enhanced network for video
anomaly detection. <em>PR</em>, <em>121</em>, 108232. (<a
href="https://doi.org/10.1016/j.patcog.2021.108232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection aims to detect abnormal segments in a video sequence, which is a key problem in video surveillance. Based on deep prediction methods, we propose a spatiotemporal consistency-enhanced network to generate spatiotemporal consistency predictions. A 3D CNN-based encoder and 2D CNN-based decoder constitute the main part of our model. A resampling strategy is applied to the latent space vector when the model is trained by the normal data, yet this can cause the model to perform poorly if the data include abnormal data. Moreover, we combine an input clip with a generated frame into a reformed video clip, which is then fed into a discriminator that is constructed by the 3D CNN to evaluate the consistency of the input clip. Owing to the adversarial training between the generator and discriminator, the spatiotemporal consistency of the generated results is enhanced. During the testing stage, the abnormal data generates a different appearance and motion changes, which affect the ability of our model to predict spatiotemporal consistency in future images. Then, the prediction quality gap between normal and anomalous contents is used to infer whether anomalies occur. Extensive experiments confirm that the proposed method achieves state-of-the-art performance on three benchmark datasets, including ShanghaiTech, CUHK Avenue, and UCSD Ped2.},
  archive      = {J_PR},
  author       = {Yi Hao and Jie Li and Nannan Wang and Xiaoyu Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2021.108232},
  journal      = {Pattern Recognition},
  pages        = {108232},
  shortjournal = {Pattern Recognition},
  title        = {Spatiotemporal consistency-enhanced network for video anomaly detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiconNet: An edge-preserved connectivity-based approach for
salient object detection. <em>PR</em>, <em>121</em>, 108231. (<a
href="https://doi.org/10.1016/j.patcog.2021.108231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is viewed as a pixel-wise saliency modeling task by traditional deep learning-based methods. A limitation of current SOD models is insufficient utilization of inter-pixel information, which usually results in imperfect segmentation near edge regions and low spatial coherence. As we demonstrate, using a saliency mask as the only label is suboptimal. To address this limitation, we propose a connectivity-based approach called bilateral connectivity network (BiconNet), which uses connectivity masks together with saliency masks as labels for effective modeling of inter-pixel relationships and object saliency. Moreover, we propose a bilateral voting module to enhance the output connectivity map, and a novel edge feature enhancement method that efficiently utilizes edge-specific features. Through comprehensive experiments on five benchmark datasets, we demonstrate that our proposed method can be plugged into any existing state-of-the-art saliency-based SOD framework to improve its performance with negligible parameter increase.},
  archive      = {J_PR},
  author       = {Ziyun Yang and Somayyeh Soltanian-Zadeh and Sina Farsiu},
  doi          = {10.1016/j.patcog.2021.108231},
  journal      = {Pattern Recognition},
  pages        = {108231},
  shortjournal = {Pattern Recognition},
  title        = {BiconNet: An edge-preserved connectivity-based approach for salient object detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning discriminative region representation for person
retrieval. <em>PR</em>, <em>121</em>, 108229. (<a
href="https://doi.org/10.1016/j.patcog.2021.108229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-level representation learning plays a key role in providing discriminative information for person retrieval. Current methods rely on heuristically coarse-grained region strips or directly borrow pixel-level annotations from pretrained human parsing models for region representation learning . How to learn a discriminative region representation within fine-grained segments while avoiding expensive pixel-level annotations is rarely discussed. To that end, we introduce a novel identity-guided human region segmentation (HRS) method for person retrieval. Via learning a set of distinct region bases that are consistent across a given dataset, HRS can predict informative region segments by grouping intermediate feature vectors based on their similarity to these bases. The predicted segments are iteratively refined for discriminative region representation learning. HRS enjoys two advantages: (1) HRS learns region segmentation using only identity labels, making it a much more practical solution to person retrieval. (2) By jointly learning global appearance and local granularity cues, HRS enables a comprehensive feature representation learning. We verify the effectiveness of the proposed HRS on four challenging benchmark datasets of Market1501, DukeMTMC-reID, CUHK03, and Occluded-DukeMTMC. Extensive experiments demonstrate superior performance over the state-of-the-art region-based methods. For instance, on the CUHK03-labeled dataset, the performance increases from 74.1\% mAP and 76.5\% rank-1 accuracy to 81.5\% ( + 7.4\%) mAP and 83.2\% ( + 6.7\%) rank-1 accuracy.},
  archive      = {J_PR},
  author       = {Yang Zhao and Xiaohan Yu and Yongsheng Gao and Chunhua Shen},
  doi          = {10.1016/j.patcog.2021.108229},
  journal      = {Pattern Recognition},
  pages        = {108229},
  shortjournal = {Pattern Recognition},
  title        = {Learning discriminative region representation for person retrieval},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Why is this an anomaly? Explaining anomalies using
sequential explanations. <em>PR</em>, <em>121</em>, 108227. (<a
href="https://doi.org/10.1016/j.patcog.2021.108227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most applications, anomaly detection operates in an unsupervised mode by looking for outliers hoping that they are anomalies. Unfortunately, most anomaly detectors do not come with explanations about which features make a detected outlier point anomalous. Therefore, it requires human analysts to manually browse through each detected outlier point’s feature space to obtain the subset of features that will help them determine whether they are genuinely anomalous or not. This paper introduces sequential explanation (SE) methods that sequentially explain to the analyst which features make the detected outlier anomalous. We present two methods for computing SEs called the outlier and sample-based SE that will work alongside any anomaly detector. The outlier-based SE methods use an anomaly detector’s outlier scoring measure guided by a search algorithm to compute the SEs. Meanwhile, the sample-based SE methods employ sampling to turn the problem into a classical feature selection problem. In our experiments, we compare the performances of the different outlier- and sample-based SEs. Our results show that both the outlier and sample-based methods compute SEs that perform well and outperform sequential feature explanations.},
  archive      = {J_PR},
  author       = {Tshepiso Mokoena and Turgay Celik and Vukosi Marivate},
  doi          = {10.1016/j.patcog.2021.108227},
  journal      = {Pattern Recognition},
  pages        = {108227},
  shortjournal = {Pattern Recognition},
  title        = {Why is this an anomaly? explaining anomalies using sequential explanations},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable weight algorithm for convolutional neural networks
and its applications to classification of seizure phases and types.
<em>PR</em>, <em>121</em>, 108226. (<a
href="https://doi.org/10.1016/j.patcog.2021.108226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have recently achieved impressive results and raised expectations in the domains of medical diagnosis and physiological signal processing. The widely adopted methods include convolutional neural networks (CNNs) and recurrent neural networks (RNNs). However, the existing models possess static connection weights between layers, which might limit the generalization capability and the classification performance of the models as the weights of different layers are fixed after training. Furthermore, to deal with a large amount of data, a neural network with a sufficiently large size is required. This paper proposes the variable weight convolutional neural networks (VWCNNs), which are a type of network structure employing dynamic weights instead of static weights in their convolutional layers and fully-connected layers. VWCNNs are able to adapt to different characteristics of input data and can be viewed as an infinite number of traditional, fixed-weight CNNs. We will show that the proposed VWCNN structure outperforms the conventional CNN in terms of the classification accuracy , generalization capability, and robustness when the inputs are contaminated by noise. In this paper, VWCNNs are applied to the classification of three seizure phases (seizure-free, pre-seizure and seizure) based on measured electroencephalography (EEG) data. VWCNNs achieve 100\% test accuracy and show strong robustness in the classification of the three seizure phases, and thus show the potential to be a useful classification tool for medical diagnosis. Furthermore, the classification of seven types of seizures is investigated in this paper using the world’s largest open source database of seizure recordings, TUH EEG seizure corpus. Comparisons with conventional CNNs, RNN, MobileNet, ResNet , DenseNet and traditional machine learning methods including random forest , decision tree , support vector machine , K-nearest neighbours, standard neural networks, and Naïve Bayes are being conducted using realistic test data sets. The results demonstrate that VWCNNs have advantages over other classifiers in terms of classification accuracy and robustness.},
  archive      = {J_PR},
  author       = {Guangyu Jia and Hak-Keung Lam and Kaspar Althoefer},
  doi          = {10.1016/j.patcog.2021.108226},
  journal      = {Pattern Recognition},
  pages        = {108226},
  shortjournal = {Pattern Recognition},
  title        = {Variable weight algorithm for convolutional neural networks and its applications to classification of seizure phases and types},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blitz-SLAM: A semantic SLAM in dynamic environments.
<em>PR</em>, <em>121</em>, 108225. (<a
href="https://doi.org/10.1016/j.patcog.2021.108225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static environment is a prerequisite for most of visual simultaneous localization and mapping systems. Such a strong assumption limits the practical application of most existing SLAM systems. When moving objects enter the camera’s view field, dynamic matching points will directly interrupt the camera localization , and the noise blocks formed by moving objects will contaminate the constructed map. In this paper, a semantic SLAM system working in indoor dynamic environments named Blitz-SLAM is proposed. The noise blocks in the local point cloud are removed by combining the advantages of semantic and geometric information of mask, RGB and depth images. The global point cloud map can be obtained by merging the local point clouds. We evaluate Blitz-SLAM on the TUM RGB-D dataset and in the real-world environment. The experimental results demonstrate that Blitz-SLAM can work robustly in dynamic environments and generate a clean and accurate global point cloud map simultaneously.},
  archive      = {J_PR},
  author       = {Yingchun Fan and Qichi Zhang and Yuliang Tang and Shaofen Liu and Hong Han},
  doi          = {10.1016/j.patcog.2021.108225},
  journal      = {Pattern Recognition},
  pages        = {108225},
  shortjournal = {Pattern Recognition},
  title        = {Blitz-SLAM: A semantic SLAM in dynamic environments},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural networks-based relevant latent representation
learning for hyperspectral image classification. <em>PR</em>,
<em>121</em>, 108224. (<a
href="https://doi.org/10.1016/j.patcog.2021.108224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of hyperspectral image is a challenging task due to the high dimensional space, with large number of spectral bands , and low number of labeled training samples. To overcome these challenges, we propose a novel methodology for hyperspectral image classification based on multi-view deep neural networks which fuses both spectral and spatial features by using only a small number of labeled samples. Firstly, we process the initial hyperspectral image in order to extract a set of spectral and spatial features . Each spectral vector is the spectral signature of each pixel of the image. The spatial features are extracted using a simple deep autoencoder , which seeks to reduce the high dimensionality of data taking into account the neighborhood region for each pixel. Secondly, we propose a multi-view deep autoencoder model which allows fusing the spectral and spatial features extracted from the hyperspectral image into a joint latent representation space. Finally, a semi-supervised graph convolutional network is trained based on thee fused latent representation space to perform the hyperspectral image classification. The main advantage of the proposed approach is to allow the automatic extraction of relevant information while preserving the spatial and spectral features of data, and improve the classification of hyperspectral images even when the number of labeled samples is low. Experiments are conducted on three real hyperspectral images respectively Indian Pines, Salinas, and Pavia University datasets. Results show that the proposed approach is competitive in classification performances compared to state-of-the-art.},
  archive      = {J_PR},
  author       = {Akrem Sellami and Salvatore Tabbone},
  doi          = {10.1016/j.patcog.2021.108224},
  journal      = {Pattern Recognition},
  pages        = {108224},
  shortjournal = {Pattern Recognition},
  title        = {Deep neural networks-based relevant latent representation learning for hyperspectral image classification},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object-based cluster validation with densities. <em>PR</em>,
<em>121</em>, 108223. (<a
href="https://doi.org/10.1016/j.patcog.2021.108223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering validity indices are typically used as tools to find the correct number of clusters in a data set and/or to evaluate the quality of the clusters formed by clustering algorithms . Clustering validity indices measure separation and compactness of clusters. Typically, when applying a clustering algorithm, the input includes the number of clusters. After applying the algorithm with several different numbers of clusters, we determine the number of clusters to be the one with the best validity index. There are two types of clustering validity indices: external indices that are supervised, and internal indices that are unsupervised. The focus of this paper is on internal validity indices. Some existing internal validity indices capture the properties of the clusters by using representative statistics such as mean, variance, diameter, etc., however, these do not perform well when clusters have arbitrary shapes. One approach to overcome this issue is to use the density of the data objects in each cluster. That provides the advantage of capturing the full characteristics of the cluster which is most beneficial when there are clusters with arbitrary shapes. In the literature, a few density-based clustering validity indices have been proposed. However, some of them show poor performance when the clusters are not perfectly separated. Some others perform poorly because they use only representative objects from each cluster instead of all objects. The contribution of this paper is an internal validity index named the object-based clustering validity index with densities (OCVD). OCVD is a single number that averages the density-based contribution of individual data objects to both separation and compactness of clusters. The methodology behind calculating the density-based contributions of the objects is kernel density estimation. We show through several experiments that OCVD performs well in detecting the correct number of clusters in data sets with different cluster shapes including arbitrary shapes.},
  archive      = {J_PR},
  author       = {Behnam Tavakkol and Jeongsub Choi and Myong Kee Jeong and Susan L. Albin},
  doi          = {10.1016/j.patcog.2021.108223},
  journal      = {Pattern Recognition},
  pages        = {108223},
  shortjournal = {Pattern Recognition},
  title        = {Object-based cluster validation with densities},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The residual generator: An improved divergence minimization
framework for GAN. <em>PR</em>, <em>121</em>, 108222. (<a
href="https://doi.org/10.1016/j.patcog.2021.108222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GAN is a generative modelling framework which has been proven as able to minimise various types of divergence measures under an optimal discriminator . However, there is a gap between the loss function of GAN used in theory and in practice. In theory, the proof of the Jensen divergence minimisation involves the min-max criterion, but in practice the non-saturating criterion is instead used to avoid gradient vanishing. We argue that the formulation of divergence minimization via GAN is biased and may yield a poor convergence of the algorithm. In this paper, we propose the Residual Generator for GAN (Rg-GAN), which is inspired by the closed-loop control theory, to bridge the gap between theory and practice. Rg-GAN minimizes the residual between the loss of the generated data to be real and the loss of the generated data to be fake from the perspective of the discriminator. In this setting, the loss terms of the generator depend only on the generated data and therefore contribute to the optimisation of the model. We formulate the residual generator for standard GAN and least-squares GAN and show that they are equivalent to the minimisation of reverse-KL divergence and a novel instance of f-divergence, respectively. Furthermore, we prove that Rg-GAN can be reduced to Integral Probability Metrics (IPMs) GANs (e.g., Wasserstein GAN) and bridge the gap between IPMs and f-divergence. Additionally, we further improve on Rg-GAN by proposing a loss function for the discriminator that has a better discrimination ability. Experiments on synthetic and natural images data sets show that Rg-GAN is robust to mode collapse, and improves the generation quality of GAN in terms of FID and IS scores.},
  archive      = {J_PR},
  author       = {Aurele Tohokantche Gnanha and Wenming Cao and Xudong Mao and Si Wu and Hau-San Wong and Qing Li},
  doi          = {10.1016/j.patcog.2021.108222},
  journal      = {Pattern Recognition},
  pages        = {108222},
  shortjournal = {Pattern Recognition},
  title        = {The residual generator: An improved divergence minimization framework for GAN},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning scale awareness in keypoint extraction and
description. <em>PR</em>, <em>121</em>, 108221. (<a
href="https://doi.org/10.1016/j.patcog.2021.108221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To recover relative camera motion accurately and robustly, establishing a set of point-to-point correspondences in the pixel space is an essential yet challenging task in computer vision . Even though multi-scale design philosophy has been used with significant success in computer vision tasks, such as object detection and semantic segmentation , learning-based image matching has not been fully exploited. In this work, we explore a scale awareness learning approach in finding pixel-level correspondences based on the intuition that keypoints need to be extracted and described on an appropriate scale. With that insight, we propose a novel scale-aware network and then develop a new fusion scheme that derives high-consistency response maps and high-precision descriptions. We also revise the Second Order Similarity Regularization (SOSR) to make it more effective for the end-to-end image matching network, which leads to significant improvement in local feature descriptions. Experimental results run on multiple datasets demonstrate that our approach performs better than state-of-the-art methods under multiple criteria.},
  archive      = {J_PR},
  author       = {Xuelun Shen and Cheng Wang and Xin Li and Yifan Peng and Zijian He and Chenglu Wen and Ming Cheng},
  doi          = {10.1016/j.patcog.2021.108221},
  journal      = {Pattern Recognition},
  pages        = {108221},
  shortjournal = {Pattern Recognition},
  title        = {Learning scale awareness in keypoint extraction and description},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pedestrian attribute recognition: A survey. <em>PR</em>,
<em>121</em>, 108220. (<a
href="https://doi.org/10.1016/j.patcog.2021.108220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian Attribute Recognition (PAR) is an important task in computer vision community and plays an important role in practical video surveillance. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition, including the fundamental concepts and formulation of pedestrian attributes and corresponding challenges. Secondly, we analyze popular solutions for this task from eight perspectives. Thirdly, we discuss the specific attribute recognition, then, give a comparison between deep learning and traditional algorithm based PAR methods. After that, we show the connections between PAR and other computer vision tasks. Fourthly, we introduce the benchmark datasets, evaluation metrics in this community, and give a brief performance comparison. Finally, we summarize this paper and give several possible research directions for PAR. The project page of this paper can be found at: https://sites.google.com/view/ahu-pedestrianattributes/ .},
  archive      = {J_PR},
  author       = {Xiao Wang and Shaofei Zheng and Rui Yang and Aihua Zheng and Zhe Chen and Jin Tang and Bin Luo},
  doi          = {10.1016/j.patcog.2021.108220},
  journal      = {Pattern Recognition},
  pages        = {108220},
  shortjournal = {Pattern Recognition},
  title        = {Pedestrian attribute recognition: A survey},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning sequentially diversified representations for
fine-grained categorization. <em>PR</em>, <em>121</em>, 108219. (<a
href="https://doi.org/10.1016/j.patcog.2021.108219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning representation carrying rich local information is essential for recognizing fine-grained objects. Existing methods to this task resort to multi-stage frameworks to capture fine-grained information. However, they usually require multiple forward passes of the backbone network , resulting in efficiency deterioration. In this paper, we propose Sequentially Diversified Networks (SDNs) that enrich representation by promoting their diversity while maintaining the extraction efficiency. Specifically, we construct multiple lightweight sub-networks to model mutually different scales of discriminative patterns. The design of these sub-networks follows the sequentially diversified constraint, encouraging them to be varied in spatial attention . By inserting these sub-networks into a single backbone network , SDNs enable information interaction among local regions of the fine-grained image. In this way, SDNs jointly promote diversity in terms of scale and spatial attention in the one-stage pipeline, thereby facilitating the learning of diversified representation efficiently. We evaluate our proposed method on three challenging datasets, namely CUB-200-2011, Stanford-Cars, and FGVC-Aircraft. Experiments demonstrate its effectiveness in learning diversified information. Moreover, our method achieves state-of-the-art performance, only requiring a single forward pass of the backbone network, which reduces inference time noticeably.},
  archive      = {J_PR},
  author       = {Lianbo Zhang and Shaoli Huang and Wei Liu},
  doi          = {10.1016/j.patcog.2021.108219},
  journal      = {Pattern Recognition},
  pages        = {108219},
  shortjournal = {Pattern Recognition},
  title        = {Learning sequentially diversified representations for fine-grained categorization},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Financial time series forecasting with multi-modality graph
neural network. <em>PR</em>, <em>121</em>, 108218. (<a
href="https://doi.org/10.1016/j.patcog.2021.108218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series analysis plays a central role in hedging market risks and optimizing investment decisions. This is a challenging task as the problems are always accompanied by multi-modality streams and lead-lag effects. For example, the price movements of stock are reflections of complicated market states in different diffusion speeds, including historical price series, media news, associated events, etc. Furthermore, the financial industry requires forecasting models to be interpretable and compliant. Therefore, in this paper, we propose a multi-modality graph neural network (MAGNN) to learn from these multimodal inputs for financial time series prediction. The heterogeneous graph network is constructed by the sources as nodes and relations in our financial knowledge graph as edges. To ensure the model interpretability , we leverage a two-phase attention mechanism for joint optimization, allowing end-users to investigate the importance of inner-modality and inter-modality sources. Extensive experiments on real-world datasets demonstrate the superior performance of MAGNN in financial market prediction. Our method provides investors with a profitable as well as interpretable option and enables them to make informed investment decisions.},
  archive      = {J_PR},
  author       = {Dawei Cheng and Fangzhou Yang and Sheng Xiang and Jin Liu},
  doi          = {10.1016/j.patcog.2021.108218},
  journal      = {Pattern Recognition},
  pages        = {108218},
  shortjournal = {Pattern Recognition},
  title        = {Financial time series forecasting with multi-modality graph neural network},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint multi-label learning and feature extraction for
temporal link prediction. <em>PR</em>, <em>121</em>, 108216. (<a
href="https://doi.org/10.1016/j.patcog.2021.108216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks derived from various disciplinary of sociality and nature are dynamic and incomplete, and temporal link prediction has wide applications in recommendation system and data mining system , etc. The current algorithms first obtain features by exploiting the topological or latent structure of networks, and then predict temporal links based on the obtained features. These algorithms are criticized by the separation of feature extraction and link prediction, which fails to fully characterize the dynamics of networks, resulting in undesirable performance. To overcome this problem, we propose a novel algorithm by joint multi-label learning and feature extraction (called MLjFE ), where temporal link prediction and feature extraction are integrated into an overall objective function. The main advantage of MLjFE is that the features and parameter matrix for temporal link prediction are simultaneously learned during optimization procedure , which is more precise to capture dynamics of networks, improving the performance of algorithms. The experimental results on a number of artificial and real-world temporal networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods, showing joint learning with feature extraction and temporal link prediction is promising.},
  archive      = {J_PR},
  author       = {Xiaoke Ma and Shiyin Tan and Xianghua Xie and Xiaoxiong Zhong and Jingjing Deng},
  doi          = {10.1016/j.patcog.2021.108216},
  journal      = {Pattern Recognition},
  pages        = {108216},
  shortjournal = {Pattern Recognition},
  title        = {Joint multi-label learning and feature extraction for temporal link prediction},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolutional autoencoders with co-learning of graph
structure and node attributes. <em>PR</em>, <em>121</em>, 108215. (<a
href="https://doi.org/10.1016/j.patcog.2021.108215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph representation learning based on autoencoders has received much attention. However, these methods suffer from two limitations. First, most graph autoencoders ignore the reconstruction of either the graph structure or the node attributes, which often leads to a poor latent representation of the graph-structured data. Second, for existing graph autoencoders models, the encoder and decoder are mainly composed of an initial graph convolutional network (GCN) or its variants. These traditional GCN-based graph autoencoders more or less encounter the problem of incomplete filtering, which causes these models to be unstable in practical applications. To address the above issues, this paper proposes the Graph convolutional Autoencoders with co-learning of graph Structure and Node attributes (GASN) based on variational autoencoders. Specifically, the proposed GASN encodes and decodes the node attributes and graph structure comprehensively in the graph-structured data. Furthermore, we design a completely low-pass graph encoder and a high-pass graph decoder. The experimental results on real-world datasets demonstrate that the proposed GASN achieves state-of-the-art performance on node clustering, link prediction, and visualization tasks.},
  archive      = {J_PR},
  author       = {Jie Wang and Jiye Liang and Kaixuan Yao and Jianqing Liang and Dianhui Wang},
  doi          = {10.1016/j.patcog.2021.108215},
  journal      = {Pattern Recognition},
  pages        = {108215},
  shortjournal = {Pattern Recognition},
  title        = {Graph convolutional autoencoders with co-learning of graph structure and node attributes},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recursive multi-model complementary deep fusion for robust
salient object detection via parallel sub-networks. <em>PR</em>,
<em>121</em>, 108212. (<a
href="https://doi.org/10.1016/j.patcog.2021.108212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck . In sharp contrast to the conventional “deeper” schemes, this paper proposes a “wider” network architecture which consists of parallel sub-networks with totally different network architectures. In this way, those deep features obtained via these two sub-networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub-networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.},
  archive      = {J_PR},
  author       = {Zhenyu Wu and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
  doi          = {10.1016/j.patcog.2021.108212},
  journal      = {Pattern Recognition},
  pages        = {108212},
  shortjournal = {Pattern Recognition},
  title        = {Recursive multi-model complementary deep fusion for robust salient object detection via parallel sub-networks},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep tree-ensembles for multi-output prediction.
<em>PR</em>, <em>121</em>, 108211. (<a
href="https://doi.org/10.1016/j.patcog.2021.108211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure , some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks , leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.},
  archive      = {J_PR},
  author       = {Felipe Kenji Nakano and Konstantinos Pliakos and Celine Vens},
  doi          = {10.1016/j.patcog.2021.108211},
  journal      = {Pattern Recognition},
  pages        = {108211},
  shortjournal = {Pattern Recognition},
  title        = {Deep tree-ensembles for multi-output prediction},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Head pose estimation using deep neural networks and 3D point
clouds. <em>PR</em>, <em>121</em>, 108210. (<a
href="https://doi.org/10.1016/j.patcog.2021.108210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose head pose estimation using deep neural networks and 3D point cloud. Unlike existing methods that either take 2D RGB image or 2D depth image as input, we adopt 3D point cloud data generated from depth to estimate 3D head poses. To further improve robustness and accuracy of head pose estimation, we classify 3D angles of head poses into 36 classes with 5 ∘ ∘ interval and predict the probability of each angle in a class based on multi-layer perceptron (MLP). While traditional iterative methods for head model construction require high computation and memory costs, the proposed method is lightweight and computationally efficient by utilizing a sampled 3D point cloud as input combined with a graph convolutional neural network (GCNN). Experimental results on Biwi Kinect Head Pose dataset show that the proposed method achieves outstanding performance in head pose estimation and outperforms state-of-the-art ones in terms of accuracy.},
  archive      = {J_PR},
  author       = {Yuanquan Xu and Cheolkon Jung and Yakun Chang},
  doi          = {10.1016/j.patcog.2021.108210},
  journal      = {Pattern Recognition},
  pages        = {108210},
  shortjournal = {Pattern Recognition},
  title        = {Head pose estimation using deep neural networks and 3D point clouds},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAR-to-optical image translation based on improved CGAN.
<em>PR</em>, <em>121</em>, 108208. (<a
href="https://doi.org/10.1016/j.patcog.2021.108208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SAR images have the advantages of being less susceptible to clouds and light, while optical images conform to the human vision system. Both of them are widely applied in the field of scene classification, natural environment monitoring, disaster warning, etc. However, due to the speckle noise caused by the SAR imaging principle, it is difficult for people to distinguish the ground objects from complex background without professional knowledge. One commonly used solution is to exploit Generative Adversarial Networks (GAN) to translate SAR images to optical images which is able to clearly present ground objects with rich color information, i.e., SAR-to-optical image translation. Traditional GAN-based translation methods are apt to cause blurring of contour, disappearance of texture and inconsistency of color. To this end, we propose an improved conditional GAN (ICGAN) method. Compared with the basic CGAN model, the translation ability of our method is improved in the following three aspects. (1) Contour sharpness. We utilize the parallel branches to combine low-level and high-level features, and thus the image contour information is improved without the influence of noise. (2) Texture fine-grainedness. We discriminate the image using multi-scale receptive fields to enrich the local and global texture features of the image. (3) Color fidelity . We use the chromatic aberration loss which is based on Gaussian blur convolution to reduce the color gap between the generated image and the real optical image. Our method considers both the visual layer and the conceptual layer of the image to complete the SAR-to-optical image translation task. The model is able to preserve the contours and textures of the SAR image, while more closely approximates the colors of the ground truth. The experimental results show that the generated image not only has preferable results in visual effects and favorable evaluation metrics (subjective and objective), but also achieves outstanding classification accuracy , which proves the superiority of our method over the state-of-the-arts in the SAR-to-optical image translation task.},
  archive      = {J_PR},
  author       = {Xi Yang and Jingyi Zhao and Ziyu Wei and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2021.108208},
  journal      = {Pattern Recognition},
  pages        = {108208},
  shortjournal = {Pattern Recognition},
  title        = {SAR-to-optical image translation based on improved CGAN},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud based scalable object recognition from video streams
using orientation fusion and convolutional neural networks. <em>PR</em>,
<em>121</em>, 108207. (<a
href="https://doi.org/10.1016/j.patcog.2021.108207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93\%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97\%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are three most commonly used deep learning models for visual object recognition and classification.},
  archive      = {J_PR},
  author       = {Muhammad Usman Yaseen and Ashiq Anjum and Giancarlo Fortino and Antonio Liotta and Amir Hussain},
  doi          = {10.1016/j.patcog.2021.108207},
  journal      = {Pattern Recognition},
  pages        = {108207},
  shortjournal = {Pattern Recognition},
  title        = {Cloud based scalable object recognition from video streams using orientation fusion and convolutional neural networks},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised student-teacher learning for single image
super-resolution. <em>PR</em>, <em>121</em>, 108206. (<a
href="https://doi.org/10.1016/j.patcog.2021.108206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing approaches for single image super-resolution (SISR) resort to quality low-high resolution (LR-HR) pairs and available degradation kernels to train networks for a specific task in hand in a fully supervised manner. Labeled data used for training are, however, usually limited in terms of the quantity and the diversity degradation kernels. The learned SR networks with one degradation kernel ( e.g ., bicubic) do not generalize well and their performance sharply deteriorates on other kernels ( e.g ., blurred or noise). In this paper, we address the critical challenge for SISR: limited labeled LR images and degradation kernels. We propose a novel S emi-supervised S tudent- T eacher S uper- R esolution approach called S 2 2 TSR that super-resolves both labelled and unlabeled LR images via adversarial learning. To better exploit the information from labeled LR images, we propose a student-teacher framework (S-T) via knowledge transfer from supervised learning (T) to unsupervised learning (S). Specifically, the S-T knowledge transfer is based on a shared SR network, partial weight sharing of dual discriminators , and a pair matching network which also plays as a ‘latent discriminator’. Lastly, to learn better features from the limited labeled LR images, we propose a new SR network via non-local and attention mechanisms . Experiments demonstrate that our approach substantially improves unsupervised methods and performs favorably over fully supervised methods.},
  archive      = {J_PR},
  author       = {Lin Wang and Kuk-Jin Yoon},
  doi          = {10.1016/j.patcog.2021.108206},
  journal      = {Pattern Recognition},
  pages        = {108206},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised student-teacher learning for single image super-resolution},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking more than 100 arbitrary objects at 25 FPS through
deep learning. <em>PR</em>, <em>121</em>, 108205. (<a
href="https://doi.org/10.1016/j.patcog.2021.108205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.},
  archive      = {J_PR},
  author       = {Lorenzo Vaquero and Víctor M. Brea and Manuel Mucientes},
  doi          = {10.1016/j.patcog.2021.108205},
  journal      = {Pattern Recognition},
  pages        = {108205},
  shortjournal = {Pattern Recognition},
  title        = {Tracking more than 100 arbitrary objects at 25 FPS through deep learning},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating fund-raising performance for start-up projects
from a market graph perspective. <em>PR</em>, <em>121</em>, 108204. (<a
href="https://doi.org/10.1016/j.patcog.2021.108204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the online innovation market, the fund-raising performance of the start-up project is a concerning issue for creators, investors and platforms. Unfortunately, existing studies always focus on modeling the fund-raising process after the publishment of a project but the predicting of a project attraction in the market before setting up is largely unexploited. Usually, this prediction is always with great challenges to making a comprehensive understanding of both the start-up project and market environment. To that end, in this paper, we present a focused study on this important problem from a market graph perspective. Specifically, we propose a Graph-based Market Environment (GME) model for predicting the fund-raising performance of the unpublished project by exploiting the market environment. In addition, we discriminatively model the project competitiveness and market preferences by designing two graph-based neural network architectures and incorporating them into a joint optimization stage . Furthermore, to explore the information propagation problem with dynamic environment in a large-scale market graph, we extend the GME model with parallelizing competitiveness quantification and hierarchical propagation algorithm . Finally, we conduct extensive experiments on real-world data. The experimental results clearly demonstrate the effectiveness of our proposed model.},
  archive      = {J_PR},
  author       = {Likang Wu and Zhi Li and Hongke Zhao and Qi Liu and Enhong Chen},
  doi          = {10.1016/j.patcog.2021.108204},
  journal      = {Pattern Recognition},
  pages        = {108204},
  shortjournal = {Pattern Recognition},
  title        = {Estimating fund-raising performance for start-up projects from a market graph perspective},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph variational auto-encoder for deriving EEG-based graph
embedding. <em>PR</em>, <em>121</em>, 108202. (<a
href="https://doi.org/10.1016/j.patcog.2021.108202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding is an effective method for deriving low-dimensional representations of graph data. The power of graph deep learning methods to characterize electroencephalogram (EEG) graph embedding is still in question. We designed a novel graph variational auto-encoder (GVAE) method to extract nodal features of brain functional connections. A new decoder model for the GVAEs network is proposed, which considers the node neighborhood of the reconstructed adjacency matrix . The GVAE is applied and tested on 3 biometric databases which contain 64 to 9 channels’ EEG recordings. For all datasets, promising results with more than 95\% accuracy and considerably low computational cost are achieved compared to state-of-the-art user identification methods. The proposed GVAE is robust to a limited number of nodes and stable to users’ task performance. Moreover, we developed a traditional variational auto-encoder to demonstrate that more accurate features can be obtained when observing EEG-based brain connectivity from a graph perspective.},
  archive      = {J_PR},
  author       = {Tina Behrouzi and Dimitrios Hatzinakos},
  doi          = {10.1016/j.patcog.2021.108202},
  journal      = {Pattern Recognition},
  pages        = {108202},
  shortjournal = {Pattern Recognition},
  title        = {Graph variational auto-encoder for deriving EEG-based graph embedding},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel fuzzy clustering algorithm with variable multi-pixel
fitting spatial information for image segmentation. <em>PR</em>,
<em>121</em>, 108201. (<a
href="https://doi.org/10.1016/j.patcog.2021.108201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial information is often used to enhance the robustness of traditional fuzzy c-means (FCM) clustering algorithms. Although some recently emerged improvements are remarkable, the computational complexity of these algorithms is high, which may lead to lack of practicability. To address this problem, an efficient variant named the fuzzy clustering algorithm with variable multi-pixel fitting spatial information (FCM-VMF) is presented. First, a fuzzy clustering algorithm with multi-pixel fitting spatial information (FCM-MF) is developed. Specifically, by dividing the input image into several filter windows, the spatial information of all pixels in each filter window can be obtained simultaneously by fitting the pixels in its corresponding neighbourhood window, which enormously reduces the computational complexity . However, the FCM-MF may result in the loss of edge information. Therefore, the FCM-VMF integrates a variable window strategy with FCM-MF. In this strategy, to preserve more edge information, the sizes of the filter window and generalized neighbourhood window are adaptively reduced. The experimental results show that FCM-VMF is as effective as some recent algorithms. Notably, the FCM-VMF has extremely high efficiency, which means it has a better prospect of application.},
  archive      = {J_PR},
  author       = {Hang Zhang and Haili Li and Ning Chen and Shengfeng Chen and Jian Liu},
  doi          = {10.1016/j.patcog.2021.108201},
  journal      = {Pattern Recognition},
  pages        = {108201},
  shortjournal = {Pattern Recognition},
  title        = {Novel fuzzy clustering algorithm with variable multi-pixel fitting spatial information for image segmentation},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified learning approach for egocentric hand gesture
recognition and fingertip detection. <em>PR</em>, <em>121</em>, 108200.
(<a href="https://doi.org/10.1016/j.patcog.2021.108200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-mounted device-based human-computer interaction often requires egocentric recognition of hand gestures and fingertips detection. In this paper, a unified approach of egocentric hand gesture recognition and fingertip detection is introduced. The proposed algorithm uses a single convolutional neural network to predict the probabilities of finger class and positions of fingertips in one forward propagation . Instead of directly regressing the positions of fingertips from the fully connected layer, the ensemble of the position of fingertips is regressed from the fully convolutional network . Subsequently, the ensemble average is taken to regress the final position of fingertips. Since the whole pipeline uses a single network, it is significantly fast in computation. Experimental results show that the proposed method outperforms the existing fingertip detection approaches including the Direct Regression and the Heatmap-based framework. The effectiveness of the proposed method is also shown in-the-wild scenario as well as in a use-case of virtual reality.},
  archive      = {J_PR},
  author       = {Mohammad Mahmudul Alam and Mohammad Tariqul Islam and S.M. Mahbubur Rahman},
  doi          = {10.1016/j.patcog.2021.108200},
  journal      = {Pattern Recognition},
  pages        = {108200},
  shortjournal = {Pattern Recognition},
  title        = {Unified learning approach for egocentric hand gesture recognition and fingertip detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware co-supervision for accurate object detection.
<em>PR</em>, <em>121</em>, 108199. (<a
href="https://doi.org/10.1016/j.patcog.2021.108199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art object detection approaches are often composed of two stages, namely, proposing a number of regions on an image and classifying each of them into one class. Both stages share a network backbone which builds visual features in a bottom-up manner. In this paper, we advocate the importance of equipping two-stage detectors with top-down signals, in order to which provides high-level contextual cues to complement low-level features. In practice, this is implemented by adding a side path in the detection head to predict all object classes in the image, which is co-supervised by image-level semantics and requires little extra overheads. Our approach is easily applied to two popular object detection algorithms , and achieves consistent performance gain in the MS-COCO dataset.},
  archive      = {J_PR},
  author       = {Junran Peng and Haoquan Wang and Shaolong Yue and Zhaoxiang Zhang},
  doi          = {10.1016/j.patcog.2021.108199},
  journal      = {Pattern Recognition},
  pages        = {108199},
  shortjournal = {Pattern Recognition},
  title        = {Context-aware co-supervision for accurate object detection},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ANCES: A novel method to repair attribute noise in
classification problems. <em>PR</em>, <em>121</em>, 108198. (<a
href="https://doi.org/10.1016/j.patcog.2021.108198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise negatively affects the complexity and performance of models built in classification problems. The most common approach to mitigate its consequences is the usage of preprocessing techniques , known as noise filters, which are designed to remove noisy samples from the training data. Nevertheless, they are specifically oriented to deal with errors affecting class labels. Their employment may not always result in an improvement when noise affects attribute values. In these cases, correcting the errors is an interesting alternative to traditional noise filtering that has not been enough studied so far in the specialized literature. This research proposes an attribute noise correction method with the final aim of increasing the performance of the classification algorithms used later. The identification of noisy data is based on an error score assigned to each one of the attribute values in the dataset, which are then passed through an optimization process to correct their potential noise. The validity of the proposed method is studied in an exhaustive experimental study, in which it is compared to several well-known preprocessing methods to deal with noisy datasets. The results obtained show the suitability of attribute noise correction with respect to the other alternatives when data suffer from attribute noise.},
  archive      = {J_PR},
  author       = {José A. Sáez and Emilio Corchado},
  doi          = {10.1016/j.patcog.2021.108198},
  journal      = {Pattern Recognition},
  pages        = {108198},
  shortjournal = {Pattern Recognition},
  title        = {ANCES: A novel method to repair attribute noise in classification problems},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified hierarchical XGBoost model for classifying
priorities for COVID-19 vaccination campaign. <em>PR</em>, <em>121</em>,
108197. (<a href="https://doi.org/10.1016/j.patcog.2021.108197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current ML approaches do not fully focus to answer a still unresolved and topical challenge, namely the prediction of priorities of COVID-19 vaccine administration. Thus, our task includes some additional methodological challenges mainly related to avoiding unwanted bias while handling categorical and ordinal data with a highly imbalanced nature. Hence, the main contribution of this study is to propose a machine learning algorithm , namely Hierarchical Priority Classification eXtreme Gradient Boosting for priority classification for COVID-19 vaccine administration using the Italian Federation of General Practitioners dataset that contains Electronic Health Record data of 17k patients. We measured the effectiveness of the proposed methodology for classifying all the priority classes while demonstrating a significant improvement with respect to the state of the art. The proposed ML approach, which is integrated into a clinical decision support system , is currently supporting General Pracitioners in assigning COVID-19 vaccine administration priorities to their assistants.},
  archive      = {J_PR},
  author       = {Luca Romeo and Emanuele Frontoni},
  doi          = {10.1016/j.patcog.2021.108197},
  journal      = {Pattern Recognition},
  pages        = {108197},
  shortjournal = {Pattern Recognition},
  title        = {A unified hierarchical XGBoost model for classifying priorities for COVID-19 vaccination campaign},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent and diverse multi-view subspace clustering with
structure constraint. <em>PR</em>, <em>121</em>, 108196. (<a
href="https://doi.org/10.1016/j.patcog.2021.108196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering algorithms have recently been developed to process multi-view dataset clustering by accurately depicting the essential characteristics of multi-view data. Most existing methods focus on conduct self-representation property using a consistent representation and a set of specific representations with well-designed regularization to learn the common and specific knowledge among different views. However, specific representations only contain the unique information of each individual view, which limits their ability to fully excavate the diversity of multi-view data to enhance the complementarity among different views. Moreover, when conducting multi-view subspace clustering, the learned subspace self-representation and clustering are sequential and independent, which lacks consideration of the interaction between representation learning and the final clustering calculation. In this paper, a novel method termed consistent and diverse multi-view subspace clustering with structure constraint (CDMSC 2 2 ) is proposed to overcome the above-described deficiencies. (1) An exclusivity constraint term is employed to enhance the diversity of specific representations among different views for modeling consistency and diversity in a unified framework. (2) A clustering structure constraint is imposed on the subspace self-representation by factorizing the learned subspace self-representation into the cluster centroids and the cluster assignments with the goal of obtaining a clustering-oriented subspace self-representation. In addition, we carefully designed an efficient optimization algorithm to solve the objective function through relaxation and alternating minimization. Extensive experiments on five benchmark datasets in terms of six evaluation metrics demonstrate that our method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiaomeng Si and Qiyue Yin and Xiaojie Zhao and Li Yao},
  doi          = {10.1016/j.patcog.2021.108196},
  journal      = {Pattern Recognition},
  pages        = {108196},
  shortjournal = {Pattern Recognition},
  title        = {Consistent and diverse multi-view subspace clustering with structure constraint},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Indefinite twin support vector machine with DC functions
programming. <em>PR</em>, <em>121</em>, 108195. (<a
href="https://doi.org/10.1016/j.patcog.2021.108195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machine (TWSVM) is an efficient algorithm for binary classification . However, the lack of the structural risk minimization principle restrains the generalization of TWSVM and the guarantee of convex optimization constraints TWSVM to only use positive semi-definite kernels (PSD). In this paper, we propose a novel TWSVM for indefinite kernel called indefinite twin support vector machine with difference of convex functions programming (ITWSVM-DC). The indefinite TWSVM (ITWSVM) leverages a maximum margin regularization term to improve the generalization of TWSVM and a smooth quadratic hinge loss function to make the model continuously differentiable. The representer theorem is applied to the ITWSVM and the convexity of the ITWSVM is analyzed. In order to address the non-convex optimization problem when the kernel is indefinite, a difference of convex functions (DC) is used to decompose the non-convex objective function into the subtraction of two convex functions and a line search method is applied in the DC algorithm to accelerate the convergence rate. A theoretical analysis illustrates that ITWSVM-DC can converge to a local optimum and extensive experiments on indefinite and positive semi-definite kernels show the superiority of ITWSVM-DC.},
  archive      = {J_PR},
  author       = {Yuexuan An and Hui Xue},
  doi          = {10.1016/j.patcog.2021.108195},
  journal      = {Pattern Recognition},
  pages        = {108195},
  shortjournal = {Pattern Recognition},
  title        = {Indefinite twin support vector machine with DC functions programming},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards robust explanations for deep neural networks.
<em>PR</em>, <em>121</em>, 108194. (<a
href="https://doi.org/10.1016/j.patcog.2021.108194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks . But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.},
  archive      = {J_PR},
  author       = {Ann-Kathrin Dombrowski and Christopher J. Anders and Klaus-Robert Müller and Pan Kessel},
  doi          = {10.1016/j.patcog.2021.108194},
  journal      = {Pattern Recognition},
  pages        = {108194},
  shortjournal = {Pattern Recognition},
  title        = {Towards robust explanations for deep neural networks},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online aggregation of probability forecasts with confidence.
<em>PR</em>, <em>121</em>, 108193. (<a
href="https://doi.org/10.1016/j.patcog.2021.108193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents numerical experiments and some theoretical developments in prediction with expert advice (PEA). One experiment deals with predicting electricity consumption depending on temperature and uses real data. As the pattern of dependence can change with season and time of the day, the domain naturally admits PEA formulation with experts having different “areas of expertise”. We consider the case where several competing methods produce online predictions in the form of probability distribution functions . The dissimilarity between a probability forecast and an outcome is measured by a loss function (scoring rule). A popular example of scoring rule for continuous outcomes is Continuous Ranked Probability Score ( CRPS CRPS ). In this paper the problem of combining probabilistic forecasts is considered in the PEA framework. We show that CRPS CRPS is a mixable loss function and then the time-independent upper bound for the regret of the Vovk aggregating algorithm using CRPS CRPS as a loss function can be obtained. Also, we incorporate a “smooth” version of the method of specialized experts in this scheme which allows us to combine the probabilistic predictions of the specialized experts with overlapping domains of their competence.},
  archive      = {J_PR},
  author       = {Vladimir V’yugin and Vladimir Trunov},
  doi          = {10.1016/j.patcog.2021.108193},
  journal      = {Pattern Recognition},
  pages        = {108193},
  shortjournal = {Pattern Recognition},
  title        = {Online aggregation of probability forecasts with confidence},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Creating synthetic minority class samples based on
autoencoder extreme learning machine. <em>PR</em>, <em>121</em>, 108191.
(<a href="https://doi.org/10.1016/j.patcog.2021.108191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports a new method (simplified as AE-ELM-SynMin) to create the Syn thetic Min ority class samples for imbalanced classification based on A uto E ncoder E xtreme L earning M achine (AE-ELM). AE-ELM-SynMin first trains an AE-ELM which is a special ELM with the same input and output, i.e., the original minority class samples. Second, the crossover, mutation and filtration operations are conducted on the hidden-layer output of AE-ELM and then the synthetic hidden-layer output is obtained. Third, the synthetic minority class samples are created by decoding the synthetic hidden-layer output with output-layer weights of AE-ELM. AE-ELM-SynMin guarantees that the synthetic minority class has the higher information amount than original minority class and meanwhile keeps the consistent probability distribution with the original minority class. The experimental results demonstrate the better imbalanced classification performances of AE-ELM-SynMin in comparison with the regular synthetic minority over-sampling technique (Regular-SMOTE) and its variants, e.g., Borderline-SMOTE, Random-SMOTE, and SMOTE-IPF.},
  archive      = {J_PR},
  author       = {Yu-Lin He and Sheng-Sheng Xu and Joshua Zhexue Huang},
  doi          = {10.1016/j.patcog.2021.108191},
  journal      = {Pattern Recognition},
  pages        = {108191},
  shortjournal = {Pattern Recognition},
  title        = {Creating synthetic minority class samples based on autoencoder extreme learning machine},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained mutual convex cone method for image set based
recognition. <em>PR</em>, <em>121</em>, 108190. (<a
href="https://doi.org/10.1016/j.patcog.2021.108190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose convex cone-based frameworks for image-set classification. Image-set classification aims to classify a set of images, usually obtained from video frames or multi-view cameras, into a target object. To accurately and stably classify a set, it is essential to accurately represent structural information of the set. There are various image features , such as histogram-based features and convolutional neural network features. We should note that most of them have non-negativity and thus can be effectively represented by a convex cone. This leads us to introduce the convex cone representation to image-set classification. To establish a convex cone-based framework, we mathematically define multiple angles between two convex cones, and then use the angles to define the geometric similarity between them. Moreover, to enhance the framework, we introduce two discriminant spaces. We first propose a discriminant space that maximizes gaps between cones and minimizes the within-class variance. We then extend it to a weighted discriminant space by introducing weights on the gaps to deal with complicated data distribution. In addition, to reduce the computational cost of the proposed methods, we develop a novel strategy for fast implementation. The effectiveness of the proposed methods is demonstrated experimentally by using five databases.},
  archive      = {J_PR},
  author       = {Naoya Sogi and Rui Zhu and Jing-Hao Xue and Kazuhiro Fukui},
  doi          = {10.1016/j.patcog.2021.108190},
  journal      = {Pattern Recognition},
  pages        = {108190},
  shortjournal = {Pattern Recognition},
  title        = {Constrained mutual convex cone method for image set based recognition},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WC-KNNG-PC: Watershed clustering based on k-nearest-neighbor
graph and pauta criterion. <em>PR</em>, <em>121</em>, 108177. (<a
href="https://doi.org/10.1016/j.patcog.2021.108177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watershed clustering utilizes the concept of watershed algorithm to process clustering or cluster analyzes. The most attractive characteristic of this method is the capability to determine automatically the number of clusters from the data sets. However, in terms of the literature, the purposes of the original watershed clustering algorithm and the improved version are the detection of the clusters within two-dimensional linear data sets. In order to enable watershed clustering to deal with the dataset with multiple dimensions and nonlinear structures, we introduce k -nearest neighbor graph (KNNG), the shared nearest neighbor method and Pauta Criterion into watershed clustering to present a new watershed graph clustering with noise detection, WC-KNNG-PC. This approach first calculates a KNNG for the data sets, and then compute catchment basins (subclusters), basin immersions (connectivity between basins) and outliers. To prevent the merger of illegal subclusters , a maximum normalization stability factor, based on t -nearest neighbors and angle, MNSF, is proposed to detect the invalid basin immersions. Finally, a basin level similarity using median criterion is presented to merge the catchment basins to obtain the final clustering. Experiments on complex synthetic datasets and multidimensional real-world datasets have successfully demonstrated that the performance of the WC-KNNG-PC in clustering some various dimensional and complex datasets with heterogeneous density and diverse shapes.},
  archive      = {J_PR},
  author       = {Jianhua Xia and Jinbing Zhang and Yang Wang and Lixin Han and Hong Yan},
  doi          = {10.1016/j.patcog.2021.108177},
  journal      = {Pattern Recognition},
  pages        = {108177},
  shortjournal = {Pattern Recognition},
  title        = {WC-KNNG-PC: Watershed clustering based on k-nearest-neighbor graph and pauta criterion},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifelong robotic visual-tactile perception learning.
<em>PR</em>, <em>121</em>, 108176. (<a
href="https://doi.org/10.1016/j.patcog.2021.108176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong machine learning can learn a sequence of consecutive robotic perception tasks via transferring previous experiences. However, 1) most existing lifelong learning based perception methods only take advantage of visual information for robotic tasks, while neglecting another important tactile sensing modality to capture discriminative material properties; 2) Meanwhile, they cannot explore the intrinsic relationships across different modalities and the common characterization among different tasks of each modality, due to the distinct divergence between heterogeneous feature distributions. To address above challenges, we propose a new L ifelong V isual- T actile L earning (LVTL) model for continuous robotic visual-tactile perception tasks, which fully explores the latent correlations in both intra-modality and cross-modality aspects. Specifically, a modality-specific knowledge library is developed for each modality to explore common intra-modality representations across different tasks, while narrowing intra-modality mapping divergence between semantic and feature spaces via an auto-encoder mechanism. Moreover, a sparse constraint based modality-invariant space is constructed to capture underlying cross-modality correlations and identify the contributions of each modality for new coming visual-tactile tasks. We further propose a modality consistency regularizer to efficiently align the heterogeneous visual and tactile samples, which ensures the semantic consistency between different modality-specific knowledge libraries. After deriving an efficient model optimization strategy , we conduct extensive experiments on several representative datasets to demonstrate the superiority of our LVTL model. Evaluation experiments show that our proposed model significantly outperforms existing state-of-the-art methods with about 1.16\% ∼ ∼ 15.36\% improvement under different lifelong visual-tactile perception scenarios.},
  archive      = {J_PR},
  author       = {Jiahua Dong and Yang Cong and Gan Sun and Tao Zhang},
  doi          = {10.1016/j.patcog.2021.108176},
  journal      = {Pattern Recognition},
  pages        = {108176},
  shortjournal = {Pattern Recognition},
  title        = {Lifelong robotic visual-tactile perception learning},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action recognition via pose-based graph convolutional
networks with intermediate dense supervision. <em>PR</em>, <em>121</em>,
108170. (<a href="https://doi.org/10.1016/j.patcog.2021.108170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-based action recognition has drawn considerable attention recently. Existing methods exploit the joint position to extract body-part features from the activation maps of the backbone CNN to assist human action recognition . However, there are two limitations: (1) the body-part features are independently used or simply concatenated to obtain a representation, where the prior knowledge about the structured correlations between body parts are not fully exploited; (2) the backbone CNN, from which the body-part features are extracted, is “lazy”. It always contents itself with identifying patterns from the most discriminative areas of the input, which causes no information on the features extracted from other areas. This consequently hampers the performance of the followed aggregation process and makes the model easy to be misled by the training data bias. To address these problems, we encode the body-part features into a human-based spatiotemporal graph and employ a light-weight graph convolutional module to explicitly model the dependencies between body parts. Besides, we introduce a novel intermediate dense supervision to promote the backbone CNN to treat all regions equally, which is simple and effective, without extra parameters and computations. The proposed approach, namely, the pose-based graph convolutional network (PGCN), is evaluated on three popular benchmarks, where our approach significantly outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},
  doi          = {10.1016/j.patcog.2021.108170},
  journal      = {Pattern Recognition},
  pages        = {108170},
  shortjournal = {Pattern Recognition},
  title        = {Action recognition via pose-based graph convolutional networks with intermediate dense supervision},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GLMNet: Graph learning-matching convolutional networks for
feature matching. <em>PR</em>, <em>121</em>, 108167. (<a
href="https://doi.org/10.1016/j.patcog.2021.108167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional networks (GCNs) have been employed for graph matching problem. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. However, first, the matching graphs feeding to existing graph matching networks are generally fixed and independent of graph matching task, which thus are not guaranteed to be optimal for the graph matching task. Second, existing methods generally employ smoothing-based graph convolution to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learns a pair of optimal graphs for graph matching task. (2) It further employs a Laplacian sharpening graph convolution to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments demonstrate the effectiveness of GLMNet.},
  archive      = {J_PR},
  author       = {Bo Jiang and Pengfei Sun and Bin Luo},
  doi          = {10.1016/j.patcog.2021.108167},
  journal      = {Pattern Recognition},
  pages        = {108167},
  shortjournal = {Pattern Recognition},
  title        = {GLMNet: Graph learning-matching convolutional networks for feature matching},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delving deep into spatial pooling for squeeze-and-excitation
networks. <em>PR</em>, <em>121</em>, 108159. (<a
href="https://doi.org/10.1016/j.patcog.2021.108159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Squeeze-and-Excitation (SE) blocks have demonstrated significant accuracy gains for state-of-the-art deep architectures by re-weighting channel-wise feature responses. The SE block is an architecture unit that integrates two operations: a squeeze operation that employs global average pooling to aggregate spatial convolutional features into a channel feature, and an excitation operation that learns instance-specific channel weights from the squeezed feature to re-weight each channel. In this paper, we revisit the squeeze operation in SE blocks, and shed lights on why and how to embed rich (both global and local ) information into the excitation module at minimal extra costs. In particular, we introduce a simple but effective two-stage spatial pooling process: rich descriptor extraction and information fusion . The rich descriptor extraction step aims to obtain a set of diverse ( i.e ., global and especially local) deep descriptors that contain more informative cues than global average-pooling. While, absorbing more information delivered by these descriptors via a fusion step can aid the excitation operation to return more accurate re-weight scores in a data-driven manner. We validate the effectiveness of our method by extensive experiments on ImageNet for image classification and on MS-COCO for object detection and instance segmentation . For these experiments, our method achieves consistent improvements over the SENets on all tasks, in some cases, by a large margin.},
  archive      = {J_PR},
  author       = {Xin Jin and Yanping Xie and Xiu-Shen Wei and Bo-Rui Zhao and Zhao-Min Chen and Xiaoyang Tan},
  doi          = {10.1016/j.patcog.2021.108159},
  journal      = {Pattern Recognition},
  pages        = {108159},
  shortjournal = {Pattern Recognition},
  title        = {Delving deep into spatial pooling for squeeze-and-excitation networks},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transtrack: Online meta-transfer learning and otsu
segmentation enabled wireless gesture tracking. <em>PR</em>,
<em>121</em>, 108157. (<a
href="https://doi.org/10.1016/j.patcog.2021.108157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual diversity poses a cross-user performance variance challenge that stumbles the practicality, especially for the wireless gesture tracking systems. Since the difficulty of annotating low-semantic wireless data limits constructing a big dataset, the recognizer should quickly adjust to different individuals via small datasets. To this end, we present TransTrack, an accurate wireless indoor gesture tracking system that can adjust to different users quickly. The key insight is that each unlabeled gesture contains learnable individual features that can help the gesture tracking model learning how to adapt to different users. Specifically, TransTrack uses recursive Otsu segmentation to separate gesture-induced signals with the background noise inspired by image segmentation . It then augments training data to learn the transferable features by leveraging the redundant information. A datum-based alignment method is proposed to unlock the limitation of classifier selection without distortion. Finally, TransTrack proposes an online meta-transfer learning method that collects unlabeled data transparently to train the tracking model for different tasks. Extensive experiments show that TransTrack can quickly adapt to different users and conditions.},
  archive      = {J_PR},
  author       = {Jiang Xiao and Huichuwu Li and Hai Jin},
  doi          = {10.1016/j.patcog.2021.108157},
  journal      = {Pattern Recognition},
  pages        = {108157},
  shortjournal = {Pattern Recognition},
  title        = {Transtrack: Online meta-transfer learning and otsu segmentation enabled wireless gesture tracking},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixture factor analysis with distance metric constraint for
dimensionality reduction. <em>PR</em>, <em>121</em>, 108156. (<a
href="https://doi.org/10.1016/j.patcog.2021.108156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) is a key preprocessing stage in high-dimensional data classification. Traditional linear DR algorithms, e.g., Linear Discriminant Analysis, transform the original data into a low-dimensional subspace with a linear transformation matrix. However, these methods cannot handle complex nonlinearly separable data. Although some nonlinear DR methods, e.g., Locally Linear Embedding, are proposed to solve this problem, most of them are unsupervised, which only focus on the data structure hidden in the original high-dimensional space, rather than maximizing the inter-class separability of the transformed data, thus reducing the classification accuracy. To tackle this challenge, a novel supervised nonlinear DR algorithm, distance metric restricted mixture factor analysis (DMR-MFA), is proposed for high-dimensional data classification. In DMR-MFA, the original data is divided into several clusters, and the generation of original data in each cluster is described via a factor analysis model. Meanwhile, the distance metric constraint (DMC) is used for maximizing the separability of transformed low-dimensional data from different classes. Moreover, the optimal model parameters are learned via the joint optimization of log-likelihood function and DMC loss function, which makes the DMR-MFA possible to obtain the more separable low-dimensional embeddings while accurately describing the original data. Experimental results on synthetic data, benchmark datasets and high-resolution range profile data demonstrate that our method can handle nonlinearly separable data and improves the classification accuracy of data with high dimensionality.},
  archive      = {J_PR},
  author       = {Jian Chen and Leiyao Liao and Wei Zhang and Lan Du},
  doi          = {10.1016/j.patcog.2021.108156},
  journal      = {Pattern Recognition},
  pages        = {108156},
  shortjournal = {Pattern Recognition},
  title        = {Mixture factor analysis with distance metric constraint for dimensionality reduction},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 4D computed tomography super-resolution reconstruction based
on tensor product and nuclear norm optimization. <em>PR</em>,
<em>121</em>, 108150. (<a
href="https://doi.org/10.1016/j.patcog.2021.108150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Four-dimensional computed tomography (4D-CT) has been widely used in preoperative evaluation and radiotherapy planning of lung tumors. To reduce the damage to healthy tissue, it is a better way to limit the scan time and the number of CT slices. Yet, it leads to the reduction of CT image resolution in the superior-inferior direction. To improve the resolution of the 4D-CT image, we propose a super-resolution (SR) algorithm based on tensor product and nuclear norm optimization. The proposed cost function includes a tensor fidelity term and a nuclear norm regularization term. The tensor fidelity term consists of low-resolution (LR) and high-resolution (HR) image tensors, as well as SR operators. The nuclear norm regularization term is used to preserve the operators’ low-rank. The optimization problem can be effectively solved by an alternative direction method of the multipliers (ADMM) technique. The SR operators can extract useful information from each dimension of LR image tensors to enhance the equality of 4D-CT SR reconstruction. Experimental results show that the proposed method can preserve the edge details of the 4D-CT image. Moreover, quantitative comparisons show that the proposed method increases peak signal-to-noise ratio from 1.5 dB to 5.5 dB, structural similarity index from 2\% 2\% to 11\% , 11\%, visual information fidelity from 6\% 6\% to 20\% , 20\%, edge model-based blur metric from 5\% 5\% to 15\% , 15\%, and decreases the spatial-spectral entropy-based quality index from 1\% 1\% to 5\% , 5\%, compared with conventional 4D-CT SR algorithms.},
  archive      = {J_PR},
  author       = {Shu Zhang and Youshen Xia},
  doi          = {10.1016/j.patcog.2021.108150},
  journal      = {Pattern Recognition},
  pages        = {108150},
  shortjournal = {Pattern Recognition},
  title        = {4D computed tomography super-resolution reconstruction based on tensor product and nuclear norm optimization},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge computing enabled video segmentation for real-time
traffic monitoring in internet of vehicles. <em>PR</em>, <em>121</em>,
108146. (<a href="https://doi.org/10.1016/j.patcog.2021.108146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things enabled intelligent transportation systems , a huge amount of vehicle video data has been generated and real-time and accurate video analysis are very important and challenging work, especially in situations with complex street scenes. Therefore, we propose edge computing based video pre-processing to eliminate the redundant frames, so that we migrate the partial or all the video processing task to the edge, thereby diminishing the computing, storage and network bandwidth requirements of the cloud center, and enhancing the effectiveness of video analyzes. To eliminate the redundancy of the traffic video, the magnitude of motion detection based on spatio-temporal interest points (STIP) and the multi-modal linear features combination are presented which splits a video into super frame segments of interests. After that, we select the key frames from these interesting segments of the long videos with the design and detection of the prominent region. Finally, the extensive numerical experimental verification results show our methods are superior to the previous algorithms for different stages of the redundancy elimination , video segmentation, key frame selection and vehicle detection.},
  archive      = {J_PR},
  author       = {Shaohua Wan and Songtao Ding and Chen Chen},
  doi          = {10.1016/j.patcog.2021.108146},
  journal      = {Pattern Recognition},
  pages        = {108146},
  shortjournal = {Pattern Recognition},
  title        = {Edge computing enabled video segmentation for real-time traffic monitoring in internet of vehicles},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation-aware dynamic attributed graph attention network
for stocks recommendation. <em>PR</em>, <em>121</em>, 108119. (<a
href="https://doi.org/10.1016/j.patcog.2021.108119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent properties of the graph structure of the financial market and the correlation attributes that actually exist in the system inspire us to introduce the concept of the graph to solve the problem of prediction and recommendation in the financial sector. In this paper, we are adhering to the idea of recommending high return ratio stocks and put forward an attributed graph attention network model based on the correlation information, with encoded timing characteristics derived from time series module and global information originating from the stacked graph neural network(GNN) based models, which we called Relation-aware Dynamic Attributed Graph Attention Network (RA-AGAT). On this basis, we have verified the practicality and applicability of the application of graph models in finance. Our innovative structure first captures the local correlation topology information and then introduce a stacked graph neural network structure to recommend Top-N return ratio of stock items. Experiments on the real China A-share market demonstrate that the RA-AGAT architecture is capable of surpassing the previously applicable methods in the prediction and recommendation of stock return ratio.},
  archive      = {J_PR},
  author       = {Shibo Feng and Chen Xu and Yu Zuo and Guo Chen and Fan Lin and Jianbing XiaHou},
  doi          = {10.1016/j.patcog.2021.108119},
  journal      = {Pattern Recognition},
  pages        = {108119},
  shortjournal = {Pattern Recognition},
  title        = {Relation-aware dynamic attributed graph attention network for stocks recommendation},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-dimensional clustering through fusion of high-order
similarities. <em>PR</em>, <em>121</em>, 108108. (<a
href="https://doi.org/10.1016/j.patcog.2021.108108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering objects with heterogeneous attributes captured from different dimensions remains challenging in integrating the multiple dimensional information. Most of the current multi-dimensional clustering models pin on direct sample-wised similarity and fail to exploit hidden mutual affinity among different sampling spaces. Thus, it is hard to capture a legible cluster structure. To tackle this issue, we propose a H igh-order multi-dimensional S pectral C lustering method (HSC). The proposed HSC aims to learn a high-order similarity to characterize the intrinsic relationship among different dimensional spaces instead of the ordinary similarity. It then performs a clustering task within a latent space by jointly learning the high-order similarity and ordinary similarity. Extensive experiments over synthetic and real-world data sets show that the proposed HSC outperforms benchmark multi-dimensional methods in most scenarios and is capable of revealing a reliable structure concealed across multi-dimensional spaces.},
  archive      = {J_PR},
  author       = {Hong Peng and Haiyan Wang and Yu Hu and Weiwei Zhou and Hongmin Cai},
  doi          = {10.1016/j.patcog.2021.108108},
  journal      = {Pattern Recognition},
  pages        = {108108},
  shortjournal = {Pattern Recognition},
  title        = {Multi-dimensional clustering through fusion of high-order similarities},
  volume       = {121},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
