<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---63">DKE - 63</h2>
<ul>
<li><details>
<summary>
(2022). PROADAPT: Proactive framework for adaptive partitioning for
big data warehouses. <em>DKE</em>, <em>142</em>, 102102. (<a
href="https://doi.org/10.1016/j.datak.2022.102102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel DBMSs have become more and more mature and getting several success stories in the industry. This situation has been reached by powerful data partitioning and data allocation techniques and algorithms. By analyzing these findings closely, we figure out that they are quickly stressed by the workload changes, which represents the usual case of business analytics applications. To deal with this challenge in the context of big data warehouses , several studies proposed to move to another processing paradigm outside the DBMS realm such as Spark by proposing adaptive partitioning solutions to tackle the workload changes. The majority of approaches are offline and those that are online cause significant random disk I/O costs. This is because the correlation that may exist between jobs and data blocks read from the disk is not captured to refine the adaptive partitioning algorithms. This represents one of the major causes of providing high performance of dynamic workloads . To solve such limitations, we propose in this paper a proactive framework for query-aware adaptive partitioning (called PROADAPT) that uses an AI-inspired methodology that can be connected to any query optimizer managing partitioned data. This methodology uses a genetic algorithm to solve our formalized problem that considers the interaction that may exist among workload queries . PROADAPT intensively rewrites queries by exploiting dimension hierarchies to skip irrelevant data and then improves I/O performance. Different technical modules of our framework are discussed. Finally, we conduct intensive experiments on Postgres-XL and a Spark SQL parallel cluster to show the effectiveness and efficiency of our approach.},
  archive      = {J_DKE},
  author       = {Soumia Benkrid and Ladjel Bellatreche and Yacine Mestoui and Carlos Ordonez},
  doi          = {10.1016/j.datak.2022.102102},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102102},
  shortjournal = {Data Knowl. Eng.},
  title        = {PROADAPT: Proactive framework for adaptive partitioning for big data warehouses},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preface. <em>DKE</em>, <em>142</em>, 102101. (<a
href="https://doi.org/10.1016/j.datak.2022.102101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Sihem Amer-Yahia ( Associate Editors of the Special Issue of Best of EGC 2022 ) and Arnaud Soulet},
  doi          = {10.1016/j.datak.2022.102101},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102101},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An effective strategy for churn prediction and customer
profiling. <em>DKE</em>, <em>142</em>, 102100. (<a
href="https://doi.org/10.1016/j.datak.2022.102100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer churn prediction and profiling are two major economic concerns for many companies. Different learning approaches have been proposed, however the a priori choice of the most suitable model to perform both tasks remains non-trivial as it is highly dependent on the intrinsic characteristics of the churn data. Our study compares eight supervised machine learning methods combined with seven sampling approaches on thirteen public churn data sets. Our evaluations, reported in terms of area under the curve ( A U C AUC ), explore the influence of rebalancing strategies and data properties on the performance of learning methods. We rely on the Nemenyi test and Correspondence Analysis as means of visualizing the association between models, rebalancing and data. This work identifies the most appropriate methods in an attrition context and proposes an effective pipeline based on an ensemble approach and deep autoencoders segmentation. Our strategy can enlighten marketing or human resources services on the behavioral patterns of customers and their attrition probability. The described experiments are fully reproducible and our proposal can be successfully applied to a wide range of churn-like datasets.},
  archive      = {J_DKE},
  author       = {Louis Geiler and Séverine Affeldt and Mohamed Nadif},
  doi          = {10.1016/j.datak.2022.102100},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102100},
  shortjournal = {Data Knowl. Eng.},
  title        = {An effective strategy for churn prediction and customer profiling},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). French translation of a dialogue dataset and text-based
emotion detection. <em>DKE</em>, <em>142</em>, 102099. (<a
href="https://doi.org/10.1016/j.datak.2022.102099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots allow computer programs to interact naturally with a user. However, they remain limited due to their lack of sensitivity to the user’s state of mind and emotions. This sensitivity will allow the chatbots to provide more accurate answers. Text-based emotion detection has already been explored for the english language (Chatterjee et al., 2019), yet no satisfying french dataset is available. We propose to translate the emotion corpus of multi-party conversation EmotionLines, which is based on the Friends TV show, by exploiting its french broadcasting. Our translation-based dataset generation method is adaptable to any dataset deriving from foreign movies, or TV shows broadcasted in french. Using this translated dataset, we propose a classifier based on BERT , able to detect the user’s emotion from text. It takes into account the context of the discussion to improve its inferences.},
  archive      = {J_DKE},
  author       = {Pierre-Yves Genest and Laurent-Walter Goix and Yasser Khalafaoui and Előd Egyed-Zsigmond and Nistor Grozavu},
  doi          = {10.1016/j.datak.2022.102099},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102099},
  shortjournal = {Data Knowl. Eng.},
  title        = {French translation of a dialogue dataset and text-based emotion detection},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classifying encyclopedia articles: Comparing machine and
deep learning methods and exploring their predictions. <em>DKE</em>,
<em>142</em>, 102098. (<a
href="https://doi.org/10.1016/j.datak.2022.102098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comparative study of supervised classification approaches applied to the automatic classification of encyclopedia articles written in French. Our dataset includes all  70k text articles from Diderot and d’Alembert’s Encyclopédie (1751-72). In a two-task experiment we test combinations of (1) text vectorization methods (bags-of-words and word embeddings) and (2) traditional Machine Learning and newer Deep Learning classification methods (including transformer architectures). In addition to evaluating each approach, we review the results quantitatively and qualitatively. The best model obtains an average F-score of 86\% for 38 classes. Using network analysis , we highlight the difficulty of labeling semantically close classes. We also discuss misclassifications in order to understand the relationship between content and different ways of ordering knowledge. We openly release all code and results, and data is available on request. 1},
  archive      = {J_DKE},
  author       = {Alice Brenon and Ludovic Moncla and Katherine McDonough},
  doi          = {10.1016/j.datak.2022.102098},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102098},
  shortjournal = {Data Knowl. Eng.},
  title        = {Classifying encyclopedia articles: Comparing machine and deep learning methods and exploring their predictions},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In pursuit of the hidden features of GNN’s internal
representations. <em>DKE</em>, <em>142</em>, 102097. (<a
href="https://doi.org/10.1016/j.datak.2022.102097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of explaining Graph Neural Networks (GNNs). While most attempts aim at explaining the final decision of the model, we focus on the hidden layers to examine what the GNN actually captures and shed light on the hidden features built by the GNN. To that end, we first extract activation rules that identify sets of exceptionally co-activated neurons when classifying graphs in the same category. These rules define internal representations having a strong impact in the classification process. Then – this is the goal of the current paper – we interpret these rules by identifying a graph that is fully embedded in the related subspace identified by the rule. The graph search is based on a Monte Carlo Tree Search directed by a proximity measure between the graph embedding and the internal representation of the rule, as well as a realism factor that constrains the distribution of the labels of the graph to be similar to that observed on the dataset. Experiments including 6 real-world datasets and 3 baselines demonstrate that our method DISCERN generates realistic graphs of high quality which allows providing new insights into the respective GNN models.},
  archive      = {J_DKE},
  author       = {Luca Veyrin-Forrer and Ataollah Kamal and Stefan Duffner and Marc Plantevit and Céline Robardet},
  doi          = {10.1016/j.datak.2022.102097},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102097},
  shortjournal = {Data Knowl. Eng.},
  title        = {In pursuit of the hidden features of GNN’s internal representations},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RETRACTED: Pollen risk levels prediction from multi-source
historical data. <em>DKE</em>, <em>142</em>, 102096. (<a
href="https://doi.org/10.1016/j.datak.2022.102096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article has been retracted: please see Elsevier Policy on Article Withdrawal ( http://www.elsevier.com/locate/withdrawalpolicy ). This article has been retracted at the request of Editor as the study had not received the appropriate permissions for use of the data.},
  archive      = {J_DKE},
  author       = {Esso-Ridah Bleza and Valérie Monbet and Pierre-François Marteau},
  doi          = {10.1016/j.datak.2022.102096},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102096},
  shortjournal = {Data Knowl. Eng.},
  title        = {RETRACTED: Pollen risk levels prediction from multi-source historical data},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parameter-free KNN for rating prediction. <em>DKE</em>,
<em>142</em>, 102095. (<a
href="https://doi.org/10.1016/j.datak.2022.102095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the most popular collaborative filtering algorithms are methods based on the K K nearest neighbors (KNN). In their basic operation, KNN methods consider a fixed number of neighbors to make recommendations. However, it is not easy to choose an appropriate number of neighbors. Thus, it is generally fixed by calibration to avoid inappropriate values which would negatively affect the accuracy of the recommendations. In the literature, some authors have addressed the problem of dynamically finding an appropriate number of neighbors. But they use additional parameters which limit their proposals because these parameters also require calibration. In this paper, we propose a parameter-free KNN method for rating prediction. It is able to dynamically select an appropriate number of neighbors to use. The experiments that we did on four publicly available datasets demonstrate the efficiency of our proposal. It rivals those of the state of the art in their best configurations.},
  archive      = {J_DKE},
  author       = {Junior Medjeu Fopa and Modou Gueye and Samba Ndiaye and Hubert Naacke},
  doi          = {10.1016/j.datak.2022.102095},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102095},
  shortjournal = {Data Knowl. Eng.},
  title        = {A parameter-free KNN for rating prediction},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a better identification of bitcoin actors by
supervised learning. <em>DKE</em>, <em>142</em>, 102094. (<a
href="https://doi.org/10.1016/j.datak.2022.102094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is the most widely used crypto-currency, and one of the most studied. Thanks to the open nature of the Blockchain , transaction records are freely accessible and can be analyzed by anyone. The first step in most analytics work is to group anonymous addresses into a set of addresses, called aggregates, that are meant to correspond to unique actors. In this paper, we propose new methods to discover more accurate address aggregates using supervised learning. We introduce a way to create a labeled training set based on reliable heuristics and external information, and propose two methods. The first method automatically finds address aggregates from a set of transactions. The second one improves an address aggregate of a target actor by specializing the training for a single actor. We empirically validate our results on large-scale datasets. A striking result of our analysis is that training a model to recognize the change addresses of a particular actor is more efficient than using a larger dataset that does not target that particular actor. In doing so, we clearly show the feasibility and interest of supervised machine learning to identify Bitcoin actors.},
  archive      = {J_DKE},
  author       = {Rafael Ramos Tubino and Céline Robardet and Rémy Cazabet},
  doi          = {10.1016/j.datak.2022.102094},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102094},
  shortjournal = {Data Knowl. Eng.},
  title        = {Towards a better identification of bitcoin actors by supervised learning},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wikipedia searches and the epidemiology of infectious
diseases: A systematic review. <em>DKE</em>, <em>142</em>, 102093. (<a
href="https://doi.org/10.1016/j.datak.2022.102093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review aims to collect, analyse and synthesize the available evidence that can be provided by Wikipedia for epidemiologic surveillance purposes. PRISMA guidelines were followed. PubMed/Medline and Scopus were consulted. Out of 238 retrieved articles, 16 articles were included in the systematic review. The most frequently assessed infectious disease was Influenza, followed by arboviruses and measles. Influenza studies show that Wikipedia could be considered a scientifically valid surveillance system that fills the main gaps in existing traditional surveillance systems. As regards arboviruses, searches on the Web have positively mediated the relationship between epidemiological data and the number of Wikipedia page visualization. Regarding measles, studies showed a strong/moderate temporal correlation between infectious disease notification bulletins and Wikipedia search trends. Despite the type of infectious agents , three main aims can be detected: (i) understand the public’s interest, (ii) explore the use of Wikipedia by organizations, and (iii) assess the accuracy of Wikipedia content. These new strategies for surveillance of infectious diseases should be implemented, to date they could be useful in supporting traditional surveillance.},
  archive      = {J_DKE},
  author       = {Omar Enzo Santangelo and Vincenza Gianfredi and Sandro Provenzano},
  doi          = {10.1016/j.datak.2022.102093},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102093},
  shortjournal = {Data Knowl. Eng.},
  title        = {Wikipedia searches and the epidemiology of infectious diseases: A systematic review},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explicit and implicit oriented aspect-based sentiment
analysis with optimal feature selection and deep learning for
demonetization in india. <em>DKE</em>, <em>142</em>, 102092. (<a
href="https://doi.org/10.1016/j.datak.2022.102092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Analysis (ABSA) is a popular scheme that looks for the prediction of the sentiment of positive characteristics in text. The sentiment of text sequences is analyzed by deep neural networks and attained noteworthy results. Conversely, these models also have some problems with the limitation of past-training word embeddings and lack of communication between the context and the particular characteristic of the attention scheme. The main part of this task is to develop the novel ABSA concerning both explicit and implicit aspects using demonetization dataset reviews from India. Initially, the pre-processing of online tweets is performed by stop word removal, tokenization, lower case conversion, and stemming. Further, the explicit aspects are extracted, as it is simple to extract from the sentence and the polarity score is computed. A machine learning algorithm termed as Neural Network (NN) is utilized that helps for training the data regarding the implicit aspects, and further, helps to differentiate properly for the testing data with exact polarity score. Optimal feature selection is performed using the Self Adaptive Beetle Swarm Optimization (SA-BSO). These optimal features are given to a deep structured architecture called Recurrent Neural Network (RNN) with hidden neuron optimization by SA-BSO, which categorizes the demonetization reviews into positive, negative, or neutral. While taking the findings, the accuracy of the offered SA-BSO-RNN is secured at 4.67\%, 6.56\%, 3.54\%, and 7.12\% progressed than PSO-RNN, FF-RNN, CSA-RNN, and BSO-RNN, at 3-fold analysis for dataset 1. Results show that the designed ABSA concerning both explicit and implicit aspects using the demonetization method that provides enriched performance with diverse performance metrics.},
  archive      = {J_DKE},
  author       = {K. Ananthajothi and K. Karthikayani and R. Prabha},
  doi          = {10.1016/j.datak.2022.102092},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102092},
  shortjournal = {Data Knowl. Eng.},
  title        = {Explicit and implicit oriented aspect-based sentiment analysis with optimal feature selection and deep learning for demonetization in india},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Watch out, pothole! Featuring road damage detection in an
end-to-end system for autonomous driving. <em>DKE</em>, <em>142</em>,
102091. (<a href="https://doi.org/10.1016/j.datak.2022.102091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While autonomous driving technology made significant progress in the last decade, road damage detection as a relevant challenge for ensuring safety and comfort is still under development. This paper addresses the lack of algorithms for detecting road damages that meet autonomous driving systems’ requirements. We investigate the environmental perception systems’ architecture and current algorithm designs for road damage detection. Based on the autonomous driving architecture, we develop an end-to-end concept that leverages data from low-cost pre-installed sensors for real-time road damage and damage severity detection as well as cloud- and crowd-based HD Feature Maps to share information across vehicles. In a design science research approach, we develop three artifacts in three iterations of expert workshops and design cycles: the end-to-end concept featuring road damages in the system architecture and two lightweight deep neural networks , one for detecting road damages and another for detecting their severity as the central components of the system. The research design draws on new self-labeled automotive-grade images from front-facing cameras in the vehicle and interdisciplinary literature regarding autonomous driving architecture and the design of deep neural networks. The road damage detection algorithm delivers cutting-edge performance while being lightweight compared to the winners of the IEEE Global Road Damage Detection Challenge 2020, which makes it applicable in autonomous vehicles. The road damage severity algorithm is a promising approach, delivering superior results compared to a baseline model . The end-to-end concept is developed and evaluated with experts of the autonomous driving application domain.},
  archive      = {J_DKE},
  author       = {Felix Kortmann and Pascal Fassmeyer and Burkhardt Funk and Paul Drews},
  doi          = {10.1016/j.datak.2022.102091},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102091},
  shortjournal = {Data Knowl. Eng.},
  title        = {Watch out, pothole! featuring road damage detection in an end-to-end system for autonomous driving},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ERP failure: A systematic mapping of the literature.
<em>DKE</em>, <em>142</em>, 102090. (<a
href="https://doi.org/10.1016/j.datak.2022.102090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of different technologies, the use and importance of Enterprise Resource Planning (ERP) systems continue to increase daily. In parallel with this increasing use, a lot of research is being done to successfully complete ERP implementation projects. However, despite these researches, reported case studies show that the success rates of ERP projects are meager. Based on the examples of experienced failure, researchers determine very different failure factors with varying perspectives for companies in different industry sectors, cultures, and sizes. It is becoming increasingly difficult for many practitioners and researchers to understand these failure factors correctly. Our objective is to investigate the state-of-the-art ERP Failure Factors that could benefit practitioners to utilize that information potentially. We review the body of knowledge related to ERP failure factors in the form of a systematic literature mapping (SLM). We pose four sets of research questions and systematically develop and refine a classification schema. The initial pool consisted of 353 articles. Systematic voting was conducted among the authors regarding the inclusion/exclusion criteria. As a result, there were 72 technical articles in our final pool. This SLM provides an overview of ERP critical failure factors (CFF) with different focused headings. These headings cover qualitative coding about CFF names, CFF rankings, the relation between CFF and ERP processes and failure modes, etc. The results of this study would benefit three groups of stakeholders: (i) Researchers who work on ERP Failure Factors, (ii) Solution implementers who provide consultancy services to companies that carry out ERP Implementation projects, and (iii) ERP project implementation managers. These stakeholders could utilize the results of this SLM to catch the trend of ERP implementation challenges.},
  archive      = {J_DKE},
  author       = {Evren Coşkun and Bahar Gezici and Murat Aydos and Ayça Kolukısa Tarhan and Vahid Garousi},
  doi          = {10.1016/j.datak.2022.102090},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102090},
  shortjournal = {Data Knowl. Eng.},
  title        = {ERP failure: A systematic mapping of the literature},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A workload-driven method for designing aggregate-oriented
NoSQL databases. <em>DKE</em>, <em>142</em>, 102089. (<a
href="https://doi.org/10.1016/j.datak.2022.102089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scalability and availability problems with traditional relational database systems , a variety of NoSQL stores have emerged over the last decade to deal with big data. How data are structured in a NoSQL store has a large impact on the query and update performance and the storage usage. Thus, different from the traditional database design, not only the data structure but also the data access patterns need to be considered in the design of NoSQL database schemas. In this paper, we present a general workload-driven method for designing key-value, wide-column, and document NoSQL database schemas. We first present a generic logical model Query Path Graph (QPG) that can represent the data structures of the UML class diagram . We also define mappings from the SQL-based query patterns to QPG and from QPG to aggregate-oriented NoSQL schemas. We use a cost model to measure the query and update performance and optimize the QPG schemas. We evaluate the proposed method with several typical case studies by simulating workloads on databases with different schema designs. The results demonstrate that our method preserves the generality and the quality of the design.},
  archive      = {J_DKE},
  author       = {Liu Chen and Ali Davoudian and Mengchi Liu},
  doi          = {10.1016/j.datak.2022.102089},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102089},
  shortjournal = {Data Knowl. Eng.},
  title        = {A workload-driven method for designing aggregate-oriented NoSQL databases},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the explanatory power of boolean decision trees.
<em>DKE</em>, <em>142</em>, 102088. (<a
href="https://doi.org/10.1016/j.datak.2022.102088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees have long been recognized as models of choice in sensitive applications where interpretability is of paramount importance. In this paper, we examine the computational ability of Boolean decision trees for the explanation purpose. We focus on both abductive explanations (suited to explaining why a given instance has been classified as such by the decision tree at hand) and on contrastive explanations (suited to explaining why a given instance has not been classified by the decision tree as it was expected). More precisely, we are interested in deriving, minimizing, and counting abductive explanations and contrastive explanations. We prove that the set of all irredundant abductive explanations (also known as PI-explanations or sufficient reasons) for an instance given a decision tree can be exponentially larger than the size of the input (the instance and the decision tree). Therefore, generating the full set of sufficient reasons for an instance can be out of reach. In addition, deriving a single sufficient reason, though computationally easy when dealing with decision trees, does not prove enough in general; indeed, two sufficient reasons for the same instance may differ on many features. To deal with this issue and generate synthetic views of the set of all sufficient reasons, we define notions of relevant features and of necessary features that characterize the (possibly negated) features appearing in at least one or in every sufficient reason for an instance, and we show that they can be computed in polynomial time . We also introduce the notion of explanatory importance, that indicates how frequent each (possibly negated) feature is in the set of all sufficient reasons. We show how the explanatory importance of a (possibly negated) feature and the number of sufficient reasons for an instance can be obtained via a model counting operation, which turns out to be practical in many cases. We also explain how to enumerate minimum-size sufficient reasons. We finally show that, unlike sufficient reasons, the set of all contrastive explanations for an instance given a decision tree can be derived, minimized and counted in polynomial time.},
  archive      = {J_DKE},
  author       = {Gilles Audemard and Steve Bellart and Louenas Bounia and Frédéric Koriche and Jean-Marie Lagniez and Pierre Marquis},
  doi          = {10.1016/j.datak.2022.102088},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102088},
  shortjournal = {Data Knowl. Eng.},
  title        = {On the explanatory power of boolean decision trees},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hepatitis c virus data analysis and prediction using machine
learning. <em>DKE</em>, <em>142</em>, 102087. (<a
href="https://doi.org/10.1016/j.datak.2022.102087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical decision support systems have been on the rise with technological advances and they have been the subject of many studies. Developing an effective medical decision support system requires a high amount of accuracy, precision, and sensitivity as well as time efficiency that is inversely proportional to the complexity of the model. Hepatitis C virus (HCV) infection is one of the most important causes of chronic liver disease worldwide. In this study, data discovery has been made by applying data science processes, and the HCV has been estimated with machine learning methods. By analyzing and visualizing the values in the data set, features that may be important for HCV was determined, and HCV estimation was made using various machine learning methods, pre-processing and feature extraction. According to the features obtained from this study, the estimation of HCV can be made automatically and can be a decision support system that helps the researchers and clinicians. In this study, HCV was obtained with 99.31\% accuracy by adding new features and eliminating imbalances between classes. The model in this study can be used as an alternative method in the prediction of Hepatitis C-related diseases.},
  archive      = {J_DKE},
  author       = {Mete Yağanoğlu},
  doi          = {10.1016/j.datak.2022.102087},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102087},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hepatitis c virus data analysis and prediction using machine learning},
  volume       = {142},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On efficient top-k transaction path query processing in
blockchain database. <em>DKE</em>, <em>141</em>, 102079. (<a
href="https://doi.org/10.1016/j.datak.2022.102079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of blockchain , user’s daily financial bills and transfer records can be safely and completely stored in a transaction form. These transactions express commercial preferences for various scenarios and can be used to improve the quality of application services such as data analysis, data security, and recommendation systems. Therefore, there is a growing demand for processing diverse queries over blockchain transactions. In this paper, we formulate a new problem of blockchain top- k transaction path query (BCT k PQ) which returns first k transactions in a given query path according to the user-specified conditions. For processing BCT k PQ, we design a novel Collaborative Query Model (CQM) consisting of a set of collaborative peers and each of which contains three key components, i.e., parser, indexer and executor. First, in the parser, we propose a graph-based structure to denote the transaction paths. Then, in the indexer, we propose a two-level index to increase query and verification efficiency. Finally, in the executor, we present an optimized query algorithm for answering BCT k PQ securely and efficiently and further give a verification algorithm to guarantee the correctness of the query results. We conduct extensive experiments to illustrate the efficiency and effectiveness of our solution.},
  archive      = {J_DKE},
  author       = {Kun Hao and Junchang Xin and Zhiqiong Wang and Zhongming Yao and Guoren Wang},
  doi          = {10.1016/j.datak.2022.102079},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102079},
  shortjournal = {Data Knowl. Eng.},
  title        = {On efficient top-k transaction path query processing in blockchain database},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining specialized word embeddings and subword semantic
features for lexical entailment recognition. <em>DKE</em>, <em>141</em>,
102077. (<a href="https://doi.org/10.1016/j.datak.2022.102077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of L exical E ntailment R ecognition (LER) aims to identify the is-a relation between words. This problem has recently received attention from researchers in the field of natural language processing because of its application to varied downstream tasks. However, almost all prior studies have only focused on datasets that include single words; thus, how to handle compound words effectively is still a challenge. In this study, we propose a novel method called LERC ( L exical E ntailment R ecognition C ombination) to solve this problem by combining embedding representations and subword semantic features . For this aim, firstly a specialized word embedding model for the LER tasks is trained. Secondly, subword semantic information of word pairs is exploited to compute another feature vector. This feature vector is combined with embedding vectors for supervised classification . We considered three LER tasks, including Lexical Entailment Detection , Lexical Entailment Directionality , and Lexical Entailment Determination . Experimental results conducted on several benchmark datasets in English and Vietnamese languages demonstrated that the subword semantic feature is useful for these tasks. Moreover, LERC outperformed several methods published recently.},
  archive      = {J_DKE},
  author       = {Van-Tan Bui and Phuong-Thai Nguyen and Van-Lam Pham},
  doi          = {10.1016/j.datak.2022.102077},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102077},
  shortjournal = {Data Knowl. Eng.},
  title        = {Combining specialized word embeddings and subword semantic features for lexical entailment recognition},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing classification capacity of CNN models with deep
feature selection and fusion: A case study on maize seed classification.
<em>DKE</em>, <em>141</em>, 102075. (<a
href="https://doi.org/10.1016/j.datak.2022.102075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing computer-assisted agricultural analysis systems using machine learning methods is a focused area in recent years. As in every field, it is aimed to improve the production process and product quality in agricultural applications with computer-assisted systems. The maize plant is a very important species in terms of providing sufficient food to the world’s population. The separation process of the haploid and diploid maize seeds is a critical issue in terms of maize breeding time and production efficiency. In this study, a haploid–diploid maize seed classification method is proposed using deep features obtained from convolutional neural networks (CNN). In the first stage, deep features were obtained from fully connected layers of different CNN models. Then, the best 100 features were selected by using the MRMR (Max-Relevance and Min-Redundancy) feature selection method for 1000 features obtained in each CNN model. These selected features have been fused according to different combinations of the CNN models. These fused features have been used in the training and testing stages of a conventional classifier method in the last stage. Eventually, according to the experimental results, it was determined that the general accuracy performance has been around 96.74\%. It has been observed that the proposed approach exhibits high performance in the classification process of maize seeds.},
  archive      = {J_DKE},
  author       = {Emrah Dönmez},
  doi          = {10.1016/j.datak.2022.102075},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102075},
  shortjournal = {Data Knowl. Eng.},
  title        = {Enhancing classification capacity of CNN models with deep feature selection and fusion: A case study on maize seed classification},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reasoning on company takeovers: From tactic to strategy.
<em>DKE</em>, <em>141</em>, 102073. (<a
href="https://doi.org/10.1016/j.datak.2022.102073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate takeovers — the purchases of a company by another — are significant economic events. They affect the parties involved in the transaction, inducing relevant dynamics in the enterprises’ lifecycle, and are also relevant at an aggregate level for the whole economy: takeovers can produce efficiency gains and improved capital allocation, while, on the other hand, they can also increase the market power of specific companies and hamper competition. In this paper, we propose a logic-probabilistic reasoning framework to study the determinants of company takeovers and predict future ones. In particular, we model the domain of interest as a logic-based knowledge graph, where the extensional knowledge contains facts concerning company ownership structures and the characteristics of the shareholders, and the intensional knowledge encodes a set of takeover suitability criteria in the form of reasoning rules, whose conditional dependencies are modeled with a Bayesian network . Our rules are expressed in Vadalog, a language of the Datalog+/- family. Our framework revolves around a data engineering process that allows eliciting the takeover determinants from a corpus of anecdotal cases, refining and encoding them into logic rules, and finally combining their outcomes. We implement and operate the framework in the Vadalog System, a state-of-the-art reasoner , and apply it to the knowledge graph of the Italian companies of the Central Bank of Italy. We provide an extensive experimental evaluation.},
  archive      = {J_DKE},
  author       = {Luigi Bellomarini and Lorenzo Bencivelli and Claudia Biancotti and Livia Blasi and Francesco Paolo Conteduca and Andrea Gentili and Rosario Laurendi and Davide Magnanimi and Michele Savini Zangrandi and Flavia Tonelli and Stefano Ceri and Davide Benedetto and Markus Nissl and Emanuel Sallinger},
  doi          = {10.1016/j.datak.2022.102073},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102073},
  shortjournal = {Data Knowl. Eng.},
  title        = {Reasoning on company takeovers: From tactic to strategy},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). System: A core conceptual modeling construct for capturing
complexity. <em>DKE</em>, <em>141</em>, 102062. (<a
href="https://doi.org/10.1016/j.datak.2022.102062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitalization of human society continues at a relentless rate. However, to develop modern information technologies, the increasing complexity of the real-world must be modeled, suggesting the general need to reconsider how to carry out conceptual modeling . This research proposes that the often-overlooked notion of “system” should be a separate, and core, conceptual modeling construct and argues for incorporating it and related concepts, such as emergence, into existing approaches to conceptual modeling. The work conducts a synthesis of the ontology of systems and general systems theory . These modeling foundations are then used to propose a CESM+ template for conducing systems-grounded conceptual modeling. Several new conceptual modeling notations are introduced. The systemist modeling is then applied to a case study on the development of a citizen science platform. The case demonstrates the potential contributions of the systemist approach and identifies specific implications of explicit modeling with systems for theory and practice. The paper provides recommendations for how to incorporate systems into existing projects and suggests fruitful opportunities for future conceptual modeling research.},
  archive      = {J_DKE},
  author       = {Roman Lukyanenko and Veda C. Storey and Oscar Pastor},
  doi          = {10.1016/j.datak.2022.102062},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102062},
  shortjournal = {Data Knowl. Eng.},
  title        = {System: A core conceptual modeling construct for capturing complexity},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A feature selection method based on term frequency
difference and positive weighting factor. <em>DKE</em>, <em>141</em>,
102060. (<a href="https://doi.org/10.1016/j.datak.2022.102060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firstly, a new concept of term frequency difference factor is proposed to balance the influences of term frequency and document frequency on feature selection. Secondly, the idea of positive weighting factor is advanced to balance the roles of the document frequency in the positive and negampared with six popular algorithms on six datasets using two classifiers of Naive Bayes and Support tive categories. And finally, a new feature selection algorithm based on term frequency difference and positive weighting factor, PWTF-TCM, is presented based on the two above concepts. In the experiments, PWTF-TCM is coVector Machines. The experimental results show that PWTF-TCM outperforms by 75\% for Macro-F 1 and 58.33\% for Micro-F 1 . In addition, PWTF-TCM improves the classification accuracy by 4.58\% compared with Trigonometric comparison measure.},
  archive      = {J_DKE},
  author       = {Hongfang Zhou and Xiang Li and Chenguang Wang and Yiming Ma},
  doi          = {10.1016/j.datak.2022.102060},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102060},
  shortjournal = {Data Knowl. Eng.},
  title        = {A feature selection method based on term frequency difference and positive weighting factor},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From base data to knowledge discovery – a life cycle
approach – using multilayer networks. <em>DKE</em>, <em>141</em>,
102058. (<a href="https://doi.org/10.1016/j.datak.2022.102058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of complex data sets to infer/discover meaningful information/knowledge involves (after data collection and cleaning): (i) Modeling the data — an approach for deriving a suitable representation of data for analysis, (ii) translating analysis objectives into computations on the generated model instance; these computations can be as simple as a query or a complex computation (e.g., community detection over multiple layers), (iii) computation of expressions generated — considering efficiency and scalability, and (iv) drill-down of results to understand them clearly. Beyond this, it is also useful to visualize results for easier understanding. Covid-19 visualization dashboard presented in this paper is an example of this. This paper covers the above steps of data analysis life cycle using a representation (or model) that is gaining importance. With complex data sets containing multiple entity types and relationships, an appropriate model to represent the data is important. For these data sets, we first establish the advantages of Multilayer Networks (or MLNs) as a data model. Then we use an entity-relationship based approach to convert the data set into MLNs for a precise representation of the data set. After that, we outline how expected analysis objectives can be translated using keyword-mapping to aggregate analysis expressions. Finally, we demonstrate, through a set of example data sets and objectives, how the expressions corresponding to objectives are evaluated using an efficient decoupling-based approach. Results are further drilled down to obtain actionable knowledge from the data set. Using the widely popular Enhanced Entity Relationship (EER) approach for requirements representation, we demonstrate how to generate EER diagrams for data sets and further generate, algorithmically, MLNs as well as Relational schema for analysis and drill down, respectively. Using communities and centrality for aggregate analysis, we demonstrate the flexibility of the chosen model to support diverse set of objectives. We also show that compared to current analysis approaches, a “decoupling-based” approach using MLNs is more appropriate as it preserves structure as well as semantics of the results and is very efficient. For this computation, we need to derive expressions for each analysis objective using the MLN model. We provide guidelines to translate English queries into analysis expressions based on keywords. Finally, we use several data sets to establish the effectiveness of modeling using MLNs and their analysis using the decoupling approach that has been proposed recently. For coverage, we use different types of MLNs for modeling, and community and centrality computations for analysis. The data sets used are from US commercial airlines, IMDb (a large international movie data set), the familiar DBLP (or bibliography database), and the Covid-19 data set. Our experimental analyses using the identified steps validate modeling, breadth of objectives that can be computed, and overall versatility of the life cycle approach . Correctness of results is verified, where possible, using independently available ground truth. Furthermore, we demonstrate drill-down that is afforded by this approach (due to structure and semantics preservation) for a better understanding and visualization of results.},
  archive      = {J_DKE},
  author       = {Abhishek Santra and Kanthi Komar and Sanjukta Bhowmick and Sharma Chakravarthy},
  doi          = {10.1016/j.datak.2022.102058},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102058},
  shortjournal = {Data Knowl. Eng.},
  title        = {From base data to knowledge discovery – a life cycle approach – using multilayer networks},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid methodology for analysis of structured and
unstructured data to support decision-making in public security.
<em>DKE</em>, <em>141</em>, 102056. (<a
href="https://doi.org/10.1016/j.datak.2022.102056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a hybrid methodology that enables the integration of structured and unstructured data to support the decision-making process in public security contexts. The proposed methodology facilitates classification and prediction of crime in a given region, making it possible to identify actions to improve public security based on the results. The integration of the data takes place in two main steps: (1) loading and analyzing structured data made available by government agencies; and (2) absorbing, classifying, and analyzing unstructured data from digital platforms such as Twitter, Where I Was Robbed, and CityCop. In this way, it becomes possible to transform these unstructured data into structured data to be incorporated into a historical database on which algorithms can act to classify, measure, and predict crime. To illustrate the applicability of this methodology, we conducted a study in the city of Recife, Brazil . Structured and unstructured data were gathered in order to conduct a neighborhood classification analysis of crime hot spots . Based on that analysis, we conducted a series of actions intended to bring improvements to the region by the local police. We obtained an increase in the algorithms’ accuracy rate of 80\%, indicating that public security organizations can base their actions on the results of the proposed methodology.},
  archive      = {J_DKE},
  author       = {Jean Gomes Turet and Ana Paula Cabral Seixas Costa},
  doi          = {10.1016/j.datak.2022.102056},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102056},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hybrid methodology for analysis of structured and unstructured data to support decision-making in public security},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical graph-based model for mobility data
representation and analysis. <em>DKE</em>, <em>141</em>, 102054. (<a
href="https://doi.org/10.1016/j.datak.2022.102054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical representations of transportation networks should provide a better understanding of mobility patterns and the underlying structures at various abstraction levels. This paper introduces a hierarchical graph-based model for representing moving objects and trajectories according to multiple spatial, temporal and semantic scales. This formal model is implemented in a graph database and experimented with historical maritime data. Several experimental analyses explore and extract knowledge patterns from the hierarchical graph database. A series of queries applied to an European maritime network derive mobility patterns and highlight network structures.},
  archive      = {J_DKE},
  author       = {Maryam Maslek Elayam and Cyril Ray and Christophe Claramunt},
  doi          = {10.1016/j.datak.2022.102054},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102054},
  shortjournal = {Data Knowl. Eng.},
  title        = {A hierarchical graph-based model for mobility data representation and analysis},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based valuation of smart grid initiatives:
Foundations, open issues, requirements, and a research outlook.
<em>DKE</em>, <em>141</em>, 102052. (<a
href="https://doi.org/10.1016/j.datak.2022.102052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the value assessment of technically feasible smart grid initiatives there exist several valuation methods. To determine whether those methods address all concerns relevant for smart grid valuation, we carry out a literature analysis aiming at (1) identifying existing valuation methods and the steps they propose, (2) identifying important valuation considerations, and (3) confronting these considerations with artifacts proposed by the existing valuation methods to identify open issues, requirements, and remaining challenges. Based on the conducted analysis we identify, among others, the following main deficiencies: (1) only a limited scope of concerns relevant to valuation is covered, particularly a systematic consideration of stakeholders goals , value exchange scenarios, and the IT infrastructure is lacking; and (2) a lack of instruments dedicated to fostering accessibility of valuation, in terms of establishing a shared understanding, communicating results, or actively involving different stakeholders in the process. Based on the findings, we suggest the application of conceptual modeling as an instrument to address the identified deficiencies . Therefore, we reflect on the role that current modeling approaches can play in smart grid valuation. This paper is a part of a larger project whose ultimate goal is to develop a model-based method for multi-perspective valuation of smart grid initiatives. The purpose of this paper is to establish a foundation for the realization of the envisioned method. The design of the model-based valuation method itself, its application and evaluation, are subjects of future work.},
  archive      = {J_DKE},
  author       = {Sybren de Kinderen and Monika Kaczmarek-Heß and Qin Ma and Iván S. Razo-Zapata},
  doi          = {10.1016/j.datak.2022.102052},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102052},
  shortjournal = {Data Knowl. Eng.},
  title        = {Model-based valuation of smart grid initiatives: Foundations, open issues, requirements, and a research outlook},
  volume       = {141},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fuzzy clustering technique for enhancing the convergence
performance by using improved fuzzy c-means and particle swarm
optimization algorithms. <em>DKE</em>, <em>140</em>, 102050. (<a
href="https://doi.org/10.1016/j.datak.2022.102050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clustering is a well-established technique among the well-known clustering techniques in several real-world applications due to easy implementation and produces satisfactory clustering result . However, it has some deficiency such as sensitive to outliers, result dependency on choosing initial centroid , etc. To eradicate the shortcoming of FCM algorithm , this article introduces a robust clustering technique, particle swarm optimization improved fuzzy c-means is developed by the hybridization of particle swarm optimization and improved fuzzy c-means techniques, to deal with noisy data and initialization problem. In this article, a fuzzy clustering technique is developed to increase the convergence performance of clustering techniques. Fuzzy c-means is improved by developing a new metric to tolerate the noisy environment . Particle swarm optimization has an inbuilt guidance strategy which leads the solution in particle swarm optimization to obtain useful information from the better solution and thereby helping them improve their own solution. To handle the initialization problem of fuzzy c-means, particle swarm optimization technique is used. PSO effectively enhance the performance of improved FCM to increase the effectiveness of clustering. The effectiveness of the proposed clustering technique over existing techniques in literature has been illustrated by adopting eight real worlds and three artificial data sets. The results show that the proposed algorithm generates encouraging results as compared to the established clustering technique in literature.},
  archive      = {J_DKE},
  author       = {Niteesh Kumar and Harendra Kumar},
  doi          = {10.1016/j.datak.2022.102050},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102050},
  shortjournal = {Data Knowl. Eng.},
  title        = {A fuzzy clustering technique for enhancing the convergence performance by using improved fuzzy c-means and particle swarm optimization algorithms},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An approach to detect backbones of information diffusers
among different communities of a social platform. <em>DKE</em>,
<em>140</em>, 102048. (<a
href="https://doi.org/10.1016/j.datak.2022.102048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information diffusion in social networks is a classic and, at the same time, very current problem. In fact, information diffusers are always looking for new techniques to disseminate information of their interest by creating backbones among them. In this paper, we focus on a specific, but very current and relevant, scenario regarding this way of proceeding. In fact, we propose an approach for the detection of possible backbones of information diffusers among different communities of a social network. Our approach is based on a new centrality measure that we call disseminator centrality. It is specifically designed to detect the so-called disseminator bridges, i.e., users belonging to multiple communities of a single social network, who want to disseminate information of their interest from one community to another by supporting each other. This paper describes the proposed approach, presents the disseminator centrality, illustrates the differences with respect to the related literature and presents the results of the experiments carried out to evaluate its performance.},
  archive      = {J_DKE},
  author       = {Gianluca Bonifazi and Francesco Cauteruccio and Enrico Corradini and Michele Marchetti and Alberto Pierini and Giorgio Terracina and Domenico Ursino and Luca Virgili},
  doi          = {10.1016/j.datak.2022.102048},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102048},
  shortjournal = {Data Knowl. Eng.},
  title        = {An approach to detect backbones of information diffusers among different communities of a social platform},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preface to the data and knowledge engineering special issue
on selected papers from RCIS 2021. <em>DKE</em>, <em>140</em>, 102046.
(<a href="https://doi.org/10.1016/j.datak.2022.102046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Samira Cherfi and Anna Perini and Renata Guizzardi},
  doi          = {10.1016/j.datak.2022.102046},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102046},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface to the data and knowledge engineering special issue on selected papers from RCIS 2021},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum to “an efficient integer coding index algorithm
for multi-scale time information management” [data knowl. Eng. 119
(2019) 123–138]. <em>DKE</em>, <em>140</em>, 102045. (<a
href="https://doi.org/10.1016/j.datak.2022.102045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Xiaochong Tong and Chengqi Cheng and Rong Wang and Lu Ding and Yong Zhang and Guangling Lai and Lin Wang and Bo Chen},
  doi          = {10.1016/j.datak.2022.102045},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102045},
  shortjournal = {Data Knowl. Eng.},
  title        = {Corrigendum to “An efficient integer coding index algorithm for multi-scale time information management” [Data knowl. eng. 119 (2019) 123–138]},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Architecture for ontology-supported multi-context reasoning
systems. <em>DKE</em>, <em>140</em>, 102044. (<a
href="https://doi.org/10.1016/j.datak.2022.102044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern smart systems such as those needed for Industry 4.0 integrate data from various sources and increasingly require that data be contextualized with domain knowledge. The integration and contextualization of data allows for the advanced reasoning needed to generate knowledge grounded in the data under consideration. In this paper, we propose an architecture for an ontology-supported multi-context reasoning system which inherently supports a number of desired system qualities including data transparency, system interactivity, and graceful aging. The architecture is inspired by the Presentation–Abstraction–Control architecture style, which is an interaction-based architecture. Our architecture uses a two level hierarchy with three agents and can incorporate and utilize multiple contexts. It is flexible, supporting an interface between data and users, highly interactive, and easily maintained. The evolution of data is isolated to a single component of the system and therefore does not cascade to several others. A domain of application can be easily determined by the use of archetypes and domain-specification components. Our architecture is demonstrated using a case study involving data from the city of San Francisco.},
  archive      = {J_DKE},
  author       = {Andrew LeClair and Jason Jaskolka and Wendy MacCaull and Ridha Khedri},
  doi          = {10.1016/j.datak.2022.102044},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102044},
  shortjournal = {Data Knowl. Eng.},
  title        = {Architecture for ontology-supported multi-context reasoning systems},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning-based risk prediction model for
cardiovascular disease using a hybrid dataset. <em>DKE</em>,
<em>140</em>, 102042. (<a
href="https://doi.org/10.1016/j.datak.2022.102042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CVD (cardiovascular disease) is one of the most common causes of death in the world today. CVD prediction allows health professionals to make an informed decision about their patients’ health. Data mining is the process of transforming large amounts of medical data in its raw form into actionable insights that can be used to make intelligent forecasts and decisions. Machine learning (ML) based prediction models provide a better solution to help patients’ health diagnoses in the health care industry . The objective of this research is to create a hybrid dataset to aid in the development of a best CVD risk prediction model. The Hungarian, the Switzerland, the Cleveland, and the Long Beach datasets are the most commonly used datasets in heart disease (HD) prediction. These datasets have a maximum of 303 instances with missing values in their features, and the presence of missing values reduces the accuracy of the prediction model. So, in this article, we created the ”Sathvi” dataset by combining these datasets, and it has 531 instances with 12 attributes with no missing data. The Pearson’s correlation method was used to eliminate redundant features during the feature selection process. The Naive Bayes (NB), XGBoost , k-nearest neighbour (k-NN), multilayer perceptron (MLP), support vector machine (SVM), and CatBoost ML classifiers have been applied for prediction. The CatBoost ML classifier was validated with 10-fold cross validation, and the best accuracy ranged from 88.67\% to 98.11\%, with a mean of 94.34\%.},
  archive      = {J_DKE},
  author       = {Karthick Kanagarathinam and Durairaj Sankaran and R. Manikandan},
  doi          = {10.1016/j.datak.2022.102042},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102042},
  shortjournal = {Data Knowl. Eng.},
  title        = {Machine learning-based risk prediction model for cardiovascular disease using a hybrid dataset},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conceptual model visual simulation and the inductive
learning of missing domain constraints. <em>DKE</em>, <em>140</em>,
102040. (<a href="https://doi.org/10.1016/j.datak.2022.102040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conceptual modeling plays a fundamental role in information systems engineering, and in data and systems interoperability. To play their role as instruments for domain modeling, conceptual models must contain the exact set of constraints that represent the worldview of the relevant domain stakeholders. However, as empirical results show, conceptual modelers are subject to cognitive limitations and biases and, hence, in practice, they systematically produce models that fall short in that respect. Moreover, automating the process of formally assessing conceptual models in this sense (i.e., model validation) is notoriously hard, mainly because the intended worldview at hand lies in the mind of these stakeholders. In this paper, we provide a novel approach to model validation and automated constraint learning that combines, on one hand, Model Finding via the visual simulation of that model’s valid instances and, on the other hand, Inductive Logic Programming techniques. In our approach, we properly channel the results produced by the application of a visual model finding technique as input to a learning process. We then show how the approach is able to support the modeler in identifying missing constraints from the original model. The approach is validated against a catalog of empirically-elicited conceptual modeling anti-patterns. As we show here, the approach is able to support the automated learning of constraints that are needed to rectify a number of relevant anti-patterns in this catalog.},
  archive      = {J_DKE},
  author       = {Mattia Fumagalli and Tiago Prince Sales and Fernanda Araujo Baião and Giancarlo Guizzardi},
  doi          = {10.1016/j.datak.2022.102040},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102040},
  shortjournal = {Data Knowl. Eng.},
  title        = {Conceptual model visual simulation and the inductive learning of missing domain constraints},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated identity and access management metamodel and
pattern system for secure enterprise architecture. <em>DKE</em>,
<em>140</em>, 102038. (<a
href="https://doi.org/10.1016/j.datak.2022.102038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity and access management (IAM) is one of the key components of the secure enterprise architecture for protecting the digital assets of the information systems. The challenge is: How to model an integrated IAM for a secure enterprise architecture to protect digital assets? This research aims to address this question and develops an ontology based integrated IAM metamodel for the secure digital enterprise architecture (EA). Business domain and technology agnostic characteristics of the developed IAM metamodel will allow it to develop IAM models for different types of information systems . Well-known design science research (DSR) methodology was adopted to conduct this research. The developed IAM metamodel is evaluated by using the demonstration method. Furthermore, as a part of the evaluation, a pattern system has been developed, consisting of eight IAM patterns. Each pattern offers a solution to a specific IAM related problem. The outcome of this research indicates that enterprise, IAM and information systems architects and academic researchers can use the proposed IAM metamodel and the pattern system to design and implement situation-specific IAM models within the overall context of a secure EA for information systems.},
  archive      = {J_DKE},
  author       = {Kamrun Nahar and Asif Qumer Gill},
  doi          = {10.1016/j.datak.2022.102038},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102038},
  shortjournal = {Data Knowl. Eng.},
  title        = {Integrated identity and access management metamodel and pattern system for secure enterprise architecture},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A modified attention mechanism powered by bayesian network
for user activity analysis and prediction. <em>DKE</em>, <em>140</em>,
102034. (<a href="https://doi.org/10.1016/j.datak.2022.102034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing and predicting user activity is important in the current digital era with a lot of use-cases and applications. In this paper, we present an approach that facilitates a modification of the attention mechanism in a Transformer model. This work enables to improve the predictive capacity of a forecasting model which is progressively fed by somewhat erratic and small data generated by the early stages of online activity. The key element of the work is to use a Bayesian Network (BN) as a tool for feature engineering that helps to modify the attention mechanism in the Transformer model in that scenario. The model predicts the next activity on a sequence of online activities that the user will engage in while interacting with a Learning Management System (LMS). Click-stream data refers to a detailed log of how participants navigate through an online platform during a working session. The main application of our work is to improve the Predictor module of a smart hybrid-classifier for an LMS. Several configurations and architectures for the RNN-powered predictor, are tested and assessed. The results of the improved predictive capacity of this work can be useful to users in an online learning environment where early assistance in quasi-real time is required. This research answers the questions of how click-stream data can assist in refining the tasks of the Attention mechanism to improve the quality of the prediction. Performance is measured by the accuracy , right-content and first-state accuracy scores for the incoming sequence and compared across alternative models. The method also provides systematic customization of the attention mechanism in Transformers that can be applied to a range of problems involving click-stream data.},
  archive      = {J_DKE},
  author       = {Alexis Amezaga Hechavarria and M. Omair Shafiq},
  doi          = {10.1016/j.datak.2022.102034},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102034},
  shortjournal = {Data Knowl. Eng.},
  title        = {A modified attention mechanism powered by bayesian network for user activity analysis and prediction},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding internet of things resources: A state-of-the-art
study. <em>DKE</em>, <em>140</em>, 102025. (<a
href="https://doi.org/10.1016/j.datak.2022.102025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its emergence, the Internet of Things (IoT) has aimed to join the physical world to the virtual world with a basic vision, which is to create intelligent spaces in which users interact seamlessly with IoT objects. Therefore, the user will be surrounded by a large number of services offered by these connected objects which he/she will inevitably interact with them. However, the IoT environments are characterized by a large number of heterogeneous connected objects. These characteristics make it difficult to find their adjacent resources. Importantly, because the traditional discovery solutions are inefficient, therefore, a mechanism that makes it possible to dynamically inventory and control the resources present in an IoT environment is crucial. In fact, a resource discovery mechanism basically aims to dynamically and regularly update the base of available resources in the IoT environment. To meet these needs, many research works were conducted where many architectures have been proposed to provide solutions for resource discovery in the IoT environment. In this work, we provide a state-of-the-art study of resource discovery techniques in IoT environment. First, the resource discovery techniques are classified according to their search approaches. Then, a review of a selection of recent works that propose solutions for resource discovery in the IoT environment is provided. Finally, the study gives some implications for further study and a number of guidelines that helps to meet a large part of the requirements of a resource discovery solution in the IoT environment.},
  archive      = {J_DKE},
  author       = {Hela Zorgati and Raoudha Ben Djemaa and Ikram Amous Ben Amor},
  doi          = {10.1016/j.datak.2022.102025},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102025},
  shortjournal = {Data Knowl. Eng.},
  title        = {Finding internet of things resources: A state-of-the-art study},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A random walk sampling on knowledge graphs for
semantic-oriented statistical tasks. <em>DKE</em>, <em>140</em>, 102024.
(<a href="https://doi.org/10.1016/j.datak.2022.102024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph (KG) manages large-scale and real-world facts as a big graph in a schema-flexible manner, which has recently attracted considerable attention . It is very common that users deploy some statistical tasks on a KG to achieve the latent information of interest. There are two types of the statistical tasks, that are, topology-oriented and semantic-oriented statistical tasks. Many efforts have been made for the former one (e.g., finding the average degree of a KG). The basic idea is concluded as: estimating an approximate statistical result based on a random sample collected through a topology-aware KG sampling approach. Unfortunately, this method cannot be directly deployed to support semantic-oriented statistical tasks (e.g., achieving the average fuel economy of cars produced in Germany), because the topology-aware sampling does not consider the semantics of a KG (or we say the sample is collected only based on the topological information of a KG, while excluding the semantics of a KG), hence leading to a low-quality random sample and would significantly affect the accuracy. In this paper, we propose a semantic-aware random walk sampling on KGs to quickly and accurately collect samples that match the semantic constraint of the semantic-oriented statistical task, and obtain an approximate statistical result by well-designed unbiased estimators . Moreover, we propose an optimization on our semantic-aware sampling to improve the sampling efficiency. Finally, extensive experiments were conducted on our method, which confirmed the effectiveness and efficiency of our approach.},
  archive      = {J_DKE},
  author       = {Xiaoliang Xu and Qifan Hong and Yuxiang Wang and Jiahui Jin and Xinle Xuan and Tao Fu},
  doi          = {10.1016/j.datak.2022.102024},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102024},
  shortjournal = {Data Knowl. Eng.},
  title        = {A random walk sampling on knowledge graphs for semantic-oriented statistical tasks},
  volume       = {140},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to build data-driven strategy maps? A methodological
framework proposition. <em>DKE</em>, <em>139</em>, 102019. (<a
href="https://doi.org/10.1016/j.datak.2022.102019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Strategy Map is a strategic tool that enables companies to formulate, control and communicate their strategy and positively influence their performance. Introduced in 2000, the methodology for developing Strategy Maps has evolved over the past two decades, but still relies exclusively on human input. In practice, Strategy Map causalities – the core elements of this tool – are determined by managers’ opinions and judgments, which can lead to a lack of accuracy, completeness and longitudinal perspective. Although authors in the literature have pointed out these problems in the past, there are few recommendations on how to address them. In this paper, we propose a methodological framework which uses operational data and data mining techniques to systematize the detection of causalities in Strategy Maps. We apply time series techniques and Granger causality tests to increase the efficiency of such strategic tool. We demonstrate the feasibility and relevance of this methodology using data from skeyes , the Belgian air traffic control company. 1},
  archive      = {J_DKE},
  author       = {Lhorie Pirnay and Corentin Burnay},
  doi          = {10.1016/j.datak.2022.102019},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102019},
  shortjournal = {Data Knowl. Eng.},
  title        = {How to build data-driven strategy maps? a methodological framework proposition},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph data temporal evolutions: From conceptual modelling to
implementation. <em>DKE</em>, <em>139</em>, 102017. (<a
href="https://doi.org/10.1016/j.datak.2022.102017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data management systems are designed for managing highly interconnected data. However, most of the existing work on the topic does not take into account the temporal dimension of such data, even though they may change over time: new interconnections, new internal characteristics of data (etc.). For decision makers , these data changes provide additional insights to explain the underlying behaviour of a business domain. The objective of this paper is to propose a complete solution to manage temporal interconnected data. To do so, we propose a new conceptual model of temporal graphs . It has the advantage of being generic as it captures the different kinds of changes that may occur in interconnected data. We define a set of translation rules to convert our conceptual model into the logical property graph. Based on the translation rules, we implement several temporal graphs according to benchmark and real-world datasets in the Neo4j data store. These implementations allow us to carry out a comprehensive study of the feasibility and usability (through business analyses), the efficiency (saving up to 99\% query execution times comparing to classic approaches) and the scalability of our solution.},
  archive      = {J_DKE},
  author       = {Landy Andriamampianina and Franck Ravat and Jiefu Song and Nathalie Vallès-Parlangeau},
  doi          = {10.1016/j.datak.2022.102017},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102017},
  shortjournal = {Data Knowl. Eng.},
  title        = {Graph data temporal evolutions: From conceptual modelling to implementation},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Matching and analysing conservation–restoration
trajectories. <em>DKE</em>, <em>139</em>, 102015. (<a
href="https://doi.org/10.1016/j.datak.2022.102015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The context of this work is an on-going project at the French National Library (BnF), which aims at providing predictions of the documents physical state based on their conservation–restoration histories. A document can be either in a good state and available to the readers, or damaged and unavailable to them. As libraries may contain millions of documents, the manual monitoring and analysis of their physical state is not realistic in practice. We therefore propose to analyse their conservation histories in order to derive reliable predictions of their physical state. To achieve this goal, we introduce in this paper the following contributions. First, we propose a representation of a document conservation history as a conservation–restoration trajectory, and we define its different types of events. We also propose a trajectory matching process that computes a similarity score between two conservation–restoration trajectories considering the terminological heterogeneity of the events, using an ontological model that represents the domain experts knowledge. Second, we provide a trajectory analysis process which identifies the most representative sequences of events of the deteriorated documents. Finally, we propose a prediction model for the physical state of the documents based on the trajectory analysis process. We present some experiments showing the effectiveness of the matching process as well as the prediction model.},
  archive      = {J_DKE},
  author       = {Alaa Zreik and Zoubida Kedad},
  doi          = {10.1016/j.datak.2022.102015},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102015},
  shortjournal = {Data Knowl. Eng.},
  title        = {Matching and analysing conservation–restoration trajectories},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applying the CRISP-DM data mining process in the financial
services industry: Elicitation of adaptation requirements. <em>DKE</em>,
<em>139</em>, 102013. (<a
href="https://doi.org/10.1016/j.datak.2022.102013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining techniques have gained widespread adoption over the past decades, particularly in the financial services domain. To achieve sustained benefits from these techniques, organizations have adopted standardized processes for managing data mining projects , most notably CRISP-DM. Research has shown that these standardized processes are often not used as prescribed, but instead, they are extended and adapted to address a variety of requirements. To improve the understanding of how standardized data mining processes are extended and adapted in practice, this paper reports on a case study in a financial services organization, aimed at identifying perceived gaps in the CRISP-DM process and characterizing how CRISP-DM is adapted to address these gaps. The case study was conducted based on documentation from a portfolio of data mining projects, complemented by semi-structured interviews with project participants. The results reveal 18 perceived gaps in CRISP-DM alongside their perceived impact and mechanisms employed to address these gaps. The identified gaps are grouped into six categories. Next, they were triangulated and augmented with the gaps discovered in the other studies. Then, the requirements for adapting CRISP-DM to address the gaps were derived, and the directions for the potential adaptations were outlined. The study presents a two-fold contribution. It provides practitioners with a structured set of gaps to be considered when applying CRISP-DM, or similar processes, in the financial services sector. Additionally, the study elicits the requirements and sketches the potential solutions to address these gaps. Also, the number of the identified gaps is generic and applicable to other sectors with similar concerns (e.g. privacy), such as telecom or e-commerce.},
  archive      = {J_DKE},
  author       = {Veronika Plotnikova and Marlon Dumas and Fredrik P. Milani},
  doi          = {10.1016/j.datak.2022.102013},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102013},
  shortjournal = {Data Knowl. Eng.},
  title        = {Applying the CRISP-DM data mining process in the financial services industry: Elicitation of adaptation requirements},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ontologically correct taxonomies by construction.
<em>DKE</em>, <em>139</em>, 102012. (<a
href="https://doi.org/10.1016/j.datak.2022.102012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taxonomies play a central role in conceptual domain modeling, having a direct impact in areas such as knowledge representation, ontology engineering , and software engineering , as well as knowledge organization in information sciences. Despite this, there is little guidance on how to build high-quality taxonomies, with notable exceptions being the OntoClean methodology, and the ontology-driven conceptual modeling language OntoUML. These techniques take into account the ontological meta-properties of types to establish well-founded rules on the formation of taxonomic structures. In this paper, we show how to leverage the formal rules underlying these techniques in order to build taxonomies which are correct by construction . We define a set of correctness-preserving operations to systematically introduce types and subtyping relations into taxonomic structures. In addition to considering the ontological micro-theory of endurant types underlying OntoClean and OntoUML, we also employ the MLT (Multi-Level Theory) micro-theory of high-order types, which allows us to address multi-level taxonomies based on the powertype pattern. To validate our proposal, we formalize the model building operations as a graph grammar that incorporates both micro-theories. We apply automatic verification techniques over the grammar language to show that the graph grammar is sound , i.e., that all taxonomies produced by the grammar rules are correct, at least up to a certain size. We also show that the rules can generate all correct taxonomies up to a certain size (a completeness result).},
  archive      = {J_DKE},
  author       = {Jeferson O. Batista and João Paulo A. Almeida and Eduardo Zambon and Giancarlo Guizzardi},
  doi          = {10.1016/j.datak.2022.102012},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102012},
  shortjournal = {Data Knowl. Eng.},
  title        = {Ontologically correct taxonomies by construction},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forecasting cryptocurrency prices using recurrent neural
network and long short-term memory. <em>DKE</em>, <em>139</em>, 102009.
(<a href="https://doi.org/10.1016/j.datak.2022.102009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of cryptocurrencies over the past decade is one of the most controversial and ambiguous innovations in the modern global economy. Numerous and unpredictable fluctuations in cryptocurrencies rates, as well as the lack of intelligent and proper management of transactions of this type of currency in most developing countries and users of this type of currency, has led to increased risk and distrust of these roses in investors. Capitalists and investors prefer to invest in programs which have the least risk, the most profit and the least time to achieve the main profit. Therefore, the issue of developing appropriate methods and models for predicting the price of cryptographic products is essential both for the scientific community and for financial analysts, investors and traders. In this research, a new deep learning model is used to predict the price of cryptocurrencies. The proposed model uses a Recurrent Neural Networks (RNN) algorithm based on Long Short-Term Memory (LSTM) method to predict the price. In the presented results of the simulation of the proposed method, factors such as the Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), R-Squared (R2) were compared with other similar methods. Finally, the superiority of the proposed method over other methods was proven.},
  archive      = {J_DKE},
  author       = {I. Nasirtafreshi},
  doi          = {10.1016/j.datak.2022.102009},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102009},
  shortjournal = {Data Knowl. Eng.},
  title        = {Forecasting cryptocurrency prices using recurrent neural network and long short-term memory},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OFES: Optimal feature evaluation and selection for
multi-class classification. <em>DKE</em>, <em>139</em>, 102007. (<a
href="https://doi.org/10.1016/j.datak.2022.102007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and accuracy of classification algorithms largely depend on the size and the quality of the feature set used to build classifiers. Feature evaluation and selection are critical steps to decide a small set of high-quality features to build accurate and efficient classifiers since low-quality features not only have negative impacts on classification results but also increase the complexity of classification algorithms. Current popular feature selection algorithms are not sufficient in selecting a set of high-quality features and discarding low-quality features, especially for streaming data. This paper proposes a novel and efficient approach, optimal feature evaluation and selection (OFES), to evaluate and select high-quality features for multi-class classification. OFES first measures the difference between any two classes based on the feature that is to be evaluated. Then, it defines two quantitative measures to evaluate quality of the feature and identify high-quality features. Applying OFES in a multi-class classification application that identifies users based on their arm movement patterns, we find when compared with other popular feature evaluation and selection approaches, such as Information Gain Feature Ranking and Random Projections with Matlab feature ranking, OFES identifies a set of high-quality features that improves the accuracy of classification regardless of different classification algorithms. It also demonstrates great scalability with the increase of number of classes and yields a higher accuracy of 95\%.},
  archive      = {J_DKE},
  author       = {Vallam Sudhakar Sai Ram and Namrata Kayastha and Kewei Sha},
  doi          = {10.1016/j.datak.2022.102007},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102007},
  shortjournal = {Data Knowl. Eng.},
  title        = {OFES: Optimal feature evaluation and selection for multi-class classification},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Location-privacy preserving partial nearby friends querying
in urban areas. <em>DKE</em>, <em>139</em>, 102006. (<a
href="https://doi.org/10.1016/j.datak.2022.102006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the location-privacy preserving proximity querying in the context of proximity-based services (PBSs), a special kind of location-based services (LBSs). The users register with the trusted PBS provider and specify their own personalized location privacy profile to be enforced against the curious friends (other registered users). Due to the urban area constraint, the user mobility is only on the city road network which is modeled as a weighted directed graph . The users share their own precise locations with the PBS provider and also query the nearby friends, the metric of which is defined on the shortest path on the graph. The proposed location privacy model ensures the location anonymity of the friends on the graph. To this end, two anonymity models, called weak location k-anonymity and strong location k-anonymity , are introduced to protect against the identified consecutive attack scenarios. The attack scenarios model the belief of the attacker (the query issuer) on the whereabouts of the friends. The PBS provider simulates the belief of each attacker on every users’ whereabouts and suppresses some friends from the query result to ensure the location anonymity of each and every user at all times. Effective and efficient algorithms, needing no cryptographic protocols , have been developed to provide weak/strong location k-anonymity. An extensive experimental evaluation, mainly addressing the issues of privacy/utility tradeoff and runtime efficiency, on two real graphs with a simulated mobility is presented.},
  archive      = {J_DKE},
  author       = {Osman Abul},
  doi          = {10.1016/j.datak.2022.102006},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102006},
  shortjournal = {Data Knowl. Eng.},
  title        = {Location-privacy preserving partial nearby friends querying in urban areas},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An embedding driven approach to automatically detect
identifiers and references in document stores. <em>DKE</em>,
<em>139</em>, 102003. (<a
href="https://doi.org/10.1016/j.datak.2022.102003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NoSQL stores have become ubiquitous since they offer a new cost-effective and schema-free system. Although NoSQL systems are widely accepted today, Business Intelligence &amp; Analytics (BI&amp;A) wields relational data sources. Exploiting schema-free data for analytical purposes is a challenge since it requires reviewing all the BI&amp;A phases, particularly the Extract-Transform-Load (ETL) process, to fit big data sources as document stores. In the ETL process, the join of several collections, with a lack of explicitly known join fields is a significant dare. Detecting these fields manually is time and effort-consuming and infeasible in large-scale datasets. In this paper, we study the problem of discovering join fields automatically. We introduce an algorithm that aims to automatically detect both identifiers and references on several document stores. The modus operandi of our approach underscores three core stages: (i) global schema extraction; (ii) discovery of candidate identifiers ; and (iii) identifying candidate pairs of identifier and reference fields. We use scoring features and pruning rules to discover true candidate identifiers from many initial ones efficiently. To find candidate pairs between several document stores, we put into practice node2vec as a graph embedding technique, which yields significant advantages while using syntactic and semantic similarity measures for pruning pointless candidates. Finally, we report our experimental findings that show encouraging results.},
  archive      = {J_DKE},
  author       = {Manel Souibgui and Faten Atigui and Sadok Ben Yahia and Samira Si-Said Cherfi},
  doi          = {10.1016/j.datak.2022.102003},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102003},
  shortjournal = {Data Knowl. Eng.},
  title        = {An embedding driven approach to automatically detect identifiers and references in document stores},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entity alignment with adaptive margin learning knowledge
graph embedding. <em>DKE</em>, <em>139</em>, 101987. (<a
href="https://doi.org/10.1016/j.datak.2022.101987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of knowledge graphs have been constructed at present. However, there is diversity and heterogeneity among different knowledge graphs. The relation and attribute of the knowledge graph contain rich semantic information, which helps construct the potential semantic representation of the knowledge graph. At present, the method based on knowledge representation is an important method of entity alignment, which can align entities by transforming them into spatial vectors. And it helps to reduce the heterogeneity among different knowledge domains. However, existing methods use the same optimization goal for triples under different relations, ignoring the difference between relationships. In this article, we put forward a kind of entity alignment method based on the TransE model and use adaptive margin strategies in training. At the same time, this paper studies the LSTM encoder model and the BERT pre-training model in the application of entity alignment. To enhance the model’s robustness, we put forward the triple selection strategy based on attribute similarity. Experimental results on real datasets show that this method is significantly improved compared with the baseline model .},
  archive      = {J_DKE},
  author       = {Linshan Shen and Rongbo He and Shaobin Huang},
  doi          = {10.1016/j.datak.2022.101987},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101987},
  shortjournal = {Data Knowl. Eng.},
  title        = {Entity alignment with adaptive margin learning knowledge graph embedding},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FaNDS: Fake news detection system using energy flow.
<em>DKE</em>, <em>139</em>, 101985. (<a
href="https://doi.org/10.1016/j.datak.2022.101985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the term “fake news” has been broadly and extensively utilized for disinformation, misinformation , hoaxes, propaganda, satire, rumors, click-bait, and junk news. It has become a serious problem around the world. We present a new system, FaNDS, that detects fake news efficiently. The system is based on several concepts used in some previous works but in a different context. There are two main concepts: an Inconsistency Graph and Energy Flow. The Inconsistency Graph contains news items as nodes and inconsistent opinions between them for edges. Energy Flow assigns each node an initial energy and then some energy is propagated along the edges until the energy distribution on all nodes converges. To illustrate FaNDS we use the original data from the Fake News Challenge (FNC-1). First, the data has to be reconstructed in order to generate the Inconsistency Graph. The graph contains various subgraphs with well-defined shapes that represent different types of connections between the news items. Then the Energy Flow method is applied. The nodes with high energy are the candidates for being fake news. In our experiments, all these were indeed fake news as we checked each using several reliable web sites. We compared FaNDS to several other fake news detection methods and found it to be more sensitive in discovering fake news items.},
  archive      = {J_DKE},
  author       = {Jiawei Xu and Vladimir Zadorozhny and Danchen Zhang and John Grant},
  doi          = {10.1016/j.datak.2022.101985},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101985},
  shortjournal = {Data Knowl. Eng.},
  title        = {FaNDS: Fake news detection system using energy flow},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chinese named-entity recognition via self-attention
mechanism and position-aware influence propagation embedding.
<em>DKE</em>, <em>139</em>, 101983. (<a
href="https://doi.org/10.1016/j.datak.2022.101983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese Named Entity Recognition (NER) has received extensive research attention in recent years. However, Chinese texts lack delimiters to divide the boundaries of words, and some existing approaches cannot capture the long-distance interdependent features. In this paper, we propose a novel end-to-end model for Chinese NER. A new global word boundary detection approach is designed to capture the semantic dependency via a self-attention mechanism to represent character embedding by assigning compatible weights for each character in a sentence. To improve the representation ability of Chinese named-entity boundaries, we introduce position-aware influence propagation with the Gaussian kernel for each character, which combines convergence propagation and radiation propagation. Convergence propagation mainly measures the influence of surrounding characters on the target character. The purpose of radiation propagation is to measure the range of influence of the target character on surrounding characters. The proposed method has been evaluated and shown to offer strong performance in two Chinese NER datasets: MSRA and PFR.},
  archive      = {J_DKE},
  author       = {Bo Zhang and Kehao Liu and Haowen Wang and Maozhen Li and Jianguo Pan},
  doi          = {10.1016/j.datak.2022.101983},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101983},
  shortjournal = {Data Knowl. Eng.},
  title        = {Chinese named-entity recognition via self-attention mechanism and position-aware influence propagation embedding},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sense, transform &amp; send for the internet of things
(STS4IoT): UML profile for data-centric IoT applications. <em>DKE</em>,
<em>139</em>, 101971. (<a
href="https://doi.org/10.1016/j.datak.2021.101971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT , a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses .},
  archive      = {J_DKE},
  author       = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe de Vaulx and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
  doi          = {10.1016/j.datak.2021.101971},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101971},
  shortjournal = {Data Knowl. Eng.},
  title        = {Sense, transform &amp; send for the internet of things (STS4IoT): UML profile for data-centric IoT applications},
  volume       = {139},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preface. <em>DKE</em>, <em>138</em>, 101994. (<a
href="https://doi.org/10.1016/j.datak.2022.101994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Elisabeth Métais and Farid Meziane and Helmut Horacek and Philipp Cimiano},
  doi          = {10.1016/j.datak.2022.101994},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101994},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering sequence graphs. <em>DKE</em>, <em>138</em>,
101981. (<a href="https://doi.org/10.1016/j.datak.2022.101981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In application domains ranging from social networks to e-commerce, it is important to cluster users with respect to both their relationships (e.g., friendship or trust) and their actions (e.g., visited locations or rated products). Motivated by these applications, we introduce here the task of clustering the nodes of a sequence graph , i.e., a graph whose nodes are labeled with strings (e.g., sequences of users’ visited locations or rated products). Both string clustering algorithms and graph clustering algorithms are inappropriate to deal with this task, as they do not consider the structure of strings and graph simultaneously. Moreover, attributed graph clustering algorithms generally construct poor solutions because they need to represent a string as a vector of attributes, which inevitably loses information and may harm clustering quality . We thus introduce the problem of clustering a sequence graph. We first propose two pairwise distance measures for sequence graphs, one based on edit distance and shortest path distance and another one based on SimRank. We then formalize the problem under each measure, showing also that it is NP-hard. In addition, we design a polynomial-time 2-approximation algorithm, as well as a heuristic for the problem. Experiments using real datasets and a case study demonstrate the effectiveness and efficiency of our methods.},
  archive      = {J_DKE},
  author       = {Haodi Zhong and Grigorios Loukides and Solon P. Pissis},
  doi          = {10.1016/j.datak.2022.101981},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101981},
  shortjournal = {Data Knowl. Eng.},
  title        = {Clustering sequence graphs},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extraction and analysis of text patterns from NSFW adult
content in reddit. <em>DKE</em>, <em>138</em>, 101979. (<a
href="https://doi.org/10.1016/j.datak.2022.101979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reddit is one of the few social networks that handles Not Safe For Work (NSFW) content in an explicit and well-structured way. Despite this, in the past literature on Reddit, there are very few researches concerning this topic. In particular, a study on the text of NSFW comments and posts published in this social medium is missing. In this paper, we aim at contributing to fill this gap by proposing an approach for extracting and analyzing text patterns from NSFW adult content in Reddit. Some peculiarities of our approach are the following: (i) text patterns are extracted based not only on frequency but also, and mostly, on several utility measures; (ii) extracted patterns contribute to the definition of social networks whose analysis allows us to extract several useful information about the users publishing and/or accessing NSFW content and the language adopted by them; (iii) our approach is not only descriptive but also predictive, because, in addition to identifying already existing user communities, it is able to propose new ones; these are made up of users who do not yet know each other but share the same interests and the same language.},
  archive      = {J_DKE},
  author       = {Francesco Cauteruccio and Enrico Corradini and Giorgio Terracina and Domenico Ursino and Luca Virgili},
  doi          = {10.1016/j.datak.2022.101979},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101979},
  shortjournal = {Data Knowl. Eng.},
  title        = {Extraction and analysis of text patterns from NSFW adult content in reddit},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A core ontology on the human–computer interaction
phenomenon. <em>DKE</em>, <em>138</em>, 101977. (<a
href="https://doi.org/10.1016/j.datak.2021.101977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–Computer Interaction (HCI) is a complex communication phenomenon involving human beings and computer systems that gained large attention from industry and academia with the advent of new types of interactive systems (mobile applications, smart cities, smart homes, ubiquitous systems and so on). Despite of its importance, there is still a lack of formal and explicit representations of what the HCI phenomenon is. In this paper, we intend to clarify the main notions involved in the HCI phenomenon, by establishing an explicit conceptualization of it. To do so, we need to understand what interactive computer systems are, which types of actions users perform when interacting with an interactive computer system, and finally what human–computer interaction itself is. The conceptualization is presented as a core reference ontology, called HCIO (HCI Ontology), which is grounded in the Unified Foundational Ontology (UFO). HCIO was evaluated using ontology verification and validation techniques and has been used as core ontology of an HCI ontology network.},
  archive      = {J_DKE},
  author       = {Simone Dornelas Costa and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and Tayana Conte and Káthia M. de Oliveira},
  doi          = {10.1016/j.datak.2021.101977},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101977},
  shortjournal = {Data Knowl. Eng.},
  title        = {A core ontology on the Human–Computer interaction phenomenon},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aligning and linking entity mentions in image, text, and
knowledge base. <em>DKE</em>, <em>138</em>, 101975. (<a
href="https://doi.org/10.1016/j.datak.2021.101975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A picture is worth a thousand words, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed reveal different and complementary information that, if combined will result in more information than the sum of that contained in a single media. The combination of visual and textual information can be obtained by linking the entities mentioned in the text with those shown in the pictures. To further integrate this with the agent’s background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL) . In this article, after providing a precise definition of the VTKEL task, we present two datasets called VTKEL1k* and VTKEL30k. These datasets consisting of images and corresponding captions, in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology. The datasets can be used for training and evaluating algorithms of the VTKEL task. Successively, we introduce a baseline algorithm called VT-LinKEr (Visual-Textual-Knowledge Entity Linker) for the solution of the VTKEL task. We evaluate the performances of VT-LinKEr on both datasets. We then contribute a supervised algorithm called ViTKan (Visual-Textual-Knowledge Alignment Network) . We trained the ViTKan algorithm using features data of the VTKEL1k* dataset. The experimental results on VTKEL1k* and VTKEL30k datasets show that ViTKan substantially outperforms the baseline algorithm.},
  archive      = {J_DKE},
  author       = {Shahi Dost and Luciano Serafini and Marco Rospocher and Lamberto Ballan and Alessandro Sperduti},
  doi          = {10.1016/j.datak.2021.101975},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101975},
  shortjournal = {Data Knowl. Eng.},
  title        = {Aligning and linking entity mentions in image, text, and knowledge base},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating character-level and word-level representation
for affect in arabic tweets. <em>DKE</em>, <em>138</em>, 101973. (<a
href="https://doi.org/10.1016/j.datak.2021.101973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affect tasks, which range from sentiment polarity classification to finer grained sentiment strength and emotional intensity detection, have become of increasing interest due to the vast amount of user-generated content and advanced learning models. Word representation models have been leveraged effectively within a variety of natural language processing tasks. However, these models are not always effective in the context of social media. When dealing with social media posts in Arabic, the use of Arabic dialects needs to be considered. Although using informal text to train word-level models can lead to the identification of words that convey the same meaning, these models are unable to capture the full extent of the words that are used in the real world due to out-of-vocabulary (OOV) words. The inability to identify such words is one of the main limitations of word-level models. One approach of overcoming OOV is through the use of character-level embeddings as they can effectively learn the vectors of word parts or character n-grams. This study uses a combination of character-level and word-level models to identify the most effective methods by which affective Arabic words in tweets can be represented semantically and morphologically. We evaluate our generated models and the proposed method by integrating them in a supervised learning framework that was used for a range of affect tasks and other related tasks. Our findings reveal that the developed models surpassed the performance of state-of-the-art Arabic pre-trained word embeddings over eight datasets. In addition, our models enhance previous state-of-the-art outcomes on tasks involving Arabic emotion intensity, outperforming the top-systems that used advanced ensemble learning models and several additional features.},
  archive      = {J_DKE},
  author       = {Abdullah I. Alharbi and Phillip Smith and Mark Lee},
  doi          = {10.1016/j.datak.2021.101973},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101973},
  shortjournal = {Data Knowl. Eng.},
  title        = {Integrating character-level and word-level representation for affect in arabic tweets},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning english and arabic question similarity with siamese
neural networks in community question answering services. <em>DKE</em>,
<em>138</em>, 101962. (<a
href="https://doi.org/10.1016/j.datak.2021.101962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the task of similar question retrieval (QR) which is essential for Community Question Answering (cQA) and aims to retrieve historical questions that are semantically equivalent to the new queries. Over time, with the sharp increase of community archives and the accumulation of duplicated questions, the QR problem has become increasingly challenging due to the shortness of the community questions as well as the word mismatch problem as users can formulate the same query using different wording. Although many efforts have been devoted to address this problem, existing methods mostly relied on supervised models which significantly depend on massive training data sets and manual feature engineering. Such methods are chiefly constrained by their specificities that ignore the word order and do not capture enough syntactic and semantic information in questions. In this paper, we rely on Neural Networks (NNs) which use a deep analysis of words and questions to take into consideration the semantics as well as the structure of questions to predict the semantic text similarity. We propose a deep learning approach based on a Siamese architecture with Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism to let the model give different words different attention while modeling questions. We also explore the use of Convolutional Neural Networks (CNN) nested within the Siamese architecture to retrieve relevant questions. Different similarity measures were tested to predict the semantic similarity between the pairs of questions. To evaluate the proposed approach, we conducted experiments on large-scale datasets in English and Arabic.},
  archive      = {J_DKE},
  author       = {Nouha Othman and Rim Faiz and Kamel Smaïli},
  doi          = {10.1016/j.datak.2021.101962},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101962},
  shortjournal = {Data Knowl. Eng.},
  title        = {Learning english and arabic question similarity with siamese neural networks in community question answering services},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The impact of psycholinguistic patterns in discriminating
between fake news spreaders and fact checkers. <em>DKE</em>,
<em>138</em>, 101960. (<a
href="https://doi.org/10.1016/j.datak.2021.101960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news is a threat to society. A huge amount of fake news is posted every day on social networks which is read, believed and sometimes shared by a number of users. On the other hand, with the aim to raise awareness, some users share posts that debunk fake news by using information from fact-checking websites. In this paper, we are interested in exploring the role of various psycholinguistic characteristics in differentiating between users that tend to share fake news and users that tend to debunk them. Psycholinguistic characteristics represent the different linguistic information that can be used to profile users and can be extracted or inferred from users’ posts. We present the CheckerOrSpreader model that uses a Convolution Neural Network (CNN) to differentiate between spreaders and checkers of fake news. The experimental results showed that CheckerOrSpreader is effective in classifying a user as a potential spreader or checker. Our analysis showed that checkers tend to use more positive language and a higher number of terms that show causality compared to spreaders who tend to use a higher amount of informal language, including slang and swear words.},
  archive      = {J_DKE},
  author       = {Anastasia Giachanou and Bilal Ghanem and Esteban A. Ríssola and Paolo Rosso and Fabio Crestani and Daniel Oberski},
  doi          = {10.1016/j.datak.2021.101960},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101960},
  shortjournal = {Data Knowl. Eng.},
  title        = {The impact of psycholinguistic patterns in discriminating between fake news spreaders and fact checkers},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OLAP patterns: A pattern-based approach to multidimensional
data analysis. <em>DKE</em>, <em>138</em>, 101948. (<a
href="https://doi.org/10.1016/j.datak.2021.101948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of a business intelligence (BI) system employ an approach referred to as online analytical processing (OLAP) to view multidimensional data from different perspectives. Query languages , e.g., SQL or MDX, allow for flexible querying of multidimensional data but query formulation is often time-consuming and cognitively challenging for many users. Alternatives to using a query language, e.g., graphical OLAP clients, parameterized reports, or dashboards, are often not a full-blown alternative to using a query language. Experience in cooperative research projects with industry led to the following observations regarding the use of OLAP queries in practice. First, within the same organization, similar OLAP queries are repeatedly composed from scratch in order to satisfy similar information needs. Second, across different organizations and even domains, OLAP queries with similar structures are repeatedly composed from scratch. Finally, vague requirements regarding frequently composed OLAP queries in the early stages of a project potentially lead to rushed development in later stages, which can be alleviated by following best practices for OLAP query composition. In engineering, knowledge about best-practice solutions to frequently arising challenges is often documented and represented using patterns. In that spirit, an OLAP pattern describes a generic solution for composing a query that allows a BI user to satisfy a certain type of information need given fragments of a conceptual model. This paper introduces a formal definition of OLAP patterns as well as an expressive, flexible, and generally applicable definition language.},
  archive      = {J_DKE},
  author       = {Ilko Kovacic and Christoph G. Schuetz and Bernd Neumayr and Michael Schrefl},
  doi          = {10.1016/j.datak.2021.101948},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101948},
  shortjournal = {Data Knowl. Eng.},
  title        = {OLAP patterns: A pattern-based approach to multidimensional data analysis},
  volume       = {138},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selecting databases for polyglot persistence applications.
<em>DKE</em>, <em>137</em>, 101950. (<a
href="https://doi.org/10.1016/j.datak.2021.101950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, new types of databases emerged, most notably NoSQL databases. Within this family of databases, there are specific models, such as document-based, graph-based, and more, each of which, along with the relational model , may be best suited to particular applications. Therefore, the issue of which database model to select for a given application is essential. Nowadays, the selection of a database model is not based on systematic methods that consider the specific requirements and characteristics of the application. This paper proposes a structured method for database model selection that considers various factors, including data-related, functional, and non-functional requirements. Based on these factors, the method recommends the most appropriate database models for the application. We discuss the sensitivity of the method and evaluate it via several case studies.},
  archive      = {J_DKE},
  author       = {Noa Roy-Hubara and Peretz Shoval and Arnon Sturm},
  doi          = {10.1016/j.datak.2021.101950},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101950},
  shortjournal = {Data Knowl. Eng.},
  title        = {Selecting databases for polyglot persistence applications},
  volume       = {137},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly explanation: A review. <em>DKE</em>, <em>137</em>,
101946. (<a href="https://doi.org/10.1016/j.datak.2021.101946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has been studied intensively by the data mining community for several years. As a result, many methods to detect anomalies have emerged, and others are still under development. But during the recent years, anomaly detection, just like a lot of machine learning tasks, is facing a wall. This wall, erected by the lack of trust of the final users, has slowed down the usage of these algorithms in the real-world situations for which they are designed. Having the best empirical accuracy is not enough anymore; there is a need for algorithms to explain their outputs to the users in order to increase their trust. Consequently, a new expression has emerged recently: eXplainable Artificial Intelligence (XAI). This expression, which gathers all the methods that provide explanations to the output of algorithms has gained popularity, especially with the outbreak of deep learning . A lot of work has been devoted to anomaly detection in the literature, but not as much to anomaly explanation. There is so much work on anomaly detection that several reviews can be found on the topic. In contrast, we were not able to find a survey on anomaly explanation in particular, while there are a lot of surveys on XAI in general or on XAI for neural networks for example. With this paper, we want to provide a comprehensive review of the anomaly explanation field. After a brief recall of some important anomaly detection algorithms, the anomaly explanation methods that we discovered in the literature will be classified according to a taxonomy that we define. This taxonomy stems from an analysis of what is really important when trying to explain anomalies.},
  archive      = {J_DKE},
  author       = {Véronne Yepmo and Grégory Smits and Olivier Pivert},
  doi          = {10.1016/j.datak.2021.101946},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101946},
  shortjournal = {Data Knowl. Eng.},
  title        = {Anomaly explanation: A review},
  volume       = {137},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SQL query extensions for imprecise questions. <em>DKE</em>,
<em>137</em>, 101944. (<a
href="https://doi.org/10.1016/j.datak.2021.101944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the big data tsunami, relational databases and SQL remain inescapable in most cases for accessing data. If SQL is easy-to-use and has proved its robustness over the years, it is not always easy to formulate SQL queries as it is more and more frequent to have databases with hundreds of tables and/or attributes. Identifying the pertinent conditions to select the desired data, or even the relevant attributes, is not trivial, especially when the user only has an imprecise question in mind, and is not sure of how to translate its conditions directly into SQL. To make it easier to write SQL queries when the initial question is imprecise, we propose SQL query extensions: given a query, it suggests several possible additional selection clauses, to complete the Where clause of the query, as a form of SQL query semantic autocompletion. This is helpful for both understanding the initial query’s results, and refining the query to reach the desired tuples. The process is iterative, as a query constructed using an extension can also be completed. It is also adaptable, as the number of extensions to compute is flexible. A prototype has been implemented in a SQL editor on top of a database management system , and two types of evaluation are proposed. A first one looks at the scaling of the system with a large number of tuples. Then a user study examines two questions: does the extension tool speed up the writing of SQL queries? And is it easily adopted by users? A thorough experiment was conducted on a group of 70 computer science students divided in two groups (one with the extension tool and the other one without) to answer those questions. In the end, the results showed a faster answering time for students that could use the extensions: 32 min on average to complete the test for the group with extensions, against 48 min for the others.},
  archive      = {J_DKE},
  author       = {Marie Le Guilly and Jean-Marc Petit and Vasile-Marian Scuturici},
  doi          = {10.1016/j.datak.2021.101944},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101944},
  shortjournal = {Data Knowl. Eng.},
  title        = {SQL query extensions for imprecise questions},
  volume       = {137},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LEAPME: Learning-based property matching with embeddings.
<em>DKE</em>, <em>137</em>, 101943. (<a
href="https://doi.org/10.1016/j.datak.2021.101943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration tasks such as the creation and extension of knowledge graphs involve the fusion of heterogeneous entities from many sources. Matching and fusion of such entities require to also match and combine their properties (attributes). However, previous schema matching approaches mostly focus on two sources only and often rely on simple similarity measurements. They thus face problems in challenging use cases such as the integration of heterogeneous product entities from many sources. We therefore present a new machine learning-based property matching approach called LEAPME (LEArning-based Property Matching with Embeddings) that utilizes numerous features of both property names and instance values. The approach heavily makes use of word embeddings to better utilize the domain-specific semantics of both property names and instance values. The use of supervised machine learning helps exploit the predictive power of word embeddings. Our comparative evaluation against five baselines for several multi-source datasets with real-world data shows the high effectiveness of LEAPME. We also show that our approach is even effective when training data from another domain (transfer learning) is used.},
  archive      = {J_DKE},
  author       = {Daniel Ayala and Inma Hernández and David Ruiz and Erhard Rahm},
  doi          = {10.1016/j.datak.2021.101943},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101943},
  shortjournal = {Data Knowl. Eng.},
  title        = {LEAPME: Learning-based property matching with embeddings},
  volume       = {137},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
