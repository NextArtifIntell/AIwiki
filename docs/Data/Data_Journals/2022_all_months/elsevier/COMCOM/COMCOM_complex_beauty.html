<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom---398">COMCOM - 398</h2>
<ul>
<li><details>
<summary>
(2022). Air-to-ground path loss prediction using ray tracing and
measurement data jointly driven DNN. <em>COMCOM</em>, <em>196</em>,
268–276. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide application of unmanned aerial vehicle (UAV), air-to-ground (A2G) channel characterization is important for efficient and stable UAV-related communications. In this paper, a novel deep neural network (DNN) based path loss (PL) prediction model is presented for A2G communications. The new model considers the effects of path delay, carrier frequency, and reflection angle (RA) of non-line-of-sight (NLoS) paths. Specifically, a measurement-optimization-matrix (MOM) based DNN with multiple input neurons is designed. In order to solve the problem of insufficient measurement data, the new DNN is divided into two parts, i.e., initial-trained network and optimized network. The massive ray tracing (RT) simulation data is used to initially train the network and a little measurement data is used to further optimize the network. Finally, the proposed model is simulated and validated under a campus scenario at 2 GHz, 26 GHz, and 39 GHz, respectively. The predicted results are in good agreement with the validation set of RT simulation and measurement data. Moreover, our proposed model is consistent with the close-in (CI) model and 3rd Generation Partnership Project (3GPP) model under traditional terrestrial scenarios with small RAs, but it is also applicable for A2G scenarios with all frequency bands and large RAs.},
  archive      = {J_COMCOM},
  author       = {Hanpeng Li and Xiaomin Chen and Kai Mao and Qiuming Zhu and Yanheng Qiu and Xijuan Ye and Weizhi Zhong and Zhipeng Lin},
  doi          = {10.1016/j.comcom.2022.10.007},
  journal      = {Computer Communications},
  pages        = {268-276},
  shortjournal = {Comput. Commun.},
  title        = {Air-to-ground path loss prediction using ray tracing and measurement data jointly driven DNN},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). PQKLP: Projected quantum kernel based link prediction in
dynamic networks. <em>COMCOM</em>, <em>196</em>, 249–267. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction in dynamic networks finds new or future links based on the previously seen structure of the network. Its study is crucial to comprehending network evolution and its effects on individual nodes. Accuracy and efficiency of link prediction on dynamic networks are the two aspects research. We present Projected Quantum Kernel-based Link Prediction ( P Q K L P PQKLP ), a quantum-enhanced feature-based framework for solving link prediction problems in dynamic networks. According to our study, the Projected Quantum Kernel has not been utilized in the field of link prediction. Thus, we propose this method that combines the disciplines of social networks and quantum computing . We employed high-dimensional Hilbert spaces to enhance the prediction data in this model, which otherwise we only have access to via inner products provided by measurements. Such enhancement leads to better prediction results from machine learning-based link prediction techniques. We trained six classical machine learning models and their quantum-enhanced counterparts based on the enhanced features generated by the Projected Quantum Kernel ( P Q K PQK ) technique. The proposed model outperforms traditional link prediction methods, classical machine learning approaches , and current state-of-the-art methods on five well-known dynamic network datasets, as per the results of four performance metrics.},
  archive      = {J_COMCOM},
  author       = {Mukesh Kumar and Shivansh Mishra and Bhaskar Biswas},
  doi          = {10.1016/j.comcom.2022.10.006},
  journal      = {Computer Communications},
  pages        = {249-267},
  shortjournal = {Comput. Commun.},
  title        = {PQKLP: Projected quantum kernel based link prediction in dynamic networks},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Priority based dynamic spectrum management using virtual
utility functions in cognitive radio enabled internet of things.
<em>COMCOM</em>, <em>196</em>, 239–248. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast growth of wireless devices in IoT (Internet of Things), industry 5.0 and 6G cellular networks , is dealing with the challenge of spectrum scarcity and uneven utilization of available resources. To achieve Ambient Intelligence (AmI) in IoT communication network, dynamic utilization of available network resources, is a challenge faced by resource constraint IoT devices. Cognitive radio technology enables these devices to intelligently sense the free spectrum holes and smartly manage available channels opportunistically. To jointly deal with the challenge of spectrum scarcity and application specific network service provision, we propose a novel multi-attribute based two-tier framework called Cognitive Radio Efficient Spectrum Utilization (CR-ESU) which integrates the concept of Network Function Virtualization (NFV) as Virtual Utility Functions (VUFs) with cognitive radio enabled IoT devices to achieve the dynamic and flexible network services as well as spectral efficiency. The proposed framework is one of the pioneer works that incorporate the two ground breaking technologies, NFV and CRN to create a smart and flexible CR-enabled IoT network. CR-ESU also ensures the best channel selection using Dynamic Channel Reservation (DCR) scheme to decrease the channel switching rate. Continuous Time Markov Chain (CTMC) model is used for network modeling and mathematical expression formation. Promising results show the potential of CR-ESU framework to form a reconfigurable and efficient network.},
  archive      = {J_COMCOM},
  author       = {Laraib Abbas and Umar Shoaib and Ali Kashif Bashir},
  doi          = {10.1016/j.comcom.2022.10.002},
  journal      = {Computer Communications},
  pages        = {239-248},
  shortjournal = {Comput. Commun.},
  title        = {Priority based dynamic spectrum management using virtual utility functions in cognitive radio enabled internet of things},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint beamforming design and resource allocation for
double-IRS-assisted RSMA SWIPT systems. <em>COMCOM</em>, <em>196</em>,
229–238. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to achieve high passive beamforming gains cost-effectively, intelligent reflecting surface (IRS) is envisioned as a promising technique in assisting simultaneous wireless information and power transfer (SWIPT). Different from existing works that consider deploying a single IRS or applying conventional multiple access schemes like space division multiple access (SDMA) and non-orthogonal multiple access (NOMA), we consider a double-IRS-assisted multi-user SWIPT system applying the rate-splitting multiple access (RSMA) scheme in this paper. To maximize the minimum achievable rate of all users and guarantee their minimum energy harvesting amount, we jointly optimize the transmit beamforming for both the common and private messages at the access point of the system, the reflecting beamforming of the two IRSs, as well as the power splitting coefficients and the individual common rates of all users. Although the considered optimization problem is difficult to solve due to its coupling variables and non-convex structure, we propose an efficient alternative optimization-based algorithm to obtain a high-quality solution to it. Simulation results validate the effectiveness of the proposed algorithm and demonstrate the superior performance of deploying two IRSs and applying RSMA, as compared to the baseline schemes that deploy a single IRS or use SDMA/NOMA.},
  archive      = {J_COMCOM},
  author       = {Haijian Pang and Miao Cui and Guangchi Zhang and Qingqing Wu},
  doi          = {10.1016/j.comcom.2022.10.004},
  journal      = {Computer Communications},
  pages        = {229-238},
  shortjournal = {Comput. Commun.},
  title        = {Joint beamforming design and resource allocation for double-IRS-assisted RSMA SWIPT systems},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive formal parallel technique with reputation
integration for the enforcement of security policy in the cloud
environment. <em>COMCOM</em>, <em>196</em>, 207–228. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting sensitive data from unauthorized users is a major challenge while sharing data via cloud storage. This has been the objective of mechanisms to control access. This paper proposes an adaptive formal technique for security policy enforcement in Cloud environment. In this approach, the actions of the user performed in the cloud are modeled as a process algebra expression, with a new variant of Algebra Communication Process ACP , ACP, with reputation integration. Security policies are expressed by logical formula. Our system enables us to check whether the process meets the security policy and the reputation limit that is required. If it does not, automatic enforcement generates a new process that satisfies a security policy. To prove the efficacy of our security policy enforcement, a software prototype has been implemented and evaluate. The results show a decrease in computation costs and an improvement in cloud defense against insider attacks. It demonstrates also, that our solution outperforms SKMFA-SC (Prabha and Saraswathi, 2020), SEAPP (Hu et al., 2021), FDAC-TR (Yan et al., 2017), RMTAC (Lin et al., 2015), RRAC (Amoon et al., 2020) and DTRM (Lin et al., 2018) in terms of unauthorized access blocking rate, false positive and negative rates, average reputation evaluation accuracy rate, and average response system time .},
  archive      = {J_COMCOM},
  author       = {Faiza Benmenzer and Rachid Beghdad},
  doi          = {10.1016/j.comcom.2022.09.023},
  journal      = {Computer Communications},
  pages        = {207-228},
  shortjournal = {Comput. Commun.},
  title        = {An adaptive formal parallel technique with reputation integration for the enforcement of security policy in the cloud environment},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WOGRU-IDS — an intelligent intrusion detection system for
IoT assisted wireless sensor networks. <em>COMCOM</em>, <em>196</em>,
195–206. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key mechanisms of the current electronic and wireless frameworks is the assistance of Wireless Sensor Networks (WSN) in Internet of Things (IoT) networks. A WSN typically consists of multipurpose sensor hubs for data sensing, processing, and communication. These networks are more suited to conveying medical data from various geographical regions and sending private medical data to the network owner. However, the worry about various attacks on health care data normally grows daily. These assaults could quickly have adverse impacts on the WSN-IoT (Internet of Things) nodes. Additionally, the low detection rate, significant processing overhead, and resource limitations of current intrusion detection systems all contribute to an increase in false alarm rates when trying to identify various attacks. The unique Whale Optimized Gate Recurrent Unit (WOGRU) Intrusion Detection System (IDS) for WSN-IoT networks is proposed in this research in light of the aforementioned issues in order to effectively identify various attacks. The whale algorithm was used in the proposed framework to tune the hyperparameters of the deep long short-term memory in order to achieve low computational overhead and great performance. Last but not least, validations are carried out using the WSN-DS dataset, and the performance of the suggested work is evaluated using the parameters accuracy, recall, precision, specificity, and F1-score. Additionally, the comparison study was conducted using the current frameworks. The data demonstrates that the suggested framework had an average performance of 99.85 percent for the detection of flooding, scheduling, black hole, and gray hole attacks.},
  archive      = {J_COMCOM},
  author       = {Kadiyala Ramana and A. Revathi and A. Gayathri and Rutvij H. Jhaveri and C.V. Lakshmi Narayana and B. Naveen Kumar},
  doi          = {10.1016/j.comcom.2022.10.001},
  journal      = {Computer Communications},
  pages        = {195-206},
  shortjournal = {Comput. Commun.},
  title        = {WOGRU-IDS — an intelligent intrusion detection system for IoT assisted wireless sensor networks},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning meets graph neural networks:
Exploring a routing optimization use case. <em>COMCOM</em>,
<em>196</em>, 184–194. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) has shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve many relevant optimization problems (e.g., routing) in self-driving networks. However, existing DRL-based solutions applied to networking fail to generalize, which means that they are not able to operate properly when applied to network topologies not observed during training. This lack of generalization capability significantly hinders the deployment of DRL technologies in production networks. This is because state-of-the-art DRL-based networking solutions use standard neural networks (e.g., fully connected, convolutional), which are not suited to learn from information structured as graphs. In this paper, we integrate Graph Neural Networks (GNN) into DRL agents and we design a problem specific action space to enable generalization. GNNs are Deep Learning models inherently designed to generalize over graphs of different sizes and structures. This allows the proposed GNN-based DRL agent to learn and generalize over arbitrary network topologies . We test our DRL+GNN agent in a routing optimization use case in optical networks and evaluate it on 180 and 232 unseen synthetic and real-world network topologies respectively. The results show that the DRL+GNN agent is able to outperform state-of-the-art solutions in topologies never seen during training.},
  archive      = {J_COMCOM},
  author       = {Paul Almasan and José Suárez-Varela and Krzysztof Rusek and Pere Barlet-Ros and Albert Cabellos-Aparicio},
  doi          = {10.1016/j.comcom.2022.09.029},
  journal      = {Computer Communications},
  pages        = {184-194},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decision fusion for multi-route and multi-hop wireless
sensor networks over the binary symmetric channel. <em>COMCOM</em>,
<em>196</em>, 167–183. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision fusion for multi-route and multi-hop Wireless Sensor Networks (WSNs) is studied, wherein a discrete memoryless channel model, i.e., the Binary Symmetric Channel (BSC), is considered to characterize the relay transmission of each hop from the local sensor to the fusion center. In particular, we first develop the optimal log-likelihood ratio (LLR) based decision fusion rule, wherein the fusion center is assumed to have perfect knowledge of both the local sensor performance indices and the Channel State Information (CSI), i.e., crossover probability for each BSC. Secondly, we derive the suboptimum and robust fusion rules for two cases. In the first case, channel condition from the source to the local sensor is considered to be ideal. In the second case, the crossover probability for each BSC is assumed to be relatively large or small. Our result show that our suboptimum fusion detectors require less or no a priori information about crossover probability and/or the local sensor performance indices, and thus are easy to implement. We also show that the simple decision fusion statistic, i.e., the counting-based statistic, can be directly derived from the optimal LLR-based statistic for both cases. These suboptimum fusion rules are clearly desired for applications, wherein perfect estimation of the local sensor performance and CSI is complexity-intensive or unachievable. Furthermore, the optimal LLR-based scheme for joint decision fusion and CSI estimation is proposed. We uniformly quantize the equivalent crossover probability into discrete status, and thus give a suboptimum but more computationally practical scheme. The performance evaluation is finally developed both analytically and through simulation.},
  archive      = {J_COMCOM},
  author       = {Gaoyuan Zhang and Kai Chen and Congfang Ma and Sravan Kumar Reddy and Baofeng Ji and Yongen Li and Congzheng Han and Xiaohui Zhang and Zhumu Fu},
  doi          = {10.1016/j.comcom.2022.09.025},
  journal      = {Computer Communications},
  pages        = {167-183},
  shortjournal = {Comput. Commun.},
  title        = {Decision fusion for multi-route and multi-hop wireless sensor networks over the binary symmetric channel},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on accelerating technologies for fast network
packet processing in linux environments. <em>COMCOM</em>, <em>196</em>,
148–166. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The path a packet takes when handled by the Linux Kernel has been well established for a long time. Its overhead/bottleneck issues are also known. Nonetheless, complexity has increased with the introduction of new paradigms such as virtual machines, containers, software-defined networks, multicore CPU architectures, etc. Coupled with the need for low delay and high bandwidth services, multiple technologies emerged in an attempt to solve these issues, generally known as Fast Packet Processing Frameworks. However, each technology provides a better network processing in different ways, using different methods and solutions, yielding to different benefits and trade-offs. There is a need to shed light not only on the processing overhead of standard Linux processing, but also highlight the sometimes overlapping and confusing Fast Packet Processing solutions. In this work, we propose a taxonomy to classify these current solutions into the three main groups corresponding to their targeted architecture, (i) hardware, (ii) software, (iii) and virtualization . We detail how each solution works and then discuss its applicability in real-world scenarios according to four different criteria: (i) host resource usage, (ii) high packet rate, (iii) system security and (iv) flexibility/expandability. Followed by a discussion of real use-cases that combine and compare multiple solutions discussed in the paper.},
  archive      = {J_COMCOM},
  author       = {Eduardo Freitas and Assis T. de Oliveira Filho and Pedro R.X. do Carmo and Djamel Sadok and Judith Kelner},
  doi          = {10.1016/j.comcom.2022.10.003},
  journal      = {Computer Communications},
  pages        = {148-166},
  shortjournal = {Comput. Commun.},
  title        = {A survey on accelerating technologies for fast network packet processing in linux environments},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster head selection method of multiple UAVs under
COVID-19 situation. <em>COMCOM</em>, <em>196</em>, 141–147. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As COVID-19 continues to spread, people are unable to move freely when their residence region is temporarily lockdown, supplies cannot normally enter into such zones, leading to the shortage of supplies in these areas. Thus to ensure the delivery of supplies while reducing contact, the unmanned aerial vehicle (UAV) deliveries have become a common way. In order to efficiently use UAV resources and reduce energy loss in data transmission while performing the tasks, clustering is often used for achieving the above objectives, where the selected cluster heads centrally plan tasks so that reduce the communication times. However, problems such as unreasonable clustering, high energy consumption of cluster heads, and high mortality of cluster heads, directly lead the low cooperation efficiency and short life cycle of UAVs. Considering the nodes often died earlier through the k-means algorithm and ant colony algorithm, and highly dependent on the base station , these factors affect the working cycle and coordination efficiency of the UAVs. Facing the issues above, the cluster head selection algorithm of UAV based on game (CHSA) is proposed, where the mixed game model is adopted to select cluster heads for each region after regional division, and selecting the representative node to perform the cluster head selection algorithm, which help to reduce the energy consumption of each round of communication between nodes. Moreover, the key properties of the CHSA algorithm are proved, and the comparison experiment are conducted to prove the CHSA algorithm can effectively reduce energy consumption and prolong the network life cycle.},
  archive      = {J_COMCOM},
  author       = {Jun Dai and Qunpeng Hu and Xu Liu and Yonglong Zhang and Junwu Zhu},
  doi          = {10.1016/j.comcom.2022.09.026},
  journal      = {Computer Communications},
  pages        = {141-147},
  shortjournal = {Comput. Commun.},
  title        = {Cluster head selection method of multiple UAVs under COVID-19 situation},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LoRa network reconfiguration with markov decision process
and fuzzy c-means clustering. <em>COMCOM</em>, <em>196</em>, 129–140.
(<a href="https://doi.org/10.1016/j.comcom.2022.08.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Range (LoRa) is a proprietary modulation technique that uses Chirp Spread Spectrum (CSS) modulation for low power and wide area communications. Despite the advantages of LoRa technology, the reconfiguration of transmission parameters such as Spreading Factor (SF) and Transmission Power ( P t x Ptx ), remains limited to maximize the uplink traffic. In this paper, we look upon additional parameters such as the Bandwidth (BW) and the Coding Rate (CR). We apply Fuzzy C-Means (FCM) algorithm to acquire knowledge about the quality of each transmission setting. Then, we use this knowledge in Q-learning and Markov Decision Process (MDP) algorithms as a state transition matrix to converge better and faster to the set of transmission settings that maximize the uplink data rate. As the solution should cope with different scenarios, we vary the number of End Devices (EDs), Base Stations (BSs), Packet Sizes (PSs) and Packet Rates (PRs). In addition, we compare our solution with many algorithms such as EXP3, ADR and EXPLoRaTS. Simulation results show that MDP with FCM clustering preprocessing improves better several Quality of Service (QoS) metrics including the Data Rate (DR), Packet Delivery Ratio (PDR), Time on Air (ToA) and Transmission Energy ( E t x Etx ). Thus, the PDR and the DR were improved by 25\%, the ToA was reduced by 40\% and E t x Etx was reduced by 20\%.},
  archive      = {J_COMCOM},
  author       = {Aghiles Djoudi and Rafik Zitouni and Nawel Zangar and Laurent George},
  doi          = {10.1016/j.comcom.2022.08.020},
  journal      = {Computer Communications},
  pages        = {129-140},
  shortjournal = {Comput. Commun.},
  title        = {LoRa network reconfiguration with markov decision process and fuzzy C-means clustering},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ambient intelligence assisted fog computing for industrial
IoT applications. <em>COMCOM</em>, <em>196</em>, 117–128. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient Intelligence (AmI) is a key concept that uses environmental and contextual information for improving the application experience. The adaptive approach used by AmI offers several benefits for the Industrial Internet of Things (IIoT) such as enhancing machine productivity and improving different processes. IIoT applications involve many tasks that need to be computed in real-time using fog nodes, thus efficient computing techniques are a major challenge in IIoT. In this paper, we propose an ambient intelligence-assisted computing technique for Industrial IoT to maximize the number of served tasks and reduce task outages at fog nodes. We utilize contextual information such as transmission rate and task delay requirements to efficiently offload the tasks from machine-embedded sensors to the fog nodes. We propose an adaptive computing resource unit sizing to serve an individual task at the fog node. Moreover, we propose a many-to-one matching-based algorithm for mapping between tasks and computing resources. We perform extensive simulations to show that the proposed algorithm improves the number of served tasks by 54\% and computational resource utilization at the fog nodes by 47\%.},
  archive      = {J_COMCOM},
  author       = {Usman Mahmood Malik and Muhammad Awais Javed},
  doi          = {10.1016/j.comcom.2022.09.024},
  journal      = {Computer Communications},
  pages        = {117-128},
  shortjournal = {Comput. Commun.},
  title        = {Ambient intelligence assisted fog computing for industrial IoT applications},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SLA-trust-energy aware path computation for critical
services in blockchain-enabled intelligent transport system.
<em>COMCOM</em>, <em>196</em>, 109–116. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The innovation in the Internet of Things (IoT) has amplified numerous time-bounded critical applications in different areas such as healthcare, banking, industries, product delivery services, and online food services. In this paper, an effort has been made an effort to propose a novel framework for critical services in Intelligent Transport systems (ITS). In ITS, critical services involve activities such as the exchange of information or transmission of data among vehicles, accessing of information in the case of accidents, determination of the best-routed route from the current location to a hospital, and determination of a low-congestion traffic route during working hours. The unavailability of any information leads to considerable loss of business, life, or property. This study aims to quantify the transportation service assurance by conceptualizing Service Level Agreements (SLAs) for various critical services, the energy requirement for complete data transmission, and providing Blockchain-enabled secured data transmission by calculating trust metrics of IoT nodes using a trust analyzer. In conclusion, the proposed mechanism is evaluated against various routing parameters by quantifying mean satisfied routes in ITS, throughput, delay, energy efficiency, trust in data transmission, and results with security configurations for tolerance toward attacks.},
  archive      = {J_COMCOM},
  author       = {Ashutosh Sharma and Mohammad Shabaz},
  doi          = {10.1016/j.comcom.2022.09.019},
  journal      = {Computer Communications},
  pages        = {109-116},
  shortjournal = {Comput. Commun.},
  title        = {SLA-trust-energy aware path computation for critical services in blockchain-enabled intelligent transport system},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A seven-dimensional state flow traffic modelling for
multi-controller software-defined networks considering multiple
switches. <em>COMCOM</em>, <em>196</em>, 89–108. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) improves flexibility in network management and programmability through decoupling the control and the data planes. In SDN, traffic analysis and modelling are important for tracking network activity and availability to detect abnormalities, such as security and operational problems. In this paper, a seven-dimensional state model is used to model SDN TCP and UDP flows, unlike other models that are used in previous work which use three or four-dimensional states. We formulate seven-dimensional states with transitions which consider flow-level arrivals to get more accurate knowledge about modelling the SDN traffic. The proposed work uses multiple controllers and multiple switches with limited capacities unlike other models in related work which may use one controller or switch with an infinite buffer. Using multiple controllers improves the security of the network, due to the backup capability. In addition, it enhances the network’s scalability since there are enough controllers to serve more switches. Also, using multiple switches can provide more accurate results to better reflect the actual case of traffic in the network. The packet average delay and loss probability are tracked to measure the performance of the developed models. Simulation results are formulated and compared to analytical ones. The results prove that the proposed modelling is accurate compared to the recent work.},
  archive      = {J_COMCOM},
  author       = {Waheed G. Gadallah and Hosny M. Ibrahim and Nagwa M. Omar},
  doi          = {10.1016/j.comcom.2022.09.027},
  journal      = {Computer Communications},
  pages        = {89-108},
  shortjournal = {Comput. Commun.},
  title        = {A seven-dimensional state flow traffic modelling for multi-controller software-defined networks considering multiple switches},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A points of interest recommendation framework based on
effective representation of heterogeneous nodes in the internet of
things. <em>COMCOM</em>, <em>196</em>, 76–88. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based social networks are a special kind of the Internet of Things in which users and points of interest (POIs) are heterogeneous network nodes. When matching POIs for users, traditional recommendation methods either map users to POI space or map POIs to user space. This direct mapping of the heterogeneous space introduces some noise to the node representation. The sparse interaction between users and POIs makes it difficult to effectively distinguish the representations of similar heterogeneous nodes. Based on the above problems, this paper proposes a recommendation framework that projects heterogeneous network nodes: users and POIs into a unified representation space. Obtain the matching results of nodes in the new problem space and effectively solve the problem of unified representation of heterogeneous nodes in the Internet of Things . The setting of the intermediate latent space can distinguish the representations of similar behavior nodes as much as possible and obtain more accurate recommendation results. We conduct extensive experiments on a four real-world datasets which show that framework has superior performance compared to the state-of-the-art framework for POI recommendation.},
  archive      = {J_COMCOM},
  author       = {Ruichang Li and Xiangwu Meng and Yujie Zhang},
  doi          = {10.1016/j.comcom.2022.09.014},
  journal      = {Computer Communications},
  pages        = {76-88},
  shortjournal = {Comput. Commun.},
  title        = {A points of interest recommendation framework based on effective representation of heterogeneous nodes in the internet of things},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sliding window based ON/OFF flow watermarking on tor.
<em>COMCOM</em>, <em>196</em>, 66–75. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyze traffic in the anonymity network , an active network flow watermark scheme, the ON/OFF watermarking, is proposed to embed timing information into the network flow. This scheme exploits a technique that drops packets at prescribed time intervals to create a watermark sequence at the sending side of a flow, then extracts inter-packet delay at the receiving side, and computes the L1 distance between generated sequences to identify the network flow. A sliding window based L1 distance computation algorithm is proposed to detect flow watermark at a low overhead. Experimental results demonstrate that this timing-based flow watermarking scheme can identify watermarks efficiently and is resistant to timing disturbances in Tor.},
  archive      = {J_COMCOM},
  author       = {Kai Yang and Zhihong Liu and Yong Zeng and Jianfeng Ma},
  doi          = {10.1016/j.comcom.2022.09.028},
  journal      = {Computer Communications},
  pages        = {66-75},
  shortjournal = {Comput. Commun.},
  title        = {Sliding window based ON/OFF flow watermarking on tor},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-empowered secure federated learning system:
Architecture and applications. <em>COMCOM</em>, <em>196</em>, 55–65. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising paradigm to realize distributed machine learning on heterogeneous clients without exposing their private data. However, there is the risk of single point failure with FL because it relies on a central server to gather the model updates from clients, moreover, malicious behaviors of some clients may lead to low-quality or even poisoned global models. Blockchain as a revolutionary distributed ledger technology can alleviate the above problems to significantly enhance the security and scalability of FL systems. Therefore, this article presents a general framework of Blockchain-based Federated Learning (BFL) system with detailed description of its key technologies and operation steps. We then review and compare the most recent representative BFL applications. And we outlook some key challenges and opportunities of the future BFL system in terms of security, cost, and scalability. Finally, we propose PoS-BFL in IoT scenarios with malicious devices. The validator voting mechanism and role switching mechanism in PoS-BFL ensure the stakes of legitimate nodes , and effectively reduce the impact of malicious nodes on the accuracy of the system model. And the experiments are conducted to demonstrate that PoS-BFL can achieve 86\% accuracy, which is much higher than vanilla FL and pFedMe, and PoS-BFL is robust to some extent by adjusting the ratio of workers, validators and miners.},
  archive      = {J_COMCOM},
  author       = {Feng Yu and Hui Lin and Xiaoding Wang and Abdussalam Yassine and M. Shamim Hossain},
  doi          = {10.1016/j.comcom.2022.09.008},
  journal      = {Computer Communications},
  pages        = {55-65},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-empowered secure federated learning system: Architecture and applications},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A user-centric privacy-preserving authentication protocol
for IoT-AmI environments. <em>COMCOM</em>, <em>196</em>, 45–54. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient Intelligence (AmI) in Internet of Things (IoT) has empowered healthcare professionals to monitor, diagnose, and treat patients remotely. Besides, the AmI-IoT has improved patient engagement and gratification as doctors’ interactions have become more comfortable and efficient. However, the benefits of the AmI-IoT-based healthcare applications are not availed entirely due to the adversarial threats. IoT networks are prone to cyber attacks due to vulnerable wireless mediums and the absentia of lightweight and robust security protocols. This paper introduces computationally-inexpensive privacy-assuring authentication protocol for AmI-IoT healthcare applications. The use of blockchain &amp; fog computing in the protocol guarantees unforgeability, non-repudiation, transparency, low latency, and efficient bandwidth utilization . The protocol uses physically unclonable functions (PUF), biometrics , and Ethereum powered smart contracts to prevent replay, impersonation, and cloning attacks. Results prove the resource efficiency of the protocol as the smart contract incurs very minimal gas and transaction fees. The Scyther results validate the robustness of the proposed protocol against cyber-attacks. The protocol applies lightweight cryptography primitives (Hash, PUF) instead of conventional public-key cryptography and scalar multiplications . Consequently, the proposed protocol is better than centralized infrastructure-based authentication approaches.},
  archive      = {J_COMCOM},
  author       = {Mehedi Masud and Gurjot Singh Gaba and Pardeep Kumar and Andrei Gurtov},
  doi          = {10.1016/j.comcom.2022.09.021},
  journal      = {Computer Communications},
  pages        = {45-54},
  shortjournal = {Comput. Commun.},
  title        = {A user-centric privacy-preserving authentication protocol for IoT-AmI environments},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Short packet communications for cooperative UAV-NOMA-based
IoT systems with SIC imperfections. <em>COMCOM</em>, <em>196</em>,
37–44. (<a href="https://doi.org/10.1016/j.comcom.2022.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a non-orthogonal multiple access (NOMA)-enabled unmanned aerial vehicle (UAV)-aided Internet of Things (IoT) system with short packet communications over Nakagami- m m fading channel. In this system, the multi-antennas UAV plays role as a relay to assist communication between two IoT device pairs, which results in two NOMA stages implemented at the UAV for both uplink and downlink transmissions . To evaluate the performance of the system, we first determine the cumulative distribution function of the received signal-to-interference-plus-noise ratio, base on this, the average end-to-end (e2e) block-error rates (BLERs) under the presence of perfect and imperfect successive interference cancellation (SIC) procedure are deduced. Aiming to reveals insights into system designs, we have further investigate the system performance at high signal-to-noise ratio regime, where destination’s diversity gain is obtained. Additionally, the effects of the system parameters such as the transmit power, the power allocation coefficient, altitude and speed fly of the UAV, and the coefficients of channel model on the system performance are studied. The results demonstrate that increase in the number of antennas at the UAV improves significantly the performance of the users in terms of the average e2e BLERs of the users. Particularly, Monte Carlo simulations confirm the derived theoretical analysis.},
  archive      = {J_COMCOM},
  author       = {Tien-Tung Nguyen and Sang Quang Nguyen},
  doi          = {10.1016/j.comcom.2022.09.020},
  journal      = {Computer Communications},
  pages        = {37-44},
  shortjournal = {Comput. Commun.},
  title        = {Short packet communications for cooperative UAV-NOMA-based IoT systems with SIC imperfections},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating the difference between trolls, social bots,
and humans on twitter. <em>COMCOM</em>, <em>196</em>, 23–36. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become apparent that human accounts are not the sole actors in the social media scenario. The expanding role of social media in the consumption and diffusion of information has been accompanied by attempts to influence public opinion. Researchers reported several instances where social bots, automated accounts designed to impersonate humans, have been deployed for this purpose. More recently, platforms such as Twitter provided evidence pointing to governments creating and using fake accounts in this kind of abuse. These accounts are known as state-backed trolls. Although these different actors have been widely studied, there is little understanding of how they differ when examined together. In this paper, we contribute to understanding the characteristics of the different types of accounts and increase our awareness of Twitter’s state-backed trolls, which so far have received limited attention from quantitative researchers. We propose a large-scale quantitative analysis , which relies on both datasets released by Twitter and researchers in recent years to characterize the different actors that take part in the social network scenario. In particular, we represent each account with a large number of features categorized into three distinct traits: credibility, initiative, and adaptability concerning the underlying aspects into which they best fit. We conducted subsequent experiments, isolating features on their respective traits and using them all. First, we apply dimensionality reduction to project accounts onto the same bi-dimensional space and visualize how they distribute across it. Then, we experiment with different combinations of two parameters that affect the dimensionality reduction and clustering algorithm to find which trait is best suited to distinguish the different actors. In our best combination in terms of effectiveness, we obtain high-quality clusters, achieving a purity score of 0.9, which results in homogeneous clusters where accounts of the same category are grouped. Beyond that, we explore our results by visualizing and studying clustering results to determine the differences between account categories. Using our defined traits, we show that it is possible to distinguish the different accounts through clustering, obtaining the best results while leveraging the three traits simultaneously. An additional analysis shows that features related to retweeting patterns and URLs sharing are effective in differentiating trolls and humans. At the same time, social bot accounts suffer from recall degradation in cross-domain evaluation. Moreover, we show that accounts belonging to the same dataset are not necessarily similar in the defined traits. Finally, we perform a feature importance analysis using SHAP to gain insights into which features best differentiate the account when examined in pairs.},
  archive      = {J_COMCOM},
  author       = {Michele Mazza and Marco Avvenuti and Stefano Cresci and Maurizio Tesconi},
  doi          = {10.1016/j.comcom.2022.09.022},
  journal      = {Computer Communications},
  pages        = {23-36},
  shortjournal = {Comput. Commun.},
  title        = {Investigating the difference between trolls, social bots, and humans on twitter},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient reusable attribute-based signature scheme for
mobile services with multi access policies in fog computing.
<em>COMCOM</em>, <em>196</em>, 9–22. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an emergent computing paradigm that supports the mobility and geographic distribution of Internet of Things (IoT) nodes and delivers context-aware applications with low latency to end-users. In fog computing, the critical obstacle restricting widespread deployment is security and privacy, especially how to build a lightweight fine-grained message authentication scheme in fog computing. In this paper, we first propose and give the formalization definition to a new variant of the attribute-based signature primitive (ABS), which we called Verifier-Policy Attribute-based Signature (VP-ABS). Distinct from the traditional ABS, in our VP-ABS primitive, the signature generated by the signer is decoupled with the access policy, which means the signer can generate a signature associated with some of his own attributes. Therefore, our VP-ABS can be reusable for multiple access policies. Then we also give a concrete VP-ABS construction over Type-3 pairing under Linear Secret Sharing Scheme (LSSS) policy which supports both AND and OR access gates. In addition, we rigorously prove our VP-ABS scheme is existentially unforgeable in the selective policy model under the adaptive chosen message attack (sP-EUF-CMA). Next, we give the feature comparison and theoretical analysis to our VP-ABS scheme as well as some other representative attribute authentication schemes to show the comprehensiveness of our scheme. To prove the correctness of the theoretical analysis and test the actual performance, we also do simulation experiments. Finally, we use our VP-ABS scheme to build an attribute-based fine-grained message authentication scheme for fog nodes in mobile microservices architecture with multiple access policies.},
  archive      = {J_COMCOM},
  author       = {Zhishuo Zhang and Wen Huang and Songying Cai and Lin Yang and Yongjian Liao and Shijie Zhou},
  doi          = {10.1016/j.comcom.2022.09.017},
  journal      = {Computer Communications},
  pages        = {9-22},
  shortjournal = {Comput. Commun.},
  title        = {An efficient reusable attribute-based signature scheme for mobile services with multi access policies in fog computing},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multi-agent reinforcement learning for resource
allocation in NOMA-enabled MEC. <em>COMCOM</em>, <em>196</em>, 1–8. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) and mobile edge computing (MEC) are being considered as promising technologies to address the stringent demands of the emerging fifth generation (5G) networks. This paper investigates the resource allocation problem in NOMA-enabled MEC system for multiple users, by joint optimization of power and computation resources to enhance effective throughput of the system. Because of the severe non convexity of the problem, a decentralized multi-agent reinforcement learning (MARL) scheme is proposed, where each user to base station (U2B) link acts as an agent, and collectively interacts with the network environment, in order to maximize the objective function under limited power and computation resource constraints. Simulation results demonstrate that the proposed MARL scheme results in considerable improvement in the effective throughput of the system, which is comparable to the optimal results derived from the exhaustive optimal search, with significantly low overhead.},
  archive      = {J_COMCOM},
  author       = {Noor Waqar and Syed Ali Hassan and Haris Pervaiz and Haejoon Jung and Kapal Dev},
  doi          = {10.1016/j.comcom.2022.09.018},
  journal      = {Computer Communications},
  pages        = {1-8},
  shortjournal = {Comput. Commun.},
  title        = {Deep multi-agent reinforcement learning for resource allocation in NOMA-enabled MEC},
  volume       = {196},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on uncertainty quantification of shadowing
reconstruction and signal measurements in radio tomographic imaging.
<em>COMCOM</em>, <em>195</em>, 488–498. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Tomographic Imaging (RTI) is a novel technology to reconstruct the target-induced shadowing effect of signal propagation in the Radio-Frequency (RF) sensing network, through the Received Signal Strength (RSS) measurements in the RF propagation links. As a novel wireless-signal reconstruction technology, RTI is widely utilized in the Device-Free Localization (DFL) and Location-Based Services (LBS) applications. However, the multipath interference in RF sensing network often induces serious uncertainty in RTI system , including the RSS measurement noise and the RTI reconstruction degradation. Addressing this problem, as the RTI system is to reconstruct the probability image of shadowing by using the RSS measurements, from the perspective of probabilistic modeling , the RTI reconstruction issue is modeled as the Maximum a Posterior Probability (MAP) of the reconstructed shadowing image from the RSS measurements. Then lots of RTI reconstruction methods for uncertainty quantification , have been developed to improve the RSS measurement accuracy and the RTI reconstruction quality. Therefore, for the first time, this article conducts a comprehensive survey of the recent RTI methods from the viewpoint of uncertainty quantification . At first, the uncertainty quantification in these RTI methods of the shadowing image and RSS measurements are united into the Epistemic uncertainty and Aleatoric uncertainty respectively. Besides, these methods are further classified into different categories based on their technical commonalities. Moreover, the presenting challenges of uncertainty quantification in the actual RTI applications are also discussed, and based on that, the potential development directions of these methods for improving the RSS measurement accuracy and RTI reconstruction performance in future are reasonably predicted.},
  archive      = {J_COMCOM},
  author       = {Jiaju Tan and Qili Zhao and Xuemei Guo and Xin Zhao and Guoli Wang},
  doi          = {10.1016/j.comcom.2022.09.006},
  journal      = {Computer Communications},
  pages        = {488-498},
  shortjournal = {Comput. Commun.},
  title        = {A review on uncertainty quantification of shadowing reconstruction and signal measurements in radio tomographic imaging},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource allocation for network slicing in dynamic
multi-tenant networks: A deep reinforcement learning approach.
<em>COMCOM</em>, <em>195</em>, 476–487. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the wide range of 5G use cases in a cost-efficient way, network slicing has been considered a promising solution, which makes it possible to serve multiple customized and isolated network services on a common physical infrastructure. In this paper, we investigate the resource allocation problem of network slicing in multi-tenant networks where network resources can be used by low-priority tenants change dynamically due to the preemption of high-priority tenants. We formulate the problem as an energy-minimizing mathematical optimization problem considering practical constraints. Due to the dynamic characteristics of the problem, the complexity of the optimization problem is exceptionally high, making it impossible to solve the problem in real-time using traditional optimization approaches. With discovering the special structure of the problem, we propose a Dueling-Deep Q Network (DQN)-based algorithm to solve the problem efficiently. The experimental results show that the proposed algorithm outperforms compared algorithms in terms of total energy cost, runtime, and robustness.},
  archive      = {J_COMCOM},
  author       = {Yanghao Xie and Yuyang Kong and Lin Huang and Sheng Wang and Shizhong Xu and Xiong Wang and Jing Ren},
  doi          = {10.1016/j.comcom.2022.09.015},
  journal      = {Computer Communications},
  pages        = {476-487},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation for network slicing in dynamic multi-tenant networks: A deep reinforcement learning approach},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient routing in LEO satellite networks for
extending satellites lifetime. <em>COMCOM</em>, <em>195</em>, 463–475.
(<a href="https://doi.org/10.1016/j.comcom.2022.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellites, when exposed to the sun, use solar energy for operation, processing, and communication, and with excess energy they recharge their batteries. However, when satellites are in an area with no sunlight, called eclipse areas, they operate using only their battery power. The batteries have limitations on the amount of recharges/discharges, also known as the depth of discharge (DOD) cycle. Therefore, this restricts the useful life of the batteries themselves and also of the satellites. In this paper, we propose two different efficient routing methods for LEO satellite networks, which optimize traffic in order to reduce the DOD of satellites. We improved the Energy and Capacity Aware Routing (ECARS) metric, existing in the literature, by adding the Energy Routing prUning (ERU)-DOD and Energy Routing penAlty (ERA)-DOD methods. These proposed methods prune and penalize, respectively, the links whose satellites have reached a certain minimum battery charge threshold. With this procedure, we avoid over-discharging the satellites’ battery, and thus, the lifetime is extended. Simulations results show that ERU-DOD and ERA-DOD can increase 133.19\% and 11.88\% the satellites’ batteries lifetime, respectively. Moreover, when comparing our ERU-DOD and ERA-DOD proposals with the ECARS in terms of the average residual energy of the batteries, they provide an increase of 109.77\% and 32.73\%, respectively. In addition, ERU-DOD and ERA-DOD proposals showed a gain in throughput of 7.28\% and 3.06\% higher than ECARS. Furthermore, compared to the ECARS metric, the ERA-DOD proposal has a 1.1\% lower delay, a 1.5\% lower number of hops, and 0.49\% less in the total of blocked sources.},
  archive      = {J_COMCOM},
  author       = {Renata do N. Mota Macambira and Celso Barbosa Carvalho and José Ferreira de Rezende},
  doi          = {10.1016/j.comcom.2022.09.009},
  journal      = {Computer Communications},
  pages        = {463-475},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient routing in LEO satellite networks for extending satellites lifetime},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy-MAC: An FIS based MAC protocol for a multi-constrained
traffic in wireless body area networks. <em>COMCOM</em>, <em>195</em>,
451–462. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Fuzzy-MAC protocol to facilitate the uniform transmission of the multi-constrained data generated by the sensor devices in a wireless body area network (WBAN). A physiological sensor device allows us to monitor various health-related parameters. In the process, the sensed data become associated with diverse Quality of Service (QoS)-specific constraints depending on the application requirements, thus imposing a challenge in front of the enabling communication protocols such as IEEE 802.15.4. To deal with these multi-constrained traffic data, Fuzzy-MAC first categorizes the sensor data according to the application-specific QoS requirements. Moreover, improved channel access mechanisms are applied according to the priority class value assigned to the different data types . Lastly, employing a Fuzzy Inference System (FIS), Fuzzy-MAC brings context-awareness to a health monitoring environment. We conduct rigorous simulations of the proposed Fuzzy-MAC to justify the competence with the existing literature. The results show that the proposed MAC protocol outperforms the benchmarks in terms of throughput, average delay, and overall reliability of a WBAN.},
  archive      = {J_COMCOM},
  author       = {Kaushik Ray and Vipin Pal and Gaurav Singal and Soumen Moulik},
  doi          = {10.1016/j.comcom.2022.09.013},
  journal      = {Computer Communications},
  pages        = {451-462},
  shortjournal = {Comput. Commun.},
  title        = {Fuzzy-MAC: An FIS based MAC protocol for a multi-constrained traffic in wireless body area networks},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning based trajectory optimization
for magnetometer-mounted UAV to landmine detection. <em>COMCOM</em>,
<em>195</em>, 441–450. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have emerged as a viable choice for data collection and landmine (LM) detection. The LM buried under the dirt or sand is detected using a UAV-mounted magnetometer in this paper. A UAV is deployed to gather data along the intended route when the magnetometer receives a signal from the LMs. During a whole round of data collection, we want to reduce the total energy consumption of the UAV-Magnetometer-LM system. To do this, we turn the energy consumption reduction issue into a limited combinatorial optimization problem by concurrently picking time slots and arranging the UAV’s visitation sequence to identify the LM. The problem of minimizing energy usage is NP-hard, making it difficult to solve optimally. In order to tackle this challenge, we used the deep reinforcement learning (DRL) based deep deterministic policy gradient (DDPG) scheme. DDPG is used to enhance the convergence speed and eliminate redundant computations. Furthermore, to improve the detection in real-time, we proposed the proximal online policy technique (POPT). Numerical results demonstrate that the proposed scheme consumes 37.14\%, 31.25\%, and 21.42\% better results than synthetic aperture radar (SAR), convolution neural network (CNN), and double deep recurrent Q-network (DDRQN).},
  archive      = {J_COMCOM},
  author       = {Ahmed Barnawi and Neeraj Kumar and Ishan Budhiraja and Krishan Kumar and Amal Almansour and Bander Alzahrani},
  doi          = {10.1016/j.comcom.2022.09.002},
  journal      = {Computer Communications},
  pages        = {441-450},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based trajectory optimization for magnetometer-mounted UAV to landmine detection},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning-based optimization for end-to-end latency
in TSN networks. <em>COMCOM</em>, <em>195</em>, 424–440. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-low latency (ULL) is a challenging but nonetheless essential network requirement to achieve in many contexts. Industrial automation (IEC/IEEE 60802), in-vehicle communications (IEEE P802.1DG), audio and video bridging (IEEE Std 802.1BA), aerospace, and 5G fronthaul (IEEE 802.1CM) applications demand a low network latency in the order of few milliseconds or even microseconds . ULL services target a more controlled latency for end-to-end device communication and, in critical environments, near real-time connections. Time-Sensitive Networks (TSN) aim at providing deterministic connectivity over IEEE 802 networks guaranteeing packet transport with bounded latency, low packet delay variation, and low packet loss . This work analyzes TSN latency mechanisms and examines how they influence E2E latency. Moreover, it builds a regression-based surrogate model that enables the creation of optimization models to find optimal configurations for a given TSN network. Our findings span the discussion of the characteristics of each individual TSN latency mechanism to the benefits achieved by the proposed optimization models and using these to achieve optimal E2E latency.},
  archive      = {J_COMCOM},
  author       = {Daniel Bezerra and Assis T. de Oliveira Filho and Iago Richard Rodrigues and Marrone Dantas and Gibson Barbosa and Ricardo Souza and Judith Kelner and Djamel Sadok},
  doi          = {10.1016/j.comcom.2022.09.011},
  journal      = {Computer Communications},
  pages        = {424-440},
  shortjournal = {Comput. Commun.},
  title        = {A machine learning-based optimization for end-to-end latency in TSN networks},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An asynchronous distributed training algorithm based on
gossip communication and stochastic gradient descent. <em>COMCOM</em>,
<em>195</em>, 416–423. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–Physical Systems (CPS) applications are playing an increasingly important role in our lives, hence the use of centralized distributed machine learning in CPS to secure applications to start widespread use. However, existing centralized distributed machine learning (ML) algorithms have significant shortcomings in CPS scenarios. As a result, its synchronization algorithm has high latency and sensitivity to drop-off, which affects the security of CPS. Therefore, this paper combining the Gossip protocol with Stochastic Gradient Descent (SGD), this paper proposes a communication framework Gossip Ring SGD (GR-SGD) for machine learning . GR-SGD is decentralized and asynchronous, and solves the problem of long communication waiting time. This paper uses the ImageNet data set and the ResNet model to verify the feasibility of the algorithm and compares it with Ring AllReduce and D-PSGD. Moreover, this paper also indicates that some data redundancy can reduce communication overhead and increase system fault tolerance , it can be better applied to CPS and all kinds of machine learning models.},
  archive      = {J_COMCOM},
  author       = {Jun Tu and Jia Zhou and Donglin Ren},
  doi          = {10.1016/j.comcom.2022.09.010},
  journal      = {Computer Communications},
  pages        = {416-423},
  shortjournal = {Comput. Commun.},
  title        = {An asynchronous distributed training algorithm based on gossip communication and stochastic gradient descent},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cellular network capacity and coverage enhancement with MDT
data and deep reinforcement learning. <em>COMCOM</em>, <em>195</em>,
403–415. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years witnessed a remarkable increase in the availability of data and computing resources in communication networks. This contributed to the rise of data-driven over model-driven algorithms for network automation. This paper investigates a Minimization of Drive Tests (MDT)-driven Deep Reinforcement Learning (DRL) algorithm to optimize coverage and capacity by tuning antennas tilts on a cluster of cells from TIM’s cellular network . We jointly utilize MDT data, electromagnetic simulations , and network Key Performance indicators (KPIs) to define a simulated network environment for the training of a Deep Q-Network (DQN) agent. Some tweaks have been introduced to the classical DQN formulation to improve the agent’s sample efficiency, stability and performance. In particular, a custom exploration policy is designed to introduce soft constraints at training time. Results show that the proposed algorithm outperforms baseline approaches like DQN and best-first search in terms of long-term reward and sample efficiency. Our results indicate that MDT-driven approaches constitute a valuable tool for autonomous coverage and capacity optimization of mobile radio networks.},
  archive      = {J_COMCOM},
  author       = {Marco Skocaj and Lorenzo M. Amorosa and Giorgio Ghinamo and Giuliano Muratore and Davide Micheli and Flavio Zabini and Roberto Verdone},
  doi          = {10.1016/j.comcom.2022.09.005},
  journal      = {Computer Communications},
  pages        = {403-415},
  shortjournal = {Comput. Commun.},
  title        = {Cellular network capacity and coverage enhancement with MDT data and deep reinforcement learning},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFAFed—asynchronous fair adaptive federated learning for IoT
stream applications. <em>COMCOM</em>, <em>195</em>, 376–402. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we design, analyze the convergence properties , address the implementation aspects, and numerically test the performance of AFAFed . This is a novel A synchronous F air A daptive Fed erated learning framework for stream-oriented IoT application environments, which are featured by time-varying operating conditions, heterogeneous resource-limited devices (i.e., coworkers), non-i.i.d. local training data and unreliable communication links. The key new of AFAFed is the synergic co-design of: (i) two sets of adaptively tuned tolerance thresholds and fairness coefficients at the coworkers and central server, respectively; and, (ii) a distributed adaptive mechanism, which allows each coworker to adaptively tune own communication rate. The convergence properties of AFAFed under (possibly) non-convex loss functions is guaranteed by a set of new analytical bounds, which formally unveil the impact on the resulting AFAFed convergence rate of a number of Federated Learning (FL) parameters, like, first and second moments of the per-coworker number of consecutive model updates, data skewness, communication packet-loss probability, and maximum/minimum values of the (adaptively tuned) mixing coefficient used for model aggregation. Extensive numerical tests show that AFAFed is capable to improve test accuracy by up to 20\% and reduce training time by up to 50\%, compared to state-of-the-art FL schemes, even under challenging learning scenarios featured by deep Machine Learning (ML) models, data skewness, coworker heterogeneity and unreliable communication.},
  archive      = {J_COMCOM},
  author       = {Enzo Baccarelli and Michele Scarpiniti and Alireza Momenzadeh and Sima Sarv Ahrabi},
  doi          = {10.1016/j.comcom.2022.09.016},
  journal      = {Computer Communications},
  pages        = {376-402},
  shortjournal = {Comput. Commun.},
  title        = {AFAFed—Asynchronous fair adaptive federated learning for IoT stream applications},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Slicing a FANET for heterogeneous delay-constrained
applications. <em>COMCOM</em>, <em>195</em>, 362–375. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s networks use a “one size fits all” approach that makes them unsuitable to satisfy the performance requirements of heterogeneous applications in terms of latency, scalability, availability and reliability. To this purpose, the concept of network slices has been recently introduced to meet heterogeneous requirements of vertical applications deployed on top of common network infrastructures. The objective of this paper is to define a 6G zero-touch management framework based on a Flying Aerial Network (FANET) made by a set of UAVs , which is able to provide ground devices with network slices that are able to guarantee edge computing to remote areas according to heterogeneous requirements in terms of mean delay and jitter. An inter-slice orchestrator is introduced to split the computation power of the Computing Element of each UAV between the different slices, while an intra-slice orchestrator is in charge of managing horizontal offload among UAVs to obtain load balancing, so decreasing mean delay and delay jitter . An optimization problem based on Reinforcement Learning (RL) is defined to optimize the horizontal-offload decision process of the Intra-slice Orchestrators at run-time. An extensive simulation campaign is used to evaluate the performance of the system and to demonstrate the ability of the proposed framework to achieve energy efficiency, therefore increasing the flight autonomy of the FANET.},
  archive      = {J_COMCOM},
  author       = {Christian Grasso and Raoul Raftopoulos and Giovanni Schembra},
  doi          = {10.1016/j.comcom.2022.08.024},
  journal      = {Computer Communications},
  pages        = {362-375},
  shortjournal = {Comput. Commun.},
  title        = {Slicing a FANET for heterogeneous delay-constrained applications},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning for intrusion detection system: Concepts,
challenges and future directions. <em>COMCOM</em>, <em>195</em>,
346–361. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Internet and smart devices trigger surge in network traffic making its infrastructure more complex and heterogeneous. The predominated usage of mobile phones, wearable devices and autonomous vehicles are examples of distributed networks which generate huge amount of data each and every day. The computational power of these devices have also seen steady progression which has created the need to transmit information, store data locally and drive network computations towards edge devices. Intrusion detection systems (IDS) play a significant role in ensuring security and privacy of such devices. Machine Learning (ML) and Deep Learning (DL) with Intrusion Detection Systems have gained great momentum due to their achievement of high classification accuracy . However the privacy and security aspects potentially gets jeopardized due to the need of storing and communicating data to centralized server. On the contrary, Federated Learning (FL) fits in appropriately as a privacy-preserving decentralized learning technique that does not transfer data but trains models locally and transfers the parameters to the centralized server. The present paper aims to present an extensive and exhaustive review on the use of FL in intrusion detection system. In order to establish the need for FL, various types of IDS, relevant ML approaches and its associated issues are discussed. The paper presents detailed overview of the implementation of FL in various aspects of anomaly detection . The allied challenges of FL implementations are also identified which provides idea on the scope of future direction of research. The paper finally presents the plausible solutions associated with the identified challenges in FL based intrusion detection system implementation acting as a baseline for prospective research.},
  archive      = {J_COMCOM},
  author       = {Shaashwat Agrawal and Sagnik Sarkar and Ons Aouedi and Gokul Yenduri and Kandaraj Piamrat and Mamoun Alazab and Sweta Bhattacharya and Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu},
  doi          = {10.1016/j.comcom.2022.09.012},
  journal      = {Computer Communications},
  pages        = {346-361},
  shortjournal = {Comput. Commun.},
  title        = {Federated learning for intrusion detection system: Concepts, challenges and future directions},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cold-start aware cloud-native service function chain caching
in resource-constrained edge: A reinforcement learning approach.
<em>COMCOM</em>, <em>195</em>, 334–345. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualized Network Functions (VNF), Service Function Chains (SFC) and Network Functions Virtualization (NFV) architecture are promising basis of modern network infrastructures. How to make the best use of the limited resource at the edge yet achieve acceptable latency performance is one of the key challenges. Further, cloud-native network functions (CNF) enables a more flexible architecture with container-based virtualization, yet brings the problem of cold-start handling, since transferring and booting a container image can bring an indispensable latency. We formulate the cold-start aware cloud-native SFC caching problem as a mathematical optimization problem with a set of constraints based on the resource limitation and performance requirement. To efficiently handle this problem, which has been proved to be NP-Hard, we design a deep reinforcement learning (DRL) approach, along with two graph neural network-based embedding networks for the extraction of backbone network graph and caching request information, respectively. The resulting DRL agent is able to learn caching decisions, aiming at optimizing the processing latency, sub-frame processing latency, and launch latency performance while maintaining the request acceptance ratio. Extensive simulations conducted on multiple backbone network structures and various request load suggest that the proposed approach outperforms the state-of-the-art solutions in request acceptance ratio, latency performance under high loads, and cold-start handling with little extra execution time overhead.},
  archive      = {J_COMCOM},
  author       = {Jiayin Zhang and Huiqun Yu and Guisheng Fan and Zengpeng Li},
  doi          = {10.1016/j.comcom.2022.09.004},
  journal      = {Computer Communications},
  pages        = {334-345},
  shortjournal = {Comput. Commun.},
  title        = {Cold-start aware cloud-native service function chain caching in resource-constrained edge: A reinforcement learning approach},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge intelligence for smart airport runway: Architectures
and enabling technologies. <em>COMCOM</em>, <em>195</em>, 323–333. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the management efficiency and reduce the operational risks , a modern airport should not only be able to track the vehicles driving on the runway, but also be capable of detecting the deformation on the runway. To build such a smart airport , this paper takes measures from four perspectives: data, algorithm, computing power and platform. In terms of data, the optic fiber sensors (OFSs) rather than the traditional electromechanical sensors are used to collect the airport environmental data (AED). Compared with the electromechanical sensors, OFSs are cheaper, more reliable, and easier to be deployed. In terms of algorithms, the intelligent algorithms such as Convolutional Neural Networks (CNN), fast Fourier transform (FFT) and K-means are applied to analyze the AED. Compared with the traditional algorithms which detect the vehicle traces and runway deformation directly by signal pressure and amplitude, these algorithms are more precise and adaptable. In terms of computing power, the domain-specific architecture (DSA) technique is applied to increase the computing performance while keeping high energy efficiency. By designing several specific FPGA accelerators dedicated to the algorithms, the large quantity of AEDs can be processed quickly in real time. In terms of platform, a real-world edge-cloud collaborative platform based on the improved KubeEdge and Huawei openLooKeng is built. This platform can provide low-latency and high-performance computing, as well as data fusion for the AED processing in the airport. The work of this article has been practically applied to the Ezhou Huahu International Airport, and the real-world experimental results show that the proposed approaches have high detection accuracy, real-time data processing capability, low cost and also high energy efficiency.},
  archive      = {J_COMCOM},
  author       = {Xing Liu and Qi Wang and Chengming Zou and Mei Yu and Denghong Liao},
  doi          = {10.1016/j.comcom.2022.09.003},
  journal      = {Computer Communications},
  pages        = {323-333},
  shortjournal = {Comput. Commun.},
  title        = {Edge intelligence for smart airport runway: Architectures and enabling technologies},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ambient intelligence approach: Internet of things based
decision performance analysis for intrusion detection. <em>COMCOM</em>,
<em>195</em>, 315–322. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent infrastructures, Internet of Things (IoT) have become an important technology for connecting various actuators and sensors over wireless networks. Due to increase in mission-critical infrastructures, we make use of these new technologies for reliable communication but their security is always not promising in terms of availability, confidentiality, integrity, and privacy of network services . Users can be compromised and vulnerable by a motivated malicious opponent unless they are not adequately protected by a robust defense. Due to this reason, an ambient intelligence approach for Intrusion Detection System (IDS) is required. In this research, ww proposed Ambient Approach based on Reinforcement Learning Integrated Deep Q-Neural Network (RL-DQN) model for WSNs and IoT in which it leverages the Markov decision process (MDP) formalism to enhance the decision performance in IDS. We deploy RL-DQN-IDS over Edge-cloud intrusion detection infrastructure in which binary attack classification of the network traffic is performed at the edge network while multi-attack classification is performed at the cloud network. To identify intrusions, we use a two-phase process that includes an initial learning phase that relies on RL, followed by a detection and classification phase that relies on DQN . We used four datasets namely UNSW-NB-15, BoTNeTIoT-L01, CICIDS2017 and IoTID20 with a smart house simulation environment configured with WSN and IoT technologies to evaluate performance. Accuracy, precision, and recall were all considered while assessing the dataset under consideration. When compared to five other machine learning models, the RL-DQN model method has demonstrated superior performance. This model outperforms the other five that were tested.},
  archive      = {J_COMCOM},
  author       = {T.V. Ramana and M. Thirunavukkarasan and Amin Salih Mohammed and Ganesh Gopal Devarajan and Senthil Murugan Nagarajan},
  doi          = {10.1016/j.comcom.2022.09.007},
  journal      = {Computer Communications},
  pages        = {315-322},
  shortjournal = {Comput. Commun.},
  title        = {Ambient intelligence approach: Internet of things based decision performance analysis for intrusion detection},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving LoRaWAN downlink performance in the EU868
spectrum. <em>COMCOM</em>, <em>195</em>, 303–314. (<a
href="https://doi.org/10.1016/j.comcom.2022.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN is one of the most widely used Internet of Things protocols. It is developed driven by the easiness of the deployment and the support of any type of application. However, LoRaWAN has been made mostly for uplink transmissions rather than downlink traffic . As it has been shown by many studies in the literature, it suffers from very poor performance even with moderate downlink traffic . This is mainly due to the radio duty cycle restrictions applied on gateways but also due to the half-duplex nature of LoRa transceivers . To mitigate the problem of the reduced downlink performance, this paper proposes different channel, band, and downlink window schemes taking into account the recently announced Band 47b in the EU868 spectrum which adds four extra downlink channels with a total radio duty cycle of 10\%. The main issue that is tackled is how the additional downlink time can be used effectively in the new schemes. The advantages and disadvantages of each scheme are discussed and ranked based on the easiness of their integration into the native LoRaWAN and the number of modifications they require. Extensive simulation results are presented and are compared to the baseline. The results reveal that schemes which require more changes to the protocol exhibit higher performance gains. More specifically, if a 10\% duty cycle channel is applied in the first receive window, a higher than 200\% performance gain in terms of packet delivery ratio and energy consumption can be achieved.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas},
  doi          = {10.1016/j.comcom.2022.09.001},
  journal      = {Computer Communications},
  pages        = {303-314},
  shortjournal = {Comput. Commun.},
  title        = {Improving LoRaWAN downlink performance in the EU868 spectrum},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Path selection and resource allocation for 5G multi-hop D2D
networks. <em>COMCOM</em>, <em>195</em>, 292–302. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-hop Device-to-Device (D2D) communication has emerged as one of the promising technologies to improve the network capacity of fifth generation (5G) networks. Existing research works focus on either increasing the throughput by streamlining radio-resource and power allocation approach or finding the best suitable path based on next-hop distance. However, there is very limited research work that covers the selection of the best path and resource allocation in multi-hop D2D communication. Therefore, in this paper, we proposed a path selection and resource allocation method. The proposed path selection method is based on the Q learning model in which the cumulative reward is calculated from source to destination. The proposed resource allocation method is divided into two-stage. In the first stage, a Max Mean Far (MMF) method is used to allocate the resource block in forming a multi-hop D2D. In the second stage, a hybrid combination of particle swarm optimization and gray wolf optimization (HPSOGWO) is applied over the first stage results. The HPSOGWO maximizes the system throughput by optimizing the power distribution of cellular and D2D users. In comparison to previously proposed approaches, this research process and findings ensure that system consistency and implementation are improved in all aspects.},
  archive      = {J_COMCOM},
  author       = {Panduranga Ravi Teja and Pavan Kumar Mishra},
  doi          = {10.1016/j.comcom.2022.08.021},
  journal      = {Computer Communications},
  pages        = {292-302},
  shortjournal = {Comput. Commun.},
  title        = {Path selection and resource allocation for 5G multi-hop D2D networks},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concurrent multi-beam transmissions for reliable
communication in millimeter-wave networks. <em>COMCOM</em>,
<em>195</em>, 281–291. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) communication is an important technology to meet the demand of high data rate in sixth generation (6G) wireless networks. Blockage is a major obstacle to reliable transmissions for mmWave communication. In order to prevent complete blocking of concurrent multiple beams and measure the reliability of concurrent multi-beam transmissions, beam space isolation degree (BSID) and reliability coefficient are defined, based on which a joint multi-beam selection and transmission power allocation scheme that maximizes the tradeoff utility of instantaneous achievable rate and reliability. We break the original optimization problem into two subproblems (i.e., multi-beam selection subproblem and transmission power allocation subproblem) to reduce the computational complexity , and solve them iteratively until no further improvement of the system utility, where exact potential game is used to solve the former under fixed transmission power policy, and first-order Taylor expansion and Lagrangian multiplier algorithm are utilized to solve the latter under fixed concurrent multi-beam selection strategy. Evaluation results indicate that the proposed scheme provides better sum utility and sum reliability with lower complexity, which indicates that concurrent multi-beam transmissions can improve the reliability of mmWave communication.},
  archive      = {J_COMCOM},
  author       = {Yanping Liu and Chunju Tang},
  doi          = {10.1016/j.comcom.2022.08.023},
  journal      = {Computer Communications},
  pages        = {281-291},
  shortjournal = {Comput. Commun.},
  title        = {Concurrent multi-beam transmissions for reliable communication in millimeter-wave networks},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective optimization for coverage aware energy
consumption in wireless 3D video sensor network. <em>COMCOM</em>,
<em>195</em>, 262–280. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of minimizing energy consumption as well as maximizing the coverage area for 3D video sensor nodes deployed over a 2D target area in a wireless video sensor network. Reducing the number of active video sensor nodes helps decrease energy consumption, but decrease the coverage area as well as connectivity in the network. The problem is solved initially by a multi-objective integer linear programming based approach (APP_7) which provides an optimal solution but leads to intractability for a large-sized network due to the non-deterministic polynomial time-hard complexity of the problem. This fact motivates us to design a non-dominated sorting genetic algorithm-II based heuristic (APP_8), which generates a near-optimal solution and that is substantiated by comparing the simulation results of APP_7 with APP_8, and two existing greedy heuristics (EX_1, EX_11) for small-sized problem. Simulation experiments are conducted also to compare the results of APP_8 with EX_1 and EX_11 for large-sized network. Between two heuristics, EX_11/(EX_1) is better in terms of energy consumption/(area coverage). It has been observed that for the large-sized network (i.e. node density 100 on the target area of size 75 × 75 square meters) APP_8 is able to reduce energy consumption by 47.76\% from EX_11, whereas area coverage is decreased by 0.51\% from EX_1, which confirms the acceptability of APP_8 as the proper heuristic in such a scenario.},
  archive      = {J_COMCOM},
  author       = {Kishalay Bairagi and Sulata Mitra and Uma Bhattacharya},
  doi          = {10.1016/j.comcom.2022.08.010},
  journal      = {Computer Communications},
  pages        = {262-280},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective optimization for coverage aware energy consumption in wireless 3D video sensor network},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated approach to web offensive security.
<em>COMCOM</em>, <em>195</em>, 248–261. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web Application Penetration testing is a popular approach that aims at discovering vulnerabilities by emulating real attacks. Experts often use a variety of publicly available attack tools, define attack methodologies and orchestrate them throughout the separate phases of testing. In doing so, they leverage personal experience and intuition, making any automation effort very challenging. In this paper, we propose the design and implementation of a framework for Web Penetration Testing that allows for the integration, as well as orchestration, of several types of attacks. We identify the generic tasks performed during a penetration test. Then, we provide a way to integrate attacks that implement such tasks in a component responsible for executing them. A further component holds the logic that decides which task to execute and aggregates the results of completed tasks. We also define the communication protocol between the two components to enable the orchestration of tasks across all phases of a testing campaign. As a concrete example of the application of the proposed framework, we show how it is possible to integrate several types of attacks, as well as embed an ad hoc defined behavioral model in order to discover cross-site scripting vulnerabilities.},
  archive      = {J_COMCOM},
  author       = {Nicola Auricchio and Andrea Cappuccio and Francesco Caturano and Gaetano Perrone and Simon Pietro Romano},
  doi          = {10.1016/j.comcom.2022.08.018},
  journal      = {Computer Communications},
  pages        = {248-261},
  shortjournal = {Comput. Commun.},
  title        = {An automated approach to web offensive security},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). INIDS: SWOT analysis and TOWS inferences of state-of-the-art
NIDS solutions for the development of intelligent network intrusion
detection system. <em>COMCOM</em>, <em>195</em>, 227–247. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of ubiquitous networked devices and the proliferation of geographically dispersed ‘Internet of Thing’ devices have exponentially increased network traffic. The socio-economical society is highly dependent on modern devices, and unavailability may lead to catastrophic results for even a short time. The less secure and heterogeneous devices in the public domain have shaped a cyber-attack surface in the cloud environment. Traditional approaches for Network Intrusion Detection Systems have proven ineffective and insufficient in defending against zero-day attacks. This article visited the advancements in the intrusion detection realm in the last five years and conducted a comprehensive retrospection of modern network intrusion detection systems. The authors have performed a comprehensive SWOT (Strength, Weakness, Opportunities, Threats) analysis of contemporary Network Intrusion Detection Systems in multiple technology dimensions, including big-data processing of high volume network traffic, machine learning , deep learning for self-learning machines, readiness for zero-day attacks, distributed processing, cost-effective solution, and ability to perform autonomous operations. The paper turns SWOT analysis into TOWS inferences from the retrospective study for strategy formulation and features the attributes of a futuristic NIDS solution. The article concludes with the discussion and future scope as the pinnacle of security solution development against zero-day attacks.},
  archive      = {J_COMCOM},
  author       = {Jyoti Verma and Abhinav Bhandari and Gurpreet Singh},
  doi          = {10.1016/j.comcom.2022.08.022},
  journal      = {Computer Communications},
  pages        = {227-247},
  shortjournal = {Comput. Commun.},
  title        = {INIDS: SWOT analysis and TOWS inferences of state-of-the-art NIDS solutions for the development of intelligent network intrusion detection system},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy and congestion-aware load balanced multi-path routing
for wireless sensor networks in ambient environments. <em>COMCOM</em>,
<em>195</em>, 217–226. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In wireless sensor networks (WSNs), a large number of sensor nodes (SNs) equipped with batteries are randomly distributed around an area in order to detect and capture sensor data. In WSNs, congestion is created in the node due to inefficient load balancing and routing. We proposed the congestion-aware load balancing approach considering the highly dynamic traffic scenario. We proposed the traffic rate aware cluster formation for efficient load balancing using a harmony search algorithm. In addition, modified Giraffe kicking optimization has been used for clustering and routing. The load has been aggressively balancing, and the slot has been utilized the slot when no data packets are available with the owner node. The proposed work has been simulated and validated with state-of-the-art congestion-aware algorithms. In our simulation, we validate our proposal on several standard parameters such as network lifetime, throughput, re-transmission frequency, standard deviation of energy consumption etc.},
  archive      = {J_COMCOM},
  author       = {P. Suman Prakash and D. Kavitha and P. Chenna Reddy},
  doi          = {10.1016/j.comcom.2022.08.012},
  journal      = {Computer Communications},
  pages        = {217-226},
  shortjournal = {Comput. Commun.},
  title        = {Energy and congestion-aware load balanced multi-path routing for wireless sensor networks in ambient environments},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel insurance claim blockchain scheme based on
zero-knowledge proof technology. <em>COMCOM</em>, <em>195</em>, 207–216.
(<a href="https://doi.org/10.1016/j.comcom.2022.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to ensure the privacy and authenticity of patients’ medical data in the medical insurance claim process, but in the current medical insurance claim process, there are some problems such as low efficiency, complex service, unreliable data and data leakage. Therefore, considering the privacy and sensitivity of patients’ medical data, we can improve the current issues by employing blockchain , smart contracts and zero-knowledge proof technology. In this paper, we propose a novel medical insurance claim scheme based on smart contracts, blockchain and zero-knowledge proof. Our scheme mainly involves two scenarios: medical insurance purchasing and medical insurance claiming. In the privacy-preserving transaction phases of the two scenarios, we can ensure the legitimacy and privacy of the transactions between the patients and the insurance companies by using a non-interactive zero-knowledge proof and the homomorphic encryption algorithm under the Decisional Bilinear Diffie–Hellman (DBDH) assumption. In the identity privacy-preserving phases of the two scenarios, we can ensure the legitimacy and the privacy of patients’ identities by integrating Schnorr protocol and Fiat–Shamir heuristic method . The security analysis, the computation cost and the communication cost of our scheme are given. Compared with our referred schemes, the performance evaluation shows that our scheme not only meets the requirements of the legality of the medical insurance claim, but also ensures the authenticity and privacy of the patients’ medical data. Moreover, the experimental results demonstrate that our scheme is feasible and has an acceptable time overhead.},
  archive      = {J_COMCOM},
  author       = {Houyu Zheng and Lin You and Gengran Hu},
  doi          = {10.1016/j.comcom.2022.08.007},
  journal      = {Computer Communications},
  pages        = {207-216},
  shortjournal = {Comput. Commun.},
  title        = {A novel insurance claim blockchain scheme based on zero-knowledge proof technology},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact of hidden node problem in association and data
transmission for LAA wi-fi coexistence. <em>COMCOM</em>, <em>195</em>,
187–206. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-cell LTE and Wi-Fi networks has been globally deployed in the unlicensed 5 GHz bands, leading to the need for new coexistence regulations between two very different access technologies. To address coexistence challenges, 3GPP standardized LTE Licensed Assisted Access (LAA) in 5 GHz bands through the incorporation of similar sensing and back-off features. The success of LAA’s fair and efficient coexistence with Wi-Fi can be considered a benchmark for collaborative cellular operation in unlicensed bands. In this paper, we discuss the hidden node scenario when T-Mobile LAA coexists with the Wi-Fi APs that we deployed at the Illinois Institute of Technology (IIT) university campus. In this setup, we observed problems on the Wi-Fi client association and inefficient data transmission when it tries to connect to its corresponding Wi-Fi APs. As LAA is unaware of the Wi-Fi AP’s transmission (low transmission power received at LAA BS), LAA BS continuously transmits with maximum transmission opportunity time, thus highly impacting Wi-Fi performance. Further, the hidden node problem is modeled analytically capturing the root cause of inefficiency.},
  archive      = {J_COMCOM},
  author       = {Vanlin Sathya and Muhammad Iqbal Rochman and Thomas Valerian Pasca and Monisha Ghosh},
  doi          = {10.1016/j.comcom.2022.08.009},
  journal      = {Computer Communications},
  pages        = {187-206},
  shortjournal = {Comput. Commun.},
  title        = {Impact of hidden node problem in association and data transmission for LAA wi-fi coexistence},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple objective optimization-based DV-hop localization
for spiral deployed wireless sensor networks using non-inertial
opposition-based class topper optimization (NOCTO). <em>COMCOM</em>,
<em>195</em>, 173–186. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of localization is one of most important issues in wireless sensor networks . Furthermore, it is critical to monitor and evaluate the data gathered. For a variety of factors, such as upkeep, lifespan, and breakdown, the fixed density of these beacons may be increased or decreased. Because of its robustness, flexibility, and economic viability, a well-known technique for locating wireless sensor network nodes is the Distance Vector-Hop (DV-Hop) algorithm. As a result, researchers continue to look for ways to develop it. A new Non-inertial Opposition based Class Topper Optimization (NOCTO) based enhanced DV-Hop localization algorithm is proposed. It also focuses through an optimized formulation to compute the average hop-size with weight of beacon nodes in order to reduce the localization error with estimated distance between the beacon and the dumb node, due to improved localization accuracy . For spiral deployed 2 D 2D wireless sensor networks , this paper proposes a multi-objective NOCTO-based DV-Hop localization. The simulation results indicate that our suggested multi-objective function outperforms some existing techniques.},
  archive      = {J_COMCOM},
  author       = {Tapan Kumar Mohanta and Dushmanta Kumar Das},
  doi          = {10.1016/j.comcom.2022.08.019},
  journal      = {Computer Communications},
  pages        = {173-186},
  shortjournal = {Comput. Commun.},
  title        = {Multiple objective optimization-based DV-hop localization for spiral deployed wireless sensor networks using non-inertial opposition-based class topper optimization (NOCTO)},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TDMA policy to optimize resource utilization in wireless
sensor networks using reinforcement learning for ambient environment.
<em>COMCOM</em>, <em>195</em>, 162–172. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data packet reaches from the end node to sink in a multihop fashion in the internet of things (IoTs) and sensor networks. Usually, a head node (among neighboring or special purpose nodes) can collect data packets from the nodes and forward them further to sink or other head nodes. In Time-division multiple access (TDMA) driven scheduling, nodes often own slots in a time frame and are scheduled for data forwarding in the allotted time slot (owner node) in each time frame. A time frame in which the owner node does not have data to forward goes into sleep mode. Though the supposed owner node is in sleep mode, the corresponding head node is active throughout the time frame. This active period of a head node can cause an increase in energy consumption. Besides, because the head node in an active state does not receive a data packet, it is causing significantly to the throughput, ultimately leading to low channel utilization. We propose the Markov design policy (MDP) for such head nodes to reduce the number of time slots wasted in the time frame in our work. The proposal is the first such kind of MDP-based modeling for node scheduling in TDMA. The simulation results show that the proposed method outperforms existing adaptive scheduling algorithms for channel utilization, end-to-end delay, system utilization, and balance factor.},
  archive      = {J_COMCOM},
  author       = {Dinesh Kumar Sah and Tarachand Amgoth and Korhan Cengiz and Yasser Alshehri and Noha Alnazzawi},
  doi          = {10.1016/j.comcom.2022.08.013},
  journal      = {Computer Communications},
  pages        = {162-172},
  shortjournal = {Comput. Commun.},
  title        = {TDMA policy to optimize resource utilization in wireless sensor networks using reinforcement learning for ambient environment},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cooperative mobility model for multiple autonomous
vehicles. <em>COMCOM</em>, <em>195</em>, 148–161. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mobility model is a basis of constructing the simulation environment for vehicular ad hoc network (VANET) research. Most existing models mainly focus on the geographical movement of individual mobile communication devices. However, few works focus on the cooperative movement of multiple autonomous vehicles. In this paper, we propose a cooperative mobility model for multiple autonomous vehicles, making vehicles run as a swarm in an orderly manner. Specifically, inspired by artificial fish swarm algorithms, we draw on the cooperative behaviors of the fish swarm to model the collaboration and self-organization in multi-vehicle formation. Then we design several force functions to express the interactions between vehicles and the influence of the driving environment based on the artificial potential field . Under Newtonian dynamics, the proposed mobility model determines the coordinated movement of multiple autonomous vehicles by force functions. Furthermore, we introduce a parallel orientation area in the interaction area division to improve vehicle stability. Following existing works, we assume that the road is straight and of infinite length. This is, the considered environment is suitable for intersection-free double-lane roads. To comprehensively verify the effectiveness of our proposed approach, we conduct extensive simulations under different traffic scenarios. Simulation results confirm that using our mobility model, multiple vehicles are able to keep driving in the center of the lane at the allowed speed limit, form an ordered collision-free motorcade, and collaboratively avoid collisions with obstacles. Particularly, our proposed mobility model has better stability.},
  archive      = {J_COMCOM},
  author       = {Libing Wu and Shuqin Cao and Yanjiao Chen and Jianxin Li and Jianqun Cui and Yanan Chang},
  doi          = {10.1016/j.comcom.2022.08.017},
  journal      = {Computer Communications},
  pages        = {148-161},
  shortjournal = {Comput. Commun.},
  title        = {A cooperative mobility model for multiple autonomous vehicles},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Floating nodes assisted cluster-based routing for efficient
data collection in underwater acoustic sensor networks. <em>COMCOM</em>,
<em>195</em>, 137–147. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exhaust research work has been carried out in the field of Terrestrial Wireless Sensor Networks (TWSN). Now, research penetration is moving toward Underwater Acoustic Sensor Networks (UASNs). The communication of UASNs has dependencies on acoustic instead of radio. Efficient data collection by the UASN seems a tedious job in comparison with TWSN due to the peculiar features of underwater communication. However, there is a way to accomplish efficient data collection metrics through the design of routing protocols by considering the unique features of UASN communications. In this context, we propose a novel scheme and we call it Floating Nodes assisted Cluster-Based Routing (FNCBR) scheme for efficient data collection in UASNs. In FNCBR, the network space is divided into cuboids to form clusters. Then, every cuboid is consigned with a floating node (FN) at the surface layer and two fixed cluster heads (CHs) are suspended at different depth levels. All CHs of cuboids are supposed to be connected with a floating node via a wired connection, while source nodes are haphazardly distributed in the whole network region. In FNCBR, source nodes are liable to send the sensed data either to the FN or to the nearest CH. The data collected by the CHs is moved towards the FNs, which further disseminate the data to the on-shore monitoring center via a Radio Frequency (RF) link. We conduct the simulations in Network Simulator (NS-3) to appraise the proposed FNCBR protocol with other clustering protocols (e.g., CVBF, BS-CVBF, and ANCRP) in terms of different performance metrics. After describing the results, a quantitative comparison table is also given. The FNCBR scheme is also evaluated in terms of multiple FN densities. The simulation outcomes justify that the proposed FNCBR scheme has given better results in all performance metrics than the baseline schemes.},
  archive      = {J_COMCOM},
  author       = {Ghullam Murtaza Jatoi and Bhagwan Das and Sarang Karim and Jitander Kumar Pabani and Moez Krichen and Roobaea Alroobaea and Mahender Kumar},
  doi          = {10.1016/j.comcom.2022.08.014},
  journal      = {Computer Communications},
  pages        = {137-147},
  shortjournal = {Comput. Commun.},
  title        = {Floating nodes assisted cluster-based routing for efficient data collection in underwater acoustic sensor networks},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resilience of space information network based on combination
of complex networks and hypergraphs. <em>COMCOM</em>, <em>195</em>,
124–136. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The space information network provides global users with high-speed information services that can be accessed anytime, anywhere. Its enormous economic, social and military value makes it vulnerable to large-scale targeted cyber attacks . Therefore, enhancing the resiliency of services is critical. We propose to solve the above problems from the perspective of complex networks and hypergraph fusion. A space information network model integrating complex networks and hypergraphs is constructed. Edges represent physical links between entities, and hyperedges represent information services logically composed of multiple entities. In the new model, the network efficiency within each service is comprehensively considered, and a more reasonable new measure for evaluating network service capability is proposed, called service efficiency. In the vulnerability stage of resilience, a local edge efficiency-betweenness removal strategy is proposed to identify important edges in the network, which can preserve the most performance by hardening them before an attack occurs. When the network suffers an inevitable attack, the recovery strategies of the central node based on global information and community information are proposed to restore the network performance with the fewest components. The former considers the degree of nodes in the entire hypergraph, and the latter considers the more closely connected hypergraph community structure. Simulation results under five sets of network parameters show that all our schemes apply in all cases. Hardening the links elected by the local edge efficiency-betweenness strategy in advance can enhance the network’s survivability . When the network is damaged, the recovery strategy of the central node based on global information is better under single-step recovery, while the recovery strategy of the central node based on community information is better under synchronous recovery. Using the proposed hardening and recovery strategies together, resiliency can be improved by up to 160\%.},
  archive      = {J_COMCOM},
  author       = {Le Zhang and Ye Du},
  doi          = {10.1016/j.comcom.2022.08.016},
  journal      = {Computer Communications},
  pages        = {124-136},
  shortjournal = {Comput. Commun.},
  title        = {Resilience of space information network based on combination of complex networks and hypergraphs},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BioMixD: A bio-inspired and traffic-aware mix zone placement
strategy for location privacy on the internet of drones.
<em>COMCOM</em>, <em>195</em>, 111–123. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the next years, drones will be part of our daily lives composing a parallel environment “over our heads,” having a dynamic traffic flow. The Internet of Drones (IoD) defines a robust and reliable distributed mobile network to manage and provide fair navigation and communication to drones. In this envisioned network environment, location privacy will be a paramount requirement to protect aerial and grounded devices. There is a lack of studies that investigate location privacy in the IoD. Our previous work was the first to propose a Location Privacy Protection Mechanism (LPPM) for IoD to the best of our knowledge. However, this mechanism does not consider the dynamics of the IoD traffic flow, hindering its performance. Hence, in this study, we present BioMixD, a bio-inspired and traffic-aware Mix Zone placement strategy for location privacy on the IoD. We integrate this strategy with our previous solution, leveraging an enhanced LPPM named t-MixDrones. We conducted an extensive experimental evaluation, investigating a wide range of IoD scenarios, and compared t-MixDrones with other mechanisms. The results highlighted that t-MixDrones overcomes the mechanisms in all scenarios, providing a better location privacy level, mainly drone coverage, and anonymization rate. Hence, we advance the state-of-the-art of LPPMs for the IoD environment.},
  archive      = {J_COMCOM},
  author       = {Alisson R. Svaigen and Azzedine Boukerche and Linnyer B. Ruiz and Antonio A.F. Loureiro},
  doi          = {10.1016/j.comcom.2022.07.012},
  journal      = {Computer Communications},
  pages        = {111-123},
  shortjournal = {Comput. Commun.},
  title        = {BioMixD: A bio-inspired and traffic-aware mix zone placement strategy for location privacy on the internet of drones},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on vital signs monitoring based on wi-fi CSI data.
<em>COMCOM</em>, <em>195</em>, 99–110. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic further highlighted the need to use low-cost remote monitoring procedures for medical patients. Since the results reported in the literature have shown that the use of Channel State Information (CSI) from Wi-Fi networks to remotely monitor patients can provide means to obtain a powerful medical information package in a non-invasive way and at low cost, a consistent review and analysis of the state of the art on this applied technique is developed in the present work. Initially, a mathematical overview of the CSI technology and its functional model is done. Subsequently, details about the technical approach necessary to use CSI in medical applications and a summary of the studies reported in the literature with such applications are presented. Based on the analyses and discussions carried out throughout this work, a better understanding of the current state of the art is achieved. Challenges and perspectives for future research are also highlighted.},
  archive      = {J_COMCOM},
  author       = {Julio C.H. Soto and Iandra Galdino and Egberto Caballero and Vinicius Ferreira and Débora Muchaluat-Saade and Célio Albuquerque},
  doi          = {10.1016/j.comcom.2022.08.004},
  journal      = {Computer Communications},
  pages        = {99-110},
  shortjournal = {Comput. Commun.},
  title        = {A survey on vital signs monitoring based on wi-fi CSI data},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Channel hopping for blind rendezvous in cognitive radio
networks: A review. <em>COMCOM</em>, <em>195</em>, 82–98. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive radio networks enable unlicensed users to communicate using the licensed spectrum without causing interference to legitimate users. To establish a communication link, two or more cognitive users must simultaneously visit a common available channel and exchange the handshake information, such a process is referred to as rendezvous . The simplest solution to the rendezvous problem is to use a dedicated common control channel . However, it can suffer severe congestion or even be occupied by incumbent users, limiting the functioning of the cognitive network. Channel hopping (CH) techniques overcome these drawbacks and allow cognitive users to rendezvous on any common available channel. In this paper a classification framework is proposed based on relevant features for the design of CH rendezvous sequences. Using the proposed framework, an unprecedented number of CH sequences are reviewed and classified. Theoretical performance metrics of existing CH sequences are summarized. Finally, open issues and research directions for CH sequences design are discussed.},
  archive      = {J_COMCOM},
  author       = {Erik Ortiz Guerra and Vitalio Alfonso Reguera and Cristian Duran-Faundez and Thi Mai Trang Nguyen},
  doi          = {10.1016/j.comcom.2022.08.011},
  journal      = {Computer Communications},
  pages        = {82-98},
  shortjournal = {Comput. Commun.},
  title        = {Channel hopping for blind rendezvous in cognitive radio networks: A review},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-view attention-based deep learning framework for
malware detection in smart healthcare systems. <em>COMCOM</em>,
<em>195</em>, 73–81. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent security attack reports show that the number of malware attacks is gradually growing over the years due to the rapid adoption of smart healthcare systems. The development of a safe and secure smart healthcare system is considered to be important from a security point of view. Malware detection is an essential subsystem in healthcare ecosystems to secure the system from malware attacks. The literature survey shows that malware detection is done using deep learning with either portable executable (PE)-Header or PE-Imports or PE-Image or application programming interface (API) calls. However, each of these feature sets is important in PE files to boost the malware detection rate. This work proposes a Multi-View attention-based deep learning framework for malware detection by considering features of PE-Header, PE-Imports, PE-Image, and API calls. Detailed evaluation and experimental analysis of the proposed method is shown on the malware detection benchmark datasets. The proposed approach performed better than the machine learning-based and non-attention-based approaches with an accuracy of 95\% for malware detection using features from PE-Header, PE-Imports, PE-Image, and API calls. In addition, detailed evaluation results are included for image-based malware detection on datasets from Windows and Android operating systems. In the Windows-based dataset, the proposed approach showed an accuracy of 98\% and an accuracy of 97\% in the Android-based dataset. Also, the proposed approach performed better than the existing malware detection approaches. Experimental results on three malware datasets indicate that the proposed method is robust and generalizable for both Windows and Android-based malware detection in smart healthcare systems.},
  archive      = {J_COMCOM},
  author       = {Vinayakumar Ravi and Mamoun Alazab and Shymalagowri Selvaganapathy and Rajasekhar Chaganti},
  doi          = {10.1016/j.comcom.2022.08.015},
  journal      = {Computer Communications},
  pages        = {73-81},
  shortjournal = {Comput. Commun.},
  title        = {A multi-view attention-based deep learning framework for malware detection in smart healthcare systems},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient collision-slot utilization for missing tags
identification in RFID system. <em>COMCOM</em>, <em>195</em>, 61–72. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Frequency Identification (RFID) technology has become one of the most promising building blocks in future IoT-enabled applications, including supply chain management, intelligent logistics, inventory control, etc. . This paper studies the fundamental problem of completely identifying missing tags in RFID systems. The most important measure for missing tag identification is to minimize the execution time. Most of the existing missing tag identification schemes compare the singleton slot in the expected mapping vector with the actual mapping vector observed by a reader to determine whether the corresponding tag is missing or not. However, they are of low efficiency because the useless collision slots account for a large proportion of the slots in a time frame. This paper proposes a Collision Unfolding-based Missing tag Identification (CUMI) protocol, which applies a novel indicator vector and Manchester coding techniques for transforming parts of abandoned collision-slots into valuable slots, thereby the utilization of slots in a time frame gets a promotion. However, CUMI requires transmitting long indicator vectors to tags, which incurs too much communication overhead . To address this problem, we propose an Enhanced Collision Unfolding-based Missing tag Identification (ECUMI) protocol to further compress the length of indicator vectors at the expense of longer tag responses. Throughout the simulation, we find that ECUMI has a higher time efficiency in missing tag identification than CUMI does. Extensive simulation results reveal that CUMI and ECUMI outperform the state-of-the-art missing tag identification protocols by at least 1 . 02 × 1.02× and 1 . 8 × 1.8× in the single-reader scenario, respectively.},
  archive      = {J_COMCOM},
  author       = {Kaimin Guo and Xin Xie and Sheng Chen and Heng Qi and Keqiu Li},
  doi          = {10.1016/j.comcom.2022.07.053},
  journal      = {Computer Communications},
  pages        = {61-72},
  shortjournal = {Comput. Commun.},
  title        = {Efficient collision-slot utilization for missing tags identification in RFID system},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HDECO: A method for decreasing energy and cost by using
virtual machine migration by considering hybrid parameters.
<em>COMCOM</em>, <em>195</em>, 49–60. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy and cost are important issues in cloud computing , which led to increased power supply consumption and more carbon dioxide production. In this paper, cost and energy have been studied with other methods and ECIB algorithm. By the study of the ECIB and previous methods, the HDECO method is presented. This paper provides a method to decrease energy and cost in instance-intensive cloud workflows, using virtual machine migration . In the proposed method, creating a classification of inputs, calculating real execution time, and distance parameters, as well as using an intelligent threshold detector (ITD) compared to the study of the previous methods, has improved cost and energy, which according to the specified parameters, an optimum solution is achieved. In the HDECO method, energy has been improved by creating an objective function and combining parameters into the fitness function. By observing the provided parameters, the actual execution time is improved, and also dynamic threshold is used in the proposed method to reduce energy and cost. In this method, the accuracy of prediction increased by classification of inputs to different levels, as well as applying the ITD to the previous tasks and the ECIB algorithm. In our method, the allocation of the specified parameters such as ITD, the actual execution time, and the number of migrations have changed compared to the previous methods and ECIB. Finally, by categorizing the inputs related to the processing power of the resources as well as the proper allocation of them to a resource that has less work and suitable load-balancing, the execution time is optimized and the amount of energy consumption and cost is improved.},
  archive      = {J_COMCOM},
  author       = {Arash Ghorbannia Delavar and Reza Akraminejad and Sahar Mozafari},
  doi          = {10.1016/j.comcom.2022.08.006},
  journal      = {Computer Communications},
  pages        = {49-60},
  shortjournal = {Comput. Commun.},
  title        = {HDECO: A method for decreasing energy and cost by using virtual machine migration by considering hybrid parameters},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Passive delay measurement for fidelity monitoring of
distributed network emulation. <em>COMCOM</em>, <em>195</em>, 40–48. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emulation has become a popular approach for the validation and evaluation of network research. It provides researchers with a contained, customizable, and scalable testing environment, which can be easily packaged and published for potential readers to reproduce their results. However, as the network components are only virtual, emulation lacks the inherent realism of physical testbeds . In light of this, monitoring specific metrics of the emulated network has been proposed as a solution to mitigate to some degree inaccuracies caused by emulation. While this is not difficult to implement in a single-machine setting (e.g. with M ininet), monitoring is limited by the lack of time synchronization in scenarios where the emulation is distributed over multiple physical machines (e.g., D istrinet). In this paper we tackle the case of packet delay monitoring, to which we propose a methodology for passively measuring one-way delays with underlying assumptions about time synchronization, and round-trip delays otherwise. For an efficient implementation of our methodology, we propose an e BPF-based packet measurement tool that performs better than current packet sniffers under emulation-specific assumptions. We implement and evaluate our system in an open testbed and show that it can reach results within few microseconds of perfect accuracy and precision.},
  archive      = {J_COMCOM},
  author       = {Houssam ElBouanani and Chadi Barakat and Walid Dabbous and Thierry Turletti},
  doi          = {10.1016/j.comcom.2022.07.004},
  journal      = {Computer Communications},
  pages        = {40-48},
  shortjournal = {Comput. Commun.},
  title        = {Passive delay measurement for fidelity monitoring of distributed network emulation},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure biometric-based access control scheme for future
IoT-enabled cloud-assisted video surveillance system. <em>COMCOM</em>,
<em>195</em>, 27–39. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new access control mechanism for video surveillance system based on user’s biometrics , which allows to access real-time video surveillance data from deployed Internet of Things (IoT) smart devices. For this purpose, mutual authentication is performed between a user and accessed smart devices via the cloud server(s) and the session keys are then established to make secure communication. The proposed scheme allows accessing real time video and older videos that are in cloud servers, and also user’s credential update phase. Through the security analysis, it has been shown that the proposed scheme is robust against a variety of potential passive/active attacks. A detailed comparative study shows the efficacy of the proposed scheme as compared to other existing solutions. Finally, the real testbed experiments have been carried out for secure surveillance system using Raspberry PI devices as IoT devices, user’s devices and servers to show its practical application.},
  archive      = {J_COMCOM},
  author       = {Palak Bagga and Ankush Mitra and Ashok Kumar Das and Pandi Vijayakumar and YoungHo Park and Marimuthu Karuppiah},
  doi          = {10.1016/j.comcom.2022.08.003},
  journal      = {Computer Communications},
  pages        = {27-39},
  shortjournal = {Comput. Commun.},
  title        = {Secure biometric-based access control scheme for future IoT-enabled cloud-assisted video surveillance system},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defect identification for oil and gas pipeline safety based
on autonomous deep learning network. <em>COMCOM</em>, <em>195</em>,
14–26. (<a href="https://doi.org/10.1016/j.comcom.2022.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safety detection for oil and gas pipelines is more and more worthy of attention. It not only promotes the development of pipeline safety work, but also provides a guarantee for pipeline safety decision management. However, there are more and more safety problems in pipeline operation, causing immeasurable consequences. Therefore, the pipeline safety detection technology needs to be further improved. In this paper, a two-axis magnetic flux leakage detection device is used for safety detection of an oil and gas pipeline, and the detection results are analyzed and studied. 77 sets of detection data are collected through the detection device. Due to the harsh environment of the oil and gas station, the data is severely disturbed, so the data is filtered firstly. The filtered data can better reflect the safety status information of the pipeline. Secondly, In order to avoid the random error of single-axis data, a two-dimensional data fusion method is proposed. The fusion data improves the accuracy of recognition of pipeline failure features. Thirdly, autonomous deep learning recognition algorithm is used to classify and recognize pipeline failure features. The network in this algorithm includes convolutional layers , pooling layers and fully connected layers. Through multiple simulation calculations, the number of network layers has been optimized. Finally, experiments are carried out based on the data collected on-site. The experiment results show that the training accuracy is 99.19\%, and the testing accuracy is 97.38\%. In short, the entire pipeline safety inspection data processing algorithm reliably identifies the types of pipeline failure defects. And it will provide a basis for the safe construction of pipelines.},
  archive      = {J_COMCOM},
  author       = {Min Zhang and Yanbao Guo and Qiuju Xie and Yuansheng Zhang and Deguo Wang and Jinzhong Chen},
  doi          = {10.1016/j.comcom.2022.08.001},
  journal      = {Computer Communications},
  pages        = {14-26},
  shortjournal = {Comput. Commun.},
  title        = {Defect identification for oil and gas pipeline safety based on autonomous deep learning network},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel sensor deployment approach in IIoT-based
environmental monitoring system in underground coal mines.
<em>COMCOM</em>, <em>195</em>, 1–13. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bord and Pillar methods are widely popular in an underground coal mine. Continuous monitoring of the environment is crucial to ensure the safety of miners. The Industrial Internet of Things (IIoT) is the contemporary technological advancement of wireless networks that makes environmental monitoring more sophisticated. However, deployment of sensor nodes is a complicated task as it is linear in structure. Furthermore, it is prone to failure owing to energy drain near the sink node. This paper proposes a hybrid sensor deployment approach which is implemented in a linear and integrated mine networks. This hybrid approach is an integration of two sensor deployment methods namely multi-level and grid methods. The simulation results prove that the proposed technique enhances the network’s lifetime by limiting the tunneling effect near the sink. Furthermore, it uses more than 80\% of the mean energy in the network system, compared to previous algorithms, which frequently fail even before utilizing 50\% of the energy, and in some circumstances, even less. Hence, the proposed method brings an economic benefit by expanding the lifespan of the network through an increased number of nodes around the sink and minimizes the tunneling effect at each level in the network.},
  archive      = {J_COMCOM},
  author       = {G. Thirumal and Chiranjeev Kumar},
  doi          = {10.1016/j.comcom.2022.08.002},
  journal      = {Computer Communications},
  pages        = {1-13},
  shortjournal = {Comput. Commun.},
  title        = {Multilevel sensor deployment approach in IIoT-based environmental monitoring system in underground coal mines},
  volume       = {195},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controller robust placement with dynamic traffic in
software-defined networking. <em>COMCOM</em>, <em>194</em>, 458–467. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For dynamic traffic of switches in the software-defined networking, the existing methods mostly adopt the switch migration to balance the controller with an uneven load. However, these researches have overlooked that the frequent migration of switches will cause instability in the control plane and increase the network delay. To reduce switch migration, this paper proposes a robust controller placement model that considers dynamic traffic while placing the controllers. Firstly, the bounded symmetric interval is used to describe the dynamic traffic in the network, and the magnitude of the traffic change is represented by the variation parameter σ σ . Secondly, the control parameter Γ Γ is introduced to control the conservativeness of the solution and construct a multi-controller robust placement model. Finally, the model is transformed into a linear programming problem based on the strong duality theory, and we use the tabu search algorithm to implement an efficient solution. The simulation results show that different controller placement strategies can be obtained by changing the control parameter. When the service level is low, the cost increase can greatly improve the service level; however, when the service level exceeds 90\%, the 10\% increase in placement cost can only improve the service level by 1\%.},
  archive      = {J_COMCOM},
  author       = {Zhen Zhang and Jie Lu and Hongchang Chen},
  doi          = {10.1016/j.comcom.2022.07.018},
  journal      = {Computer Communications},
  pages        = {458-467},
  shortjournal = {Comput. Commun.},
  title        = {Controller robust placement with dynamic traffic in software-defined networking},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traffic flow prediction using multi-view graph convolution
and masked attention mechanism. <em>COMCOM</em>, <em>194</em>, 446–457.
(<a href="https://doi.org/10.1016/j.comcom.2022.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is one of the essential technologies in the intelligent transportation system . Graph convolution is naturally suitable for graph structure data mining and is widely used to forecast traffic data. However, most graph convolution methods rely on the Euclidean distance adjacency matrix , which cannot mine richer semantic information. In addition, regardless of static or dynamic graph convolution methods, many graph convolution methods adopt the same adjacency matrix to mine graph information for each convolution layer . In reality, the proportion of abnormal traffic flows such as congestion is slight. Therefore, those networks tend to learn normal traffic patterns. This paper proposes a deep learning model including a dilated Temporal causal convolution module, multi-view diffusion Graph convolution module, and masked multi-head Attention module (TGANet) to address the above issues. The dilated temporal causal convolution is used to mine the temporal correlation of traffic data. Multi-view diffusion graph convolution operation leverages non-euclidean metrics like Jaccard distance and Pearson correlation to mine semantic information. In addition, the masked multi-head attention module is applied to mine fine-grained spatiotemporal dynamics. Experimental results on two public traffic data sets demonstrate that the proposed method outperforms other counterpart methods in most cases.},
  archive      = {J_COMCOM},
  author       = {Lingqiang Chen and Pei Shi and Guanghui Li and Tao Qi},
  doi          = {10.1016/j.comcom.2022.08.008},
  journal      = {Computer Communications},
  pages        = {446-457},
  shortjournal = {Comput. Commun.},
  title        = {Traffic flow prediction using multi-view graph convolution and masked attention mechanism},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end delay guaranteed service function chain
deployment: A multi-level mapping approach. <em>COMCOM</em>,
<em>194</em>, 433–445. (<a
href="https://doi.org/10.1016/j.comcom.2022.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) enables service providers to maximize the business profit via resource-efficient QoS provisioning for customer requested Service Function Chains (SFCs). In recent applications, end-to-end delay is one of the crucial QoS requirements that need to be guaranteed in SFC deployment. Whereas it has been considered in the literature, accurate and comprehensive modeling of the delay in the problem of maximizing the service provider’s profit has not yet been addressed. In this paper, at first, the problem is formulated as a mixed-integer convex programming model. Then, by decomposing the model, a two-level mapping algorithm is developed, that at the first level, maps the functions of the SFCs to virtual network function instances; and in the second level, deploys the instances in the physical network. Simulation results show that the proposed approach achieves a near-optimal solution in comparison to the performance bound obtained by the optimization model, and also superiors the existing solutions.},
  archive      = {J_COMCOM},
  author       = {Fatemeh Yaghoubpour and Bahador Bakhshi and Fateme Seifi},
  doi          = {10.1016/j.comcom.2022.08.005},
  journal      = {Computer Communications},
  pages        = {433-445},
  shortjournal = {Comput. Commun.},
  title        = {End-to-end delay guaranteed service function chain deployment: A multi-level mapping approach},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vulnerabilities of the 6P protocol for the industrial
internet of things: Impact analysis and mitigation. <em>COMCOM</em>,
<em>194</em>, 411–432. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 6TiSCH architecture defined by the IETF provides a standard solution for extending the Internet of Things (IoT) paradigm to industrial applications with stringent reliability and timeliness requirements. In this context, communication security is another crucial requirement, which is currently less investigated in the literature. In this article, we present a deep assessment of the security vulnerabilities of 6P, the protocol used for resource negotiation at the core of the 6TiSCH architecture. Specifically, we highlight two possible attacks against 6P, namely the Traffic Dispersion and the Overloading attacks. These two attacks effectively and stealthy alter the communication schedule of victim nodes and severely thwart network basic functionalities and efficiency, by specifically impacting network availability and energy consumption of victim nodes. To assess the impact of the attacks two analytical models have been defined, while, to demonstrate their feasibility, they have been implemented in Contiki-NG. The implementation has been used to quantitatively evaluate the impact of the two attacks by both simulations and measurements in a real testbed . Our results show that the impact of both attacks may be very significant. The impact, however, strongly depends on the position of the victim node(s) in the network and it is highly influenced by the dynamics of the routing protocol. We have investigated mitigation strategies to alleviate this impact and proposed an extended version of the Minimal Scheduling Function (MSF), i.e., the reference scheduling algorithm for 6TiSCH. This allows network nodes to early detect anomalies in their schedules possibly due to an Overloading attack, and thus curb the attack impact by appropriately revising their schedule.},
  archive      = {J_COMCOM},
  author       = {Francesca Righetti and Carlo Vallati and Marco Tiloca and Giuseppe Anastasi},
  doi          = {10.1016/j.comcom.2022.07.054},
  journal      = {Computer Communications},
  pages        = {411-432},
  shortjournal = {Comput. Commun.},
  title        = {Vulnerabilities of the 6P protocol for the industrial internet of things: Impact analysis and mitigation},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balanced multi-access edge computing offloading strategy in
the internet of things scenario. <em>COMCOM</em>, <em>194</em>, 399–410.
(<a href="https://doi.org/10.1016/j.comcom.2022.07.048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things devices can offload part of the tasks to the edge servers through the wireless network, thus the computing pressure and energy consumption are reduced. However, this will increase the cost of communication. When designing the offloading strategy for the edge computing scenario of the Internet of things , it is necessary to maintain the balance between task execution energy and experiment. Therefore, this paper proposes an offloading strategy which can optimize the energy consumption and time delay of task execution at the same time. This strategy satisfies different preferences of users. First, the above task is modeled as a multi-objective optimization problem, and the Pareto solution set is found by improving the strength Pareto evolutionary algorithm (SPEA2). Based on the Pareto set , the offloading strategy satisfying the requires of users with different preferences by offloading cost estimation. Second, a simulation experiment is carried out to verify the robustness of the improved SPEA2 algorithm under the influence of different main parameters. Compared with other representative algorithms, the improved SPEA2 algorithm is proved to minimize the task execution delay and energy consumption jointly.},
  archive      = {J_COMCOM},
  author       = {Dan Ye and Xiaogang Wang and Jin Hou},
  doi          = {10.1016/j.comcom.2022.07.048},
  journal      = {Computer Communications},
  pages        = {399-410},
  shortjournal = {Comput. Commun.},
  title        = {Balanced multi-access edge computing offloading strategy in the internet of things scenario},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A quantitative framework for network resilience evaluation
using dynamic bayesian network. <em>COMCOM</em>, <em>194</em>, 387–398.
(<a href="https://doi.org/10.1016/j.comcom.2022.07.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring and evaluating network resilience has become an important aspect since the network is vulnerable to both uncertain disturbances and malicious attacks . Networked systems are often composed of many dynamic components and change over time, which makes it difficult for existing methods to access the changeable situation of network resilience. This paper establishes a novel quantitative framework for evaluating network’s multi-stage resilience using the Dynamic Bayesian Network . First, we define the dynamic capacities of network components and establish the network’s five core resilience capabilities to describe the resilient networking stages including preparation, resistance, adaptation, recovery, and evolution; the five core resilience capabilities consist of rapid response capability, sustained resistance capability, continuous running capability, rapid convergence capability, and dynamic evolution capability. Then, we employ a two-time slices approach based on the Dynamic Bayesian Network to quantify five crucial performances of network resilience based on proposed core capabilities. The proposed approach can ensure the time continuity of resilience evaluation in time-varying networks. Finally, our proposed evaluation framework is applied to different attack and recovery conditions in typical simulations and real-world network topology . Results and comparisons with existing studies indicate that the proposed method can achieve more accurate and comprehensive evaluation and can be applied to network scenarios under various intensities of attack and recovery.},
  archive      = {J_COMCOM},
  author       = {Shanqing Jiang and Lin Yang and Guang Cheng and Xianming Gao and Tao Feng and Yuyang Zhou},
  doi          = {10.1016/j.comcom.2022.07.042},
  journal      = {Computer Communications},
  pages        = {387-398},
  shortjournal = {Comput. Commun.},
  title        = {A quantitative framework for network resilience evaluation using dynamic bayesian network},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A CPS based social distancing measuring model using edge and
fog computing. <em>COMCOM</em>, <em>194</em>, 378–386. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–physical system (CPS) is one of the leading topics for research in academic and industry fields. CPS is an integrated system built with a collection of computation, communication, control, and physical elements to solve real-life problems. Lots of research is going on CPS, but in today’s point of view, the Covid-19 is one of the most relevant. Nowadays, Covid-19 has become a headache in our society. Social or physical distancing is one of the most useful non-pharmaceutical interventions (NPI) to minimize virus infections. The regular lifestyle of every human being has been changing rapidly. A contactless lifestyle is becoming a necessity day by day. Society is gradually dependent upon smart technological devices for a contactless lifestyle. In the new-normal lifestyle, many new technologies have been introduced. The government also makes some restrictions on human transmission. However, maintaining social distancing is one of the main challenges of our society. There is no such model that effectively helps people to maintain physical distancing. This paper highlights a framework that will guide maintaining physical distance in a social gathering. The proposed CPS-based model is entirely deployed on edge and fog computing architecture. The proposed model calculates the distance among all paired edge devices owned by human beings and informs the user whether the location is safe or not. This fog and edge-based model improves the latency and network usage compared to the cloud computing model .},
  archive      = {J_COMCOM},
  author       = {Manash Kumar Mondal and Riman Mandal and Sourav Banerjee and Utpal Biswas and Pushpita Chatterjee and Waleed Alnumay},
  doi          = {10.1016/j.comcom.2022.07.029},
  journal      = {Computer Communications},
  pages        = {378-386},
  shortjournal = {Comput. Commun.},
  title        = {A CPS based social distancing measuring model using edge and fog computing},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mobility-aware incentive mechanism for relaying D2D
communications. <em>COMCOM</em>, <em>194</em>, 361–377. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-to-Device (D2D) communications emerged as a promising technology to improve the efficiency of 5G cellular networks . However, users should be encouraged to participate in content sharing and relaying, which are necessary for D2D communications. Thus, an incentive mechanism is essential to encourage the content owners and relay nodes to participate in D2D communications. In this study, a contract-based incentive mechanism is proposed for relaying D2D communications. In contrast to previous work, this mechanism simultaneously motivates both content owners and relays to participate in D2D communications. Furthermore, user mobility is considered in the proposed approach. Assuming that the devices in the network are mobile, mobility awareness can be effective in the performance of the proposed incentive mechanism since we need more appropriate contracts that are less likely to be violated due to link failures which are the results of the mobility. Therefore, in the proposed mobility-aware incentive mechanism, the selection of the contract is performed according to the predicted location of devices in the next time step, as obtained from Markov method. The simulation results show that the proposed incentive mechanism increases the participation of devices in D2D content sharing compared to the baseline. Also, it is more likely that a content owner earns more utility due to the cooperation of a relay, which leads to an increase in the utility of the base station . Moreover, the increased data transmission rate which is obtained via encouraging relays to participate in D2D communications, reduces the latency and increases the residual energy of the devices. Also, using the proposed mobility-aware incentive mechanism, the utility of BS is improved compared to a similar scenario without mobility awareness.},
  archive      = {J_COMCOM},
  author       = {Faegheh Seifhashemi and Behrouz Shahgholi Ghahfarokhi and Neda Moghim},
  doi          = {10.1016/j.comcom.2022.07.051},
  journal      = {Computer Communications},
  pages        = {361-377},
  shortjournal = {Comput. Commun.},
  title        = {Mobility-aware incentive mechanism for relaying D2D communications},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flow-by-flow traffic matrix prediction methods: Achieving
accurate, adaptable, low cost results. <em>COMCOM</em>, <em>194</em>,
348–360. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic matrix (TM) prediction methods aim to accurately and efficiently predict future network traffic demands by using previous traffic matrices. These methods are critical for network operation and management. The recent research focuses on designing complex models to extract spatiotemporal correlations from TMs and network topologies using traffic volume datasets that consist of all the origin–destination (OD) flows sampled from the network. These entire-matrix models are confined to a specific network topology and are difficult to apply to other networks. To solve such problems, we investigate a flow-by-flow prediction method in this paper which utilizes the intra-flow temporal correlations. State-of-the-art results have been achieved by our method on Abilene and GÉANT datasets. Using only 10\% of the OD flows as the training samples, the flow-by-flow models can outperform the previously achieved state-of-the-art prediction performance. This small sample greatly reduces the end-to-end traffic monitoring costs. In addition, the flow-by-flow models can achieve outstanding cross-dataset evaluation performance. We suggest that leveraging a large external dataset with a flow-by-flow method can be a better choice when the measurement environment is difficult on the target network. The source code for our prediction method is publicly available at https://github.com/FreeeBird/Flow-By-Flow-Prediction .},
  archive      = {J_COMCOM},
  author       = {Weiping Zheng and Yiyong Li and Minli Hong and Xiaomao Fan and Gansen Zhao},
  doi          = {10.1016/j.comcom.2022.07.052},
  journal      = {Computer Communications},
  pages        = {348-360},
  shortjournal = {Comput. Commun.},
  title        = {Flow-by-flow traffic matrix prediction methods: Achieving accurate, adaptable, low cost results},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent reinforcement learning for long-term network
resource allocation through auction: A V2X application. <em>COMCOM</em>,
<em>194</em>, 333–347. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate offloading of computational tasks from a dynamic group of mobile agents (e.g., cars) as decentralized decision making among autonomous agents . We design an interaction mechanism that incentivizes such agents to align private and system goals by balancing between competition and cooperation. In the static case, the mechanism provably has Nash equilibria with optimal resource allocation. In a dynamic environment, this mechanism’s requirement of complete information is impossible to achieve. For such environments, we propose a novel multi-agent online learning algorithm that learns with partial, delayed and noisy state information, thus greatly reducing information need. Our algorithm is also capable of learning from long-term and sparse reward signals with varying delay. Empirical results from the simulation of a V2X application confirm that through learning, agents with the learning algorithm significantly improve both system and individual performance, reducing up to 30\% of offloading failure rate, communication overhead and load variation, increasing computation resource utilization and fairness. Results also confirm the algorithm’s good convergence and generalization property in different environments.},
  archive      = {J_COMCOM},
  author       = {Jing Tan and Ramin Khalili and Holger Karl and Artur Hecker},
  doi          = {10.1016/j.comcom.2022.07.047},
  journal      = {Computer Communications},
  pages        = {333-347},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent reinforcement learning for long-term network resource allocation through auction: A V2X application},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advanced computing and communication technologies for
internet of drones. <em>COMCOM</em>, <em>194</em>, 329–332. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Neeraj Kumar and Gagangeet Singh Aujla and Mohammad S. Obaidat and Rongxing Lu and Song Guo},
  doi          = {10.1016/j.comcom.2022.06.022},
  journal      = {Computer Communications},
  pages        = {329-332},
  shortjournal = {Comput. Commun.},
  title        = {Advanced computing and communication technologies for internet of drones},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed hierarchical deep optimization for federated
learning in mobile edge computing. <em>COMCOM</em>, <em>194</em>,
321–328. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently attracted great attention in many application fields, especially for big data analysis in the field of edge computing . Federated learning , as a promising machine learning technology , applies training data on distributed edge nodes to design shared learning systems to protect data privacy. Due to the system update in federated learning is at the expense of parameter exchange between edge nodes, it is extremely bandwidth consuming. A novel distributed hierarchical tensor depth optimization algorithm is proposed, which compresses the model parameters from the high-dimensional tensor space to a union of low-dimensional subspaces to reduce bandwidth consumption and storage demands of federated learning. In addition, an update method based on hierarchical tensor back propagation is developed by directly calculating the gradient of low-dimensional parameters to reduce the memory requirement and improve the training efficiency caused by edge node training. Finally, a large number of simulation experiments were performed to evaluate performance based on classical data sets with different local data distributions. Experimental results show that the proposed algorithm reduces the burden of communication bandwidth and the energy consumption of edge nodes.},
  archive      = {J_COMCOM},
  author       = {Xiao Zheng and Syed Bilal Hussain Shah and Ali Kashif Bashir and Raheel Nawaz and Umer Rana},
  doi          = {10.1016/j.comcom.2022.07.028},
  journal      = {Computer Communications},
  pages        = {321-328},
  shortjournal = {Comput. Commun.},
  title        = {Distributed hierarchical deep optimization for federated learning in mobile edge computing},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PbCP: A profit-based cache placement scheme for
next-generation IoT-based ICN networks. <em>COMCOM</em>, <em>194</em>,
311–320. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large number of connected devices generating a massive amount of data in the emerging technologies such as the Internet of Things (IoT), will surpass the capabilities of the current Internet infrastructure. Therefore, it is essential to design a new communication paradigm to meet the demands and requirements of new applications. Information-Centric Networking (ICN) is a new candidate for future Internet architecture that aims at addressing different challenges in the currently used host-centric Internet. In ICN, the data/content is searched, routed, located, and retrieved through the name of the content rather than the address of data source. ICN also enables in-network caching where the intermediate nodes can cache the content. In-network caching helps in improving the network scalability and reduces the content retrieval delay. To date, multiple caching schemes have been designed for IoT data management in ICN; however, most of them suffer from scalability or ignore constraints on the IoT devices. Moreover, the placement of content in a large-scale environment needs further improvement and optimization since it impacts overall network performance. To fill the gaps, in this paper, we combine various metrics that affect caching performance. We formulate the content placement as an optimization problem and then propose a profit-based caching scheme that harnesses the benefits of the Tabu local search algorithm . The simulation results and comparative analysis show that the proposed strategy outperforms the existing state-of-art schemes, notably in terms of memory resources utilization, data transmission time , content diversity ratio, and cache replacement operations.},
  archive      = {J_COMCOM},
  author       = {Oussama Serhane and Khadidja Yahyaoui and Boubakr Nour and Rasheed Hussain and Syed Muhammad Ahsan Kazmi and Hassine Moungla},
  doi          = {10.1016/j.comcom.2022.07.044},
  journal      = {Computer Communications},
  pages        = {311-320},
  shortjournal = {Comput. Commun.},
  title        = {PbCP: A profit-based cache placement scheme for next-generation IoT-based ICN networks},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chameleon: A self-adaptive cache strategy under the
ever-changing access frequency in edge network. <em>COMCOM</em>,
<em>194</em>, 301–310. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the maturity of 5G and Internet of Things technologies , the number of mobile applications and the amount of data access have increased explosively. However, the frequency of these accesses varies considerably at different times of the day, requiring different caching strategies in those limited-capacity edge servers. Existing caching strategies perform well when the access frequency is stable. However, they ignore the time-varying characteristics of user access frequency in different periods, resulting in a low hit rate in ever-changing frequency scenarios. To improve the hit rate in such scenarios, we propose a cache replacement policy called Chameleon , which consists of two components, AutoFre , and Crates . AutoFre is an admission algorithm that predicts the future access frequency category and calculates the admission thresholds based on the prediction result. While Crates is an eviction algorithm, it selects the contents evicted by designing a customized principal component analysis algorithm. We conduct a series of experiments with real application traces from ChuangCache. The trace has 9,839,213 user accesses in 48 h. The results demonstrate that Chameleon reaches about 98\% in caching hit rate and outperforms SecondHit-Crates algorithm about 8\% in frequency-changing edge networks.},
  archive      = {J_COMCOM},
  author       = {Pengmiao Li and Yuchao Zhang and Wendong Wang and Weiliang Meng and Yi Zheng and Ke Xu and Zhili Zhang},
  doi          = {10.1016/j.comcom.2022.07.036},
  journal      = {Computer Communications},
  pages        = {301-310},
  shortjournal = {Comput. Commun.},
  title        = {Chameleon: A self-adaptive cache strategy under the ever-changing access frequency in edge network},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving secure and low-cost medical data
communication scheme for smart healthcare. <em>COMCOM</em>,
<em>194</em>, 292–300. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless body area network (WBAN) remotely offers various medical services for the benefits of patients, doctors, and society in which medical data is transferred between different entities (i.e., patients, doctors, and hospitals) to analyze such data and provide medical treatments timely to the patients. Since such sensitive data is exchanged in a public network, it is highly required to protect this private and confidential medical data from attackers. Researchers designed various medical data transmission mechanisms but most of them are vulnerable to vital security and privacy threats. Besides, the existing relevant schemes require high computational resources to perform necessary operations on mobile devices (with fixed resources). We, thus, propose a new data communication scheme using low-cost cryptographic functions that can withstand various security and privacy attacks while consuming very less computational resources comparatively. We perform security analysis of the proposed scheme to confirm its robustness and discuss performance analysis based on the test bed setup to verify its computational efficiency.},
  archive      = {J_COMCOM},
  author       = {Mukesh Soni and Dileep Kumar Singh},
  doi          = {10.1016/j.comcom.2022.07.046},
  journal      = {Computer Communications},
  pages        = {292-300},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving secure and low-cost medical data communication scheme for smart healthcare},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards enhanced threat modelling and analysis using a
markov decision process. <em>COMCOM</em>, <em>194</em>, 282–291. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of socio-technical systems using Ambient Intelligence (AmI) and the Internet of Things (IoT) is growing exponentially, involving numerous entities, such as humans, infrastructures, and cyber systems. Achieving and maintaining a specified level of security and privacy in such systems is challenging and crucial. Attack Tree is a powerful technique used in safety and reliability engineering. In this paper, we attempted to enhance Attack Tree analysis by transforming it into a Markov Decision Process (MDP) model. We propose an algorithm to transform an Attack Tree into an MDP model. We argue that formal methods, such as probabilistic model checking can significantly improve the security analysis capabilities. Moreover, the mixture of MDP and probabilistic model checking can overcome the limitations of Attack Trees, such as state explosion, scalability, and manual interaction. We used a probabilistic model checker , namely PRISM to model an attack scenario and perform security analysis on it. To demonstrate the significance, we took a real-world use case and performed a probabilistic analysis on it. The results revealed that formal analysis can prove certain properties, which were not possible to verify using attack trees.},
  archive      = {J_COMCOM},
  author       = {Saif U.R. Malik and Adeel Anjum and Syed Atif Moqurrab and Gautam Srivastava},
  doi          = {10.1016/j.comcom.2022.07.038},
  journal      = {Computer Communications},
  pages        = {282-291},
  shortjournal = {Comput. Commun.},
  title        = {Towards enhanced threat modelling and analysis using a markov decision process},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Festschrift to honor the lifetime achievements of prof.
Marco ajmone marsan. <em>COMCOM</em>, <em>194</em>, 280–281. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Falko Dressler and Carla Fabiana Chiasserini},
  doi          = {10.1016/j.comcom.2022.07.043},
  journal      = {Computer Communications},
  pages        = {280-281},
  shortjournal = {Comput. Commun.},
  title        = {Festschrift to honor the lifetime achievements of prof. marco ajmone marsan},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous UAVs assisted mobile edge computing for energy
consumption minimization of the edge side. <em>COMCOM</em>,
<em>194</em>, 268–279. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on the Internet of Things (IoT) and edge computing , especially Unmanned Aerial Vehicles assisted Mobile Edge Computing (UAV-assisted MEC) attracts more and more interests of researchers. Nowadays, MEC systems with multiple UAVs have great research value, especially for emergency communication scenarios. In this paper, a heterogeneous UAVs assisted MEC system was proposed, and with the goal of minimization the edge side energy consumption, a parallel processing was involved to jointly optimize the communication scheduling, forwarding power, offloading data size, computing frequency and the trajectory of the UAV. The problem is formulated as a Mixed Integer Non-Linear Programming (MINLP), hard to solve. Therefore, we divided the MINLP into two sub-problems by applying the Block Coordinate Descent (BCD) method, and solved it using the Lagrangian Duality (LD) method and Successive Convex Optimization (SCO). The UAV’s trajectory was initialized as a circular, and then it was optimized by using the standardized convex solver design algorithm . In addition, a heuristic algorithm was put forward to obtain the optimization solution, while we set other benchmarks and the Monte Carlo (MC) algorithm to justify its validity and rationality. The simulation results showed that our strategy has better performance at minimizing energy consumption compared with other algorithms.},
  archive      = {J_COMCOM},
  author       = {Qiang Tang and Linjiang Li and Caiyan Jin and Lixin Liu and Jin Wang and Zhuofan Liao and Yuansheng Luo},
  doi          = {10.1016/j.comcom.2022.07.023},
  journal      = {Computer Communications},
  pages        = {268-279},
  shortjournal = {Comput. Commun.},
  title        = {Heterogeneous UAVs assisted mobile edge computing for energy consumption minimization of the edge side},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driver’s emotion and behavior classification system based on
internet of things and deep learning for advanced driver assistance
system (ADAS). <em>COMCOM</em>, <em>194</em>, 258–267. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of driver’s emotion is an important issue that can be used to increase awareness of driving habits of drivers as many drivers are overconfident and are unaware of their bad driving habits. If the drivers driving behaviors are identified automatically, the drivers can be aware of their bad habits which can assist them to avoid potential car accidents. Many researches have suggested many methods or techniques that can be helpful in determining driver’s behavior but there is no comprehensive method existed that can cater almost all types of distractions which occur while driving. In this paper the emotional as well as behavioral distraction states of the driver are analyzed and classified using advance deep learning algorithms like convolutional neural networks (CNN) and visual geometry group (VGG16). The handling of both the emotional and behavioral states can be helpful in developing a comprehensive driver’s detection system in an efficient manner addressing the previous limitations or challenges that are needed to be solved. The results show that the accuracy of CNN was far better than that of VGG16 but the training time of VGG16 was far less than that of CNN This system can be further integrated with the in-vehicle driving systems or built-in safety systems.},
  archive      = {J_COMCOM},
  author       = {Mariya Tauqeer and Saddaf Rubab and Muhammad Attique Khan and Rizwan Ali Naqvi and Kashif Javed and Abdullah Alqahtani and Shtwai Alsubai and Adel Binbusayyis},
  doi          = {10.1016/j.comcom.2022.07.031},
  journal      = {Computer Communications},
  pages        = {258-267},
  shortjournal = {Comput. Commun.},
  title        = {Driver’s emotion and behavior classification system based on internet of things and deep learning for advanced driver assistance system (ADAS)},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic ensemble algorithm for anomaly detection in IoT
imbalanced data streams. <em>COMCOM</em>, <em>194</em>, 250–257. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of ambient intelligence (AmI) in the Internet of Things (IoT), many data streams are generated from sensing devices in intelligent scenarios. Due to the deployment issues of IoT devices and the system’s complexity, abnormal behavior is inevitable, resulting in imbalanced data categories. Moreover, data streams generated in IoT systems are dynamic, continuous, and as the environment changes, further increasing the difficulty of anomaly detection . Therefore, we model the monitored historical and current data from the perspective of dynamic imbalanced data streams classification to discover abnormal behaviors in IoT systems. In this paper we propose a dynamic ensemble algorithm for anomaly detection in IoT environments. First the abnormal data samples are synthesized by the borderline-synthetic minority over-sampling technique (Borderline-SMOTE) to relieve the sample imbalance problem. Then considering the dynamics and continuity of data streams we adopt a chunk-based strategy to train a LightGBM classifier for each chunk of data to adapt to the current data distribution. To improve the ensemble model’s processing efficiency and anomaly detection accuracy we adopt a dynamic weighting strategy for base classifiers and remove the classifier whose accuracy performance is lower than the threshold. Finally we evaluate our proposed algorithm by conducting comparative experiments on real-world data streams. Experimental results show that our proposed algorithm outperforms the comparative anomaly detection methods in IoT scenarios.},
  archive      = {J_COMCOM},
  author       = {Jun Jiang and Fagui Liu and Yongheng Liu and Quan Tang and Bin Wang and Guoxiang Zhong and Weizheng Wang},
  doi          = {10.1016/j.comcom.2022.07.034},
  journal      = {Computer Communications},
  pages        = {250-257},
  shortjournal = {Comput. Commun.},
  title        = {A dynamic ensemble algorithm for anomaly detection in IoT imbalanced data streams},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized resource allocation and time partitioning for
integrated communication, sensing, and edge computing network.
<em>COMCOM</em>, <em>194</em>, 240–249. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diverse computational tasks generated from advanced applications are becoming more difficult to process by mobile user equipments due to their limited computing capability and battery supply. With the fast development of wireless technology and infrastructure, edge computing is becoming a paradigm to alleviate these problems by offloading the computation tasks to the edge nodes with more computation resources. In addition, the integrated sensing and communication is a promising technology, where the wireless communication and radar sensing share unified hardware platform and radio resources. In this paper, the capabilities of communication, radar sensing and edge computing are integrated together in the proposed base station architecture to support the comprehensive services of data transmission, target sensing, and edge computing. Based on the proposed scheme, a resource allocation and time partitioning problem is investigated to jointly optimize time partitioning , computation task processing mode selection, spectrum resource allocation and target sensing location selection to maximize the weighted sum of task processing and communication performance while guaranteeing the radar sensing performance. Since the problem is non-convex, we decouple the primal problem into three subproblems which are solved separately. Simulation results show that our proposed scheme outperforms the typical relevant schemes and can converge within an acceptable iterations.},
  archive      = {J_COMCOM},
  author       = {Kaijun Cheng and Xuming Fang and Xianbin Wang},
  doi          = {10.1016/j.comcom.2022.07.030},
  journal      = {Computer Communications},
  pages        = {240-249},
  shortjournal = {Comput. Commun.},
  title        = {Optimized resource allocation and time partitioning for integrated communication, sensing, and edge computing network},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Power and delay optimization based uplink resource
allocation for wireless networks with device-to-device communications.
<em>COMCOM</em>, <em>194</em>, 226–239. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the resource allocation problem for all devices is investigated in a multi-sharing uplink scenario of mmWave cyclic prefix-orthogonal frequency division multiplexing (CP-OFDM) based wireless networks with device-to-device (D2D) communications. More specifically, a power and delay optimization based uplink resource allocation (PDO-URA) algorithm is proposed divided into two steps. At first, network resources are allocated to cellular user equipments (CUEs) in terms of power and transmission rate through a proposed approach that intends to maximize throughput. Next, idle resources are allocated by considering delay minimization . To this end, we propose to apply an algorithm based on delay optimization where idle resources are shared with D2D user equipments (DUEs) in the network considering the formed conflicts graphs and the delay information estimated through Network Calculus concepts such as envelope process and service curve . Computational simulations are carried out considering a communication scenario with 5G characteristics such as millimeter waves at frequencies above 6 GHz, comparing also the performance of other algorithms from the literature in terms of quality of service (QoS) parameters such as throughput and delay. The results confirm that the proposed algorithm presents superior performance in terms of throughput and delay for the mmWave CP-OFDM wireless networks with D2D communications than the other considered algorithms.},
  archive      = {J_COMCOM},
  author       = {Marcus V.G. Ferreira and Flávio H.T. Vieira and Álisson A. Cardoso},
  doi          = {10.1016/j.comcom.2022.07.040},
  journal      = {Computer Communications},
  pages        = {226-239},
  shortjournal = {Comput. Commun.},
  title        = {Power and delay optimization based uplink resource allocation for wireless networks with device-to-device communications},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge computing vs centralized cloud: Impact of communication
latency on the energy consumption of LTE terminal nodes.
<em>COMCOM</em>, <em>194</em>, 213–225. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing brings several advantages, such as reduced latency, increased bandwidth, and improved locality of traffic. One aspect that is not sufficiently understood is to what extent the different communication latency experienced in the edge-cloud continuum impacts on the energy consumption of clients. We studied the energy consumption of a request–response communication scheme when an LTE node communicates with edge-based or cloud-based servers. Results show that the reduced latency of edge servers bring significant benefits in terms of energy consumption. Experiments also show how the energy savings brought by edge computing are influenced by the prevalent direction of data transfer (upload vs download), load of the server, and daytime/nighttime operation.},
  archive      = {J_COMCOM},
  author       = {Chiara Caiazza and Silvia Giordano and Valerio Luconi and Alessio Vecchio},
  doi          = {10.1016/j.comcom.2022.07.026},
  journal      = {Computer Communications},
  pages        = {213-225},
  shortjournal = {Comput. Commun.},
  title        = {Edge computing vs centralized cloud: Impact of communication latency on the energy consumption of LTE terminal nodes},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance analysis of heterogeneous cloud-radio access
networks: A user-centric approach with network scalability.
<em>COMCOM</em>, <em>194</em>, 202–212. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous cloud-radio access network (HC-RAN) is a very promising network architecture for future generation wireless communication systems. The centralized computation of HC-RAN provides flexibility to coordinate multi-point (CoMP) transmission of remote radio heads (RRHs). These RRHs can cooperatively form static or dynamic clusters which are network-centric or user-centric to serve the user equipment (UE). To meet the demands of future generation wireless networks, the user-centric approach is gaining much attention, especially in dynamic clustering. In dynamic user-centric CoMP, a set of overlapped clusters can cooperate dynamically to serve UEs. In a large network, the main challenge is the scalability of the computational complexity and fronthaul load with the increase of the number of UEs. To address this, we developed a scalable user-centric HC-RAN by utilizing dynamic cooperation clustering (DCC) framework. We presented an algorithm for joint user association, resource allocation, and cluster formation. We derived the expressions for different combining and precoding vectors to satisfy the scalability condition. We evaluated the performance of the scalable user-centric HC-RAN in terms of the achievable rate per UE at two different levels of cooperation with imperfect channel state information (CSI). The performance of the derived schemes is nearly optimal compared to the unscalable benchmark schemes and the ideal scalable system .},
  archive      = {J_COMCOM},
  author       = {Hareesh Ayanampudi and Ravindra Dhuli},
  doi          = {10.1016/j.comcom.2022.07.041},
  journal      = {Computer Communications},
  pages        = {202-212},
  shortjournal = {Comput. Commun.},
  title        = {Performance analysis of heterogeneous cloud-radio access networks: A user-centric approach with network scalability},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing AGV mobility control method for MANET coverage
optimization using procedural simulation. <em>COMCOM</em>, <em>194</em>,
189–201. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial applications continuous wireless connectivity of mobile clients can rarely by guaranteed. Lack of communication negatively impacts the performance of industrial automation systems, e.g. Automated Guided Vehicle (AGV) fleets. Utilizing industrial Mobile Ad-hoc NETworks (MANETs) and an adaptive positioning scheme can reduce the number of disconnections in AGV fleets. Therefore, the performance of the mobile system (e.g. AGV fleet) is improved and factory efficiency increased. In this work procedural simulation is used to examine wireless communication in industrial applications. This method enables the observation of the interaction of mobility control system, network status and robotic system performance independently from a specific environment or scenario. Novel insights on the effectiveness of ad-hoc communication in industrial applications and the correlation of AGV fleet connectedness and AGV fleet transport performance are presented. Additionally, a control method is proposed, which improves the network coverage of an industrial MANET and efficiency of AGV fleets.},
  archive      = {J_COMCOM},
  author       = {Christian Sauer and Eike Lyczkowski and Marco Schmidt and Andreas Nüchter and Tobias Hoßfeld},
  doi          = {10.1016/j.comcom.2022.07.033},
  journal      = {Computer Communications},
  pages        = {189-201},
  shortjournal = {Comput. Commun.},
  title        = {Testing AGV mobility control method for MANET coverage optimization using procedural simulation},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physical-layer security based mobile edge computing for
emerging cyber physical systems. <em>COMCOM</em>, <em>194</em>, 180–188.
(<a href="https://doi.org/10.1016/j.comcom.2022.07.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a secure mobile edge computing (MEC) for emerging cyber physical systems (CPS), where there exist K K eavesdroppers in the network, which can threaten the task offloading . These K K eavesdroppers can work either in a colluding mode where they cooperate to decode the secret message , or in a non-colluding mode where the eavesdroppers decode the message individually. For both eavesdropping nodes, we design the secure MEC system by devising a computation offloading ratio, transmit power and computational capability allocation to optimize the system performance mainly measured by the latency. In particular, a novel deep reinforcement learning (DRL) together with convex optimization (DRCO) is proposed, where the DRL is used to find a proper solution to the offloading ratio, while the convex optimization is implemented to solve the allocation of transmission power and computational capability. Simulation results show that the proposed DRCO method is superior to other conventional methods, and can provide a guaranteed secrecy and latency.},
  archive      = {J_COMCOM},
  author       = {Lunyuan Chen and Shunpu Tang and Venki Balasubramanian and Junjuan Xia and Fasheng Zhou and Lisheng Fan},
  doi          = {10.1016/j.comcom.2022.07.037},
  journal      = {Computer Communications},
  pages        = {180-188},
  shortjournal = {Comput. Commun.},
  title        = {Physical-layer security based mobile edge computing for emerging cyber physical systems},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trust management for service migration in multi-access edge
computing environments. <em>COMCOM</em>, <em>194</em>, 167–179. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access Edge Computing relies on the concept of moving part of the cloud resources closer to the users to address limitations of the traditional cloud in order to reduce communication latency and increase security. This makes Multi-access Edge Computing (MEC) suitable for time-critical applications such as autonomous vehicles where they can support connectivity with a safe and more efficient experience. Nevertheless, MECs are generally deployed on constrained devices with limited resources, which may affect the infrastructure reliability and thus its trustworthiness. In this paper, we focus on a trust mechanism based on the interactions between MECs to increase reliability in the context of a service migration scenario, where MEC nodes can decide based on a trust score to which node to migrate their services and user information. Our proposed algorithm leverages ideas of the EigenTrust and RLIoT reputation system and combined with a novel correlation concept and a dynamic distance algorithm. From our simulations, our model shows better results in different scenarios and communication ranges. This work provides the core component of a complete trust management system for MECs and could be applied in various use cases.},
  archive      = {J_COMCOM},
  author       = {Van Thanh Le and Nabil El Ioini and Hamid R. Barzegar and Claus Pahl},
  doi          = {10.1016/j.comcom.2022.07.039},
  journal      = {Computer Communications},
  pages        = {167-179},
  shortjournal = {Comput. Commun.},
  title        = {Trust management for service migration in multi-access edge computing environments},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GLMLP-TRANS: A transportation mode detection model using
lightweight sensors integrated in smartphones. <em>COMCOM</em>,
<em>194</em>, 156–166. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation mode detection (TMD), as an essential part of Intelligent Transportation Systems , aims at analyzing human current transportation activities, and can be widely adopted in road planning and traffic prediction . Although several relevant works have been conducted on TMD, there are still some great challenges due to complex factors including: (i) availability that several sensors are limited in certain scenarios; (ii) lightweight that both data collection sensors and the architecture of most current neural network are heavy and sophisticated; and (iii) expert knowledge that numerous researches for TMD are based on traditional machine learning . To address these challenges, we propose a deep learning-based transportation mode detection network using smartphone-based sensors called GLMLP-TRANS, which is inspired by Self-attention and MLP-Mixer. Our proposed GLMLP-TRANS network can capture both global and local temporal features and adaptively combine them to learn comprehensive information of each transportation mode. Residual technique is introduced to accelerate the progress of learning and enhance the accuracy of transportation mode detection. Extensive experimental results on SHL dataset demonstrate that our proposed GLMLP-TRANS network outperforms other six baselines and improve 12.8\% than the best baseline on SHL dataset.},
  archive      = {J_COMCOM},
  author       = {Xuyang Liu},
  doi          = {10.1016/j.comcom.2022.07.006},
  journal      = {Computer Communications},
  pages        = {156-166},
  shortjournal = {Comput. Commun.},
  title        = {GLMLP-TRANS: A transportation mode detection model using lightweight sensors integrated in smartphones},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tree sketch: An accurate and memory-efficient sketch for
network-wide measurement. <em>COMCOM</em>, <em>194</em>, 148–155. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic measurement provides essential information for bandwidth management and Quality of Service (QoS), which is an enabler of network status discovery. However, with the explosive growth of network traffic, traditional network measurement methods face challenges in terms of memory, computation, accuracy, especially in the high-speed networks. Network devices have no enough CPU or memory resources for collecting, counting and analyzing data streams, which makes measurement results inaccurate and unreliable. To address the problem, we design an accurate and memory-efficient Tree sketch. It consists of three different structures for heavy flow detection, Distributed Denial-of-Service (DDoS) attack detection, super-spreaders detection, respectively. The proposed heavy cuckoo algorithm stores the flows in categories according to their flow sizes reducing hash collisions in the sketch. Besides, Tree sketch employs the Single Instruction Multiple Data (SIMD) instructions to speed up packet processing . Experimental results show that F1 score of Tree sketch is 0.997 with 30 KB memory usage in heavy hitter detection, which is 0.019–0.137 higher than the state-of-the-art sketch algorithms. It is an efficient tool of traffic measurement in the backbone network and provides a good trade-off among accuracy, speed and memory usage.},
  archive      = {J_COMCOM},
  author       = {Lei Liu and Tong Ding and Hui Feng and Zhongmin Yan and Xudong Lu},
  doi          = {10.1016/j.comcom.2022.07.009},
  journal      = {Computer Communications},
  pages        = {148-155},
  shortjournal = {Comput. Commun.},
  title        = {Tree sketch: An accurate and memory-efficient sketch for network-wide measurement},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driving under influence: Robust controller migration for
MEC-enabled platooning. <em>COMCOM</em>, <em>194</em>, 135–147. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected cars are becoming more common. With the development of multi-access edge computing (MEC) for low-latency applications, it will be possible to manage the cooperative adaptive cruise control (CACC, also known as platooning) of such vehicles from the edge of cellular networks . In this paper, we present a controller that manages platooning from the network edge by adapting to varying network conditions. We incorporate a mechanism in the controller that allows vehicles to switch to automated cruise control when delays exceed safety thresholds, and back to platooning when the delays are sufficiently low to support it. We also formulate the problem of maintaining a low-latency connection in the presence of high mobility through migration, and propose a Q-Learning algorithm to solve this problem. We finally propose an Asynchronous Shared Learning scheme that enables multiple migration agents to cooperate, in order to expedite the convergence of migration policies. Compared to state-of-the-art migration techniques, our scheme exhibits better compliance of vehicle speed and spacing values to preset targets, and ameliorates statistical dispersion.},
  archive      = {J_COMCOM},
  author       = {Constantine Ayimba and Michele Segata and Paolo Casari and Vincenzo Mancuso},
  doi          = {10.1016/j.comcom.2022.07.014},
  journal      = {Computer Communications},
  pages        = {135-147},
  shortjournal = {Comput. Commun.},
  title        = {Driving under influence: Robust controller migration for MEC-enabled platooning},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed relay switching in the presence of dynamic
obstacles in millimeter wave D2D communication. <em>COMCOM</em>,
<em>194</em>, 124–134. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the presence of dynamic obstacles as well as motion of user equipments (UEs), blockage of line of sight (LOS) is highly probable which makes relay selection in millimeter wave device to device (D2D) communication quite challenging. We propose a geometric approach to find the priority of the relays for a given D2D pair based on the LOS blockage probabilities. In case of blockage of LOS, the respective D2D pair might switch to the other high priority relay. We develop a game-theoretic auction-based framework to perform centralized relay selection using the global information provided by the base station where the goal is to increase throughput and reduce blockage fraction for any D2D pair. We found that this global solution might not be sufficient to reduce packet loss adequately. Accordingly, we develop a distributed relay switching (DRS) algorithm, using the local information of the UEs to reduce any further packet loss due to the blockages. Through simulation we show that DRS not only improves throughput and packet loss but also lowers the blockage fraction of D2D pairs and thereby provides higher energy efficiency than the existing approaches which do not consider switching of relays in case of blockage.},
  archive      = {J_COMCOM},
  author       = {Ravi Shukla and Sasthi C. Ghosh},
  doi          = {10.1016/j.comcom.2022.07.024},
  journal      = {Computer Communications},
  pages        = {124-134},
  shortjournal = {Comput. Commun.},
  title        = {Distributed relay switching in the presence of dynamic obstacles in millimeter wave D2D communication},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on the role of UAVs in the communication process: A
technological perspective. <em>COMCOM</em>, <em>194</em>, 86–123. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the use of unmanned aerial vehicles (UAVs) has extended to include many aspects of life such as the commercial, military, search and rescue, monitoring and communications aspects, to name a few. It is unthinkable nowadays to imagine life without robust and far-reaching communication systems . This has become quite evident with the recent COVID-19 pandemic that has struck humanity with profound effects like no other. Many of the human activities had to be switched from being face-to-face activities to the online mode. Therefore, the use of UAVs to extend the reach of communication systems will prove invaluable to many citizens of the world. We see that UAVs can assume different functional roles in the communications process. In this study, we present a novel classification of UAV-related communications along two axes. First, this classification examines the role of the UAV in the communications process where it classifies the UAV as a provider of communications, a relay (service extender) of communications or a consumer of the communications. Second, it examines the communication technologies most used in conjunction with UAV utilization. We provide a critical discussion of the studies that deal with different applications as well as the issues encountered in using a certain technology with UAVs as they assume a specific role. Finally, we present the lessons learned as well as the future directions for using the UAVs in their different roles in connection to the different communication technologies.},
  archive      = {J_COMCOM},
  author       = {Ghada Alsuhli and Ahmed Fahim and Yasser Gadallah},
  doi          = {10.1016/j.comcom.2022.07.021},
  journal      = {Computer Communications},
  pages        = {86-123},
  shortjournal = {Comput. Commun.},
  title        = {A survey on the role of UAVs in the communication process: A technological perspective},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective data management strategy and RDD weight cache
replacement strategy in spark. <em>COMCOM</em>, <em>194</em>, 66–85. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the dramatic increase in internet users and their demand for real-time network performance, Spark has distributed computing environment has emerged. It is widely used due to its high-performance caching mechanism and high scalability. In the face of the unpredictability of data access patterns in the current big data environment , the data shuffling phase is prone to the problems of under-utilization of Spark cluster resources, high computational latency, and high task processing latency. Based on this, this paper proposes an intermediate data management strategy based on the data shuffling phase. Firstly, the size of the data generated in the data shuffling phase of the Spark platform is predicted by random sampling. The strength division strategy obtains the skewed data degree to obtain the part with excessive skew deviation. Finally, the adaptive data management strategy is applied to perform the corresponding computation tasks by the data deviation. In addition, to improve the response time, memory usage, and computation latency of Spark applications, an adaptive cache replacement algorithm based on RDD partition weights is proposed, which takes into account the influence of four weight factors such as computation cost, usage times, partition size and life cycle of RDDs by reasonably calculating the RDD partition weight values. Compared with the current mainstream baseline algorithms, the data management algorithm based on the data mash-up phase proposed in this paper can effectively reduce resource usage and computational response latency . The RDD-based partition weighted adaptive cache replacement algorithm proposed in this paper can fully use memory resources and effectively reduce the problem of resource wastage.},
  archive      = {J_COMCOM},
  author       = {Kun Jiang and Shaofeng Du and Fu Zhao and Yong Huang and Chunlin Li and Youlong Luo},
  doi          = {10.1016/j.comcom.2022.07.008},
  journal      = {Computer Communications},
  pages        = {66-85},
  shortjournal = {Comput. Commun.},
  title        = {Effective data management strategy and RDD weight cache replacement strategy in spark},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective network intrusion detection via representation
learning: A denoising AutoEncoder approach. <em>COMCOM</em>,
<em>194</em>, 55–65. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of deep learning techniques in intrusion detection problems has enabled an enhanced standard of detection effectiveness. However, most of the progress has occurred in supervised learning, which required a vast amount of labeled training samples. In the real world, there is a limited amount of labeled data available to train a deep neural network , affecting the classifier’s detection performance. Therefore, to address the lack of labeled network traffic required to train an effective supervised classifier, this study introduces a semi-supervised intrusion detection framework that combines the unsupervised and supervised techniques. The unsupervised pre-training approach is implemented based on a denoising autoencoder (DAE), to compress the intrusion dataset and obtain the lower-dimensional features representation. Then a portion of the compressed data is used to train the DNN classifier based on a multiclass supervised approach. The network architecture is optimized by tuning hyper-parameters using a trial-and-error approach. Comparative analysis is performed between the proposed approach and the most relevant deep learning methods available in the literature against the CICIDS2018 dataset, consisting of recent network attack traces. Our approach outperforms competitive methods while maintaining stable classification results above 99.6\% on F1-score, precision, and recall metrics. Additionally, it is trained in 64 min while achieving a low false alarm rate . Furthermore, the DAE module reduces the input network traffic data to one-tenth of the size of the input dataset.},
  archive      = {J_COMCOM},
  author       = {Ivandro O. Lopes and Deqing Zou and Ihsan H. Abdulqadder and Francis A. Ruambo and Bin Yuan and Hai Jin},
  doi          = {10.1016/j.comcom.2022.07.027},
  journal      = {Computer Communications},
  pages        = {55-65},
  shortjournal = {Comput. Commun.},
  title        = {Effective network intrusion detection via representation learning: A denoising AutoEncoder approach},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient sharing of privacy-preserving sensing data on
consortium blockchain via group key agreement. <em>COMCOM</em>,
<em>194</em>, 44–54. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the self-organizing and distributed nature of Wireless Sensor Networks (WSN), valuable data in sensor networks faces security and privacy risks. A blockchain-based approach enables secure and convenient sharing of sensor information among different users. Compared to public and private blockchains, consortium blockchain is widely used across different industries and use cases in WSN due to its auditability and high transaction rate. However, sensing data sharing via consortium blockchain raises the privacy issue. Therefore, the data from the sensor node is encrypted by the key (named s e n s o r k e y sensorkey ) shared by the sensor node and the sink node and then sent to the blockchain network to not reveal the privacy of the sensing data. Since the infrastructure in large-scale WSN is usually owned and managed by multiple organizations, encrypted sensing data needs to be authorized by these multiple organizations for computation. Organizations requesting privacy-preserving data are referred to as data sharers. Distributing the s e n s o r k e y sensorkey to each data sharer requires separate encryption of the key using the data sharer’s public key . The sink node needs to be online when each data sharer asks for the s e n s o r k e y sensorkey , and one encryption of the s e n s o r k e y sensorkey for each data sharer consumes precious resources. This work proposes GSChain for efficient privacy-preserving sensing data sharing on consortium blockchain. Multiple data sharers resort to asymmetric group key agreement protocol to maintain a shared group encryption key and their respective group decryption keys, enabling efficient s e n s o r k e y sensorkey retrieval from the consortium blockchain. The s e n s o r k e y sensorkey is encrypted only once by the group encryption key and stored on the consortium blockchain along with the privacy-preserving sensing data. Our scheme improves the efficiency of privacy-preserving data sharing among multiple data sharers while reducing the online demand for sink nodes. Although the data-sharing group should remain stable for a long time, we design the group key update scheme. We also discuss how old and new data sharers access different ranges of privacy-preserving sensing data as the data-sharing group changes. We build a complete implementation of GSChain based on the Hyperledger Fabric framework and conduct a comprehensive set of experimental studies. Our experimental results demonstrate that GSChain improves the privacy-preserving sensing data sharing efficiency with tolerable time and storage overhead . In addition, the time overhead caused by the recovery of the GSChain system is tolerable when the membership of the data-sharing group changes.},
  archive      = {J_COMCOM},
  author       = {Xiaoyan Hu and Xiaoyi Song and Guang Cheng and Hua Wu and Jian Gong},
  doi          = {10.1016/j.comcom.2022.07.035},
  journal      = {Computer Communications},
  pages        = {44-54},
  shortjournal = {Comput. Commun.},
  title        = {Efficient sharing of privacy-preserving sensing data on consortium blockchain via group key agreement},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating predictive model-based control to achieve
reliable consistent multipath mmWave communication. <em>COMCOM</em>,
<em>194</em>, 29–43. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radio is a key building block in 5G and beyond cellular networks . However, mmWave channels are very sensitive to environmental conditions and depend on Line-of-Sight connections to provide very high data rates. Achieving reliable, consistent communication – i.e., a steady link rate together with low delay – over mmWave links is therefore a challenging problem. The goal of this work is to explore the use of predictive control to manage and simultaneously use multiple available mmWave paths to achieve reliable consistent communication by means of a multipath proxy. We investigate transient solutions of Markov Modulated Fluid Queues (MMFQ) to model the short-term evolution of the proxy’s packet queue, consistent with the use of Markovian models to capture the behavior of mmWave channel blocking. We propose a combination of models that can be solved using newly proposed matrix-analytic techniques in a timely enough manner for use in real-time control. This gives us a prediction, over a short time horizon, of either proxy queue distributions or probabilities of reaching particular proxy buffer levels. Thus, it enables the proxy to make preemptive path decisions in order to maintain a desired Quality of Service. A proof-of-concept simulation study demonstrates the efficacy of our proposed MMFQ-based predictive approach over both static and purely reactive control approaches. Further, we explore the potential benefits of a hybrid approach to path management, combining both predictive and reactive control. This can allow the controller to cater for unforeseen events that cannot be forecast by the predictive controller, mitigating the resulting extra queuing and corresponding delay spikes.},
  archive      = {J_COMCOM},
  author       = {David A. Hayes and David Ros and Özgü Alay and Peyman Teymoori and Tine Margretha Vister},
  doi          = {10.1016/j.comcom.2022.07.011},
  journal      = {Computer Communications},
  pages        = {29-43},
  shortjournal = {Comput. Commun.},
  title        = {Investigating predictive model-based control to achieve reliable consistent multipath mmWave communication},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric differential routing for low orbit satellite
constellations. <em>COMCOM</em>, <em>194</em>, 15–28. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) constellations create a network that includes the satellites (as routing nodes) connected by Inter-Satellite Links (ISLs) while the satellite terminals are dynamically connected to one or more satellites. The combination of transient, high-rate changes with high latency presents a unique challenge for designing a routing protocol that can provide guaranteed bandwidth , and support the frequent changes without packet drops. Current works focus on end-to-end routing between multiple gateways and terminals and do not provide guaranteed service. This paper addresses the problem of routing traffic from a source terminal to a destination terminal on a LEO constellation using Asymmetric Differential Routing (ADR) to plan ‘semi-fixed’ routes. ADR keeps most of the planned route fixed and only minor (differential) adjustments are required to account for handovers . Using ADR, a terminal and a gateway (GW) communicate using a fixed route with a time complexity of O ( 1 ) O(1) to account for handovers .},
  archive      = {J_COMCOM},
  author       = {Oren Markovitz and Michael Segal},
  doi          = {10.1016/j.comcom.2022.07.022},
  journal      = {Computer Communications},
  pages        = {15-28},
  shortjournal = {Comput. Commun.},
  title        = {Asymmetric differential routing for low orbit satellite constellations},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sub-messages extraction for industrial control protocol
reverse engineering. <em>COMCOM</em>, <em>194</em>, 1–14. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things (IIoT) connects various industrial devices and processes for smart manufacturing purposes. The industrial devices and processes may employ standard or private communication protocols. Protocol Reverse Engineering (PRE) can infer the format of the unknown protocol by analyzing traffic traces. Existing work in the field mainly focuses on Internet protocol only, handling text messages. PRE for industrial control protocols is difficult and particularly designed for IIoT for real-time interconnection among industrial devices. Given the phenomenon that many consecutive sub-messages are often embedded in a lengthy message payload and have a similar format, a novel sub-messages extraction algorithm is proposed in this work by using template iteration as an intermediate step to form a full message format inference framework. An improved evaluation criterion is also proposed to evaluate the sub-messages extraction results. We carry out our algorithm on three standard industrial control protocols and two unknown protocols. Experiments show that adding our sub-messages extraction in PRE for IIoT can greatly improve the accuracy of the overall protocol format inference compared with the existing work.},
  archive      = {J_COMCOM},
  author       = {Yuhuan Liu and Fengyun Zhang and Yulong Ding and Jie Jiang and Shuang-Hua Yang},
  doi          = {10.1016/j.comcom.2022.07.025},
  journal      = {Computer Communications},
  pages        = {1-14},
  shortjournal = {Comput. Commun.},
  title        = {Sub-messages extraction for industrial control protocol reverse engineering},
  volume       = {194},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECiS: Encryption prior to compression for digital image
security with reduced memory. <em>COMCOM</em>, <em>193</em>, 410–417.
(<a href="https://doi.org/10.1016/j.comcom.2022.07.049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital images are a popular source of information and play an important role in several applications. However, the large amount of available data in the form of images causes serious security concerns. Although digital images are widely available, the storage of these images requires a large amount of data. Compression is an efficient way to reduce image size and efficiently utilize network resources. In this study, a joint encryption and compression technique , namely, ECiS, is developed to solve the two major issues with digital images outlined above. To achieve high security, we encrypt an image by using DNA, SHA-256, and a chaotic-based encryption technique . To reduce the bandwidth or storage space demand, we compress the encrypted image by using a compression technique based on zero memory set partitioned embedded block (ZM-SPECK). The strength of our ECiS technique is performing the analysis by applying several standard tests, including key analysis, statistical analysis, differential analysis, and time cost evaluation on a USC-SIPI dataset. Furthermore, the extensive evaluations on the dataset demonstrate that the proposed technique is secure and has a lower memory and encryption overhead than similar techniques.},
  archive      = {J_COMCOM},
  author       = {Kedar Nath Singh and Om Prakash Singh and Amit Kumar Singh},
  doi          = {10.1016/j.comcom.2022.07.049},
  journal      = {Computer Communications},
  pages        = {410-417},
  shortjournal = {Comput. Commun.},
  title        = {ECiS: Encryption prior to compression for digital image security with reduced memory},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous background processing for accelerated
simulation of wireless communication on multi-core systems.
<em>COMCOM</em>, <em>193</em>, 396–409. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete event simulation (DES) is an important tool for the development and analysis of wireless networks. However, with increasing network size and complexity, the computational effort and simulation time increases significantly, often exponentially. This increase in response time may be critical if DES is interfacing real-time systems like Hardware in the Loop (HIL) or network emulation. It also slows down development cycles of users designing or debugging simulation models. Most popular DES software packages run single-threaded. Thus, they achieve only limited performance improvements from more modern multi-core CPUs. At the same time, existing approaches for parallel simulation of networks do not perform well on wireless systems or require complex paradigm shifts in simulation models. In this paper, we propose Asynchronous Background Processing (ABP) to accelerate the simulation of wireless communication on multi-core systems. By moving expensive computation from the main thread into asynchronous tasks computed by background threads, it accelerates the progression of events and thus reduces response time. Tasks are started as early as possible to exploit the time the main thread spends processing other events, ideally providing results before they are needed in the simulation. We showcase the application of ABP using Veins, a popular vehicular network simulator , demonstrating speedups of up to 3.5 on typical desktop platforms . We further perform an in-depth analysis using advanced profiling techniques to investigate the effectiveness of the parallelization and guide further optimizations.},
  archive      = {J_COMCOM},
  author       = {Dominik S. Buse and Georg Echterling and Falko Dressler},
  doi          = {10.1016/j.comcom.2022.07.032},
  journal      = {Computer Communications},
  pages        = {396-409},
  shortjournal = {Comput. Commun.},
  title        = {Asynchronous background processing for accelerated simulation of wireless communication on multi-core systems},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure aggregate signature scheme for smart city
applications. <em>COMCOM</em>, <em>193</em>, 388–395. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent advances in technology, smart cities are rapidly earning momentum. To provide better lives, smart cities should host several applications to increase the efficiency and accessibility of services. Since these services involve the conveyance of significant data among the smart cities’ cloud and the dwellers over the Internet, security and privacy are critical. This paper proposes an authentication protocol with a full aggregation signature to secure the smart city applications . A certificate-less aggregate signature (CLAS) is used for smart city real applications such as secure routing and database outsourcing. The proposed CLAS achieves message authentication , user anonymity, constant signature size, unlinkability, and is resistant to replay attacks. Also, we have proved that the proposed scheme is resistant to all malicious adversaries . Finally, the performance evaluation results show that the proposed CLAS performs better than existing relevant schemes.},
  archive      = {J_COMCOM},
  author       = {Nabeil Eltayieb and Rashad Elhabob and Muhammad Umar Aftab and Ramil Kuleev and Manuel Mazzara and Muhammad Ahmad},
  doi          = {10.1016/j.comcom.2022.07.050},
  journal      = {Computer Communications},
  pages        = {388-395},
  shortjournal = {Comput. Commun.},
  title        = {Secure aggregate signature scheme for smart city applications},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hadamard walk model and its application in identification
of important edges in complex networks. <em>COMCOM</em>, <em>193</em>,
378–387. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Hadamard coin driven quantum walk (i.e., Hadamard walk) model is proposed to identify the important edges of undirected complex networks. In this proposed model, the importance of an edge is scored through the observed probabilities on a pair of nodes with a common edge, based on which the rankings of all important edges are obtained according to the score of each edge. By considering the robustness index of the complex network as estimation standard, experimental results indicate that the capability of the proposed Hadamard walk model to identify important edges is 4.59\% ∼ ∼ 20.03\% higher than existing algorithms in static complex networks. Moreover, to further establish its utility, the proposed model was deployed in a dynamic complex network involving a typical communication scenario found in unmanned aerial vehicle (UAV) swarms. Specifically, we implemented the proposed model in simulations to select significant UAV nodes in a dynamic network and outcomes indicate that our model outperforms various algorithms in the verification of epidemic dynamics model.},
  archive      = {J_COMCOM},
  author       = {Wen Liang and Fei Yan and Abdullah M. Iliyasu and Ahmed S. Salama and Kaoru Hirota},
  doi          = {10.1016/j.comcom.2022.07.045},
  journal      = {Computer Communications},
  pages        = {378-387},
  shortjournal = {Comput. Commun.},
  title        = {A hadamard walk model and its application in identification of important edges in complex networks},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secrecy performance analysis of amplify-and-forward relay
cooperative networks with simultaneous wireless information and power
transfer. <em>COMCOM</em>, <em>193</em>, 365–377. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous wireless information and power transfer (SWIPT) is a promising technology to enhance the power constraint issue in the wireless networks. In this paper, we investigate the secrecy performance of a selective cooperative relaying network with an energy harvesting (EH) technique, in which a source terminal communicates with a legitimate user at the destination terminal through N amplify-and-forward (AF) relays in the presence of multiple eavesdroppers. The relays deploy SWIPT with power splitting-based relaying (PSR) and time switching-based relaying (TSR) protocols to harvest energy. We present closed-form expressions for the secrecy performance metrics, consisting of the probability of non-zero secrecy capacity , the secrecy outage probability , and the average secrecy capacity to evaluate the physical layer security (PLS) of the system. Using the mathematical expressions, we obtain numerical results under the parameters of the system. These results demonstrate that using relay selection can significantly improve the secrecy performance of the system. Moreover, we compare the cooperative relay system without SWIPT and the system with SWIPT. It is observed that depending on the distance of eavesdroppers from the relays and the destination, the system based on SWIPT may achieve a better PLS than the system without SWIPT. Finally, we provide Monte-Carlo simulation results to confirm the validity of our analysis.},
  archive      = {J_COMCOM},
  author       = {Sahar Parkouk and Mohammad Torabi and Saeed Shokrollahi},
  doi          = {10.1016/j.comcom.2022.07.020},
  journal      = {Computer Communications},
  pages        = {365-377},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy performance analysis of amplify-and-forward relay cooperative networks with simultaneous wireless information and power transfer},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EC-MASS: Towards an efficient edge computing-based
multi-video scheduling system. <em>COMCOM</em>, <em>193</em>, 355–364.
(<a href="https://doi.org/10.1016/j.comcom.2022.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video cameras have been deployed widely today. Although existing systems aim to optimize live video analytics from a variety of perspectives, they are agnostic to the workload dynamics in real-world. We propose EC-MASS, an edge computing-based video scheduling system achieving both cost and performance optimization with multiple cameras and edge data centers . The intuition behind EC-MASS is to adaptively map cameras to different edge data centers according to dynamically updated configurations of cameras. We prove that generating the optimal mapping scheduling scheme is NP-Complete, and develop the scheduling algorithm by leveraging the insights of the economy consideration of camera allocation. Using the algorithm, EC-MASS is able to balance the workload among edge data centers while reducing the cost of video analytics system. We evaluate EC-MASS with datasets of video configurations from real-world cameras which randomly generate configurations for cameras, with a testbed that consists of 60 cameras and 4 edge data centers. Our results show that EC-MASS consistently outperforms the status quo in terms of cost and performance stability.},
  archive      = {J_COMCOM},
  author       = {Shu Yang and Qingzhen Dong and Laizhong Cui and Xun Chen and Siyu Lei and Yulei Wu and Chengwen Luo},
  doi          = {10.1016/j.comcom.2022.07.002},
  journal      = {Computer Communications},
  pages        = {355-364},
  shortjournal = {Comput. Commun.},
  title        = {EC-MASS: Towards an efficient edge computing-based multi-video scheduling system},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An early stage convolutional feature extracting method using
for mining traffic detection. <em>COMCOM</em>, <em>193</em>, 346–354.
(<a href="https://doi.org/10.1016/j.comcom.2022.06.044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency is becoming more and more popular due to its superiority to traditional currencies, resulting in the boom of mining. Mining cryptocurrencies requires tremendous computing resources, and extensive high-performance computers are used for mining nowadays. A significant consequent problem is the huge amount of energy consuming for mining. Thus, managing mining behaviors become an urgent issue. There are two main ways to detecting mining behavior. One is to deploy a detecting program on the target host, and use features of system calls to detect the mining behaviors. The other is to deploy detection models on network, and identify mining behaviors via network traffic. Comparing with the former method, detecting mining behavior by traffic is “non-contact”, and can monitor a whole network instead of a single host. We propose in this paper a convolutional function based method to extract the features from the first few packets of flows to identify mining traffic. We first extract the size of each packet of a flow, and then design a convolution function with a sliding window to extract meaningful features from the packet size sequence. This method maps the flows to a feature space in which the mining flows can be distinguished from the normal flows easily. We collect a set of mining traffic traces including 8 types of cryptocurrency mining behaviors in a real network, and launch a set of empirical studies using this data set. We also develop an online mining traffic identification platform to validate the performance of our proposal. Both the offline experimental results and the online validation results suggests that our proposal can achieve high performance satisfying the real mining traffic detecting requirements.},
  archive      = {J_COMCOM},
  author       = {Peifa Sun and Mengda Lyu and Hui Li and Bo Yang and Lizhi Peng},
  doi          = {10.1016/j.comcom.2022.06.044},
  journal      = {Computer Communications},
  pages        = {346-354},
  shortjournal = {Comput. Commun.},
  title        = {An early stage convolutional feature extracting method using for mining traffic detection},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A website fingerprint defense technology with low delay and
controllable bandwidth. <em>COMCOM</em>, <em>193</em>, 332–345. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Website Fingerprinting (WF) attacks have shown a serious threat to users’ privacy on Tor networks. To resist WF attacks, many defenses have been proposed, of which padding without any delay is the state-of-the-art method. With the extra data overhead, they managed to reduce the accuracy of WF attacks. However, it is challenging to defeat WF attacks effectively while costing low bandwidth and latency. In this paper, we present RanDePad, a traffic protection and obfuscation method. It achieves low delay and controllable bandwidth. RanDePad uses an adaptive random delaying technique to destroy the website traffic’s time distribution characteristics. It could adjust the time interval between the real traffic packets without changing its order and limits the required traffic latency. Through a bandwidth evaluation algorithm, RanDePad evaluates the traffic bandwidth and dynamically adjusts the random bandwidth padding scheme to hide traffic space features. It ensures low and controllable bandwidth overhead. We evaluated the performance of RanDePad against WF attacks under different bandwidth overhead settings. Experimental results show that with the same total bandwidth overhead and 5.89\% total latency overhead, RanDePad can decrease the state-of-the-art attack DF accuracy from 94.57\% to 62.40\%, while the Front reduces it to 71.01\% and the WTF-PAD to 86.44\%. Furthermore, when the accuracy is defended to 70\%, our method consumes only 21.66\% total bandwidth overhead, which is half of the Front and WTF-PAD. In addition, the single bandwidth overhead of RanDePad is also lower than the existing defenses, where the Font spends three times as much as RanDePad does. The results indicate that the RanDePad performs better than the current state-of-the-art WTF-PAD and FRONT methods.},
  archive      = {J_COMCOM},
  author       = {Xueshu Hong and Xingkong Ma and Shaoyong Li and Houjie Qiu and Bo Liu},
  doi          = {10.1016/j.comcom.2022.06.028},
  journal      = {Computer Communications},
  pages        = {332-345},
  shortjournal = {Comput. Commun.},
  title        = {A website fingerprint defense technology with low delay and controllable bandwidth},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent and secure framework for critical infrastructure
(CPS): Current trends, challenges, and future scope. <em>COMCOM</em>,
<em>193</em>, 302–331. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–Physical Systems (CPS) are developed by the integration of computational algorithms and physical components and they exist as a result of technological advancement in embedded systems , distributed systems, and sophisticated networking technologies. Typically, these systems are intended to monitor and manipulate real-environment processes and objects. There had been advancements in CPS that affect various aspects of human lives and enable a wider range of applications and services. However, the interconnectivity between the cyber and physical world in CPS introduces new threats and security challenges and also increases the attack surface. The CPS is more prone to cyber-attacks than physical attacks because of the high accessibility of cyber components than physical ones. If we consider the last decade, it is no longer a myth to see industries relying on CPS being a victim of security attacks. In this paper, considering the applicability and dangers of CPS, we thus presented the key security aspects of CPS by reviewing published literature since 2018. We have examined the CPS requirements, challenges, security methods, security standards, threats, vulnerabilities, and past attacks. Moreover, we have considered the role of machine learning and deep learning in the enhancement of the security of CPS and reviewed the performance of existing works, and analyzed the challenges faced, and possible improvement techniques to enhance performance. We identified some key issues and challenges (such as CPS constraints, the performance of learning models, and security of learning models) in the adaption of learning-based methods for CPS security and accordingly proposed a security framework for CPS. Finally, several suggestions and recommendations are proposed considering the lessons learned from the comprehensive review.},
  archive      = {J_COMCOM},
  author       = {Zakir Ahmad Sheikh and Yashwant Singh and Pradeep Kumar Singh and Kayhan Zrar Ghafoor},
  doi          = {10.1016/j.comcom.2022.07.007},
  journal      = {Computer Communications},
  pages        = {302-331},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent and secure framework for critical infrastructure (CPS): Current trends, challenges, and future scope},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TTSL: An indoor localization method based on temporal
convolutional network using time-series RSSI. <em>COMCOM</em>,
<em>193</em>, 293–301. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing indoor location methods are mainly oriented towards the study of single Received Signal Strength Indication ( RSSI ), which does not make full use of the time information attached to RSSI, so the location accuracy is limited. In this paper, considering the correlation of RSSI in time and space, Temporal Convolutional Network (TCN) based Time Series Localization (TTSL) method is proposed by using the temporal and spatial characteristics of signals in continuous locations. The neural network model is used to extract the time fluctuation characteristics of signals in continuous locations, and the nonlinear mapping relationship between signal characteristics and time and space to location coordinates is learned. The correlation of RSSI time and location information in the trajectory is realized, and the discrete location task is transformed into a continuous time series feature discovery task. A large number of experiments were carried out in the space of approximately 1000 square meters, and a comprehensive comparison was made with the existing methods. The average location error of TTSL was 3.73 m, and the performance is found to more stable than the existing methods. The TTSL method has a relatively small dependence on data volume, eliminates spatial ambiguity, and significantly reduces the influence of noise on location results.},
  archive      = {J_COMCOM},
  author       = {Bing Jia and Jingbin Liu and Tao Feng and Baoqi Huang and Thar Baker and Hissam Tawfik},
  doi          = {10.1016/j.comcom.2022.07.003},
  journal      = {Computer Communications},
  pages        = {293-301},
  shortjournal = {Comput. Commun.},
  title        = {TTSL: An indoor localization method based on temporal convolutional network using time-series RSSI},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of a multi-armed bandit solution to improve the
spatial reuse of next-generation WLANs. <em>COMCOM</em>, <em>193</em>,
279–292. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next generation of WLANs will be, for the most part, ubiquitous in urban areas, densely deployed, and implementing the latest amendment of IEEE 802.11 standard, namely 802.11ax also known as Wi-Fi 6. Among the main purposes of 802.11ax is the improvement of the spatial reuse of radio channels by allowing the dynamical update of the sensitivity threshold and the transmission power at each node. In this regard, our contributions are twofold. First, we investigate the performance improvement resulting from a more efficient spatial reuse of radio channels with 802.11ax. Second, we introduce a centralized solution based on the Multi-Armed Bandit (MAB) framework and a sub-sampling technique to quickly discover an appropriate configuration of the sensitivity threshold and transmission power at each access point. We evaluate our solution with the network simulator ns-3 on different network topologies . The simulation results show the ability of our solution to quickly and robustly adjust these parameters of access points in order to significantly improve the behavior of WLANs .},
  archive      = {J_COMCOM},
  author       = {Anthony Bardou and Thomas Begin and Anthony Busson},
  doi          = {10.1016/j.comcom.2022.07.015},
  journal      = {Computer Communications},
  pages        = {279-292},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of a multi-armed bandit solution to improve the spatial reuse of next-generation WLANs},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of time-weighted LoRa-based positioning using
machine learning. <em>COMCOM</em>, <em>193</em>, 266–278. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localization systems have gained attention owing to the paradigm shift from human-centric communication systems (1G to 4G) to the machine-to-machine architectures (5G and beyond). The commercial localization applications standardized for 5G systems have served as a precursor to the cardinality of positioning technologies in the next-generation communication systems . The stringent requirements of these use-cases have motivated researchers to propose novel architectures and techniques to develop scalable, accurate, reliable, robust, and low-power positioning and tracking systems. Low-power wide-area network (LPWAN) technologies have found their niche in the Internet-of-thing (IoT)-focused industrial and research communities, since they promise wide area coverage to many battery-operated devices. LoRaWAN, with regulatory features and high network density, has emerged as the widely adopted long-range, low-power solution for scheduled IoT applications. This paper explores the feasibility of LoRa technology for satellite navigation-independent positioning, using received signal strength indicator (RSSI) fingerprinting. We explore traditional path-loss models, machine learning and deep learning techniques to develop an accurate RSSI-to-distance mapping. We further use the analytically optimal model as the underlying ranging function for trilateration-based deterministic positioning. The results indicate that LoRa technology is a feasible alternate for fingerprinting-based positioning in line-of-sight and non-line-of-sight scenarios, with accuracies ranging from 6 to 15 m.},
  archive      = {J_COMCOM},
  author       = {Mahnoor Anjum and Muhammad Abdullah Khan and Syed Ali Hassan and Haejoon Jung and Kapal Dev},
  doi          = {10.1016/j.comcom.2022.07.010},
  journal      = {Computer Communications},
  pages        = {266-278},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of time-weighted LoRa-based positioning using machine learning},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ARCMT: Anchor node-based range free cooperative multi
trusted secure underwater localization using fuzzifier. <em>COMCOM</em>,
<em>193</em>, 246–265. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure and precise node localization is a critical challenge for the underwater acoustic sensor network. The sensor node distribution in the aquatic environment is the most vulnerable to many external attacks, interrupting the nodes to find the precise location information and causing complete network failure due to excessive, unnecessary communication and power consumption . This paper proposes an anchor nodes-based range-free cooperative multi-trust management system using fuzzy logic to provide a secure and reliable underwater localization . The multi-trust-based scheme protects the sensor nodes from untrusted or malicious nodes, and the lightweight Mamdani fuzzy model is used to reduce circuit complexity and memory consumption. The proposed scheme does not require the cluster head to perform trust-related tasks, which reduces the heavy overburden on the cluster heads with their regular task and energy budget to enhance their lifetime. Therefore, the proposed method uses Anchor node-based joint trust calculation and localization of unlocalized cluster members, providing a better reliable trust evaluation and better accuracy and stability in terms of the localization error . The simulation report shows that the proposed scheme improves the average accuracy by 10.77\%, average stability by 23.7\%, average malicious node detection rate by 42.4\%, and average false detection rate by 38.25\% compared to other single-cluster head-based and traditional energy trust-based systems.},
  archive      = {J_COMCOM},
  author       = {Souvik Saha and Rajeev Arya},
  doi          = {10.1016/j.comcom.2022.07.016},
  journal      = {Computer Communications},
  pages        = {246-265},
  shortjournal = {Comput. Commun.},
  title        = {ARCMT: Anchor node-based range free cooperative multi trusted secure underwater localization using fuzzifier},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preventing failures of cooperative maneuvers among connected
and automated vehicles. <em>COMCOM</em>, <em>193</em>, 234–245. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vehicles will be able to drive autonomously in various environments. An essential part of that is to predict other vehicles’ intents and to coordinate maneuvers jointly. Such cooperative maneuvers have the ability to make driving safer and traffic more efficient. However, among the various communication protocols proposed for maneuver coordination, no single one satisfies all requirements. This paper assesses failure risks and mitigation strategies for cooperative maneuvers, including an analysis of popular protocols regarding this aspect. We evaluate two cooperation protocols representative for different approaches to cooperation, the complex vehicular interactions protocol as well as trajectory sharing, concerning the performance of mitigation mechanisms and their influence on maneuver success rates or times to reach consensus among maneuver participants. Via simulation, we show that both are suitable for cooperative maneuvers in realistic scenarios and investigate the trade-offs individual mitigation mechanisms face. These results are well-suited as guidelines and benchmark for other researchers developing cooperative maneuver protocols.},
  archive      = {J_COMCOM},
  author       = {Bernhard Häfner and Josef Jiru and Henning Schepker and Georg Albrecht Schmitt and Jörg Ott},
  doi          = {10.1016/j.comcom.2022.07.013},
  journal      = {Computer Communications},
  pages        = {234-245},
  shortjournal = {Comput. Commun.},
  title        = {Preventing failures of cooperative maneuvers among connected and automated vehicles},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geolocation-based sector selection for
vehicle-to-infrastructure 802.11ad communication. <em>COMCOM</em>,
<em>193</em>, 224–233. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve range, IEEE 802.11ad uses directional communication, the first step of which is to choose the best antenna configuration , known as sector. We studied the sector selection behavior of Commercial Off-The-Shelf (COTS) 802.11ad equipment in an experimental Vehicle-to-Infrastructure (V2I) communication scenario. First, we created a framework for mm-Wave data collection in mobile scenarios, highlighting the challenges, justifying our solutions, and making them available, thus flattening the path for future experimentation by others. We then proceeded to use said framework to collect data in a realistic V2I communication scenario. We performed two independent measurement campaigns in the same area. Analysis of the collected data revealed the following inefficiencies in the devices’ sector selection algorithm : (i) a large number of sector selection attempts that do not result in a sector change; and (ii) a “ping-pong” effect in which a node oscillates between two sectors. With this in mind we propose an alternative antenna sector selection scheme that uses spatially-indexed historical performance data to pick the statistically-best sector for any given geolocation . A trace-based analysis showed that a position-based strategy can improve throughput by more than 10\% for 30\% of locations, and that gains can be as high as 60\%, in some instances.},
  archive      = {J_COMCOM},
  author       = {Mateus Mattos and António Rodrigues and Rui Meireles and Ana Aguiar},
  doi          = {10.1016/j.comcom.2022.07.005},
  journal      = {Computer Communications},
  pages        = {224-233},
  shortjournal = {Comput. Commun.},
  title        = {Geolocation-based sector selection for vehicle-to-infrastructure 802.11ad communication},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-based caching in mobile information-centric
networks. <em>COMCOM</em>, <em>193</em>, 214–223. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless networking is expected to sustain the direct interaction between personal users’ devices, and to provide connectivity on large-scale resource-constrained devices. However, conventional networking protocols fail in large scale mobile wireless environments, due to node mobility, dynamic topologies , and intermittent connectivity. Information-Centric Networking (ICN) has been considered the most promising candidate to overcome the drawbacks of host-centric architectures where Named Data Networking (NDN) is one of the well-known and studied architectures within the ICN paradigm. The main objective of this work is to improve both content availability and network performance in mobile environments regarding the ICN paradigm. This is provided through a context-based approach for the caching admission policy providing in-network caching and content replication, facilitating the efficient and timely delivery of information. Content popularity, freshness, proximity, source mobility type and network density are some of the metrics considered in the caching decision. We conducted a comparative study between our proposal and the NDN caching strategy by using two different datasets with real mobility and connectivity traces, addressing intermittent communication. According to our results, we observed that using a multi-criteria context-based cache admission policy improves cache hits, cache evictions, and request satisfaction ratios in mobile environments, thus improving content delivery and network efficiency.},
  archive      = {J_COMCOM},
  author       = {Luís Leira and Miguel Luís and Susana Sargento},
  doi          = {10.1016/j.comcom.2022.07.017},
  journal      = {Computer Communications},
  pages        = {214-223},
  shortjournal = {Comput. Commun.},
  title        = {Context-based caching in mobile information-centric networks},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A path characteristics based adaptive subflow switching
algorithm for dynamic heterogeneous MPTCP network. <em>COMCOM</em>,
<em>193</em>, 204–213. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipath Transmission Control Protocol (MPTCP) can support simultaneous transmission of data packets over multiple paths, improve transmission reliability and aggregate network bandwidth to improve throughput. However, MPTCP cannot adjust the access state of subflows in real time according to the actual situation in the dynamic network, resulting in poor performance under the limitation of limited buffers. The existing path management algorithms, such as Fullmesh and PCDC, only select the subflows for concurrent transmission at the beginning, and cannot dynamically manage the subflow during the transmission. Therefore, we propose a path characteristics based adaptive subflow switching management (PASS) algorithm to manage multiple paths of MPTCP in real time and dynamically manage whether a subflow joins the transmission or not. Experimental results show that PASS is more stable and flexible than traditional Fullmesh algorithm and PCDC algorithm, and can obtain higher network throughput.},
  archive      = {J_COMCOM},
  author       = {Yantao Cai and Huayi Xiong and Min Chen and Xing Zhou},
  doi          = {10.1016/j.comcom.2022.07.001},
  journal      = {Computer Communications},
  pages        = {204-213},
  shortjournal = {Comput. Commun.},
  title        = {A path characteristics based adaptive subflow switching algorithm for dynamic heterogeneous MPTCP network},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SLICES, a scientific instrument for the networking
community. <em>COMCOM</em>, <em>193</em>, 189–203. (<a
href="https://doi.org/10.1016/j.comcom.2022.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A science is defined by a set of encyclopedic knowledge related to facts or phenomena following rules or evidenced by experimentally-driven observations. Computer Science and in particular computer networks is a relatively new scientific domain maturing over years and adopting the best practices inherited from more fundamental disciplines. The design of past, present and future networking components and architectures have been assisted, among other methods, by experimentally-driven research and in particular by the deployment of test platforms, usually named as testbeds . However, often experimentally-driven networking research used scattered methodologies, based on ad-hoc, small-sized testbeds , producing hardly repeatable results. We believe that computer networks needs to adopt a more structured methodology, supported by appropriate instruments, to produce credible experimental results supporting radical and incremental innovations. This paper reports lessons learned from the design and operation of test platforms for the scientific community dealing with digital infrastructures. We introduce the SLICES initiative as the outcome of several years of evolution of the concept of a networking test platform transformed into a scientific instrument. We address the challenges, requirements and opportunities that our community is facing to manage the full research-life cycle necessary to support a scientific methodology.},
  archive      = {J_COMCOM},
  author       = {Serge Fdida and Nikos Makris and Thanasis Korakis and Raffaele Bruno and Andrea Passarella and Panayiotis Andreou and Bartosz Belter and Cédric Crettaz and Walid Dabbous and Yuri Demchenko and Raymond Knopp},
  doi          = {10.1016/j.comcom.2022.07.019},
  journal      = {Computer Communications},
  pages        = {189-203},
  shortjournal = {Comput. Commun.},
  title        = {SLICES, a scientific instrument for the networking community},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using self-deferral to achieve fairness between wi-fi and
NR-u in downlink and uplink scenarios. <em>COMCOM</em>, <em>193</em>,
176–188. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless networks operating in unlicensed bands generally use one of two channel access paradigms: random access (e.g., Wi-Fi) or scheduled access (e.g., LTE License Assisted Access, LTE LAA and New Radio-Unlicensed, NR-U). The coexistence between these two paradigms is based on listen before talk (LBT), which was, however, designed for random access. Meanwhile, scheduled systems require that their transmissions start at the beginning of a slot boundary. Synchronizing this boundary to the end of LBT usually requires transmitting a reservation signal (RS) to block the channel. Since the RS is a waste of channel resources, we investigate an alternative self-deferral approach (gap-based access) using analytical and simulation models. We put forth a proposal to employ only self-deferral, treat the gap mechanism as a partial backoff, and adjust the contention window (CW) settings to the number of coexisting nodes. We demonstrate that this approach not only ensures fairness in Wi-Fi/NR-U coexistence but also avoids wasting radio channel resources and improves aggregate network throughput. Furthermore, we show that the proposed approach outperforms RS-based access and provides significant throughput and fairness gains. Finally, we implement a long short-term memory-based (LSTM) regression model to predict those Wi-Fi/NR-U CW settings which lead to coexistence fairness.},
  archive      = {J_COMCOM},
  author       = {Szymon Szott and Katarzyna Kosek-Szott and Alice Lo Valvo and Ilenia Tinnirello},
  doi          = {10.1016/j.comcom.2022.06.040},
  journal      = {Computer Communications},
  pages        = {176-188},
  shortjournal = {Comput. Commun.},
  title        = {Using self-deferral to achieve fairness between wi-fi and NR-U in downlink and uplink scenarios},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Staggered HLL: Near-continuous-time cardinality estimation
with no overhead. <em>COMCOM</em>, <em>193</em>, 168–175. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing cardinality estimation algorithms do not support natively interval queries under a sliding window model and are thereby insensitive to data recency. We present Staggered-HyperLogLog (ST-HLL), a probabilistic data structure that takes inspiration from HyperLogLog (HLL) and provides nearly continuous-time estimation of cardinality rates , rather than absolute counts . Our solution has zero-bit overhead with respect to vanilla HLL and negligible additional computational complexity . It is based on a periodic staggered reset of HLL registers and a register equalization operation at query times to compensate for staggered counting. We tested ST-HLL over both synthetic and real Internet traffic traces, showing its ability to track variations of the flow cardinality, quickly adapting to variations under non-stationary flow arrival processes. We show that for the same amount of memory footprint , our algorithm improves the accuracy up to a factor 2x with respect to the state-of-the-art solution, Sliding HLL.},
  archive      = {J_COMCOM},
  author       = {Alessandro Cornacchia and Giuseppe Bianchi and Andrea Bianco and Paolo Giaccone},
  doi          = {10.1016/j.comcom.2022.06.038},
  journal      = {Computer Communications},
  pages        = {168-175},
  shortjournal = {Comput. Commun.},
  title        = {Staggered HLL: Near-continuous-time cardinality estimation with no overhead},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cognitive quality of service predictions in multi-node
wireless sensor networks. <em>COMCOM</em>, <em>193</em>, 155–167. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks lead the way to the realization and penetration of the internet of things into daily life. As the sensors and communication devices interconnect things and people more, the quality of service demands become stringent and diverse. Optimizing conflicting quality of service goals is NP-hard. Moreover, the need for the communication systems to dynamically adapt has grown as factors like scale, application-specific performance demands, and deployment scenarios evolve. In this article, we intend to predict the quality of service with an aim to improve the performance of services in the internet of things . We conducted experiments using a real test-bed and collected performance data under a wide range of communication parameter configurations . Statistical analysis revealed a significant relationship between communication parameters and quality of service metrics. Based on the correlations, we trained deep learning models to assess the predictability of the metrics. The prediction results are encouraging. The outcomes of this research pave the way to lay the foundation for a data-driven design of adaptive quality of service in wireless sensor networks and the internet of things.},
  archive      = {J_COMCOM},
  author       = {Muhammad Ateeq and Muhammad Khalil Afzal and Sheraz Anjum and Byung-Seo Kim},
  doi          = {10.1016/j.comcom.2022.06.042},
  journal      = {Computer Communications},
  pages        = {155-167},
  shortjournal = {Comput. Commun.},
  title        = {Cognitive quality of service predictions in multi-node wireless sensor networks},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-slotted LoRa MAC with variable payload support.
<em>COMCOM</em>, <em>193</em>, 146–154. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN is an upcoming Low Power Wide Area Network (LPWAN) technology for Internet of Things implementations in various application domains. Despite its various advantages, LoRaWAN employs an Aloha-based MAC, which in terms of performance cannot guarantee high packet delivery ratio and low latency. To overcome this issue, we propose a time-slotted scheme, called TS-VP-LoRa, which supports multiple transmission times and packet sizes at the same time. In TS-VP-LoRa, scheduling is coordinated by the LoRa gateway, broadcasting beacon frames periodically for the synchronization of LoRa end-devices. A channel hopping mechanism is also proposed in order to minimize the occurrence of collisions and to evenly split the transmission load among all channels. TS-VP-LoRa is evaluated and compared to three other MAC-layer schemes in single gateway simulation scenarios with up to 500 nodes. The proposed scheme has proven to achieve low latency with high packet delivery ratios, significantly minimize collisions and maintain a relatively low energy consumption despite the scaling of the LoRa network.},
  archive      = {J_COMCOM},
  author       = {Anna Triantafyllou and Dimitrios Zorbas and Panagiotis Sarigiannidis},
  doi          = {10.1016/j.comcom.2022.06.043},
  journal      = {Computer Communications},
  pages        = {146-154},
  shortjournal = {Comput. Commun.},
  title        = {Time-slotted LoRa MAC with variable payload support},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive task offloading of rechargeable UAV edge computing
network based on double decision value iteration. <em>COMCOM</em>,
<em>193</em>, 136–145. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet of Things technology, unmanned aerial vehicles (UAVs) have attracted tremendous attentions due to its broad applications in the conditions of high rate and low delay communication. In this paper, we consider a wireless-powered and UAV-enabled MEC system. Because of the limitation of UAV’s battery capacity, we deploy microwave power stations next to the base stations to provide microwave energy to the UAV from ground microwave antenna array . The UAV receives tasks from mobile users and processes the task locally or offloads task to edge servers for processing, or charges its battery through microwave stations when its battery is low. Due to the uncertainty of the UAV’s trajectory and the working status of the microwave power stations, we build the offloading decision process as a finite Markov Decision Process(MDP) model. In the MDP model, we solve the optimization problem of minimizing the total consumption of time and energy through an optimal offloading algorithm. The algorithm we proposed is based on the fast-converging value iteration algorithm . Furthermore, extensive numerical results shows that the proposed algorithm outperforms other four conventional baseline schemes.},
  archive      = {J_COMCOM},
  author       = {Wenbin Pan and Yi Liu and Chao Yang},
  doi          = {10.1016/j.comcom.2022.06.026},
  journal      = {Computer Communications},
  pages        = {136-145},
  shortjournal = {Comput. Commun.},
  title        = {Adaptive task offloading of rechargeable UAV edge computing network based on double decision value iteration},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Two-attribute privacy protection method of MCS based on
blockchain smart contract. <em>COMCOM</em>, <em>193</em>, 126–135. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the problem of privacy leakage caused by the centralized location privacy protection method in mobile crowd sensing, a privacy protection method based on blockchain smart contract was proposed. Firstly, the trusted distributed environment blockchain is adopted to replace the third-party server. In order to avoid the leakage of personal information, users register an anonymous identity on the blockchain. In addition, in order to solve the problem of user’s location privacy leakage caused by mutual deceiving behavior among users in distributed structure, a dummy location algorithm based on two-attribute decision is designed on smart contract . In this algorithm, the distance between each location and the user location and the historical query probability of each location are obtained by calculating the peripheral location. Then, different thresholds are set for each location and a cyclic filtering is carried out. As a result, an anonymous set is obtained that effectively protects the user’s location. Finally, the simulation experiment proves that the method can not only improve the efficiency of protecting user’s location privacy, but also has lower time and computation overhead.},
  archive      = {J_COMCOM},
  author       = {Jian Wang and Guanghui Liu and Guosheng Zhao},
  doi          = {10.1016/j.comcom.2022.06.045},
  journal      = {Computer Communications},
  pages        = {126-135},
  shortjournal = {Comput. Commun.},
  title        = {Two-attribute privacy protection method of MCS based on blockchain smart contract},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural language models for network configuration:
Opportunities and reality check. <em>COMCOM</em>, <em>193</em>, 118–125.
(<a href="https://doi.org/10.1016/j.comcom.2022.06.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosted by deep learning , natural language processing (NLP) techniques have recently seen spectacular progress, mainly fueled by breakthroughs both in representation learning with word embeddings (e.g. word2vec) as well as novel architectures (e.g. transformers). This success quickly invited researchers to explore the use of NLP techniques to other field, such as computer programming languages , with the promise to automate tasks in software programming (bug detection, code synthesis, code repair, cross language translation etc.). By extension, NLP has potential for application to network configuration languages as well, for instance considering tasks such as network configuration verification, synthesis, and cross-vendor translation. In this paper, we survey recent advances in deep learning applied to programming languages , for the purpose of code verification, synthesis and translation: in particularly, we review their training requirements and expected performance, and qualitatively assess whether similar techniques can benefit corresponding use-cases in networking.},
  archive      = {J_COMCOM},
  author       = {Zied Ben Houidi and Dario Rossi},
  doi          = {10.1016/j.comcom.2022.06.035},
  journal      = {Computer Communications},
  pages        = {118-125},
  shortjournal = {Comput. Commun.},
  title        = {Neural language models for network configuration: Opportunities and reality check},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GA-based geometrically optimized topology robustness to
improve ambient intelligence for future internet of things.
<em>COMCOM</em>, <em>193</em>, 109–117. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an ambient intelligence oriented, topological robustness scheme for internet of things (IoT). The scheme primarily exploits the underlying geometric properties of scale-free IoT networks for a substantial improvement of genetic algorithm (GA) based state of the art robustness techniques. The geometrically optimized GA (Go-GA) is subsequently extended to traditional heuristics algorithms by proposing their geometrically optimized variants. All three techniques are comparatively evaluated over a simulated scale-free IoT architecture employing Schneider R R as metric of robustness. The study follows a data-driven approach where information about nodes and edges is pulled from a central big data server, and topological robustness of a given scale-free IoT is tested against existing benchmarks. The proposed scheme aims to achieve convergence to global optima and conserve computational costs by efficient edge swapping (EES) and node removal based thresholding (NRT). Performance evaluations show that Go-GA outperforms state of the art variants of GA by a margin of 20\% for Schneider R R . Traditional techniques of hill climbing algorithm (HCA), simulated annealing algorithm (SAA) and ROSE also improve by a margin of 11\%, 12\% and 14\% respectively with consideration of geometric aspects . Moreover, as the network size increases, a mere decline of 7.6\% in robustness R R is observed for Go-GA as compared to 18\% degradation for classical algorithms.},
  archive      = {J_COMCOM},
  author       = {Sabir Ali Changazi and Asim Dilawar Bakhshi and Muhammad Yousaf and Muhammad Hasan Islam and Syed Muhammad Mohsin and Shahab S. Band and Abdulmajeed Alsufyani and Sami Bourouis},
  doi          = {10.1016/j.comcom.2022.06.030},
  journal      = {Computer Communications},
  pages        = {109-117},
  shortjournal = {Comput. Commun.},
  title        = {GA-based geometrically optimized topology robustness to improve ambient intelligence for future internet of things},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deployment of UAV-mounted access points for VoWiFi service
with guaranteed QoS. <em>COMCOM</em>, <em>193</em>, 94–108. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV) networks have emerged as a promising means to provide wireless coverage in open geographical areas. Nevertheless, in wireless networks such as WiFi, signal coverage alone is insufficient to guarantee that network performance meets the quality of service (QoS) requirements of real-time communication services, as it also depends on the traffic load produced by ground users sharing the medium access. We formulate a new problem for UAVs optimal deployment in which the QoS level is guaranteed for real-time voice over WiFi (VoWiFi) communications. More specifically, our goal is to dispatch the minimum number of UAVs possible to provide VoWiFi service to a set of ground users subject to coverage, call-blocking probability, and QoS constraints. Optimal solutions are found using well-known heuristics that include K-means clusterization and genetic algorithms . Via numerical results, we show that the WiFi standard revision (e.g. IEEE 802.11a/b/g/n/ac) in use plays an important role in both coverage and QoS performance and hence, in the number of UAVs required to provide the service.},
  archive      = {J_COMCOM},
  author       = {Vicente Mayor and Rafael Estepa and Antonio Estepa and Germán Madinabeitia},
  doi          = {10.1016/j.comcom.2022.06.037},
  journal      = {Computer Communications},
  pages        = {94-108},
  shortjournal = {Comput. Commun.},
  title        = {Deployment of UAV-mounted access points for VoWiFi service with guaranteed QoS},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative driving: A comprehensive perspective, the role
of communications, and its potential development. <em>COMCOM</em>,
<em>193</em>, 82–93. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter-vehicle communications may have many reasons to be, but improving road safety and efficiency is arguably the only reason that may differentiate them from other communication infrastructures and justify a special effort in their study and deployment. This work overviews (some of) the past research on the topic to draw some lessons for the future, and tries to dissipate some of the fog that still veils the future of cooperative and autonomous vehicles: Can communications improve mobility or selfish-autonomous vehicles will dominate roads in the future? The paper is not a survey, but rather a critical analysis of what Cooperative Driving (CD) means and how communication is essential for some functions and useful for others, never detrimental. We dedicate a special part to platooning, as iconic application of CD, one of the most studied and also closer to be market ready, at least technologically. A final section discusses the potentialities of CD and what threatens its adoption.},
  archive      = {J_COMCOM},
  author       = {Renato Lo Cigno and Michele Segata},
  doi          = {10.1016/j.comcom.2022.06.034},
  journal      = {Computer Communications},
  pages        = {82-93},
  shortjournal = {Comput. Commun.},
  title        = {Cooperative driving: A comprehensive perspective, the role of communications, and its potential development},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical pattern matching for anomaly detection in time
series. <em>COMCOM</em>, <em>193</em>, 75–81. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As companies rely on an ever increasing number of connected devices for their day to day operations, a need arises for automated anomaly detectors to constantly observe crucial device metrics in real time to prevent downtime and data loss. As production environments tend to monitor a huge amount of these metrics, it prevents current state-of-the-art techniques to be deployed as the required computational resources is too high. This paper proposes a lightweight anomaly detection method that can be deployed in these environments without a reduction in accuracy. The approach works fully online, and does not require an extensive history set to be kept in memory. The method is benchmarked on the publicly available Numenta dataset, as well as a network monitoring dataset from different environments provided by a network management solution vendor. These benchmarks show the proposed technique to be very competitive with the current state-of-the-art and exceeding it in production applicability.},
  archive      = {J_COMCOM},
  author       = {M. Van Onsem and D. De Paepe and S. Vanden Hautte and P. Bonte and V. Ledoux and A. Lejon and F. Ongenae and D. Dreesen and S. Van Hoecke},
  doi          = {10.1016/j.comcom.2022.06.027},
  journal      = {Computer Communications},
  pages        = {75-81},
  shortjournal = {Comput. Commun.},
  title        = {Hierarchical pattern matching for anomaly detection in time series},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A binary gray wolf optimization algorithm for deployment of
virtual network functions in 5G hybrid cloud. <em>COMCOM</em>,
<em>193</em>, 63–74. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Virtual Network Function (VNF) is responsible for running codes that have been offloaded by mobile users, and are hosted in the cloud and edge servers of the 5G Internet’s hybrid cloud infrastructure. The two key design goals of VNF deployment in a 5G hybrid cloud are to reduce deployment cost and minimize service latency experienced by users (i.e., to maximize their Quality-of-Experiences). However, these two service parameters oppose each other as the reduction of user service latency requires the deployment of a higher number of VNF instances, incurring additional costs. In this work, the aforementioned VNF deployment problem is formulated as a Multi-objective Linear Programming (MOLP) problem that brings a trade-off between the two conflicting objectives. Due to the NP-hardness of the above MOLP framework, we develop an artificial intelligence (AI) driven meta-heuristic Binary Gray Wolf Optimization (BGWO) algorithm for VNF deployment that achieves a near-optimal solution in polynomial time . In comparison to state-of-the-art works, the results of simulated experiments, developed by Python programming version 3 . 10 . 2 3.10.2 , demonstrate a significant improvement in minimizing VNF deployment costs and maximizing users’ QoE up to 30\% and 10\%, respectively.},
  archive      = {J_COMCOM},
  author       = {Mohammad Shahjalal and Nusrat Farhana and Palash Roy and Md. Abdur Razzaque and Kuljeet Kaur and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.comcom.2022.06.041},
  journal      = {Computer Communications},
  pages        = {63-74},
  shortjournal = {Comput. Commun.},
  title        = {A binary gray wolf optimization algorithm for deployment of virtual network functions in 5G hybrid cloud},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BotStop: Packet-based efficient and explainable IoT botnet
detection using machine learning. <em>COMCOM</em>, <em>193</em>, 53–62.
(<a href="https://doi.org/10.1016/j.comcom.2022.06.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase in the adoption of the Internet of Things has increased the attack surface of these devices, encouraging malicious actors to target these devices. Vulnerable Internet of Things devices are susceptible to botnet infections that give attackers control over these devices from where they can launch attacks on other targets. In this paper, we present an efficient packet-based botnet detection system based on explainable machine learning . Our proposed approach also focuses on feature selection to produce a data set with only seven features to train a machine learning classifier that achieves very high accuracy. Testing the proposed system demonstrates an accuracy exceeding 99\% relying on these seven selected characteristics extracted from the network packets . The proposed model is explained using Shapley additive explanation to provide transparency to the classifier prediction process.},
  archive      = {J_COMCOM},
  author       = {Mohammed M. Alani},
  doi          = {10.1016/j.comcom.2022.06.039},
  journal      = {Computer Communications},
  pages        = {53-62},
  shortjournal = {Comput. Commun.},
  title        = {BotStop: Packet-based efficient and explainable IoT botnet detection using machine learning},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward native explainable and robust AI in 6G networks:
Current state, challenges and road ahead. <em>COMCOM</em>, <em>193</em>,
47–52. (<a href="https://doi.org/10.1016/j.comcom.2022.06.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6G networks are expected to face the daunting task of providing support to a set of extremely diverse services, each more demanding than those of previous generation networks (e.g., holographic communications, unmanned mobility, etc.), while at the same time integrating non-terrestrial networks, incorporating new technologies, and supporting joint communication and sensing. The resulting network architecture , component interactions, and system dynamics are unprecedentedly complex, making human-only operation impossible, and thus calling for AI-based automation and configuration support. For this to happen, AI solutions need to be robust and interpretable, i.e., network engineers should trust the way AI operates and understand the logic behind its decisions. In this paper, we revise the current state of tools and methods that can make AI robust and explainable, shed light on challenges and open problems, and indicate potential future research directions.},
  archive      = {J_COMCOM},
  author       = {Claudio Fiandrino and Giulia Attanasio and Marco Fiore and Joerg Widmer},
  doi          = {10.1016/j.comcom.2022.06.036},
  journal      = {Computer Communications},
  pages        = {47-52},
  shortjournal = {Comput. Commun.},
  title        = {Toward native explainable and robust AI in 6G networks: Current state, challenges and road ahead},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Securing internet of things devices against code tampering
attacks using return oriented programming. <em>COMCOM</em>,
<em>193</em>, 38–46. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code tampering is serious issue in Internet of Things(IOT). IoT devices are used to collect environment data like temperature value, light intensity, hart pulse etc. Once an IoT device deployed it will left untouched forever. In that situation an adversary can tamper running code through malicious software . This paper discusses a novel way to protect a binary program running inside an IoT device. This paper uses Return Oriented Programming (ROP), which is mainly used by an adversary to hijack a software program. We use ROP to prevent binary from tampering. In this method, we combine code and stack segment to execute the code. The main program call ROP enabled chain which will fetch the address of executable binary function. if any adversary try to tamper running program then they also tamper ROP chain. So, running binary will stop and send an alert to admin. we also verify our approach with RIPE benchmark and our binary is tamper proof against code tampering attach and our performance overhead is up to 5\%.},
  archive      = {J_COMCOM},
  author       = {Rajesh Kumar Shrivastava and Simar Preet Singh and Mohammad Kamrul Hasan and Gagandeep and Shayla Islam and Salwani Abdullah and Azana Hafizah Mohd Aman},
  doi          = {10.1016/j.comcom.2022.06.033},
  journal      = {Computer Communications},
  pages        = {38-46},
  shortjournal = {Comput. Commun.},
  title        = {Securing internet of things devices against code tampering attacks using return oriented programming},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving 5G network performance for OFDM-IDMA system
resource management optimization using bio-inspired algorithm with RSM.
<em>COMCOM</em>, <em>193</em>, 23–37. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The OFDM-IDMA system is a new era in the wireless domain. It is affected by the MAI and CFO effects. There are no specific CFO reduction techniques that have been identified as the best solution for a multicarrier multiuser system. This paper proposes a bio-inspired scheme for optimizing the performance of an OFDM-IDMA system. To improve BER performance in the presence of CFOs in a multiuser environment, the SIC-MUD and SIC-MUD with SU-LA algorithms are presented. The MAI effect is mitigated by the SIC-MUD technique. The SU-LA algorithm, on the other hand, improves channel estimation performance by optimizing pilot positions. The combination of these algorithms contributes to the reduction of estimation errors and, as a result, ensures the achievement of 0.0472 MSE . The CFO values used in the simulation are 0, 0.1, and 0.2, and the users are 1, 4, 8, with 16 QAM over the Rayleigh channel . When compared to the SIC-MUD algorithm, the proposed algorithm improved the BER by 41.17 percent and can tolerate 0.1 CFO in the presence of 8 users. The analysis’s second-order mathematical regression RSM model has an R2 value of 91.35 percent and accurately predicts the system response. It has been discovered that the proposed SU-LA method outperforms the HS bio-inspired algorithm.},
  archive      = {J_COMCOM},
  author       = {Makarand Jadhav and Vivek Deshpande and Divya Midhunchakkaravarthy and Dattatray Waghole},
  doi          = {10.1016/j.comcom.2022.06.031},
  journal      = {Computer Communications},
  pages        = {23-37},
  shortjournal = {Comput. Commun.},
  title        = {Improving 5G network performance for OFDM-IDMA system resource management optimization using bio-inspired algorithm with RSM},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMOBAs: A data marketplace on blockchain with arbitration
using side-contracts mechanism. <em>COMCOM</em>, <em>193</em>, 10–22.
(<a href="https://doi.org/10.1016/j.comcom.2022.06.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of a data-driven society has sparked interest in data trading marketplaces. Traditional centralized marketplaces are unable to meet the needs of a transparent, safe, and privacy-protected data marketplace. To address the above challenges, we propose a data marketplace on blockchain with arbitration using side-contracts mechanisms (dMOBAs), in which a consortium of data owners, data buyers, and arbitrators collaborate to create an ecosystem in the blockchain’s witnesses. Our marketplace’s goals are to provide a decentralized platform for trading personal data for both service providers and data owners. In the meantime, the transaction method can be arbitrated. The arbitrators alliance, which is responsible for resolving data disputes, is formed by a service provider jointly nominated by the buyer and seller. The integrity and value of transacted data can be validated thanks to the arbitrators’ consortium. The side-contracts concept decreases the public chain’s overhead and enhances the marketplace’s operating efficiency. The marketplace’s orderly functioning is ensured by explicit incentives and rigorous arbitration methods. To demonstrate the utility and efficiency of the mechanisms, we implement dMOBAs on Ethereum , Fabric and IPFS platforms and analyze its performance. According to the testing results, our arbitration method is 2 times faster than full public chain-based arbitration.},
  archive      = {J_COMCOM},
  author       = {Hangyun Tang and Yanan Qiao and Fan Yang and Bowen Cai and Ruiquan Gao},
  doi          = {10.1016/j.comcom.2022.06.029},
  journal      = {Computer Communications},
  pages        = {10-22},
  shortjournal = {Comput. Commun.},
  title        = {DMOBAs: A data marketplace on blockchain with arbitration using side-contracts mechanism},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IoT enabled HELMET to safeguard the health of mine workers.
<em>COMCOM</em>, <em>193</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information and communication technologies have aided advancements in providing a healthy and safe working environment at the workplace. Using ICT-based Personal Protective Equipment (PPE) minimizes the likelihood of workplace accidents due to the technology embedded equipment able to make decisions based on environmental factors. The Internet of Things (IoT) and Communication Technologies enable the generation of PPE models and devices with innovative features such as environmental sensing, monitoring, and risk identification, among others. Human resources working underground are confronted with various real-time issues resulting in significant health problems or even death. Human resources involved in working underground are confronted with various real-time issues that can result in significant health problems or even death. Mine workers are particularly vulnerable to toxic gases and changes in ambient conditions such as temperature, humidity, and so on. Various warning generation devices have been created, with a few implanted in mining fields. Existing warning generation systems are ineffective in protecting and saving workers’ life pneumoconiosis diseases. They cannot recognize health symptoms that occur in real-time, such as choking, heart rate fluctuations, and others. The proposed work emphasizes the development of a real-time surveillance helmet that incorporates IoT Sensors that can rescue with early-warning intelligence on the presence of fire, silicosis dust particles, temperature, harmful gases, and others that will reduce worker health risks. A built-in GPS tracker will aid in tracking the miner’s current location. Proposed work will reduce the number of hazardous accidents and deaths in such areas. Based on the accuracy parameter, this work is evaluated in three different working environments. According to the analysis, the proposed prototype’s accuracy in the coal mine working environment is approximately 96 percent, 99 percent in the indoor, and 97 percent in the outdoor (industrial) working environments.},
  archive      = {J_COMCOM},
  author       = {Ninni Singh and Vinit Kumar Gunjan and Gopal Chaudhary and Rajesh Kaluri and Nancy Victor and Kuruva Lakshmanna},
  doi          = {10.1016/j.comcom.2022.06.032},
  journal      = {Computer Communications},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {IoT enabled HELMET to safeguard the health of mine workers},
  volume       = {193},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic buffer-and-server-aided relay-assisted mobile
edge computing in time-slotted systems. <em>COMCOM</em>, <em>192</em>,
417–429. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) has attracted significant research efforts in the recent years. However, these works consider mostly the computation resources located at the cloud centers and wireless access nodes, ignoring the possibility of utilizing server-empowered relays to improve the performance. In this paper, we study stochastic relay-assisted MEC in systems with discrete transmission time-line and block fading wireless channels. In order to clearly identify and inspect the fundamental affecting factors, we investigate the building block of this architecture, namely a hierarchical network consisting of a source, a buffer-and-server-aided relay and another higher-level computing node. We provide a framework to take into account the effects of the fading channels, the task arrival dynamics as well as the queuing delays in both the transmission and computation buffers, which facilitates the derivation of the expression for the Average Response Time (ART). Based on that and the system average power consumption in each slot, we introduce the concept of Average Response Energy (ARE) as a novel metric to capture the energy efficiency in MEC while considering the stochastic nature of the system parameters. Accordingly, we propose two offloading schemes with their respective problem formulations, namely the Minimum ART (MART) and the Minimum ARE (MARE) schemes, to optimize the transmission power and task assignment probability while keeping the system queues stable. We demonstrate the difference of the formulated problems with the relevant problem in a recent work, analyze the properties of the problems and noting them, we propose effective solution methods. Using extensive simulations, we validate the presented analysis and show the effectiveness of the proposed schemes in comparison with various baseline methods adapting existing approaches. Moreover, we provide essential insights on the performance of the proposed schemes in terms of the average delay and power consumption in the different scenarios.},
  archive      = {J_COMCOM},
  author       = {Javad Hajipour},
  doi          = {10.1016/j.comcom.2022.06.025},
  journal      = {Computer Communications},
  pages        = {417-429},
  shortjournal = {Comput. Commun.},
  title        = {Stochastic buffer-and-server-aided relay-assisted mobile edge computing in time-slotted systems},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A discrete time-varying greywolf IoT botnet detection
system. <em>COMCOM</em>, <em>192</em>, 405–416. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) is an emerging network that is trending due to advances in computing and networking development. A botnet is an attack that threatens the IoT system because of the increased number of compromised connected IoT devices. The conventional counter measurements are unable to detect this attack. This problem becomes a hot topic for researchers and practitioners who introduced many secured solutions to stop the risks of a botnet attack. The intrusion detection systems are promising solutions to tackle botnet attacks and discover malicious patterns. Recently, many studies investigated the impact of reducing the number of dataset’s attributes (features) on the performance of detecting IoT attacks. Selecting the relevant features in a dataset is a data mining technique that has been efficiently integrated with designing secured systems for detecting botnets. This research paper proposes a new system for discovering botnet attacks in the context of IoT by applying a wrapper feature selection (FS) technique using an improved algorithm inspired by the natural swarming architecture of gray wolves called Gray Wolves Optimization (GWO). The transfer function (TF) maps the standard GWO which is originally developed to work in continuous search space to perform its optimization job in discrete search space. Different types of TFs that belong to S type and V type groups are used to generate eight discrete versions of GWO to optimize the binary feature space. This study contributes by adopting time-variant TFs to identify the best time to switch the global search into local search to achieve trade-offs in the search job of the BGWO and approach to the best-optimized solution. Time-variant TFs facilitate the global search at the beginning of the search process to fetch new solutions in new regions of feature space. In the later phases of the search process, the need is to expose more searches in the local region to get the most optimized solution among the neighborhood solutions. A real IoT traffic that is represented by the N-BaIoT dataset is utilized to evaluate the BGWO and other compared methods. The comparison results of the experiments show that the time-variant TFs enhance the capability of the GWO optimizer in alleviating the premature conversion and finding the best feature subset within a reasonable running time. Therefore, the BGWO-TV-S1 is recommended to be integrated into the IoT network as an intrusion detection algorithm with accuracy 98.97\%, fitness value 1.31\%, 51.2210 selected features and running time 503.7132 s.},
  archive      = {J_COMCOM},
  author       = {Moutaz Alazab},
  doi          = {10.1016/j.comcom.2022.06.016},
  journal      = {Computer Communications},
  pages        = {405-416},
  shortjournal = {Comput. Commun.},
  title        = {A discrete time-varying greywolf IoT botnet detection system},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based mobile services in 5G network.
<em>COMCOM</em>, <em>192</em>, 402–404. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  doi          = {10.1016/j.comcom.2022.06.023},
  journal      = {Computer Communications},
  pages        = {402-404},
  shortjournal = {Comput. Commun.},
  title        = {Learning-based mobile services in 5G network},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formal verification of persistence and liveness in the
trust-based blockchain crowdsourcing consensus protocol.
<em>COMCOM</em>, <em>192</em>, 384–401. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is a potential computing paradigm that exploits collective human intelligence to solve complex tasks, but it suffers from various safety and security problems. Blockchain has emerged as a promising technology to address most of the security issues, however, it is challenging to find an appropriate and trusted blockchain-based consensus protocol for crowdsourcing services. This work proposes a novel Trust-based Blockchain Crowdsourcing consensus protocol that selects a leader and validators based on various trust factors. The proposed protocol addresses a major issue of ensuring correctness associated with the safety and security-critical systems which has a vital importance because failure of such systems may lead to adverse consequences. Mainly it is focused on persistence and liveness properties preventing invalid block insertion and consensus delay attacks. Model checking technique is utilized because of its effectiveness and automatic nature to perform formal verification . The proposed protocol is specified using Communicating Sequential Programs, and the persistence and liveness properties are specified through Linear Temporal Logic . The model verification is performed by giving the formal model and the properties as input to the Process Analysis Toolkit which checks for the satisfaction or violation of the properties.},
  archive      = {J_COMCOM},
  author       = {Hamra Afzaal and Muhammad Imran and Muhammad Umar Janjua},
  doi          = {10.1016/j.comcom.2022.06.014},
  journal      = {Computer Communications},
  pages        = {384-401},
  shortjournal = {Comput. Commun.},
  title        = {Formal verification of persistence and liveness in the trust-based blockchain crowdsourcing consensus protocol},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on prediction-based caching and computing in
cognitive communications. <em>COMCOM</em>, <em>192</em>, 382–383. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Yin Zhang and Iztok Humar and Jeungeun Song and Jiafu Wan},
  doi          = {10.1016/j.comcom.2022.06.021},
  journal      = {Computer Communications},
  pages        = {382-383},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on prediction-based caching and computing in cognitive communications},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy aware internet of medical things data certification
framework on healthcare blockchain of 5G edge. <em>COMCOM</em>,
<em>192</em>, 373–381. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 5G communication technology and edge computing jointly form 5G Edge and eliminate the constraints of Internet-of-Medical Things (IoMT) devices to facilitate several real-time healthcare services , including patient monitoring and diagnoses from anywhere. Nevertheless, the 5G Edge platform introduces risks of internal attacks to the data integrity and privacy of IoMT data. Hence, end users cannot trust data retrieved from the 5G Edge. This paper proposes a privacy-preserving search result certification framework for IoMT data on 5G and edge computing-assisted blockchain networks. The novelty of this paper is twofold. First , the proposed framework introduces a blockchain platform involving 5G Edge servers to ensure tamperproof data storage. In addition, an encrypted data storage model and a symmetric-key cryptography-based privacy-preserving search mechanism are developed for the 5G Edge blockchain. Second , the framework leverages an efficient multi-signature scheme to design a tamperproof search result certification mechanism for IoMT data on the blockchain of 5G Edge. Several experiments are conducted to evaluate the performance of the proposed framework.},
  archive      = {J_COMCOM},
  author       = {Mohammad Saidur Rahman and Abdulatif Alabdulatif and Ibrahim Khalil},
  doi          = {10.1016/j.comcom.2022.06.013},
  journal      = {Computer Communications},
  pages        = {373-381},
  shortjournal = {Comput. Commun.},
  title        = {Privacy aware internet of medical things data certification framework on healthcare blockchain of 5G edge},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A privacy-preserving mutual authentication scheme for group
communication in VANET. <em>COMCOM</em>, <em>192</em>, 357–372. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Vehicular Ad-Hoc Network (VANET) represents a collection of vehicles that communicate among themselves and the road infrastructure for exchange of information through a wireless network. It provides services like road safety, infotainment , optimized traffic flow, location-based services and other on-demand services. However, due to its intrinsic ad-hoc nature, there are several challenges in the deployment of VANETs. One such challenging issue is ensuring security and privacy of the exchanged messages. Therefore, it is important that the vehicles are properly authenticated and necessary security associations are properly established. Most of the recent works in this area sends plain text messages while communicating, that results in compromise of message confidentiality. Moreover, in majority of these works, an active communication with a trusted authority is required during authentication that results in increased communication latency . This paper puts forward a privacy preserving mutual authentication scheme for group communication in VANET that does not require an active communication with a trusted authority. Furthermore, the proposed scheme use pseudonym for identity privacy and the messages exchanged in the scheme are encrypted before transmission. Formal and informal security analysis shows that the proposed scheme is robust against various security attacks. Through a performance analysis, the scheme is found to be efficient.},
  archive      = {J_COMCOM},
  author       = {Himun Jyoti Nath and Hiten Choudhury},
  doi          = {10.1016/j.comcom.2022.06.024},
  journal      = {Computer Communications},
  pages        = {357-372},
  shortjournal = {Comput. Commun.},
  title        = {A privacy-preserving mutual authentication scheme for group communication in VANET},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial of ACM MSWiM 2020 special issue. <em>COMCOM</em>,
<em>192</em>, 355–356. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Antonio A.F. Loureiro and Carlo Giannelli},
  doi          = {10.1016/j.comcom.2022.06.020},
  journal      = {Computer Communications},
  pages        = {355-356},
  shortjournal = {Comput. Commun.},
  title        = {Editorial of ACM MSWiM 2020 special issue},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Load balancing scheduling algorithms for virtual computing
laboratories in a desktop-as-a-service cloud computing services.
<em>COMCOM</em>, <em>192</em>, 343–354. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present research work is interested in the study of the virtual computing laboratories in a Desktop-As-A-Service (DAAS) scheduling in a cloud computing environment . The issue emanates from a real-world study in which the Saudi Electronic University expresses a need to efficiently schedule the labs in predefined sessions for one of its colleges over all its branches. The problem under investigation consists, on the one hand, in load balancing the assignment of the labs to the sessions regarding the number of participants, and on the other hand, in determining the types and number of hosts required to ensure the best sessions hosts assignments. While the load balancing is evaluated by measuring the distance between the maximum and the minimum number of the virtual machines load, the maximum number of the virtual machines load, and the variance of number of the virtual machines load, the efficiency of the host selection and assignments are assessed in terms of the unbalance of the remaining capacity of each used host. We propose linearized mathematical models for the subproblems and the whole problem with the different objective functions. We firstly suggest a two-stage optimization approach in which the mathematical models via CPLEX are solved. Then, we deal with the combined method which does not divide the whole problem into its two sub-problems. Next, we run the combined model on CPLEX, and we propose two heuristics to have a feasible good solution in reasonable time. Subsequently, we compare the obtained results for diverse sizes of instances we generate according to the University features. The proposed heuristics are therefore proven to be a compelling solution to the powerful software CPLEX’s failure to solve the problem for large-scale cases. Moreover, the obtained results demonstrate that they outperform those currently in use in the University.},
  archive      = {J_COMCOM},
  author       = {Mohamed Jarraya and Sonda Elloumi},
  doi          = {10.1016/j.comcom.2022.06.004},
  journal      = {Computer Communications},
  pages        = {343-354},
  shortjournal = {Comput. Commun.},
  title        = {Load balancing scheduling algorithms for virtual computing laboratories in a desktop-as-A-service cloud computing services},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mathematical analysis and design of PMTD strategies for an
SIRO model of OS virus propagation. <em>COMCOM</em>, <em>192</em>,
332–342. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly aims to explore the propagation behaviors of Operating System (OS) virus in Multi-OSs network, as well as propose virus transmission-suppression strategy based on Platform Moving Target Defense (PMTD). To this end, a dynamical Susceptible–Infected–Removed–Others (SIRO) model is established and analyzed theoretically, including the basic reproduction number, equilibrium point and their stability. Systematic analysis finds that the Operating System virus will die out or persist in the network depending on the basic reproduction number, which is mainly associated with the network topology , Operating System migration frequency and Operating System scale. Two network defense strategies based on Platform Moving Target Defense as well as an Operating System migration management algorithm are proposed, and a set of simulation experiments are designed to verify the effectiveness. Experimental results show that the propagation of Operating System virus can be inhibited effectively by adjusting Operating System migration probability. Besides, proposed methods can replenish the drawback of traditional link interrupt strategy, providing various options for the defense vacuum period of vulnerability patch development.},
  archive      = {J_COMCOM},
  author       = {Enning Zhang and Gang Wang and Yun Feng and Runnian Ma},
  doi          = {10.1016/j.comcom.2022.06.006},
  journal      = {Computer Communications},
  pages        = {332-342},
  shortjournal = {Comput. Commun.},
  title        = {Mathematical analysis and design of PMTD strategies for an SIRO model of OS virus propagation},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning security attacks and defense approaches for
emerging cyber physical applications: A comprehensive survey.
<em>COMCOM</em>, <em>192</em>, 316–331. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cyber physical systems integrate the sensing, computation, control and networking processes into physical objects and infrastructure, which are connected through the Internet to execute a common task. Cyber physical systems can be applied in various applications, like healthcare, transportation, industrial production, environment and sustainability, and security and surveillance. However, the tight coupling of cyber systems with physical systems introduce challenges in addressing stability, security, efficiency and reliability. The machine learning (ML) security is the inclusion of cyber security mechanism to provide protection to the machine learning models against various cyber attacks . The ML models work through the traditional training and testing approaches. However, execution of such kind of approaches may not function effectively in case if a system is connected to the Internet. As online hackers can exploit deployed security mechanisms and poison the data. This data is then taken as the input by the ML models. In this article, we provide the details of various machine learning security attacks in cyber physical systems . We then discuss some defense mechanisms to protect against these attacks. We also present a threat model of ML security mechanisms deployed in cyber systems. Furthermore, we discuss various issues and challenges of ML security mechanisms deployed in cyber systems. Finally, we provide a detailed comparative study on performance of the ML models under the influence of various ML attacks in cyber physical systems.},
  archive      = {J_COMCOM},
  author       = {Jaskaran Singh and Mohammad Wazid and Ashok Kumar Das and Vinay Chamola and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2022.06.012},
  journal      = {Computer Communications},
  pages        = {316-331},
  shortjournal = {Comput. Commun.},
  title        = {Machine learning security attacks and defense approaches for emerging cyber physical applications: A comprehensive survey},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource orchestration in 5G and beyond: Challenges and
opportunities. <em>COMCOM</em>, <em>192</em>, 311–315. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {5G networks have strict constraints regarding the services in terms of latency, reliability, and availability, which pose additional challenges to the traditional orchestration solutions, and 6G networks will increment the number of slices and services deployed over different technological domains, adding more difficulties for the orchestrator. Distributed and automated solutions will be essential for this context. This article identifies the main challenges in 5G/6G orchestration and then describes the utility of Artificial Intelligence-driven solutions, outlining an orchestrator architecture for 5G networks. The architecture is then explored as an orchestration solution for two ongoing research projects focused on the deployment of critical services over 5G networks.},
  archive      = {J_COMCOM},
  author       = {Karima Velasquez and David Perez Abreu and Marilia Curado and Edmundo Monteiro},
  doi          = {10.1016/j.comcom.2022.06.019},
  journal      = {Computer Communications},
  pages        = {311-315},
  shortjournal = {Comput. Commun.},
  title        = {Resource orchestration in 5G and beyond: Challenges and opportunities},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FEDGAN-IDS: Privacy-preserving IDS using GAN and federated
learning. <em>COMCOM</em>, <em>192</em>, 299–310. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a promising distributed training model that aims to minimize the data sharing to enhance privacy and performance. FL requires sufficient and diverse training data to build efficient models. Lack of data balance as seen in rare classes affects the model accuracy. Generative Adversarial Networks (GAN) are remarkable in data augmentation to balance the available training data. In this article, we propose a novel Federated Deep Learning (DL) Intrusion Detection System (IDS) using GAN, named FEDGAN-IDS, to detect cyber threats in smart Internet of Things (IoT) systems; smarthomes, smart e-healthcare systems and smart cities. We distribute the GAN network over IoT devices to act as a classifier and train using augmented local data. We compare the convergence and accuracy of our model with other federated intrusion detection models. Extensive experiments with multiple datasets demonstrates the effectiveness of the proposed FEDGAN-IDS. The model performs better and converges earlier than the state-of-the-art standalone IDS.},
  archive      = {J_COMCOM},
  author       = {Aliya Tabassum and Aiman Erbad and Wadha Lebda and Amr Mohamed and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2022.06.015},
  journal      = {Computer Communications},
  pages        = {299-310},
  shortjournal = {Comput. Commun.},
  title        = {FEDGAN-IDS: Privacy-preserving IDS using GAN and federated learning},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Node localization algorithm for wireless sensor networks
based on static anchor node location selection strategy.
<em>COMCOM</em>, <em>192</em>, 289–298. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better solve the contradiction between the localization accuracy , localization coverage, and the location of anchor nodes in wireless sensor networks , a node localization algorithm for wireless sensor networks based on static anchor node location selection strategy is proposed in this paper. Firstly, collect the signal strength between wireless sensor network nodes, judge whether there is a connection between nodes according to the set signal strength threshold, and convert the node distribution diagram into the node connection relationship diagram, that is, the topology diagram. Then, the closeness centrality value of each node is calculated by using the closeness centrality algorithm, and the obtained closeness centrality values are sorted in descending order, the node with the largest closeness centrality value is the first anchor node, the closeness centrality values are traversed at equal intervals, and the optimal equal interval is selected by using the quick sort algorithm, and the selected nodes are used as the other anchor nodes. Finally, other unknown nodes in the network are located according to the location of anchor nodes. Simulation results show that the proposed algorithm is superior to the existing typical algorithms in localization accuracy and localization coverage.},
  archive      = {J_COMCOM},
  author       = {Wenyan Liu and Xiangyang Luo and Guo Wei and Huaixing Liu},
  doi          = {10.1016/j.comcom.2022.06.010},
  journal      = {Computer Communications},
  pages        = {289-298},
  shortjournal = {Comput. Commun.},
  title        = {Node localization algorithm for wireless sensor networks based on static anchor node location selection strategy},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low computational complexity joint iterative detection and
decoding without ARQ in massive MIMO systems with UAVs. <em>COMCOM</em>,
<em>192</em>, 279–288. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have a wide range of military and commercial applications. UAVs play an important role in 6G networks due to their low cost and flexible positioning. UAVs can play different roles in the network and act as an air base station , as a relay, or as users in cellular networks . One of the major challenges in these systems is to increase lifespan with low energy consumption . The automatic repeat request (ARQ) protocol is used to increase throughput and obtain dependable communications for multi-input multiple-input multiple-output (MIMO) systems, even in the presence of severe propagation conditions . On the other hand, (UAV)-enabled communication system provides flexibility and reliability compared to conventional ones which have been considered in the new generation of wireless communications . Thus, using ARQ protocols in this system faces serious challenges such as increasing delay and operational implementation complexities in the receiver. Therefore, to meet the ARQ challenge in the uplink of massive MIMO systems, we have investigated utilizing joint iterative detection and decoding methods with reduced computational complexity have been proposed in a UAV-enabled communication system in the condition that users are equipped with two antennas to provide a solution to reduce energy consumption. In this structure, system performance improves as the computational complexity increases. Therefore, we look for methods that reduce complexity while maintaining system performance at an acceptable level. In this study, we seek to provide a solution that can establish a reasonable compromise between system performance and computational complexity. This aim is achieved by establishing a connection between the components of soft joint detection and decoding, linear detection with approximation methods, and sorting. The first part of the receiver is sorting the users before detection, then, in the next module, a turbo-process-based low-complexity MIMO iterative detection and decoding (IDD) algorithm, with minimum mean square error (MMSE) detector and soft channel decoder , is used. For solving the challenge of computational complexity, utilizing of approximation-based detection method is proposed. This structure works in such a way that after sorting the users, in the first iteration, a certain number of users are decoded using a hard decoding scheme after the soft detection and decoding. Therefore, these data are subtracted from the soft feedback information (prior data) to the detector, as inter user interference (IUI) and the Joint IDD module continue the iterative cycle until all users’ data are decoded.},
  archive      = {J_COMCOM},
  author       = {Seyed Hosein Mousavi and Jafar Pourrostam and Mahdi Nangir},
  doi          = {10.1016/j.comcom.2022.06.009},
  journal      = {Computer Communications},
  pages        = {279-288},
  shortjournal = {Comput. Commun.},
  title        = {Low computational complexity joint iterative detection and decoding without ARQ in massive MIMO systems with UAVs},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EESR: Energy efficient sector-based routing protocol for
reliable data communication in UWSNs. <em>COMCOM</em>, <em>192</em>,
268–278. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a remarkable need of advancement in data communication for application of sensor and ad-hoc networks for reliable and secure delivery of data. Due to the dynamic behavior of the network, it is an emerging demand to achieve reliability in data communication. The existence of different network environment features and diverse dynamic topology leads to the need of protocol design for efficient and reliable delivery of data in underwater acoustic sensor networks. The paper proposes energy efficient and reliable protocol design for routing data packets in Underwater Wireless Sensor Networks environment using sector based forwarding mechanism. The sector based forwarding mechanism helps in routing the reliable data in an efficient and secure manner The proposal suggests to model the network in such a way that the entire network is divided into sectors to optimize the hop count that further leads towards the energy efficient delivery of data. The mechanism of data forwarding is done vertically by maintaining sector wise sequence through pivot node which is considered as sector head. The proposed sector-based network topology mechanism is useful in identifying the number of hops used in communication along with enhancement in energy for effective data communication. The experiment performed and its consecutive results suggests that the proposed protocol enriches the energy utilization of the network. The enhancement of data communication reliability in Underwater Wireless Sensor Networks environment is managed by the mechanism of reduced hop count.},
  archive      = {J_COMCOM},
  author       = {Rakesh Kumar and Shashi Shekhar and Hitendra Garg and Mukesh Kumar and Bhisham Sharma and Sanjay Kumar},
  doi          = {10.1016/j.comcom.2022.06.011},
  journal      = {Computer Communications},
  pages        = {268-278},
  shortjournal = {Comput. Commun.},
  title        = {EESR: Energy efficient sector-based routing protocol for reliable data communication in UWSNs},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hop-by-hop bandwidth allocation and deployment for SFC with
end-to-end delay QoS guarantees. <em>COMCOM</em>, <em>192</em>, 256–267.
(<a href="https://doi.org/10.1016/j.comcom.2022.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization and 5G network slicing technology can provide a more resilient and manageable Internet by deploying various requests on different slices. In this paper, we propose a hop-by-hop bandwidth allocation framework for service function chains (SFCs) to guarantee delay QoS constraints of requests. A end-to-end (E2E) SFC contains multiple virtual network functions (VNFs), which is constructed as a tandem queueing system . The arrival is modeled as an interrupted bernoulli process (IBP) or two-aggregation Markov modulated bernoulli process (2A-MMBP) to represent the traffic of typical services, data or video call. And the effective bandwidth (EB) formulas for these intricate Markovian processes are derived. Then we use EB/ effective capacity (EC) theory to allocate the bandwidth for SFCs hop by hop and deduce the service probabilities of the nodes. On the basis of EB/EC bandwidth allocation , the departure process is fitted into an IBP or a MMBP and then becomes the arrival of the next hop, which is beneficial to engineering implementation. The novel delay evaluation metrics , E2E packet delay and delay violation probability, are developed to assess the performance of SFC deployment. We formulate the SFC deployment as an integral linear programming (ILP) problem with the objective of minimizing the total cost. Finally, a heuristic algorithm called hop-by-hop bandwidth allocation and SFC deployment (HBASD) is designed to solve this problem. Simulation results show that compared with the up-to-date methods, our algorithm can guarantee the QoS requirements of E2E delay for services and save resources.},
  archive      = {J_COMCOM},
  author       = {Yuexin Sun and Xuefen Chi and Baozhu Yu and Shuang Zhao and Shuai Li and Qinglu Meng},
  doi          = {10.1016/j.comcom.2022.06.002},
  journal      = {Computer Communications},
  pages        = {256-267},
  shortjournal = {Comput. Commun.},
  title        = {Hop-by-hop bandwidth allocation and deployment for SFC with end-to-end delay QoS guarantees},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resiliency-aware analysis of complex IoT process chains.
<em>COMCOM</em>, <em>192</em>, 245–255. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resiliency-aware Internet of Thing (IoT) can protect sensitive information depending on the ongoing agenda in smart cities in case of failure in critical processes. Guaranteeing resiliency and availability are two critical issues in making safe real-world IoT scenarios. Existing strategies behave differently and most of them fail to pick up all the significant aspects of resiliency-aware IoT in a dynamic system. This paper finds critical activities in IoT process chains in real domains of IoT according to the proposed parameter-based greedy strategy . It analyzes the resiliency and availability of the chains under the failure of these nodes. In this paper, resiliency is measured as the ratio of performance and availability of an activity network before and after disrupting the critical activities. Resiliency is then assessed based on the four selected structural metrics for process chains(Eigenvector Centrality, Closeness Centrality , Betweenness Centrality , and Markov Chain). The results show that the decline rates of network performance, resiliency, and availability are maximum in the case of the failure of activities with a higher value of the Markov Chain (MC) metric, nearly 70\%. Moreover, the loss rate of resiliency and availability is evaluated after the failure of high-ranked structural activities and is compared to detect critical activities in a process chain. A replication-based experiment, named the MC-based replication method, is finally proposed to introduce a resilience strategy across multiple situations in IoT settings. Compared to the available replication protocols, the suggested MC-based replication method improves response time and reduces resource redundancy by about 18\% and 45\%, respectively, for a significant scenario, by selecting efficiently replicated nodes.},
  archive      = {J_COMCOM},
  author       = {Maryam Nooraei Abadeh and Mansooreh Mirzaie},
  doi          = {10.1016/j.comcom.2022.06.007},
  journal      = {Computer Communications},
  pages        = {245-255},
  shortjournal = {Comput. Commun.},
  title        = {Resiliency-aware analysis of complex IoT process chains},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning-based joint task and energy
offloading in UAV-aided 6G intelligent edge networks. <em>COMCOM</em>,
<em>192</em>, 234–244. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge networks are expected to play an important role in 6G where machine learning-based methods are widely applied, which promote the concept of Edge Intelligence . Meanwhile, Unmanned Aerial Vehicle (UAV)-enabled aerial network is significant in 6G networks to achieve seamless coverage and super-connectivity. To this end, a joint task and energy offloading problem is studied under a UAV-aided and energy-constrained intelligent edge network, consisting of a high altitude platform (HAP), multiple UAVs, and on-ground fog computing nodes (FCNs). To guarantee the energy supply of UAVs and FCNs, both simultaneous wireless information and power transfer (SWIPT), as well as laser charging techniques are considered. Specifically, we investigate a scenario where each UAV needs to execute a computation-intensive task during each time slot and can be powered by the laser beam transmitted from the HAP. Due to the limited computation resources, each UAV can offload part of the task and energy to the FCNs for collaborative computing , to reduce local energy consumption and the overall task execution delay by adopting SWIPT. Considering the dynamics of the network, e.g., the time-varying locations of UAVs and available computation resources of FCNs, the problem is formulated as a cooperative multi-agent Markov game for UAVs, which aims to maximize the total system utility, by optimizing the task partitioning and power allocation strategies of each UAV, regarding task size, average delay and energy consumption of task execution. To tackle this problem, we propose a multi-agent soft actor–critic (MASAC)-based approach to resolve the problem. Numerical simulation results prove the superiority of our proposed approach as compared with benchmark methods.},
  archive      = {J_COMCOM},
  author       = {Zhipeng Cheng and Minghui Liwang and Ning Chen and Lianfen Huang and Xiaojiang Du and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2022.06.017},
  journal      = {Computer Communications},
  pages        = {234-244},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning-based joint task and energy offloading in UAV-aided 6G intelligent edge networks},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Timely and sustainable: Utilising correlation in status
updates of battery-powered and energy-harvesting sensors using deep
reinforcement learning. <em>COMCOM</em>, <em>192</em>, 223–233. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a system with energy-constrained sensors, each transmitted observation comes at a price. The price is the energy the sensor expends to obtain and send a new measurement. The system has to ensure that sensors’ updates are timely, i.e., their updates represent the observed phenomenon accurately, enabling services to make informed decisions based on the information provided. If there are multiple sensors observing the same physical phenomenon, it is likely that their measurements are correlated in time and space. To take advantage of this correlation to reduce the energy use of sensors, in this paper we consider a system in which a gateway sets the intervals at which each sensor broadcasts its readings. We consider the presence of battery-powered sensors as well as sensors that rely on Energy Harvesting (EH) to replenish their energy. We propose a Deep Reinforcement Learning (DRL)-based scheduling mechanism that learns the appropriate update interval for each sensor, by considering the timeliness of the information collected measured through the Age of Information (AoI) metric, the spatial and temporal correlation between readings, and the energy capabilities of each sensor. We show that our proposed scheduler can achieve near-optimal performance in terms of the expected network lifetime.},
  archive      = {J_COMCOM},
  author       = {Jernej Hribar and Luiz A. DaSilva and Sheng Zhou and Zhiyuan Jiang and Ivana Dusparic},
  doi          = {10.1016/j.comcom.2022.05.030},
  journal      = {Computer Communications},
  pages        = {223-233},
  shortjournal = {Comput. Commun.},
  title        = {Timely and sustainable: Utilising correlation in status updates of battery-powered and energy-harvesting sensors using deep reinforcement learning},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive regional model spectrum sensing algorithm based
on TD-LTE system. <em>COMCOM</em>, <em>192</em>, 210–222. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive radio is an important way to improve spectrum efficiency and spectrum sensing is the key technology of cognitive radio. However, the performance of spectrum sensing only from time–frequency dimension is insufficient. In order to get better sensing performance, an adaptive regional model spectrum sensing algorithm is proposed in this paper. Firstly, an improved region model is proposed through interference analysis. By modifying the boundary calculation method and adjusting the region sensing strategy, the proposed region model can achieve higher spectrum utilization. Secondly, the time-sharing spectrum sensing strategy and cognitive database are proposed to reduce system power consumption , and give full play to the systematic advantages of the regional model. Thirdly, an adaptive spectrum sensing algorithm is designed according to the characteristics of the regional model to realize the time-sharing spectrum sensing strategy, which ensures the consistency of the overall regional sensing performance through the optimal sampling frequency and hyperplane-distance based cooperative sensing. Finally, the spectrum prediction algorithm is used to further optimize the spectrum sensing performance. The simulation results show that the proposed regional model has higher spectrum utilization and better adaptability. Compared with other regional models, the spectrum utilization can be improved by more than 11.12\% in the region with high occupancy channel and the activity range of primary users is greater than 2 km. And the control error of model classification performance can be controlled within 2.11\% by sampling frequency. At the same time, the combination of spectrum prediction technology can effectively improve the spectrum sensing performance.},
  archive      = {J_COMCOM},
  author       = {Yuhao Wang and Jiaxun Xiao and Changgeng Li},
  doi          = {10.1016/j.comcom.2022.06.008},
  journal      = {Computer Communications},
  pages        = {210-222},
  shortjournal = {Comput. Commun.},
  title        = {An adaptive regional model spectrum sensing algorithm based on TD-LTE system},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting application usage based on latent contextual
information. <em>COMCOM</em>, <em>192</em>, 197–209. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting application usage is useful for offering personalized services , improving mobile energy consumption, and mobile system resource management optimization. Currently, however, there are many possible applications, and each user has his/her own preferences and usage patterns, which makes the application prediction task very challenging. In this study we use different representation methods to represent mobile users’ contextual information in order to predict application usage. We focus on the spatial information context (i.e., where the applications are used) and represent it with graph embeddings, which capture the locations users have visited based on their movement. We use multimodal embeddings to represent the temporal context, users’ identifiers, and previously used applications. Then, the contextual information’s latent representation is used in a deep learning framework composed of a GRU (gated recurrent unit), attention layer, and a softmax layer to provide application usage predictions. We evaluate our method on two real-world datasets comprised of data collected from mobile users’ devices. Our results show that the proposed application usage prediction method outperforms various machine learning models and state-of-the-art solutions. We also found that the spatial information’s latent representation derived from graph embeddings outperformed traditional and commonly used representation methods when predicting application usage. Our findings also reveal interesting usage patterns regarding users’ predictability, which can help us better understand users’ behavior.},
  archive      = {J_COMCOM},
  author       = {Adir Solomon and Bracha Shapira and Lior Rokach},
  doi          = {10.1016/j.comcom.2022.06.005},
  journal      = {Computer Communications},
  pages        = {197-209},
  shortjournal = {Comput. Commun.},
  title        = {Predicting application usage based on latent contextual information},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual speed test: An AP tool for passive analysis of
wireless LANs. <em>COMCOM</em>, <em>192</em>, 185–196. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet speed tests assess end-to-end network performance by measuring throughput for 10’s of MB of TCP uploads and downloads. While such tests provide valuable insights into network health, they are of little use to network administrators since (1) the results are only available on the client that performs the test and (2) the tests can saturate the network, increasing load and worsening performance for other clients. In this paper, we present virtual speed test, a measurement based framework that enables an AP to estimate speed test results for any of its associated clients without any special-purpose probing, with zero end-user co-operation and purely based on passively observable parameters at the AP. We implemented virtual speed test using commodity hardware, deployed it in office and residential environments, and conducted measurements spanning multiple days having different network loads and channel conditions. Overall, virtual speed test has mean estimation error less than 6\% compared to ground truth speed tests, yet with zero overhead, and outcomes available at the AP.},
  archive      = {J_COMCOM},
  author       = {Peshal Nayak and Edward W. Knightly},
  doi          = {10.1016/j.comcom.2022.05.031},
  journal      = {Computer Communications},
  pages        = {185-196},
  shortjournal = {Comput. Commun.},
  title        = {Virtual speed test: An AP tool for passive analysis of wireless LANs},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative cellular UAV-to-everything (c-U2X) communication
based on 5G sidelink for UAV swarms. <em>COMCOM</em>, <em>192</em>,
173–184. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A swarm of cellular-connected Unmanned Aerial Vehicles (UAVs) enables new possibilities for emerging services and applications as the UAVs can autonomously coordinate their activities and cooperate to accomplish a given task. Because of spatio-temporal dynamics of swarm topology, robust and reliable network formation with seamless connectivity among UAVs is highly critical for any successful mission. This work focuses on the problem of cooperative and communication-aware UAV channel scheduling of data transmission from a set of target points of interests (PoIs) towards a cellular base station (BS). A novel, cooperative multi-hop communication model based on the C-V2X (Cellular Vehicle-to-Everything) Mode 4 cellular sidelink (PC5) radio interface is presented for efficient UAV data flow scheduling within the swarm. The model design aims at optimizing the cellular communications on UAV-to-UAV (U2U) and UAV-to-Infrastructure (U2I) links via a novel interference-aware scheduling, hence envisioning a new Cellular UAV-to-Everything (C-U2X) communication paradigm. An optimization model is used to solve the centralized version of the problem, while a distributed Dynamic Consensus-Based Bundle Algorithm (D-CBBA) is proposed to generate the best subchannel scheduling for maximizing data transmission in a distributed setting. Extensive simulation results demonstrate that the proposed distributed algorithm is able to extend the cellular infrastructure coverage while improving the multi-hop communications by autonomously adapting to the network conditions.},
  archive      = {J_COMCOM},
  author       = {Debashisha Mishra and Angelo Trotta and Emiliano Traversi and Marco Di Felice and Enrico Natalizio},
  doi          = {10.1016/j.comcom.2022.06.001},
  journal      = {Computer Communications},
  pages        = {173-184},
  shortjournal = {Comput. Commun.},
  title        = {Cooperative cellular UAV-to-everything (C-U2X) communication based on 5G sidelink for UAV swarms},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial for special issue on machine learning approaches
in IoT scenarios. <em>COMCOM</em>, <em>192</em>, 171–172. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Gaia Maselli and Laura Galluccio and Imen Grida Ben Yahia and Noura Limam},
  doi          = {10.1016/j.comcom.2022.05.034},
  journal      = {Computer Communications},
  pages        = {171-172},
  shortjournal = {Comput. Commun.},
  title        = {Editorial for special issue on machine learning approaches in IoT scenarios},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-based passive crowd monitoring through WiFi beacons.
<em>COMCOM</em>, <em>192</em>, 163–170. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking people’s flows has become crucial, not only for safety and security, but also for numerous practical business applications and better management of urban spaces, facilities and services. In this paper, we proposed methodologies that, exploiting IoT technology deployed at the edge of the network, allow for the analysis of people’s movement in urban environments, both outdoors and indoors. In particular, leveraging the use of WiFi probe packets sent by smart devices carried by people on the move, we first describe an implementation of our methodology using off-the-shelf hardware to count people boarding public transportation vehicles. We then present an alternate implementation using commercial WiFi scanners connected to the edge and leveraging suitably deployed virtual network functions to process the data collected by a OneM2M IoT platform, proposing also a mobility tracking procedure that can be applied to anonymized data provided by commercial WiFi scanners. Our experimental results show that the proposed approaches to people counting and mobility detection can achieve a good level of accuracy, while overall carrying a low price tag.},
  archive      = {J_COMCOM},
  author       = {Kalkidan Gebru and Marco Rapelli and Riccardo Rusca and Claudio Casetti and Carla Fabiana Chiasserini and Paolo Giaccone},
  doi          = {10.1016/j.comcom.2022.06.003},
  journal      = {Computer Communications},
  pages        = {163-170},
  shortjournal = {Comput. Commun.},
  title        = {Edge-based passive crowd monitoring through WiFi beacons},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A federated calibration scheme for convolutional neural
networks: Models, applications and challenges. <em>COMCOM</em>,
<em>192</em>, 144–162. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been created as a practical artificial intelligence strategy that takes various layers of information and gives the best in the effects of different classes. The use of deep learning has indicated exceptional execution in other regions, especially in image clustering, division, and recognition. The ongoing advanced learning strategies implement image clustering, which expects to recognize subsequent-level of classifications. This paper gives a definite audit of different deep arrangements and models featuring attributes of a specific convolutional neural network model. Initially, we depicted the working of Convolutional neural networks and their segments, followed by a point-by-point display of various Convolutional Neural Network models beginning with the old-style LeNet model to AlexNet, GoogleNet, VGGNet, ResNet , DenseNet, Xception, PNAS/ENAS, and EfficientNet. We concluded the significant challenges associated with Spatial Exploitation based Convolutional neural network architecture, Depth Based Convolutional neural network architecture, Multi-Path based Convolutional neural network architectures, and width based Convolutional neural network architectures. A definite summary of the review, including the frameworks, information base, application, and precision for every model, is discussed for serving it as the future scope in the above areas.},
  archive      = {J_COMCOM},
  author       = {Shivani Gaba and Ishan Budhiraja and Vimal Kumar and Sahil Garg and Georges Kaddoum and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.comcom.2022.05.035},
  journal      = {Computer Communications},
  pages        = {144-162},
  shortjournal = {Comput. Commun.},
  title        = {A federated calibration scheme for convolutional neural networks: Models, applications and challenges},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on IFIP networking 2019. <em>COMCOM</em>,
<em>192</em>, 143. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Alex Liu and Ali Munir and Jacek Rak and Steve Uhlig and Jordi Domingo-Pascual},
  doi          = {10.1016/j.comcom.2022.05.029},
  journal      = {Computer Communications},
  pages        = {143},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on IFIP networking 2019},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An integrated acoustic/LoRa system for transmission of
multimedia sensor data over an internet of underwater things.
<em>COMCOM</em>, <em>192</em>, 132–142. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, underwater research has been gaining momentum due to the plethora of emerging application scenarios that are appearing, typically associated to marine wildlife protection, national borders monitoring or underwater historical sites preservation. However, research in this area is still at infancy, and implementation of the Internet of Underwater Things vision is not yet at its adulthood. One of the most critical issues is indeed associated to the need for developing a network of devices which can connect and remotely deliver data to a front-end elaboration center. To this purpose, the need for increasing network lifetime and improving the quality of the monitored parameters, while guaranteeing real time control of the network, calls for the design of tools for remotisation, actuation and control. In this paper we report about the design and development of an integrated acoustic/LoRa system for transmission of multimedia sensor data over the Internet of Underwater Things. We detail the blocks composing the system and show results obtained through field tests that assess the possibility to transmit multimedia data, as well as control data, for real time reconfiguration of the system parameters through properly designed Android or Web applications.},
  archive      = {J_COMCOM},
  author       = {A.A. Brincat and F. Busacca and L. Galluccio and J.S. Mertens and A. Musumeci and S. Palazzo and A. Panebianco},
  doi          = {10.1016/j.comcom.2022.05.032},
  journal      = {Computer Communications},
  pages        = {132-142},
  shortjournal = {Comput. Commun.},
  title        = {An integrated acoustic/LoRa system for transmission of multimedia sensor data over an internet of underwater things},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal probing with statistical guarantees for network
monitoring at scale. <em>COMCOM</em>, <em>192</em>, 119–131. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring large-scale cloud networks is a complex task because their scale is prohibitively large, monitoring budgets are limited, network topologies are not regular and the estimates produced are a function of traffic patterns. In this work, we take a statistical approach to estimating a network metric, such as the latency of a set of paths, with guarantees on the estimation error. We aim to do so in an intelligent and scalable manner, without observing all existing traffic , and minimizing the estimation error at a fixed probing budget per unit of time. Our algorithms produce a distribution of probes/samples across network paths which can be used in conjunction with existing probers (or samplers). These algorithms are based on A- and E-optimal experimental designs in statistics, which guarantee a bounded estimation error for any monitoring budget. Unfortunately, these designs are too computationally intensive to be used in production at scale. We propose a scalable and near-optimal approximate implementations based on the Frank–Wolfe algorithm. We validate our approaches with two metrics (latency and loss) in simulations on real network topologies , and also using a production probing system in a real cloud network. We show major gains in reducing the probing budget compared to both production and academic baselines, while maintaining low errors in estimates, even with very low probing budgets.},
  archive      = {J_COMCOM},
  author       = {Branislav Kveton and Muhammad Jehangir Amjad and Christophe Diot and Dimitris Konomis and Augustin Soule and Xiaolong Yang},
  doi          = {10.1016/j.comcom.2022.05.023},
  journal      = {Computer Communications},
  pages        = {119-131},
  shortjournal = {Comput. Commun.},
  title        = {Optimal probing with statistical guarantees for network monitoring at scale},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On building human-aware opportunistic communication
strategies for cost-effective content delivery. <em>COMCOM</em>,
<em>192</em>, 106–118. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile networks face challenges such as data traffic growth, increasing connected devices, spectrum limitations, and costly infrastructure updates. In this context, opportunistic communication strategies appear to assist in scenarios such as data offloading, expanding the capillarity given by users’ mobility, delivering content in challenging situations (e.g., emergency, rural areas, or crowded places), and fostering innovative applications. State-of-art opportunistic strategies extract human mobility characteristics (e.g., social and spatial) to improve content delivery’s cost-effectiveness. Nevertheless, due to constraints, such as the limited availability of real-world datasets or the lack of a human-centered vision in networking, most previous works relied on traditional or simplified human-aware metrics to reach their purposes. Next-generation proposals need a more in-depth vision of peculiar aspects hidden into mobility datasets while working with more realistic scenarios. While building TOOTS, a novel human-aware opportunistic communication strategy for improved content delivery cost-effectiveness, we learned that several phases play a significant role in reaching strategy’s superior performance. This work guides the reader through the whole process necessary for building the Tactful Opportunistic Communication Strategy (TOOTS) . The proposal consists of the following steps: learning human-aspect best practices from state-of-art; characterizing the traces while obtaining strategy’s insights targeting superior performance; using, proposing, and analyzing human-aware metrics; combining such metrics and insights into a complete strategy for cost-effective content delivery in a more realistic scenario, and finally, evaluating the strategy through discussions of its best practices and shortcomings for future work. This work shows that TOOTS can improve the performance of an opportunistic scenario in terms of overhead, delivery rate, and delivery latency by following this proposed process.},
  archive      = {J_COMCOM},
  author       = {Rafael L. Costa and Aline C. Viana and Artur Ziviani and Leobino N. Sampaio},
  doi          = {10.1016/j.comcom.2022.05.033},
  journal      = {Computer Communications},
  pages        = {106-118},
  shortjournal = {Comput. Commun.},
  title        = {On building human-aware opportunistic communication strategies for cost-effective content delivery},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge gaming: A greening perspective. <em>COMCOM</em>,
<em>192</em>, 89–105. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of how to support gaming at the edge of the cellular network . The reduced latency and higher bandwidth that the edge enjoys with respect to cloud-based solutions implies that transferring cloud-based games to the edge could be a premium service for end-users. The goal of this work is to design a scheme compatible with MEC and network slicing principles of 5G and beyond, and which maximizes the utility of a service/infrastructure provider with time-varying edge node capacities due to the access to intermittent renewable energy. We formulate a multi -dimensional integer linear programming problem, proving that it is NP-hard in the strong sense. We prove that our problem is sub-modular and propose an efficient heuristic, GREENING , which considers the allocation of gaming sessions and their migration. For the mentioned scenario, we analyze a wide variety of realistic configurations at the edge, studying how the performance depends on (i) whether the games have a static or dynamic workload , (ii) the distribution of renewable energy through nodes and time, or (iii) the topology of the edge network. Through simulations, we show that our heuristic achieves performance close to that achieved by solving the NP-hard optimization problem , except with extremely lower complexity, and performs up to 25\% better than state-of-the-art algorithms.},
  archive      = {J_COMCOM},
  author       = {Francesco Spinelli and Antonio Bazco-Nogueras and Vincenzo Mancuso},
  doi          = {10.1016/j.comcom.2022.05.022},
  journal      = {Computer Communications},
  pages        = {89-105},
  shortjournal = {Comput. Commun.},
  title        = {Edge gaming: A greening perspective},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and implementation of a software tester for
benchmarking stateful NATxy gateways: Theory and practice of extending
siitperf for stateful tests. <em>COMCOM</em>, <em>192</em>, 75–88. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our siitperf is the world’s first RFC 8219 compliant free software SIIT (Stateless IP/ICMP Translation, also called stateless NAT64) benchmarking tool. It was written in C++ using DPDK (Intel Data Plane Development Kit). Our current effort aims to design and implement a test program for stateful NATxy gateways, including both stateful NAT64 and stateful NAT44 (also called NAPT: Network Address and Port Translation). Due to the object-oriented design of siitperf , it is feasible to extend it for stateful tests, while keeping its original design and features. In this paper, we introduce the problem of benchmarking stateful NATxy gateways and propose various solutions. We disclose the design and the most important implementation decisions of the stateful extension of siitperf . We prove the viability of our design and implementation by a functional NAT64 test and performing the maximum connection establishment rate, throughput, and frame loss rate measurements of the Jool stateful NAT64 implementation. We also carry out an initial performance estimation of the stateful extension of siitperf . Our tester is distributed as free software under the GPLv3 license for the benefit of the research, benchmarking and networking communities.},
  archive      = {J_COMCOM},
  author       = {Gábor Lencse},
  doi          = {10.1016/j.comcom.2022.05.028},
  journal      = {Computer Communications},
  pages        = {75-88},
  shortjournal = {Comput. Commun.},
  title        = {Design and implementation of a software tester for benchmarking stateful NATxy gateways: Theory and practice of extending siitperf for stateful tests},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A blockchain-based infection tracing and notification system
by non-fungible tokens. <em>COMCOM</em>, <em>192</em>, 66–74. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-CoV2 pandemic is heavily affecting our lives. Many actions have been undertaken to slow down its expansion and, among the others, contact tracing applications are the less invasive to monitor the spread of the virus. The idea behind contact tracing is to track contacts between people by the exchange of identifiers, not linked to individuals, exploiting the use of Bluetooth Low Energy (BLE) technology to estimate the duration and proximity of contacts. The data collected in this way is used for the sole purpose of notifying a potential contact with an infected person without revealing their identity and location. This paper presents a contact tracing protocol based on blockchain technology that exploits smart contracts for reporting contacts at risk of contagion. The novelty of the proposed solution is the use of Non Fungible Tokens (NFT) to guarantee user privacy through a decentralized approach, equipped with a reliable non-proprietary notification mechanism that allows public access to anonymous infections data.},
  archive      = {J_COMCOM},
  author       = {Alessio Ferone and Antonio Della Porta},
  doi          = {10.1016/j.comcom.2022.05.027},
  journal      = {Computer Communications},
  pages        = {66-74},
  shortjournal = {Comput. Commun.},
  title        = {A blockchain-based infection tracing and notification system by non-fungible tokens},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LoRa support for long-range real-time inter-cluster
communications over bluetooth low energy industrial networks.
<em>COMCOM</em>, <em>192</em>, 57–65. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial monitoring and control applications typically require real-time communications between a large number of nodes distributed over the plant. For the sake of network manageability and to reduce the overall network workload, wireless nodes are organized in clusters, which typically encompass neighboring nodes that frequently exchange data with each other. However, different clusters also cooperate to realize distributed applications and this raises the need for enabling communications between multiple clusters spread over large areas in the plant. This paper presents LoRaBLE, a long-range communication protocol that leverages the Long Range (LoRa) technology to provide inter-cluster communications over Bluetooth Low Energy networks with bounded delays, so as to meet the time constraints of real-time industrial traffic flows. The paper presents the design of LoRaBLE and a proof-of-concept implementation on a lab testbed made up of commercial-off-the-shelf devices.},
  archive      = {J_COMCOM},
  author       = {Luca Leonardi and Lucia Lo Bello and Gaetano Patti},
  doi          = {10.1016/j.comcom.2022.05.026},
  journal      = {Computer Communications},
  pages        = {57-65},
  shortjournal = {Comput. Commun.},
  title        = {LoRa support for long-range real-time inter-cluster communications over bluetooth low energy industrial networks},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel secured multi-access edge computing based VANET with
neuro fuzzy systems based blockchain framework. <em>COMCOM</em>,
<em>192</em>, 48–56. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicle ad-hoc networks, the progression of wireless communication technology to 6G, overcomes storage, processing, privacy, and power limits to create an efficient and intelligent next generation transportation system. Vehicular ad hoc network may now offer remarkable availability, reliability, and throughput using 6G technology. However, the VANET system’s data should be protected. This paper proposes an effective batch authentication and key exchange technique to avoid contact with hostile vehicle users. Moreover, three types of systems are proposed: PKI, ID-based, and MAC-based. The neuro-fuzzy inference technique was used to predict VANET security ratings. The Homogeneous Discrete-Time Markov Chain model is used to secure data transit. Additionally, this research examined the work from a blockchain perspective combined with MEC . There are 3 level to comprise the architecture: perception, edge computing , and services. Throughout the blockchain transmission process, the first layer make certain the security of VANET data. The perception layer makes use of edge computing and cloud services on the edge. The service layer protects data by using both traditional cloud storage and blockchain technology. The lowest layer of the system architecture is dedicated to the throughput and quality of service requirements of MEC users. The primary challenge is achieving consensus across blockchain nodes while maintaining the MEC system’s and blockchain’s performance. To simulate the joint optimization problem , a Markov decision process with reward function is utilized. The simulation results are conferred to illustrate the validity of study assertions.},
  archive      = {J_COMCOM},
  author       = {Poongodi M. and Sami Bourouis and Ahmed Najat Ahmed and Vijayaragavan M. and Venkatesan K.G.S. and Wajdi Alhakami and Mounir Hamdi},
  doi          = {10.1016/j.comcom.2022.05.014},
  journal      = {Computer Communications},
  pages        = {48-56},
  shortjournal = {Comput. Commun.},
  title        = {A novel secured multi-access edge computing based VANET with neuro fuzzy systems based blockchain framework},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Keyword search over encrypted cloud data based on blockchain
in smart medical applications. <em>COMCOM</em>, <em>192</em>, 33–47. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In consideration of the inconvenience of sharing user identity authorization, and the lack of correctness verification for searching electronic medical records in hospitals under the many-to-many search model, we propose an blockchain-aided searchable attribute-based encryption scheme that supports verification, we apply the scheme to electronic health record systems. Using blockchain technology, the ciphertext document is stored in the medical cloud, the secure index is stored in the blockchain , and the smart contract is used for searching, which reduces the computational burden of the medical cloud. In addition, this scheme can verify whether the results returned by the cloud server are correct, ensure data integrity and confidentiality. The scheme eliminates duplicate data and reduces the storage space of the medical cloud. Furthermore, users’ privacy security is guaranteed due to the hiding of access policy. The security analysis shows that the proposed scheme satisfies the semantic security of adaptively chosen-keyword attacks, can effectively protect the confidentiality of medical data and anonymity of users. Performance analysis and experiments demonstrate that the proposed scheme achieves optimizations in security index generation, trapdoor generation, search efficiency and result verification; hence, it is more suitable for many-to-many search scenarios such as smart medical applications. It effectively realizes the sharing of patient electronic medical records between doctors and third-party users without infringing on patient privacy.},
  archive      = {J_COMCOM},
  author       = {Shufen Niu and Mi Song and Lizhi Fang and Fei Yu and Song Han and Caifen Wang},
  doi          = {10.1016/j.comcom.2022.05.018},
  journal      = {Computer Communications},
  pages        = {33-47},
  shortjournal = {Comput. Commun.},
  title        = {Keyword search over encrypted cloud data based on blockchain in smart medical applications},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Future generation communications with game strategies: A
comprehensive survey. <em>COMCOM</em>, <em>192</em>, 1–32. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an effective, low latency, and ultra reliable pervasive connectivity among next-generation wireless networks such as Internet of Things (IoT) devices, machine-to-machine (M2M) communication, and wireless sensor network (WSN), the users play intelligent strategies which enable them to take crucial decisions in order to obtain the optimal solutions. Game theory, a mathematical tool helps in solving various problems of wireless communication related to security, resource allocation, power management, energy harvesting , spectrum usage , coverage, connectivity, capacity, reliability, efficiency, optimum bandwidth, rewards and punishments of wireless nodes , and balancing of various trade-offs. This paper presents a comprehensive review, potential benefits of applying game theory (GT) in wireless communication (WC). For this purpose, a detailed overview of GT including cooperative and non-cooperative games, Q-learning, and reinforcement learning for different applications like cellular communication , multiple-input-multiple-output (MIMO), unmanned aerial vehicle (UAV), vehicle-to-vehicle (V2V) communication, cognitive radio (CR), device-to-device (D2D), wireless sensor networks (WSN) and many other applications that are pertinent to wireless networks is presented. In addition, various important design and optimization challenges are addressed. Two GT-based case studies related to physical layer security and resource allocation are also presented. In a nutshell, GT models enhanced by various learning algorithms has the potential to optimize the configuration parameters of any wireless network. Finally, we reflect the future directions and the challenges based on GT to improvise the performance of the wireless systems in the 5G technology and beyond.},
  archive      = {J_COMCOM},
  author       = {Rajni Gupta and Juhi Gupta},
  doi          = {10.1016/j.comcom.2022.05.024},
  journal      = {Computer Communications},
  pages        = {1-32},
  shortjournal = {Comput. Commun.},
  title        = {Future generation communications with game strategies: A comprehensive survey},
  volume       = {192},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of vehicular visible light
communication based on angle-oriented receiver. <em>COMCOM</em>,
<em>191</em>, 500–509. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible Light Communication (VLC) has emerged as a promising technology to complement radio frequency-based vehicular communications . Initial studies in Vehicle-to-Vehicle (V2V) VLC systems assumed that two vehicles follow each other with perfect alignment. Such idealistic assumption is not always maintained during traveling along the road. The lateral shift between the vehicles might strongly impact the system performance. In addition, the effect of the transceiver and system parameters on the performance of V2V-VLC systems should be taken into account. In this paper, we fill this research gap by investigating the performance of V2V-VLC systems under the impact of the lateral shift between the vehicles and transceiver parameters. Then, we introduce the use of the angle-oriented receiver (AOR) in V2V-VLC systems to enhance the system performance in terms of achievable capacity, maximum achievable distance, and packet delivery ratio (PDR). The AOR consists of multiple receiving elements oriented in different directions. We further investigate the impact of the number of AOR elements, both the field-of-view (FoV) and the aperture diameter of each receiving element, and the bandwidth on the system performance. Our results demonstrate that with a carefully chosen system and AOR parameters, a higher system capacity of up to 61 Mb/s is achieved at a communication distance of 50 m.},
  archive      = {J_COMCOM},
  author       = {Selma Yahia and Yassine Meraihi and Amar Ramdane-Cherif and Asma Benmessaoud Gabis and Hossien B. Eldeeb},
  doi          = {10.1016/j.comcom.2022.05.025},
  journal      = {Computer Communications},
  pages        = {500-509},
  shortjournal = {Comput. Commun.},
  title        = {Performance evaluation of vehicular visible light communication based on angle-oriented receiver},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of cybersecurity spoofing attacks in vehicular
networks with recurrence quantification analysis. <em>COMCOM</em>,
<em>191</em>, 486–499. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity aspects in modern automotive vehicles are becoming increasingly important due to the recent demonstration of successful cybersecurity attacks. Intrusion Detection System (IDS) is one of the approaches proposed in the research literature to detect such attacks. This paper proposes a novel IDS approach based on the application of Recurrence Quantification Analysis (RQA), in combination with a sliding window, to the information of the CAN-bus message arrival time, which has the benefit of not requiring the processing of the arbitration field or the payload data of the CAN-bus message. The rationale for the application of RQA to this problem is to consider the in-vehicle network as a dynamic system where the CAN-bus message time is used as an observable. This approach is evaluated with various machine learning algorithms on two public data sets recently published by the research community with a focus on spoofing attacks since they are the most difficult to detect. The proposed approach is compared with the application of entropy measures for attack detection, which are commonly adopted in the literature and with the results from the literature on the same data sets. The results show that the RQA based approach provides a better detection accuracy than entropy measures in a consistent way across different sliding window sizes in both data sets and it is competitive against other approaches in the literature. This paper provides also an extensive evaluation of the impact on detection accuracy of the sliding window size and the hyper-parameters present in the definition of RQA and the machine learning algorithms.},
  archive      = {J_COMCOM},
  author       = {Gianmarco Baldini},
  doi          = {10.1016/j.comcom.2022.05.021},
  journal      = {Computer Communications},
  pages        = {486-499},
  shortjournal = {Comput. Commun.},
  title        = {Detection of cybersecurity spoofing attacks in vehicular networks with recurrence quantification analysis},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid metaheuristic technique for optimal container
resource allocation in cloud. <em>COMCOM</em>, <em>191</em>, 477–485.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource allocation in cloud is becoming more challenging and complex due to the intensifying requirements of cloud services. Efficient management of virtual resources in the cloud is of enormous importance, as it has a large impact on both the operational cost and scalability of the cloud’s surroundings. However, one of the most difficult aspects of cloud computing is optimal resource allocation. The resource allocation is done using the objective of reducing the costs connected with it. Hence, an attempt is made to optimize the resource allocation of containers using a new Combined Spider and Honey Bee Optimization (CS-HBO). In the proposed model, the optimal allocation relies on certain constraints like “Total Network Distance (TND), System Failure, Balanced Cluster Use, and threshold distance”. At last, the supremacy of the presented approach is examined over prevailing techniques regarding various metrics such as cost and convergence analysis . Accordingly, for cost analysis, the proposed method attains the least cost, which is 65.57\%, 4.15\%, 1.04\%, 6.87\% and 6.76\% improved than existing Marriage in Honey Bee optimization (MHBO), Spider Monkey Optimization (SMO), Whale Random update assisted Lion Algorithm (WR-LA), Genetic Algorithms (GA), and Accelerated particle swarm optimization (APSO) models at the 100th iteration.},
  archive      = {J_COMCOM},
  author       = {Majid Alotaibi},
  doi          = {10.1016/j.comcom.2022.04.012},
  journal      = {Computer Communications},
  pages        = {477-485},
  shortjournal = {Comput. Commun.},
  title        = {Hybrid metaheuristic technique for optimal container resource allocation in cloud},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Host identification based on self-similarity of network
activity. <em>COMCOM</em>, <em>191</em>, 467–476. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The randomness and variability of IP addresses challenge the identity uniqueness of internet hosts. Accurately identifying internet hosts on the premise of protecting users’ privacy is difficult. In this paper, we demonstrate that the network behaviour characteristics of internet hosts often have self-similar characteristics, and propose a new method for host identification based on network activity self-similarity (NASS). In this method, the multidimensional behaviour features of internet hosts are collected from network traffic, and the time series of behaviour features are constructed. Then, after noise reduction, Mahalanobis distance is applied to measure the distance between the time series of different time windows of any two hosts. The distance measurement results are ranked, and host identification is realized according to the ranking. NASS can accurately identify the network host without violating user privacy and is suitable for encrypted communication environments. The experimental results show that the accuracy of NASS is 83.67\%.},
  archive      = {J_COMCOM},
  author       = {Lisheng Huang and Guanling Zhao and Lu Li and Fengjun Zhang},
  doi          = {10.1016/j.comcom.2022.05.017},
  journal      = {Computer Communications},
  pages        = {467-476},
  shortjournal = {Comput. Commun.},
  title        = {Host identification based on self-similarity of network activity},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient resource allocation in 5G URLLC with packet
duplication based macro-diversity. <em>COMCOM</em>, <em>191</em>,
459–466. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packet duplication based macro-diversity is a potential solution to achieve high reliability and low latency, for ultra-reliable low-latency communication (URLLC) applications, like mission critical internet of things (IoT), in 5G cellular systems . Increased radio resource usage is the price paid for the benefits accrued. This paper proposes a framework to find the relaxed per link reliability targets and to perform radio resource and power allocation over the links, which can guarantee the user equipment (UE) reliability target and also provide spectrum efficient resource usage, when multiple base stations (BSs) serve each UE. The resource allocation problem that involves BS selection, packet length adaptation, and power allocation , is formulated to minimize the overall resource usage within the constraints of reliability, latency, resource availability, and transmit power. Inherent system dynamics that exist among packet length, decoding error probability, and transmit power are used in devising a lightweight heuristic solution . The two important findings are that: (i) with each UE, a stringent reliability target for the best link and relaxed reliability targets for the other links can provide a spectrum efficient solution; (ii) additional resource savings can be achieved by allocating more power to the UEs with poor channel conditions, at each BS. The proposed method can achieve up to 20\% resource saving when compared to the blind duplication with equal power allocation .},
  archive      = {J_COMCOM},
  author       = {Jihas Khan and Lillykutty Jacob},
  doi          = {10.1016/j.comcom.2022.05.020},
  journal      = {Computer Communications},
  pages        = {459-466},
  shortjournal = {Comput. Commun.},
  title        = {Efficient resource allocation in 5G URLLC with packet duplication based macro-diversity},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). PWAF: Path weight aggregation feature for link prediction
in dynamic networks. <em>COMCOM</em>, <em>191</em>, 438–458. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of online social networks is obvious these days, and it provides researchers with an opportunity to find solutions for a variety of practical applications. The approach of recognizing network structure and finding missing and future links in a social network is known as link prediction. On time-varying or dynamic networks, the link prediction problem has two major challenges: accuracy and efficiency. The improvement of accuracy in link prediction problem in dynamic networks is the objective of this research. The goal of this paper is to propose the Path Weight Aggregation Feature ( P W A F PWAF ), which is a new feature based on ranking multi edge occurrences across the entire network. Different topological aspects of the networks ( L o c a l Local , G l o b a l Global , and Q u a s i − l o c a l Quasi−local ) as well as Clustering Coefficient based features are taken into consideration for feature generation, in addition to the suggested Path Weight-Based Aggregation Feature ( P W A F PWAF ). One of the features used for better prediction is the L e v e l − 2 Level−2 node clustering coefficient ( C C L P 2 CCLP2 ). Different machine learning models, such as Neural Network ( N N NN ), Logistic Regression ( L R LR ), XGBoost ( X G B XGB ), Random Forest Classifier ( R F C RFC ), and linear Discriminant Analysis ( L D A LDA ), are evaluated and verified for link prediction. The experiments are carried out on seven different well-known dynamic networks data sets in terms of five performance evaluation metrics , including AUPR, F1-score, AVG PRECISION, BAL ACC SCORE, and AUC, and the results show that our proposed method and its variants outperform state-of-the-art methods. The results demonstrate that P W A F − R F C PWAF−RFC is the best performing variation out of all machine learning classifiers we have experimented with.},
  archive      = {J_COMCOM},
  author       = {Mukesh Kumar and Shivansh Mishra and Bhaskar Biswas},
  doi          = {10.1016/j.comcom.2022.05.019},
  journal      = {Computer Communications},
  pages        = {438-458},
  shortjournal = {Comput. Commun.},
  title        = {PWAF: Path weight aggregation feature for link prediction in dynamic networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A secured and lightweight RFID-tag based authentication
protocol with privacy-preserving in telecare medicine information
system. <em>COMCOM</em>, <em>191</em>, 425–437. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a result of remarkable and continuous progress in Micro-Electro-Mechanical technologies, improved user-friendliness of low cast telecommunications like IoTs, mobile networks virtualization and intelligent technologies, we have novel healthcare delivery services with non-connection identification electronic components such as Radio Frequency Identification (RFID). RFID is extensively utilized in medical-oriented services like patient monitoring, object traceability, drug administration system, and Telecare medicine information system (TMIS), which receives special attention from healthcare management. TMIS works as a suspension bridge among patients staying at home, and doctors at healthcare organizations are authorized to validate the preciseness of exchanged data records among dissimilar organization participants. In recent times, TMIS adopted for patient data confidentiality , privacy protection, authentication , security, and safety over public channels. Researchers proposed a wide range of authentication protocols for RFID and TMIS by exploiting different cryptographic primitive solutions. However, most of them have large complex operations, scarce storage, and are not efficient. Whatever the security of TMIS improves, its application becomes wider. This paper designed an efficient and improved authentication protocol with high security for TMIS. The proposed protocol performance analysis in both Scyther and AVISPA tools shows that the protocol achieves high security and is well-suited for TMIS.},
  archive      = {J_COMCOM},
  author       = {Bhanu Chander and Kumaravelan Gopalakrishnan},
  doi          = {10.1016/j.comcom.2022.05.002},
  journal      = {Computer Communications},
  pages        = {425-437},
  shortjournal = {Comput. Commun.},
  title        = {A secured and lightweight RFID-tag based authentication protocol with privacy-preserving in telecare medicine information system},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recent trends in clustering algorithms for wireless sensor
networks: A comprehensive review. <em>COMCOM</em>, <em>191</em>,
395–424. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past two decades, network clustering has been proven as efficient approach for data collection and routing in wireless sensor networks (WSNs). It provides several advantages over other methods in terms of energy efficiency, scalability, even energy distribution, etc. Given the limited capabilities of sensor nodes energy resources, processing power, and communication range, cluster-based protocols accommodate the network’s operation with these constraints. Several survey papers present and compare many clustering algorithms from various perspectives. However, most of these surveys either are outdated or have limited scope. This paper provides a comprehensive review of clustering algorithms where the new ideas and concepts proposed in each phase of the clustering process are extensively studied. Three topics are discussed in this review. First, we present the objectives, characteristics and challenges of clustering algorithms. Second, the cluster-head selection methods for different types of WSNs are extensively studied. Third, this review presents a detailed description of newly proposed methods to handle energy heterogeneity, energy harvesting , fault-tolerance, scalability, mobility and data correlation in WSNs. Furthermore, the protocols taxonomy in each phase is discussed to provide a deeper understanding of current clustering approaches . Finally, a set of criteria is presented to simplify the comparison and identify each protocol’s pros and cons. This review presents a comprehensive introduction and can be a useful guidance for new researchers in this field. Also, it will help system designers to identify alternative solutions for selecting an appropriate method in each phase of the clustering process .},
  archive      = {J_COMCOM},
  author       = {Adnan Ismail Al-Sulaifanie and Bayez Khorsheed Al-Sulaifanie and Subir Biswas},
  doi          = {10.1016/j.comcom.2022.05.006},
  journal      = {Computer Communications},
  pages        = {395-424},
  shortjournal = {Comput. Commun.},
  title        = {Recent trends in clustering algorithms for wireless sensor networks: A comprehensive review},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure and intelligent slice resource allocation in
vehicles-assisted cyber physical systems. <em>COMCOM</em>, <em>191</em>,
386–394. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber physical systems (CPSs) are multi-dimensional complex systems integrated with computing, networking and physical modules, enabling to realize complex computing and communication tasks. Vehicles are composed of many electrical and electronic hardware modules. With integrating mobile vehicles, CPSs can be extended to realize more remote tasks. Network slicing (NS) is recognized as the key enabler towards the agile enabler of vehicles-assisted CPSs. In NS, one crucial technical issue is the slice resource allocation. Intelligent approaches, such as deep learning (DP), emerge and can be adopted to allocate slice resource allocation. In addition, due to the heterogeneity nature of physical elements in CPSs, security is strongly required to be addressed. On these basis, secure and intelligent slice resource allocation in vehicles-assisted CPSs is researched in this paper. At first, system models of vehicles-assisted CPSs with NS scheme and slices are described. Then, the intelligent resource allocation framework, abbreviated as Sec-Slice-Intell , is proposed and described. The goal of Sec-Slice-Intell framework is to achieve secure resource allocation per slice for NS-enabled CPSs. In order to validate the merits of the proposed framework, evaluation work is conducted and illustrated, in the form of simulation.},
  archive      = {J_COMCOM},
  author       = {Haotong Cao and Sahil Garg and Georges Kaddoum and Mohammad Mehedi Hassan and Salman A. AlQahtani},
  doi          = {10.1016/j.comcom.2022.05.015},
  journal      = {Computer Communications},
  pages        = {386-394},
  shortjournal = {Comput. Commun.},
  title        = {Secure and intelligent slice resource allocation in vehicles-assisted cyber physical systems},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPMBot: Discovering the frequent pattern of IoT-botnet
domain queries in large-scale network. <em>COMCOM</em>, <em>191</em>,
378–385. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet of Things (IoT), new types of security issues has emerged, and one of the most severe one is the IoT-based botnet . A number of traditional works dedicate in analyzing DNS traffic for its wide misuse in botnets. However, most works are limited to specific behaviors, such as regularity in domains queries and periodicity in C&amp;C(Command and Control) processes, which are often sheltered by attackers. Moreover, ever-growing numbers of domains and queries caused by IoT devices make most approaches not effective anymore since their architectures are hard to confront traffic explosion. In this paper, we illustrate the essential property of botnet , i.e., the frequent pattern of DNS request relationships, and propose a generic and scalable IoT-botnet detection system, named FPMBot, to detect bots domain queries in large-scale DNS traffic. The key insight is that bots in the same botnet inevitably query same sets of domains or servers whenever they try to conduct attacks or connect to C&amp;C servers, and form frequent patterns in a bipartite graph of requesters and responsers. This frequent pattern in domain queries is an essential behavior for bots since a significant advantage of bots is the ability to launch large-scale attacks synchronously. We utilize a frequent pattern mining algorithm to detect such patterns and implement FPMBot based on Apache Spark parallel computing architecture to handle the daily increasing traffic. The experiment result on more than 14 billion records in four days real-world DNS logs shows that FPMBot has a high detection precision over 95\% and performs well in large-scale network.},
  archive      = {J_COMCOM},
  author       = {Kexiang Qian and Yixin Li and Muyijie Zhu and Lihua Yin and Bin Wang and Xi Luo},
  doi          = {10.1016/j.comcom.2022.05.012},
  journal      = {Computer Communications},
  pages        = {378-385},
  shortjournal = {Comput. Commun.},
  title        = {FPMBot: Discovering the frequent pattern of IoT-botnet domain queries in large-scale network},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SecDH: Security of COVID-19 images based on data hiding with
PCA. <em>COMCOM</em>, <em>191</em>, 368–377. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, image security and copyright protection become challenging, especially after the COVID-19 pandemic. In the paper, we develop SecDH as a medical data hiding scheme , which can guarantee the security and copyright protection of the COVID-19 images. Firstly, the cover image is normalized, which offers high resistance against the geometric attacks . Secondly, the normalized principal component as embedding factor is computed, which are calculated based on principal component analysis (PCA) between cover and mark image. Thirdly, the medical image is invisibly marked with secret mark based on normalized component, redundant discrete wavelet transform (RDWT) and randomized singular value decomposition (RSVD) is introduced. Finally, Arnold cat map scheme employed to ensure the security of the watermarking system . Under the experimental evaluation, our SecDH tool is not only imperceptible, but also has a satisfactory advantage in robustness and security compared with the traditional watermarking schemes .},
  archive      = {J_COMCOM},
  author       = {O.P. Singh and Amit Kumar Singh and Amrit Kumar Agrawal and Huiyu Zhou},
  doi          = {10.1016/j.comcom.2022.05.010},
  journal      = {Computer Communications},
  pages        = {368-377},
  shortjournal = {Comput. Commun.},
  title        = {SecDH: Security of COVID-19 images based on data hiding with PCA},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cellular automata trust-based energy drainage attack
detection and prevention in wireless sensor networks. <em>COMCOM</em>,
<em>191</em>, 360–367. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) connect many tiny, self-configured, and limited power sensor nodes . The primary purposes of these nodes are to sense information from the environment or system, process this information, and send it to the base station . In recent research, communities have drawn significant attention to Fifth Generation (5G) based WSNs for real-time communications over 5G . These networks are deployed and operated in hard-to-reach terrains to collect data. During such operations, WSNs face resource limitations, battery power, and inadequate resources for computation. The tremendous demands of 5G in WSNs make energy conservation a significant concern. An energy-efficient routing protocol such as Low Energy Adaptive Clustering Hierarchy (LEACH) that can extend the lifetime of these networks is prevalent. There is a dire need to analyze the efficiency of the LEACH protocol. In this paper, we analyze LEACH protocol under energy-drainage attacks, Gray-Hole, and scheduling. Subsequently, We propose a Trust-based Cellular Automata energy drainage detection and prevention scheme (CAT-EDP) to detect and then minimize the effects of these attacks. The simulation results demonstrate that the proposed technique uses cellular automata to detect and prevent energy drainage attacks on LEACH protocol. Moreover, the proposed technique has optimized the network lifetime up to 11\%.},
  archive      = {J_COMCOM},
  author       = {Jahanzeb Shahid and Zia Muhammad and Zafar Iqbal and Ahmad S. Almadhor and Abdul Rehman Javed},
  doi          = {10.1016/j.comcom.2022.05.011},
  journal      = {Computer Communications},
  pages        = {360-367},
  shortjournal = {Comput. Commun.},
  title        = {Cellular automata trust-based energy drainage attack detection and prevention in wireless sensor networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved DECPSOHDV-hop algorithm for node location of WSN
in cyber–physical–social-system. <em>COMCOM</em>, <em>191</em>, 349–359.
(<a href="https://doi.org/10.1016/j.comcom.2022.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-Physical Social System (CPSS) refers to the interaction between cyber-physical systems and social networks. The accuracy of information transmission in Cyber-Physical System is very important, and the information transmission of WSN node positioning occupies an important position. Aiming at the ubiquitous positioning accuracy and resource allocation problems in the node positioning, we propose an improved Chaotic Particle Swarm Optimization Hybrid Distance Vector Hopping Algorithm Based on Differential Evolution (DECPSOHDV-Hop) algorithm. This algorithm uses an improved algorithm parallel mechanism in the WSN node positioning, which can be more accurately positioned unknown node through dynamic optimization. In addition, when solving high-dimensional function optimization problems , particle swarm optimization (PSO) algorithms tend to converge prematurely and fall into local extremes. Therefore, based on the (DE)/current-to-best/1 operator, this paper proposes chaotic PSO algorithms of hybrid and parallel. For particle optimization in the difference operator, Gaussian/normal distribution is used for balance optimization. The hybrid method selects the basic particle swarm update strategy according to the probability, simplified the particle swarm update strategy or differential optimization to update the individual. The parallel method uses the improved model to update the Gaussian mutation operator of DE to update the entire population. In the simulation experiment results, the results of testing 20 high-dimensional benchmark functions show that compared with PSO and CPSO, the above two improved algorithms have better search accuracy and higher convergence speed, as well as the optimization results of the test function; In WSN node positioning, the positioning accuracy of the DECPSOHDV-Hop algorithm is as high as 90\%, and the excellent positioning stability of the algorithm is verified from three different standards.},
  archive      = {J_COMCOM},
  author       = {Tan Deng and Xiaoyong Tang and Zhiqiang Wu and Xiao Liu and Wei Wei and Zeng Zeng},
  doi          = {10.1016/j.comcom.2022.05.008},
  journal      = {Computer Communications},
  pages        = {349-359},
  shortjournal = {Comput. Commun.},
  title        = {An improved DECPSOHDV-hop algorithm for node location of WSN in Cyber–Physical–Social-system},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated identification of network anomalies and their
causes with interpretable machine learning: The CIAN methodology and
TTrees implementation. <em>COMCOM</em>, <em>191</em>, 327–348. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging machine learning (ML) for the detection of network problems dates back to handling call-dropping issues in telephony. However, troubleshooting cellular networks is still a manual task, assigned to experts who monitor the network around the clock. To help in this task we present CIAN (from Causality Inference of Anomalies in Networks), a practical and interpretable ML methodology, which we implement in the form of a software tool named TTrees (from Troubleshooting Trees). We have designed CIAN to automate the identification of the causes of performance anomalies in cellular networks . Our methodology is unsupervised and combines multiple ML algorithms (e.g., decision trees and clustering) and Kolmogorov complexity-inspired data analysis tools that we have developed for this work. CIAN can be used with small volumes of data and is quick at training. Our experiments use diverse data sets obtained from measurements in operational commercial mobile networks. They show that the TTrees implementation of CIAN can automatically identify and accurately classify network anomalies – e.g., cases for which a network low performance is not apparently justified by operational conditions – training with just a few hundreds of data samples. The resulting information hence enables precise troubleshooting actions. In particular, we showcase how TTrees can be flexibly used to monitor the performance of TCP and QUIC protocols when they are adopted to serve mobile users.},
  archive      = {J_COMCOM},
  author       = {Mohamed Moulay and Rafael Garcia Leiva and Pablo J. Rojo Maroni and Fernando Diez and Vincenzo Mancuso and Antonio Fernández Anta},
  doi          = {10.1016/j.comcom.2022.05.013},
  journal      = {Computer Communications},
  pages        = {327-348},
  shortjournal = {Comput. Commun.},
  title        = {Automated identification of network anomalies and their causes with interpretable machine learning: The CIAN methodology and TTrees implementation},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A parameter optimization method in predicting algorithms for
smart living. <em>COMCOM</em>, <em>191</em>, 315–326. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of digital networks and communication, our life has become more intelligent and convenient. Automation systems and services are more and more popular with our daily life. To make full use of the data collected from smart living for accurate prediction, hybrid algorithms are proposed to optimize the parameters in machine learning . In this paper, multiple linear regression with a Kalman filter is proposed to forecast continuous variables, and support vector machine with a Kalman filter is proposed to forecast discrete variables. To present the effectiveness and accuracy of the proposed hybrid algorithms with less training data, we conduct experiments of predicting auction results based on two real datasets downloaded from a smarting system. The experimental results show that the proposed algorithms could improve prediction accuracy and reduce the error rates by calibration metrics.},
  archive      = {J_COMCOM},
  author       = {Xiaohui Li and Hongbin Dong and Xiaodong Yu},
  doi          = {10.1016/j.comcom.2022.05.007},
  journal      = {Computer Communications},
  pages        = {315-326},
  shortjournal = {Comput. Commun.},
  title        = {A parameter optimization method in predicting algorithms for smart living},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A QoS-aware and channel-aware radio resource management
framework for multi-numerology systems. <em>COMCOM</em>, <em>191</em>,
299–314. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the diversified 5G usage scenarios , the 5G New Radio (NR) physical layer is enriched with the flexible multi-numerology feature, where multiple frame structures with different sub-carrier spacings coexist in one frequency band. However, this flexibility brings with it a further challenge in Radio Resource Management (RRM), how to best allocate the available band spectrum among the different non-orthogonal numerologies. This work presents a novel Quality of Service (QoS)-aware and channel-aware RRM framework that manages diversified type of services, i.e., Best Effort (BE) services and Guaranteed Bit Rate (GBR) services with different priorities. The goal is to maximize the number of satisfied GBR services, according to the priority, and the throughput of the BE services. To provide a feasible solution, the proposed framework consists of two control levels. The first one appropriately splits the available bandwidth among the various numerologies, with the aim of achieving the above goal, trying to assign continuous portion of band to the same Numerology, in order to mitigate the Inter-Numerology Interference (INI) phenomenon. Moreover, it includes an advanced dropping strategy to support overload conditions. The second level distributes the Physical Resource Blocks (PRBs) to the User Equipments (UEs) belonging to the same numerology, by exploiting scheduling algorithms available in literature. Through an extensive simulation campaign, under different traffic load and channel conditions, we compare the proposed RRM framework with other reference schemes available in literature. The results show that the proposed framework outperforms them in any simulation setup.},
  archive      = {J_COMCOM},
  author       = {Luciano Miuccio and Daniela Panno and Pietro Pisacane and Salvatore Riolo},
  doi          = {10.1016/j.comcom.2022.05.009},
  journal      = {Computer Communications},
  pages        = {299-314},
  shortjournal = {Comput. Commun.},
  title        = {A QoS-aware and channel-aware radio resource management framework for multi-numerology systems},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NOMA-inspired coexistence enabling method in cell-free
massive MIMO using opportunistic interference alignment.
<em>COMCOM</em>, <em>191</em>, 285–298. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel scheme referred to as non-orthogonal multiple access (NOMA)-inspired coexistence enabling (NICE) is introduced for the downlink of cell-free massive MIMO (CF-mMIMO) networks. Motivated by the cognitive concept in NOMA, the NICE strategy is designed to accommodate secondary users (SUs) within the CF-mMIMO. Despite of existing works in the CF-mMIMO, this paper considers the same transmitters for both primary and secondary users. Besides, the promising opportunistic interference alignment (OIA) strategy is exploited for the first time in CF-mMIMO networks. However, CF-mMIMO deals with a large number of users and access points (APs) which may enlarge the minimum number of transmit/receive antennas. The proposed NICE scheme resolves this problem such that a large outer bound on the degrees of freedom for SUs is achieved while a small number of transmit/receive antennas is required. The effectiveness of the proposed strategy is demonstrated through simulations for both perfect and imperfect channel models.},
  archive      = {J_COMCOM},
  author       = {Zeynab Khodkar and Jamshid Abouei},
  doi          = {10.1016/j.comcom.2022.05.016},
  journal      = {Computer Communications},
  pages        = {285-298},
  shortjournal = {Comput. Commun.},
  title        = {NOMA-inspired coexistence enabling method in cell-free massive MIMO using opportunistic interference alignment},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient deep-predictive airborne base station
selection and power allocation for UAV-assisted wireless networks.
<em>COMCOM</em>, <em>191</em>, 274–284. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless networks with unmanned aerial vehicles (UAVs) as the airborne base station (ABS) become a promising technology to enhance terrestrial users’ coverage area. Moving at high speed over the wireless network covered by UAV requires an effective ABS selection scheme during the handover process to maintain the quality of service (QoS). On the other hand, the network load must be distributed fairly between the ABSs to prevent overloading at some base stations. Due to the limited power of ABSs, power saving is an essential task. Energy efficiency is a measure that balances data rate and power consumption . This study proposes a deep-predictive target ABS selection, resource block (RB), and power allocation algorithm that jointly maximizes energy efficiency and load balancing. The proposed method constructs the neighbor lists based on the reference signal received power (RSRP), signal to interference plus noise ratio (SINR), and the sojourn time estimated by the deep neural network model to reduce the unnecessary handovers . The simulation results show that the proposed method reduces the ping-pong rate and the outage probability and increases load balancing, spectral, and energy efficiencies compared to previous works.},
  archive      = {J_COMCOM},
  author       = {Parinaz Dastranj and Vahid Solouk and Hashem Kalbkhani},
  doi          = {10.1016/j.comcom.2022.05.001},
  journal      = {Computer Communications},
  pages        = {274-284},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient deep-predictive airborne base station selection and power allocation for UAV-assisted wireless networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoPaM: Cost-aware VM placement and migration for mobile
services in multi-cloudlet environment: An SDN-based approach.
<em>COMCOM</em>, <em>191</em>, 257–273. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Cloud Computing (ECC) is a new approach for bringing Mobile Cloud Computing (MCC) services closer to mobile users in order to facilitate the complicated application execution on resource-constrained mobile devices . The main objective of the ECC solution with the cloudlet approach is mitigating the latency and augmenting the available bandwidth. This is basically done by deploying servers (a.k.a. “Cloudlets”) close to the user’s device on the edge of the cellular network . When considering the users’ mobility along with the limited resource of the cloudlets serving them, the user-cloudlet communication may need to go through multiple hops, which may seriously affect the communication delay between them and the quality of services (QoS). To reduce as much as possible the negative effects of such a QoS degradation, service execution must be dynamically migrated to a better placement. This study proposes a novel Cost-aware Virtual Machine (VM) placement and migration (CoPaM) framework for mobile services in a network of cloudlets. A Mixed Integer Linear Programming (MILP) model is proposed to select the least cost cloudlets for serving mobile users to a given trajectory in advance based on path prediction methods. Since the proposed model is NP-hard, it is not applicable within large-scale environments with a centralized approach and the use of Software Defined Networking (SDN) technology. Therefore, a heuristic algorithm is presented. Experiments are conducted by emulating the proposed framework in Mininet-WiFi with the Floodlight usage as the SDN controller. The simulation results show the superiority of the performance of the proposed architecture in terms of the service rate and the costs imposed on the operator in comparison to existing approaches.},
  archive      = {J_COMCOM},
  author       = {Shirzad Shahryari and Farzad Tashtarian and Seyed-Amin Hosseini-Seno},
  doi          = {10.1016/j.comcom.2022.05.005},
  journal      = {Computer Communications},
  pages        = {257-273},
  shortjournal = {Comput. Commun.},
  title        = {CoPaM: cost-aware VM placement and migration for mobile services in multi-cloudlet environment: an SDN-based approach},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comprehensive survey on aerial mobile edge computing:
Challenges, state-of-the-art, and future directions. <em>COMCOM</em>,
<em>191</em>, 233–256. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the visions of Internet of Things (IoT), there is an ever-increasing demand for computation resources of IoT users to support diverse applications. Mobile edge computing (MEC) has been deemed a promising solution to settle the conflict between the resource-hungry mobile applications and the resource-constrained IoT users. On the other hand, in order to provide ubiquitous and reliable connectivity in wireless networks, unmanned aerial vehicles (UAVs) can be leveraged as efficient aerial platforms by exploiting their inherent attributes, such as the on-demand deployment, high cruising altitude , and controllable maneuverability in three-dimensional (3D) space. Thus, the UAV-enabled aerial MEC is believed as a win-win solution to facilitate cost-effective and energy-saving communication and computation services in various environments. In this paper, we provide a comprehensive survey on the UAV-enabled aerial MEC. Firstly, the related advantages and research challenges for aerial MEC are discussed. Then, we provide a comprehensive review of the recent research advances, which is categorized by different domains, including the joint optimization of UAV trajectory, computation offloading and resource allocation, UAV deployment, task scheduling and load balancing, interplay between aerial MEC and other technologies, as well as the machine-learning (ML)-driven optimization. Finally, some important research directions deserved more efforts in future work are summarized.},
  archive      = {J_COMCOM},
  author       = {Zhengyu Song and Xintong Qin and Yuanyuan Hao and Tianwei Hou and Jun Wang and Xin Sun},
  doi          = {10.1016/j.comcom.2022.05.004},
  journal      = {Computer Communications},
  pages        = {233-256},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive survey on aerial mobile edge computing: Challenges, state-of-the-art, and future directions},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of HARQ for improved link efficiency within dense
IEEE 802.11 networks. <em>COMCOM</em>, <em>191</em>, 217–232. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progressive adoption of Wi-Fi networks in commercial verticals (dense deployments in buildings, robotics control, gaming) is leading to densification , which demands improvements in reliability and performance guarantees (improved throughput with low latency). Based on these requirements, the IEEE 802.11 working group aims to develop a new amendment called IEEE 802.11be. Hybrid Automatic Repeat Request (HARQ), an enhanced link adaptation and retransmission protocol, is one of the leading candidate features introduced in the IEEE 802.11be. HARQ has the potential to be used in any Wi-Fi configuration (number of antennas, use of MIMO , or channel bandwidth , etc.). In this work, HARQ Chase Combining is modeled in the ns-3 simulator. This new simulation code has been designed and developed considering a contribution to developing the future IEEE 802.11be simulator model within ns-3. Through extensive simulations, we demonstrate the potential advantages of introducing HARQ in dense deployments, particularly for stations amid a large number of contending stations that make transmission attempts simultaneously. Based on the insightful results, we indicate HARQ as a remedy to alleviate the side effects of Dynamic Sensitivity Control (DSC) technique proposed to improve spatial reuse (i.e., combined throughput increase of approx. 18\% and a decrease of 4\% in error rate for the spatial reuse method). To the best of our knowledge, this is the first and novel comprehensive evaluation of HARQ under extremely heavily loaded Wi-Fi conditions.},
  archive      = {J_COMCOM},
  author       = {M. Shahwaiz Afaqui and Joseph Finnegan and Stephen Brown},
  doi          = {10.1016/j.comcom.2022.04.034},
  journal      = {Computer Communications},
  pages        = {217-232},
  shortjournal = {Comput. Commun.},
  title        = {Evaluation of HARQ for improved link efficiency within dense IEEE 802.11 networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognition of interactive human groups from mobile sensing
data. <em>COMCOM</em>, <em>191</em>, 208–216. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real life, people often participate in activities in groups. During the activities, group members commonly engage in interactions such as shaking hands, waving hands, embracing, and hooking arms. Existing approaches to recognize human groups assume that the individuals’ locations or sensing signals are similar; the interactions among them are probably regarded as dissimilar data and affect the recognition accuracy. Moreover, not all persons undertake the same interactions simultaneously. In this study, we propose an approach named interactive group recognizing (IGR) to solve this problem. We collected the sensing data from individuals to deduce their interactions, and compute the disparity between two individuals. Subsequently, a majority-voting based method is applied to recognize the human groups to eliminate the inconsistency among interactions. We also analyzed the number of interactions that could occur without adversely affecting the performance of our approach. Compared with the results from existing approaches, divergence-based affiliation detection (DBAD) and cross-correlation based approach, IGR improved the group recognition accuracy by 6.9\% and 26.7\%, respectively, and F1-score by 13.6\% and 54.6\%, respectively, when interactions among people account for no less than 8\% of the total execution time .},
  archive      = {J_COMCOM},
  author       = {Weiping Zhu and Jiaojiao Chen and Lin Xu and Jiannong Cao},
  doi          = {10.1016/j.comcom.2022.04.028},
  journal      = {Computer Communications},
  pages        = {208-216},
  shortjournal = {Comput. Commun.},
  title        = {Recognition of interactive human groups from mobile sensing data},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging unlinkability and data utility: Privacy preserving
data publication schemes for healthcare informatics. <em>COMCOM</em>,
<em>191</em>, 194–207. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publishing patient data without revealing their sensitive information is one of the challenging research issues in the healthcare sector . Patient records contain useful information that is often released to healthcare industries and government institutions to support medical and census research. There are several existing privacy models in protecting healthcare data privacy, which are mainly built upon the anonymity of patients. In this paper, we incorporate unlinkability in the context of healthcare data publication, where two new privacy notions namely identity unlinkability and attribute unlinkability are introduced. We design two schemes using the proposed models to address identity disclosure and attribute disclosure problems in publishing healthcare data. Experimental results on real and synthetic datasets show that our schemes efficiently achieve data utility preservation and privacy protection simultaneously.},
  archive      = {J_COMCOM},
  author       = {Kah Meng Chong and Amizah Malip},
  doi          = {10.1016/j.comcom.2022.04.032},
  journal      = {Computer Communications},
  pages        = {194-207},
  shortjournal = {Comput. Commun.},
  title        = {Bridging unlinkability and data utility: Privacy preserving data publication schemes for healthcare informatics},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFPSE: Multi-user forward private searchable encryption with
dynamic authorization in cloud computing. <em>COMCOM</em>, <em>191</em>,
184–193. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Searchable Symmetric Encryption (DSSE) allows users to outsource data with ciphertext format to untrusted servers and supports the operations of data adding and deleting, which is adopted in common usage by government and business. Recently, the primitive of Forward Privacy in DSSE has drawn great public interest owing to its beneficial feature, which is that it can guard against the newly uploaded files from linking to previous search tokens. However, most of existing Forward Private Searchable Encryption (FPSE) schemes focus on single-user environment, which means that only the users themselves can utilize the data, greatly limits the wide application in cloud computing . To our knowledge, it is difficult to migrate the FPSE schemes to multi-user network. First, to realize forward privacy, the data owner should share the entire key group to the legitimate users, which means the users have the privilege of tampering or deleting data rather than query only; secondly, some FPSE schemes use the special structures and cannot be directly developed to multi-user network. Inspired by this, we present a scheme of multi-user forward private searchable encryption with dynamic authorization. The proposed scheme is based on a new structure of Multi-user State Chain and involves dynamic keyword-oriented authorization management. We prove our scheme can meet the secure characteristics, then conduct the performance evaluation and experiments. The results demonstrate that compared with the existing solutions, our scheme is superior in efficiency and practicability.},
  archive      = {J_COMCOM},
  author       = {Jianwei Li and Xiaoming Wang and Qingqing Gan and Fengling Wang},
  doi          = {10.1016/j.comcom.2022.04.026},
  journal      = {Computer Communications},
  pages        = {184-193},
  shortjournal = {Comput. Commun.},
  title        = {MFPSE: Multi-user forward private searchable encryption with dynamic authorization in cloud computing},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain based secure and reliable cyber physical
ecosystem for vaccine supply chain. <em>COMCOM</em>, <em>191</em>,
173–183. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunization has become essential for prevention of infectious diseases during pandemic situations across the globe, and unsafe vaccines are seriously affecting public health and economy of countries. However, issues such as falsified or substandard and expired vaccines are still exist due to complex global supply chains and the fact that present vaccine management systems are under the control of different entities in the vaccine supply chain. Therefore, the requirement to develop the Cyber Physical ecosystem that is more resilient, responsive and having smart track and trace is urgently required. To achieve these objectives, blockchain and Internet of Things (IoT) based vaccine supply chain is proposed in this work. The proposed system takes advantages of blockchain database to store the generated data through interactions among stakeholders and from IoT devices in vaccine supply chain. The proposed system is implemented on Hyperledger Fabric (HLF) blockchain framework through which the proposed system achieves resiliency, track and trace of vaccine circulation. The performance evaluations are obtained by testing a HLF with smart contracts . The evaluation is further driven by clients via Hyperledger Caliper, which is a bench mark tool used to measure the performance of blockchain solutions developed on Hyperledger. The results exhibit that the proposed system outperforms other benchmark schemes in terms of throughput and latency etc.},
  archive      = {J_COMCOM},
  author       = {M. Sreenu and Nitin Gupta and Chandrashekar Jatoth and Aldosary Saad and Abdullah Alharbi and Lewis Nkenyereye},
  doi          = {10.1016/j.comcom.2022.04.031},
  journal      = {Computer Communications},
  pages        = {173-183},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain based secure and reliable cyber physical ecosystem for vaccine supply chain},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hidden terminal aware grouping scheme for IEEE 802.11ah
based dense IoT networks. <em>COMCOM</em>, <em>191</em>, 161–172. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IEEE 802.11ah, a prevalent standard for the Internet of Things , introduces major enhancements to PHY and MAC layer of the legacy standard. One of the significant improvements in IEEE 802.11ah is the restricted access window (RAW) mechanism. The RAW mechanism is a group-based contention technique that limits the contention among the devices by partitioning them into several groups, dividing the channel time into various time slots and assigning each group with a RAW slot. Although the grouping schemes reduce contention among the devices, it eventually leads to hidden terminals in the dense networks. In this article, we propose a hidden terminal aware grouping (HTAG) mechanism to alleviate the hidden terminal problem. The proposed scheme groups the devices based on the SINR and implements a hidden terminal avoidance algorithm to ensure each group with a minimum number of hidden terminals . We present a simple yet accurate analytical model to evaluate the performance of the RAW mechanism with the proposed HTAG scheme in terms of throughput, delay and energy consumption. Results show significant improvement in the performance of the RAW mechanism using the HTAG scheme compared to the state-of-the-art schemes. Finally, all the analytical findings are validated using extensive simulations in ns-3.},
  archive      = {J_COMCOM},
  author       = {Miriyala Mahesh and V.P. Harigovindan},
  doi          = {10.1016/j.comcom.2022.04.033},
  journal      = {Computer Communications},
  pages        = {161-172},
  shortjournal = {Comput. Commun.},
  title        = {Hidden terminal aware grouping scheme for IEEE 802.11ah based dense IoT networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards optimal positioning and energy-efficient UAV path
scheduling in IoT applications. <em>COMCOM</em>, <em>191</em>, 145–160.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Unmanned Aerial Vehicle (UAV) based communication has been emerged as a feasible solution for remote applications such as disaster management, and search and rescue due to its mobility and cost efficiency. The prior researches in this domain focused on positioning and path planning of UAVs; however, these approaches faced several limitations due to adverse weather conditions of the environment. In this paper, the impact of weather-based positioning and path planning of UAVs (IWPOP-UAV) is carried out to achieve increased QoS, reliability and energy efficiency in UAV communications. Initially, the prediction of weather conditions in the emergency situations is performed by utilizing the Cerebral Long Short-Term Memory (C-LSTM) which possesses negligible training loss and increased accuracy. The cell-based partitioning of emergency area is carried out in order to determine the target UEs. The decision upon number and position of the UAVs in each cell is provided by the A3C algorithm based on weather conditions and other significant factors thereby achieving increased coverage ratio and minimal requirement of UAV transmit power. The path planning of the UAV in order to perform effective collection of data is considered as a multi-objective optimization problem and executed by using Mayfly Optimization Algorithm (MOA). By doing so, the proposed approach is able to achieve increased QoS, reliability and energy efficiency in UAV based communication. The proposed IWPOP-UAV approach is experimented in NS 3.26 and evaluated in terms of performance metrics such as coverage ratio, cell coverage, delay, path gain, number of collected packets, UAV transmit power, and energy consumption. The results obtained are summarized and concluded to demonstrate the efficacy of the proposed approach.},
  archive      = {J_COMCOM},
  author       = {Mohammed Saleh Ali Muthanna and Ammar Muthanna and Tu N. Nguyen and Abdullah Alshahrani and Ahmed A. Abd El-Latif},
  doi          = {10.1016/j.comcom.2022.04.029},
  journal      = {Computer Communications},
  pages        = {145-160},
  shortjournal = {Comput. Commun.},
  title        = {Towards optimal positioning and energy-efficient UAV path scheduling in IoT applications},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cross entropy based approach to minimum propagation
latency for controller placement in software defined network.
<em>COMCOM</em>, <em>191</em>, 133–144. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Software Defined Network (SDN) has emerged as a pivotal element not only in data-centers and wide-area networks, but also in next generation networking architectures. SDN is characterized by decoupled data and control planes with logically centralized architecture. In order to span across the networks and avoid single point of failure , one major challenge in the SDN is to select appropriate locations for controllers to shorten the latency between controllers and switches, especially in wide area networks. For this purpose, we formulate the Controller Placement Problem (CPP) as an integer programming problem, which takes both the communication cost and synchronization cost into account. Due to its high computational complexity , the cross entropy belonging to the field of Stochastic Optimization is proposed and can sample the problem space and approximate the distribution of good solutions. As a result, we propose a cross entropy based approach to solve CPP, and we conduct experiments on 6 real topologies from the Internet Topology Zoo and Internet2 OS3E. The results verify that the proposed approach can realize the minimum propagation latency for different network scales with different number of controllers, with a less than 5.30\% margin from the optimal solution. Moreover, the cross entropy can promise the calculation result be stable with a less than 2\% margin, and can apply to all the network scales including large network topologies .},
  archive      = {J_COMCOM},
  author       = {Jue Chen and Yu-Jie Xiong and Xihe Qiu and Dun He and Hanmin Yin and Changwei Xiao},
  doi          = {10.1016/j.comcom.2022.04.030},
  journal      = {Computer Communications},
  pages        = {133-144},
  shortjournal = {Comput. Commun.},
  title        = {A cross entropy based approach to minimum propagation latency for controller placement in software defined network},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NetSentry: A deep learning approach to detecting incipient
large-scale network attacks. <em>COMCOM</em>, <em>191</em>, 119–132. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) techniques are increasingly adopted to tackle ever-evolving high-profile network attacks, including Distributed Denial of Service (DDoS), botnet , and ransomware , due to their unique ability to extract complex patterns hidden in data streams. These approaches are however routinely validated with data collected in the same environment, and their performance degrades when deployed in different network topologies and/or applied on previously unseen traffic, as we uncover. This suggests malicious/benign behaviors are largely learned superficially and ML-based Network Intrusion Detection Systems (NIDS) need revisiting, to be effective in practice. In this paper we dive into the mechanics of large-scale network attacks, with a view to understanding how to use ML for Network Intrusion Detection (NID) in a principled way. We reveal that, although cyberattacks vary significantly in terms of payloads, vectors and targets, their early stages , which are critical to successful attack outcomes, share many similarities and exhibit important temporal correlations . Therefore, we treat NID as a time-sensitive task and propose NetSentry, perhaps the first of its kind NIDS that builds on Bidirectional Asymmetric LSTM (Bi-ALSTM), an original ensemble of sequential neural models , to detect network threats before they spread. We cross-evaluate NetSentry using two practical datasets, training on one and testing on the other, and demonstrate F1 score gains above 33\% over the state-of-the-art, as well as up to 3 × × higher rates of detecting attacks such as Cross-Site Scripting (XSS) and web bruteforce. Further, we put forward a novel data augmentation technique that boosts the generalization abilities of a broad range of supervised deep learning algorithms , leading to average F1 score gains above 35\%. Lastly, we shed light on the feasibility of deploying NetSentry in operational networks, demonstrating affordable computational overhead and robustness to evasion attacks.},
  archive      = {J_COMCOM},
  author       = {Haoyu Liu and Paul Patras},
  doi          = {10.1016/j.comcom.2022.04.020},
  journal      = {Computer Communications},
  pages        = {119-132},
  shortjournal = {Comput. Commun.},
  title        = {NetSentry: A deep learning approach to detecting incipient large-scale network attacks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A channel perceiving attack and the countermeasure on
long-range IoT physical layer key generation. <em>COMCOM</em>,
<em>191</em>, 108–118. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical layer key generation is a lightweight technique to generate secret keys from wireless channels for resource-constrained Internet of things (IoT) applications. The security of the key generation relies on spatial decorrelation , which assumes that eavesdroppers observe uncorrelated channel measurements when they are located over a half-wavelength away from legitimate users. Unfortunately, no experimental validation exists for communications environments with both large-scale and small-scale fading effects. Furthermore, while the current key generation work mainly focuses on short-range communications techniques such as WiFi and ZigBee , the exploration with long-range communications, e.g., LoRa, is somewhat limited. This paper presents a long-range key generation testbed and reveals a new attack scenario that perceives and utilizes large-scale fading effects in key generation channels, by using multiple eavesdroppers circularly around a legitimate user. We formalized such an attack and validated it through extensive experiments conducted in indoor and outdoor environments. It is corroborated that the attack reduces secret key capacity when large-scale fading is predominant. We further investigated potential defenses by proposing a conditional entropy and high-pass filter-based countermeasure to estimate and eliminate large-scale fading components. The experimental results demonstrated that the countermeasure significantly improved the key generation’s security when both large-scale and small-scale fading existed. The keys generated by legitimate users have a desirable low key disagreement rate (KDR) and are validated by the NIST randomness tests. In contrast, eavesdroppers’ average KDR is increased from 0.25 to 0.49.},
  archive      = {J_COMCOM},
  author       = {Lu Yang and Yansong Gao and Junqing Zhang and Seyit Camtepe and Dhammika Jayalath},
  doi          = {10.1016/j.comcom.2022.04.027},
  journal      = {Computer Communications},
  pages        = {108-118},
  shortjournal = {Comput. Commun.},
  title        = {A channel perceiving attack and the countermeasure on long-range IoT physical layer key generation},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A resilient trust management framework towards trust related
attacks in the social internet of things. <em>COMCOM</em>, <em>191</em>,
92–107. (<a href="https://doi.org/10.1016/j.comcom.2022.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a huge number of heterogeneous smart things in the Social Internet of Things (SIoT) paradigm are connected to exchange services, security principles are needed to ensure safe communication in the network. From this perspective, various Trust Management models have been set forward in the literature in order to assess nodes trustworthiness and therefore to avoid malicious connections in the SIoT network. However, these models still lack practical solutions to address SIoT trust related attacks performed towards the trust evaluation process. In this regard, the present work stands for an in-depth study on the effectiveness of the trust attack management, as it is integrated into the trust model, with the aim of classifying the nodes behaviors more accurately in order to ensure secure connections in the network. For this reason, nodes behaviors analysis, based on Machine Learning (ML) algorithms, is carried out within our trust assessment process. Our objective lies in limiting interactions with not only poor service provider, but also with attacker nodes. The proposed trust method’s acquired outcome reveals that our introduced TM model demonstrates an improved efficiency in identifying malicious nodes compared to a trust classification algorithm without nodes behaviors evaluation. Experimentation conducted within a simulated dataset based on real social data through Cooja; corroborates the efficiency of our proposed solution in terms of coping with simulated trust related attacks performed by malicious nodes.},
  archive      = {J_COMCOM},
  author       = {Rim Magdich and Hanen Jemal and Mounir Ben Ayed},
  doi          = {10.1016/j.comcom.2022.04.019},
  journal      = {Computer Communications},
  pages        = {92-107},
  shortjournal = {Comput. Commun.},
  title        = {A resilient trust management framework towards trust related attacks in the social internet of things},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimedia services placement algorithm for cloud–fog
hierarchical environments. <em>COMCOM</em>, <em>191</em>, 78–91. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile communication, multimedia services have experienced explosive growth in the last few years. The high quantity of mobile users, both consuming and producing these services to and from the Cloud Computing (CC), can outpace the available bandwidth capacity. Fog Computing (FG) presents itself as a solution to improve on this and other issues. With a reduction in network latency , real-time applications benefit from improved response time and greater overall user experience . Taking this into account, the main goal of this work is threefold. Firstly, it is proposed a method to build an environment based on Cloud–Fog Computing (CFC). Secondly, it is designed two models based on Autoregressive Integrated Moving Average (ARIMA) and Long Short-Term Memory (LSTM). The goal is to predict demand and reserve the nodes’ storage capacity to improve the positioning of multimedia services . Later, an algorithm for the multimedia service placement problem which is aware of data traffic prediction is proposed. The goal is to select the minimum number of nodes, considering their hardware capacities for providing multimedia services in such a way that the latency for servicing all the demands is minimized. An evaluation with actual data showed that the proposed algorithm selects the nodes closer to the user to meet their demands. This improves the services delivered to end-users and enhances the deployed network to mitigate provider costs. Moreover, reduce the demand to Cloud allowing turning off servers in the data center not to waste energy .},
  archive      = {J_COMCOM},
  author       = {Fillipe Santos and Roger Immich and Edmundo R.M. Madeira},
  doi          = {10.1016/j.comcom.2022.04.009},
  journal      = {Computer Communications},
  pages        = {78-91},
  shortjournal = {Comput. Commun.},
  title        = {Multimedia services placement algorithm for cloud–fog hierarchical environments},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cache-based green distributed cell dormancy technique for
dense heterogeneous networks. <em>COMCOM</em>, <em>191</em>, 69–77. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green communication is becoming an impacting quality feature in cellular communications nowadays. Several techniques discuss how to improve cellular network energy efficiency without affecting the system throughput. However, caching combing with dormancy approach in heterogeneous dense networks (HDN) has not attracted enough attention. Due to this, this paper proposes a Distributed Cell Dormant Technique that considers caching for improving the energy efficiency of HDN while overcoming the problem of excessive back-haul overhead. The proposed approach considers the power consumption in a system that a macro cell shares with multiple micro cells. And, it manages reconciled network statistics of each cell in a tabular format that uses the wake-up and dormancy operations in a distributed network. The performance evaluation shows that the proposed method significantly reduces system power consumption without decreasing average throughput in the dense heterogeneous networks .},
  archive      = {J_COMCOM},
  author       = {Wanying Guo and Shiraz Ali Wagan and Dong Ryeol Shin and Nawab Muhammad Faseeh Qureshi},
  doi          = {10.1016/j.comcom.2022.04.025},
  journal      = {Computer Communications},
  pages        = {69-77},
  shortjournal = {Comput. Commun.},
  title        = {Cache-based green distributed cell dormancy technique for dense heterogeneous networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 6Blocks: 6G-enabled trust management scheme for
decentralized autonomous vehicles. <em>COMCOM</em>, <em>191</em>, 53–68.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a 6G-envisioned blockchain (BC)-based scheme for cellular vehicle-to-everything (C-V2X) ecosystems (termed as 6Blocks ), which optimizes the edge-resource provisioning of 5G C-V2X nodes via network function virtualization (NFV)-enabled infrastructure. 6Blocks operates in three phases. In the first phase, a secure data aggregation through 6G sensors is performed at the data plane. Then, in the second phase, the aggregated data is sent to the 6G-enabled NFV control plane to leverage resource allocation to connected autonomous smart vehicles (CASVs) through edge nodes via service container operations. In the third phase, management plane operations are designed that includes registration and auction smart contracts (SC) for connected autonomous vehicle owner (CAVO) and CASVs over the BC with public/private pairs fetched from interplanetary file systems (IPFS). The proposed scheme is compared against existing NFV schemes like distant, edge, and SDN-based edge control. At 20 CASVs, 6Blocks achieves an edge-service latency of 0 . 05 ms 0.05ms against 0 . 76 ms 0.76ms , 0.27, and 0 . 15 ms 0.15ms for cloud, edge, and SDN-edge schemes, respectively, with high connection density of 1 0 7 107 nodes, at maximum utilized IPFS bandwidth of 0.2 Mbps. Then, we validate the trust of our model through the automated validation of the internet security protocol and applications (AVISPA) tool and modeled the vehicle and sensor nodes . The obtained results show the efficacy of the proposed scheme against existing state-of-the-art approaches with respect to parameters such as network, security, transaction costs, and auction.},
  archive      = {J_COMCOM},
  author       = {Pronaya Bhattacharya and Arpit Shukla and Sudeep Tanwar and Neeraj Kumar and Ravi Sharma},
  doi          = {10.1016/j.comcom.2022.04.024},
  journal      = {Computer Communications},
  pages        = {53-68},
  shortjournal = {Comput. Commun.},
  title        = {6Blocks: 6G-enabled trust management scheme for decentralized autonomous vehicles},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAM: A frame aggregation based method to infer the load
level in IEEE 802.11 networks. <em>COMCOM</em>, <em>191</em>, 36–52. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many environments, connected devices are exposed to and must choose between multiple Wi-Fi networks. However, the procedure for selecting an access point is still based on simple criteria that consider the device to be unique in the network. In particular, the network load is not taken into account even though it is a key parameter for the quality of service and experience. In this paper, we investigate how an unmodified vanilla device could estimate the load of a network in the user space with no interventions from the access points. In this regard, we propose a novel and practical method, FAM ( F rame A ggregation based M ethod). It leverages the frame aggregation mechanism introduced in recent IEEE 802.11 amendments to estimate the network load through its channel busy time fraction. FAM combines an active probing technique to measure the actual packet aggregation and Markovian models that provide the expected rate as a function of the volume and nature of the traffic on the network. We validate the effectiveness of FAM against both ns-3 simulations and test-bed experiments under several scenarios. Results show that our method FAM is able to infer the network load with a granularity based on six different levels of network loads for the considered scenarios.},
  archive      = {J_COMCOM},
  author       = {Nour El Houda Bouzouita and Anthony Busson and Herve Rivano},
  doi          = {10.1016/j.comcom.2022.04.021},
  journal      = {Computer Communications},
  pages        = {36-52},
  shortjournal = {Comput. Commun.},
  title        = {FAM: A frame aggregation based method to infer the load level in IEEE 802.11 networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolution of network time synchronization towards
nanoseconds accuracy: A survey. <em>COMCOM</em>, <em>191</em>, 26–35.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We expose the state of the art in the topic of network time synchronization. Many distributed applications require a common notion of time to function properly. Without time synchronization, the nodes clocks will drift and report different values for the same instant. This problem is exacerbated by varying network delays between the cooperating nodes. Our survey covers how this issue is tackled by standard time synchronization mechanisms and a representative range of recent research works. We expose how some of them achieve micro and nanoseconds accuracy in wired networks. The reviewed techniques are classified in two categories based on whether they change the hosts clocks or not. The latter category includes schemes that detect and remove clock skew from network traffic trace. We discuss the advantages and drawbacks of the techniques in each category; compare them according to their application environment, accuracy and cost; and conclude this survey with a summary of learned lessons and insights into future work.},
  archive      = {J_COMCOM},
  author       = {Djalel Chefrour},
  doi          = {10.1016/j.comcom.2022.04.023},
  journal      = {Computer Communications},
  pages        = {26-35},
  shortjournal = {Comput. Commun.},
  title        = {Evolution of network time synchronization towards nanoseconds accuracy: A survey},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A decentralized blockchain-based key management protocol for
heterogeneous and dynamic IoT devices. <em>COMCOM</em>, <em>191</em>,
11–25. (<a href="https://doi.org/10.1016/j.comcom.2022.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure communication is one of the main challenges that are slowing down the development of the Internet of Things (IoT). Key Management ( KM KM ) is particularly a difficult security issue, mainly because of the lack of resources of the IoT devices. Most of the existing solutions do not consider the heterogeneous and dynamic nature of the IoT. They do not regard the difference in capability of its components and impose equal loads on them. Moreover, they store keys in device memories before deployment, which makes adding devices difficult afterwards. We propose a novel decentralized blockchain-based KM KM protocol for the IoT. Our solution balances the loads between nodes according to their capabilities. We prove that this makes it efficient and scalable. Furthermore, our solution securely rekey the network upon a change. To decentralize the KM KM , we use the blockchain technology and smart contracts . We show that the system continues to operate when an entity fails and that the compromise of an entity does not jeopardize the whole network. We also prove that our solution fulfills the IoT requirements in terms of security and performance. Finally, we propose an implementation on IoT platforms to validate our theoretical analysis and simulation results.},
  archive      = {J_COMCOM},
  author       = {Mohamed Ali Kandi and Djamel Eddine Kouicem and Messaoud Doudou and Hicham Lakhlef and Abdelmadjid Bouabdallah and Yacine Challal},
  doi          = {10.1016/j.comcom.2022.04.018},
  journal      = {Computer Communications},
  pages        = {11-25},
  shortjournal = {Comput. Commun.},
  title        = {A decentralized blockchain-based key management protocol for heterogeneous and dynamic IoT devices},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning based scheduling for minimizing
age of information in wireless powered sensor networks. <em>COMCOM</em>,
<em>191</em>, 1–10. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For real-time monitoring system, the age of information (AoI) is usually used to quantify the freshness of information at a monitor about some stochastic processes observed by the source node. In this paper, we consider the wireless powered sensor networks (WPSNs) where multiple sensor nodes send update packets to the base station . Time is divided into slots of equal duration and at each slot either wireless energy transfer or packet transmission is conducted. We aim to minimize the long-term average weighted sum-AoI of different processes at the base station . Specifically, we first formulate the average weighted sum-AoI minimization problem as a multi-stage stochastic non-linear integer programming (NLP) subject to the energy causality constraints. Second, we design an algorithm which first applies Lyapunov optimization to decouple the multi-stage stochastic NLP into per-frame deterministic NLP problems. Then in each frame, our algorithm utilizes the model-free DRL to solve the per-frame NLP problem with very low computational complexity where one exploration policy is designed to obtain multiple one-hot candidate actions based on single real-number output of neural network . We demonstrate through simulations that, our proposed algorithm can achieve greatly smaller average weighted sum-AoI than the available DQN-based algorithm and also alleviate the problem that some source nodes may have large instantaneous AoIs.},
  archive      = {J_COMCOM},
  author       = {Weiwei Jin and Juan Sun and Kaikai Chi and Shubin Zhang},
  doi          = {10.1016/j.comcom.2022.04.007},
  journal      = {Computer Communications},
  pages        = {1-10},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based scheduling for minimizing age of information in wireless powered sensor networks},
  volume       = {191},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on smart green computing for wireless sensor
networks. <em>COMCOM</em>, <em>190</em>, 216–218. (<a
href="https://doi.org/10.1016/j.comcom.2022.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Chetna Singhal ( Guest Editors ) and Deepak Kumar Jain and Alberto Tarable and Anand Nayyar},
  doi          = {10.1016/j.comcom.2022.05.003},
  journal      = {Computer Communications},
  pages        = {216-218},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on smart green computing for wireless sensor networks},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analytical model for task offloading in a fog computing
system with batch-size-dependent service. <em>COMCOM</em>, <em>190</em>,
201–215. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading is one of the main concepts in fog computing which improves the system efficiency and decreases latency. Previously proposed models, such as exponential queue models, addressed the offloading models in a simple model. This study proposes a novel analytical model that examines batch queuing systems and the influence of batch size-dependent service time on system performance. Some of the system’s properties are indicated using this model, and the correctness of the suggested model via numerical evaluations and simulations is shown. The evaluation results show that our proposed model provides acceptable accuracy and enables efficient task offloading, applied to fog computing systems.},
  archive      = {J_COMCOM},
  author       = {Tina Samizadeh Nikoui and Amir Masoud Rahmani and Ali Balador and Hamid Haj Seyyed Javadi},
  doi          = {10.1016/j.comcom.2022.04.010},
  journal      = {Computer Communications},
  pages        = {201-215},
  shortjournal = {Comput. Commun.},
  title        = {Analytical model for task offloading in a fog computing system with batch-size-dependent service},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel clustering based algorithm to mitigate the demand of
forecasting errors for newly deployed LTE cells with insufficient
historical data. <em>COMCOM</em>, <em>190</em>, 190–200. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel clustering-based algorithm to mitigate the demand of forecasting errors of newly deployed LTE (Long-Term Evolution) cells with insufficient historical data. The numbers and the usage of mobile networks are growing day by day. So, new base stations are set every day, and the newly deployed cells do not have enough historical data to forecast. We developed a clustering-based algorithm to overcome this problem. We compared our approach with different forecasting methods such as classical time series methods, time series decomposition-based methods, and deep NN (Neural Network) methods. We tested our clustering-based solution compared with other approaches using seventy LTE cells’ daily historical performance data for two years. We collected this data from a Tier-1 Mobile Network Operator (MNO). We also analyzed the clustering features and benchmarked them for their contribution to the solution, and we measured the error rate by MAPE (Mean Absolute Percentage Error). As a result, we decreased the previous forecasting error rate from 133\% to approximately 35\%, showing that our novel algorithm is an efficient tool for this process.},
  archive      = {J_COMCOM},
  author       = {Yakup Tarik Kranda and Ruya Samli},
  doi          = {10.1016/j.comcom.2022.04.022},
  journal      = {Computer Communications},
  pages        = {190-200},
  shortjournal = {Comput. Commun.},
  title        = {A novel clustering based algorithm to mitigate the demand of forecasting errors for newly deployed LTE cells with insufficient historical data},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted utility aware computational overhead minimization
of wireless power mobile edge cloud. <em>COMCOM</em>, <em>190</em>,
178–189. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fifth-generation (5G) wireless networks are projected to support a large number of low-power devices, which are an essential component of Internet of Things (IoT) technologies. The existing network design is expected to face a number of challenges in serving such massive additional devices. Furthermore, these devices are bound by limited power processing and storage capacity, despite the fact that they are supposed to analyze massive volumes of data. In the last decade, the concepts of cloud and edge computing have received significant attention in order to increase network performance. Furthermore, wireless power transfer (WPT) is integrated with the cloud for energy transmission to low-power devices. This work presents a mathematical model for scheduling tasks that are being executed at low-power devices and concurrently on the cloud edge. The proposed mathematical model takes into account practical constraints and is intended to distribute resources among devices optimally while minimizing network overhead. To deal with the conflicting constraints, the model is further transformed into an unconstrained one. To solve the proposed model, evolutionary methods are being explored. Finally, a Monte Carlo simulation is used to validate the model. The simulation results show that partial offloading outperforms the edge-only computation scheme, with the partial offloading scheme demonstrating a 20\% improvement in network overhead.},
  archive      = {J_COMCOM},
  author       = {Asad Mahmood and Ashfaq Ahmed and Muhammad Naeem and Muhammad Rizwan Amirzada and Arafat Al-Dweik},
  doi          = {10.1016/j.comcom.2022.04.017},
  journal      = {Computer Communications},
  pages        = {178-189},
  shortjournal = {Comput. Commun.},
  title        = {Weighted utility aware computational overhead minimization of wireless power mobile edge cloud},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint optimization of collaborative interactive charging and
charging lane placement for cyclic electric vehicles. <em>COMCOM</em>,
<em>190</em>, 166–177. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicle (EV) usually has limited battery capacity, which has to be charged frequently during service time. Although lots of work on deployment of charging lanes can support charging in-motion EVs, it cannot guarantee that every EV can be operational in its entire route. We are motivated by the following two observations. First, with the development on the circuit design of energy transmit antennas , it is possible to transfer energy between EVs bi-directionally and efficiently. Second, EVs usually have repetitive motions and may cyclically encounter with each other, so that in-motion EVs can have chance to charge each other. So we considered distribute energy among EVs in a collaborative and interactive manner. We proposed a joint optimization of collaborative interactive charging and charging lane placement to minimize the wireless energy loss and the cost of placing charging lanes. We then proved the joint optimization problem is NP-complete and proposed an efficient heuristic algorithm for it. Also extensive simulation results were conducted which showed our proposed algorithm can reduce the overall cost by 20.0\% on average and 43.3\% at most.},
  archive      = {J_COMCOM},
  author       = {Yu Liang and Sheng Zhang and Jidong Ge},
  doi          = {10.1016/j.comcom.2022.04.014},
  journal      = {Computer Communications},
  pages        = {166-177},
  shortjournal = {Comput. Commun.},
  title        = {Joint optimization of collaborative interactive charging and charging lane placement for cyclic electric vehicles},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous flying IoT: A synergy of machine learning,
digital elevation, and 3D structure change detection. <em>COMCOM</em>,
<em>190</em>, 154–165. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research work presented in this paper has been funded by a national research project whose aims are to enable an Unmanned Aerial Vehicle (UAV) to fly autonomously with the use of a Digital Elevation Model (DEM) of the target area and to detect terrain changes with the use of a 3D Structure Change Detection Model (3D SCDM). A Convolutional Neural Network (CNN) works with both models in training the UAV in autonomous flying and in detecting terrain changes. The usability of such an autonomous flying IoT is demonstrated through its deployment in the search for water resources in areas where a satellite would not normally be able to retrieve images, e.g., inside gorges, ravines, or caves. Our experiment results show that it can detect water flows by considering different surface shapes such as standing water polygons, watersheds, water channel incisions, and watershed delineations with a 99.6\% level of accuracy.},
  archive      = {J_COMCOM},
  author       = {Faris A. Almalki and Marios C. Angelides},
  doi          = {10.1016/j.comcom.2022.03.022},
  journal      = {Computer Communications},
  pages        = {154-165},
  shortjournal = {Comput. Commun.},
  title        = {Autonomous flying IoT: A synergy of machine learning, digital elevation, and 3D structure change detection},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Fast asymmetric encryption and decryption of SimpleMatrix
scheme for internet of things. <em>COMCOM</em>, <em>190</em>, 145–153.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric cryptography plays an essential role in many areas, including cloud computing , big data, blockchain , and the Internet of Things (IoT). However, most of them are based on the difficulty of factorizing large numbers or discrete logarithm problems , which are not secure to quantum computer attacks. SimpleMatrix is a new multivariate encryption scheme based on simple matrix multiplications, which can resist quantum computer attacks. Because of the low speed and demands of large finite fields, SimpleMatrix is limited in applications that use small finite fields. As a result, it is critical to improve the efficiency of SimpleMatrix to make its applications broader. In this paper, we speed up the encryption and decryption of SimpleMatrix by building efficient small finite field arithmetics based on Field-Programmable Gate Arrays (FPGAs) technology. We propose a fast architecture for encryption and decryption of SimpleMatrix based on table look-up based composite field multiplications and inversions and fast Gauss–Jordan elimination for solving systems of linear equations in a composite field. We test and verify the hardware architecture of SimpleMatrix on an FPGA, and the experimental results confirm our estimates and comparisons show that our design is much faster than other implementations. Thus, the hardware architecture can be used in FPGA-based systems of cloud computing , IoT, etc., for accelerating encryption and decryption.},
  archive      = {J_COMCOM},
  author       = {Haibo Yi},
  doi          = {10.1016/j.comcom.2022.04.013},
  journal      = {Computer Communications},
  pages        = {145-153},
  shortjournal = {Comput. Commun.},
  title        = {Fast asymmetric encryption and decryption of SimpleMatrix scheme for internet of things},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel reputation-based consensus framework (RCF) in
distributed ledger technology. <em>COMCOM</em>, <em>190</em>, 126–144.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed ledger technology (DLT) has emerged as a key and effective technology which has caught the attention of variety of disciplines and is capable of changing lots of sciences and related industries. Selecting the transactions of members in DLT-based technologies such as blockchain , especially in finance, is essential and vital. Therefore, in this paper, we present a novel framework for selecting members’ transactions through trust and reputation models called the reputation-based consensus framework (RCF), so that by combining trust and reputation models with blockchain , the target node (or verifier), which has the highest reputation based on its transactions history, is selected to create and publish the current block of blockchain. We have also provided a state machine model to accurately describe the behavior of the RCF. In addition, the RCF algorithms are presented and a prototype is developed based on it. The results of simulation and performance and security analysis proved that RCF leads to fair selection of transactions, improves scalability and throughput, and can resist against attacks such as Sybil and malicious behaviors .},
  archive      = {J_COMCOM},
  author       = {Ali Mohsenzadeh and Amir Jalaly Bidgoly and Yaghoub Farjami},
  doi          = {10.1016/j.comcom.2022.04.015},
  journal      = {Computer Communications},
  pages        = {126-144},
  shortjournal = {Comput. Commun.},
  title        = {A novel reputation-based consensus framework (RCF) in distributed ledger technology},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CARD-b: A stacked ensemble learning technique for
classification of encrypted network traffic. <em>COMCOM</em>,
<em>190</em>, 110–125. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of network traffic data into different applications, services, or types is critical for network service providers to monitor networks and maintain Quality of Service (QoS). With the continuous evolution of technological advancements and with the rapid increase in security and user privacy concerns, encryption techniques are commonly used. Encrypted network traffic makes it challenging for network service providers to monitor a network using classical network monitoring techniques and tools. Due to security and privacy reasons, data cannot be decrypted. The dynamics of encrypted network traffic data cannot be interpreted, and it makes the task of classifying the encrypted network traffic a major challenge. This presents Internet Service Providers with challenges as varying Quality of Service is being provided to clients along with the security and privacy concerns to monitor network traffic. Machine learning and deep learning techniques are being utilized to classify encrypted network traffic data. This paper presents an ensemble learning technique that is based on existing data pre-processing machine learning and deep learning techniques. We examine different models and identify the best and relevant statistical features from encrypted network traffic for the classification of non-VPN encrypted network traffic data. We performed multiple experiments that led us to developing an ensemble learning model based on the existing deep learning and machine learning models for the classification of non-VPN encrypted network traffic data. The proposed solution (named as CARD-B) is composed of C apsule Neural Networks , A rtificial Neural Networks , R andom Forest, D ecision Trees, along with B oosting techniques such as Adaptive Boosting and Extreme Gradient Boosting. The techniques are stacked using Random Forest Classifier . The result of the experiment shows that the proposed model achieved an overall accuracy of 96\% and an AUC of 98\% using different possible extracted statistical features.},
  archive      = {J_COMCOM},
  author       = {ThankGod Obasi and M. Omair Shafiq},
  doi          = {10.1016/j.comcom.2022.02.006},
  journal      = {Computer Communications},
  pages        = {110-125},
  shortjournal = {Comput. Commun.},
  title        = {CARD-B: A stacked ensemble learning technique for classification of encrypted network traffic},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lower boundary based nonlinear model predictive control of
transmission power for smart grid WSNs. <em>COMCOM</em>, <em>190</em>,
99–109. (<a href="https://doi.org/10.1016/j.comcom.2022.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the application of data collection and communication in smart grid based on wireless sensor networks (WSNs), communication reliability is the key technical index of WSNs. Theoretically, adjusting the transmission power can control the signal-to-noise ratio (SNR), thereby improving the reliability of wireless communication . However, wireless signal is random changes caused by external device interference or environmental factors. Hence, we study a control of transmission power to meet reliability requirements while maximizing the overall efficiency of the network. Considering the random characteristics of wireless links , we introduce the predictive lower boundary with probability guarantees, and establish a nonlinear state-space model that chooses the SNR as the state variable. Subsequently, we adopt the lower boundary based nonlinear model predictive control (LB-NMPC) algorithm to solve the optimal transmission power, and creatively prove the feasibility and stability. Furthermore, we compare the proposed LB-NMPC algorithm with the adaptive transmission power control (ATPC) algorithm, the potential feedback control (PFC) algorithm and the confidence interval based model predictive control (CI-MPC) algorithm by simulation, and the comparative results show that our LB-NMPC algorithm has lower transmission power and less bad controls. Finally, we also test the LB-NMPC algorithm in real-world indoor substation and outdoor substation to verify its practicability and accuracy.},
  archive      = {J_COMCOM},
  author       = {Xue Xue and Wei Sun and Jianping Wang and Qiyue Li and Haiyan Zhang and Daoming Mu},
  doi          = {10.1016/j.comcom.2022.04.016},
  journal      = {Computer Communications},
  pages        = {99-109},
  shortjournal = {Comput. Commun.},
  title        = {Lower boundary based nonlinear model predictive control of transmission power for smart grid WSNs},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight device-level public key infrastructure with
DRAM based physical unclonable function (PUF) for secure cyber physical
systems. <em>COMCOM</em>, <em>190</em>, 87–98. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–Physical Systems (CPS) are becoming increasingly ubiquitous and serve as a platform for connecting vast majority of everyday devices to the Internet. Traditional computer-based security systems are not preferred in CPS devices due to limited storage, security issues, and computation power, thus necessitating the demand for efficient and secure computation techniques that would ensure robust CPS platforms. In this paper, we proposed a lightweight Public Key Infrastructure (PKI) scheme that can be used in CPS devices. Conventional PKI systems are characterized with three resource-intensive processes, namely, key generation, certificate validation , and certificate revoking. The proposed method uses Physical Unclonable Function (PUF) to simplify the key generation process and introduces the session key concept to address the other two limitations, viz., certificate validation and revoking. In addition to this, the proposed method addresses some crucial security needs of the PKI scheme, such as node registration, strong random number generation , defining trust relationships among the different “thing” nodes. Additionally, PUF-based trust relationships require reduced infrastructural setup, lesser power, and storage capacity while also incorporating Elliptic Curve Cryptography (ECC) for asymmetric key pair generation. The proposed method has been realized using a Dynamic Random Access Memory (DRAM)-based PUF. The experimental result proves the technique’s efficiency compared to the state-of-the-art methods with a 4–7 times, consuming 5–10 times, lesser energy.},
  archive      = {J_COMCOM},
  author       = {Susovan Chanda and Ashish Kumar Luhach and Waleed Alnumay and Indranil Sengupta and Diptendu Sinha Roy},
  doi          = {10.1016/j.comcom.2022.03.012},
  journal      = {Computer Communications},
  pages        = {87-98},
  shortjournal = {Comput. Commun.},
  title        = {A lightweight device-level public key infrastructure with DRAM based physical unclonable function (PUF) for secure cyber physical systems},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A closed-loop control architecture of UAV and WSN for
traffic surveillance on highways. <em>COMCOM</em>, <em>190</em>, 78–86.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excessive speed is the leading cause of accidents on highways. The use of fixed road-side surveillance systems has proven ineffective because people take caution around only those locations. With more advanced technologies and exceptional maneuvering capabilities, Unmanned Aerial Vehicles (UAVs) is a promising candidate for real-time monitoring of vehicles on the highways. The dependency of recently proposed UAV-based architectures on terrestrial networks suffers from high deployment costs. To address this shortcoming, we propose a unique closed-loop control architecture for traffic surveillance on highways that increases the surveillance effectiveness and adapts according to varying traffic patterns. The proposed scheme is comprised of Wireless Sensor Network (WSN) and UAV, named Collaborative Highway Surveillance (CHS), that works without human intervention and enables the capture of over-speeding drivers on a highway. The WSN provides routing services to the UAV and dynamically guides UAV to the best hotspot to position itself. If any vehicle exceeds the speed limit allowed, the UAV immediately informs this event to the Mobile Base Station (MBS). However, to conserve the energy resources of the WSN, low-level speed violations are forwarded to the MBS only when the UAV gets closer to the MBS. Performance results obtained with our proposed architecture show an increase in surveillance efficiency with an improved response time compared with cooperative and stand-alone unguided UAV networks.},
  archive      = {J_COMCOM},
  author       = {Nouman Bashir and Saadi Boudjit and Sherali Zeadally},
  doi          = {10.1016/j.comcom.2022.04.008},
  journal      = {Computer Communications},
  pages        = {78-86},
  shortjournal = {Comput. Commun.},
  title        = {A closed-loop control architecture of UAV and WSN for traffic surveillance on highways},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of uniform resource locator using boosting
algorithms for forensic purpose. <em>COMCOM</em>, <em>190</em>, 69–77.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovations are taking up new roles in all fields. It still has a crucial role in Internet technology, as the ease with which the Internet is available everywhere and accessible from any device has resulted in a slew of cyber-attacks., A prevalent scenario during and before a pandemic is phishing, which is accomplished by smartly altering the URL as a legitimate one and then redirecting the user to other sites and extracting personal information. The benchmark URL datasets used for the study considering an equal balance between phishing/ malicious URLs and benign/ legitimate URLs. URLs are parsed in this procedure to extract valuable elements that aid in the identification of URL phishing. Our research emphasized using different machine learning boosting algorithms such as Extreme Gradient Boosting, Light Gradient Boosting, Adaptive Boosting , and Gradient Boosting and have achieved an accuracy of more than 98\% for most of the algorithms considered.},
  archive      = {J_COMCOM},
  author       = {K.A. Apoorva and S. Sangeetha},
  doi          = {10.1016/j.comcom.2022.04.002},
  journal      = {Computer Communications},
  pages        = {69-77},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of uniform resource locator using boosting algorithms for forensic purpose},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized k-anonymization of trajectories via
privacy-preserving tit-for-tat. <em>COMCOM</em>, <em>190</em>, 57–68.
(<a href="https://doi.org/10.1016/j.comcom.2022.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility data, and specifically trajectories, are used to monitor the mobility of the population and are crucial to improve public health, transportation, urban planning, economic planning, etc. However, trajectories are personally identifiable information and hence they should be anonymized before releasing them for secondary use. Anonymization cannot be limited to suppressing the metadata containing the subject’s identity, because the origin, the destination and even the intermediate points of a trajectory may allow re-identifying the subject who followed it. Proper anonymization requires masking detailed spatiotemporal information. The standard approach to build anonymized data sets is centralized: the subjects send their original movement data to a controller, who takes care of producing an anonymized mobility data set. This requires subjects to blindly trust the controller. In this paper, we empower subjects with the ability to anonymize their trajectories locally by adhering to a privacy model in order to achieve formal privacy guarantees. After reviewing the state of the art, we motivate our choice of k k -anonymity as a privacy model. We then set out to decentralize k k -anonymity in a rational setting: a subject k k -anonymizes her completed trajectory by aggregating with k − 1 k−1 similar trajectories obtained from other (unknown) subjects. The latter trajectories are gathered via an anonymous and privacy-preserving tit-for-tat data exchange protocol, which runs on a fully decentralized peer-to-peer network. Experiments show that, without relying on a (trusted) data controller and while ensuring privacy w.r.t. other peers, our approach yields k k -anonymized mobility data sets that are still reasonably useful compared to the near-optimal data sets obtained in the centralized approach.},
  archive      = {J_COMCOM},
  author       = {Josep Domingo-Ferrer and Sergio Martínez and David Sánchez},
  doi          = {10.1016/j.comcom.2022.04.011},
  journal      = {Computer Communications},
  pages        = {57-68},
  shortjournal = {Comput. Commun.},
  title        = {Decentralized k-anonymization of trajectories via privacy-preserving tit-for-tat},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bio-inspired approach: Firefly algorithm for multi-depot
vehicle routing problem with time windows. <em>COMCOM</em>,
<em>190</em>, 48–56. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, an improved bio-inspired metaheuristic named firefly is utilized to solve the Multi-Depot Vehicle Routing Problem with Time Windows (MDVRP-TW) and it is a generalization of Vehicle Routing Problem with Time Windows. This paper discusses the use of biological intelligence in analysis and optimization on MDVRP-TW. The objective of the MDVRP-TW is to minimize the overall distance, means of transportation within a specified time gap, along with other constraints such as limitation of vehicle capacity, restrictions of route duration length to satisfy the customer’s requirements. The routes must be designed where each customer is visited only once in a particular time gap with one vehicle and all the routes must originate and end at the same depot. The overall demand should not go beyond the vehicle’s capacity on each route. Multi-objective problems have attracted the attentions of many researchers in recent years, as they are more closely related to real-world situations. However, transportation is a significant factor for the current society, exhibiting and planning a complicated transport system was a very thought-provoking task. Many research studies have shown several successful metaheuristics and heuristic methodologies to solve MDVRP-TW in which, instead of a single depot, numerous depots with varying locations. In this work considered both the earliest time and latest arrival time window (double-sided time window). It is assumed that if an arrival time is more significant than the latest time, it will lead to an infeasible solution whereas, if an arrival time is lesser than the earliest time, then it will lead to a waiting time. However, due to its challenging task and multi-objective and limitations, there is still a space to solve it. In this work, a novel technique that combines an improved firefly algorithm (IFA), which uses different mechanisms, namely, inter-depot approach in assignment phase, Clarke &amp; wright algorithm, and local search method in routing and scheduling phase were applied. The suggested technique has been evaluated using Cordeau standard benchmark instances and proved the technique’s novelty and feasibility. In a few circumstances, the modified firefly algorithm outperforms other algorithms, and optimal benchmark solution in terms of results and competence.},
  archive      = {J_COMCOM},
  author       = {R. Yesodha and T. Amudha},
  doi          = {10.1016/j.comcom.2022.04.005},
  journal      = {Computer Communications},
  pages        = {48-56},
  shortjournal = {Comput. Commun.},
  title        = {A bio-inspired approach: Firefly algorithm for multi-depot vehicle routing problem with time windows},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Node localization algorithm for detecting malicious nodes to
prevent connection failures and improve end-to-end delay.
<em>COMCOM</em>, <em>190</em>, 37–47. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sensor network applications, the measured data makes sense only if the location is known. The location of the node is known, and the wireless communication line and transmission method between the two nodes can be an unknown collaboration process where a sensor with a known location is used to locate the node in an unknown location. Various localization algorithms are available that use linear and nonlinear methods to find nodes. Security is a major issue in localization . After the data is sent from the beacon node to the binding node, an error occurs because the virus-safe or unsafe mobile beacon acts like the original mobile beacon, sending the wrong message. One of the basic problems of wireless sensor networks is the localization and security of nodes. This article analyses the performance of optimization algorithms like Ant Colony Optimization algorithms, opportunity routing algorithms , and the proposed Buffalo optimization algorithm. The performance of the buffalo algorithm is compared with these optimization algorithms . Performance metrics such as throughput, residual energy , packet forwarding rate, packet loss rate, delay, and cost were measured. Then the algorithm is implemented as a hardware system for measuring system performance in real-time. Based on a detailed analysis of the performance of optimization algorithms in wireless sensor networks , the buffalo algorithm provides higher efficiency and performance in wireless sensor network applications.},
  archive      = {J_COMCOM},
  author       = {G. Hemanth Kumar and Ramesh G.P.},
  doi          = {10.1016/j.comcom.2022.04.001},
  journal      = {Computer Communications},
  pages        = {37-47},
  shortjournal = {Comput. Commun.},
  title        = {Node localization algorithm for detecting malicious nodes to prevent connection failures and improve end-to-end delay},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on the 16th wireless on-demand network systems
and services conference. <em>COMCOM</em>, <em>190</em>, 36. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Raphaël Frank and Michele Segata and Uichin Lee},
  doi          = {10.1016/j.comcom.2022.03.016},
  journal      = {Computer Communications},
  pages        = {36},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on the 16th wireless on-demand network systems and services conference},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new intrusion detection method for cyber–physical system
in emerging industrial IoT. <em>COMCOM</em>, <em>190</em>, 24–35. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Industrial Internet-of-Things, data streams across heterogeneous networks which results in several cyber–physical attacks. Moreover, the security of unlabeled data is a challenging task. For the same, this paper presents a new clustering method for intrusion detection . The proposed method employs a novel variant of gravitational search algorithm to obtain optimal clusters. In the proposed variant, K b e s t Kbest is modified as an exponentially decreasing function with logistic-mapping-based chaotic behavior. To validate the proposed variant, a comparative analysis on IEEE CEC2013 benchmark functions is conducted against five existing algorithms. Experimental results are investigated in terms of mean error value, Wilcoxon rank-sum test, convergence graph, box-plot, and time complexity. It has been observed that proposed variant attained best values for maximum number of times on each dimension, i.e. 10, 13, 15, and 10 on 10, 30, 50, and 90 dimensions, respectively. Further, the efficacy of the proposed clustering method is tested on five Industrial Internet-of-Things datasets. The evaluation is performed in terms of F-measure and computation time. Experiments affirm that the proposed method outperforms considered methods on 80\% of the datasets in terms of F-measure and computation time for ensuring security in a real-time Industrial Internet-of-Things environment.},
  archive      = {J_COMCOM},
  author       = {Himanshu Mittal and Ashish Kumar Tripathi and Avinash Chandra Pandey and Mohammad Dahman Alshehri and Mukesh Saraswat and Raju Pal},
  doi          = {10.1016/j.comcom.2022.04.004},
  journal      = {Computer Communications},
  pages        = {24-35},
  shortjournal = {Comput. Commun.},
  title        = {A new intrusion detection method for cyber–physical system in emerging industrial IoT},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GAR: Gradient assisted routing for topology
self-organization in dynamic mesh networks. <em>COMCOM</em>,
<em>190</em>, 10–23. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern mobile handheld devices, such as smartphones and tablets, feature multiple wireless interfaces, some of which can support device-to-device communications, which enable mesh networks on even when the infrastructure is unavailable. One of the key technological challenges hampering the use of multi-hop mesh networks is the extremely high communication overhead of route discovery and maintenance algorithms. The problem is especially pronounced under dynamic network conditions caused by user mobility and nodes joining and leaving the network. In this paper, we propose a fully distributed algorithm for constructing a virtual coordinate system used for geo-like routing by approximating the physical network nodes coordinate. The proposed algorithm, called gradient assisted routing (GAR), builds upon two-hop neighbors’ information exchanged in beacons in contrast to conventional geographic routing protocols which rely on external positioning information. We evaluate the proposed solution using algorithmic, topological, and routing-related metrics of interest. We further numerically quantify how the node mobility increases the time needed for topology stabilization, and how network size affects the route discovery success rate. Our comparison also shows that for small to mid-size mesh networks (up to 60 nodes), the performance of the proposed routing procedure is similar to the conventional geographic routing protocols that exploit external positioning information. The proposed solution may efficiently supplement the traditional on-demand routing in small to mid-size mesh systems by independently establishing 50 to 70\% of paths and thereby reducing the discovery overheads.},
  archive      = {J_COMCOM},
  author       = {Andrey Samuylov and Dmitri Moltchanov and Roman Kovalchukov and Anna Gaydamaka and Alexander Pyattaev and Yevgeni Koucheryavy},
  doi          = {10.1016/j.comcom.2022.03.023},
  journal      = {Computer Communications},
  pages        = {10-23},
  shortjournal = {Comput. Commun.},
  title        = {GAR: Gradient assisted routing for topology self-organization in dynamic mesh networks},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QoCoVi: QoE- and cost-aware adaptive video streaming for the
internet of vehicles. <em>COMCOM</em>, <em>190</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in embedded systems and communication technologies enable novel, non-safety applications in Vehicular Ad Hoc Networks (VANETs). Video streaming has become a popular core service for such applications. In this paper, we present QoCoVi as a QoE- and cost-aware adaptive video streaming approach for the Internet of Vehicles (IoV) to deliver video segments requested by mobile users at specified qualities and deadlines. Considering a multitude of transmission data sources with different capacities and costs, the goal of QoCoVi is to serve the desired video qualities with minimum costs. By applying Dynamic Adaptive Streaming over HTTP (DASH) principles, QoCoVi considers cached video segments on vehicles equipped with storage capacity as the lowest-cost sources for serving requests. We design QoCoVi in two SDN-based operational modes : (i) centralized and (ii) distributed. In centralized mode, we can obtain a suitable solution by introducing a mixed-integer linear programming (MILP) optimization model that can be executed on the SDN controller. However, to cope with the computational overhead of the centralized approach in real IoV scenarios, we propose a fully distributed version of QoCoVi based on the proximal Jacobi alternating direction method of multipliers (ProxJ-ADMM) technique. The effectiveness of the proposed approach is confirmed through emulation with Mininet-WiFi in different scenarios.},
  archive      = {J_COMCOM},
  author       = {Alireza Erfanian and Farzad Tashtarian and Christian Timmerer and Hermann Hellwagner},
  doi          = {10.1016/j.comcom.2022.03.003},
  journal      = {Computer Communications},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {QoCoVi: QoE- and cost-aware adaptive video streaming for the internet of vehicles},
  volume       = {190},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “MystifY”: A proactive moving-target defense for a resilient
SDN controller in software defined CPS. <em>COMCOM</em>, <em>189</em>,
205–220. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent devastating mission Cyber–Physical System (CPS) attacks, failures, and the desperate need to scale and to dynamically adapt to changes, revolutionized traditional CPS to what we name as Software Defined CPS (SD-CPS). SD-CPS embraces the concept of Software Defined (SD) everything where CPS infrastructure is more elastic, dynamically adaptable and online-programmable. However, in SD-CPS, the threat became more immanent, as the long-been physically-protected assets are now programmatically accessible to cyber attackers. In SD-CPSs, a network failure hinders the entire functionality of the system. In this paper, we present MystifY, a spatiotemporal runtime diversification for Moving-Target Defense (MTD) to secure the SD-CPS infrastructure. In this paper, we relied on Smart Grid networks as crucial SD-CPS application to evaluate our presented solution. MystifY’s MTD relies on a set of pillars to ensure the SDN controller resiliency against failures and attacks. The 1st pillar is a grid-aware algorithm that optimally allocates the most suitable controller–deployment​ location in large-scale grids. The 2nd pillar is a special diversifier that dynamically relocates the controller between heterogeneously configured hosts to avoid host-based attacks. The 3rd pillar is a temporal diversifier that dynamically detours controller–workload between multiple controllers to enhance their reliability and to detect and avoid controller intrusions. Our experimental results showed the efficiency and effectiveness of the presented approach.},
  archive      = {J_COMCOM},
  author       = {Mohamed Azab and Mohamed Samir and Effat Samir},
  doi          = {10.1016/j.comcom.2022.03.019},
  journal      = {Computer Communications},
  pages        = {205-220},
  shortjournal = {Comput. Commun.},
  title        = {“MystifY”: A proactive moving-target defense for a resilient SDN controller in software defined CPS},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task offloading in vehicular edge computing networks via
deep reinforcement learning. <em>COMCOM</em>, <em>189</em>, 193–204. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rapid increase of various applications in vehicular networks , it is crucial to consider a flexible architecture to improve the Quality of Service (QoS). Utilizing Multi-access Edge Computing (MEC) as a distributed paradigm, with resource capabilities closer to the vehicles, would be a promising solution to reduce response time in such a network. However, MEC suffers from limited resources and is deprived of handling high mobilities with many diverse applications. This paper proposes cooperation between MEC and central cloud decisions for different vehicular application offloading. We formulate a new resource allocation problem to guarantee the required response time. To solve such an NP-hard problem, we utilize deep reinforcement learning , a proper computational model , to automatically learn the dynamics of the network state and rapidly capture an optimal solution. Extensive numerical analysis and results illustrate how our proposed scheme can achieve a high acceptance rate with a low response time.},
  archive      = {J_COMCOM},
  author       = {Elham Karimi and Yuanzhu Chen and Behzad Akbari},
  doi          = {10.1016/j.comcom.2022.04.006},
  journal      = {Computer Communications},
  pages        = {193-204},
  shortjournal = {Comput. Commun.},
  title        = {Task offloading in vehicular edge computing networks via deep reinforcement learning},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mitigation of black hole attacks in 6LoWPAN RPL-based
wireless sensor network for cyber physical systems. <em>COMCOM</em>,
<em>189</em>, 182–192. (<a
href="https://doi.org/10.1016/j.comcom.2022.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cyber–physical system (CPS) classically comprises the physical components, computational units, the controller, and the communication network. The Wireless sensor networks (WSNs) is a key component in the CPS and connects the distributed sensors for computation and communication. As the usage of such networks increases, the attack surface also increases and mechanisms for mitigating security threats must be developed. The nodes in the WSN are low-powered devices that cannot host traditional security systems. Moreover, specialized architectures of such networks mean that new attacks specifically targeting such architectures would be discovered. The proposed work introduces a security mechanism for black hole attack in Ripple Routing Protocol (RPL) networks, utilizing features such as IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN) network discovery. The state-of-the-art mechanisms existing today for such threats are either heavyweight intrusion detection systems or require nodes working in promiscuous mode . The promiscuity of nodes can be a security concern in itself, whereas large intrusion detection systems require huge processing and network overheads. The proposed work does not rely on either but utilizes a distributed timer-based mechanism to perform malicious node detection. The work has been evaluated using the Cooja simulator, and it has been seen that it can detect black holes with high accuracy resulting in a decrease in packet loss . The true positive rate can reach up to 100\%.},
  archive      = {J_COMCOM},
  author       = {Deepak Kumar Sharma and Sanjay K. Dhurandher and Shubham Kumaram and Koyel Datta Gupta and Pradip Kumar Sharma},
  doi          = {10.1016/j.comcom.2022.04.003},
  journal      = {Computer Communications},
  pages        = {182-192},
  shortjournal = {Comput. Commun.},
  title        = {Mitigation of black hole attacks in 6LoWPAN RPL-based wireless sensor network for cyber physical systems},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent deep fusion network for urban traffic flow
anomaly identification. <em>COMCOM</em>, <em>189</em>, 175–181. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel deep learning architecture for identifying outliers in the context of intelligent transportation systems . The use of a convolutional neural network with an efficient decomposition strategy is explored to find the anomalous behavior of urban traffic flow data. The urban traffic flow data set is decomposed into similar clusters, each containing homogeneous data. The convolutional neural network is used for each data cluster. In this way, different models are trained, each learned from highly correlated data . A merging strategy is finally used to fuse the results of the obtained models. To validate the performance of the proposed framework, intensive experiments were conducted on urban traffic flow data. The results show that our system outperforms the competition on several accuracy criteria.},
  archive      = {J_COMCOM},
  author       = {Youcef Djenouri and Asma Belhadi and Hsing-Chung Chen and Jerry Chun-Wei Lin},
  doi          = {10.1016/j.comcom.2022.03.021},
  journal      = {Computer Communications},
  pages        = {175-181},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent deep fusion network for urban traffic flow anomaly identification},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and performance evaluation of QoE/QoS-oriented scheme
for reliable data transmission in internet of things environments.
<em>COMCOM</em>, <em>189</em>, 158–174. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) technology has become the focus of many researchers, and quality of service (QoS) is considered one of the most important challenges for IoT technology. Additionally, the expectations of end users in the IoT environment are of great importance, which means that quality of experience (QoE) evaluations are significant. Therefore, this paper proposed a scheme to guarantee QoS in IoT environments. For this scheme, the most efficient QoS methodology was selected based on the data and network types in the IoT environment. The proposed QoS scheme classified the data transmitted through IoT systems into two main statuses. In the first status, called “individual”, the data were exchanged through one type of large-scale network at a time. In the second status, “overlapped”, the data passed through an area that comprised a mixture of different networks. The proposed scheme handled the QoS issue for both statuses. A QoE scheme evaluating the IoT data using different layers was also proposed. The proposed QoS and QoE schemes were then combined to create a more generalized QoE/QoS-oriented scheme, which was used to adapt the QoS scheme to achieve the highest efficiency. The performance of the QoE/QoS-oriented scheme was measured in a simulation environment that was constructed using an NS-3 package. First, the performance of the proposed QoS scheme was measured by two groups of metrics. The first group was general and comprised throughput, delay, and energy consumption. The second group was particular to the proposed QoS scheme and comprised transformation between individual and overlapped statuses, usage percentages for each QoS model, and the degree of complexity of the overlapped areas. The first group of metrics showed that the proposed QoE/QoS scheme had a higher efficiency than that of the previous individual QoS models. The second group of metrics showed that the features of the proposed QoS scheme worked efficiently. Finally, delay, energy consumption, throughput, and jitter were improved after the QoS methodologies were adapted using the proposed QoE scheme.},
  archive      = {J_COMCOM},
  author       = {Omar Said},
  doi          = {10.1016/j.comcom.2022.03.020},
  journal      = {Computer Communications},
  pages        = {158-174},
  shortjournal = {Comput. Commun.},
  title        = {Design and performance evaluation of QoE/QoS-oriented scheme for reliable data transmission in internet of things environments},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended indirect controller-legacy switch forwarding for
link discovery in hybrid multi-controller SDN. <em>COMCOM</em>,
<em>189</em>, 148–157. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though widely perceived as a promising network architecture , Software Defined Networks (SDN) have inherent deployment issues, particularly in terms of a full SDN deployment paving the path for Hybrid Software Defined Networks (hSDN) and subsequently Multi-Controller h-SDN (MC-hSDN) to address limited scalability of the former. Networks allocate resources to applications based on their requirements which are taken care of by the controller. Existing link discovery protocols such as Link Layer Discovery Protocol (LLDP) and Broadcast Domain Discovery Protocol (BDDP) are able to detect links only for OpenFlow switch (OFS) in direct and indirect manners while Indirect Controller Legacy Forwarding (ICLF) scheme detects all links in h-SDN only for a single-controller environment, thus rendering link detection in MC-hSDN incomplete. This paper proposes a novel Extended ICLF (E-ICLF) scheme to fetch the topology in a multi-controller environment. A solitary PACKET_OUT (P out ) is emanated from every domain controller which circulates across the domain’s as nodes appending information which gets dispatched to the initiating controller. Experiments performed with Mininet indicate ports detection improvement and PACKET_ IN (P in ) message exchange reduction by an average of about (0.8\%–44.8\%) and (56.8\%–93.2\%) respectively with a fewer overhead of forwarded messages and convergence time to build the topology.},
  archive      = {J_COMCOM},
  author       = {Mir Wajahat Hussain and Mohammad S. Khan and K. Hemant Kumar Reddy and Diptendu Sinha Roy},
  doi          = {10.1016/j.comcom.2022.03.017},
  journal      = {Computer Communications},
  pages        = {148-157},
  shortjournal = {Comput. Commun.},
  title        = {Extended indirect controller-legacy switch forwarding for link discovery in hybrid multi-controller SDN},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State-of-the-art solutions of blockchain technology for data
dissemination in smart cities: A comprehensive review. <em>COMCOM</em>,
<em>189</em>, 120–147. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a smart city has been introduced to address the shortfalls of urbanization. The aid of technologies in smart cities is envisioned to be the key initiative to mitigate these challenges. However, the technologies exposure imposes challenging issues for the traditional centralized data dissemination schemes in smart cities. Consequently, smart city is undergoing a transition to a decentralized system through the implementation of blockchain technology . This paper presents an extensive survey of blockchain applicability for data dissemination in smart cities. We focus on the components of smart: transportation, healthcare, education, energy and building in smart cities and analyze state-of-the-art work on the integration of these components with blockchain. We discuss the advantages and shortcomings of existing literature and evaluate the performance efficiency and the extent of security and privacy achieved. The analysis leads to our new design of a secure smart city data dissemination framework using blockchain technology . To the best of our knowledge, this is the first comprehensive abstraction in the literature that can be utilized for the designation of future blockchain-smart cities data dissemination schemes with the inclusion of participating entities. Finally, we present open research issues and challenges and discuss future research directions.},
  archive      = {J_COMCOM},
  author       = {Nur Fadhilah Mohd Shari and Amizah Malip},
  doi          = {10.1016/j.comcom.2022.03.013},
  journal      = {Computer Communications},
  pages        = {120-147},
  shortjournal = {Comput. Commun.},
  title        = {State-of-the-art solutions of blockchain technology for data dissemination in smart cities: A comprehensive review},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Dynamic link prediction method of task and user in mobile
crowd sensing. <em>COMCOM</em>, <em>189</em>, 110–119. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem that the existing Mobile Crowd Sensing task allocation methods only considering the perspective of sensing users, ignoring the potential relationship between tasks and users, this paper proposes a task allocation method based on link prediction. First, we analyze the characteristics of sensing users and sensing tasks in Mobile Crowd Sensing, especially the common attribute characteristics of both, and establish the Mobile Crowd Sensing knowledge graph. Then the deep link relationship between sensing users and sensing tasks is mined by the link prediction method based on knowledge graph reasoning. Finally, according to the predicted fit between users and tasks, high-quality sensing users that meet the requirements of the task are selected, to improve the perceptual quality . The experiment results on different datasets show that the method proposed in this paper is high-quality to other comparison methods in sensing data quality and coverage.},
  archive      = {J_COMCOM},
  author       = {Jian Wang and Jia Liu and Guosheng Zhao},
  doi          = {10.1016/j.comcom.2022.03.014},
  journal      = {Computer Communications},
  pages        = {110-119},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic link prediction method of task and user in mobile crowd sensing},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reinforcement learning agent for mixed-numerology
interference-aware slice spectrum allocation with non-deterministic and
deterministic traffic. <em>COMCOM</em>, <em>189</em>, 100–109. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {5G RAN slicing is an essential tool to support the simultaneous coexistence of enhanced mobile broadband (eMBB) and ultra-reliable low-latency communications (URLLC) network slices on a shared mixed-numerology physical layer . Moreover, due to recent advance of the private network paradigm, RAN slicing assumes a central role to provide dedicated radio coverage to industry 4.0 applications as a standalone RAN. Unlike the stochastic traffic behavior characterizing URLLC slices in classical mobile networks, industrial networks support URLLC services with deterministic and periodic traffic patterns. Based on this alternative network characterization, we design a deep reinforcement learning (DRL) agent that simultaneously provides a spectrum allocation fulfilling the eMBB and URLLC service requirements and mitigates the inter-numerology interference (INI). Furthermore, by exploiting the information about the deterministic traffic patterns, we specialize the agent reward function to improve the spectrum allocation effectiveness for URLLC slices deployed in industrial environments. We assess the agent performance with respect to resource allocation schemes that are INI agnostic. Results reveal that the proposed solution outperforms the benchmark schemes in terms of service provisioning performance in both network scenarios (e.g. mobile and industrial) and showcase the benefit of INI mitigation .},
  archive      = {J_COMCOM},
  author       = {Marco Zambianco and Giacomo Verticale},
  doi          = {10.1016/j.comcom.2022.03.010},
  journal      = {Computer Communications},
  pages        = {100-109},
  shortjournal = {Comput. Commun.},
  title        = {A reinforcement learning agent for mixed-numerology interference-aware slice spectrum allocation with non-deterministic and deterministic traffic},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data detection in decentralized and distributed massive MIMO
networks. <em>COMCOM</em>, <em>189</em>, 79–99. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to meet the user demands in performance and quality of services (QoS) for beyond fifth generation (B5G) communication systems , research on decentralized and distributed massive multiple-input multiple-output (M-MIMO) is initiated. Data detection techniques are playing a crucial role in realization and implementation of M-MIMO networks. Although most of detection techniques were proposed for centralized M-MIMO, there is a notable trend to propose efficient detection techniques for decentralized and distributed M-MIMO networks. This paper aims to provide insights on data detection techniques for decentralized and distributed M-MIMO to generalists of wireless communications . We garner the detection techniques for decentralized and distributed M-MIMO and present their performance, computational complexity , throughput, and latency so that a reader can find a distinction between different algorithms from a wider range of solutions. We present the detection techniques based on the following architectures: decentralized baseband processing (DBP), feedforward fully decentralized (FD), and feedforward partially decentralized (PD), FD based on coordinate descent (FD-CD), and FD based on recursive methods. In addition, the role of expectation propagation algorithm (EPA) in decentralized architectures is comprehensively reviewed. In each section, we also discuss the pros, cons, throughput, latency, performance, and complexity profile of each detector and related implementations. Moreover, the energy efficiency of several decentralized M-MIMO architectures is also illustrated. The cell-free M-MIMO (CF-M-MIMO) architecture is discussed with an overview of deployed detection schemes. This paper also illustrates the challenges and future research directions in decentralized and distributed M-MIMO networks.},
  archive      = {J_COMCOM},
  author       = {Mahmoud A. Albreem and Alaa Alhabbash and Ammar M. Abu-Hudrouss and Tarik Adnan Almohamad},
  doi          = {10.1016/j.comcom.2022.03.015},
  journal      = {Computer Communications},
  pages        = {79-99},
  shortjournal = {Comput. Commun.},
  title        = {Data detection in decentralized and distributed massive MIMO networks},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). On the properties of device-free multi-point CSI
localization and its obfuscation. <em>COMCOM</em>, <em>189</em>, 67–78.
(<a href="https://doi.org/10.1016/j.comcom.2022.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Channel State Information (CSI) as a means of sensing the environment through Wi-Fi communications, and in particular to locate the position of unaware people, was proven feasible several years ago and now it is moving from feasibility studies to high precision applications, thus posing a serious threat to people’s privacy in workplaces, at home, and maybe even outdoors. The work we present in this paper explores how the use of multiple localization receivers can enhance the precision and robustness of device-free CSI-based localization with a method based on a state-of-the-art Convolutional Neural Network . Furthermore, we explore the effect of the inter-antenna distance on localization, both with multiple receivers and with a single MIMO receiver. Next we discuss how a randomized pre-filtering at the transmitter can hide the information that the CSI carries on the location of one person indoor. We formalize the pre-filtering as a per-frame, per-subcarrier amplitude multiplication based on a Markovian stochastic process , and we discuss different signal clipping and smoothing methods highlighting the existence of a trade-off between communication performance and obfuscation efficiency. The methodology can in any case guarantee almost unhampered communications with very good localization obfuscation. Results are presented discussing two different ways of exploiting the multi-receiver or multi-antenna redundancy and how, in any case, properly randomized pre-distortion at the transmitter can prevent localization even if the attack is carried out with multiple localization devices (receivers controlled by the attacker) and not only with a multi-antenna (MIMO) receiver.},
  archive      = {J_COMCOM},
  author       = {Marco Cominelli and Francesco Gringoli and Renato Lo Cigno},
  doi          = {10.1016/j.comcom.2022.03.011},
  journal      = {Computer Communications},
  pages        = {67-78},
  shortjournal = {Comput. Commun.},
  title        = {On the properties of device-free multi-point CSI localization and its obfuscation},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Device scheduling and channel allocation for
energy-efficient federated edge learning. <em>COMCOM</em>, <em>189</em>,
53–66. (<a href="https://doi.org/10.1016/j.comcom.2022.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Edge Learning (FEEL) is a promising distributed machine learning paradigm in the era of edge intelligence , which supports to learn the knowledge in the dataset on the premise of protecting users’ privacy. However, this learning paradigm has a big defect. As the training process is carried out at the user end and is very power-consuming, the learning task is a serious challenge for mobile devices with limited battery capacity, which may also hinders the implementation of FEEL. In practical applications, FEEL usually needs to comply with the requirements for training delay and model performance, and may also be affected by the inter-cell interference which is common in the cellular networks . However, the current works only consider the demand for training delay. In this paper, we consider the implementation of FEEL in a general cellular network, and propose an empirical assumption to characterize the relationship between model performance and training data, based on which, a workload constraint is added to the formulated problem to guarantee the model performance. For the formulated problem that contains a summation term of integral variables and an interference term with complex structure at the denominator of the objective function, we propose a device scheduling and channel allocation strategy, also called double-greedy strategy, to obtain its suboptimal solution with low computational complexity . Simulation results verify the advancement of our proposed strategy relative to the existing works, that is, achieving the best energy efficiency on the premise of ensuring the model performance. This advancement makes our strategy more flexible to satisfy the possible various requirements of service providers for model performance.},
  archive      = {J_COMCOM},
  author       = {Youqiang Hu and Hejiao Huang and Nuo Yu},
  doi          = {10.1016/j.comcom.2022.03.008},
  journal      = {Computer Communications},
  pages        = {53-66},
  shortjournal = {Comput. Commun.},
  title        = {Device scheduling and channel allocation for energy-efficient federated edge learning},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards achieving efficient access control of medical data
with both forward and backward secrecy. <em>COMCOM</em>, <em>189</em>,
36–52. (<a href="https://doi.org/10.1016/j.comcom.2022.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare service providers store the patients’ electronic medical records in the cloud in order to provide high quality healthcare services. Patients’ sensitive data is typically encrypted before storing in the cloud. However, it gives rise to a new challenge, namely access control over encrypted data . Attribute-Based Encryption (ABE) is a promising cryptographic technique to achieve the fine grained access control on outsourced encrypted data . Traditional ABE schemes assume a fixed access policy that is not suitable for the present dynamic environment. Although some ABE schemes with a dynamic access control policy have been proposed in the literature, these schemes have not addressed forward security, backward security, and user revocation after a policy update. In this paper, we propose an ABE scheme that supports access policy updates, and also provides forward security, backward security and user revocation at the same time. We have formally shown that the proposed scheme is Chosen Plaintext Attack (CPA)-secure under the Decisional Bilinear Diffie–Hellman (DBDH) assumption. Finally, the performance analysis exhibits that the proposed scheme is efficient in communication and computation, and is also suitable for resource constrained devices.},
  archive      = {J_COMCOM},
  author       = {Suryakanta Panda and Samrat Mondal and Rinku Dewri and Ashok Kumar Das},
  doi          = {10.1016/j.comcom.2022.03.001},
  journal      = {Computer Communications},
  pages        = {36-52},
  shortjournal = {Comput. Commun.},
  title        = {Towards achieving efficient access control of medical data with both forward and backward secrecy},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Source node detection in social networks based on trust–GMLA
algorithm. <em>COMCOM</em>, <em>189</em>, 28–35. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of source node detection when privacy information leakage occurs in social networks, especially in the case of asynchronous propagation and low forwarding rate, this paper proposes a single source node detection algorithm named Trust–GMLA(Trust-Gradient Maximum Likelihood Algorithm) based on trust network and GMLA(gradient maximum likelihood algorithm). According to the randomness of asynchronous propagation, the hysteresis matrix of GMLA algorithm is reconstructed, and the matrix parameters are automatically adjusted according to the forwarding rate, which improves the success rate of asynchronous diffusion source node detection. Also, aiming at the problem that a large number of repeated paths need to be calculated in GMLA, a trust inference matrix is proposed to infer the trust value between nodes, determine the trust propagation path , reduce unnecessary path calculation, and improve the speed of the algorithm. Experiments on several social networks datasets show that the proposed Trust–GMLA algorithm can effectively improve the source node detection accuracy and speed.},
  archive      = {J_COMCOM},
  author       = {Xueqin Zhang and Yan Wang and Weiyi Zhu and Gang Liu and Chunhua Gu},
  doi          = {10.1016/j.comcom.2022.02.024},
  journal      = {Computer Communications},
  pages        = {28-35},
  shortjournal = {Comput. Commun.},
  title        = {Source node detection in social networks based on Trust–GMLA algorithm},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware environment monitoring to support LPWAN-based
battlefield applications. <em>COMCOM</em>, <em>189</em>, 18–27. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of IoT-related technologies is growing in several areas. Applications of environmental monitoring, logistics, smart cities are examples of applications that benefit from advances in IoT . In the military context, IoT applications can support the decision-making process by delivering information collected directly from the battlefield to Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) systems. Taking the benefit of the installed IoT network in the battlefield, the use of the data collected by the IoT nodes is a way to improve resiliency and increase the survivability of networks, as well as to optimize the use of available resources. Towards improving the communication network present on the battlefield, this work presents a context-aware environmental monitoring system that uses real-time battlefield information to increase military networks’ resilience and survivability . The proposed approach is validated by a proof-of-concept experiment. The obtained results show that the implementation of this system can improve the communication process even when the network is exposed to unfavorable climatic factors.},
  archive      = {J_COMCOM},
  author       = {Guilherme Rotth Zibetti and Juliano Araujo Wickboldt and Edison Pignaton de Freitas},
  doi          = {10.1016/j.comcom.2022.02.020},
  journal      = {Computer Communications},
  pages        = {18-27},
  shortjournal = {Comput. Commun.},
  title        = {Context-aware environment monitoring to support LPWAN-based battlefield applications},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cache management in content delivery networks using the
metadata of online social networks. <em>COMCOM</em>, <em>189</em>,
11–17. (<a href="https://doi.org/10.1016/j.comcom.2022.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of requests on content delivery networks (CDN) originating from the online social networks (OSN) by sharing the content weblink increases according to recorded data. The sequence of the OSN originated requests shows temporal burstiness with a typical interval shorter than the ordinary requests. We consider CDN and OSN as a multilayer network and exploit the average spreading power of each user in the OSN to predict the temporal pattern of the corresponding consecutive social requests that may originate from this user to improve the underlying cache management mechanism. The traditional least recently used (LRU) content replacement algorithm uses the statistical popularity of contents to increase the cache’s hit ratio. We propose LRU-Social, which defers the eviction of social requests for a specific amount of time to take advantage of the possible burstiness in the underlying interval without missing the popular contents’ hits. We model the content link sharing by the susceptible–infected–recovered (SIR) spreading process in the underlying OSN to compute the user spreading power. We provide numerical studies for synthetic streams consisting of ordinary requests that follow Zipf’s popularity model and social requests to justify the effectiveness of the LRU-Social compared to the LRU.},
  archive      = {J_COMCOM},
  author       = {Abdorasoul Ghasemi and Amirhosein Ahmadi},
  doi          = {10.1016/j.comcom.2022.02.021},
  journal      = {Computer Communications},
  pages        = {11-17},
  shortjournal = {Comput. Commun.},
  title        = {Cache management in content delivery networks using the metadata of online social networks},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovery privacy threats via device de-anonymization in
LoRaWAN. <em>COMCOM</em>, <em>189</em>, 1–10. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN (Long Range WAN) is one of the well-known emerging technologies for the Internet of Things (IoT). Many IoT applications involve simple devices that transmit their data toward network gateways or access points that, in their turn, redirect data to application servers . While several security issues have been addressed in the LoRaWAN specification v1.1, there are still some aspects that may undermine privacy and security of the interconnected IoT devices. In this paper, we tackle a privacy aspect related to LoRaWAN device identity. The proposed approach, by monitoring the network traffic in LoRaWAN, is able to derive, in a probabilistic way, the unique identifier of the IoT device from the temporal address assigned by the network. In other words, the method identifies the relationship between the LoRaWAN DevAddress and the device manufacturer DevEUI. The proposed approach, named DEVIL (DEVice Identification and privacy Leakage), is based on temporal patterns arising in the packets transmissions . The paper presents also a detailed study of two real datasets: i) one derived by IoT devices interconnected to a prominent network operator in Italy; ii) one taken from the literature (the LoED dataset in Bhatia et al. (2020)). DEVIL is evaluated on the first dataset while the second is analyzed to support the hypothesis under the DEVIL operation. The results of our analysis, compared with other literature approaches, show how device identification through DEVIL can expose IoT devices to privacy leakage. Finally, the paper also provides some guidelines to mitigate the user re-identification threats.},
  archive      = {J_COMCOM},
  author       = {Pietro Spadaccino and Domenico Garlisi and Francesca Cuomo and Giorgio Pillon and Patrizio Pisani},
  doi          = {10.1016/j.comcom.2022.02.017},
  journal      = {Computer Communications},
  pages        = {1-10},
  shortjournal = {Comput. Commun.},
  title        = {Discovery privacy threats via device de-anonymization in LoRaWAN},
  volume       = {189},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing pollutant emissions through virtual traffic lights.
<em>COMCOM</em>, <em>188</em>, 167–177. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the key goals of urban policies, the reduction of traffic congestion and pollutant emissions surely tops the list. Although solutions leveraging vehicular communication , such as GLOSA, have been proposed to smooth traffic at regulated intersections, cities normally have a large number of unregulated intersections where queues can build up, worsening emissions caused by the stop-and-go motion of vehicles. The problem is further compounded by the future presence of self-driving cars, where the need to coordinate with other autonomous cars at intersections will be even greater. In this paper, we propose V 3 3 TL, a Vehicle-to-Vehicle (V2V) Virtual Traffic Light system for infrastructure-less unregulated crossroads. We aim at providing a low-complexity, yet effective, algorithm and communication protocol to let vehicles at an unregulated intersection decide if a virtual traffic light is needed and, in that case, self-organize to establish one. Our results highlight significant improvements compared to both unregulated and traffic light-based intersections. We tested the performance of V 3 3 TL in different, realistic scenarios (isolated or consecutive three- and four-way junctions, in single- and multi-lane configuration) and different vehicle generation rates, obtaining improvements in terms of number of passing vehicles per minute, number of stop-and-go maneuvers and scheduling fairness, when compared to unregulated or traffic-light regulated junctions.},
  archive      = {J_COMCOM},
  author       = {Ahmadreza Jame and Marco Rapelli and Claudio Casetti},
  doi          = {10.1016/j.comcom.2022.03.018},
  journal      = {Computer Communications},
  pages        = {167-177},
  shortjournal = {Comput. Commun.},
  title        = {Reducing pollutant emissions through virtual traffic lights},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cost-efficient, load-balanced and fragmentation-aware
approach for deployment of VNF service chains in elastic optical
networks. <em>COMCOM</em>, <em>188</em>, 156–166. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By emerging Network Function Virtualization (NFV), service providers use VNFs (Virtual Network Functions) and VNF service chain (VNF-SC) as an agile, scalable, and cost-efficient way for providing new network services . As traditional backbone networks fail to deal with the high volume and burstiness of traffic patterns generated by today popular services, using Elastic Optical Networks (EONs) for interconnecting data centers (inter-DC EONs) gets more attention. EONs are an ideal backbone for supporting NFV technology and transporting VNF-SC heterogeneous traffics. In this paper, we propose a solution named CELFA for the problem of VNF-SC deployment to minimize resource costs, balance resource utilization, and avoid spectrum fragmentation in EONs. At first, to find an optimal solution for saving computing and optical resources, we formulate the VNF placement problem as an Integer Quadratic Program (IQP). Then, we propose an efficient load-balanced and fragmentation-aware algorithm for the problem of Routing and Spectrum Assignment (RSA). The performance of the proposed strategy is evaluated based on resource balancing and blocking probability metrics. The simulation results verify that the proposed algorithm outperforms existing algorithms in terms of all important metrics.},
  archive      = {J_COMCOM},
  author       = {Atefeh Khatiri and Ghasem Mirjalily},
  doi          = {10.1016/j.comcom.2022.03.005},
  journal      = {Computer Communications},
  pages        = {156-166},
  shortjournal = {Comput. Commun.},
  title        = {A cost-efficient, load-balanced and fragmentation-aware approach for deployment of VNF service chains in elastic optical networks},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying the outlier in tunnel monitoring data: An
integration model. <em>COMCOM</em>, <em>188</em>, 145–155. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural health monitoring (SHM) system based on the Internet of Things is an important method to evaluate the safety of tunnel operation through the real-time monitoring data analysis. Identifying the outlier in SHM data is a non-trivial task but it is challenging for the tunnel engineers because the measurements are quite complicated with the characteristics of time series, unlabeled, high-dimensional, inter-correlations between variables, etc. To detect the outliers, an integration model is developed based on the independent data analysis from the Probabilistic, Proximity-Based (global), Proximity-Based (local), Linear Model and Outlier Ensembles. The model is examined with the shuttle data set in the University of California Irvine (UCI) database and its precision rate is up to 94.5\%, highlighting the favorable performance in identifying the outliers. This method is thus applied to the outlier detection of the SHM data in the Nanjing Yangtze River tunnel. 6698 data sets collected from SHM are evaluated and 270 groups of outliers are identified effectively. By eliminating these outliers, comparisons between the proposed integrated model and the single model (i.e. IForest, ABOD, KNN, LOF) are further conducted to discuss the model performance based on the regression analysis. Results show that the integrated model is better than the single model and it possesses the great potential to detect the outliers in SHM system.},
  archive      = {J_COMCOM},
  author       = {Jinquan Liu and Tongtong Zou},
  doi          = {10.1016/j.comcom.2022.03.002},
  journal      = {Computer Communications},
  pages        = {145-155},
  shortjournal = {Comput. Commun.},
  title        = {Identifying the outlier in tunnel monitoring data: An integration model},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrusion detection in cyber–physical environment using
hybrid naïve bayes—decision table and multi-objective evolutionary
feature selection. <em>COMCOM</em>, <em>188</em>, 133–144. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are motivated to build effective Intrusion Detection Systems because of the implications of malicious actions in computing, communication, and cyber–physical systems (IDSs). In order to develop signature-based intrusion detection techniques that are suitable for use in cyber–physical environments, state-of-the-art supervised learning algorithms are devised. The main contribution of this research is the introduction of a signature-based intrusion detection model that is based on a hybrid Decision Table and Naive Bayes technique. In addition, the contribution of the suggested method is evaluated by comparing it to the existing literature in the field. In the preprocessing stage , Multi-Objective Evolutionary Feature Selection (MOEFS) feature selection has been used to select only five attack features from the recent CICIDS017 dataset. Keeping in view the class imbalance nature of CICIDS2017 dataset, adequate attack samples has been selected with more weightage to the attack classes having a smaller number of instances in the dataset. A hybrid of Decision Table and Naive Bayes models were combined to train and detect intrusions. Detection of botnets , port scans, Denial of Service (DoS)/Distributed Denial of Service (DDoS) attacks, such as Golden-Eye, Hulk, Slow httptest, slowloris, Heartbleed, Brute Force attacks, such as Patator (FTP), Patator (SSH), and Web attacks such as Infiltration , Web Brute Force, SQL Injection, and XSS, are all successfully detected by the proposed hybrid detection model. The proposed approach shows an accuracy of 96.8\% using five features of CICIDS2017, which is higher than the accuracy of methods discussed in the literatures.},
  archive      = {J_COMCOM},
  author       = {Ranjit Panigrahi and Samarjeet Borah and Moumita Pramanik and Akash Kumar Bhoi and Paolo Barsocchi and Soumya Ranjan Nayak and Waleed Alnumay},
  doi          = {10.1016/j.comcom.2022.03.009},
  journal      = {Computer Communications},
  pages        = {133-144},
  shortjournal = {Comput. Commun.},
  title        = {Intrusion detection in cyber–physical environment using hybrid naïve Bayes—Decision table and multi-objective evolutionary feature selection},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reliable cardinality estimation for missing tags over a
noisy channel. <em>COMCOM</em>, <em>188</em>, 125–132. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of missing tags is an important issue for administration of RFID systems. The existing cardinality estimation schemes assume a perfect tag-to-reader channel to estimate the number of missing tags. As the wireless tag-to-reader channel is not error-free, this paper considers the problem of estimating the number of missing tags over a noisy tag-to-reader channel. Given the symmetric bit error rate as input , the RMTE (Reliable Missing Tag Estimation) protocol presented in this paper estimates the number of missing tags by checking whether the first assumed non-empty slot in the virtual frame becomes an empty slot in the response frame. Then we analyze the RMTE protocol under the random error model to demonstrate that the specified accuracy and reliable requirements can be satisfied by a suitable choice of the number of rounds to be executed. Compared with the related cardinality estimation schemes for missing tags, simulations results show that the presented RMTE scheme can yield an accurate and reliable estimation for the number of missing tags under the random error model.},
  archive      = {J_COMCOM},
  author       = {Bin Wang and Guoqing Duan},
  doi          = {10.1016/j.comcom.2022.03.007},
  journal      = {Computer Communications},
  pages        = {125-132},
  shortjournal = {Comput. Commun.},
  title        = {A reliable cardinality estimation for missing tags over a noisy channel},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource allocation based on multi-grouping and frame
expansion for NOMA backscatter communication network. <em>COMCOM</em>,
<em>188</em>, 117–124. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) and Backscatter Communication (BackCom) have great development potential in the Internet of Things (IoT). However, the power regulation and energy harvesting capacity of backscatter node (BN) in backscatter network are limited, and it is necessary to explore an appropriate resource allocation scheme to combine NOMA with BackCom. This work presents a resource allocation scheme based on multi-grouping and frame expansion to ensure reliable communication. The BNs are grouped twice to improve the total power by setting more accurate and appropriate reflection coefficients for BNs. Considering the difference of energy harvesting efficiency between BNs, different time slot lengths are set to ensure the energy supply of BNs, and the selection criterion of minimum time slot length is given. The experimental results show that the proposed scheme can effectively improve system throughput and decoding performance.},
  archive      = {J_COMCOM},
  author       = {Shibao Li and Quanyu Li and Jianhang Liu and Tingpei Huang and Xuerong Cui},
  doi          = {10.1016/j.comcom.2022.03.004},
  journal      = {Computer Communications},
  pages        = {117-124},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation based on multi-grouping and frame expansion for NOMA backscatter communication network},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud-gaming: Analysis of google stadia traffic.
<em>COMCOM</em>, <em>188</em>, 99–116. (<a
href="https://doi.org/10.1016/j.comcom.2022.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive, real-time, and high-quality cloud video games pose a serious challenge to the Internet due to simultaneous high-throughput and low round trip delay requirements. In this paper, we investigate the traffic characteristics of Stadia, the cloud-gaming solution from Google, which is likely to become one of the dominant players in the gaming sector. To do that, we design several experiments, and perform an extensive traffic measurement campaign to obtain all required data. Our first goal is to gather a deep understanding of Stadia traffic characteristics by identifying the different protocols involved for both signaling and video/audio contents, the traffic generation patterns, and the packet size and inter-packet time probability distributions. Then, our second goal is to understand how different Stadia games and configurations, such as the video codec and the video resolution selected, impact on the characteristics of the generated traffic. We also evaluate the ability of Stadia to adapt to different link capacity conditions, including cases where the capacity drops suddenly, as well as sudden increases in the network latency . Our results and findings, besides illustrating the characteristics of Stadia traffic, are also valuable for planning and dimensioning future networks, as well as for designing new resource management strategies. Finally, we compare Stadia traffic to other video streaming applications, showcasing the main differences between them, and introduce a traffic model using our captures. We show that this model can be used in simulations to further investigate the network performance in presence of Stadia traffic.},
  archive      = {J_COMCOM},
  author       = {Marc Carrascosa and Boris Bellalta},
  doi          = {10.1016/j.comcom.2022.03.006},
  journal      = {Computer Communications},
  pages        = {99-116},
  shortjournal = {Comput. Commun.},
  title        = {Cloud-gaming: Analysis of google stadia traffic},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved communication resource allocation strategy for
wireless networks based on deep reinforcement learning. <em>COMCOM</em>,
<em>188</em>, 90–98. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of 5G networks , user demand for high-speed, low-latency, and high-reliability services continues to grow. When traditional communication technologies cannot meet the needs, wireless network LoRa technology has emerged. Although LoRa has low power consumption , Long-distance, and other advantages, terminal nodes still face frequent data collection and energy consumption issues how to more efficiently combine the deep reinforcement learning method for LoRa wireless network communication and allocate resources reasonably and effectively. This paper proposes a communication channel resource allocation strategy based on deep reinforcement learning , the CL-LoRa strategy. It uses extended preamble and low-power interception technologies to achieve on-demand synchronization and low-power communication. The basic idea of this strategy is to detect channel quality based on CAD , coordinate node scheduling, and wireless channel allocation. The node will choose different ways to acquire the channel according to the current network load, namely CSMA-CA competition and dynamic duty cycle communication. In this way, the channel utilization rate is improved, and the energy consumption problem of the long-distance communication data volume is perfectly solved. The duty cycle access method is based on the imbalance of energy consumption in the Internet of Things . It uses the remaining energy of the remote central node to dynamically adjust the duty cycle of the node, wake up the working time of the node, and send more beacons to the sleeping node. Reduce the sleep delay of the node. Through theoretical analysis of CL-LoRa protocol performance, compared with DDC-LoRa protocol and ADC-LoRa protocol, CL-LoRa protocol can increase channel utilization by 9\%, reduce terminal energy consumption by 1.6\%, and increase throughput by 1.5\%.},
  archive      = {J_COMCOM},
  author       = {Ting Xu and Ming Zhao and Xin Yao and Yusen Zhu},
  doi          = {10.1016/j.comcom.2022.02.018},
  journal      = {Computer Communications},
  pages        = {90-98},
  shortjournal = {Comput. Commun.},
  title        = {An improved communication resource allocation strategy for wireless networks based on deep reinforcement learning},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IADF-CPS: Intelligent anomaly detection framework towards
cyber physical systems. <em>COMCOM</em>, <em>188</em>, 81–89. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–Physical Systems (CPSs) becoming one of the most complex, intelligent, and sophisticated system. Ensuring security is an important aspect towards CPSs. However, increase in sophisticated and complexity attacks in CPSs, the conventional anomaly detection methods are facing problems and also growth in volume of data becomes challenging which requires domain specific knowledge that could be applied directly to analyze these challenges. In order to overcome this problem, various deep learning based anomaly detection system is developed. In this research, we propose an anomaly detection approach by integration of intelligent deep learning technique named Convolutional Neural Network (CNN) with Kalman Filter (KF) based Gaussian-Mixture Model (GMM). The proposed model is used for identifying and detecting anomalous behavior in CPSs. This proposed framework consists of two important process. First is to pre-process the data by transforming and filtering original data into new format and achieved privacy preservation of the data. Secondly, we proposed GMM-KF integrated deep CNN model for anomaly detection and accurately estimated the posterior probabilities of anomalous and legitimate events in CPSs.},
  archive      = {J_COMCOM},
  author       = {Senthil Murugan Nagarajan and Ganesh Gopal Deverajan and Ali Kashif Bashir and Rajendra Prasad Mahapatra and Mohammed S. Al-Numay},
  doi          = {10.1016/j.comcom.2022.02.022},
  journal      = {Computer Communications},
  pages        = {81-89},
  shortjournal = {Comput. Commun.},
  title        = {IADF-CPS: Intelligent anomaly detection framework towards cyber physical systems},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improved routing protocol for raw data collection in
multihop wireless sensor networks. <em>COMCOM</em>, <em>188</em>, 66–80.
(<a href="https://doi.org/10.1016/j.comcom.2022.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks (WSNs) are an effective and efficient method for collecting data from a target area, and prolonging the lifetime of WSNs has been a focus of scientific research due to the limited energy of sensor nodes . However, traditional studies of WSNs are based on the assumption that WSNs collect aggregated data with redundant sensor nodes in an ideal radio environment. These assumptions may not be acceptable in practical applications. To address this problem, this paper introduces a novel application scenario for WSNs in which raw data are collected by a multihop network without redundant sensor nodes, and a new hybrid tree-based and cluster-based routing protocol for raw data collection (HTC-RDC) is proposed to prolong the lifetime of WSNs that can work in novel application scenarios. The experimental results demonstrate that the proposed HTC-RDC enables the WSNs to achieve their expected functions and prolongs the network lifetime by an average of 11.4\% compared to the existing protocols in the explored application scenario.},
  archive      = {J_COMCOM},
  author       = {Yangbin Zhang and Lihua Liu and Mao Wang and Jibing Wu and Hongbin Huang},
  doi          = {10.1016/j.comcom.2022.02.016},
  journal      = {Computer Communications},
  pages        = {66-80},
  shortjournal = {Comput. Commun.},
  title        = {An improved routing protocol for raw data collection in multihop wireless sensor networks},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survey on watermarking methods in the artificial
intelligence domain and beyond. <em>COMCOM</em>, <em>188</em>, 52–65.
(<a href="https://doi.org/10.1016/j.comcom.2022.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, machine/deep learning has become a promising solution for some intelligent tasks. It can be actively used for watermarking but less so for conventional tasks such as prediction, classification and regression. This article presents a comprehensive study on watermarking using trending technologies, such as artificial intelligence , machine learning and deep learning . Also, it briefly discusses the introduction of watermarking, background information and the most interesting and utilized applications. The major role of trending technologies in watermarking has also been highlighted. The contribution of the surveyed scheme is also summarized and compared for different technical perspectives. Lastly, the article highlights recent challenges and directions of potential research that could fill gaps in this area for researchers and developers.},
  archive      = {J_COMCOM},
  author       = {Preetam Amrit and Amit Kumar Singh},
  doi          = {10.1016/j.comcom.2022.02.023},
  journal      = {Computer Communications},
  pages        = {52-65},
  shortjournal = {Comput. Commun.},
  title        = {Survey on watermarking methods in the artificial intelligence domain and beyond},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-tenant resource sharing with equitable-priority-based
performance isolation of slices for 5G cellular systems.
<em>COMCOM</em>, <em>188</em>, 39–51. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is a promising technique to enable multi-tenant operation in fifth generation (5G) cellular systems . However, efficient implementation of this technique at the air interface requires an adequate resource allocation policy that provides slice isolation and takes into account the heterogeneity of services and their performance requirements. We propose a new slicing scheme aimed at slice performance isolation, efficient capacity utilization, and fair resource allocation among all users. The policy ensures a slice-specific minimum data rate to all slice users as long as their number remains within a contracted limit, yet provides higher data rates whenever capacity is available. Specifically, we incorporate the best features of the complete partitioning and complete sharing policies, while ensuring flexibility and customization for network operators. The proposed scheme is based on an iterative solution of a convex programming problem characterized by polynomial complexity , which makes it suitable for on-line implementation. Associated algorithms are also proposed. Our numerical results demonstrate that the proposed slicing scheme is capable to accommodate heterogeneous traffic and provides performance isolation of slices while maintaining resource utilization at the level of complete sharing. Depending on the system parameters, the proposed scheme allows to improve session loss probability by an order of magnitude as compared to static slicing, while keeping the user data rates comparable to that of the complete sharing scheme and improving the average user satisfaction index by up to 90\%. Overall, the proposed scheme results in efficient resource utilization compared to slicing policies in which slice isolation is provided via reservation by setting fixed upper or lower capacity bounds.},
  archive      = {J_COMCOM},
  author       = {Natalia Yarkina and Luis M. Correia and Dmitri Moltchanov and Yuliya Gaidamaka and Konstantin Samouylov},
  doi          = {10.1016/j.comcom.2022.02.019},
  journal      = {Computer Communications},
  pages        = {39-51},
  shortjournal = {Comput. Commun.},
  title        = {Multi-tenant resource sharing with equitable-priority-based performance isolation of slices for 5G cellular systems},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable routing in low-earth orbit satellite
constellations: Architecture and algorithms. <em>COMCOM</em>,
<em>188</em>, 26–38. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-Earth orbit satellite constellations (LEO-SCs) are attractive for provisioning global, high-speed and low latency Internet access services. Due to the fast movement of satellites and the lack of inter-satellite links (ISLs), the LEO-SC topology is highly dynamic. Applying shortest path routing directly to LEO-SCs may suffer from poor scalability and frequent route changes. In this paper, a scalable two-layer routing architecture is first proposed. Based on it, two stable routing algorithms , delay-bounded routing (DBR) and delay-aware routing (DAR), are designed to minimize route changes. DBR is flow-based. It provides bounded network latency but at the cost of a larger forwarding table. DAR is destination-based. Although network latency is not bounded, we show that the further reduction in route changes is significant and the increase in average latency is minimal.},
  archive      = {J_COMCOM},
  author       = {Shengyu Zhang and Kwan L. Yeung},
  doi          = {10.1016/j.comcom.2022.02.015},
  journal      = {Computer Communications},
  pages        = {26-38},
  shortjournal = {Comput. Commun.},
  title        = {Scalable routing in low-earth orbit satellite constellations: Architecture and algorithms},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Full-duplex cell-free mMIMO networks with coarse ADCs/DACs
over ricean fading for beyond 5G. <em>COMCOM</em>, <em>188</em>, 13–25.
(<a href="https://doi.org/10.1016/j.comcom.2022.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore a practical full-duplex (FD) cell-free massive multiple-input multiple-output (mMIMO) network equipped with coarse analog-to-digital converters/digital-to-analog converters (ADCs/DACs) at the access points (APs) over Ricean fading channels for beyond 5G communication. First, we suppose that the minimum mean square error scheme is applied to estimate channel state information . The accurate sum achievable rate expressions for both uplink (UL) and downlink (DL) are obtained in closed-form by performing the maximum ratio combining and conjugate beamforming at the APs, respectively. Then, the expressions of energy efficiency for UL and DL can be obtained in closed-form based on the established power dissipation model. Finally, the numerical results are conducted to demonstrate our analytical results about the spectral efficiency of considered system. It is also found that when the total number of coarse ADCs/DACs precision is fixed, employing the higher-precision ADCs at the receive end of APs can notably improve the FD sum rate of proposed networks. In addition, the simulation results also show the impacts of number of antennas at each AP and resolution of coarse ADCs/DACs on the energy efficiency (EE) performance. It is revealed that the moderate quantization bits can contribute to the optimal EE.},
  archive      = {J_COMCOM},
  author       = {Meng Wang and Dian-Wu Yue and Si-Nian Jin},
  doi          = {10.1016/j.comcom.2022.02.014},
  journal      = {Computer Communications},
  pages        = {13-25},
  shortjournal = {Comput. Commun.},
  title        = {Full-duplex cell-free mMIMO networks with coarse ADCs/DACs over ricean fading for beyond 5G},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel cryptosystem based on DNA cryptography, hyperchaotic
systems and a randomly generated moore machine for cyber physical
systems. <em>COMCOM</em>, <em>188</em>, 1–12. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a large amount of data is transmitted over the Internet and in Cyber–Physical Systems (CPS). A significant portion of this data is highly confidential. Bank account details, credit card details, One Time Passwords (OTP), financial data and other sensitive information are some examples of highly confidential data. A secure encryption and decryption technique is needed to ensure the secure transmission of data. These techniques use a secret key and guarantee that confidential data is only accessible to authorized individuals. This paper proposes a novel encryption process based on Deoxyribonucleic Acid (DNA) cryptography, a hyperchaotic system and a Moore machine . The hyperchaotic system generates four pseudo-random number sequences used in DNA-based operations. The Moore machine performs substitutions in the DNA sequence that makes the system more secure. The proposed technique can protect a system from various attacks, namely man-in-the-middle attacks, ciphertext-only attacks, known-plaintext attacks, brute force attacks and differential cryptanalysis attacks. The proposed scheme gives an average avalanche effect of 54.75\%, which guarantees a high level of robustness. Moreover, experimental results show that the proposed scheme is more secure and efficient than the existing schemes.},
  archive      = {J_COMCOM},
  author       = {Pramod Pavithran and Sheena Mathew and Suyel Namasudra and Gautam Srivastava},
  doi          = {10.1016/j.comcom.2022.02.008},
  journal      = {Computer Communications},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {A novel cryptosystem based on DNA cryptography, hyperchaotic systems and a randomly generated moore machine for cyber physical systems},
  volume       = {188},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning-based multi-objective edge
server placement in internet of vehicles. <em>COMCOM</em>, <em>187</em>,
172–180. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the typical scenario of the Internet of Vehicles (IoV), the edge servers (ESs) are laid out near the road side units (RSUs) to process the collected data for a variety of IoV services in real time. Generally, because ESs are lightweight compared with cloud servers, if the ESs are not appropriately distributed, it will cause the unbalanced workload of the ESs. Thus, developing an ES plan to avoid the risk of overload and improve the quality of service (QoS) remains a challenge. To tackle it, a deep reinforcement learning-based multi-objective edge server placement strategy, named DESP, is fully explored, to promote the coverage rate, the workload balancing and reduce the average delay of finishing tasks in the IoV. In particular, the Markov Decision Process (MDP) of the ES placement problem is formulated and the deep reinforcement learning , i.e., Deep Q-Network (DQN) is applied to obtain the optimal placement scheme achieving the multiple objectives above. At last, a real vehicular data set is used for assessing the validity of DESP.},
  archive      = {J_COMCOM},
  author       = {Jiawei Lu and Jielin Jiang and Venki Balasubramanian and Mohammad R. Khosravi and Xiaolong Xu},
  doi          = {10.1016/j.comcom.2022.02.011},
  journal      = {Computer Communications},
  pages        = {172-180},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning-based multi-objective edge server placement in internet of vehicles},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sustainable deep learning framework for fault detection in
6G industry 4.0 heterogeneous data environments. <em>COMCOM</em>,
<em>187</em>, 164–171. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of 5G and Beyond 5G (B5G)/6G in Machine-to-Machine (M2M) communications, is making Industry 4.0 smarter. However, the goal of having a sustainable self-monitored industry has not been reached yet. State-of-the-art deep learning-based Fault Detection algorithms cannot handle heterogeneous data , meaning that more than one fault detection computational device has to be used for each data format , in addition to the inability to take advantage of the combination of all the information available in different formats to derive more accurate conclusions. Moreover, these algorithms rely on inefficient hyper-parameters tuning strategies. In this paper, we propose an Advanced Deep Learning framework for Fault Diagnosis in Industry 4.0 (ADL-FDI4), which combines Long Short Term Memory (LSTM), Convolutional Neural Networks (CNN) and graph CNN (GNN), to handle heterogeneous data. Furthermore, our novel framework uses a Branch-and-Bound procedure to guide the learning process. Our experimental results show that ADL-FDI4 outperforms the state-of-the-art solutions in terms of detection rate and running time, and for that, it consumes less energy. In addition to handling heterogeneous data, which implies that one computational device is sufficient to handle all data formats.},
  archive      = {J_COMCOM},
  author       = {Tinhinane Mezair and Youcef Djenouri and Asma Belhadi and Gautam Srivastava and Jerry Chun-Wei Lin},
  doi          = {10.1016/j.comcom.2022.02.010},
  journal      = {Computer Communications},
  pages        = {164-171},
  shortjournal = {Comput. Commun.},
  title        = {A sustainable deep learning framework for fault detection in 6G industry 4.0 heterogeneous data environments},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A trust change detection mechanism in mobile ad-hoc
networks. <em>COMCOM</em>, <em>187</em>, 155–163. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The versatility and efficiency of mobile ad-hoc networks comes at a certain cost. The network nodes are exposed to a variety of attacks and at the same time they are under memory, processing and energy limitations. Once deployed, the network should operate for as long as possible, mitigating security threats and ensuring the integrity of the data exchanged. This article describes our research on the field, proposing a mechanism to identify malicious nodes and promptly alerting the network. In addition, the same mechanism is used to restore the trust of a malicious node, in case it resumes healthy operation or it was a misclassification issue. The mechanism can be fine-tuned to balance the sensitivity of the reaction and the timely detection of a status change, as required.},
  archive      = {J_COMCOM},
  author       = {Michail Chatzidakis and Stathes Hadjiefthymiades},
  doi          = {10.1016/j.comcom.2022.02.007},
  journal      = {Computer Communications},
  pages        = {155-163},
  shortjournal = {Comput. Commun.},
  title        = {A trust change detection mechanism in mobile ad-hoc networks},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-tier delay-aware load balancing strategy for 5G HC-RAN
architecture. <em>COMCOM</em>, <em>187</em>, 144–154. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifth-generation (5G) cellular network aim to provide ultra-low latency and enhance data rate service to its associated users. The centralized baseband processing of incoming traffic increases the computational complexity and end-to-end delays in a conventional C-RAN architecture. This paper proposes a multi-tier HC-RAN architecture using the edge computing concept at the Remote Radio Head (RRH) to overcome the above limitations. Further, to reduce load at the C-BBU and fronthaul, we have proposed a multi-tier delay-aware load balancing (MDALB) algorithm. The proposed algorithm is run over the EO to strategically distribute incoming flow between eRRH and C-BBU based on a load distribution ratio. The efficiency of the proposed strategy is analyzed through mathematical models and simulations based on various performance matrices. The performance parameters’ results show that the use of multi-tier processing in HC-RAN minimizes end-to-end delay and packet loss whereas increases the system throughput and packet delivery ratio .},
  archive      = {J_COMCOM},
  author       = {Byomakesh Mahapatra and Ashok Kumar Turuk and Sarat Kumar Patra},
  doi          = {10.1016/j.comcom.2022.02.012},
  journal      = {Computer Communications},
  pages        = {144-154},
  shortjournal = {Comput. Commun.},
  title        = {Multi-tier delay-aware load balancing strategy for 5G HC-RAN architecture},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel rate control algorithm for low latency video coding
base on mobile edge cloud computing. <em>COMCOM</em>, <em>187</em>,
134–143. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current low-latency video coding rate control algorithm has similar video sequence structure, low rate control accuracy, long control time, etc., which cannot meet the expectations of modern workers. Based on this, this paper proposes a mobile edge cloud computing-based Low-latency video coding rate control algorithm, optimize the original control algorithm, improve its rate control accuracy, etc. By analyzing the relationship between mobile edge computing and cloud computing , combined with edge computing in the context of cloud computing , this paper proposes a mobile edge cloud computing method and designs the mobile edge cloud computing architecture. Low latency video sequences are obtained using mobile edge cloud computing, and a mobile edge cloud computing model is established to control the update rate parameters of delayed video files. According to the principle of rate control, based on joint rate–distortion theory and frame coding complexity calculation model, the low latency video coding rate control is realized. The experimental results show that the byte data volume of the low-latency video coding sequence of the proposed method is 5653 dB. The rate control accuracy can be as high as 99.64\%, and the rate control time is 1.37 s. The proposed method has a high similarity of video sequence structure and high rate control accuracy and can effectively shorten the rate control time of low latency video coding.},
  archive      = {J_COMCOM},
  author       = {Jinlei Zhu and Houjin Chen and Pan Pan},
  doi          = {10.1016/j.comcom.2022.02.009},
  journal      = {Computer Communications},
  pages        = {134-143},
  shortjournal = {Comput. Commun.},
  title        = {A novel rate control algorithm for low latency video coding base on mobile edge cloud computing},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). URLLC in UAV-enabled multicasting systems: A dual time and
energy minimization problem using UAV speed, altitude and beamwidth.
<em>COMCOM</em>, <em>187</em>, 125–133. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realizing ultra-high reliability for short packets in sixth-generation (6G) networks is a crucial task for network designers as classical Shannonian capacity bounds become obsolete. Moreover, for multiuser communications, due to distinct aerodynamics, the trajectory design for a fixed-wing unmanned aerial vehicle (UAV) fundamentally differs from rotary-wing UAVs, which poses a challenge for systems designers. This paper addresses these challenges and relies on a fixed-wing UAV-enabled multicasting system to deliver common short blocklength ultra-reliable and low-latency (URLLC) packets to the ground nodes (GNs) using a Snake Traversal trajectory path. To accomplish this task, we consider the fly-and-communicate protocol for the UAV, where the UAV sweeps a large rectangular area to disperse a common file to GNs with obscure positions. In this vein, we investigate the dual time and energy minimization problems by presenting a quasi-optimal design of the UAV’s flying speed, altitude, and antenna beamwidth. Simulation results of numerical search reveal the optimal altitude and half-power beamwidth, which minimize the completion time and energy consumption, respectively. Moreover, for optimized beamwidth, the UAV speed monotonically increases with the altitude, whereas both the completion time and energy consumption monotonically decrease with the altitude. We also analyze the effects of the blocklength and decoding error probability on optimal UAV speed, completion time, and energy consumption. Additionally, simulation results show that for the completion time, the percentage difference between minimum UAV altitude h min hmin and maximum UAV altitude h max hmax is 195.252\%, whereas for the energy consumption, the percentage difference at h min hmin and h max hmax is 196.912\%. Similarly, for the completion time, the percentage difference at minimum UAV antenna beamwidth θ min θmin and maximum UAV antenna beamwidth θ max θmax is 181.586\%, whereas for the energy consumption the percentage difference at θ min θmin and θ max θmax is 189.269\%. This demonstrates the efficacy of the proposed technique is almost two times in reducing the completion time and the energy consumption effectively between the minimum and the maximum values of the UAV altitude and the UAV antenna beamwidth. Lastly, the results show that for both the blocklength and decoding error probability, the UAV speed monotonically increases, while completion time and energy consumption monotonically decrease.},
  archive      = {J_COMCOM},
  author       = {Ali Ranjha and Georges Kaddoum and Muddasir Rahim and Kapal Dev},
  doi          = {10.1016/j.comcom.2022.02.013},
  journal      = {Computer Communications},
  pages        = {125-133},
  shortjournal = {Comput. Commun.},
  title        = {URLLC in UAV-enabled multicasting systems: A dual time and energy minimization problem using UAV speed, altitude and beamwidth},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A first look at HTTP/3 adoption and performance.
<em>COMCOM</em>, <em>187</em>, 115–124. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The third version of the Hypertext Transfer Protocol (HTTP) is in the final standardization phase by the IETF. In addition to better security and greater flexibility, it promises performance benefits. HTTP/3 uses a more efficient header compression scheme and replaces TCP with QUIC, a transport protocol over UDP that was originally proposed by Google and is also currently being standardized. Although initial implementations of HTTP/3 already exist and some websites have announced their support, few studies have been conducted to assess its benefits. We measure the adoption and performance of HTTP/3 and show how it has been adopted by some of the leading Internet companies such as Google, Facebook, and Cloudflare in 2020. We conduct a large-scale measurement campaign on thousands of websites using HTTP/3 to understand the extent to which it outperforms HTTP/2 in web browsing applications. We find that websites using HTTP/3 often host most web page objects on third-party servers that only support HTTP/2 or even HTTP/1.1. Websites that load objects from a limited number of third-party domains are the ones that see larger performance gains. However, our experiments show that HTTP/3 offers significant benefits only in high-latency or mobile networks. Finally, we run an experimental campaign to study the impact of HTTP/3 on video streaming applications. In this direction, our results show that HTTP/3 currently does not provide benefits.},
  archive      = {J_COMCOM},
  author       = {Gianluca Perna and Martino Trevisan and Danilo Giordano and Idilio Drago},
  doi          = {10.1016/j.comcom.2022.02.005},
  journal      = {Computer Communications},
  pages        = {115-124},
  shortjournal = {Comput. Commun.},
  title        = {A first look at HTTP/3 adoption and performance},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Softwarized management of 6G network for green internet of
things. <em>COMCOM</em>, <em>187</em>, 103–114. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose 6G-SDI , a Software-Defined Network (SDN)-based green communication scheme for 6G-enabled Internet of Things (IoT) to control real-time actuation and flow-table configuration. 6G-SDI considers collisions during uplink transmissions and allocates a downlink slot to send critical alerts and commands such as CO 2 2 level in a region. The SDN controller uses probe messages to periodically update the Remote Radio Head (RRH) for synchronizing channel access and power-saving of mobile nodes among different cells. The required node address and flow-rules are transferred to an RRH as per the location of a mobile node. We adopt a Markov Chain-based approach to predict the collision in the uplink channel and location of a mobile node. RRH determines the different sleeping schedules for itself and other low-power IoT devices, as per the current demands collected from the controller and stations. We show a use case of 6G-SDI, where the SDN controller predicts CO 2 2 emissions of a vehicular network and send a set of critical alerts caring information such as CO 2 2 level and the number of vehicles. The proposed scheme helps in taking adequate actions for sending commands or alerts with improved network energy consumption, latency, and throughput. 6G-SDI reduces critical traffic latency and energy consumption up to 55\% and 62\%, respectively, compared to the existing state-of-the-art.},
  archive      = {J_COMCOM},
  author       = {Abhishek Shukla and Nurzaman Ahmed and Arijit Roy and Subhas Chandra Misra},
  doi          = {10.1016/j.comcom.2022.01.018},
  journal      = {Computer Communications},
  pages        = {103-114},
  shortjournal = {Comput. Commun.},
  title        = {Softwarized management of 6G network for green internet of things},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group acknowledgement mechanism for beacon-enabled wireless
sensor networks. <em>COMCOM</em>, <em>187</em>, 93–102. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks are network of a large number of sensor nodes used for remote applications. To the expressive growth in remote sensor networks, medium access control (MAC) works faster in wireless sensor networks. The MAC protocol has to be appropriately designed to ensure the proper operation of the sensor nodes in WSNs. This paper provides energy saving by proposing group acknowledgement MAC (GACK-MAC) for an aggregated data frame transmitted to a single destination, and reducing the overhead in IEEE 802.15.4 beacon enabled wireless sensor networks. In our proposed scheme, type of acknowledgement of each sensor node is decided by the coordinator based on traffic load of the network or number of packets present in the sensor node. We performed this by adopting the star and peer-to-peer tree topologies with a coordinator having coordination with all other member nodes. Simulations are conducted, and compared with the analytical models constructed to measure the throughput, and delay metrics. Moreover, our method enhances the battery lifetime by 35\% on average, throughput of 42\% at maximum, and decrease the delay by 33\% for each packet in network of star topology and twice the above mentioned values for battery lifetime, throughput and reduces the delay by 49\% in P2P topology of wireless sensor networks.},
  archive      = {J_COMCOM},
  author       = {S. Santhameena and Manikandan J.},
  doi          = {10.1016/j.comcom.2022.02.001},
  journal      = {Computer Communications},
  pages        = {93-102},
  shortjournal = {Comput. Commun.},
  title        = {Group acknowledgement mechanism for beacon-enabled wireless sensor networks},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Path extension similarity link prediction method based on
matrix algebra in directed networks. <em>COMCOM</em>, <em>187</em>,
83–92. (<a href="https://doi.org/10.1016/j.comcom.2022.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional link prediction methods are generally only calculated for the neighbor information of nodes, and the network path between nodes has not been fully utilized. Therefore, this paper proposes a directed network link prediction method based on path extension similarity to improve the prediction accuracy of potential edges of network nodes. Firstly, the mathematical definition of each local index is expressed in matrix form through matrix algebra ; secondly, according to the algorithm principle of global and quasi-local indices, the extension form of local indices is clarified; and the path extension of each local index is carried out respectively; finally, multiple real data sets are used to analyze the benchmark indices and extended indices. The results of the AUC and Precision evaluation metrics show that the path extension similarity proposed in this paper has higher accuracy and stronger robustness than the benchmark indices.},
  archive      = {J_COMCOM},
  author       = {Feipeng Guo and Wei Zhou and Qibei Lu and Chen Zhang},
  doi          = {10.1016/j.comcom.2022.02.002},
  journal      = {Computer Communications},
  pages        = {83-92},
  shortjournal = {Comput. Commun.},
  title        = {Path extension similarity link prediction method based on matrix algebra in directed networks},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure routing with multi-watchdog construction using deep
particle convolutional model for IoT based 5G wireless sensor networks.
<em>COMCOM</em>, <em>187</em>, 71–82. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fifth Generation (5G) security principles are widely expected with effective cryptography models, information security models, Machine Learning (ML) based Intrusion Detection systems (IDS) for Internet of Things (IoT) based Wireless Sensor Networks (WSN). However, the current security models are insufficient against the dynamic network nature of WSNs. On this scope, the proposed system develops Deep Convolutional Neural Network (DCNN) and Distributed Particle Filtering Evaluation Scheme (DPFES) for constructing a secure and cooperative multi-watchdog system. The proposed Deep Learning (DL) based dynamic multi-watchdog system protects each sensor node by monitoring the node transmission. In addition, the proposed work encompasses secure data-centric and node-centric evaluation procedures that are required for expanding the secure medium of 5G-based IoT-WSN networks. The DL-based network evaluation procedures drive the entire network to build a secure multi-watchdog system that enables on-demand active watchdog IDS agents among dense IoT-WSN. Notably, the proposed work contains a system dynamics model, cooperative watchdog model, Dual Line Minimum Connected Dominating Set (DL-MCDS), and DL-based event analysis procedures. Based on technical aspects, the proposed system is motivated to implement DPFES to analyze network events using particle filtering frameworks to build a secure 5G environment. The system is implemented and results are compared with related works. The performance of the proposed cooperative multi-watchdog system delivers 10\% and 15\% of better results than other techniques.},
  archive      = {J_COMCOM},
  author       = {S. Rajasoundaran and A.V. Prabu and Sidheswar Routray and Prince Priya Malla and G. Sateesh Kumar and Amrit Mukherjee and Yinan Qi},
  doi          = {10.1016/j.comcom.2022.02.004},
  journal      = {Computer Communications},
  pages        = {71-82},
  shortjournal = {Comput. Commun.},
  title        = {Secure routing with multi-watchdog construction using deep particle convolutional model for IoT based 5G wireless sensor networks},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning-based approach to detect threats in
bio-cyber DNA storage systems. <em>COMCOM</em>, <em>187</em>, 59–70. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data storage is one of the main computing issues of this century. Not only storage devices are converging to strict physical limits, but also the amount of data generated by users is growing at an unbelievable rate. To face these challenges, data centres grew constantly over the past decades. However, this growth comes with a price, particularly from the environmental point of view. Among various promising media, DNA is one of the most fascinating candidate. In our previous work, we have proposed an automated archival architecture which uses bioengineered bacteria to store and retrieve data, previously encoded into DNA. The similarities between biological media and classical ones can be a drawback, as malicious parties might replicate traditional attacks on the former archival system , using biological instruments and techniques. In this paper, first we analyse the main characteristics of our storage system and the different types of attacks that could be executed on it. Then, aiming at identifying on-going attacks, we propose and evaluate detection techniques, which rely on traditional metrics and machine learning algorithms . We identify and adapt two suitable metrics for this purpose, namely generalized entropy and information distance .},
  archive      = {J_COMCOM},
  author       = {Federico Tavella and Alberto Giaretta and Mauro Conti and Sasitharan Balasubramaniam},
  doi          = {10.1016/j.comcom.2022.01.023},
  journal      = {Computer Communications},
  pages        = {59-70},
  shortjournal = {Comput. Commun.},
  title        = {A machine learning-based approach to detect threats in bio-cyber DNA storage systems},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing throughput and convergence time in dynamic
multi-connectivity 5G deployments. <em>COMCOM</em>, <em>187</em>, 45–58.
(<a href="https://doi.org/10.1016/j.comcom.2022.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fifth-generation (5G) mobile communications are expected to integrate multiple radio access technologies (RATs) within a unified access network by allowing the user equipment (UE) to utilize them concurrently. As a consequence, mobile users face even more heterogeneous connectivity options, which creates challenges for efficient decision-making when selecting a network dynamically. In this work, with the tools of queuing theory , integral geometry, and optimization theory , we develop a novel mobility-centric analytical methodology for multi-RAT deployments. Particularly, we first contribute a framework for optimal data rate allocation in the network-assisted regime. Then, we characterize the convergence time of the distributed optimization algorithms based on reinforcement learning to reduce the signaling overheads. Our findings suggest that network-assisted strategies may improve the UE throughput by up to 60\% depending on the considered deployment, where the gains increase with a higher density of millimeter-wave New Radio (NR) base stations . A user-centric solution based on reinforcement learning mechanisms is capable of approaching the performance of the network-assisted scheme. However, the associated convergence time may be prohibitive, on the order of several minutes. To improve the latter, we further propose and evaluate a transfer learning-based algorithm that allows to decrease the convergence time by up to 10 times, thus becoming a simple solution for rate-optimized operation in future 5G NR deployments.},
  archive      = {J_COMCOM},
  author       = {Rustam Pirmagomedov and Dmitri Moltchanov and Andrey Samuylov and Antonino Orsino and Johan Torsner and Sergey Andreev and Yevgeni Koucheryavy},
  doi          = {10.1016/j.comcom.2022.01.015},
  journal      = {Computer Communications},
  pages        = {45-58},
  shortjournal = {Comput. Commun.},
  title        = {Characterizing throughput and convergence time in dynamic multi-connectivity 5G deployments},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bee optimization based random double adaptive whale
optimization model for task scheduling in cloud computing environment.
<em>COMCOM</em>, <em>187</em>, 35–44. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is the computing technology that offers dynamically scalable and flexible computing resources. Task scheduling in the cloud system is the major problem that needs to be tackled for enhancing the system performance and cloud customer satisfaction level. The task scheduling scheme directly affects the execution time as well as the execution cost of the system. To overcome the above-stated issue, the novel hybrid Whale optimization algorithm-based MBA algorithm is proposed for solving the multi-objective task scheduling problems in cloud computing environments. In the hybrid WOA based MBA algorithm, the multi-objective behavior decreases the makespan by maximizing the resource utilization. The output of the Random double adaptive whale optimization algorithm (RDWOA) is enhanced by utilizing the mutation operator of the Bees algorithm. The performance evaluation is conducted and compared with other algorithms using the platform of Cloudsim tool kit for various measures such as completion, time, and computational cost. The results are analyzed for the performance measures such as makespan, execution time, resource utilization and computational cost and the analysis proves that the proposed algorithm performs better than other algorithms such as IWC, MALO, BA-ABC and MGGS. The proposed HWOA based MBA algorithm converged faster than any other approach for large search spaces and makes it appropriate for large scheduling problems. The experimental results reveal that the HWOA based MBA algorithm effectively minimizes the task completion time and also execution time.},
  archive      = {J_COMCOM},
  author       = {N. Manikandan and N. Gobalakrishnan and K. Pradeep},
  doi          = {10.1016/j.comcom.2022.01.016},
  journal      = {Computer Communications},
  pages        = {35-44},
  shortjournal = {Comput. Commun.},
  title        = {Bee optimization based random double adaptive whale optimization model for task scheduling in cloud computing environment},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Completed tasks number maximization in UAV-assisted mobile
relay communication system. <em>COMCOM</em>, <em>187</em>, 20–34. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) with high flexibility, ease of deployment, and low cost has been widely applied to our daily life. In this paper, an UAV-assisted mobile relay communication system is considered, in which one UAV and one access point (AP) jointly provide computing resources for dense user equipment (UE) on the ground. We aim to maximize the number of tasks completed on the UAV and AP by optimizing the local computing frequency, offloading ratio, offloading power, computing frequency of the AP, computing frequency of the UAV, relaying power of the UAV, and relaying ratio of the UAV. Because the formulated problem is a mixed-integer nonlinear programming (MINP) problem that is very difficult to solve directly, we convert it into the problem of minimizing the computing frequency allocated by the UAV and AP to each UE. Then block coordinate descent (BCD) method is adopted to solve the converted problem. An optimal partial offloading and relaying scheme (POARS) is proposed to obtain the maximal number of completed tasks. Finally, simulation results show that POARS has a better performance than other schemes.},
  archive      = {J_COMCOM},
  author       = {Qiang Tang and Zhiqiang Yu and Caiyan Jin and Jin Wang and Zhuofan Liao and Yuansheng Luo},
  doi          = {10.1016/j.comcom.2022.01.021},
  journal      = {Computer Communications},
  pages        = {20-34},
  shortjournal = {Comput. Commun.},
  title        = {Completed tasks number maximization in UAV-assisted mobile relay communication system},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on blockchain-based recommender systems:
Integration architecture and taxonomy. <em>COMCOM</em>, <em>187</em>,
1–19. (<a href="https://doi.org/10.1016/j.comcom.2022.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Recommender System (RS) is an integral part of present-day leading web services, such as YouTube, Amazon, Netflix, and many others. Modern RSs are challenged to go beyond their traditional role of predicting user preferences to efficiently provide reliable, carefully personalized, and highly accurate recommendations. This paper thoroughly explores and analyzes state-of-the-art literature surveys on RS to extract important challenges and open issues. Our goal in this paper is to survey the literature to extract essential features of RSs and Blockchain (BC), focusing on their integration. Because of the lack of an existing foundation of BC-based RSs, the intrinsic BC aspects in RSs are identified and described. Integrating BC technology within RSs can achieve many benefits such as transparency, decentralization, and security. To that end, a thorough study of the papers on current BC-based RSs is presented along with a synthesized comprehensive taxonomy. Furthermore, a modular RS architecture, encompassing on-chain and off-chain storage and computation processes, is designed. This paper also includes a thorough discussion on the validity of the proposed architecture, BC limitations concerning RSs, and the derivation of a rich set of pointers to future research directions.},
  archive      = {J_COMCOM},
  author       = {Loubna Mekouar and Youssef Iraqi and Issam Damaj and Tarek Naous},
  doi          = {10.1016/j.comcom.2022.01.020},
  journal      = {Computer Communications},
  pages        = {1-19},
  shortjournal = {Comput. Commun.},
  title        = {A survey on blockchain-based recommender systems: Integration architecture and taxonomy},
  volume       = {187},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and lean encrypted internet traffic classification.
<em>COMCOM</em>, <em>186</em>, 166–173. (<a
href="https://doi.org/10.1016/j.comcom.2022.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the type of a network flow or a specific application has many advantages but becomes harder in recent years due to the use of encryption, e.g., by VPN. As a result, there is a recent wave of solutions that harness deep learning for traffic classification . These solutions either require a rather long time (15–60 Seconds) of flow data or rely on handcrafted features for solutions that classify flows faster. In this work, we suggest a novel approach for classification that extracts the most out of the two simple yet defining features of a flow: packet sizes and inter-arrival times. We employ a model that uses the inter-arrival times to parameterize the derivative of the flow hidden-state using a neural network (Neural ODE). We compare our results with a solution that uses the same data without the ODE solver and show the benefit of this approach. Our results can classify flows based on 20 or 30 consecutive packets taken from anywhere in one direction of a flow. This reduces the amount of traffic between the sampling point and the analyzer and does not require matching between two directions of the flow. As a result, our solution can classify traffic with good accuracy within a few seconds, and we show how to combine it with a more accurate (and a slower) classifier to achieve (mostly) fast and accurate classifications.},
  archive      = {J_COMCOM},
  author       = {Sangita Roy and Tal Shapira and Yuval Shavitt},
  doi          = {10.1016/j.comcom.2022.02.003},
  journal      = {Computer Communications},
  pages        = {166-173},
  shortjournal = {Comput. Commun.},
  title        = {Fast and lean encrypted internet traffic classification},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted triplet loss based on deep neural networks for loop
closure detection in VSLAM. <em>COMCOM</em>, <em>186</em>, 153–165. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart living is an emerging technology that has attracted a lot of attention all around the world. As a key technology of smart space, which is the principal part of smart living, the SLAM system has effectively expanded the ability of space intelligent robots to explore unknown environments. Loop closure detection is an important part of SLAM system and plays a very important role in eliminating cumulative errors. The SLAM system without loop closure detection is degraded to an odometer. The state estimation solely relying on an odometer will be seriously deviated in the long-term and large-scale navigation and positioning. This paper proposes a metric learning method that uses deep neural networks for loop closure detection based on triplet loss. The map points obtained by metric learning are fused with all map points in the current keyframe , and the map points that do not meet the filtering conditions are eliminated. Based on the Batch Hard Triplet loss, the weighted triplet loss function avoids suboptimal convergence in the learning process by applying weighted value constraints. At the same time, considering that fixed boundary parameters cannot be well adapted to the diversity of scales between different samples, we use the semantic similarity of anchor samples and negative samples to redefine boundary parameters. Finally, a SLAM system based on metric learning is constructed, and the SLAM dataset TUM and KITTI are used to evaluate the proposed model’s accuracy rate and recall rate. The scene features in this method are extracted automatically through neural networks instead of being artificially set. Finally, a high-precision closed-loop detection method based on weight adaptive triple loss is effectively realised through the closed-loop detection experiment. The minimum relative pose error is 0.00048 m, which is 15.8\% less than that of the closed-loop detection algorithm based on the word bag model.},
  archive      = {J_COMCOM},
  author       = {Na Dong and Minghui Qin and Jianfang Chang and C.H. Wu and W.H. Ip and K.L. Yung},
  doi          = {10.1016/j.comcom.2022.01.013},
  journal      = {Computer Communications},
  pages        = {153-165},
  shortjournal = {Comput. Commun.},
  title        = {Weighted triplet loss based on deep neural networks for loop closure detection in VSLAM},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An automated context-aware IoT vulnerability assessment
rule-set generator. <em>COMCOM</em>, <em>186</em>, 133–152. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While introducing unprecedented applications, Internet of Things (IoT) has simultaneously provoked acute security challenges, in the form of the vulnerabilities. Mainly because manufacturers overlook the security considerations and produce devices that could be exploited easily. Security systems used for the protection of IoT environment usually deploy traditional rulesets which lack distinct IoT vulnerability assessment elements and therefore are inadequate for providing security to IoT eco-system. Hence, due to the variety and volume of such devices, traditional security solutions need to be more robust for IoT settings. Contrary to the traditional rule-set, IoT device vulnerability identification requires distinct understanding of IoT-specific vulnerability vectors, based on their architecture, resource constrained nature, communication primitives and context awareness . This research work has proposed an automated context-aware IoT vulnerability assessment rule-set framework. Proposed system dynamically identifies IoT devices along with the services running on them, gathers their respective vulnerabilities, transform them into rules and enforce them into the security solutions. The proposed framework has been evaluated on a dataset of 49 IoT devices. According to the results, proposed framework automatically generated rules against all the vulnerabilities present in the network under consideration. Additionally, this research has proposed IoT vulnerability assessment rule-set elements which are necessary to be considered while designing any IoT vulnerability assessment rule-set. With the proposed mechanism, this research work intends to fill the missing lines of defense against rising IoT vulnerabilities. The proposed framework will benefit researchers, security analysts and manufacturers to devise reliable IoT security solutions.},
  archive      = {J_COMCOM},
  author       = {Fabiha Hashmat Msc. Computer Engineering and Syed Ghazanfar Abbas and Sadaf Hina and Ghalib A. Shah and Taimur Bakhshi and Waseem Abbas},
  doi          = {10.1016/j.comcom.2022.01.022},
  journal      = {Computer Communications},
  pages        = {133-152},
  shortjournal = {Comput. Commun.},
  title        = {An automated context-aware IoT vulnerability assessment rule-set generator},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new RFID ultra-lightweight authentication protocol for
medical privacy protection in smart living. <em>COMCOM</em>,
<em>186</em>, 121–132. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of Radio Frequency Identification (RFID) technology in smart living, especially in smart healthcare, has greatly facilitated patient management, reduced the labor cost of medical services, and provided patients with better medical services. However, improper use of RFID tags is highly likely to threaten the safety and privacy of hospitals and even patients, which may not only cause physical and mental harm to patients but also damage the reputation of hospitals. Medical monitoring system based on RFID technology plays a significant role in guaranteeing the security of medical records as well as the privacy of patients. Nevertheless, due to the resource constraint of RFID tags/readers, it is also challenging to design an efficient and effective authentication protocol for RFID medical monitoring system. Aiming to reduce the resource overhead and meet security requirements of the RFID system, in this paper, we propose an ultra-lightweight RFID security authentication protocol utilizing a cloud server named CRUSAP, which is based on Bit-Crossing XOR rearrangement operations that can effectively resist forgery attacks , replay attacks, desynchronization attacks , and denial of service attacks while certifying the cloud server. The formal logic of BAN proves the safety and feasibility of the protocol. Additionally, the security analysis and experimental results show that the proposed authentication protocol can realize higher security goals at a lower cost and is more suitable for scaling to large-scale authentication .},
  archive      = {J_COMCOM},
  author       = {Xingmiao Wang and Kai Fan and Kan Yang and Xiaochun Cheng and Qingkuan Dong and Hui Li and Yintang Yang},
  doi          = {10.1016/j.comcom.2022.01.014},
  journal      = {Computer Communications},
  pages        = {121-132},
  shortjournal = {Comput. Commun.},
  title        = {A new RFID ultra-lightweight authentication protocol for medical privacy protection in smart living},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An anonymous authentication and key agreement protocol in
smart living. <em>COMCOM</em>, <em>186</em>, 110–120. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) play an indispensable role in the application of smart homes, smart healthcare, and precision agriculture. However, WSNs confront privacy risks that hinder its practical applications. The leakage of privacy is one of the key factors to restrict the development of WSNs. Hence, in this paper, we propose an Anonymous Authentication and Key Agreement protocol (AAKA) to accomplish identity authentication and privacy protection. Based on the dynamic sequence number, the shared secret value, and the dynamic random number, the AAKA protocol implements a two-way authentication and keys negotiation among users, gateway, and sensors, which achieves the secure access control of legitimate users to WSNs and ensures the confidential transmission of data over the public channel. We perform the security proof with BAN logic for security evaluation. The performance analysis demonstrates that compared with other WSNs authentication schemes , the AAKA protocol obtained better security features, smaller storage, and more efficient communication. Therefore, it is more suitable for applications in smart living.},
  archive      = {J_COMCOM},
  author       = {Fengyin Li and Xinying Yu and Yang Cui and Siqi Yu and Yuhong Sun and Yilei Wang and Huiyu Zhou},
  doi          = {10.1016/j.comcom.2022.01.019},
  journal      = {Computer Communications},
  pages        = {110-120},
  shortjournal = {Comput. Commun.},
  title        = {An anonymous authentication and key agreement protocol in smart living},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable name identifier lookup for industrial internet.
<em>COMCOM</em>, <em>186</em>, 102–109. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Name identifiers are considered more effective than IP addresses in routing and forwarding billions of heterogeneous resource-restrained Industrial Internet objects. Although the design of name-based longest prefix matching lookup has been developed for nearly a decade, it is still attractive to design an efficient name lookup scheme for the Industrial Internet due to hierarchical variable-length names, large-scale forwarding tables, and frequent updates. In this paper, we present a memory-optimized binary search (MOBS) algorithm in the counting Bloom filter (CBF), which decreases memory consumption by reducing the storage of prefix marker entries while maintaining the fast lookup efficiency of binary search. Furthermore, we propose CBF-HT, an efficient and scalable name lookup method that combines counting Bloom filter and hash table to facilitate content forwarding. CBF-HT consists of two stages. Firstly, the longest prefix matching results are obtained through binary search in the counting Bloom filter with MOBS, and then the final lookup results are acquired by linear backtracking in the hash table. CBF-HT can achieve high-speed lookups and fast updates, and significantly reduces memory consumption caused by marker entries through MOBS. We have implemented and evaluated CBF-HT with 10 billion name identifiers. Experimental results show that, compared with the existing binary search on hash table algorithm (BS-HT), CBF-HT increases the forwarding throughput by 45\%, eliminates 73\% and 90\% of memory consumption, and improves the update performance by 48.1\%, at the cost of a 7.8\% increase in the average number of memory accesses.},
  archive      = {J_COMCOM},
  author       = {Yunmin Wang and Ting Huang and Guohua Wei and Hui Li and Huayu Zhang},
  doi          = {10.1016/j.comcom.2022.01.017},
  journal      = {Computer Communications},
  pages        = {102-109},
  shortjournal = {Comput. Commun.},
  title        = {Scalable name identifier lookup for industrial internet},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECN-based shared bottleneck detection for multi-path TCP.
<em>COMCOM</em>, <em>186</em>, 90–101. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Path TCP (MPTCP) shows its unique advantages over single-path TCP thanks to its aggregate throughput from multiple paths, balancing efficiency and fairness over multipath transmission. However, MPTCP senders should tackle the friendliness issue carefully when sharing the same bottleneck with single-path TCP. Unfortunately, existing shared bottleneck detection schemes either utilize end-to-end delay without consideration of multiple-bottleneck scenarios, or identify subflows on switches with large operation overhead . In this paper, we propose a lightweight yet accurate approach, ECN-based MPTCP (EMPTCP), to detect shared bottleneck of multiple subflows. EMPTCP uses the widely deployed ECN scheme to capture the real congestion state of shared bottlenecks, while at the same time is compatible with various MPTCP congestion control algorithms. Through model analysis, simulation test and real experiment, we show that EMPTCP achieves higher than 85\% accuracy in shared bottleneck detection, thus improving the network efficiency and fairness.},
  archive      = {J_COMCOM},
  author       = {Jin Ye and Lin Li and Zihan Chen and Guihao Chen and Sen Liu and Jiawei Huang and Jianxin Wang and Tian He},
  doi          = {10.1016/j.comcom.2022.01.011},
  journal      = {Computer Communications},
  pages        = {90-101},
  shortjournal = {Comput. Commun.},
  title        = {ECN-based shared bottleneck detection for multi-path TCP},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time synchronization enhancements in wireless networks with
ultra wide band communications. <em>COMCOM</em>, <em>186</em>, 80–89.
(<a href="https://doi.org/10.1016/j.comcom.2022.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of low cost Ultra Wide Band (UWB) transceivers has enabled the implementation of Wireless Sensor Networks (WSN) based on this communication technology. These networks are composed of distributed autonomous low cost nodes (also known as motes) with their own processing unit, memory and communications. Usually these nodes are power-limited and due to the poor performance and quality of their clocks, time synchronization is in the order of milliseconds and in some specific scenarios till microseconds . The integration of commercial UWB transceivers in these nodes can improve the synchronization accuracy. In particular, we focus on WSN nodes based on off-the-shelf commercial products and commodity hardware. In this paper we analyze step by step, from a practical and experimental point of view, the different elements involved in the time synchronization using UWB technology on WSN with static nodes. From our experimental results, with timestamps captured during the packet exchanges , we analyze and discuss the application of different communication schemes and simple statistical methods (in order to be run in WSN nodes). The results obtained with timestamps captured at the UWB transceivers and by using linear regression show that the lowest time synchronization error achieved between two nodes is 0.14 ns. Employing the same setup and performing the synchronization with the timestamps captured internally at the microcontrollers of the nodes, the error rises to 31 ns, due to the higher time period of the microcontrollers’ timers and the inaccuracies that affect the acquisition of the timestamps. Nevertheless, the synchronization of the microcontrollers’ clocks allows the setting up of a common time reference at the network nodes, enabling the implementation of applications with tight synchronization requirements, such as collaborative beamforming and ranging.},
  archive      = {J_COMCOM},
  author       = {Juan J. Pérez-Solano and Santiago Felici-Castell and Antonio Soriano-Asensi and Jaume Segura-Garcia},
  doi          = {10.1016/j.comcom.2022.01.012},
  journal      = {Computer Communications},
  pages        = {80-89},
  shortjournal = {Comput. Commun.},
  title        = {Time synchronization enhancements in wireless networks with ultra wide band communications},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance evaluation of IoT networks: A product density
approach. <em>COMCOM</em>, <em>186</em>, 65–79. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cellular-based Internet of Things (IoT) networking is set to be enhanced with the addition of two important pillars of 5G — massive Machine Type Communications and ultra-Reliable Low Latency Communications. Temporal traffic uncertainty from diverse applications in IoT networks mandates novel modeling approaches for performance evaluation. This paper introduces a novel stochastic point process approach that can be used to evaluate the time-dependent performance of IoT base stations . Special correlation functions called Product Densities are used to (a) evaluate time-dependent offered traffic, and (b) analyze delay performance. These performance measures are evaluated at IoT base station for Poisson as well as non-Homogeneous Poisson (Beta distributed) traffic arrival processes suggested by 3rd Generation Partnership Project(3GPP). The Product Density estimates of offered traffic are found to be more accurate than the point-wise stationary approximation (PSA) under non-stationary traffic arrival rates. Results from the proposed analytical model are compared with results from a simulation of two queuing models of the base station; infinite server model for offered traffic and multi-server model for delay performance. Analytical results from Product Density functions also correlate with the simulation outcomes suggesting that the proposed Product Density technique is effective when modeling the time-dependent performance of IoT networks subjected to non-stationary traffic conditions, which reflects the real-world scenario.},
  archive      = {J_COMCOM},
  author       = {Vijayalakshmi Chetlapalli and Himanshu Agrawal and K.S.S. Iyer and Mark A. Gregory and Vidyasagar Potdar and Reza Nejabati},
  doi          = {10.1016/j.comcom.2022.01.010},
  journal      = {Computer Communications},
  pages        = {65-79},
  shortjournal = {Comput. Commun.},
  title        = {Performance evaluation of IoT networks: A product density approach},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and deployment of a practical IoT-based monitoring
system for protected cultivations. <em>COMCOM</em>, <em>186</em>, 51–64.
(<a href="https://doi.org/10.1016/j.comcom.2022.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the United Nations’ latest forecast, the world population will reach 8.5 billion people by 2030. This rapid population growth imposes severe requirements in food production to meet demand. Moreover, the rise of temperatures and climate changes are also adversely affecting food supplies. In this paper, we design a scalable IoT-based monitoring system with prediction capabilities for agricultural applications. It provides an effective four-layered architecture that consist of sensing, networking, processing, and applications with low deployment and management costs. Hence, to demonstrate its feasibility, the proposed IoT system was constructed, experimentally tested, and validated by monitoring the temperature and humidity of a commercial-size greenhouse in Mexico for six months. Additionally, we integrated a data-driven predictive model for greenhouse microclimate conditions. Temperature predictions were accurately performed 24-hour in advance within an error of 1 °C. The obtained results confirm that the proposed IoT framework would facilitate farmers to monitor crops and enable productivity gains by increasing the level of technology progressively.},
  archive      = {J_COMCOM},
  author       = {Carlos A. Hernández-Morales and J.M. Luna-Rivera and Rafael Perez-Jimenez},
  doi          = {10.1016/j.comcom.2022.01.009},
  journal      = {Computer Communications},
  pages        = {51-64},
  shortjournal = {Comput. Commun.},
  title        = {Design and deployment of a practical IoT-based monitoring system for protected cultivations},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A secure blockchain system for internet of vehicles based
on 6G-enabled network in box. <em>COMCOM</em>, <em>186</em>, 45–50. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network in Box (NIB) is a self-organizing and portable device. The six-generation wireless communication technologies (6G) can empower NIB with better spectrum efficiency by integrating satellite broadcasting. 6G-enabled NIB is promising to promote the communication efficiency of Internet of Vehicles (IoV). IoV has emerged as the concrete practice of intelligent transportation. However, IoV is vulnerable to attacks from quantum computers because they use traditional RSA and elliptic cure cryptographic systems. Therefore, it is critical to improving the security of IoV against quantum computer attacks. This paper proposes the first secure scheme based on post-quantum techniques for 6G-enabled NIB to protect IoV against quantum attacks. On the one hand, a blockchain-based public key infrastructure is proposed to authenticate the IoV devices securely. On the other hand, we design a blockchain-based multi-party key agreement and communication system to support multi-party communication among IoV devices. The extensive theoretical analysis and experimental results indicate that the proposed blockchain system based on 6G-enabled NIB can achieve high security and efficiency for IoV.},
  archive      = {J_COMCOM},
  author       = {Haibo Yi},
  doi          = {10.1016/j.comcom.2022.01.007},
  journal      = {Computer Communications},
  pages        = {45-50},
  shortjournal = {Comput. Commun.},
  title        = {A secure blockchain system for internet of vehicles based on 6G-enabled network in box},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliable auxiliary communication of UAV via relay cache
optimization. <em>COMCOM</em>, <em>186</em>, 33–44. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous reduction of drone costs and the miniaturization of equipment, many new applications have emerged in the civil and commercial fields. The remote sensing technology of Unmanned Aerial Vehicle (UAV) is used to manage and partition farmland accurately. Compared with the traditional field grid sampling method, UAV remote sensing technology can break through operating conditions, continuously collect data at low altitudes, monitor the area more flexibly, and significantly reduce labor and safety costs. Since UAVs can only provide data transmission services to users through wireless backhaul links established with ground base stations , the need to access the network is easy to lead to the disclosure of user privacy. The capacity of wireless backhaul links is limited, limiting the transmission rate of drones and reducing the user’s quality of service. Therefore, we apply edge caching technology to the assisted relay communication network to study the impact of caching technology on the performance of mobile relay systems. In particular, we propose the horizontal position design method in the buffer auxiliary relay single-user system, and the 3D position design method in the buffer auxiliary relay multi-user system. Position system, which can be reached by the maximum system average and the optimal speed, is designed. Besides, with the help of objective function conversion and classic derivation analysis method, the semi-closed expression of the optimal position of the UAV is obtained, and the intersection of the mobile relay end velocity and the user end velocity is used as the initial value. Meanwhile, the solution formula is substituted after continuous iteration so that the best advantage of local velocity can be obtained. In addition, mobile relay system is deployed to establish a two-hop wireless link to achieve reliable communication between Base Station (BS) and users security trust. The simulation experiment proves that compared with other methods, the method proposed in this paper has considerable performance improvement in power convergence, speed, trajectory path loss, and energy cost, which can provide higher-quality communication services for users in the system and better support for the broad application of drones.},
  archive      = {J_COMCOM},
  author       = {Xin He and Meixu Lin},
  doi          = {10.1016/j.comcom.2021.11.024},
  journal      = {Computer Communications},
  pages        = {33-44},
  shortjournal = {Comput. Commun.},
  title        = {Reliable auxiliary communication of UAV via relay cache optimization},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Network coding data distribution technology between streams
of emergency system based-wireless multi-hop network. <em>COMCOM</em>,
<em>186</em>, 22–32. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of network coding technology in the network can effectively improve the network performance in many aspects, such as delay, energy consumption, and throughput. Considering that multiple data streams can be encoded together, the network node forwards the data packet to reduce the transmission times of the encoded data packet in the wireless network. In particular, a measurement standard based on the number of pre-coding transmissions is proposed with the help of network coding and multiplexing technology. First, the opportunistic routing strategy between multiple data streams of network coding is applied to optimize the forwarding and transmission mode of data storage, which can effectively reduce the transmission of data packets. Second, the concept of coding is introduced into the relay nodes in the network, and the data distribution strategy is used to reduce the number of data packet transmissions in the network. Finally, a measurement standard for the number of pre-coding transmissions is designed for the resource allocation path in wireless multi-hop networks. The experimental results show that the strategy proposed in this paper can achieve a lower transmission delay compared with the single-path transmission strategies and the multi-path data distribution strategies without considering network coding.},
  archive      = {J_COMCOM},
  author       = {Bing Bu},
  doi          = {10.1016/j.comcom.2022.01.008},
  journal      = {Computer Communications},
  pages        = {22-32},
  shortjournal = {Comput. Commun.},
  title        = {Network coding data distribution technology between streams of emergency system based-wireless multi-hop network},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A seamless anonymous authentication protocol for mobile edge
computing infrastructure. <em>COMCOM</em>, <em>186</em>, 12–21. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) accommodates processing and data storage and manipulation capabilities across the scope of wireless network. In MEC environment, MEC servers along with the computing and storage capabilities are distributed at the edge of the network. However, due to the broad range of wireless communication , the fulfillment of security requirements still remain a challenging task in the for MEC environment. With the expeditious traffic expansion and growing end user requirements, the classic security protocols cannot encounter the innovative requirements of lightweightness and real-time communication. To meet these requirements, we have proposed an authentication protocol for the MEC environment. Our proposed protocol stipulates secure and efficient communication for all of the intended entities. Meanwhile, during its execution user anonymity remains intact. Moreover, our protocol is proven to be secure under the assumptions of formal security model. Additionally in this article, we have described the security properties of our protocol that it offers resistance against impersonation, session key computation and forward and backward secrecy attacks. The comparative analysis of time consumption and computation overheads are presented at the end of the paper, which is an evidence that our proposed protocol outperforms prior to various existing MEC protocols.},
  archive      = {J_COMCOM},
  author       = {Khalid Mahmood and Muhammad Faizan Ayub and Syed Zohaib Hassan and Zahid Ghaffar and Zhihan Lv and Shehzad Ashraf Chaudhry},
  doi          = {10.1016/j.comcom.2022.01.005},
  journal      = {Computer Communications},
  pages        = {12-21},
  shortjournal = {Comput. Commun.},
  title        = {A seamless anonymous authentication protocol for mobile edge computing infrastructure},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A privacy-preserving and verifiable federated learning
method based on blockchain. <em>COMCOM</em>, <em>186</em>, 1–11. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel distributed learning mechanism, federated learning has drawn widespread attention by allowing multiple parties to train an accurate model collaboratively without collecting their raw data. However, it relies on a trustworthy central server that still suffers from severe security challenges, such as model inversion attack and single point of failure . Thereby, a privacy-preserving and verifiable federated learning method based on blockchain is proposed to enable fully decentralized and reliable federated learning in untrusted network . In our scheme, we propose a secure aggregation protocol to guarantee the confidentiality of gradients while supporting clients dropping out during the workflow, and design a novel blockchain structure enabling global gradient verification to defend against potential tampering attack. In addition, a gradient compression method is proposed to reduce the communication overhead . Security analysis shows that our scheme can preserve the privacy by adding pairwise random masks to the gradients, and prevent Sybil attack by reasonable threshold setting in verifiable secret sharing. Experimental results on two real-world datasets illustrate that when the clients’ dropout rate is less than 20\%, our scheme can achieve almost the same accuracy as original federated learning, and performs better than similar blockchain-based federated learning methods in terms of computation overhead and communication overhead .},
  archive      = {J_COMCOM},
  author       = {Chen Fang and Yuanbo Guo and Jiali Ma and Haodong Xie and Yifeng Wang},
  doi          = {10.1016/j.comcom.2022.01.002},
  journal      = {Computer Communications},
  pages        = {1-11},
  shortjournal = {Comput. Commun.},
  title        = {A privacy-preserving and verifiable federated learning method based on blockchain},
  volume       = {186},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based federated incremental learning for traffic
classification in the internet of things. <em>COMCOM</em>, <em>185</em>,
168–175. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) traffic follows non-independent and identical distribution (non-IID). Traditional machine learning classification methods will cause low classification accuracy , high communication costs, and privacy leakage issues. Federated Learning enables several clients to train a deep learning model collaboratively without requiring any of the clients to share their local data with a centralized server. In this paper, we propose a novel attention-based federated incremental learning algorithm: Fed-SOINN. We introduce the attention mechanism to improve the weight of parameters uploaded by clients which are beneficial to the global model, where instead of the full gradient, only a small subset of important gradients is communicated. Meanwhile, we improve the sparsity of the model by upgrading the online optimization function in Fed-SOINN, it also brings faster convergence speed and higher accuracy in the changeable network environment. Results reveal that Fed-SOINN has improved detection accuracy by 3.1\% compared with benchmark methods and can reduce the number of communications rounds up to 73\%. When facing new traffic categories, the incremental learning mechanism in Fed-SOINN also effectively identify unknown traffic categories.},
  archive      = {J_COMCOM},
  author       = {Meng-yuan Zhu and Zhuo Chen and Ke-fan Chen and Na Lv and Yun Zhong},
  doi          = {10.1016/j.comcom.2022.01.006},
  journal      = {Computer Communications},
  pages        = {168-175},
  shortjournal = {Comput. Commun.},
  title        = {Attention-based federated incremental learning for traffic classification in the internet of things},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSTRM: Contrastive self-supervised trajectory representation
model for trajectory similarity computation. <em>COMCOM</em>,
<em>185</em>, 159–167. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trajectory representation model has become a common method for calculating the similarity of trajectories. Existing works have used the encoder–decoder model, which is trained by reconstructing the original trajectory from a noisy trajectory. However, this reconstructive model ignores the point-level differences between these two trajectories and captures only the trajectory-level features. As a result, it achieves low accuracy on ranking tasks. To solve this problem, we propose a novel contrastive model to learn trajectory representations by distinguishing the trajectory-level and point-level differences between trajectories. Furthermore, to solve the lack of training data, we propose a self-supervised approach to augment training pairs of trajectories. Compared with existing models, our model achieves a significant performance improvement on various trajectory similarity tasks.},
  archive      = {J_COMCOM},
  author       = {Xiang Liu and Xiaoying Tan and Yuchun Guo and Yishuai Chen and Zhe Zhang},
  doi          = {10.1016/j.comcom.2022.01.001},
  journal      = {Computer Communications},
  pages        = {159-167},
  shortjournal = {Comput. Commun.},
  title        = {CSTRM: Contrastive self-supervised trajectory representation model for trajectory similarity computation},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective optimization of data deployment and
scheduling based on the minimum cost in geo-distributed cloud.
<em>COMCOM</em>, <em>185</em>, 142–158. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the amount of data coming from terminal devices is increasing on a vast scale. Data centers in different geographical locations store vast amounts of data. In this paper, to optimize the data deployment problem, a data layout algorithm that can be satisfied with the load balancing of the cluster is introduced. Under the three constraints of cost, capacity, and load balance, this paper presents the idea of ant colony optimization and uses the Lagrangian relaxation method to verify the value of the optimal solution. For improving the execution efficiency of various cloud data centers, a task scheduling method is proposed. This method solves the problems of delay and transmission cost and then uses the gray Markov-based prediction method to predict the dynamic changes of resources of different cluster nodes and select the most suitable task scheduling node. As shown in the experiment, the completion time of the application can be shortened by this algorithm. Meanwhile, in this paper, the consumption of cluster resources is reduced, and the throughput of the cluster is improved.},
  archive      = {J_COMCOM},
  author       = {Tianxing Xie and Chunlin Li and Na Hao and Youlong Luo},
  doi          = {10.1016/j.comcom.2021.12.022},
  journal      = {Computer Communications},
  pages        = {142-158},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective optimization of data deployment and scheduling based on the minimum cost in geo-distributed cloud},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FakeNewsIndia: A benchmark dataset of fake news incidents in
india, collection methodology and impact assessment in social media.
<em>COMCOM</em>, <em>185</em>, 130–141. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Social Media platforms (OSMs) have become an essential source of information. The high speed at which OSM users submit data makes moderation extremely hard. Consequently, besides offering online networking to users, the OSMs have also become carriers for spreading fake news. Knowingly or unknowingly, users circulate fake news on OSMs, adversely affecting an individual’s offline activity. To counter fake news, several dedicated websites (referred to as fact-checkers) have sprung up whose sole purpose is to identify and report fake news incidents. There are well-known datasets of fake news; however, not much work has been done regarding credible datasets of fake news in India. Therefore, we design an automated data collection pipeline to collect fake incidents reported by fact-checkers in this work. We gather 4,803 fake news incidents from June 2016 to December 2019 reported by six popular fact-checking websites in India and make this dataset (FakeNewsIndia) available to the research community. We find 5,031 tweets on Twitter and 866 videos on YouTube mentioned in these 4,803 fake news incidents. Further, we evaluate the impact of fake new incidents on the two prominent OSM platforms, namely, Twitter and YouTube. We use popularity metrics based on engagement rate and likes ratio to measure impact and categorize impact into three levels — low, medium, and high. Our learning models use features extracted from text, images, and videos present in the fake news incident articles written by fact-checking websites. Experiments show that we can predict the impact (popularity) of videos (appearing on fake news incident articles) on YouTube more accurately (with baseline accuracy ranging from 86\% to 92\%) as compared to the impact (popularity) of tweets on Twitter (with baseline accuracy of 37\% to 41\%). We need to build more intelligent models that predict tweets’ impact, appearing in fact-checking incident articles on Twitter as future work.},
  archive      = {J_COMCOM},
  author       = {Apoorva Dhawan and Malvika Bhalla and Deeksha Arora and Rishabh Kaushal and Ponnurangam Kumaraguru},
  doi          = {10.1016/j.comcom.2022.01.003},
  journal      = {Computer Communications},
  pages        = {130-141},
  shortjournal = {Comput. Commun.},
  title        = {FakeNewsIndia: A benchmark dataset of fake news incidents in india, collection methodology and impact assessment in social media},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight network-coded ARQ: An approach for
ultra-reliable low latency communication. <em>COMCOM</em>, <em>185</em>,
118–129. (<a
href="https://doi.org/10.1016/j.comcom.2022.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sliding Window RLNC schemes represent a novel, promising and efficient class of algorithms for reliable data transmission over an unreliable link. They combine Random Linear Network Coding (RLNC) with the idea of sliding window from (Automatic Repeat reQuest) ARQ protocols in order to help the basic ARQ mechanism to quickly recover from losses through redundant coded packets. Although very successful, we make the observation that such schemes bear significant limitations stemming from the fact that both the ARQ mechanism and the coding process operate on the same group of packets, called the sliding window. To tackle the problem, we propose the use of two distinct windows; the sliding window , which can be used to optimize the data flow based on the link’s bandwidth-delay product, and the coding window that can be used exclusively in the coding process. We analyze the performance of the proposed strategy and show that it provides an improved coding operation while at the same time reduces the coding complexity. Then, we delineate rapidARQ, a sliding window RLNC protocol that adopts the proposed strategy, and experimentally confirm that it outperforms other state-of-the-art sliding window RLNC schemes . More specifically, it achieves superior throughput-delay performance that better fits in the context of Ultra-Reliable Low-Latency Communication (URLLC) while at the same time it significantly reduces the coding complexity. Moreover, we show that the superior performance of rapidARQ is more prominent in channels with large bandwidth-delay products, a fact that renders its utility to current and future networks more essential.},
  archive      = {J_COMCOM},
  author       = {Foteini Karetsi and Evangelos Papapetrou},
  doi          = {10.1016/j.comcom.2022.01.004},
  journal      = {Computer Communications},
  pages        = {118-129},
  shortjournal = {Comput. Commun.},
  title        = {Lightweight network-coded ARQ: An approach for ultra-reliable low latency communication},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survival backup strategy for controller placement problem in
software defined networking. <em>COMCOM</em>, <em>185</em>, 104–117. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core concept of Software Defined Networking (SDN) is to abstract the control layer from the data layer. SDN architectures can provide programmatic interfaces in communication networks that significantly simplify network management and improve utilization efficiency. Distributed multiple controller SDN environments have become the preferred solution towards better scalability of SDNs. However, there are some inevitable problems for such networks, such as network failure, especially due to the failure of controllers themselves. To further improve network performance, reliability, and survivability , one solution is to deploy backup controllers in the network to satisfy quality of service requirements. In this paper, we aim to enhance the controller placement approach by designing a reliable and survivable controller placement framework. This framework consists of two levels. The first level is to determine the number and placement of the primary controllers using the node degree, which relies on selecting the nodes with the highest degree and then generating multiple domains exploiting the Independent Dominating Set (IDS) technique to ensure efficient distribution of controllers with lowest response times. The second level aims to provide some protection for the first level through the efficient deployment of backup controllers. The approach designed at this level is called the survivable backup controller placement approach. We evaluate this framework using real topologies; the obtained results show the significant impact of integrating both levels in improving the performance of SDNs without introducing unacceptable delay times .},
  archive      = {J_COMCOM},
  author       = {Abdunasser Alowa and Thomas Fevens and Yaser Khamayseh},
  doi          = {10.1016/j.comcom.2021.12.020},
  journal      = {Computer Communications},
  pages        = {104-117},
  shortjournal = {Comput. Commun.},
  title        = {Survival backup strategy for controller placement problem in software defined networking},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). AntiSense: Standard-compliant CSI obfuscation against
unauthorized wi-fi sensing. <em>COMCOM</em>, <em>185</em>, 92–103. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel State Information (CSI)-based localization with 802.11 has been proven feasible in multiple scenarios and is becoming a serious threat to people’s privacy in workplaces, at home, and maybe even outdoors. Countering unauthorized localization without hampering communications is a non-trivial task, although some very recent works suggest that it is feasible with marginal modification of the 802.11 transmission chain, but this requires modifying 802.11 devices. Furthermore, if the attacker controls two devices and not just a receiver, transmission side signal manipulation cannot help. This work explores the possibility of countering CSI based localization with an active device that, instead of jamming signals to avoid that a malicious receiver exploits CSI information to locate a person, superimpose on frames a copy of the same frame signal whose goal is not destroying reception as in jamming, but only obfuscate the location-relevant information carried by the CSI. A prototype implementation and early results look promising; they show the feasibility of location obfuscation with high efficiency and excellent preservation of communication performance, and indicate that the technique works both against passive attacks, where the attacker controls only a receiver, and active ones, where he/she controls both a transmitter and a receiver. These results pave the road for further research on smart spaces that preserve users’ privacy with a technical solution and not only via legal prescriptions.},
  archive      = {J_COMCOM},
  author       = {Marco Cominelli and Francesco Gringoli and Renato Lo Cigno},
  doi          = {10.1016/j.comcom.2021.12.019},
  journal      = {Computer Communications},
  pages        = {92-103},
  shortjournal = {Comput. Commun.},
  title        = {AntiSense: Standard-compliant CSI obfuscation against unauthorized wi-fi sensing},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KeyClass: Efficient keyword matching for network traffic
classification. <em>COMCOM</em>, <em>185</em>, 79–91. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic classification is required for a range of network management activities like meeting the Quality of Service demands of applications and security monitoring. Deep Packet Inspection (DPI) based methods achieve better classification accuracy compared to other techniques. However, DPI is computationally demanding and requires searching patterns in the payload. Methods found in the literature suffer from performance issues as they perform multiple scans of payload. In this paper, we describe K e y C l a s s KeyClass , which is a DPI based traffic classifier and can classify network flows with single scan of payload using keyword based signatures. K e y C l a s s KeyClass achieves performance gains (speed of classification) with a combination of two things. It quickly identifies potential application(s) by scanning few initial bytes of payload and optimize the number of character comparisons while searching remaining keywords of potential application(s). In order to identify potential applications, it uses a finite state machine constructed with first keyword of every application using classic Aho–Corasick multi-pattern matching algorithm . K e y C l a s s KeyClass has an application specific signature which is generated with the remaining set of keywords of an application. By skipping portions of payload from inspection, coupled with an efficient string matching algorithm , it practically achieves sub-linear search complexity. We evaluate the classification and execution performance of K e y C l a s s KeyClass with experiments using two large datasets containing 173619 and 885405 network flows and report that it has a good average classification accuracy of ≈ ≈ 98\%. In our evaluation, K e y C l a s s KeyClass is found to be 3.79 times faster than state of the art methods .},
  archive      = {J_COMCOM},
  author       = {Neminath Hubballi and Pratibha Khandait},
  doi          = {10.1016/j.comcom.2021.12.021},
  journal      = {Computer Communications},
  pages        = {79-91},
  shortjournal = {Comput. Commun.},
  title        = {KeyClass: Efficient keyword matching for network traffic classification},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Duplicated zigzag decodable fountain codes with the unequal
error protection property. <em>COMCOM</em>, <em>185</em>, 66–78. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A low overhead fountain code with an unequal error protection property, called duplicated-zigzag unequal-error-protection (DZ-UEP) code, is proposed. Using low-density-parity-check codes of different code rates, input symbols of different important levels are first precoded into variable nodes. Based on predefined duplication factors, these variable nodes of different important levels are then duplicated to produce a group of duplicated symbols. Defining a window as the set of duplicated symbols of the same importance level, a window is randomly selected with a probability proportional to the number of symbols it contains. Duplicated symbols in the selected window and those of the more importance levels are further repeated. Subsequently based on all the repeated symbols formed, encoded symbols are derived using exclusive-or and bit-level shift operations. Theoretical analysis on the proposed DZ-UEP scheme is performed. Simulation results have also verified that input symbols with two or three levels of importance can be provided with unequal error protection under the proposed scheme.},
  archive      = {J_COMCOM},
  author       = {Yuli Zhao and Yin Zhang and Francis C.M. Lau and Zhiliang Zhu and Hai Yu},
  doi          = {10.1016/j.comcom.2021.12.016},
  journal      = {Computer Communications},
  pages        = {66-78},
  shortjournal = {Comput. Commun.},
  title        = {Duplicated zigzag decodable fountain codes with the unequal error protection property},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RFMonitor: Monitoring smoking behavior of minors using COTS
RFID devices. <em>COMCOM</em>, <em>185</em>, 55–65. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protection laws of minors in various countries have clearly banned minors from smoking. However, due to various social factors and the psychological conditions of minors, smoking among minors cannot be effectively controlled. A typical example is that many countries ban smoking in public places, and many places have special smoking areas. Previous work designed wearable sensor devices or used wireless signals (e.g., WIFI and Bluetooth etc.) to identify smoking activities. System deployment and implementation are rarely considered in existing system application scenarios. Here, we show that we can use the backscatter signal of the passive RFID tag, combined with the improved radio frequency chromatographic imaging technology, to analyze the RSSI signal of the human smoking action. Therefore, the system is specially designed for application scenarios and can realize the height detection of minors and the recognition of smoking behavior. In the end, related comparative experiments were also carried out to verify the effectiveness and real-time performance of the system. Aiming at the classification problem of different age groups, this paper proposed to distinguish population categories and identify smoking behaviors by analyzing RSSI signals extracted from commercial readers. Finally, the validity and real-time performance of the system are verified by comparative experiments .},
  archive      = {J_COMCOM},
  author       = {Biaokai Zhu and Jiayue Wang and Sanman Liu and Meiya Dong and Yanan Jia and Liyuan Tian and Chenyang Su},
  doi          = {10.1016/j.comcom.2021.12.018},
  journal      = {Computer Communications},
  pages        = {55-65},
  shortjournal = {Comput. Commun.},
  title        = {RFMonitor: Monitoring smoking behavior of minors using COTS RFID devices},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based deep learning for communication networks: A
survey. <em>COMCOM</em>, <em>185</em>, 40–54. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication networks are important infrastructures in contemporary society. There are still many challenges that are not fully solved and new solutions are proposed continuously in this active research area. In recent years, to model the network topology , graph-based deep learning has achieved the state-of-the-art performance in a series of problems in communication networks. In this survey, we review the rapidly growing body of research using different graph-based deep learning models, e.g. graph convolutional and graph attention networks , in various problems from different types of communication networks, e.g. wireless networks, wired networks, and software defined networks . We also present a well-organized list of the problem and solution for each study and identify future research directions. To the best of our knowledge, this paper is the first survey that focuses on the application of graph-based deep learning methods in communication networks involving both wired and wireless scenarios. To track the follow-up research, a public GitHub repository is created, where the relevant papers will be updated continuously.},
  archive      = {J_COMCOM},
  author       = {Weiwei Jiang},
  doi          = {10.1016/j.comcom.2021.12.015},
  journal      = {Computer Communications},
  pages        = {40-54},
  shortjournal = {Comput. Commun.},
  title        = {Graph-based deep learning for communication networks: A survey},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Route recommendation for evacuation networks using
MMPP/m/1/n queueing models. <em>COMCOM</em>, <em>185</em>, 23–39. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While natural disasters such as hurricanes, tsunamis, and earthquakes are inevitable, improving existing emergency response approaches and technology becomes critical in today’s smart city development. That is, ensuring evacuees get a fast and effective evacuation route plays a very important role in increasing survival rate and reducing damage during emergency or disaster situations for smart cities. In this research, we investigate the design and development of efficient evacuation routes. By considering the realistic response to emergency evacuations, we present a model based on the number of incoming vehicles in different periods of time. In this paper, we propose a Queueing-based Recommendation system (Q-Recomm) for dynamic route recommendations for evacuees to use at the time of a disaster. Compared with other existing studies that use static conditions, our proposed algorithms consider dynamic road conditions by using queueing theory and graph theory. Specifically, we first characterize the evacuation network of a disaster as a directed acyclic graph and then exploit an M M P P / M / 1 / N MMPP/M/1/N queueing model to derive the optimal route in terms of the minimum driving time by analyzing road congestion, vehicle wait time, and road capacity as well as vehicle arrival rates for chosen routes, using high-end sensors for dynamic data acquisition. We propose the Queueing-based Time Constrained Route Recommendation (Q-TCRR) algorithm and the Queueing-based Minimum Driving Time Route Recommendation (Q-MDTRR) algorithm for route recommendation in evacuation networks. Our experimental results validate the accuracy of Q-TCRR and Q-MDTRR, respectively, and we also demonstrate that the Q-MDTRR algorithm has a lower driving time compared with both MacroServ and the shortest path route based on Dijkstra. Thus, while edge computing in vehicular ad hoc networks becomes a promising solution for evacuation networks, our two proposed algorithms can be flexibly realized in edge computing in vehicular ad hoc networks due to their computational efficiency.},
  archive      = {J_COMCOM},
  author       = {Ting Sun and Jing Lin and Kaiqi Xiong and Chuangbai Xiao},
  doi          = {10.1016/j.comcom.2021.12.002},
  journal      = {Computer Communications},
  pages        = {23-39},
  shortjournal = {Comput. Commun.},
  title        = {Route recommendation for evacuation networks using MMPP/M/1/N queueing models},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combinatorial double auction for resource allocation with
differential privacy in edge computing. <em>COMCOM</em>, <em>185</em>,
13–22. (<a href="https://doi.org/10.1016/j.comcom.2021.11.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application service providers deploy their services by purchasing certain edge computing resources from edge service providers. In this way, they can offer computing or storage support for users which promotes the formation of edge computing resource trading market. However, few research focus on the protection of the privacy of transaction participants. In this view of challenge, we propose a combinatorial double auction scheme based on differential privacy for multi-resource allocation in the trading market. In this scheme, our objective is to maximize the social welfare of the auction and apply the differential privacy based on Gaussian Mechanism to protect the security of the auction market. The simulation results show that the proposed scheme ensures the security and privacy of the auctioneers.},
  archive      = {J_COMCOM},
  author       = {Xutong Jiang and Yuhu Sun and Bowen Liu and Wanchun Dou},
  doi          = {10.1016/j.comcom.2021.11.025},
  journal      = {Computer Communications},
  pages        = {13-22},
  shortjournal = {Comput. Commun.},
  title        = {Combinatorial double auction for resource allocation with differential privacy in edge computing},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computationally efficient topology optimization of
scale-free IoT networks. <em>COMCOM</em>, <em>185</em>, 1–12. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The malicious attacks in the scale-free Internet of Things (IoT) networks create a serious threat for the functionality of nodes. During the malicious attacks , the removal of high degree nodes greatly affects the connectivity of the remaining nodes in the networks. Therefore, ensuring the maximum connectivity among the nodes is an important part of the topology optimization . A good scale-free network has the ability to maintain the functionality of the nodes even if some of them are removed from the network. Thus, designing a robust network to support the nodes’ functionality is the aim of topology optimization in the scale-free networks. Moreover, the computational complexity of an optimization process increases the cost of the network. Therefore, in this paper, the main objective is to reduce the computational cost of the network with the aim of constructing a robust network topology . Thus, four solutions are presented to reduce the computational cost of the network. First, a Smart Edge Swap Mechanism (SESM) is proposed to overcome the excessive randomness of the standard Random Edge Swap Mechanism (RESM). Second, a threshold based node removal method is introduced to reduce the operation of the edge swap mechanism when an objective function converges at a point. Third, multiple attacks are performed in the network to find the correlation between the measures, which are degree, betweenness and closeness centralities. Fourth, based on the third solution, a Heat Map Centrality (HMC) is used that finds the set of most important nodes from the network. The HMC damages the network by utilizing the information of two positively correlated measures. It helps to provide a good attack strategy for robust optimization . The simulation results demonstrate the efficacy of the proposed SESM mechanism. It outperforms the existing RESM mechanism by almost 4\% better network robustness and 10\% less number of swaps. Moreover, 64\% removal of nodes helps to reduce the computational cost of the network.},
  archive      = {J_COMCOM},
  author       = {Muhammad Awais Khan and Nadeem Javaid},
  doi          = {10.1016/j.comcom.2021.12.013},
  journal      = {Computer Communications},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {Computationally efficient topology optimization of scale-free IoT networks},
  volume       = {185},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SDN-based gateway architecture for electromagnetic
nano-networks. <em>COMCOM</em>, <em>184</em>, 160–173. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electromagnetic nano-communication has increasing attention in recent years. Several developments have been achieved in the fabrication, communication and management of various nano-network devices serving potential applications ranging from software-defined metamaterials, wireless robotic materials and body-centric communication. Such applications need uplink and downlink communication between the deployed nano-network and the external macro-world or the Internet through nano-interfaces. As a result, heterogeneous nano-network devices and their interoperability in different Internet of nano-things applications become new challenges for nano-network communication. In this regard, dynamic, flexible and distributed micro/nano-gateways can accommodate such sustainable issues and make the nano-network fully operational, regardless of the adopted application domain or the protocols used in communication. Network functions virtualization and software-defined networking technologies altogether can overcome these challenges. This article proposes SDN-based architecture and software module for the micro/nano-gateway. The proposed software module converts data formats and protocols between nano-network and traditional network domains allowing the nano-devices to be linked to the Internet. A prototype of the module is built, and the performance of the proposed algorithm is evaluated based on two communication scenarios; single tenant and multitenant. The result shows the effect of the total number of connected nano-devices and the number of packets sent by each device on the total average round-trip processing delay and the overall throughput of the micro/nano-gateway.},
  archive      = {J_COMCOM},
  author       = {Akram Galal and Xavier Hesselbach and Wouter Tavernier and Didier Colle},
  doi          = {10.1016/j.comcom.2021.12.017},
  journal      = {Computer Communications},
  pages        = {160-173},
  shortjournal = {Comput. Commun.},
  title        = {SDN-based gateway architecture for electromagnetic nano-networks},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). H.264 and h.265 video traffic modeling using neural
networks. <em>COMCOM</em>, <em>184</em>, 149–159. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As video has become the dominant type of traffic over wired and wireless networks, the efficient transmission of video streams is of paramount importance. Hence, especially for wireless networks, the optimum utilization of the available bandwidth while preserving the users’ Quality of Service and Quality of Experience requirements is crucial. Towards this goal, the accurate prediction of upcoming video frame sizes can play a significant role. This work focuses on achieving such an accurate prediction for videos encoded with H.264 and H.265, which are the major state-of-the-art standards based on their current market share. Unlike previous studies, we use single-step and multi-step approaches to capture the long-range dependence and short-range dependence properties of variable bit rate video traces through neural networks-based modeling. We evaluate the accuracy of Long Short Term Memory , Convolutional Neural Networks and Sequence-to-Sequence models and compare them with existing approaches. Our models show significantly higher accuracy for a variety of videos. We also provide a case study on how our model can be used for traffic policing purposes.},
  archive      = {J_COMCOM},
  author       = {Khandu Om and Tanya McGill and Michael Dixon and Kok Wai Wong and Polychronis Koutsakis},
  doi          = {10.1016/j.comcom.2021.12.014},
  journal      = {Computer Communications},
  pages        = {149-159},
  shortjournal = {Comput. Commun.},
  title        = {H.264 and h.265 video traffic modeling using neural networks},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Presenting a new motif-based link prediction for predicting
activities in facebook. <em>COMCOM</em>, <em>184</em>, 137–148. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motif distribution in different complex networks holds much information about network structure. As a network evolves, some new links appear and its motifs are transitioned into each other and their distribution changes as well as a network structure. The study of how motifs transition to each other can impact on study of complex networks and the link formation process. In this paper, the distribution and transitions of triads in different Facebook activity networks such as like, comment, post, and share networks are studied. After studying motif transitions over time, a new algorithm is presented for link prediction or activity recommendation for Facebook activity networks. In addition, to analyze motif transitions easily as well as speed up the presented algorithm, new concepts for analyzing sub-graphs have been presented including the motif transition graph and also its Hasse diagram. In addition, we have found out that among 53 different triad transitions only 10 of them are valuable in terms of link prediction in activity networks, and this has been used to accelerate the link prediction algorithm. The performed experiments show that the presented method has better results versus previous link prediction methods.},
  archive      = {J_COMCOM},
  author       = {Ehsan Khadangi and Amin Shahmohammadi and Sara Zal and Hamid Reza Esmaeili},
  doi          = {10.1016/j.comcom.2021.11.016},
  journal      = {Computer Communications},
  pages        = {137-148},
  shortjournal = {Comput. Commun.},
  title        = {Presenting a new motif-based link prediction for predicting activities in facebook},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight framework for abnormal driving behavior
detection. <em>COMCOM</em>, <em>184</em>, 128–136. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most serious hazards in the world, traffic accidents have caused huge casualties and property losses. The detection of abnormal behaviors of drivers is of great importance for intelligence transportation systems. However, the training of most abnormal behavior detection artificial intelligence algorithms demands huge computation and memory resources. Therefore, the training process is preferred to be done in cloud. In this paper, we propose a lightweight abnormal driving behavior detection framework, which applies intelligence to the edge and gives the IoT (Internet of Things) devices the ability to process and understand the data. The framework consists of four modules: bus driver wearing mask detection, bus driver abnormal motion detection, bus driver fatigue driving detection, and video recovery. Comprehensive experiments on real-world datasets have demonstrated the effectiveness and applicability of the proposed framework.},
  archive      = {J_COMCOM},
  author       = {Mingliang Hou and Mengyuan Wang and Wenhong Zhao and Qichao Ni and Zhen Cai and Xiangjie Kong},
  doi          = {10.1016/j.comcom.2021.12.007},
  journal      = {Computer Communications},
  pages        = {128-136},
  shortjournal = {Comput. Commun.},
  title        = {A lightweight framework for abnormal driving behavior detection},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-channel opportunistic spectrum access: A mixed-scale
decision perspective. <em>COMCOM</em>, <em>184</em>, 118–127. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a slot-based opportunistic communication system consisting of one transmitter, one receiver, and N N two-state Markov channels. In each K K continuous time slots on a large time scale, the transmitter probes one of N N channels and chooses one to access in each time slot of the K K time slots on a small time scale. For each successful access, one unit of reward is obtained. To maximize the cumulated reward over a time horizon of T T , the joint probing (on a large time scale) and accessing (on a small time scale) problem can be cast into a mixed-scale partially observable Markovian decision process which is proved to PSPACE-Hard. Then the mixed-scale sequential decision-making problem is simplified into a probing decision problem on a large time scale. Considering the huge computing complexity of the large-scale probing decision, we present a simple heuristic policy which is to probe the best or the second-best channel in terms of available probability under different probing conditions regarding missing detection rate and false alarm one. Next, we derive several sets of sufficient conditions for different scenarios under which the proposed heuristic policy is optimal. Finally, the results of numerical experiments verify our theoretical analysis.},
  archive      = {J_COMCOM},
  author       = {Helong Shen and Kehao Wang and Jihong Yu and Lin Chen},
  doi          = {10.1016/j.comcom.2021.12.012},
  journal      = {Computer Communications},
  pages        = {118-127},
  shortjournal = {Comput. Commun.},
  title        = {Multi-channel opportunistic spectrum access: A mixed-scale decision perspective},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latency and quality-aware task offloading in multi-node next
generation RANs. <em>COMCOM</em>, <em>184</em>, 107–117. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-Generation Radio Access Network (NG-RAN) is an emerging paradigm that provides flexible distribution of cloud computing and radio capabilities at the edge of the wireless Radio Access Points (RAPs). Computation at the edge bridges the gap for roaming end users, enabling access to rich services and applications. In this paper, we propose a multi-edge node task offloading system, i.e., QLRan, a novel optimization solution for latency and quality tradeoff task allocation in NG-RANs. Considering constraints on service latency, quality loss, edge capacity, and task assignment, the problem of joint task offloading , latency, and Quality Loss of Result (QLR) is formulated in order to minimize the User Equipment (UEs) task offloading utility, which is measured by a weighted sum of reductions in task completion time and QLR cost. The QLRan optimization problem is proved as a Mixed Integer Nonlinear Program (MINLP) problem, which is a NP-hard problem. To efficiently solve the QLRan optimization problem , we utilize Linear Programming (LP)-based approach that can be later solved by using convex optimization techniques. Additionally, a programmable NG-RAN testbed is presented where the Central Unit (CU), Distributed Unit (DU), and UE are realized by USRP boards and fully container-based virtualization approaches. Specifically, we use OpenAirInterface (OAI) and Docker software platforms to deploy and perform the NG-RAN testbed for different functional split options. Then, we characterize the performance in terms of data input, memory usage, and average processing time with respect to QLR levels. Simulation results show that our algorithm performs significantly improves the network latency over different configurations.},
  archive      = {J_COMCOM},
  author       = {Ayman Younis and Brian Qiu and Dario Pompili},
  doi          = {10.1016/j.comcom.2021.11.026},
  journal      = {Computer Communications},
  pages        = {107-117},
  shortjournal = {Comput. Commun.},
  title        = {Latency and quality-aware task offloading in multi-node next generation RANs},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wireless network security game based on conditional privacy
policy. <em>COMCOM</em>, <em>184</em>, 96–106. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet technology, wireless body area network technology is proposed to improve people’s scientific and technological information level in this environment. Since the information collected and processed by wireless body area network is the privacy data of users, security and privacy are the focus of wireless body area network research. In this paper, a scheme to establish a variable radius mixing region is proposed. This scheme obtains the combined communication range by analyzing the user privacy conditions after the hybrid zone is established. According to the different conditions of users, a variable radius dynamic hybrid zone was established to maximize benefits. After the establishment of the hybrid zone is initiated, the members in the zone use the temporary symmetric secret key obtained from the unit for encrypted communication to ensure the demand for real-time message transmission. And finally an anonymous change protocol is proposed. The experimental data prove the feasibility of the renaming scheme of the mixed zone. Compared with other methods, the computation and traffic of this protocol are increased by 7\% and 9\% respectively. The validation efficiency of batch validation algorithm is 93\%. The scheme in this paper is more flexible than the establishment of other fixed mixed zones, and the accuracy of this scheme is more accurate than the establishment of general dynamic mixed zones. Finally, it is proved that this scheme not only meets the conditional privacy policy but also uses the model to establish a dynamic hybrid area, which has more accurate accuracy and lower computational consumption, and plays an important role in the security protection of network privacy.},
  archive      = {J_COMCOM},
  author       = {Yanliang Yu and Wenying Peng and Jingfu Lu},
  doi          = {10.1016/j.comcom.2021.12.011},
  journal      = {Computer Communications},
  pages        = {96-106},
  shortjournal = {Comput. Commun.},
  title        = {Wireless network security game based on conditional privacy policy},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel security framework for densely populated internet of
things users in pervasive service access. <em>COMCOM</em>, <em>184</em>,
86–95. (<a href="https://doi.org/10.1016/j.comcom.2021.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User density in the Internet of Things (IoT) platform changes with different applications and user demands. Security requirements are adaptable based on the services and the authentication servers attached to the resources. However, the lack of unanimous security measures increases the complexity and authentication level in a user-centric scenario. This article introduces an Ascendable Authentication Framework (AAF) for balancing security and services in a dense IoT by considering this drawback. The proposed framework adopts a high level of authentication and user services through unanimous keying and allocations. This process is defined within the framework covering the service provider and end-users. In this authentication, hyper-elliptic curve cryptography is exploited for service authentication, whereas the key distribution follows a discrete allocation process. Here the discreteness and continuity in authentication are determined using regression learning. The change in distribution function determines the authentication level and redeeming security for further service disseminations. Therefore, the regression sequence is modified without augmenting additional keys in a pervasive manner. This reduces the computation complexity, service authentication time, response latency , and failures.},
  archive      = {J_COMCOM},
  author       = {Panneerselvam N. and Krithiga S.},
  doi          = {10.1016/j.comcom.2021.11.018},
  journal      = {Computer Communications},
  pages        = {86-95},
  shortjournal = {Comput. Commun.},
  title        = {A novel security framework for densely populated internet of things users in pervasive service access},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental investigation of round-trip time and
virtualization. <em>COMCOM</em>, <em>184</em>, 73–85. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualization is omnipresent in our networks and data centers . Little is known of its overhead, especially regarding the deployment of Virtual Network Functions and the use of existing network active measurement tools. This paper evaluates the impact on Round-Trip Time of parameters such as CPU affinity, the frequency of injection measurements echo packets, the type of virtual network driver, the use of CPU, I/O or network overload, and the number of background VMs. We compare RTT results using three virtualization technologies, namely, KVM, LXC , and Docker containers . To isolate the role of virtualization on packet timing overhead, we also include kernel-level measurement points in our experiments. We discover that the parameters differ in their impact on RTT, and that their effect changes according to how they are combined. The results reported in this paper allow a Cloud administrator to understand the overhead of virtualization technologies such as KVM, LXC , and Docker containers as well as the impact of virtual network drivers, disk I/O overhead, CPU processing and combinations therein, on network performance.},
  archive      = {J_COMCOM},
  author       = {Assis T. de Oliveira Filho and Eduardo Freitas and Pedro R.X. do Carmo and Djamel H.J. Sadok and Judith Kelner},
  doi          = {10.1016/j.comcom.2021.12.006},
  journal      = {Computer Communications},
  pages        = {73-85},
  shortjournal = {Comput. Commun.},
  title        = {An experimental investigation of round-trip time and virtualization},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximizing lower bound of energy efficiency in multi-tier
heterogeneous cellular network via stochastic geometry. <em>COMCOM</em>,
<em>184</em>, 64–72. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an optimization method is proposed to maximize the lower bound of energy efficiency (EE) in the multi-tier heterogeneous cellular network (HCN) and resolved by adjusting Base Station (BS) density and BS transmit power. Firstly, we derive the mathematical formulation of network coverage probability and the average user rate. We prove that the network coverage probability and the average user rate monotonically increase with the cumulative sum of the product of BS density and BS transmit power. Then, the network coverage probability and the average user rate requirements are converted to the BS density and the BS transmit power constraints. An EE maximization problem is formulated. Nevertheless, the intractable problem is reformulated by maximizing the lower bound of EE since the coupled parameters are tight. The simplified optimization problem can be converted to a standard form of geometric programming (GP) by a convex approximation method. The problem in GP form can be further transformed into the traditional convex problem and can be fully solved using the interior point method . Numerical simulations show that the optimal solutions can be determined with fast convergence, while the network EE is significantly improved. The gap between the approximate and the original solutions is analyzed through simulations.},
  archive      = {J_COMCOM},
  author       = {Zhixin Liu and Heng Zhu and Yazhou Yuan and Kit Yan Chan and Yi Yang and Xinping Guan},
  doi          = {10.1016/j.comcom.2021.12.010},
  journal      = {Computer Communications},
  pages        = {64-72},
  shortjournal = {Comput. Commun.},
  title        = {Maximizing lower bound of energy efficiency in multi-tier heterogeneous cellular network via stochastic geometry},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A resource allocation deep active learning based on load
balancer for network intrusion detection in SDN sensors.
<em>COMCOM</em>, <em>184</em>, 56–63. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic traffic in a software-defined network (SDN) causes explosive data to flow from one system to another. The explosive data affects the functionality of system parameters, network-level configuration, routing parameters, network characteristics, and system load factors. Adapting to the traffic flow is a key research area in SDN in today’s big data world. Load balance vehicular sensor accessibility reduces delays, lowers energy consumption , and decreases the execution time. This paper combines the entropy-based active learning model to identify intrusion patterns efficiently, which is a packet-level intrusion detection model. The developed afterload balancing model can track the attack on the network. We then proposed a load balancing algorithm that optimizes the vehicular sensor usability by using sensor computing capability and source needs. We make use of a convergence-based mechanism to achieve high resource utilization. We then perform experiments on the state-of-the-art intrusion detection dataset. Our experimental results show that the load balancing mechanism can achieve 2 × 2× in performance improvements compared to traditional approaches. Thus, we can see that the designed model can help improve the decision boundary by increasing the training instance through pooling strategy and entropy uncertainty measure.},
  archive      = {J_COMCOM},
  author       = {Usman Ahmed and Jerry Chun-Wei Lin and Gautam Srivastava},
  doi          = {10.1016/j.comcom.2021.12.009},
  journal      = {Computer Communications},
  pages        = {56-63},
  shortjournal = {Comput. Commun.},
  title        = {A resource allocation deep active learning based on load balancer for network intrusion detection in SDN sensors},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Multi-task equilibrium scheduling of internet of things: A
rough set genetic algorithm. <em>COMCOM</em>, <em>184</em>, 42–55. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) server suffers from numerous business traffic with network bandwidth growth, resulting in downtime. Providing business support without affecting the user experience is the primary problem that IoT companies need to consider and solve in the face of traffic impact. This paper proposes a load balancing scheduling algorithm based on Particle Swarm Optimization Genetic Algorithm (PSO-GA) for IoT clusters. The algorithm uses CPU occupancy rate, memory occupancy rate, network bandwidth occupancy rate, and disk Input and Output (IO) occupancy rate to comprehensively measure the server node load and establish a resource balance model. The fitness function is used to quantify the influence as the basis of weight adjustment. Then, the Particle Swarm Optimization (PSO) algorithm uses the disturbance factor and contraction operator. The optimized algorithm is used to calculate the optimal solution of the fitness function and obtain the optimal weight. Finally, the PSO-GA algorithm is simulated, tested, and compared with the other three load balancing algorithms. As seen from the test results of response delay, throughput, request error rate, and resource utilization, the performance of this algorithm is improved by more than 5\% compared with the performance of the traditional method, and the optimization ability is improved obviously. The research content of this paper provides a new way to alleviate the network load, reduce the server overload, congestion, downtime, and other problems, and realize the multi-task balanced scheduling of IoT.},
  archive      = {J_COMCOM},
  author       = {Bing Bu},
  doi          = {10.1016/j.comcom.2021.11.027},
  journal      = {Computer Communications},
  pages        = {42-55},
  shortjournal = {Comput. Commun.},
  title        = {Multi-task equilibrium scheduling of internet of things: A rough set genetic algorithm},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic power allocation strategy for uplink non-orthogonal
multiple access systems. <em>COMCOM</em>, <em>184</em>, 36–41. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power allocation has a significant impact on sum rate in non-orthogonal multiple access (NOMA) systems. In this paper, we propose a dynamic power allocation (D-NOMA) strategy that can optimize the sum rate compared with the fixed power allocation (F-NOMA) method in uplink NOMA systems. In particular, we first design an evaluation model to analyze several key constraints for optimization objectives , and then derive the optimal power allocation coefficients. The simulation results demonstrate that the proposed strategy outperforms the existing fixed power allocation strategies with different coefficients and occupies more advantage on the low SNR than other D-NOMA scheme in typical uplink pairing scenarios.},
  archive      = {J_COMCOM},
  author       = {Xingwei Wang and Tianheng Xu and Ting Zhou and Honglin Hu},
  doi          = {10.1016/j.comcom.2021.12.008},
  journal      = {Computer Communications},
  pages        = {36-41},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic power allocation strategy for uplink non-orthogonal multiple access systems},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-mobile agent and optimal itinerary planning-based
data aggregation in wireless sensor networks. <em>COMCOM</em>,
<em>184</em>, 24–35. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the energy constraints in Sensor Nodes (SN) and to increase energy efficiency in Data Aggregation (DA) approaches, Wireless Sensor nodes (WSN) requires a suitable network’s life cycle. In order to address the energy efficiency, a new system model is proposed for competent Data Aggregation (DA), and Data processing (DP) mechanism with the amalgamation of Mobile Agent (MA) in WSN intended towards collaborative signal systematized with information processing. Furthermore, with the expansion of Mobile Agent (MA) centred WSN, unique resource-efficient potentials for Data Aggregation (DA) and Data processing (DP) applications are identified. In the proposed system model, a single Mobile Agent (MA) is employed for data processing. Hence the MA-centred paradigm’s fundamental difficulty is determining the best route for agent traversal. Henceforth, using the Brownian Motion-Based Flower Pollination Algorithm (BMFPA), a new multi-MA-centred optimal itinerary planning strategy for performing data aggregation in WSN is proposed. Further, the proposed approach uses Fitness based Fuzzy C-Means (FFCM) for cluster creation and Cluster Head (CH) selection. Crossover Mutation based Firefly Algorithm (CM-FFA) is used. Moreover, the proposed model uses optimal itinerary planning-based Brownian Motion-Based Flower Pollination Algorithm (BMFPA) for MA migration and Data Gathering (DG) in WSN. Finally, after data gathering, the proposed model, the mobile Agent collects the information and communicates it to the base station or sink based on various applications scenarios. The system model’s investigation outcomes evidently exhibit that the proposed work competently performs well than prevailing algorithms for Data Gathering (DG) in WSN. Furthermore, the proposed model method can be fully utilized in a virtual wireless network’s scenario.},
  archive      = {J_COMCOM},
  author       = {Karthik S. and Karthick M. and Karthikeyan N. and Kannan S.},
  doi          = {10.1016/j.comcom.2021.11.019},
  journal      = {Computer Communications},
  pages        = {24-35},
  shortjournal = {Comput. Commun.},
  title        = {A multi-mobile agent and optimal itinerary planning-based data aggregation in wireless sensor networks},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Access-oblivious and privacy-preserving k nearest neighbors
classification in dual clouds. <em>COMCOM</em>, <em>184</em>, 12–23. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive data and data services (such as online classification) can be outsourced to the cloud for better processing and less operation cost. However, if the sensitive information is contained in outsourced data , and user’s requests and returned results are visible and controlled by the semi-trusted cloud server, this will lead to serious privacy leakage . To achieve secure and efficient outsourced classification services, both high security and low computation burden should be considered at the same time, but the existing works always focus on only one aspect of them. In this paper, a Privacy-Preserving K-Nearest-Neighbors Classification (PPKNNC) scheme based on two non-colluding servers is proposed. Specifically, two efficient privacy-preserving protocols and random permutation technique are used in the processing of user’s requests, which greatly reduces the computational cost and protects the access pattern. Besides, an Elgamal cryptosystem technique called proxy re-encryption is adopted to ensure that every user has an individual secret key, so that the exact key for data decryption is known only to the data owner and not to other entities. Finally, the security analysis and the experiment results further illustrate that the proposed scheme is superior to the existing works in terms of security, computational and communication costs.},
  archive      = {J_COMCOM},
  author       = {Bin Xie and Tao Xiang and Xiaofeng Liao},
  doi          = {10.1016/j.comcom.2021.12.005},
  journal      = {Computer Communications},
  pages        = {12-23},
  shortjournal = {Comput. Commun.},
  title        = {Access-oblivious and privacy-preserving k nearest neighbors classification in dual clouds},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LoRaWAN vs. 6TiSCH: Which one scales better?
<em>COMCOM</em>, <em>184</em>, 1–11. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) has been a hot topic in both academia and industry for the last few years. There are many possible applications, in fields as smart cities, home automation, smart buildings , agriculture, automated metering, logistic, industrial automation, among others. Such a wide range of applications resulted in various technological solutions in order to enable Machine-to-Machine (M2M) communications, in which we highlight those for wide area networks. In that sense, two widely used sub-GHz protocols for Low Power Wide Area (LPWA) communications are: Long Range (LoRa), using the upper layers defined by Long Range Wide Area Network (LoRaWAN); and IEEE 802.15.4g, using the upper layers defined by IPv6 over the Time-Slotted Channel Hopping (TSCH) mode of IEEE 802.15.4e (6TiSCH). While LoRaWAN is a well-known and widespread protocol, 6TiSCH offers IPv6 connectivity to LPWA networks and is used in important standards such as Wireless Smart Ubiquitous Network (Wi-SUN). This paper aims at determining how well each protocol scales, analyzing different aspects such as packet loss , delay, and the maximum number of nodes per square area. In order to achieve such results, computer simulations are performed using open-source simulators. The obtained results demonstrate that the best scalability depends on which scenarios are considered. For scenarios with multiple gateways requiring low latency or low packet transmission rates, LoRaWAN demonstrates better results. However, in scenarios with high packet transmission rates and where the latency is not a major concern, 6TiSCH is more appropriate. Moreover, even in the best case, the latency associated with the LoRaWAN technology may be considerably smaller than associated with 6TiSCH. Such novelty results are obtained in a fair and realistic way and, noting that both technologies could be used to build LPWA networks , it can help system designers to decide between the two technologies.},
  archive      = {J_COMCOM},
  author       = {João Luís Verdegay de Barros and Marcos Eduardo Pivaro Monteiro and Guilherme de Santi Peron and Guilherme Luiz Moritz and Ohara Kerusauskas Rayel and Richard Demo Souza},
  doi          = {10.1016/j.comcom.2021.12.004},
  journal      = {Computer Communications},
  pages        = {1-11},
  shortjournal = {Comput. Commun.},
  title        = {LoRaWAN vs. 6TiSCH: Which one scales better?},
  volume       = {184},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Game theory-based energy efficiency optimization model for
the internet of things. <em>COMCOM</em>, <em>183</em>, 171–180. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper conducts a game-theoretic based optimization study on the energy efficiency of the Internet of Things (IoT). For the problem that wireless sensor node information in the IoT requires the selection of a suitable backbone access point for energy efficiency optimization, this paper first establishes a mathematical model for the system-level energy optimization of the sensor node free-choice access point problem and then proposes a game model based on the concept of cooperation and the corresponding utility function, and after theoretical analysis. Among the new connections in the future, more than 30\% are suitable for carrying by the cellular network , so the network challenges brought by mobile operators are huge. The paper then studied the current mainstream Internet of Things technology in the development of mobile networks—the​ application principles and key technologies of Narrow Band Internet of Things (NB-IoT), combined with the current wireless network optimization and maintenance work, and studied How does the Internet of Things become the focus of the industry and actively lay out and promote industrial development for operators, and how does NB-IoT move from a concept to small-scale commercial use, become the operator’s leading role in promoting the standard industry, and hope to become an industry leader It is proved that the best access point allocation scheme is the optimal equilibrium point of the proposed game. Then a non-correlated parallel learning algorithm is proposed, according to which the system can converge to the optimal equilibrium point with a very low probability after learning, which is the optimal solution of the proposed system energy efficiency optimization problem , and achieve the global optimal system energy efficiency. Compared with other models, our model has improved efficiency by about 12\% and accuracy by about 8\%, and it can be applied in practice.},
  archive      = {J_COMCOM},
  author       = {Xiaoxia Zeng},
  doi          = {10.1016/j.comcom.2021.12.001},
  journal      = {Computer Communications},
  pages        = {171-180},
  shortjournal = {Comput. Commun.},
  title        = {Game theory-based energy efficiency optimization model for the internet of things},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latency-aware service provisioning in survivable multilayer
IP-over-elastic optical networks to support multi-class of service
transmission. <em>COMCOM</em>, <em>183</em>, 161–170. (<a
href="https://doi.org/10.1016/j.comcom.2021.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the requirements of the emerging applications, future networks must transmit multiple class of services traffic with diverse quality of service (QoS) requirements. In this paper, we formulate an integer linear programming (ILP) to design multilayer internet protocol (IP) over elastic optical networks (EONs), IP-over-EONs to support multi-class services with different latency and availability requirements. The objective of the ILP is to minimize capital expenditure (CAPEX) and spectrum usage while satisfying all QoS constraints in terms of latency and survivability mechanism. Furthermore, a heuristic algorithm is proposed to solve this problem in large-scale networks. We compare the results of the ILP formulation and the proposed heuristic algorithm in a 5-node network, where our results reveal that the performance gaps between the ILP and heuristic algorithm are on average 2.4\% and 9.8\% in terms of CAPEX and occupied spectrum, respectively, while the heuristic algorithm has a lower run time. Moreover, we quantitatively analyze the effects of end-to-end latency on the CAPEX of multilayer IP-over-EONs. Our results reveal that serving latency-sensitive demands increases the CAPEX of the network. This is because all-optical connections (i.e., single-hop IP connections) must be established to meet the delay constraints, which in turn leads to a higher CAPEX and occupied spectrum. Furthermore, we compare the effects of using different types of transponders in three practical topologies and evaluate the impacts of the length of the topology links on CAPEX and spectrum usage. Finally, we propose the best transponder types that could be used for each topology based on the gathered results.},
  archive      = {J_COMCOM},
  author       = {Ehsan Etezadi and Hamzeh Beyranvand and Jawad A. Salehi},
  doi          = {10.1016/j.comcom.2021.12.003},
  journal      = {Computer Communications},
  pages        = {161-170},
  shortjournal = {Comput. Commun.},
  title        = {Latency-aware service provisioning in survivable multilayer IP-over-elastic optical networks to support multi-class of service transmission},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance based risk driven trust (PRTrust): On modeling
of secured service sharing in peer-to-peer federated cloud.
<em>COMCOM</em>, <em>183</em>, 136–160. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peer-to-peer (P2P) federated cloud system has emerged as a promising service delivery model wherein multiple cloud service providers (CSPs) collaborate and share resources and services among themselves to fulfill the spikes in demand from their cloud service consumers (CSCs). It facilitates CSPs to accomplish the committed service level agreements (SLAs) to their CSCs. However, the lack of preexisting trust relationships and unawareness about the cloud infrastructure and service delivery performance among CSPs in a distributed environment poses a risk to the quality of service (QoS) being delivered. We address this challenge by proposing a trust model, PRTrust , for a peer-to-peer federated cloud system that capitalizes the triangular relationship of performance, risk, and trust for the participating CSPs. The main contributions of this work are: (1) providing a logical design and architecture for trust and performance management in a peer-to-peer federated cloud system; (2) providing a two-tier weighted performance evaluation mechanism for CSPs; (3) providing a risk evaluation mechanism for CSPs based on their current performance level; (4) providing an improvised mechanism to evaluate and manage personalized reputation-based trust for the participating CSPs; (5) providing a CSP selection mechanism from a trusted list of CSPs, using the evaluated performance-based risk, for sharing the resources and services in a peer-to-peer federated cloud system. The proposed PRTrust model has shown better threat resilient behavior in dealing with malicious peer CSPs when compared with the reference EigenTrust model.},
  archive      = {J_COMCOM},
  author       = {Rakesh Kumar and Rinkaj Goyal},
  doi          = {10.1016/j.comcom.2021.11.013},
  journal      = {Computer Communications},
  pages        = {136-160},
  shortjournal = {Comput. Commun.},
  title        = {Performance based risk driven trust (PRTrust): On modeling of secured service sharing in peer-to-peer federated cloud},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel edge computing architecture based on adaptive
stratified sampling. <em>COMCOM</em>, <em>183</em>, 121–135. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Internet of Things technology, the current amount of data generated by the Internet of Things system is increasing, and these data are continuously transmitted to the data center . The data processing and analysis of the traditional Internet of Things system are inefficient and cannot handle such a large number of data streams. In addition, the IoT smart device has a resource-limited feature, which cannot be ignored when analyzing data. This paper proposes a new architecture ApproxECIoT (Approximate Edge Computing Internet of Things, ApproxECIoT) suitable for real-time data stream processing of the Internet of Things. It implements a self-adjusting stratified sampling algorithm to process real-time data streams. The algorithm adjusts the size of the sample stratums according to the variance of each stratum while maintaining the given memory budget. This is beneficial to improve the accuracy of the calculation results when resources are limited. Finally, the experimental analysis was performed using synthetic datasets and real-world datasets, the results show that ApproxECIoT can still obtain high-accuracy calculation results when using memory resources similar to simple random sampling. In the case of synthetic data streams, when the sampling ratio is 10\%, compared with CalculIoT, the accuracy loss of ApproxECIoT is reduced by 89.6\%; compared with SRS, the accuracy loss of ApprxoECIoT is reduced by 99.8\%. In the case of using the real data stream of the wireless sensor network , the performance of ApproxECIoT is not the best, but as the sampling ratio increases, the accuracy loss of ApproxECIoT decreases more than other frameworks.},
  archive      = {J_COMCOM},
  author       = {De-gan Zhang and Chen-hao Ni and Jie Zhang and Ting Zhang and Peng Yang and Jia-xu Wang and Hao-ran Yan},
  doi          = {10.1016/j.comcom.2021.11.012},
  journal      = {Computer Communications},
  pages        = {121-135},
  shortjournal = {Comput. Commun.},
  title        = {A novel edge computing architecture based on adaptive stratified sampling},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain-based group key agreement protocol for vehicular
ad hoc networks. <em>COMCOM</em>, <em>183</em>, 107–120. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular ad hoc network (VANET) has been attracting widespread attention because of its ability to improve traffic efficiency and safety. Since insecure wireless communication channels face various security threats (eavesdropping, tampering, forgery attack , etc.), many group key agreement protocols have been proposed for VANET. However, most of them rely on a trusted authority (TA) to perform authentication or group key generation. Excessive dependence on TA may lead to a single point of failure . Therefore, this paper designs a decentralized group key agreement protocol to solve this problem. TA is not involved in authentication or key agreement process after user registration. With blockchain technology , rode-side unit (RSU) and on-board units (OBUs) within its communication range can negotiate a group key without private information leakage . Moreover, the proposed protocol supports dynamic management (verification, update, and revocation) of vehicles’ public keys . Formal security proof demonstrates that the proposed protocol meets basic security requirements. Besides, performance evaluation and comparison show that the proposed protocol requires lower computation costs and communication overheads than some existing protocols.},
  archive      = {J_COMCOM},
  author       = {Xincheng Li and Xinchun Yin},
  doi          = {10.1016/j.comcom.2021.11.023},
  journal      = {Computer Communications},
  pages        = {107-120},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-based group key agreement protocol for vehicular ad hoc networks},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk model of financial supply chain of internet of things
enterprises: A research based on convolutional neural network.
<em>COMCOM</em>, <em>183</em>, 96–106. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the financial supply chain provides assistance for small, medium and micro enterprises in the supply chain through a secured credit model based on real trade. Moreover, in the multi-level structure of the financial supply chain of the Internet of Things enterprise, there are information barriers and information islands. Besides, data is often not transmitted smoothly, and the intermediate offline process is complicated. What is worse, the efficiency is low, and the verification cost is high. Therefore, based on supply chain finance , an evolutionary risk model is constructed in this paper. Firstly, the income matrix of the regulatory risk model is established, and the convolutional neural network used will pool the training data to the maximum and set the local corresponding normalization layer. With the help of the evolutionary risk theory, the dynamic equation of the financial supply chain is obtained, forming the dynamic path and abnormal model of strategy selection. Then, a compact pattern tree is added to the knowledge granularity method to mine data anomalies. Finally, an experimental platform is built to verify the effectiveness of the method proposed in this paper, and experiments are performed on the accuracy of model evolution conditions, abnormal data identification, and abnormal numerical examples. The experimental results prove that the algorithm in this paper is consistent with the set parameters, and the effect is significantly higher than other comparison methods. The experimental mining time and the comparison method are shortened by 6 ∼ ∼ 13S. The research results obtained from this paper solve the problem that the decision-making of supply chain finance and the supervision and review of supply chain enterprise are complex, which improves the characteristics identification of supply chain platform, and provides reference suggestions for financial institutions and supply chain platforms.},
  archive      = {J_COMCOM},
  author       = {Jingfu Lu and Xu Chen},
  doi          = {10.1016/j.comcom.2021.10.026},
  journal      = {Computer Communications},
  pages        = {96-106},
  shortjournal = {Comput. Commun.},
  title        = {Risk model of financial supply chain of internet of things enterprises: A research based on convolutional neural network},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Security-aware spectrum sharing for NOMA in cognitive radio
networks with discrete-time energy harvesting. <em>COMCOM</em>,
<em>183</em>, 83–95. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a security-aware spectrum sharing scheme for wireless-powered cognitive radio networks (CRNs) with non-orthogonal multiple access (NOMA). In this system, a primary transmitter (PT) intends to send confidential signal to a primary receiver (PR) by assisting of a secondary transmitter (ST), while there is a potential eavesdropper (PE) within the scope of PR. It is assume that ST has no fixed energy supply but the NOMA cognitive transmission can be performed after harvesting sufficient radio frequency (RF) energy. To ensure that PT’s confidential signal is safety, PR will be equipped with two antennas and operating in full-duplex mode, so that while ST is performing cognitive transmission, one of the antennas sends artificial noise (AN) from PR to interfere the signal receiving of PE. According to whether the accumulated energy of ST is sufficient and whether ST can correctly decode the PT’s signal, the system can operate in three modes. Since ST needs to accumulate enough energy through several continuous transmission slots, the charging and discharging processes of the battery in the ST can be simulated as a discrete-time Markov chain. Based on these, we derive exact expressions of outage probabilities of the primary and secondary systems, then an approximate formula of secrecy outage probability (SOP) of primary system is also derived. In addition, in order to further improve the transmission performance of the system, optimal power allocation factor and power level of AN are obtained by maximizing the secrecy rate of the primary system, while guaranteeing the system energy efficiency. Compared with zero-forcing (ZF) technique, the proposed secure spectrum sharing scheme has a slight disadvantage in secrecy energy efficiency, but it is simple to deploy, low cost, and achieves a better secrecy rate, which can be applied for a variety of application scenarios.},
  archive      = {J_COMCOM},
  author       = {Fuan Xiao and Xiaowu Li and Kun Tang},
  doi          = {10.1016/j.comcom.2021.11.017},
  journal      = {Computer Communications},
  pages        = {83-95},
  shortjournal = {Comput. Commun.},
  title        = {Security-aware spectrum sharing for NOMA in cognitive radio networks with discrete-time energy harvesting},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards security automation in software defined networks.
<em>COMCOM</em>, <em>183</em>, 64–82. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) is a modern paradigm that provides a platform for implementing reliable, centrally managed, and automated security solutions for conventional and new generation networks, such as IoT , cloud computing , 5G/6G mobile communication networks, and vehicular communications . In these complex systems, manual security operations can delay or obstruct the identification, mitigation, and prevention of ever-increasing sophisticated threats. Thus, the idea of security automation for networks using the SDN paradigm has become fundamental, given that SDN was created to facilitate the operation and management of complex networks with minimal human intervention, which is considered error-prone. This survey studies the state-of-the-art research efforts concerned with security automation in SDN environments. We identified and ranked various classes of security solutions with different levels of automation and complexity. The level of automation is measured using four well-defined qualitative parameters: self-healing, self-adaptation, self-configuration, and self-optimization. The complexity is characterized by the amount of processing and storage resources and implementation requirements. This work represents the first endeavor to analyze the level of automation and complexity of security solutions in SDN environments. Our findings reveal important advances in the area of security automation in SDN. However, there are still several open problems and challenges, which we detail in this work.},
  archive      = {J_COMCOM},
  author       = {Noe M. Yungaicela-Naula and Cesar Vargas-Rosales and Jesús Arturo Pérez-Díaz and Mahdi Zareei},
  doi          = {10.1016/j.comcom.2021.11.014},
  journal      = {Computer Communications},
  pages        = {64-82},
  shortjournal = {Comput. Commun.},
  title        = {Towards security automation in software defined networks},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAN energy efficiency and failure rate through ANN traffic
predictions processing. <em>COMCOM</em>, <em>183</em>, 51–63. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving .},
  archive      = {J_COMCOM},
  author       = {Greta Vallero and Daniela Renga and Michela Meo and Marco Ajmone Marsan},
  doi          = {10.1016/j.comcom.2021.11.011},
  journal      = {Computer Communications},
  pages        = {51-63},
  shortjournal = {Comput. Commun.},
  title        = {RAN energy efficiency and failure rate through ANN traffic predictions processing},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning based transmission policy
enforcement and multi-hop routing in QoS aware LoRa IoT networks.
<em>COMCOM</em>, <em>183</em>, 33–50. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The LoRa wireless connectivity has become a de facto technology for intelligent critical infrastructures such as transport systems. Achieving high Quality of Service (QoS) in cooperative systems remains a challenging task in LoRa. However, high QoS can be achieved via optimizing the transmission policy parameters such as spreading factor, bandwidth, code rate and carrier frequency. Yet existing approaches have not optimized the complete LoRa parameters. Furthermore, the star of stars topology used by LoRa causes more energy consumption and a low packet reception ratio. Motivated by this, this paper presents transmission policy enforcement and multi-hop routing for QoS-aware LoRa networks (MQ-LoRa). A hybrid cluster root rotated tree topology is constructed in which gateways follow a tree topology and Internet of Things (IoT) nodes follow a cluster topology. A ‘membrane’ inspired form the cell tissues which form clusters to sharing the correct information. The membrane inspired clustering algorithm is developed to form clusters and an optimal header node is selected using the influence score. Data QoS ranking is implemented for IoT nodes where priority and non-priority information is identified by the new field of LoRa frame structure (QRank). The optimal transmission policy enforcement uses fast deep reinforcement learning called Soft Actor Critic (SAC) that utilizes the environmental parameters including QRank, signal quality and signal-to-interference-plus-noise-ratio. The transmission policy is optimized with respect to the spreading factor, code rate, bandwidth and carrier frequency. Then, a concurrent optimization multi-hop routing algorithm that uses mayfly and shuffled shepherd optimization to rank routes based on the fitness criteria. Finally, a weighted duty cycle is implemented using a multi-weighted sum model to reduce resource wastage and information loss in LoRa IoT networks. Performance evaluation is implemented using a NS3.26 LoRaWAN module. The performance is examined for various metrics such as packet reception ratio, packet rejection ratio, energy consumption, delay and throughput. Experimental results prove that the proposed MQ-LoRa outperforms the well-known LoRa methods.},
  archive      = {J_COMCOM},
  author       = {Mohammed Saleh Ali Muthanna and Ammar Muthanna and Ahsan Rafiq and Mohammad Hammoudeh and Reem Alkanhel and Stephen Lynch and Ahmed A. Abd El-Latif},
  doi          = {10.1016/j.comcom.2021.11.010},
  journal      = {Computer Communications},
  pages        = {33-50},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based transmission policy enforcement and multi-hop routing in QoS aware LoRa IoT networks},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a fast and stable filter for RSSI-based handoff
algorithms in dense indoor WLANs. <em>COMCOM</em>, <em>183</em>, 19–32.
(<a href="https://doi.org/10.1016/j.comcom.2021.10.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dense indoor wireless networks, handoffs occur frequently. The criteria to trigger handoffs are not defined by the IEEE 802.11 standard, thus being specific to each manufacturer’s implementation. Current handoff implementations typically use RSSI (Received Signal Strength Indicator) as a performance metric and commonly cause association instability in dense environments , a well-known problem referred to as the ping-pong effect . In this paper, we present a deep analysis of RSSI traces collected in dense indoor environments using the FIBRE testbed . Based on that, we conclude that the RSSI time series presents deep fast fades that occur frequently in bursts of small sizes, which can cause ping-pongs. Motivated by this behavior, we propose a new and simple filtering mechanism called Maximum which targets to eliminate these valleys in the RSSI time series. In a nutshell, this filter chooses the maximum RSSI value from a sliding window containing the last few RSSI samples of the series. We conduct simulations based on real RSSI traces from static and mobile scenarios to evaluate Maximum with respect to other filtering mechanisms found in the literature. Additionally, we present a simplified model of the behavior of Maximum that allows us to study the probability of unwanted handoffs as a function of the RSSI of the available access points. Our analysis reveals that Maximum is able to offer a better tradeoff between handoff triggering delay and stability in mobile scenarios, while also performing well in static scenarios, effectively avoiding the occurrence of ping-pongs in most cases.},
  archive      = {J_COMCOM},
  author       = {Helga D. Balbi and Diego Passos and Juan Vieira and Ricardo C. Carrano and Luiz C.S. Magalhães and Célio Albuquerque},
  doi          = {10.1016/j.comcom.2021.10.024},
  journal      = {Computer Communications},
  pages        = {19-32},
  shortjournal = {Comput. Commun.},
  title        = {Towards a fast and stable filter for RSSI-based handoff algorithms in dense indoor WLANs},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAST: Rapid and energy-efficient network formation in
TSCH-based industrial internet of things. <em>COMCOM</em>, <em>183</em>,
1–18. (<a href="https://doi.org/10.1016/j.comcom.2021.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Time Slotted Channel Hopping (TSCH) mode of the IEEE 802.15.4 standard is expected to revolutionize the Industrial Internet of Things . Indeed, it can achieve high reliability and deterministic latency with a very low duty cycle. Nevertheless, forming a TSCH network with the standard approach might not be as efficient, constituting, thus, one of the TSCH’s major issues. Such a network formation process relies on nodes passively scanning for advertised Enhanced Beacon (EB) frames to join the network. Doing so, a node wishing to join a TSCH network may stay awake randomly scanning for EBs for a considerable period of time, leading to a lengthy formation process with excessive energy consumption. To deal with these issues, this paper presents a practical and effective Radio duty-cycled, Active-Scan based network formation process for TSCH networks (RAST). Our proposal leans on active-scan procedures combined with radio duty cycling mechanisms to shorten joining delays and reduce energy consumption. Obtained results from extensive and realistic simulations show that our solution is efficient and outperforms state-of-the-art solutions, regarding the association time and energy consumption by up to two orders of magnitude.},
  archive      = {J_COMCOM},
  author       = {Mohamed Mohamadi and Badis Djamaa and Mustapha Reda Senouci},
  doi          = {10.1016/j.comcom.2021.11.015},
  journal      = {Computer Communications},
  pages        = {1-18},
  shortjournal = {Comput. Commun.},
  title        = {RAST: Rapid and energy-efficient network formation in TSCH-based industrial internet of things},
  volume       = {183},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-phase sample average approximation for video
distribution strategy of edge computing in heterogeneous network.
<em>COMCOM</em>, <em>182</em>, 255–267. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative edge computing provides a good platform for edge storage and computing at the same time. When a single edge server cannot efficiently provide video services, multiple edge servers are needed to share resources for collaborative storage and computing. Existing collaboration schemes often ignore the coupling between video caching and distribution, resulting in inefficient caching strategies. Therefore, for the scenario where multiple edge servers provide video services cooperatively, the content access delay in the video service is minimized in this paper based on the video caching strategy on the slow time scale and the video distribution strategy on the fast time scale. The problem is modeled as a random integer linear programming problem on dual time scales. Moreover, an algorithm is proposed based on the sample average approximation technique. The algorithm first designs the video caching strategy of the edge server based on the statistical information of the arrival of video requests on the slow time scale and the expected video distribution strategy. Then, the video distribution strategy is optimized based on the designed caching strategy and the video request situation on fast time scales. Finally, through the cooperation of the two time scales, the content access delay is minimized, and the simulation results verify the advantages of the proposed scheme in reducing content access delay and improving storage hit rate. The storage hit rate is increased by 7.6\%; the total content access delay is increased by 17.42\%; the number of edge servers and the arrival rate of video requests are ahead of other methods by 5\%; and the transcoding time is shortened by 4.56 s. It fully verifies that the design of a more efficient multi-user computing offload strategy in this paper can solve the problems of computing power, bandwidth, delay Energy consumption and other bottlenecks are of practical significance.},
  archive      = {J_COMCOM},
  author       = {Jingfu Lu and Jiuling Li},
  doi          = {10.1016/j.comcom.2021.11.007},
  journal      = {Computer Communications},
  pages        = {255-267},
  shortjournal = {Comput. Commun.},
  title        = {Two-phase sample average approximation for video distribution strategy of edge computing in heterogeneous network},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Protocol reverse-engineering methods and tools: A survey.
<em>COMCOM</em>, <em>182</em>, 238–254. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread utilization of network protocols raises many security and privacy concerns. To address them, protocol reverse-engineering (PRE) has been broadly applied in diverse domains, such as network management, security validation, and software analysis, by mining protocol specifications. This paper surveys the existing PRE methods and tools, which are based on network trace (NetT) or execution trace (ExeT), according to features representation. The feature-based protocol classification is proposed for the first time in literature to describe and compare different tools more clearly from a new perspective and to inspire crossover approaches in future works. We analyze the rationale, genealogy, contributions, and properties of 74 representative PRE methods/tools developed since 2004. In addition, we extend the general process of the PRE from a feature perspective and provide a detailed evaluation of the well-known methods/tools. Finally, we highlight the open issues and future research directions.},
  archive      = {J_COMCOM},
  author       = {Yuyao Huang and Hui Shu and Fei Kang and Yan Guang},
  doi          = {10.1016/j.comcom.2021.11.009},
  journal      = {Computer Communications},
  pages        = {238-254},
  shortjournal = {Comput. Commun.},
  title        = {Protocol reverse-engineering methods and tools: A survey},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Green traffic backhauling in next generation wireless
communication networks incorporating FSO/mmWave technologies.
<em>COMCOM</em>, <em>182</em>, 223–237. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-space optics (FSO) and Millimeter wave (mmWave) technologies are envisioned to be a major part of next generation wireless backhauling solutions. The FSO technology offers a fiber-like high data rate and low deployment cost, but suffers from reliability and availability problems caused by adverse weather conditions. On the other hand, the mmWave technology offers a more reliable backhauling solution, but is limited in communication range and throughput compared to the FSO technology . While both of these technologies have been investigated in recent times, none of the existing research explored the energy efficiency aspect of the hybrid FSO/mmWave backhauling solution. This research investigates the green backhauling challenge of the hybrid FSO/mmWave backhauling system. We derive two analytical models, one for FSO links and the other for mmWave links, to measure the backhaul power consumption in 5G systems. We then formulate the green backhauling challenge as a mixed integer linear programming , which considers network traffic variation and weather condition (e.g., fog, rain, and wind) and finds the optimum backhauling path. Simulation results confirm that the proposed green backhauling solution consumes 27\% less energy compared to existing solutions.},
  archive      = {J_COMCOM},
  author       = {Md Munjure Mowla and Iftekhar Ahmad and Daryoush Habibi and Quoc Viet Phung and M. Ishtiaque Aziz Zahed},
  doi          = {10.1016/j.comcom.2021.11.006},
  journal      = {Computer Communications},
  pages        = {223-237},
  shortjournal = {Comput. Commun.},
  title        = {Green traffic backhauling in next generation wireless communication networks incorporating FSO/mmWave technologies},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ZM-CTC: Covert timing channel construction method based on
zigzag matrix. <em>COMCOM</em>, <em>182</em>, 212–222. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VoLTE (Voice over Long-Term Evolution) is a widely used, high-performance, and end-to-end transmission solution in 4G and 5G . However, it also provides a new foundation for the CTC (covert timing channel), which transmits data in a hidden way. Because the packet type, transmission sequence, and transmission jitter in VoLTE have the strong regularity, it is challenging to build CTC. In this paper, a CTC construction method based on zigzag matrix named ZM-CTC is proposed to establish a connection between the codewords and dropped packets in a nonlinear rule. Moreover, a random salt and a random field in VoLTE are introduced to enhance the undetectability of ZM-CTC. The experimental results show that the ZM-CTC with 9 bits codeword could keep a balance among the undetectability, the robustness, the throughput, and the construction cost. The throughput of CTC is 0.88 bps, and the bit error rate of CTC is less than 1.5\%.},
  archive      = {J_COMCOM},
  author       = {Jiamin Zheng and Shupan Li and Shengang Hao and Yuanzhang Li and Yu Zhang},
  doi          = {10.1016/j.comcom.2021.10.040},
  journal      = {Computer Communications},
  pages        = {212-222},
  shortjournal = {Comput. Commun.},
  title        = {ZM-CTC: Covert timing channel construction method based on zigzag matrix},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mitigating TCP SYN flooding based EDOS attack in cloud
computing environment using binomial distribution in SDN.
<em>COMCOM</em>, <em>182</em>, 198–211. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing provides an auto-scaling feature for dynamic resource utilization to cope with their customers’ requirements and charge as ‘pay-per-use’. Attackers get the benefit of this auto-scaling feature by flooding DDoS attacks on the VMs or instances in the cloud environments. Such DDoS attacks give financial damages to the customers because of massive amount of resource utilization, known as the Economic Denial of Sustainability (EDOS) attack. Transmission Control Protocol (TCP) SYN flooding attack is known to be the most challenging attack resulting EDOS attack. Software Defined Network (SDN) being a cost-efficient solution for cloud service providers , uses the OpenFlow switches and flow tables to make rules for each incoming flow. In SDN, it is feasible to classify an incoming flow as an attack and block forwarding of this flow to the targeted VM. This research work proposes an SDN based fast and computationally cost-efficient statistical anomaly detection model, named EDOS-TCP SYN mitigation model (EDOS-TSM) to mitigate TCP SYN flooding attack from a single user and spoofed IPs. EDOS-TSM uses binomial probability, TTL field value of IP packet header and multi-TCP SYN requests to detect source-based and spoofing based attacks. The proposed model is implemented, and the performance is evaluated on an OpenStack production-based cloud with real-time traffic and attack generation . The attack mitigation results are compared with existing models in the literature. The results show a large number of false negatives in existing models and efficiency of EDOS-TSM is proved in both source based and spoofing attacks .},
  archive      = {J_COMCOM},
  author       = {Sayed Qaiser Ali Shah and Farrukh Zeeshan Khan and Muneer Ahmad PhD ( Associate Professor )},
  doi          = {10.1016/j.comcom.2021.11.008},
  journal      = {Computer Communications},
  pages        = {198-211},
  shortjournal = {Comput. Commun.},
  title        = {Mitigating TCP SYN flooding based EDOS attack in cloud computing environment using binomial distribution in SDN},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsatisfied today, satisfied tomorrow: A simulation
framework for performance evaluation of crowdsourcing-based network
monitoring. <em>COMCOM</em>, <em>182</em>, 184–197. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network operators need to continuously upgrade their infrastructures in order to keep their customer satisfaction levels high. Crowdsourcing-based approaches are generally adopted, where customers are directly asked to answer surveys about their experience. Since the number of collaborative users is generally low, network operators rely on Machine Learning models to predict the satisfaction levels/QoE of the users rather than directly measuring it through surveys. Finally, combining the true/predicted users satisfaction labels with information on each user mobility (e.g, which network sites each user has visited and for how long), an operator may reveal critical areas in the network and drive/prioritize investments properly. In this work, we propose an empirical framework tailored to assess the quality of the detection of under-performing cells starting from subjective user experience grades. The framework allows to simulate diverse networking scenarios, where a network characterized by a small set of under-performing cells is visited by heterogeneous users moving through it according to realistic mobility models . The framework simulates both the processes of satisfaction surveys delivery and users satisfaction prediction, considering different delivery strategies and evaluating prediction algorithms characterized by different prediction performance. We use the simulation framework to test empirically the performance of under-performing sites detection in general scenarios characterized by different users density and mobility models to obtain insights which are generalizable and that provide interesting guidelines for network operators.},
  archive      = {J_COMCOM},
  author       = {Andrea Pimpinella and Marianna Repossi and Alessandro E.C. Redondi},
  doi          = {10.1016/j.comcom.2021.11.004},
  journal      = {Computer Communications},
  pages        = {184-197},
  shortjournal = {Comput. Commun.},
  title        = {Unsatisfied today, satisfied tomorrow: A simulation framework for performance evaluation of crowdsourcing-based network monitoring},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAAS parallel task scheduling based on cloud service flow
load algorithm. <em>COMCOM</em>, <em>182</em>, 170–183. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud platform applications, the user’s goal is to obtain high-quality application services, while the service provider’s goal is to obtain revenue by performing the tasks submitted by the user. The platform built by the service provider’s application resources needs to improve the mapping between service requests and resources to achieve higher value. Through the current situation of resource management in the cloud environment, it is found that many task scheduling and resource allocation algorithms are still affected by factors such as the diversity, dynamics, and multiple constraints of resources and tasks. This paper focuses on Software as a Service (SaaS) applications’ task scheduling and resource configuration in a dynamic and uncertain cloud environment. It is a challenging online scheduling problem to automatically and intelligently allocate user task requests that continually reach SaaS applications to appropriate resources for execution. To this end, a real-time task scheduling method based on deep reinforcement learning is proposed, which automatically and intelligently allocates user task requests that continually reach SaaS applications to appropriate resources for execution. In this way, the limited virtual machine resources rented by SaaS providers can be used in a balanced and efficient manner. In the experiment, by comparing with other five task scheduling algorithms, it is proved that the algorithm proposed in this paper not only improves the execution efficiency of better deploying workflow in IaaS public cloud, but also makes the resources provided by SaaS are used in a balanced and efficient manner.},
  archive      = {J_COMCOM},
  author       = {Jian Zhu and Qian Li and Shi Ying},
  doi          = {10.1016/j.comcom.2021.10.037},
  journal      = {Computer Communications},
  pages        = {170-183},
  shortjournal = {Comput. Commun.},
  title        = {SAAS parallel task scheduling based on cloud service flow load algorithm},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-the-fly (d)DoS attack mitigation in SDN using deep neural
network-based rate limiting. <em>COMCOM</em>, <em>182</em>, 153–169. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defined Networking (SDN) has emerged as a promising paradigm offering an unprecedented programmability , scalability and fine-grained control over forwarding elements (FE). Mainly, SDN decouples the forwarding plane from the control plane which is moved to a central controller that is in charge of taking routing decisions in the network. However, SDN is rife with vulnerabilities so that several network attacks, especially Distributed Denial of Service (DDoS), can be launched from compromised hosts connected to switches. DDoS attacks can easily overload the controller processing capacity and flood switch flow-tables. This paper deals with the security issue in SDN. It proposes a real-time protection against DDoS attacks that is based on a controller-side sliding window rate limiting approach which relies on a weighted abstraction of the underlying network. A weight defines the allowable amount of data that can be transmitted by a node and is dynamically updated according to its contribution to: (1) the queueing capacity of the controller, and (2) the number of flow-rules in the switch. Hence, a new deep learning algorithm , denoted the Parallel Online Deep Learning algorithm (PODL), is defined in order to update weights on the-fly according to both aforementioned constraints simultaneously. Furthermore, the behavior of each host and each switch is evaluated through a measure of trustworthiness which is used to penalize mis-behaving ones by prohibiting new flow requests or PacketIn messages for a period of time. Host trustworthiness is based on their weights while switch trustworthiness is achieved through a computation of the Average Nearest-Neighbor Degree (ANND). Realistic experiments show that the proposed solution succeeds in minimizing the impact of DDoS attacks on both the controllers and the switches regarding the PacketIn arrival rate at the controller, the rate of accepted requests and the flow-table usage.},
  archive      = {J_COMCOM},
  author       = {Ali El Kamel and Hamdi Eltaief and Habib Youssef},
  doi          = {10.1016/j.comcom.2021.11.003},
  journal      = {Computer Communications},
  pages        = {153-169},
  shortjournal = {Comput. Commun.},
  title        = {On-the-fly (D)DoS attack mitigation in SDN using deep neural network-based rate limiting},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain technology as a fog computing security and
privacy solution: An overview. <em>COMCOM</em>, <em>182</em>, 129–152.
(<a href="https://doi.org/10.1016/j.comcom.2021.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergent of Fog computing as an extension of Cloud computing , from the center of the internet architecture to the IoT end user’s devices, aims to enhance the processing power of the resource-constraint IoT devices and deliver them other services since it locates close to these devices. This extension was also suggested to boost the standard of IoT system implementations thus decreasing energy consumption and latency for those applications that need fast responses. However, as stated in recent literature, Fog computing may have some important security and privacy challenges. On the other hand, Blockchain , which was generated and used in crypto-currencies, has been applied in a wider range of applications due to the security, privacy, distributed trust management, and reliability features provided. Among the applications, which have recently been attractive about blockchain is Fog computing. Blockchain in Fog computing may achieve a distributed and trusted, identity management, secure data, reputation, and payment systems. This survey discusses the state-of-the-art impact of the blockchain on the security and privacy of Fog computing. The findings elucidate the vision of blockchain in Fog computing-security and privacy-based enhancement and draw attention to open challenges and future research directions.},
  archive      = {J_COMCOM},
  author       = {Yehia Ibrahim Alzoubi and Ahmad Al-Ahmad and Hasan Kahtan},
  doi          = {10.1016/j.comcom.2021.11.005},
  journal      = {Computer Communications},
  pages        = {129-152},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain technology as a fog computing security and privacy solution: An overview},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A differential privacy-based classification system for edge
computing in IoT. <em>COMCOM</em>, <em>182</em>, 117–128. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blooming Internet of Things (IoTs) devices have brought many new types of sensing applications and methods to the traditional cloud-enabled IoTs framework. Hence, the new network framework becomes bidirectional gradually, in which IoTs devices can also perform moderate computation tasks instead of being solely used as data harvesters . This new coming framework known as edge computing significantly improves the traditional cloud-enabled network latency and dependency by shifting part of computation back to “local”. However, new security risks emerge when the edge computing shifts data and models back to the IoTs devices. Acies, a differential privacy based privacy-preserving classification system for edge computing is proposed to secure the classification models offloaded to edge devices. Acies supports popular classifiers such as Nearest Neighborhood, Support Vector Machine and Sparse Representation Classifier with a variety of feature selection methods. According to our evaluation on different datasets, classification models with Acies can be private and maintain high utility. Acies achieves reliable privacy protection under reconstruction attacks with minimal impact on classification accuracy (2\% ∽ ∽ 5\%) only. Acies outperforms the naive input dataset perturbation methods by up to 30\% higher classification accuracy when the privacy requirements of the applications is high (the privacy budget ε ε is less than 2).},
  archive      = {J_COMCOM},
  author       = {Wanli Xue and Yiran Shen and Chengwen Luo and Weitao Xu and Wen Hu and Aruna Seneviratne},
  doi          = {10.1016/j.comcom.2021.10.038},
  journal      = {Computer Communications},
  pages        = {117-128},
  shortjournal = {Comput. Commun.},
  title        = {A differential privacy-based classification system for edge computing in IoT},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge computing-empowered task offloading in PLC-wireless
integrated network based on matching with quota. <em>COMCOM</em>,
<em>182</em>, 110–116. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of the fifth-generation (5G) wireless communication with power line communication (PLC) provides an efficient solution to accommodate the connection demands of massive industrial internet of things (IIoT) devices. Moreover, edge computing can provide sufficient computing services for IIoT devices. By empowering PLC-wireless integrated networks with edge computing, devices can offload task data to adjacent edge servers through either PLC or 5G subchannels by leveraging orthogonal frequency division multiplexing (OFDM). However, task offloading in the edge computing-empowered PLC-wireless integrated network faces several challenges such as channel access conflict, dynamic network state, as well as long-term queue stability assurance. In this article, we construct a one-to-many task offloading architecture to maximize the total throughput. Lyapunov optimization is adopted to convert the NP-hard task offloading problem. Then, a pricing-based matching with quota task offloading algorithm is proposed to handle channel access conflicts among IIoT devices. Simulation evaluation demonstrates that the proposed algorithm realizes excellent performances in throughput, energy consumption, and queuing backlog.},
  archive      = {J_COMCOM},
  author       = {Zhan Shi and Huina Wei and Jian Zhu},
  doi          = {10.1016/j.comcom.2021.10.032},
  journal      = {Computer Communications},
  pages        = {110-116},
  shortjournal = {Comput. Commun.},
  title        = {Edge computing-empowered task offloading in PLC-wireless integrated network based on matching with quota},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bounded delay scheduling with packet dependencies.
<em>COMCOM</em>, <em>182</em>, 98–109. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common situation occurring when dealing with multimedia traffic is having large data frames fragmented into smaller IP packets, and having these packets sent independently through the network. For real-time multimedia traffic , dropping even few packets of a frame may render the entire frame useless. Such traffic is usually modeled as having inter-packet dependencies . We study the problem of scheduling traffic with such dependencies, where each packet has a deadline by which it should arrive at its destination. Such deadlines are common for real-time multimedia applications , and are derived from stringent delay constraints posed by the application. The figure of merit in such environments is maximizing the system’s goodput , namely, the number of frames successfully delivered. We study online algorithms for the problem of maximizing goodput of delay-bounded traffic with inter-packet dependencies, and use competitive analysis to evaluate their performance. We present competitive algorithms for the problem, as well as matching lower bounds that are tight up to a constant factor. We further present the results of a simulation study which further validates our algorithmic approach and shows that insights arising from our analysis are indeed manifested in practice.},
  archive      = {J_COMCOM},
  author       = {Michael Markovitch and Gabriel Scalosub},
  doi          = {10.1016/j.comcom.2021.10.008},
  journal      = {Computer Communications},
  pages        = {98-109},
  shortjournal = {Comput. Commun.},
  title        = {Bounded delay scheduling with packet dependencies},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel proactive controller deployment protocol for
5G-enabled software-defined vehicular networks. <em>COMCOM</em>,
<em>182</em>, 88–97. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the vast expansion of data loads across numerous safety and infotainment applications, the software-defined paradigm has become a critical component for 5G-enabled and intelligent vehicular networks . By transforming the network’s rigid hardware devices into programmable units, SDVN offers a broad view of the entire network status and a standard interface between heterogeneous wireless access technologies. However, deploying a centralized control unit carries several problems, such as bottleneck issues and densification issues. A distributed control plane is a potential alternative to a centralized control plane. Still, it poses many questions about the optimal location of local control units and the number of control units required for a given network structure. In this article, we propose an efficient proactive controller deployment and assignment strategy for distributed 5G-enabled software-defined Vehicular Networks based on predictive vehicles’ traffic flow information and communication delays between switch-enabled roadside units. A thorough performance evaluation is conducted under different vehicular network’s mobility scenarios. We assess the performance of the proposed method in terms of communication latency and load on the subsequent control units and their assigned set of switches. The proposed proactive clustering method is compared to different forms of controller placement methods. The results showed that the proposed approach reduced the network’s communication delays while maintaining a balanced distributed load among controllers over the simulation time.},
  archive      = {J_COMCOM},
  author       = {Noura Aljeri and Azzedine Boukerche},
  doi          = {10.1016/j.comcom.2021.09.024},
  journal      = {Computer Communications},
  pages        = {88-97},
  shortjournal = {Comput. Commun.},
  title        = {A novel proactive controller deployment protocol for 5G-enabled software-defined vehicular networks},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Socio-economic factor analysis for sustainable and smart
precision agriculture: An ensemble learning approach. <em>COMCOM</em>,
<em>182</em>, 72–87. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The socio-economic factor analyses by using the Logit and Probit model have the limitation of representing random taste variation for the unobserved factors and the issues of temporally correlated errors respectively. In socio-economic factor analysis, the observed data are essential in the random distribution for the adequate representation of the random components associated with various factors and lead to poor prediction in the case of the Logit and Probit model. In this work, Extra-trees classifier machine learning model based socio-economic factors selection has been used and found capable to find out the socio-economic factors that contain relevant information to the target variable agricultural productivity. In addition to this, the voting classifiers ensemble learning approach is used for the prediction of agricultural productivity of respondents (farmers) from their socio-economic profiles. This proposed work has been evaluated by using the test case of analyzing the socio-economic factors of the farmers affecting agricultural productivity in Sambalpur District, in Odisha State, India. In this study, the farmers’ socio-economic data are collected by using structured interviews through questionnaires that are in line with standard Participatory Rural Appraisal. It is found that, the proposed approach of socio-economic factor identification is found efficient for finding the relationships between socio-economic factors and agricultural productivity, and the proposed ensemble learning-based approach is efficient in terms of agricultural productivity prediction.},
  archive      = {J_COMCOM},
  author       = {Pandit Byomakesha Dash and Bighnaraj Naik and Janmenjoy Nayak and S. Vimal},
  doi          = {10.1016/j.comcom.2021.11.002},
  journal      = {Computer Communications},
  pages        = {72-87},
  shortjournal = {Comput. Commun.},
  title        = {Socio-economic factor analysis for sustainable and smart precision agriculture: An ensemble learning approach},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An improving approach to analyzing change impact of c
programs. <em>COMCOM</em>, <em>182</em>, 60–71. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A seemingly small change in the software can usually have a big impact. Impact analysis is an assessment of multiple risks associated with the changes. Research over the past 20 years has shown that impact analysis can decrease the time and effort needed in the development and regression testing and reduce the negative effects of change on other parts of the software system. These researches used program slicing, control flow analysis , call graph to analyze intraprocedural and interprocedural impact. However, most of them only analyze the methods affected by changes of simple variables or statements, which have coarse granularity and can provide limited help. To solve these problems, we propose an improving approach that can analyze the finer granularity change impact of multiple types of variables. This approach first breaks code changes into a combination of basic operations on variables and adopts corresponding analysis mechanisms for different types of variables. The code is preprocessed before analysis to gather essential information such as global variables table, function summaries, and pointer information map. In the intraprocedural analysis, we first divide the variables into two types: definition point and use point. For the use point, a data flow graph is used to analyze its change impact. For the definition point, we first find the paths where it is and analyze the variables on these paths that are controlled by it. In addition, to deal with the change impact of complex variables in the C programs, we have proposed a corresponding analysis procedure and algorithm. In interprocedural analysis , the impacted methods are searched according to their relationship and the global variable information table. A prototype system is developed based on our approach. In our evaluation, we demonstrate that the prototype system can be used to support various software development and maintenance tasks.},
  archive      = {J_COMCOM},
  author       = {Peng Dai and Yawen Wang and Dahai Jin and Yunzhan Gong and Wenjin Yang},
  doi          = {10.1016/j.comcom.2021.10.039},
  journal      = {Computer Communications},
  pages        = {60-71},
  shortjournal = {Comput. Commun.},
  title        = {An improving approach to analyzing change impact of c programs},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSTM-MFCN: A time series classifier based on multi-scale
spatial–temporal features. <em>COMCOM</em>, <em>182</em>, 52–59. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN , shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.},
  archive      = {J_COMCOM},
  author       = {Liang Zhao and Chunyang Mo and Jiajun Ma and Zhikui Chen and Chenhui Yao},
  doi          = {10.1016/j.comcom.2021.10.036},
  journal      = {Computer Communications},
  pages        = {52-59},
  shortjournal = {Comput. Commun.},
  title        = {LSTM-MFCN: A time series classifier based on multi-scale spatial–temporal features},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast controlling of rumors with limited cost in social
networks. <em>COMCOM</em>, <em>182</em>, 41–51. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innumerable rumors spread quickly through social networks and how to fast control the spread of rumors is crucial. An efficient way is to take different measures for users with different influence extent by rumors, but the costs of these measures vary. In this paper, we try to minimize the spread (termination) time of rumors considering the controlling cost. To solve this problem, we creatively design five different measures to control rumors. First, we propose a contact coefficient to quantify the influence weight for each user. Second, we classify the users to different groups based on their influence weights so that the rumors can be controlled by the measures accordingly. Thus, the controlling of rumors can be formulated into an optimization problem , in which decision variables based on contact coefficients are used to classify users. Then an approximation algorithm named WB-GA is designed to classify different users, ensuring that rumors can be controlled as fast as possible within given costs. The experimental results on real online networks show that our algorithm is highly efficient and effective.},
  archive      = {J_COMCOM},
  author       = {Xiaopeng Yao and Yue Gu and Chonglin Gu and Hejiao Huang},
  doi          = {10.1016/j.comcom.2021.10.041},
  journal      = {Computer Communications},
  pages        = {41-51},
  shortjournal = {Comput. Commun.},
  title        = {Fast controlling of rumors with limited cost in social networks},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and reliable hybrid deep learning-enabled model
for congestion control in 5G/6G networks. <em>COMCOM</em>, <em>182</em>,
31–40. (<a href="https://doi.org/10.1016/j.comcom.2021.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future generation networks such as millimeter-wave LAN, broadband wireless access systems, and 5th or 6th generation (5G/6G) networks demand more security, low latency with more reliable standards and communication capacity. Efficient congestion control is considered one of the key elements of 5G/6G technology that allows the operators to run various network instances using a single infrastructure for a better quality of services. Artificial intelligence (AI) and machine learning (ML) are playing an essential role in reconfiguring and optimizing the performance of a 5G/6G wireless network due to a vast amount of data. A smart decision-making mechanism is required for the incoming network traffic to ensure load balancing, restrict network slice failure and provide alternate slices in case of slice failure or overloading. To circumvent these issues, a hybrid deep learning-enabled efficient congestion control mechanism is proposed in this paper. This hybrid deep learning model consists of long short term memory (LSTM) and support vector machine (SVM). The applicability of the proposed model is validated by simulating for one week using multiple unknown devices, slice failure conditions, and overloading conditions. An overall accuracy rate of 93.23\% is calculated for the proposed hybrid model that reflects the applicability. Apart from this, other performance metrics such as specificity, recall, time consumption, varying training, test sets, true-false rates, and f-score were used for the performance evaluation purposes of the proposed model.},
  archive      = {J_COMCOM},
  author       = {Sulaiman Khan and Anwar Hussain and Shah Nazir and Fazlullah Khan and Ammar Oad and Mohammad Dahman Alshehri},
  doi          = {10.1016/j.comcom.2021.11.001},
  journal      = {Computer Communications},
  pages        = {31-40},
  shortjournal = {Comput. Commun.},
  title        = {Efficient and reliable hybrid deep learning-enabled model for congestion control in 5G/6G networks},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Providing impersonation resistance for biometric-based
authentication scheme in mobile cloud computing service.
<em>COMCOM</em>, <em>182</em>, 22–30. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cloud computing (MCC) is an emerging computing model with the growing demands of mobile devices by services diversity using mobile cloud technology. Authentication plays a fundamental role between the smart portable devices located in different places and the mobile cloud computing service providers in MCC. Several authors presented many authentication schemes using different cryptographic approaches for MCC in the past years. However, those schemes have not solved the impersonation attack well. To conquer this suffering and support more security functionalities, we propose an impersonation resistance for a biometric-based authentication scheme using hashing and symmetric parameter function in an MCC environment. The proposed scheme can provide mutual authentication and anonymity using the software AVISPA. The security of the proposed scheme is formally proved in the random oracle model . Regarding functionality and performance analysis, the comparative studies demonstrate that the design has a trade-off between resource consumption and security strengthens for MCC services. Therefore, the proposed scheme is more practical in MCC application.},
  archive      = {J_COMCOM},
  author       = {Yanrong Lu and Dawei Zhao},
  doi          = {10.1016/j.comcom.2021.10.029},
  journal      = {Computer Communications},
  pages        = {22-30},
  shortjournal = {Comput. Commun.},
  title        = {Providing impersonation resistance for biometric-based authentication scheme in mobile cloud computing service},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy consumption model for data transfer in smartphone.
<em>COMCOM</em>, <em>182</em>, 13–21. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smartphones manufactured at present are equipped with the new Wireless Local Area Network (WLAN) calibrated to IEEE standards on its interface, which supports the Multiple Input Multiple Output (MIMO) feature. This technological advancement has enhanced smartphones to satisfy the present IEEE standard requirements for WLANs. However, the merits of smartphones are restricted by battery lifecycle. In this paper, the energy consumption of a smartphone equipped with the MIMO system throughout the transmission and reception of data is studied. The impact of the various factors of recently developed WLANs (such as 802.11n, which includes modulation and coding scheme and MIMO) are considered in conditions of throughput and power/energy consumption by the use of well-known simulator, which is called Network Simulator 3 (NS-3). In addition, the energy consumed in the course of transmitting and receiving data is differentiated through per-bit energy consumption with various MIMO compositions and physical data rates. The consequences reveal that growing the system configuration farther 2 × 2 MIMO enhances the throughput and reduces the per-bit energy consumption. Furthermore, the capability to predict the energy consumed for data transmission is considered essential for energy-aware applications. For example, task offloading from modern mobile appliances to cloud computing is a highly potential approach for saving energy in mobile devices . The decision to offload is essential to make offloading more effective. Energy models are required to precisely predict the energy consumption of networking and computing processes. Thus, this precision enables the offloading technology to precisely assess whether offloading a specified task will save energy. For this purpose, an energy consumption model for transmitting and receiving is developed based on a MIMO parameter with high accuracy. The simulation demonstrates that our energy models are practical and effective in real-world scenarios. In addition, these models estimate the energy consumption per bit, and offer system designers and application developers effective tools for designing energy-efficient systems.},
  archive      = {J_COMCOM},
  author       = {Jameel Ali and Majid Altamimi},
  doi          = {10.1016/j.comcom.2021.10.014},
  journal      = {Computer Communications},
  pages        = {13-21},
  shortjournal = {Comput. Commun.},
  title        = {Energy consumption model for data transfer in smartphone},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient failure recovery techniques for segment-routed
networks. <em>COMCOM</em>, <em>182</em>, 1–12. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication networks, protecting against node and link failures is an important requirement. In data networks such as IP/MPLS networks robust protection techniques have evolved, while also depending on the underlying transport networks to provide reliability. In networks such as data center networks, it is not possible to rely on the underlying layer for handling of failures. It is necessary to implement protection mechanisms with fast recovery times. Fast reroute is one such mechanism where, upon a failure, repair is initiated at the point of local repair (PLR). However, this is not efficient in terms of the operational cost of the repair. With segment-routed networks, an opportunity arises to initiate the repair at the segment endpoints and not the PLR. In this paper, we present a segment-level recovery framework, where protection is applied for each segment of an end-to-end path rather than individual links or nodes. The proposed technique is more efficient than two other existing techniques that use segment routing for recovery, namely Topology Independent Loop Free Alternate (TI-LFA) and Topology Independent Multi-Failure Alternate (TI-MFA). Simulation-based results show that the proposed scheme provides efficient failure recovery in terms of flow drop rates, ability to recover from multiple failures and resources required for the recovery. Based on these results we find that the drop rate of flows due to failure can be up to 62\% lower and the increase in operational path length can be up to 35\% lower resulting in more robust protection and significantly better resource utilization.},
  archive      = {J_COMCOM},
  author       = {Anix Anbiah and Krishna M. Sivalingam},
  doi          = {10.1016/j.comcom.2021.10.033},
  journal      = {Computer Communications},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {Efficient failure recovery techniques for segment-routed networks},
  volume       = {182},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on security and privacy in internet of medical
things. <em>COMCOM</em>, <em>181</em>, 474–476. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Dr. Varun G. Menon ( Editors ) and Dr. Ali Kashif Bashir and Dr. Shahid Mumtaz and Dr. Syed Hassan Ahmed and Dr. Danda B. Rawat},
  doi          = {10.1016/j.comcom.2021.11.021},
  journal      = {Computer Communications},
  pages        = {474-476},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on security and privacy in internet of medical things},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on optimization of cross-layer collaborative
resource allocation for mobile edge computing, caching and
communication. <em>COMCOM</em>, <em>181</em>, 472–473. (<a
href="https://doi.org/10.1016/j.comcom.2021.11.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Shaohua Wan and Remigiusz Wiśniewski and George Alexandropoulos and Zonghua Gu and Pierluigi Siano},
  doi          = {10.1016/j.comcom.2021.11.020},
  journal      = {Computer Communications},
  pages        = {472-473},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on optimization of cross-layer collaborative resource allocation for mobile edge computing, caching and communication},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ADMS: An online attack detection and mitigation system for
LDoS attacks via SDN. <em>COMCOM</em>, <em>181</em>, 454–471. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rate Denial of Service (LDoS) attacks cause severe destructiveness to network security . Consequently, the implementation of detection and defense against them is a concern among the research communities. But it is formidable to deploy extension modules to detect and mitigate attacks online in traditional networks, because devices are deficient of flexibility and scalability. To address the problem, we design and implement an online attack detection and mitigation system (ADMS) framework via the scalable and programmable Software Defined Networking (SDN). ADMS is installed on SDN controllers and conforms to the OpenFlow policy without extra devices. ADMS consists of two modules: the two-phase detection module and the mitigation module. The two-phase detection module combines the new port traffic feature and the Lightgbm classifier based on flow table statistics traffic to precisely detect LDoS attacks. The mitigation module utilizes the novel Sequence Matching based Dynamic Series Analysing (SMDSA) algorithm to locate the attacker, and efficiently mitigates attack traffic by packet filter . The SMDSA algorithm distinguishes the victim port from benign ports by calculating the anomaly score of each port. Our evaluation on a prototype implementation of ADMS shows that the framework is able to precisely identify and efficiently mitigate LDoS attacks in real-time.},
  archive      = {J_COMCOM},
  author       = {Dan Tang and Xiyin Wang and Yudong Yan and Dongshuo Zhang and Huan Zhao},
  doi          = {10.1016/j.comcom.2021.10.007},
  journal      = {Computer Communications},
  pages        = {454-471},
  shortjournal = {Comput. Commun.},
  title        = {ADMS: An online attack detection and mitigation system for LDoS attacks via SDN},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-optimal trajectory planning of manipulator with
simultaneously searching the optimal path. <em>COMCOM</em>,
<em>181</em>, 446–453. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-optimal trajectory planning can improve the efficiency of the manipulator, which has a paramount research significance. Nonetheless, almost all the current algorithms have the problem of path limitation — the algorithm constantly searches for the optimal motion time of the manipulator on a specific motion path. To solve this problem, we design a time-optimal trajectory planning method for the manipulator by searching the optimal path simultaneously. We propose using a cubic uniform B-spline interpolation algorithm to derive the motion curve expression of the manipulator joint with unknown path points. Then we use a genetic algorithm to optimize the path point and the motion time of the manipulator at the same time to get the time-optimal motion path of the manipulator. Moreover, we prove that the algorithm proposed in this paper is comparable with similar algorithms with known paths through experiments.},
  archive      = {J_COMCOM},
  author       = {Xiuli Yu and Mingshuai Dong and Weimin Yin},
  doi          = {10.1016/j.comcom.2021.10.005},
  journal      = {Computer Communications},
  pages        = {446-453},
  shortjournal = {Comput. Commun.},
  title        = {Time-optimal trajectory planning of manipulator with simultaneously searching the optimal path},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using XorOffsetTrie for high-performance IPv6 lookup in the
backbone network. <em>COMCOM</em>, <em>181</em>, 438–445. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of IPv6 in scale and traffic requires the IPv6 lookup algorithm to perform fast longest prefix matching (LPM), especially in the backbone network . However, existing IPv6 lookup algorithms, e.g., SAIL, Poptrie, and Hi-BST have limited lookup speeds with too many memory accesses. To achieve the highest lookup speed with fewer memory accesses, we propose the XorOffsetTrie method, which is the combination of XorFilter and OffsetTrie. OffsetTrie reduces the memory accesses by combining the offset index and the next hop of rule into an integer. XorFilter further improves lookup speed by matching about 63\% rules with one memory access. If a packet matches a rule in XorFilter, we can return the next hop of this rule immediately. Otherwise, we continue to lookup in OffsetTrie. With high efficient data structure for lookup process, the lookup speed of XorOffsetTrie achieves 1.6x, 4.2x, 8.5x that of SAIL, Poptrie, and Hi-BST.},
  archive      = {J_COMCOM},
  author       = {Chunyang Zhang and Gaogang Xie},
  doi          = {10.1016/j.comcom.2021.10.027},
  journal      = {Computer Communications},
  pages        = {438-445},
  shortjournal = {Comput. Commun.},
  title        = {Using XorOffsetTrie for high-performance IPv6 lookup in the backbone network},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Link and stability-aware adaptive cooperative routing with
restricted packets transmission and void-avoidance for underwater
acoustic wireless sensor networks. <em>COMCOM</em>, <em>181</em>,
428–437. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In underwater acoustic wireless sensor networks (UAWSNs), the conventional routing protocols continuously transmit data packets containing the same information when a source node detects an event of interest. This approach, however, leads to high energy consumption, high interference and challenges the stability of the network due to heavy data load on the nodes close to water surface. To overcome these challenges, this article proposes two routing protocols for UAWSNs. They are, SRRPV: stability-aware routing with restricted packets transmission and void-avoidance and LS-ACRPV: link and stability-aware adaptive cooperative routing with restricted packets transmission and void-avoidance. In SRRPV, a source node detects the event of interest, generates and routes packets. Subsequent packets routing is performed only when a change in intensity of the event is detected. Due to restricted packets transmission, three-hop connected paths with suitable neighbors are chosen to overcome the existence of a void (non-availability of a suitable forwarder) and, therefore, control packets loss. Moreover, the defined packet holding time ensures that nodes with the highest available energy transfer the most packets by holding the packets for the shortest interval of time. This strategy, in combination with restricted packets transmission, achieves network stability. Since limited number of packets are routed, the LS-ACRPV protocol is designed that adds reliability to the SRRPV protocol by selecting a routing link with the highest probability of successfully delivering packets to the destination or using adaptive cooperative routing to combat severe link conditions. Unlike the conventional protocols, the proposed protocols do not require the geographical position coordinates of nodes, which is challenging in underwater environment as nodes change their positions with sea tides and currents. Extensive simulation results reveal promising performance of the proposed schemes in terms of energy expenditure, delay and network stability at the expense of low packets delivery due to restricted packets transmission.},
  archive      = {J_COMCOM},
  author       = {Anwar Khan and Muhammad Imran and Muhammad Shoaib and Atiq Ur Rahman and Najmus Sama},
  doi          = {10.1016/j.comcom.2021.10.012},
  journal      = {Computer Communications},
  pages        = {428-437},
  shortjournal = {Comput. Commun.},
  title        = {Link and stability-aware adaptive cooperative routing with restricted packets transmission and void-avoidance for underwater acoustic wireless sensor networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards AI-enabled traffic management in multipath TCP: A
survey. <em>COMCOM</em>, <em>181</em>, 412–427. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous applications on the web use transmission control protocol (TCP) as a transport protocol to ensure efficient and fair sharing of network resources among users. With the increased complexity in wired/wireless networks, many end-to-end congestion control (CC) algorithms have been proposed in the literature, offering solutions through their proposed TCP variants. In contrast to this, machine learning has attained great success in tackling end-to-end CC for future networks. This survey investigates the most recent research contributions on learning-based CC in general and deep reinforcement learning (DRL)-based CC in particular for traffic management in multi-path TCP (MPTCP). From the literature, it is observed that DRL is a pivotal domain for learning-based CC algorithms in highly dynamic wireless communication networks. We pinpoint key outcomes, corresponding challenges and unaddressed issues. Moreover, this survey delineates the limitations, research challenges, insights, and future opportunities to advance DRL-based traffic management in MPTCP.},
  archive      = {J_COMCOM},
  author       = {Sadia J. Siddiqi and Faisal Naeem and Saud Khan and Komal S. Khan and Muhammad Tariq},
  doi          = {10.1016/j.comcom.2021.09.030},
  journal      = {Computer Communications},
  pages        = {412-427},
  shortjournal = {Comput. Commun.},
  title        = {Towards AI-enabled traffic management in multipath TCP: A survey},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rolling bearing fault diagnosis based on wireless sensor
network data fusion. <em>COMCOM</em>, <em>181</em>, 404–411. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of intelligent manufacturing, mechanical equipment is developing in the direction of large-scale, integration, precision and intelligence. The coupling between different equipment in the operation of the system also makes the mechanical equipment increasingly become a whole, which puts forward higher requirements for the fault diagnosis of mechanical equipment, especially rolling bearings . With the rapid development of wireless network technology, rolling bearing fault diagnosis has become a reality. In this paper, the wireless sensor network will be innovatively used to collect the data of key parts of mechanical equipment, so as to improve the problem of insufficient and accurate collected data in the traditional convolution neural network fault detection algorithm , convert the corresponding time-domain signal into image signal, and combine the global average pool layer on the basis of the combination of nonlinear convolution layers , So as to optimize the corresponding wireless network structure, and finally realize the adaptive extraction and fault diagnosis of rolling bearing fault features. The experimental results show that the fault detection accuracy of the optimized convolution neural network algorithm is about 99.8\%, and has good robustness and practical value.},
  archive      = {J_COMCOM},
  author       = {Jie Hu and Sier Deng},
  doi          = {10.1016/j.comcom.2021.10.035},
  journal      = {Computer Communications},
  pages        = {404-411},
  shortjournal = {Comput. Commun.},
  title        = {Rolling bearing fault diagnosis based on wireless sensor network data fusion},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on improving the wireless communication with
adaptive antenna selection by intelligent method. <em>COMCOM</em>,
<em>181</em>, 374–403. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission applications in wireless networks have brought unprecedented demands. The demand for high-performance wireless transmission is increasing day by day. Antenna technology is an indispensable part of the development of wireless communication . One potential solution is to resort t intelligent learning techniques to help breakthroughs in the limited antenna technical field. It is based on an adaptive antenna using intelligent learning. It has laid the foundation for signal strength adjustment to enhance wireless transmission efficiency. This paper evaluates the most advanced literature and techniques. A comprehensive description from different perspectives covers several adaptive antenna structures , including diversity antennas , phased array antennas , and beamforming specific learning methods. After that, this paper divides it into different categories, from intelligent learning algorithms and feature data perspectives in a different light to analyze and discuss. This article expects to help readers understand the latest intelligent technology based on adaptive antennas. Further, it sheds novel light on future research directions to meet the development needs of adaptive antennas for future wireless networks.},
  archive      = {J_COMCOM},
  author       = {ChienHsiang Wu and Chin-Feng Lai},
  doi          = {10.1016/j.comcom.2021.10.034},
  journal      = {Computer Communications},
  pages        = {374-403},
  shortjournal = {Comput. Commun.},
  title        = {A survey on improving the wireless communication with adaptive antenna selection by intelligent method},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-algorithm UWB-based localization method for mixed
LOS/NLOS environments. <em>COMCOM</em>, <em>181</em>, 365–373. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) is considered the most promising radio technology for high-accuracy indoor localization because of its many desirable properties , including a sub-decimeter level ranging accuracy under line-of-sight (LOS) conditions, resilience to multipath fading , and low duty cycles. However, the accuracy of UWB localization deteriorates significantly in complex indoor environments due to the presence of non-light-of-sight (NLOS) propagation that may introduce a considerable positive bias in range measurements. In this paper, we present a localization method that improves the accuracy of UWB localization in mixed LOS/NLOS indoor environments by using multiple localization algorithms optimized for different localization scenarios distinguished by the number of LOS-measured distances. The method adopts a fingerprinting-based algorithm to obtain location results under NLOS-only conditions and uses the conventional multilateration algorithm when at least three LOS-measured distances are available. Additionally, the algorithm set includes two novel hybrid localization algorithms for scenarios with one or two LOS distances. These algorithms use the LOS-measured distances to limit geometrically possible locations and then employ fingerprinting to perform the final location selection. We test our approach in a realistic indoor environment over numerous experimental scenarios. The experimental results show that the proposed localization strategy reduces the mean distance error by 3 to 20 cm compared with the traditional fingerprinting-based approach.},
  archive      = {J_COMCOM},
  author       = {Sandra Djosic and Igor Stojanovic and Milica Jovanovic and Goran Lj. Djordjevic},
  doi          = {10.1016/j.comcom.2021.10.031},
  journal      = {Computer Communications},
  pages        = {365-373},
  shortjournal = {Comput. Commun.},
  title        = {Multi-algorithm UWB-based localization method for mixed LOS/NLOS environments},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a location-based opportunistic geographic routing
protocol. <em>COMCOM</em>, <em>181</em>, 357–364. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly dynamic networks face frequent topological changes. In such a network, traditional routing protocols are inefficient and even impractical, as they need to detect network topology and update locations frequently. To mitigate this problem, this paper puts forward a location-based opportunistic geographic routing (LOGR) protocol, an opportunistic packet transmission mechanism based on geographic locations. In most traditional routing protocols, the forwarding nodes are selected in advance. In the proposed protocol, the sending node does not know which is the next-hop node. Instead, the sending node broadcasts the forwarding rules and the data packet together, utilizing the broadcast nature of wireless transmission. Then, each node receiving the packet will judge whether it has the right to forward the packet, according to the preset rules. Another difference between our protocol and common protocols lies in the priority assignment of opportunistic forwarding. In common protocols, all candidate forwarding nodes are ranked in turn. In our protocol, the absolute priority method is adopted, such that any candidate receiving the packet can derive its own priority from its location information and forwarding rules, and determine the forwarding time, without knowing the information of other candidates. In this way, our protocol eliminates the need for topology detection or location update, and avoids the huge network overhead incurred in pairwise comparison of nodes for priority assignment. Therefore, unlike traditional protocols constrained by network scale and network topology , our protocol can better adapt to the frequent topology change in the highly dynamic network Experimental results show that the LOGR protocol worked effectively on the network containing fast moving nodes, and, in the same environment, the network performed much better under the LOGR protocol than under traditional routing protocols.},
  archive      = {J_COMCOM},
  author       = {Linqi Li and Xiaoyin Wang and Xinhua Ma},
  doi          = {10.1016/j.comcom.2021.10.030},
  journal      = {Computer Communications},
  pages        = {357-364},
  shortjournal = {Comput. Commun.},
  title        = {Design of a location-based opportunistic geographic routing protocol},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-tier blockchain framework to increase protection and
autonomy of smart objects in the IoT. <em>COMCOM</em>, <em>181</em>,
338–356. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Internet of Things paradigm has become pervasive in everyday life attracting the interest of the research community. Two of the most important challenges to be addressed concern the protection of smart objects and the need to guarantee them a great autonomy. For this purpose, the definition of trust and reputation mechanisms appears crucial. At the same time, several researchers have started to adopt a common distributed ledger, such as a Blockchain , for building advanced solutions in the IoT. However, due to the high dimensionality of this problem, enabling a trust and reputation mechanism by leveraging a Blockchain-based technology could give rise to several performance issues in the IoT. In this paper, we propose a two-tier Blockchain framework to increase the security and autonomy of smart objects in the IoT by implementing a trust-based protection mechanism. In this framework, smart objects are suitably grouped into communities. To reduce the complexity of the solution, the first-tier Blockchain is local and is used only to record probing transactions performed to evaluate the trust of an object in another one of the same community or of a different community. Periodically, after a time window, these transactions are aggregated and the obtained values are stored in the second-tier Blockchain. Specifically, stored values are the reputation of each object inside its community and the trust of each community in the other ones of the framework. In this paper, we describe in detail our framework, its behavior, the security model associated with it and the tests carried out to evaluate its correctness and performance.},
  archive      = {J_COMCOM},
  author       = {Enrico Corradini and Serena Nicolazzo and Antonino Nocera and Domenico Ursino and Luca Virgili},
  doi          = {10.1016/j.comcom.2021.10.028},
  journal      = {Computer Communications},
  pages        = {338-356},
  shortjournal = {Comput. Commun.},
  title        = {A two-tier blockchain framework to increase protection and autonomy of smart objects in the IoT},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face detection algorithm based on improved TinyYOLOv3 and
attention mechanism. <em>COMCOM</em>, <em>181</em>, 329–337. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An improved face detection method ground on TinyYOLOv3 algorithm is put forward in this paper in view of the low recognition rate of traditional face detection methods in complex background and the long detection time of existing face detection methods ground on deep learning The main network of TinyYOLOv3 is redesigned to extract more abundant semantic information, which is increasing the number of network layers. The deep separable convolution is used instead of the traditional convolution and the features of different network layers are fused. Meanwhile, the accuracy of face detection is guaranteed, size of this network model is reduced. The CIoU (complete intersection over union) loss is used on improving this original prediction loss of bounding box coordinates. The channel attention mechanism is integrated into the feature extraction network to improve the positioning accuracy and detection accuracy. The network convergence speed is accelerated by hyperparameter optimization and priori box clustering method . The face detection experiments were conducted on the Wider Face data sets. And the experimental results manifest that the proposed algorithm has high recognition accuracy in complex scenes. At the same time, the proposed algorithm is better than others in terms of detection speed and model size, and can meet the requirements of embedded devices.},
  archive      = {J_COMCOM},
  author       = {Jiangjin Gao and Tao Yang},
  doi          = {10.1016/j.comcom.2021.10.023},
  journal      = {Computer Communications},
  pages        = {329-337},
  shortjournal = {Comput. Commun.},
  title        = {Face detection algorithm based on improved TinyYOLOv3 and attention mechanism},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning compatibility knowledge for outfit recommendation
with complementary clothing matching. <em>COMCOM</em>, <em>181</em>,
320–328. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile networks and e-commerce, clothing recommendation has achieved considerable success in recent years. Fashion outfit matching has become an essential component to users while shopping, which helps users to select and present items to individuals in a personalized fashion recommendation. Apparently, it is an arduous task to guide complementary clothing matching due to the complexity and subjectivity of fashion items. Some existing solutions have been presented in recent years, which are tending to discover a series of visual cues to establish the matching relations. However, it would be mismatched easily due to these methods being hard to represent all the potential semantic information from the appearance of clothes. To thoroughly make use of the visual characteristics of clothing products and the related description information, we propose a complementary clothing matching method with some compatibility knowledge, named it CCMCK shortly. For visual compatibility, we adopt the graph neural network to model the visual relationship between items. To generate an outfit that satisfies the requirement of fashion compatibility, we propose a matching way under the compatibility constraint and seek to recommend compatible items based on multi-modal compatibility. Finally, we performed a qualitative investigation on the fill-in-the-blank and fashion outfit compatibility tasks to evaluate the proposed method.},
  archive      = {J_COMCOM},
  author       = {Ruomei Wang and Jianfeng Wang and Zhuo Su},
  doi          = {10.1016/j.comcom.2021.10.022},
  journal      = {Computer Communications},
  pages        = {320-328},
  shortjournal = {Comput. Commun.},
  title        = {Learning compatibility knowledge for outfit recommendation with complementary clothing matching},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy information verification of homomorphic algorithm
for aggregated data based on fog layer structure. <em>COMCOM</em>,
<em>181</em>, 309–319. (<a
href="https://doi.org/10.1016/j.comcom.2021.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Becoming a vital position in the interconnection industry of the Internet of Things , IIoT has promoted the conversion of traditional industries to intelligent industries. However, it is necessary further to solve the security and privacy threats in IIoT while reducing communication bandwidth and computing resource consumption to carry out research. In order to solve the problem of user privacy leakage caused by the access structure, a fog computing-oriented access control structure hiding scheme is proposed in the paper. The Paillier homomorphic filter algorithm is introduced in the fog calculation. The Paillier homomorphic algorithm hides the mapping function in the access structure during the data upload process, achieving the effect of completely hiding the access structure. Moreover, the ciphertext is stored separately from the access structure, and the Paillier homomorphic algorithm is run through the fog node during the decryption process to detect whether the attributes of the data user exist in the hidden access structure. If it exists, reconstruct the mapping function and send it to the data user, who then downloads and decrypts the ciphertext . If it does not exist, it means that the data user does not meet the access conditions of the data, and there is no need to download and decrypt the ciphertext. Meanwhile, a simulation experiment platform is constructed in the paper to distinguish the performance of the method proposed in the paper from other similar methods, proving the efficiency and practicability of the scheme.},
  archive      = {J_COMCOM},
  author       = {Xin Li and Ruitao Liu and Zhenmin Qiao},
  doi          = {10.1016/j.comcom.2021.08.015},
  journal      = {Computer Communications},
  pages        = {309-319},
  shortjournal = {Comput. Commun.},
  title        = {Privacy information verification of homomorphic algorithm for aggregated data based on fog layer structure},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust and distributed architecture for 5G-enabled
networks in the smart blockchain era. <em>COMCOM</em>, <em>181</em>,
293–308. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-Things (IoT) integrates ubiquitous computing to extend the Internet connectivity to the various application environments such as e-Health, smart cities, cyber–physical systems, etc. It connects the real-time objects to experience high-speed data transfer in 5G network deployment. It may periodically collect and process the data to control the access mechanism that utilizes a dedicated network protocol to fetches the information from different network domains. However, most of the access mechanisms cannot be applicable to the industrial application as it is based on a centralized architecture. This architecture has a complexity of computation overhead to deteriorate the performance of device-to-device (D2D) communication. In the industrial sectors, security and privacy preservation are majorly concerned to perform expensive operations and to prevent malicious cloud servers. Moreover, the real-time objects should be coordinated to optimize communication efficiencies. Thus, this paper presents a robust blockchain-based lightweight distributed architecture (RB-LDA) for 5G-enabled networks. The proposed RB-LDA authenticates the device access that advertises genuine data possession to restrict the key exposure. Moreover, the qualitative analysis proves the identity management continuously monitors the activities of IoT devices to perform a better auditing process. Above all, the experimental analysis shows that the proposed RB-LDA can achieve better data privacy to preserve sensitive information against the trusted third parties.},
  archive      = {J_COMCOM},
  author       = {B.D. Deebak and Fadi AL-Turjman},
  doi          = {10.1016/j.comcom.2021.10.015},
  journal      = {Computer Communications},
  pages        = {293-308},
  shortjournal = {Comput. Commun.},
  title        = {A robust and distributed architecture for 5G-enabled networks in the smart blockchain era},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NRflex: Enforcing network slicing in 5G new radio.
<em>COMCOM</em>, <em>181</em>, 284–292. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging 5G networks promise to support novel network services with different requirements in terms of Quality of Service (QoS), such as low-latency and high bandwidth. Thanks to the network slicing concept, 5G is able to fulfill these different requirements while sharing the same physical infrastructure. Although network slicing is gaining maturity, slicing the Radio Access Network (RAN) is still challenging, particularly with the emergence of new physical features added by 5G New Radio (NR), such as Bandwidth part (BWP) and physical numerology. In this paper, we introduce a new framework, namely New Radio flexibility (NRflex), which addresses the challenge of slicing the RAN in 5G. NRflex provides a solution that dynamically assigns BWP to the running slices and their associated User Equipment (UE), aiming to fulfill the slices’ required QoS. Simulation results showed the superiority of NRflex to meet network slice requirements while optimizing the 5G RAN resources, compared to other existing solutions.},
  archive      = {J_COMCOM},
  author       = {Karim Boutiba and Adlen Ksentini and Bouziane Brik and Yacine Challal and Amar Balla},
  doi          = {10.1016/j.comcom.2021.09.034},
  journal      = {Computer Communications},
  pages        = {284-292},
  shortjournal = {Comput. Commun.},
  title        = {NRflex: Enforcing network slicing in 5G new radio},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI-empowered, blockchain and SDN integrated security
architecture for IoT network of cyber physical systems. <em>COMCOM</em>,
<em>181</em>, 274–283. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) is one of the most emerging technologies nowadays and it is one of the key enablers of industrial cyber physical system (CPSs). It has started to participate in almost every aspect of our social life, ranging from financial transactions to the healthcare system, communication to national security, battlefield to smart homes, and so on. However, the wide deployment of IoT suffers certain issues as well, such as interoperability, compatibility, heterogeneity, large amount of data, processing of heterogeneous data etc. Among others, energy efficiency and security are the utmost prominent issues. Scarce computing resources of IoT devices put hindrances on information sharing across edge or IoT network. Indeed, unintentional or malicious interference with IoT data may lead to severe concerns. In this study, the researcher exploits the potential benefits of a blockchain system and integrates it with software-defined networking (SDN) while justifying energy and security issues. More in detail, the researcher proposed a new routing protocol with the cluster structure for IoT networks using blockchain-based architecture for SDN controller. The proposed architecture obviates proof-of-work (PoW) with private and public blockchains for Peer-to-Peer (P2P) communication between SDN controllers and IoT devices. In addition to this, distributed trust-based authentication mechanism makes blockchain even more adoptive for IoT devices with limited resources. The experimental results show that the proposed cluster structure based routing protocol outperforms the state-of-the-art Ad-hoc On-demand Distance Vector (AODV), Destination-Sequenced Distance Vector (DSDV), Secure Mobile Sensor Network (SMSN), Energy efficient secured cluster based distributed fault diagnosis (EESCFD), and Ad-hoc On-demand Multipath Distance Vector (AOMDV), in terms of energy consumption, network throughput, and packet latency. Proposed protocol help overcome the issues especially, energy management and security of the next generation industrial cyber physical systems.},
  archive      = {J_COMCOM},
  author       = {Sohaib A. Latif and Fang B. Xian Wen and Celestine Iwendi and Li-li F. Wang and Syed Muhammad Mohsin and Zhaoyang Han and Shahab S. Band},
  doi          = {10.1016/j.comcom.2021.09.029},
  journal      = {Computer Communications},
  pages        = {274-283},
  shortjournal = {Comput. Commun.},
  title        = {AI-empowered, blockchain and SDN integrated security architecture for IoT network of cyber physical systems},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pairing free asymmetric group key agreement protocol.
<em>COMCOM</em>, <em>181</em>, 267–273. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More and more applications are nowadays distributed and rely on collaborative input. Group key protocols, enabling the construction of a common shared key, are the most evident primitive to efficiently provide the required authentication and confidentiality in the group. Asymmetric group key agreement (AGKA) protocols allow the construction of a common shared public key , where only the participants in the group possessing different private keys are able to decrypt messages encrypted by means of this group public key . Up to now, all the proposed AGKA protocols in literature rely on compute intensive pairing operations, which are too highly demanding for resource constrained devices. We propose in this paper a one-round lightweight elliptic curve based alternative, being able to offer in addition also self-certification to the members of the group due to the usage of Elliptic Curve Qu Vanstone certificates. We show drastic improvements with respect to both communication and computation costs, compared to the pairing based approaches.},
  archive      = {J_COMCOM},
  author       = {An Braeken},
  doi          = {10.1016/j.comcom.2021.10.011},
  journal      = {Computer Communications},
  pages        = {267-273},
  shortjournal = {Comput. Commun.},
  title        = {Pairing free asymmetric group key agreement protocol},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of physical health risk dynamic evaluation system
based on sports network technology. <em>COMCOM</em>, <em>181</em>,
257–266. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The people are the pillar of national construction and the driving force of social sustainable development. Therefore, people must develop morality, intelligence and body in an all-round way. The physical health risk assessment is of great significance to the improvement of the physical health of the public. Based on the sports network technology, this paper improves the traditional human posture recognition algorithm , and combines machine learning to construct the human body dynamic feature recognition method. Moreover, this paper combines actual needs to construct a physical health risk dynamic evaluation system based on sports network technology, which can recognize sports through pose recognition and combine dynamic monitoring technology to detect daily diet and living habits. In addition, this paper constructs the network structure of this paper system and designs its functional modules according to actual needs. Finally, this paper designs experiments to verify the performance of the model constructed in this paper. The research shows that the system constructed in this paper meets actual needs and can provide theoretical references for subsequent related research.},
  archive      = {J_COMCOM},
  author       = {Lianzhen Chen and Hua Zhu},
  doi          = {10.1016/j.comcom.2021.10.002},
  journal      = {Computer Communications},
  pages        = {257-266},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of physical health risk dynamic evaluation system based on sports network technology},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Buffer-loss estimation to address congestion in 6LoWPAN
based resource-restricted “internet of healthcare things” network.
<em>COMCOM</em>, <em>181</em>, 236–256. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Healthcare Things (IoHT) consists of a wide variety of resource-restricted, heterogeneous, IoT-enabled, wearable/non-wearable medical equipment (things) that connect over the internet to transform traditional healthcare into a smart, connected, proactive, patient-centric healthcare system. The pivotal functions of the 6LoWPAN protocol stack enable comprehensive integration of such networks from wearable wireless sensor networks (W-WSN) to IoHT, as TCP/IP does not suffice the requirements of IoHT networks. As a result, the congestion in the IoHT network increases with a growing number of devices, resulting in loss of critical medical information due to buffer loss and channel loss, which is unacceptable. In this paper, we explored different applications of patient-centric IoHT architectures to draw a realistic resource-limited topological layout of IoHT for congestion estimation. After critically reviewing existing congestion schemes for 6LoWPANs, we proposed an effective buffer-loss estimation model based on the Queuing Theory to determine the number of packets lost at the node’s buffer. The buffer is modeled as an M/M/1/K Markov Chain Queue. The M/M/1/K Queue equilibrium equation is used to establish a relationship between the probabilities of the buffer being empty or completely filled. We derived the expressions for total buffer-loss probability and expected mean packet delay for the resource-constraint IoHT network. Furthermore, to validate the buffer-loss estimation, an analytical model is used to compare buffer-loss probabilities, the number of packets dropped at leaf/intermediate nodes and the number of packets successfully received at the local sink node. The results show a close correlation between both the models on varying values of the number of leaf nodes , buffer size, offered packet load and available channel capacity. Thus, in resource-restrictive IoHT, the proposed model performs better than two related works.},
  archive      = {J_COMCOM},
  author       = {Himanshu Verma and Naveen Chauhan and Narottam Chand and Lalit Kumar Awasthi},
  doi          = {10.1016/j.comcom.2021.10.016},
  journal      = {Computer Communications},
  pages        = {236-256},
  shortjournal = {Comput. Commun.},
  title        = {Buffer-loss estimation to address congestion in 6LoWPAN based resource-restricted ‘Internet of healthcare things’ network},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proactive caching and offloading technique using machine
learning for mobile edge computing users. <em>COMCOM</em>, <em>181</em>,
224–235. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mobile edge computing (MEC) paradigm provides cloud and application services at the “edge” of user networks for providing ubiquitous access to resources. The heterogeneous services cause varying network traffic that sometimes increases delay. In edge-based services, concurrency in data distribution requires caching and offloading features. This article introduces a proactive caching technique with offloading (PCTO) ability by considering the need for parallel user services. The proposed method performs demand-aware offloading to meet the concurrent service dissemination requirements. Network-level caching and its forecast in concurrent service distribution are performed to reduce the response time. The offloading and caching processes are streamlined using deep recurrent learning for the failing service distribution intervals. In the learning process, the machine is trained for prior failures and for pursuing offloading instances. Based on the learning output, the caching level and offloading rate are determined for the queuing services. The performance of the proposed method is verified using the metrics service ratio, response failures, latency, offloading rate, and caching ratio.},
  archive      = {J_COMCOM},
  author       = {Fayez Alqahtani and Mohammed Al-Maitah and Osama Elshakankiry},
  doi          = {10.1016/j.comcom.2021.10.017},
  journal      = {Computer Communications},
  pages        = {224-235},
  shortjournal = {Comput. Commun.},
  title        = {A proactive caching and offloading technique using machine learning for mobile edge computing users},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 4G/5G coexistent dynamic spectrum sharing scheme based on
dual bargaining game approach. <em>COMCOM</em>, <em>181</em>, 215–223.
(<a href="https://doi.org/10.1016/j.comcom.2021.10.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel dynamic spectrum sharing (DSS) scheme while satisfying diversified service requirements in the 4G/5G coexisting network platform. To design our scheme, we adopt the basic idea of the cooperative game theory and different allocation rules. In a cooperative game manner, our proposed scheme is formulated as a two-level dual bargaining model. This hierarchical approach can effectively share the limited wireless spectrum resource to significantly reduce the computational complexity and control overheads. To give mutual advantages for network agents, we leverage the full synergy of 4G/5G coexisting infrastructure while maximize the system efficiency. Finally, simulation results show the advantages of the proposed approach by comparing with the existing state-of-the-art spectrum sharing protocols. Specifically, the throughput, payoff and service provision are improved by about 5\%, 10\% and 10\%, respectively, under dynamically changing network system environments. It is worth noting that our dual bargaining based strategic control mechanism facilitates the coordination and cooperation among network entities to effectively allocate the limited spectrum resource. This property can improve the performance measures in the 4G/5G coexisting platform.},
  archive      = {J_COMCOM},
  author       = {Sungwook Kim},
  doi          = {10.1016/j.comcom.2021.10.025},
  journal      = {Computer Communications},
  pages        = {215-223},
  shortjournal = {Comput. Commun.},
  title        = {4G/5G coexistent dynamic spectrum sharing scheme based on dual bargaining game approach},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time cooperative data routing and scheduling in
software defined vehicular networks. <em>COMCOM</em>, <em>181</em>,
203–214. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Links in vehicular networks are highly dynamic and generally exist only for a limited amount of time. Moreover, data packets are generally transmitted over multiple hops and have diverse latency requirements. The above situations together create a dilemma on the order/priority of the packets to be transmitted. However, the existing channel access mechanism of VANET does not critically consider the aforementioned aspects when granting channel access, and rather focuses on avoiding and resolving packet collisions. In order to bridge this gap, we present an improved channel access granting mechanism for data routing and scheduling via software defined vehicular networks (SDVN). We scrutinize the aforementioned parameters via a global perspective and utilize diverse channel modes cooperatively to maximize the delivery of data packets under delay tolerance and link connectivity regions. The primary problem is formulated as an integer linear programming (ILP) problem. The model avoids possible packet transmission conflicts via constraints while scrutinizing the wireless nature of links. Moreover, the proposed model is easily extended for the cooperative data communication under road side unit (RSU) application scenario. Furthermore, we present two computational improvement strategies based on incremental optimization and maximum independent sets (MIS) for the two application scenarios to shift the computational complexity to a realizable level for real-time communication. The effectiveness of the proposed routing and scheduling framework is comparatively evaluated with realistic mobility embedded road networks .},
  archive      = {J_COMCOM},
  author       = {Kalupahana Liyanage Kushan Sudheera and Maode Ma and Peter Han Joo Chong},
  doi          = {10.1016/j.comcom.2021.10.003},
  journal      = {Computer Communications},
  pages        = {203-214},
  shortjournal = {Comput. Commun.},
  title        = {Real-time cooperative data routing and scheduling in software defined vehicular networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free machine learning of wireless SISO/MIMO
communications. <em>COMCOM</em>, <em>181</em>, 192–202. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is a highly promising tool to design the physical layer of wireless communication systems , but the training usually requires an explicit model of the signal distortion as it undergoes transmission over a wireless channel. As data rates, number of MIMO streams and carrier frequencies increase to satisfy the demand for wireless capacity, it becomes difficult to design hardware with few imperfections and to model the imperfections that there are. New machine learning schemes for the physical layer do not require an explicit model but can implicitly learn the end-to-end link including channel characteristics and non-linearities of the system directly from the training data. In this paper, we present a novel neural network architecture that provides an explicit stochastic model for both SISO and MIMO channels, by learning the parameters of a Gaussian mixture distribution from real channel samples. We use this channel model in conjunction with an autoencoder to learn a suitable modulation scheme . We experimentally validate our proposed model in an FPGA-based millimeter-wave testbed for both SISO and MIMO channels, showing that it is able to reproduce the channel characteristics with good accuracy.},
  archive      = {J_COMCOM},
  author       = {Dolores García and Jesus O. Lacruz and Damiano Badini and Danilo De Donno and Joerg Widmer},
  doi          = {10.1016/j.comcom.2021.09.033},
  journal      = {Computer Communications},
  pages        = {192-202},
  shortjournal = {Comput. Commun.},
  title        = {Model-free machine learning of wireless SISO/MIMO communications},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Construction of multi-modal perception model of
communicative robot in non-structural cyber physical system environment
based on optimized BT-SVM model. <em>COMCOM</em>, <em>181</em>, 182–191.
(<a href="https://doi.org/10.1016/j.comcom.2021.10.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of intelligent control technology, computer technology, bionics, artificial intelligence and other disciplines, more and more attention has been paid to the research of intelligent mobile robot technology, and autonomous positioning is the basis for mobile robots to conduct autonomous navigation and exploration research. Sensors assist each other to provide a wealth of perception information about the internal state of the robot and the external surrounding environment. This paper proposes a method for optimizing the Support Vector Machine (SVM) multi-classifier with a binary tree structure, which improves the accuracy of multi-modal tactile signal recognition. The improved particle swarm clustering algorithm is used to optimize the binary tree structure, reduce the error accumulation of the binary tree structure SVM multi-classifier, and further improve the accuracy of multi-modal tactile signal recognition. The effect of the method in this paper is verified by robot grasping experiments. The results show that the use of multi-modal information of two-dimensional images and three-dimensional point cloud images can effectively identify and locate target objects of different shapes. Compared with the processing method of two-dimensional or point cloud monomodal image information, the positioning error can be reduced by 54.8\%, and the direction error can be reduced by 50.8\%, which has better robustness and accuracy. The simulation results show that the improved PSOBT-SVM model has the best classification effect for artificial features, PCA features and spatio-temporal correlation features. The improved PSOBT-SVM optimizes the classification accuracy without changing the number of SVM classifiers, and proves its accuracy in classifying multimodal tactile signals.},
  archive      = {J_COMCOM},
  author       = {Hui Zeng and Jiaqi Luo},
  doi          = {10.1016/j.comcom.2021.10.019},
  journal      = {Computer Communications},
  pages        = {182-191},
  shortjournal = {Comput. Commun.},
  title        = {Construction of multi-modal perception model of communicative robot in non-structural cyber physical system environment based on optimized BT-SVM model},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Big data-driven scheduling optimization algorithm for
cyber–physical systems based on a cloud platform. <em>COMCOM</em>,
<em>181</em>, 173–181. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study big data-driven Cyber–Physical Systems (CPS) through cloud platforms and design scheduling optimization algorithms to improve the efficiency of the system. A task scheduling scheme for large-scale factory access under cloud–edge collaborative computing architecture is proposed. The method firstly merges the directed acyclic graphs on cloud-side servers and edge-side servers; secondly, divide the tasks using a critical path-based partitioning strategy to effectively improve the allocation accuracy; then achieves load balancing through reasonable processor allocation, and finally compares and analyses the proposed task scheduling algorithm through simulation experiments. The experimental system is thoroughly analysed, hierarchically designed, and modelled, simulated, and the experimental data analysed and compared with related methods. The experimental results prove the effectiveness and correctness of the worst-case execution time analysis method and the idea of big data-driven CPS proposed in this paper and show that big data knowledge can help improve the accuracy of worst-case execution time analysis. This paper implements a big data-driven scheduling optimization algorithm for Cyber–Physical Systems based on a cloud platform, which improves the accuracy and efficiency of the algorithm by about 15\% compared to other related studies.},
  archive      = {J_COMCOM},
  author       = {Chao Niu and Lizhou Wang},
  doi          = {10.1016/j.comcom.2021.10.020},
  journal      = {Computer Communications},
  pages        = {173-181},
  shortjournal = {Comput. Commun.},
  title        = {Big data-driven scheduling optimization algorithm for Cyber–Physical systems based on a cloud platform},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human motion state recognition based on MEMS sensors and
zigbee network. <em>COMCOM</em>, <em>181</em>, 164–172. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is to study the system structure scheme based on Zigbee wireless transmission, and complete the overall design of the system scheme on this basis. Human motion capture systems are widely used in the creation of film and television works, motion analysis , video games , rehabilitation medicine and other fields. This article discusses the design and implementation of a human motion capture system based on MEMS sensors and Zigbee networks. The system can be installed on the human body Multiple sensor nodes in various parts obtain the movement information of the human body, and use sensor network technology to aggregate these data and upload them to the host computer. First, this article introduces the characteristics of angular velocity sensors, acceleration sensors, magneto resistive sensors and Zigbee networks. Then, this article explains the overall structure of the system, and from a theoretical point of view, explains how the system uses angular velocity sensors, acceleration sensors, magneto resistive sensors and Zigbee networks to achieve human motion capture. This part focuses on including vector observation methods and angular velocity Two posture capture methods including the integration method, and their advantages and disadvantages are analyzed. To achieve the complementary advantages of the two algorithms, a data fusion algorithm based on complementary filtering is introduced and optimized appropriately. In addition, this article also introduces the networking principles and optimization schemes of the Zigbee network in this section. After this, this article explains in detail the system hardware structure, chip selection scheme, circuit design scheme, software workflow and implementation of core programs Method. Finally, this article shows the effect of the actual work of the system, and compares it with the theory to verify the feasibility of the theory. Based on the research of MEMS sensor measurement unit and algorithm, a Zigbee-based wireless transmission test system was established. LabVIEW software with functions of data reception, attitude angle calculation, trajectory calculation, eigenvalue extraction, BP neural network recognition, display and data saving was designed and tested the whole system functions. The test results show that the wireless data transmission of Zigbee network is normal, the data detection and processing programs of the host computer are stable, and the correct identification of the human body’s motion state can be realized. The results show that compared with the existing research, our research has increased its efficiency by 10\%, and its accuracy has increased by nearly 15\%.},
  archive      = {J_COMCOM},
  author       = {Qing Liu},
  doi          = {10.1016/j.comcom.2021.10.018},
  journal      = {Computer Communications},
  pages        = {164-172},
  shortjournal = {Comput. Commun.},
  title        = {Human motion state recognition based on MEMS sensors and zigbee network},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepPlace: Deep reinforcement learning for adaptive flow
rule placement in software-defined IoT networks. <em>COMCOM</em>,
<em>181</em>, 156–163. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel and adaptive flow rule placement system based on deep reinforcement learning , namely DeepPlace, in Software-Defined Internet of Things (SDIoT) networks. DeepPlace can provide a fine-grained traffic analysis capability while assuring QoS of traffic flows and proactively avoiding the flow-table overflow issue in the data plane. Specifically, we first investigate the traffic forwarding process in an SDIoT network, i.e., routing and flow rule placement tasks . We design a cost function for the routing to set up traffic flow paths in the data plane. Next, we propose an adaptive flow rule placement approach to maximize the number of match-fields in a flow rule at SDN switches. To deal with the dynamics of IoT traffic flows, we model the system operation by using the Markov decision process (MDP) with a continuous action space and formulate its optimization problem . Subsequently, we develop a deep deterministic policy gradient-based algorithm to help the system obtain the optimal policy . The evaluation results demonstrate that DeepPlace can efficiently maintain a significant number of match-fields in a flow rule, i.e., approximately 86\% of the maximum level, while minimizing the QoS violation ratio of traffic flows, i.e., 6.7\%, in a highly dynamic traffic scenario, which outperforms three other existing solutions, i.e., FlowMan, FlowStat, and DeepMatch.},
  archive      = {J_COMCOM},
  author       = {Tri Gia Nguyen and Trung V. Phan and Dinh Thai Hoang and Hai Hoang Nguyen and Duc Tran Le},
  doi          = {10.1016/j.comcom.2021.10.006},
  journal      = {Computer Communications},
  pages        = {156-163},
  shortjournal = {Comput. Commun.},
  title        = {DeepPlace: Deep reinforcement learning for adaptive flow rule placement in software-defined IoT networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triad link prediction method based on the evolutionary
analysis with IoT in opportunistic social networks. <em>COMCOM</em>,
<em>181</em>, 143–155. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data volume of various smart devices has exploded with the advent of the 5G communication era and the rapid development of Internet of Things technology . A large amount of data has higher requirements on the transmission protocol in the network. Most of the existing research focuses on studying a single node as the basic information transmission unit. However, social networks have complex topological structures , and such methods are difficult to grasp the evolutionary rules of the network. It usually leads to higher resource waste and problem complexity. A triad (a network subgraph containing three nodes) can not only provide accurate local topology information, but also its conversion rules are easy to describe, which can reduce the complexity of the problem. The mutual conversion between triads constitutes the evolution of the entire network. Based on this, this article first proposes a triad-based social network evolution analysis method (SNEA). SNEA includes a prediction algorithm (TPMPA) that learns the evolution of the triad transition probability matrix through time series and a quantization algorithm (TTHQA) that quantifies the impact of triad transformations on the network-based on random walks. SNEA integrates the advantages of the two algorithms to dynamically grasp the evolutionary rules of the network. Then this paper proposes a triad link prediction algorithm (TLPA) to quantitatively evaluate the results of the evolution analysis of the SNEA method. The TLPA algorithm reduces the blindness of message forwarding and unnecessary waste of resources by predicting the probability of a connection between nodes in the network. Experimental results show that compared with Epidemic, SECM, CRDNT, ICMT algorithms, our method has a prominent advantage in improving message delivery rate and reducing resource consumption.},
  archive      = {J_COMCOM},
  author       = {Fangfang Gou and Jia Wu},
  doi          = {10.1016/j.comcom.2021.10.009},
  journal      = {Computer Communications},
  pages        = {143-155},
  shortjournal = {Comput. Commun.},
  title        = {Triad link prediction method based on the evolutionary analysis with IoT in opportunistic social networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal caching scheme in D2D networks with multiple robot
helpers. <em>COMCOM</em>, <em>181</em>, 132–142. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots are playing an important role in modern industries. The deployment of robots which act as mobile helpers in a wireless network is rarely considered in the existing studies of the device-to-device (D2D) caching schemes. In this paper, we investigate the optimal caching scheme for D2D networks with multiple robot helpers with large cache size. An improved caching scheme named robot helper aided caching (RHAC) scheme is proposed to optimize the system performance by moving the robot helpers to the optimal positions. The optimal locations of the robot helpers can be found based on partitioned adaptive particle swarm optimization (PAPSO) algorithm. And based on these two algorithms, we propose a mobility-aware optimization strategy for the robot helpers. The simulation results demonstrate that compared with other conventional caching schemes, the proposed RHAC scheme can bring significant performance improvements in terms of hitting probability, cost, delay and energy consumption. Furthermore, the location distribution and mobility of the robot helpers are studied, which provides a reference for introducing robot helpers into different scenarios such as smart factories.},
  archive      = {J_COMCOM},
  author       = {Yu Lin and Hui Song and Feng Ke and Weizhao Yan and Zhikai Liu and Faming Cai},
  doi          = {10.1016/j.comcom.2021.09.027},
  journal      = {Computer Communications},
  pages        = {132-142},
  shortjournal = {Comput. Commun.},
  title        = {Optimal caching scheme in D2D networks with multiple robot helpers},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Certificateless signature schemes in industrial internet of
things: A comparative survey. <em>COMCOM</em>, <em>181</em>, 116–131.
(<a href="https://doi.org/10.1016/j.comcom.2021.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT), which is a growing domain, provides a universal connection to the Internet by spinning common objects to connected ones by changing the way people communicate and interact with the things around them. This type of setup paves the way for the creation of interconnected infrastructure to support innovative services that ensure improved efficiency and flexibility. Such benefits are attractive for user applications and industrial domain. The entry of the IoT domain into the industrial market, also termed as Industrial Internet of Things (IIoT), was recently observed. However, security threats are increasing daily with the prevalent use of IIoT technology. An efficient security solution that can help in the prevention of malicious attacks is researched despite the existence of multiple security solutions. The current study will help the research community to understand the security flaws and causes by classifying and comparing the different certificateless signature schemes of IIoT domain. This survey aims to provide a comparative analysis of the available solutions to improve security. The multi-criteria decision-making approach is utilized for the comparative analysis of the existing certificateless signature schemes by employing the EDAS technique to evaluate the previously suggested solution proposed for IIoT. The authors believe that this technique has never been previously used for any cryptographic solutions. In addition, the study addresses some of the public research issues for technologists , academia, and researchers to develop the security aspects of IIoT.},
  archive      = {J_COMCOM},
  author       = {Saddam Hussain and Syed Sajid Ullah and Ihsan Ali and Jiafeng Xie and Venkata N. Inukollu},
  doi          = {10.1016/j.comcom.2021.10.010},
  journal      = {Computer Communications},
  pages        = {116-131},
  shortjournal = {Comput. Commun.},
  title        = {Certificateless signature schemes in industrial internet of things: A comparative survey},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed antenna orientation solution for optimizing
communications in a fleet of UAVs. <em>COMCOM</em>, <em>181</em>,
102–115. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we describe how a fleet of unmanned aerial vehicles (UAVs) can optimize communication performances by having its members independently change their orientations. This distributed solution, based on a hill-climbing approach, relies on information available locally at each node, namely the reception power of the received frames. The solution is evaluated using the ns–3 network simulator , whose source code is modified to be able to deal with non-isotropic antennas in the context of Wi-Fi networks, as well as simulate angular movements. As isotropic antennas are only theoretical objects, this step is mandatory in order to increase the realism of network simulations. The results, obtained using realistic antenna models, highlight that controlled mobility, in particular controlled orientation needs to be considered in order for UAV networks to provide better performances.},
  archive      = {J_COMCOM},
  author       = {Rémy Grünblatt and Isabelle Guérin Lassous and Olivier Simonin},
  doi          = {10.1016/j.comcom.2021.09.020},
  journal      = {Computer Communications},
  pages        = {102-115},
  shortjournal = {Comput. Commun.},
  title        = {A distributed antenna orientation solution for optimizing communications in a fleet of UAVs},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auction design for cross-edge task offloading in
heterogeneous mobile edge clouds. <em>COMCOM</em>, <em>181</em>, 90–101.
(<a href="https://doi.org/10.1016/j.comcom.2021.09.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading is a promising technology to exploit the available resources in edge cloud efficiently. Many incentive mechanisms for offloading systems have been proposed. However, most of existing works study the centralized incentive mechanisms under the assumption that all mobile edge infrastructures are operated by a central cloud. In this paper, we aim to design the auction-based truthful incentive mechanisms for heavily loaded task offloading system in heterogeneous MECs . We first study the homogeneous MEC situation and present a global auction executed in the central cloud as a benchmark. For the heterogeneous MEC situation, we model the system as a dual auction framework, which enables the heterogeneous MECs to perform cross-edge task offloading without the participation of central servers. Specifically, we design two dual auction models : secondary auction-based model, which enables the system to offload tasks from a large-scale region in a single auction, and double auction-based model, which is suitable for the time sensitive tasks. Then the auctions for these two dual auction models are proposed. Through rigorous theoretical analysis, we demonstrate that the proposed auctions achieve desirable properties of computational efficiency, individual rationality , budget balance, truthfulness, and guaranteed approximation . The simulation results show that the secondary auction and double auction can obtain 14.5\% and 4.2\% more social welfare than comparison algorithm on average, respectively. In addition, the double auction has great advantage in terms of computation efficiency.},
  archive      = {J_COMCOM},
  author       = {Weifeng Lu and Weiduo Wu and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu},
  doi          = {10.1016/j.comcom.2021.09.035},
  journal      = {Computer Communications},
  pages        = {90-101},
  shortjournal = {Comput. Commun.},
  title        = {Auction design for cross-edge task offloading in heterogeneous mobile edge clouds},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reliability evaluation of markov cyber–physical system
oriented to cognition of equipment operating status. <em>COMCOM</em>,
<em>181</em>, 80–89. (<a
href="https://doi.org/10.1016/j.comcom.2021.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computing, communication, and control technologies, cyber–physical systems that integrate physical space, information space, and social space have emerged and are widely used in various important infrastructures. This article introduces the composition and basic algorithm of the hidden Markov model, and gives the mathematical description of the hidden Markov model. Since the hidden Markov model can deduce the hidden state of the observation object through the observed feature values, a device operating state cognition scheme based on the hidden Markov model is proposed. A method for analyzing cascading failures is proposed, and the critical threshold value of cyber–physical system under random attack is obtained. It is verified by simulation experiments, and the changes of system critical thresholds under different network parameters are compared and analyzed. We mainly use several sets of simulation experiments to verify the reliability of the critical threshold, and then verify near the critical threshold. Before simulating the cascading failure process, we first construct two random networks based on the average degree and the number of nodes. According to the previous description of the cyber–physical system model, a node in network B is randomly connected with three nodes in network A, so that the two networks are connected together to form a coupled system. Random attack or failure is represented by randomly deleting nodes. In the simulation experiment, we will simulate the process of cascading failure at each step, and after each step of cascading failure, we output and save the number of remaining nodes. When no nodes in the two networks are deleted, the cascading failure will stop, and then we will verify the critical threshold through the data obtained from the analysis. This provides the support of related theories and methods for the design of stable and reliable cyber–physical systems.},
  archive      = {J_COMCOM},
  author       = {Qin Zhang and Yutang Liu},
  doi          = {10.1016/j.comcom.2021.10.004},
  journal      = {Computer Communications},
  pages        = {80-89},
  shortjournal = {Comput. Commun.},
  title        = {Reliability evaluation of markov cyber–physical system oriented to cognition of equipment operating status},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Post-quantum lightweight identity-based two-party
authenticated key exchange protocol for internet of vehicles with
probable security. <em>COMCOM</em>, <em>181</em>, 69–79. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With Internet of Things (IoT) growing rapidly, the Internet of Vehicles (IoV) has become an essential part of smart cities and has attracted the full attention of both academic and business communities. Because of the public transmission channel, the security and privacy in IoV have paid serious attention. In IoV, it is crucial to generate a secret session key among the various vehicles and road-side units (RSUs) so that they can share the confidential information over the public Internet. Thus, an authenticated key agreement (AKA) protocol should be needed that can achieve the session key requirement in the IoV for secure communication. For this purpose, various AKA techniques has been designed using a number of different tools. Several existing AKA protocols either suffer from different attacks or inefficient for IoV environment due to its excessive communication and computational costs. Many such traditional schemes have used either Diffie–Hellman (DH) or prime factorization type hard problems. These hard problems are vulnerable to the futuristic technologies like quantum computer . Besides, existing quantum resistance AKA protocols use lattice cryptography for its security. However, these protocols either incurs an overhead of certificate management or have excessive communication and computational costs. Hence, there is a need of quantum resistance AKA protocols which removes the certificate overhead and also efficient for the IoV. In this paper, we propose a lattice-based two-party authenticated key agreement (LB-ID-2PAKA) protocol using identity-based cryptography (IBC). The lattice hard problems could resist the quantum computers and IBC could remove the overhead of certificate management. The security strength of proposed LB-ID-2PAKA protocol is analyzed under the random oracle model to show its robustness against the present as well as future quantum attacks. In addition, the resiliency against different security attacks such as man-in-the-middle (MITM) attack, known-key security (K-KS), unknown key-share (UK-S) attack etc. are also included. Further, the performance analysis shows that the proposed LB-ID-2PAKA protocol outperforms the existing protocols and feasible for IoV applications.},
  archive      = {J_COMCOM},
  author       = {Daya Sagar Gupta and Sangram Ray and Tajinder Singh and Madhu Kumari},
  doi          = {10.1016/j.comcom.2021.09.031},
  journal      = {Computer Communications},
  pages        = {69-79},
  shortjournal = {Comput. Commun.},
  title        = {Post-quantum lightweight identity-based two-party authenticated key exchange protocol for internet of vehicles with probable security},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making slotted ALOHA efficient and fair using reinforcement
learning. <em>COMCOM</em>, <em>181</em>, 58–68. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has been proposed as a technique that allows nodes to learn to coordinate their transmissions in order to attain much higher channel utilization. Several RL-based approaches have been proposed to improve the performance of slotted ALOHA; however, all these schemes have assumed that immediate feedback is available at the transmitters regarding the outcome of their transmissions. This paper introduces ALOHA-dQT, which is the first channel-access protocol based on the use of RL in the context of slotted ALOHA that takes into account the use of explicit acknowledgments from receivers to senders. As such, ALOHA-dQT is the first RL-based approach for channel access that is suitable for wireless networks that do not rely on centralized repeaters or base stations . ALOHA-dQT achieves high utilization by having nodes broadcast short summaries of the channel history as known to them along with their packets. Simulation results show that ALOHA-dQT leads to network utilization above 75\%, with fair bandwidth allocation among nodes.},
  archive      = {J_COMCOM},
  author       = {Molly Zhang and Luca de Alfaro and J.J. Garcia-Luna-Aceves},
  doi          = {10.1016/j.comcom.2021.09.018},
  journal      = {Computer Communications},
  pages        = {58-68},
  shortjournal = {Comput. Commun.},
  title        = {Making slotted ALOHA efficient and fair using reinforcement learning},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart unmanned aerial vehicles as base stations placement to
improve the mobile network operations. <em>COMCOM</em>, <em>181</em>,
45–57. (<a href="https://doi.org/10.1016/j.comcom.2021.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future mobile communication networks need Unmanned Aerial Vehicles as Base Stations (UAVasBSs) with the fast-moving and long-term hovering capabilities to guarantee consistent network performance. UAVasBSs help 5G/B5G mobile communication systems to rapidly recover from emergency situations and handle the instant traffic of the flash crowd. In this context, multiple UAVs might form a flying ad-hoc network to establish a flying access network to enhance the network connectivity and service quality. Therefore, it is important to determine the optimal number and locations of UAVasBSs in a fast and efficient way to cover the target area to provide temporary yet reliable cellular connectivity. The use of Artificial Intelligence (AI) and network data analysis are key tools to fulfill the above issues. In this article, we propose a smart UAVasBS placement (SUAP) mechanism to improve the mobile network operations in flash crowd and emergency situations. We have modeled such an UAVasBS placement task as an optimization problem to obtain required network connectivity and system performance, and resolved it with a genetic algorithm using the network context information. Simulation results show that our proposal could cover 90\% of mobile users, and it provides nearly 90\% packet delivery ratio for users with a fast convergence rate.},
  archive      = {J_COMCOM},
  author       = {Zhongliang Zhao and Pedro Cumino and Christian Esposito and Meng Xiao and Denis Rosário and Torsten Braun and Eduardo Cerqueira and Susana Sargento},
  doi          = {10.1016/j.comcom.2021.09.016},
  journal      = {Computer Communications},
  pages        = {45-57},
  shortjournal = {Comput. Commun.},
  title        = {Smart unmanned aerial vehicles as base stations placement to improve the mobile network operations},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying unlinkability in multi-hop wireless networks.
<em>COMCOM</em>, <em>181</em>, 32–44. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a multi-hop wireless network in which devices act as anonymizing routers. Even if devices anonymize their link transmissions, an adversary may still be able to infer key information by observing the traffic patterns in the network. In this work, we quantify how well an adversary can infer unlinkability, that is, the probability that different pairs of devices are communicating, from anonymized link transmissions. We first propose a metric to compute unlinkability using a Kalman-filter based adversary. Using this metric, we then evaluate how different network characteristics impact unlinkability. We assume that devices do not reorder packets to mix traffic and thereby increase unlinkability. Instead, we show that traffic mixing is still possible due to the use of multi-hop routing and broadcast transmissions, with the amount of mixing dependent on the network characteristics. In simulation, we find that (i) for unicast links, as network connectivity increases unlinkability decreases, while for broadcast links, as connectivity increases unlinkability increases, (ii) link dynamics tend to increase unlinkability with unicast links but decrease unlinkability with broadcast links, (iii) well-connected topologies, particularly with broadcast links, achieve the same level of unlinkability with fewer transmissions per packet delivered, (iv) a lattice topology has consistently good unlinkability in different scenarios, and (v) heterogeneous network traffic gives higher unlinkability and better anonymization efficiency than uniform traffic, even when the average rate of traffic is the same.},
  archive      = {J_COMCOM},
  author       = {Victoria Ursula Manfredi and Cameron Donnay Hill},
  doi          = {10.1016/j.comcom.2021.09.022},
  journal      = {Computer Communications},
  pages        = {32-44},
  shortjournal = {Comput. Commun.},
  title        = {Quantifying unlinkability in multi-hop wireless networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-based platoon control. <em>COMCOM</em>, <em>181</em>,
17–31. (<a href="https://doi.org/10.1016/j.comcom.2021.09.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platooning of cars or trucks is one of the most relevant applications of autonomous driving , since it has the potential to greatly improve efficiency in road utilization and fuel consumption. Traditional proposals of vehicle platooning were based on distributed architectures with computation on board platoon vehicles and direct vehicle-to-vehicle (V2V) communications (or Dedicated Short Range Communication—DSRC), possibly with the support of roadside units. However, with the introduction of the 5G technology and of computing elements at the edge of the network, according to the multi-access edge computing (MEC) paradigm, the possibility emerges of placing control of platoons on MEC, with several significant advantages with respect to the V2V approach. For this reason, in this article we investigate the feasibility of vehicle platooning in an edge-based scenario where the control of vehicle speed and acceleration is managed by the network through its MEC facilities, possibly with a platooning-as-a-service (PaaS) paradigm. Using a detailed simulator, we show that, with realistic values of latency and packet loss probability , as well as of engines and inertia of vehicles, large platoons can be effectively controlled by MEC hosts. On the one hand, we unveil that platooning on the edge is a viable and robust solution. On the other hand, we also shed light on the necessity to consider realistic characteristics of vehicles and speed profiles, since they can yield severe, yet not critical, performance degradation with respect to simple models.},
  archive      = {J_COMCOM},
  author       = {Christian Quadri and Vincenzo Mancuso and Marco Ajmone Marsan and Gian Paolo Rossi},
  doi          = {10.1016/j.comcom.2021.09.021},
  journal      = {Computer Communications},
  pages        = {17-31},
  shortjournal = {Comput. Commun.},
  title        = {Edge-based platoon control},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Special issue on scalable and secure platforms for UAV
networks. <em>COMCOM</em>, <em>181</em>, 14–16. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Luca Chiaraviglio ( Lead guest editor ) and Vinay Chamola and Biplab Sikdar and Guangjie Han},
  doi          = {10.1016/j.comcom.2021.09.032},
  journal      = {Computer Communications},
  pages        = {14-16},
  shortjournal = {Comput. Commun.},
  title        = {Special issue on scalable and secure platforms for UAV networks},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How does the traffic behavior change by using SUMO traffic
generation tools. <em>COMCOM</em>, <em>181</em>, 1–13. (<a
href="https://doi.org/10.1016/j.comcom.2021.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulations are the traditional approach used by the research community to evaluate mobile ad hoc networks. Mainly, vehicular ad hoc networks (VANETs) are a particular type of mobile ad hoc networks that raise specific technical challenges. When assessing VANETs, it is crucial to use realistic mobility models and traffic demand to produce meaningful results. In this context, vehicular traces affect vehicles’ signal strengths, radio interference, and channel occupancy. This paper provides a thorough analysis of the influence of using the different SUMO’s traffic demand generation tools on mobility and node connectivity. Using the data traffic in the district of Gracia in Barcelona (Spain), we analyze the generated traffic demand in terms of traffic measures: (i) traffic intensity, (ii) trip time/distance, (iii) emissions, and (iv) re-routing capabilities. This last feature allow cars to re-compute their routes in front of congestion situations. Then, we analyze the available tools in terms of resources usage (CPU, RAM , disk). Lastly, we analyze the node’s connectivity using well-known graph metrics. Our results provide insights into the behavior of the vehicle’s mobility and the nodes’ connectivity of SUMO demand generation tools. Additionally, we propose an automatized tool that facilitates researchers the generation of synthetic traffic based on real data.},
  archive      = {J_COMCOM},
  author       = {Pablo Barbecho Bautista and Luis Urquiza Aguiar and Mónica Aguilar Igartua},
  doi          = {10.1016/j.comcom.2021.09.023},
  journal      = {Computer Communications},
  pages        = {1-13},
  shortjournal = {Comput. Commun.},
  title        = {How does the traffic behavior change by using SUMO traffic generation tools},
  volume       = {181},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
