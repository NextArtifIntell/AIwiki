<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aoas---130">AOAS - 130</h2>
<ul>
<li><details>
<summary>
(2022). Bayesian bi-clustering methods with applications in
computational biology. <em>AOAS</em>, <em>16</em>(4), 2804–2831. (<a
href="https://doi.org/10.1214/22-AOAS1622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bi-clustering is a useful approach in analyzing large biological data sets when the observations come from heterogeneous groups and have a large number of features. We outline a general Bayesian approach in tackling bi-clustering problems in moderate to high dimensions and propose three Bayesian bi-clustering models on categorical data which increase in complexities in their modeling of the distributions of features across bi-clusters. Our proposed methods apply to a wide range of scenarios: from situations where data are cluster-distinguishable only among a small subset of features but masked by a large amount of noise to situations where different groups of data are identified by different sets of features or data exhibit hierarchical structures. Through simulation studies we show that our methods outperform existing (bi-)clustering methods in both identifying clusters and recovering feature distributional patterns across bi-clusters. We further apply the developed approaches to a human genetic dataset, a human single-cell genomic dataset, and a collection of 1774 mouse genomic datasets with a focus on 58 genes from two pathways.},
  archive      = {J_AOAS},
  author       = {Han Yan and Jiexing Wu and Yang Li and Jun S. Liu},
  doi          = {10.1214/22-AOAS1622},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2804-2831},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian bi-clustering methods with applications in computational biology},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale spectral modelling for nonstationary time series
within an ordered multiple-trial experiment. <em>AOAS</em>,
<em>16</em>(4), 2774–2803. (<a
href="https://doi.org/10.1214/22-AOAS1614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the neurosciences it is natural to observe variability across time in the dynamics of an underlying brain process. Wavelets are essential in analysing brain signals because, even within a single trial, brain signals exhibit nonstationary behaviour. However, neurological signals generated within an experiment may also potentially exhibit evolution across trials (replicates), even for identical stimuli. As neurologists consider localised spectra of brain signals to be most informative, we propose the MULtiple-Trials Locally Stationary Wavelet process (MULT-LSW) that fills the gap in the literature by directly giving a stochastic wavelet representation of the time series of ordered replicates itself. MULT-LSW yields a natural desired time- and trial-localisation of the process dynamics, capturing nonstationary behaviour both within and across trials. While current techniques are restricted by the assumption of uncorrelated replicates, here we account for between-trial correlation. We rigorously develop the associated wavelet spectral estimation framework along with its asymptotic properties. By means of thorough simulation studies, we demonstrate the theoretical estimator properties hold in practice. A real data investigation into the evolutionary dynamics of the hippocampus and nucleus accumbens, during an associative learning experiment, demonstrates the applicability of our proposed methodology as well as the new insights it provides. Our model is general and facilitates wider experimental data analysis than the current literature allows.},
  archive      = {J_AOAS},
  author       = {Jonathan Embleton and Marina I. Knight and Hernando Ombao},
  doi          = {10.1214/22-AOAS1614},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2774-2803},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multiscale spectral modelling for nonstationary time series within an ordered multiple-trial experiment},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalar on network regression via boosting. <em>AOAS</em>,
<em>16</em>(4), 2755–2773. (<a
href="https://doi.org/10.1214/22-AOAS1612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging studies have a growing interest in learning the association between the individual brain connectivity networks and their clinical characteristics. It is also of great interest to identify the sub-brain networks as biomarkers to predict the clinical symptoms, such as disease status, potentially providing insight on neuropathology. This motivates the need for developing a new type of regression model where the response variable is scalar, and predictors are networks that are typically represented as adjacent matrices or weighted adjacent matrices to which we refer as scalar-on-network regression. In this work we develop a new boosting method for model fitting with subnetwork markers selection. Our approach, as opposed to group lasso or other existing regularization methods, is, essentially, a gradient descent algorithm leveraging known network structure. We demonstrate the utility of our methods via simulation studies and analysis of the resting-state fMRI data in a cognitive developmental cohort study.},
  archive      = {J_AOAS},
  author       = {Emily L. Morris and Kevin He and Jian Kang},
  doi          = {10.1214/22-AOAS1612},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2755-2773},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Scalar on network regression via boosting},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimensions, power and factors in an observational study of
behavioral problems after physical abuse of children. <em>AOAS</em>,
<em>16</em>(4), 2732–2754. (<a
href="https://doi.org/10.1214/22-AOAS1611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many observational studies assess the impact of a treatment on an outcome that has several dimensions. In the observational study that we discuss, physical abuse of children may affect the degree to which the child exhibits depression, withdrawal or aggression. A treatment may affect all, some or none of these dimensions. In addition to the scientific interest in learning the effect on each dimension, it is also known that an appropriate combination of dimensions may increase power, efficiency and insensitivity to unmeasured biases; however, finding this appropriate combination requires corrections for multiple testing that erode power. We explore this trade-off by developing a new formula for the power of a sensitivity analysis in a simple situation with several dimensions. The methodology is applied to study the effects of physical abuse in early childhood and its possible effects on several dimensions of subsequent behavioral problems. Also, a general method is proposed for converting any signed rank test for matched pairs into an analogous test for matching each treated individual to several controls, and the performance of this extension is examined. The proposed method aids in studying the relative magnitude of the effect on different dimensions. A second evidence factor considers the dose or intensity of physical abuse.},
  archive      = {J_AOAS},
  author       = {Ting Ye and Dylan S. Small and Paul R. Rosenbaum},
  doi          = {10.1214/22-AOAS1611},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2732-2754},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dimensions, power and factors in an observational study of behavioral problems after physical abuse of children},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spatial causal analysis of wildland fire-contributed PM2.5
using numerical model output. <em>AOAS</em>, <em>16</em>(4), 2714–2731.
(<a href="https://doi.org/10.1214/22-AOAS1610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildland fire smoke contains hazardous levels of fine particulate matter (PM2.5), a pollutant shown to adversely effect health. Estimating fire attributable PM2.5 concentrations is key to quantifying the impact on air quality and subsequent health burden. This is a challenging problem since only total PM2.5 is measured at monitoring stations and both fire-attributable PM2.5 and PM2.5 from all other sources are correlated in space and time. We propose a framework for estimating fire-contributed PM2.5 and PM2.5 from all other sources using a novel causal inference framework and bias-adjusted chemical model representations of PM2.5 under counterfactual scenarios. The chemical model representation of PM2.5 for this analysis is simulated using Community Multiscale Air Quality Modeling System (CMAQ), run with and without fire emissions across the contiguous U.S. for the 2008–2012 wildfire seasons. The CMAQ output is calibrated with observations from monitoring sites for the same spatial domain and time period. We use a Bayesian model that accounts for spatial variation to estimate the effect of wildland fires on PM2.5 and state assumptions under which the estimate has a valid causal interpretation. Our results include estimates of the contributions of wildfire smoke to PM2.5 for the contiguous U.S. Additionally, we compute the health burden associated with the PM2.5 attributable to wildfire smoke.},
  archive      = {J_AOAS},
  author       = {Alexandra Larsen and Shu Yang and Brian J. Reich and Ana G. Rappold},
  doi          = {10.1214/22-AOAS1610},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2714-2731},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A spatial causal analysis of wildland fire-contributed PM2.5 using numerical model output},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling extremes of spatial aggregates of precipitation
using conditional methods. <em>AOAS</em>, <em>16</em>(4), 2693–2713. (<a
href="https://doi.org/10.1214/22-AOAS1609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference on the extremal behaviour of spatial aggregates of precipitation is important for quantifying river flood risk. There are two classes of previous approach, with one failing to ensure self-consistency in inference across different regions of aggregation and the other imposing highly restrictive assumptions. To overcome these issues, we propose a model for high-resolution precipitation data from which we can simulate realistic fields and explore the behaviour of spatial aggregates. Recent developments have seen spatial extensions of the Heffernan and Tawn (J. R. Stat. Soc. Ser. B. Stat. Methodol. 66 (2004) 497–546) model for conditional multivariate extremes which can handle a wide range of dependence structures. Our contribution is twofold: extensions and improvements of this approach and its model inference for high-dimensional data and a novel framework for deriving aggregates addressing edge effects and subregions without rain. We apply our modelling approach to gridded East Anglia, UK precipitation data. Return-level curves for spatial aggregates over different regions of various sizes are estimated and shown to fit very well to the data.},
  archive      = {J_AOAS},
  author       = {Jordan Richards and Jonathan A. Tawn and Simon Brown},
  doi          = {10.1214/22-AOAS1609},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2693-2713},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modelling extremes of spatial aggregates of precipitation using conditional methods},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping interstellar dust with gaussian processes.
<em>AOAS</em>, <em>16</em>(4), 2672–2692. (<a
href="https://doi.org/10.1214/22-AOAS1608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interstellar dust corrupts nearly every stellar observation and accounting for it is crucial to measuring physical properties of stars. We model the dust distribution as a spatially varying latent field with a Gaussian process (GP) and develop a likelihood model and inference method that scales to millions of astronomical observations. Modeling interstellar dust is complicated by two factors. The first is integrated observations. The data come from a vantage point on Earth, and each observation is an integral of the unobserved function along our line of sight, resulting in a complex likelihood and a more difficult inference problem than in classical GP inference. The second complication is scale; stellar catalogs have millions of observations. To address these challenges, we develop ziggy, a scalable approach to GP inference with integrated observations based on stochastic variational inference. We study ziggy on synthetic data and the Ananke dataset, a high-fidelity mechanistic model of the Milky Way with millions of stars. ziggy reliably infers the spatial dust map with well-calibrated posterior uncertainties.},
  archive      = {J_AOAS},
  author       = {Andrew C. Miller and Lauren Anderson and Boris Leistedt and John P. Cunningham and David W. Hogg and David M. Blei},
  doi          = {10.1214/22-AOAS1608},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2672-2692},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mapping interstellar dust with gaussian processes},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing for differential abundance in compositional counts
data, with application to microbiome studies. <em>AOAS</em>,
<em>16</em>(4), 2648–2671. (<a
href="https://doi.org/10.1214/22-AOAS1607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying which taxa in our microbiota are associated with traits of interest is important for advancing science and health. However, the identification is challenging because the measured vector of taxa counts (by amplicon sequencing) is compositional, so a change in the abundance of one taxon in the microbiota induces a change in the number of sequenced counts across all taxa. The data are typically sparse, with many zero counts present either due to biological variance or limited sequencing depth. We examine the case of Crohn’s disease, where the microbial load changes substantially with the disease. For this representative example of a highly compositional setting, we show existing methods designed to identify differentially abundant taxa may have an inflated number of false positives. We introduce a novel nonparametric approach that provides valid inference, even when the fraction of zero counts is substantial. Our approach uses a set of reference taxa that are nondifferentially abundant which can be estimated from the data or from outside information. Our approach also allows for a novel type of testing: multivariate tests of differential abundance over a focused subset of the taxa. Genera-level multivariate testing discovers additional genera as differentially abundant by avoiding agglomeration of taxa.},
  archive      = {J_AOAS},
  author       = {Barak Brill and Amnon Amir and Ruth Heller},
  doi          = {10.1214/22-AOAS1607},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2648-2671},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Testing for differential abundance in compositional counts data, with application to microbiome studies},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference for brain activity from functional
magnetic resonance imaging collected at two spatial resolutions.
<em>AOAS</em>, <em>16</em>(4), 2626–2647. (<a
href="https://doi.org/10.1214/22-AOAS1606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroradiologists and neurosurgeons increasingly opt to use functional magnetic resonance imaging (fMRI) to map functionally relevant brain regions for noninvasive presurgical planning and intraoperative neuronavigation. This application requires a high degree of spatial accuracy, but the fMRI signal-to-noise ratio (SNR) decreases as spatial resolution increases. In practice, fMRI scans can be collected at multiple spatial resolutions, and it is of interest to make more accurate inference on brain activity by combining data with different resolutions. To this end, we develop a new Bayesian model to leverage both better anatomical precision in high resolution fMRI and higher SNR in standard resolution fMRI. We assign a Gaussian process prior to the mean intensity function and develop an efficient, scalable posterior computation algorithm to integrate both sources of data. We draw posterior samples using an algorithm analogous to Riemann manifold Hamiltonian Monte Carlo in an expanded parameter space. We illustrate our method in analysis of presurgical fMRI data and show in simulation that it infers the mean intensity more accurately than alternatives that use either the high or standard resolution fMRI data alone.},
  archive      = {J_AOAS},
  author       = {Andrew S. Whiteman and Andreas J. Bartsch and Jian Kang and Timothy D. Johnson},
  doi          = {10.1214/22-AOAS1606},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2626-2647},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian inference for brain activity from functional magnetic resonance imaging collected at two spatial resolutions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical bayesian modeling of ocean heat content and its
uncertainty. <em>AOAS</em>, <em>16</em>(4), 2603–2625. (<a
href="https://doi.org/10.1214/22-AOAS1605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate quantification of changes in the heat content of the world’s oceans is crucial for our understanding of the effects of increasing greenhouse gas concentrations. The Argo program, consisting of Lagrangian floats that measure vertical temperature profiles throughout the global ocean, has provided a wealth of data from which to estimate ocean heat content. However, creating a globally consistent statistical model for ocean heat content remains challenging due to the need for a globally valid covariance model that can capture complex nonstationarity. In this paper, we develop a hierarchical Bayesian Gaussian process model that uses kernel convolutions with cylindrical distances to allow for spatial nonstationarity in all model parameters while using a Vecchia process to remain computationally feasible for large spatial datasets. Our approach can produce valid credible intervals for globally integrated quantities that would not be possible using previous approaches. These advantages are demonstrated through the application of the model to Argo data, yielding credible intervals for the spatially varying trend in ocean heat content that accounts for both the uncertainty induced from interpolation and from estimating the mean field and other parameters. Through cross-validation, we show that our model outperforms an out-of-the-box approach as well as other simpler models. The code for performing this analysis is provided as the R package BayesianOHC.},
  archive      = {J_AOAS},
  author       = {Samuel Baugh and Karen McKinnon},
  doi          = {10.1214/22-AOAS1605},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2603-2625},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hierarchical bayesian modeling of ocean heat content and its uncertainty},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian data synthesis and the utility-risk trade-off for
mixed epidemiological data. <em>AOAS</em>, <em>16</em>(4), 2577–2602.
(<a href="https://doi.org/10.1214/22-AOAS1604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the microdata used for epidemiological studies contain sensitive measurements on real individuals. As a result, such microdata cannot be published out of privacy concerns, and without public access to these data, any statistical analyses originally published on them are nearly impossible to reproduce. To promote the dissemination of key datasets for analysis without jeopardizing the privacy of individuals, we introduce a cohesive Bayesian framework for the generation of fully synthetic high-dimensional microdatasets of mixed categorical, binary, count, and continuous variables. This process centers around a joint Bayesian model that is simultaneously compatible with all of these data types, enabling the creation of mixed synthetic datasets through posterior predictive sampling. Furthermore, a focal point of epidemiological data analysis is the study of conditional relationships between various exposures and key outcome variables through regression analysis. We design a modified data synthesis strategy to target and preserve these conditional relationships, including both nonlinearities and interactions. The proposed techniques are deployed to create a synthetic version of a confidential dataset containing dozens of health, cognitive, and social measurements on nearly 20,000 North Carolina children.},
  archive      = {J_AOAS},
  author       = {Joseph Feldman and Daniel R. Kowal},
  doi          = {10.1214/22-AOAS1604},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2577-2602},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian data synthesis and the utility-risk trade-off for mixed epidemiological data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised nonparametric bayesian modelling of spatial
proteomics. <em>AOAS</em>, <em>16</em>(4), 2554–2576. (<a
href="https://doi.org/10.1214/22-AOAS1603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding subcellular protein localisation is an essential component in the analysis of context specific protein function. Recent advances in quantitative mass-spectrometry (MS) have led to high-resolution mapping of thousands of proteins to subcellular locations within the cell. Novel modelling considerations to capture the complex nature of these data are thus necessary. We approach analysis of spatial proteomics data in a nonparametric Bayesian framework, using K-component mixtures of Gaussian process regression models. The Gaussian process regression model accounts for correlation structure within a subcellular niche, with each mixture component capturing the distinct correlation structure observed within each niche. The availability of marker proteins (i.e., proteins with a priori known labelled locations) motivates a semi-supervised learning approach to inform the Gaussian process hyperparameters. We moreover provide an efficient Hamiltonian-within-Gibbs sampler for our model. Furthermore, we reduce the computational burden associated with inversion of covariance matrices by exploiting the structure in the covariance matrix. A tensor decomposition of our covariance matrices allows extended Trench and Durbin algorithms to be applied to reduce the computational complexity of inversion and hence accelerate computation. We provide detailed case-studies on Drosophila embryos and mouse pluripotent embryonic stem cells to illustrate the benefit of semi-supervised functional Bayesian modelling of the data.},
  archive      = {J_AOAS},
  author       = {Oliver M. Crook and Kathryn S. Lilley and Laurent Gatto and Paul D. W. Kirk},
  doi          = {10.1214/22-AOAS1603},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2554-2576},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semi-supervised nonparametric bayesian modelling of spatial proteomics},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering and forecasting multiple functional time series.
<em>AOAS</em>, <em>16</em>(4), 2523–2553. (<a
href="https://doi.org/10.1214/22-AOAS1602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and forecasting homogeneous age-specific mortality rates of multiple countries could lead to improvements in long-term forecasting. Data fed into joint models are often grouped according to nominal attributes, such as geographic regions, ethnic groups, and socioeconomic status, which may still contain heterogeneity and deteriorate the forecast results. Our paper proposes a novel clustering technique to pursue homogeneity among multiple functional time series, based on functional panel data modeling, to address this issue. Using a functional panel data model with fixed effects, we can extract common functional time series features. These common features could be decomposed into two components: the functional time trend and the mode of variations of functions (functional pattern). The functional time trend reflects the dynamics across time, while the functional pattern captures the fluctuations within curves. The proposed clustering method searches for homogeneous age-specific mortality rates of multiple countries by accounting for both the modes of variations and the temporal dynamics among curves. We demonstrate that the proposed clustering technique outperforms other existing methods through a Monte Carlo simulation and could handle complicated cases with slow decaying eigenvalues. In empirical data analysis we find that the clustering results of age-specific mortality rates can be explained by the combination of geographic region, ethnic groups, and socioeconomic status. We further show that our model produces more accurate forecasts than several benchmark methods in forecasting age-specific mortality rates.},
  archive      = {J_AOAS},
  author       = {Chen Tang and Han Lin Shang and Yanrong Yang},
  doi          = {10.1214/22-AOAS1602},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2523-2553},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Clustering and forecasting multiple functional time series},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating functional parameters for understanding the
impact of weather and government interventions on COVID-19 outbreak.
<em>AOAS</em>, <em>16</em>(4), 2505–2522. (<a
href="https://doi.org/10.1214/22-AOAS1601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the coronavirus disease 2019 (COVID-19) has shown profound effects on public health and the economy worldwide, it becomes crucial to assess the impact on the virus transmission and develop effective strategies to address the challenge. A new statistical model, derived from the SIR epidemic model with functional parameters, is proposed to understand the impact of weather and government interventions on the virus spread in the presence of asymptomatic infections among eight metropolitan areas in the United States. The model uses Bayesian inference with Gaussian process priors to study the functional parameters nonparametrically, and sensitivity analysis is adopted to investigate the main and interaction effects of these factors. This analysis reveals several important results, including the potential interaction effects between weather and government interventions, which shed new light on the effective strategies for policymakers to mitigate the COVID-19 outbreak.},
  archive      = {J_AOAS},
  author       = {Chih-Li Sung},
  doi          = {10.1214/22-AOAS1601},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2505-2522},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating functional parameters for understanding the impact of weather and government interventions on COVID-19 outbreak},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian hierarchical random-effects meta-analysis and
design of phase i clinical trials. <em>AOAS</em>, <em>16</em>(4),
2481–2504. (<a href="https://doi.org/10.1214/22-AOAS1600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a curve-free random-effects meta-analysis approach to combining data from multiple phase I clinical trials to identify an optimal dose. Our method accounts for between-study heterogeneity that may stem from different study designs, patient populations, or tumor types. We also develop a meta-analytic-predictive (MAP) method, based on a power prior, that incorporates data from multiple historical studies into the design and conduct of a new phase I trial. Performances of the proposed methods for data analysis and trial design are evaluated by extensive simulation studies. The proposed random-effects meta-analysis method provides more reliable dose selection than comparators that rely on parametric assumptions. The MAP-based dose-finding designs are generally more efficient than those that do not borrow information, especially when the current and historical studies are similar. The proposed methodologies are illustrated by a meta-analysis of five historical phase I studies of Sorafenib and design of a new phase I trial.},
  archive      = {J_AOAS},
  author       = {Ruitao Lin and Haolun Shi and Guosheng Yin and Peter F. Thall and Ying Yuan and Christopher R. Flowers},
  doi          = {10.1214/22-AOAS1600},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2481-2504},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian hierarchical random-effects meta-analysis and design of phase i clinical trials},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference for the effect of mobility on COVID-19
deaths. <em>AOAS</em>, <em>16</em>(4), 2458–2480. (<a
href="https://doi.org/10.1214/22-AOAS1599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop statistical methods for causal inference in epidemics. Our focus is in estimating the effect of social mobility on deaths in the first year of the Covid-19 pandemic. We propose a marginal structural model motivated by a basic epidemic model. We estimate the counterfactual time series of deaths under interventions on mobility. We conduct several types of sensitivity analyses. We find that the data support the idea that reduced mobility causes reduced deaths, but the conclusion comes with caveats. There is evidence of sensitivity to model misspecification and unmeasured confounding which implies that the size of the causal effect needs to be interpreted with caution. While there is little doubt the effect is real, our work highlights the challenges in drawing causal inferences from pandemic data.},
  archive      = {J_AOAS},
  author       = {Matteo Bonvini and Edward H. Kennedy and Valerie Ventura and Larry Wasserman},
  doi          = {10.1214/22-AOAS1599},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2458-2480},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Causal inference for the effect of mobility on COVID-19 deaths},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Phylogenetically informed bayesian truncated copula
graphical models for microbial association networks. <em>AOAS</em>,
<em>16</em>(4), 2437–2457. (<a
href="https://doi.org/10.1214/21-AOAS1598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microorganisms play critical roles in host health. The advancement of high-throughput sequencing technology provides opportunities for a deeper understanding of microbial interactions. However, due to the technological limitations of 16S ribosomal RNA sequencing, microbiome data are zero-inflated, and a quantitative comparison of microbial abundances cannot be made across subjects. By leveraging a recent microbiome profiling technique that quantifies 16S ribosomal RNA microbial counts, we propose a novel Bayesian graphical model that incorporates microorganisms’ evolutionary history through a phylogenetic tree prior and explicitly accounts for zero inflation using the truncated Gaussian copula. Our simulation study reveals that the evolutionary information substantially improves the network estimation accuracy. We apply the proposed model to the quantitative gut microbiome data of 106 healthy subjects and identify three distinct microbial communities that are not found by existing microbial network estimation models. We further find that these communities are discriminated based on microorganisms’ ability to utilize oxygen as an energy source.},
  archive      = {J_AOAS},
  author       = {Hee Cheol Chung and Irina Gaynanova and Yang Ni},
  doi          = {10.1214/21-AOAS1598},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2437-2457},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Phylogenetically informed bayesian truncated copula graphical models for microbial association networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parsimonious bayesian factor analysis for modelling latent
structures in spectroscopy data. <em>AOAS</em>, <em>16</em>(4),
2417–2436. (<a href="https://doi.org/10.1214/21-AOAS1597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, within the dairy sector, animal diet and management practices have been receiving increased attention, in particular, examining the impact of pasture-based feeding strategies on the composition and quality of milk and dairy products in line with the prevalence of premium grass-fed dairy products appearing on market shelves. To date, methods to thoroughly investigate the more relevant differences induced by the diet on milk chemical features are limited; enhanced statistical tools exploring these differences are required. Infrared spectroscopy techniques are widely used to collect data on milk samples and to predict milk related traits and characteristics. While these data are routinely used to predict the composition of the macro components of milk, each spectrum also provides a reservoir of unharnessed information about the sample. The accumulation and subsequent interpretation of these data present some challenges due to their high-dimensionality and the relationships amongst the spectral variables. In this work, directly motivated by a dairy application, we propose a modification of the standard factor analysis to induce a parsimonious summary of spectroscopic data. Our proposal maps the observations into a low-dimensional latent space while simultaneously clustering the observed variables. The method indicates possible redundancies in the data, and it helps disentangle the complex relationships among the wavelengths. A flexible Bayesian estimation procedure is proposed for model fitting, providing reasonable values for the number of latent factors and clusters. The method is applied on milk mid-infrared (MIR) spectroscopy data from dairy cows on distinctly different pasture and nonpasture based diets, providing accurate modelling of the correlation, clustering of variables, and information on differences among milk samples from cows on different diets.},
  archive      = {J_AOAS},
  author       = {Alessandro Casa and Tom F. O’Callaghan and Thomas Brendan Murphy},
  doi          = {10.1214/21-AOAS1597},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2417-2436},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Parsimonious bayesian factor analysis for modelling latent structures in spectroscopy data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-sample tests for multivariate repeated measurements of
histogram objects with applications to wearable device data.
<em>AOAS</em>, <em>16</em>(4), 2396–2416. (<a
href="https://doi.org/10.1214/21-AOAS1596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repeated observations have become increasingly common in biomedical research and longitudinal studies. For instance, wearable sensor devices are deployed to continuously track physiological and biological signals from each individual over multiple days. It remains of great interest to appropriately evaluate how the daily distribution of biosignals might differ across disease groups and demographics. Hence, these data could be formulated as multivariate complex object data, such as probability densities, histograms, and observations on a tree. Traditional statistical methods would often fail to apply, as they are sampled from an arbitrary non-Euclidean metric space. In this paper we propose novel, nonparametric, graph-based two-sample tests for object data with the same structure of repeated measures. We treat the repeatedly measured object data as multivariate object data, which requires the same number of repeated observations per individual but eliminates any assumptions on the errors of the repeated observations. A set of test statistics are proposed to capture various possible alternatives. We derive their asymptotic null distributions under the permutation null. These tests exhibit substantial power improvements over the existing methods while controlling the type I errors under finite samples as shown through simulation studies. The proposed tests are demonstrated to provide additional insights on the location, inter- and intra-individual variability of the daily physical activity distributions in a sample of studies for mood disorders.},
  archive      = {J_AOAS},
  author       = {Jingru Zhang and Kathleen R. Merikangas and Hongzhe Li and Haochang Shou},
  doi          = {10.1214/21-AOAS1596},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2396-2416},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Two-sample tests for multivariate repeated measurements of histogram objects with applications to wearable device data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended stochastic block models with application to
criminal networks. <em>AOAS</em>, <em>16</em>(4), 2369–2395. (<a
href="https://doi.org/10.1214/21-AOAS1595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliably learning group structures among nodes in network data is challenging in several applications. We are particularly motivated by studying covert networks that encode relationships among criminals. These data are subject to measurement errors, and exhibit a complex combination of an unknown number of core-periphery, assortative and disassortative structures that may unveil key architectures of the criminal organization. The coexistence of these noisy block patterns limits the reliability of routinely-used community detection algorithms, and requires extensions of model-based solutions to realistically characterize the node partition process, incorporate information from node attributes, and provide improved strategies for estimation and uncertainty quantification. To cover these gaps, we develop a new class of extended stochastic block models (esbm) that infer groups of nodes having common connectivity patterns via Gibbs-type priors on the partition process. This choice encompasses many realistic priors for criminal networks, covering solutions with fixed, random and infinite number of possible groups, and facilitates the inclusion of node attributes in a principled manner. Among the new alternatives in our class, we focus on the Gnedin process as a realistic prior that allows the number of groups to be finite, random and subject to a reinforcement process coherent with criminal networks. A collapsed Gibbs sampler is proposed for the whole esbm class, and refined strategies for estimation, prediction, uncertainty quantification and model selection are outlined. The esbm performance is illustrated in realistic simulations and in an application to an Italian mafia network, where we unveil key complex block structures, mostly hidden from state-of-the-art alternatives.},
  archive      = {J_AOAS},
  author       = {Sirio Legramanti and Tommaso Rigon and Daniele Durante and David B. Dunson},
  doi          = {10.1214/21-AOAS1595},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2369-2395},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Extended stochastic block models with application to criminal networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Full bayesian inference in hidden markov models of plant
growth. <em>AOAS</em>, <em>16</em>(4), 2352–2368. (<a
href="https://doi.org/10.1214/21-AOAS1594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately modeling the growth process of plants in interaction with their environment is important for predicting their biophysical characteristics, referred to as phenotype prediction. Most models are described by discrete dynamic systems in general state-space representation with important domain-specific characteristics: First, plant model parameters have usually clear functional meanings and may be of genetic origins, thus necessitating a precise estimation. Second, critical growth variables, specifically biomass production and dynamic allocation to organs, are hidden variables not accessible to measure. Finally, the difficulty to assess the local plant environment may imply the introduction of process noises in models. Therefore, a precise understanding of the system’s behavior requires the joint estimation of functional parameters, hidden states, and noise parameters. In this paper we describe how a full Bayesian method of estimation can accurately estimate all these key model variables using Markov chain Monte Carlo (MCMC) techniques. In the presence of both process and observation noises, it requires to use adequate particle MCMC (PMCMC) algorithms to efficiently sample the hidden states which, consequently, allows for a precise estimation of all noise parameters involved. Thanks to the Bayesian framework, appropriate choices of prior distributions for the noise parameters have enabled analytical posterior distributions and only simple updates are required. Furthermore, this estimation strategy can be easily generalized and adapted to different types of plant growth models, such as organ-scale or compartmental, provided that they are formulated as hidden Markov models. Our estimation method improves on those classically used in plant growth modeling in several aspects: First, by building upon a general probabilistic framework the estimation results allow proper statistical analyses. It is useful in prediction, no only for uncertainty and risk analysis (e.g., for crop yield prediction) but also to analyze the results of experimental trials, for example, to compare genotypes in breeding. Moreover, the care taken in the estimation of hidden variables opens new perspectives in the understanding of inner growth processes, notably the balance and interaction between biomass production and allocation (referred to as source-sink dynamics). Applications of this estimation procedure are demonstrated on the GreenLab model for Arabidopsis thaliana and the Log-Normal Allocation and Senescence (LNAS) model for sugar beet, on both synthetic and real data.},
  archive      = {J_AOAS},
  author       = {Gautier Viaud and Yuting Chen and Paul-Henry Cournède},
  doi          = {10.1214/21-AOAS1594},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2352-2368},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Full bayesian inference in hidden markov models of plant growth},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How many refugees and migrants died trying to reach europe?
Joint population size and total estimation. <em>AOAS</em>,
<em>16</em>(4), 2339–2351. (<a
href="https://doi.org/10.1214/21-AOAS1593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We estimate the number of migrants and refugees that died while trying to enter the European Union, during a period of 25 years. Only a subset of attempts with at least one casualty are reported by at least one media source. In order to obtain the estimate, we propose a regression-extrapolation approach, for joint estimation of population size (here, the number of deadly individual or group attempts) and the sum of an accompanying trait (here, the number of deaths) over the population. The trait is measured only for a biased sample of individuals, that are repeatedly observed. Closed-form expressions are derived for the estimator and its standard error. Our findings are that about 40,000 have died from January 1993 to March 2019, during about 5500 attempts to enter the European Union. The number of deaths has been steadily increasing over time, and so has the number of deaths per attempt. About 20\% of attempts with at least one casualty have not been recorded by any media source, and slightly less than 10\% of deaths have thus been overlooked by media.},
  archive      = {J_AOAS},
  author       = {Alessio Farcomeni},
  doi          = {10.1214/21-AOAS1593},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2339-2351},
  shortjournal = {Ann. Appl. Stat.},
  title        = {How many refugees and migrants died trying to reach europe? joint population size and total estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel time-series models for small area estimation at
different frequencies and domain levels. <em>AOAS</em>, <em>16</em>(4),
2314–2338. (<a href="https://doi.org/10.1214/21-AOAS1592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A small area estimation method is developed for repeatedly conducted multipurpose surveys. A multilevel time-series model is proposed that uses direct estimates for the most detailed domains observed at the highest frequency of the repeated survey. A consistent set of estimates at different aggregation levels is then derived by aggregation of the model-based predictions obtained for the most detailed domains observed at the highest frequency. The model borrows strength over time and space via smooth and local level trends at different aggregation levels. The model also borrows information from auxiliary series available from registers with coefficients that can vary over both domains and time. Regional domain random effects are allowed to vary smoothly over space according to a spatial autoregressive process. To account for the diversity of domains and for more volatile time-dependence, nonnormally distributed random effects and trend innovations are used via so-called global-local shrinkage priors. A Bayesian approach is taken, and the model is estimated by MCMC simulation. The method is illustrated with an application to the Dutch Labour Force Survey to produce monthly provincial and quarterly municipal unemployment figures.},
  archive      = {J_AOAS},
  author       = {Harm Jan Boonstra and Jan van den Brakel},
  doi          = {10.1214/21-AOAS1592},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2314-2338},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multilevel time-series models for small area estimation at different frequencies and domain levels},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal satellite data imputation using sparse
functional data analysis. <em>AOAS</em>, <em>16</em>(4), 2291–2313. (<a
href="https://doi.org/10.1214/21-AOAS1591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific applications and signal processing algorithms require complete satellite images. However, missing data in satellite images is very common due to various reasons such as cloud cover and sensor-specific problems. This paper introduces a general spatiotemporal satellite image imputation method based on sparse functional data analytic techniques. To handle observations consisting of a few longitudinally repeated satellite images that are themselves partially observed and noise-contaminated, we propose a multistep imputation method by following the best linear unbiased prediction principle and pooling information across all available locations and time points. Theoretical properties are established for the proposed approach under a new observation model for functional data that covers the dataset in question as a special case. Practical analysis on the Landsat data are conducted to illustrate and validate our algorithm which also shows that the proposed method considerably outperforms existing algorithms in terms of prediction accuracy. An efficient implementation using R and Rcpp is made available in the R package stfit.},
  archive      = {J_AOAS},
  author       = {Weicheng Zhu and Zhengyuan Zhu and Xiongtao Dai},
  doi          = {10.1214/21-AOAS1591},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2291-2313},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatiotemporal satellite data imputation using sparse functional data analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assessing treatment effect through compliance score in
randomized trials with noncompliance. <em>AOAS</em>, <em>16</em>(4),
2279–2290. (<a href="https://doi.org/10.1214/21-AOAS1590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A randomized trial is the gold standard for assessing the benefit of a treatment versus a control. When noncompliance is present, treatment effect depends on the tendency to comply—an attribute that is not directly measurable. Though the principal causal effect has been the most important for handling noncompliance, it is not immediately applicable to clinical decision-making as it targets the average effect in the latent strata of potential compliance. In this work, we propose the concept of compliance score, a linear combination of baseline characteristics, that uncovers the inherent attribute of compliance. We then assess the heterogeneous causal effect, namely, the causal effect of treatment as a function of baseline characteristics through the compliance score. A pseudo-response, along with a nonparametric estimation procedure, is proposed to ensure consistent and optimally efficient estimation. Compare to principal causal effect, the proposed effect is actionable and allows prediction of treatment effect at individual level. This work is motivated by and applied to a clinical trial to evaluate the benefit of antiretroviral regimens in HIV-infected patients.},
  archive      = {J_AOAS},
  author       = {Zonghui Hu and Zhiwei Zhang and Dean Follmann},
  doi          = {10.1214/21-AOAS1590},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2279-2290},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing treatment effect through compliance score in randomized trials with noncompliance},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An omnibus test for detection of subgroup treatment effects
via data partitioning. <em>AOAS</em>, <em>16</em>(4), 2266–2278. (<a
href="https://doi.org/10.1214/21-AOAS1589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Late-stage clinical trials have been conducted primarily to establish the efficacy of a new treatment in an intended population. A corollary of population heterogeneity in clinical trials is that a treatment might be effective for one or more subgroups, rather than for the whole population of interest. As an example, the phase III clinical trial of panitumumab in metastatic colorectal cancer patients failed to demonstrate its efficacy in the overall population, but a subgroup associated with tumor KRAS status was found to be promising (Peeters et al. (Am. J. Clin. Oncol. 28 (2010) 4706–4713)). As we search for such subgroups via data partitioning based on a large number of biomarkers, we need to guard against inflated type I error rates due to multiple testing. Commonly-used multiplicity adjustments tend to lose power for the detection of subgroup treatment effects. We develop an effective omnibus test to detect the existence of, at least, one subgroup treatment effect, allowing a large number of possible subgroups to be considered and possibly censored outcomes. Applied to the panitumumab trial data, the proposed test would confirm a significant subgroup treatment effect. Empirical studies also show that the proposed test is applicable to a variety of outcome variables and maintains robust statistical power.},
  archive      = {J_AOAS},
  author       = {Yifei Sun and Xuming He and Jianhua Hu},
  doi          = {10.1214/21-AOAS1589},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2266-2278},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An omnibus test for detection of subgroup treatment effects via data partitioning},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-adaptive efficient estimation strategies for biomarker
studies embedded in randomized trials. <em>AOAS</em>, <em>16</em>(4),
2250–2265. (<a href="https://doi.org/10.1214/21-AOAS1588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive and prognostic biomarkers are increasingly important in clinical research and practice. Biomarker studies are frequently embedded in randomized clinical trials with biospecimens collected at baseline and assayed for biomarkers, either in real time or retrospectively. This article proposes efficient estimation strategies for two study settings in terms of biomarker ascertainment: a complete-data setting in which the biomarker is measured for all subjects in the trial, and a two-phase sampling design in which the biomarker is measured retrospectively for a random subsample of subjects selected in an outcome-dependent fashion. In both settings, efficient estimating functions are characterized using semiparametric theory and approximated using data-adaptive machine learning methods, leading to estimators that are consistent, asymptotically normal and (approximately) efficient under general conditions. The proposed methods are evaluated in simulation studies and applied to real data from two biomarker studies, one in each setting.},
  archive      = {J_AOAS},
  author       = {Wei Zhang and Zhiwei Zhang and James F. Troendle and Aiyi Liu},
  doi          = {10.1214/21-AOAS1588},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2250-2265},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Data-adaptive efficient estimation strategies for biomarker studies embedded in randomized trials},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian multivariate sparse functional principal components
analysis with application to longitudinal microbiome multiomics data.
<em>AOAS</em>, <em>16</em>(4), 2231–2249. (<a
href="https://doi.org/10.1214/21-AOAS1587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbiome researchers often need to model the temporal dynamics of multiple complex, nonlinear outcome trajectories simultaneously. This motivates our development of multivariate Sparse Functional Principal Components Analysis (mSFPCA), extending existing SFPCA methods to simultaneously characterize multiple temporal trajectories and their interrelationships. As with existing SFPCA methods, the mSFPCA algorithm characterizes each trajectory as a smooth mean plus a weighted combination of the smooth major modes of variation about the mean, where the weights are given by the component scores for each subject. Unlike existing SFPCA methods, the mSFPCA algorithm allows estimation of multiple trajectories simultaneously, such that the component scores, which are constrained to be independent within a particular outcome for identifiability, may be arbitrarily correlated with component scores for other outcomes. A Cholesky decomposition is used to estimate the component score covariance matrix efficiently and guarantee positive semidefiniteness given these constraints. Mutual information is used to assess the strength of marginal and conditional temporal associations across outcome trajectories. Importantly, we implement mSFPCA as a Bayesian algorithm using R and stan, enabling easy use of packages such as PSIS-LOO for model selection and graphical posterior predictive checks to assess the validity of mSFPCA models. Although we focus on application of mSFPCA to microbiome data in this paper, the mSFPCA model is of general utility and can be used in a wide range of real-world applications.},
  archive      = {J_AOAS},
  author       = {Lingjing Jiang and Chris Elrod and Jane J. Kim and Austin D. Swafford and Rob Knight and Wesley K. Thompson},
  doi          = {10.1214/21-AOAS1587},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2231-2249},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian multivariate sparse functional principal components analysis with application to longitudinal microbiome multiomics data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accounting for survey design in bayesian disaggregation of
survey-based areal estimates of proportions: An application to the
american community survey. <em>AOAS</em>, <em>16</em>(4), 2201–2230. (<a
href="https://doi.org/10.1214/21-AOAS1585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the effects of social determinants of health on health outcomes requires data on characteristics of the neighborhoods in which subjects live. However, estimates of these characteristics are often aggregated over space and time in a fashion that diminishes their utility. Take, for example, estimates from the American Community Survey (ACS), a multiyear nationwide survey administered by the U.S. Census Bureau: estimates for small municipal areas are aggregated over 5-year periods, whereas 1-year estimates are only available for municipal areas with populations &gt;65,000. Researchers may wish to use ACS estimates in studies of population health to characterize neighborhood-level exposures. However, 5-year estimates may not properly characterize temporal changes or align temporally with other data in the study, while the coarse spatial resolution of the 1-year estimates diminishes their utility in characterizing neighborhood exposure. To circumvent this issue, in this paper we propose a modeling framework to disaggregate estimates of proportions derived from sampling surveys, which explicitly accounts for the survey design effect. We illustrate the utility of our model by applying it to the ACS data, generating estimates of poverty for the state of Michigan at fine spatiotemporal resolution.},
  archive      = {J_AOAS},
  author       = {Marco H. Benedetti and Veronica J. Berrocal and Roderick J. Little},
  doi          = {10.1214/21-AOAS1585},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2201-2230},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for survey design in bayesian disaggregation of survey-based areal estimates of proportions: An application to the american community survey},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating heterogeneous gene regulatory networks from
zero-inflated single-cell expression data. <em>AOAS</em>,
<em>16</em>(4), 2183–2200. (<a
href="https://doi.org/10.1214/21-AOAS1582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring gene regulatory networks can elucidate how genes work cooperatively. The gene-gene collaboration information is often learned by Gaussian graphical models (GGM) that aim to identify whether the expression levels of any pair of genes are dependent, given other genes’ expression values. One basic assumption that guarantees the validity of GGM is data normality, and this often holds for bulk-level expression data which aggregate biological signals from a collection of cells. However, fine-grained cell-level expression profiles collected in single-cell RNA-sequencing (scRNA-seq) reveal nonnormality features—cellular heterogeneity and zero inflation. We propose a Bayesian latent mixture GGM to jointly estimate multiple gene regulatory networks accounting for the zero inflation and unknown heterogeneity of single-cell expression data. The proposed approach outperforms competing methods on synthetic data in terms of network structure and precision matrix estimation accuracy and provides biological insights when applied to two real-world scRNA-seq datasets. An R package implementing the proposed model is available on GitHub https://github.com/WgitU/BLGGM.},
  archive      = {J_AOAS},
  author       = {Qiuyu Wu and Xiangyu Luo},
  doi          = {10.1214/21-AOAS1582},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2183-2200},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating heterogeneous gene regulatory networks from zero-inflated single-cell expression data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network differential connectivity analysis. <em>AOAS</em>,
<em>16</em>(4), 2166–2182. (<a
href="https://doi.org/10.1214/21-AOAS1581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying differences in networks has become a canonical problem in many biological applications. Existing methods try to accomplish this goal by either directly comparing the estimated structures of two networks or testing the null hypothesis that the covariance or inverse covariance matrices in two populations are identical. However, estimation approaches do not provide measures of uncertainty, for example, p-values, whereas existing testing approaches could lead to misleading results, as we illustrate in this paper. To address these shortcomings, we propose a qualitative hypothesis testing framework which tests whether the connectivity structures in the two networks are the same. Our framework is especially appropriate if the goal is to identify nodes or edges that are differentially connected. No existing approach could test such hypotheses and provide corresponding measures of uncertainty. Theoretically, we show that, under appropriate conditions, our proposal correctly controls the type-I error rate in testing the qualitative hypothesis. Empirically, we demonstrate the performance of our proposal using simulation studies and applications in cancer genomics.},
  archive      = {J_AOAS},
  author       = {Sen Zhao and Ali Shojaie},
  doi          = {10.1214/21-AOAS1581},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2166-2182},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Network differential connectivity analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical resampling for bagging in multistudy prediction
with applications to human neurochemical sensing. <em>AOAS</em>,
<em>16</em>(4), 2145–2165. (<a
href="https://doi.org/10.1214/21-AOAS1574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the “study strap ensemble,” which combines advantages of two common approaches to fitting prediction models when multiple training datasets (“studies”) are available: pooling studies and fitting one model vs. averaging predictions from multiple models each fit to individual studies. The study strap ensemble fits models to bootstrapped datasets or “pseudo-studies.” These are generated by resampling from multiple studies with a hierarchical resampling scheme that generalizes the randomized cluster bootstrap. The study strap is controlled by a tuning parameter that determines the proportion of observations to draw from each study. When the parameter is set to its lowest value, each pseudo-study is resampled from only a single study. When it is high, the study strap ignores the multistudy structure and generates pseudo-studies by merging the datasets and drawing observations like a standard bootstrap. We empirically show the optimal tuning value often lies in between and prove that special cases of the study strap draw the merged dataset and the set of original studies as pseudo-studies. We extend the study strap approach with an ensemble weighting scheme that utilizes information in the distribution of the covariates of the test dataset. Our work is motivated by neuroscience experiments using real-time neurochemical sensing during awake behavior in humans. Current techniques to perform this kind of research require measurements from an electrode placed in the brain during awake neurosurgery and rely on prediction models to estimate neurotransmitter concentrations from the electrical measurements recorded by the electrode. These models are trained by combining multiple datasets that are collected in vitro under heterogeneous conditions in order to promote accuracy of the models when applied to data collected in the brain. A prevailing challenge is deciding how to combine studies or ensemble models trained on different studies to enhance model generalizability. Our methods produce marked improvements in simulations and in this application. All methods are available in the studyStrap CRAN package.},
  archive      = {J_AOAS},
  author       = {Gabriel Loewinger and Prasad Patil and Kenneth T. Kishida and Giovanni Parmigiani},
  doi          = {10.1214/21-AOAS1574},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2145-2165},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hierarchical resampling for bagging in multistudy prediction with applications to human neurochemical sensing},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional random effects modeling of brain shape and
connectivity. <em>AOAS</em>, <em>16</em>(4), 2122–2144. (<a
href="https://doi.org/10.1214/21-AOAS1572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a statistical framework that jointly models brain shape and functional connectivity which are two complex aspects of the brain that have been classically studied independently. We adopt a Riemannian modeling approach to account for the non-Euclidean geometry of the space of shapes and the space of connectivity that constrains trajectories of covariation to be valid statistical estimates. In order to disentangle genetic sources of variability from those driven by unique environmental factors, we embed a functional random effects model in the Riemannian framework. We apply the proposed model to the Human Connectome Project dataset to explore spontaneous co-variation between brain shape and connectivity in young healthy individuals.},
  archive      = {J_AOAS},
  author       = {Eardi Lila and John A. D. Aston},
  doi          = {10.1214/21-AOAS1572},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2122-2144},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Functional random effects modeling of brain shape and connectivity},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the stillbirth rate for 195 countries using a
bayesian sparse regression model with temporal smoothing. <em>AOAS</em>,
<em>16</em>(4), 2101–2121. (<a
href="https://doi.org/10.1214/21-AOAS1571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of stillbirth rates globally is complicated because of the paucity of reliable data from countries where most stillbirths occur. We compiled data and developed a Bayesian hierarchical temporal sparse regression model for estimating stillbirth rates for 195 countries from 2000 to 2019. The model combines covariates with a temporal smoothing process so that estimates are data-driven in country-periods with high-quality data and determined by covariates for country-periods with limited or no data. Horseshoe priors are used to encourage sparseness. The model adjusts observations with alternative stillbirth definitions and accounts for various sources of uncertainty. In-sample goodness of fit and out-of-sample validation results suggest that the model is reasonably well calibrated. The model is used by the UN Interagency Group for Child Mortality Estimation to monitor the stillbirth rate for 195 countries.},
  archive      = {J_AOAS},
  author       = {Zhengfan Wang and Miranda J. Fix and Lucia Hug and Anu Mishra and Danzhen You and Hannah Blencowe and Jon Wakefield and Leontine Alkema},
  doi          = {10.1214/21-AOAS1571},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2101-2121},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the stillbirth rate for 195 countries using a bayesian sparse regression model with temporal smoothing},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric bayesian forecasting of SpatioTemporal
earthquake occurrences. <em>AOAS</em>, <em>16</em>(4), 2083–2100. (<a
href="https://doi.org/10.1214/21-AOAS1554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Epidemic Type Aftershock Sequence (ETAS) model is a self-exciting point process which is used to model and forecast the occurrence of earthquakes in a geographical region. The ETAS model assumes that the occurrence of mainshock earthquakes follows an inhomogeneous spatial point process, with their aftershock earthquakes modelled via a separate triggering kernel. Most previous studies of the ETAS model have relied on point estimates of the model parameters, due to the complexity of the likelihood function and the difficulty in estimating an appropriate spatial mainshock distribution. In order to take estimation uncertainty into account, we instead propose a fully Bayesian formulation of the ETAS model, which uses a nonparametric Dirichlet process mixture prior to capture the spatial mainshock process, and show how efficient parameter inference can be carried out using auxiliary latent variables. We demonstrate how our model can be used for medium-term earthquake forecasts in a number of geographical regions.},
  archive      = {J_AOAS},
  author       = {Gordon J. Ross and Aleksandar A. Kolev},
  doi          = {10.1214/21-AOAS1554},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2083-2100},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric bayesian forecasting of SpatioTemporal earthquake occurrences},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian precision medicine framework for calibrating
individualized therapeutic indices in cancer. <em>AOAS</em>,
<em>16</em>(4), 2055–2082. (<a
href="https://doi.org/10.1214/21-AOAS1550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and clinical implementation of evidence-based precision medicine strategies has become a realistic possibility, primarily due to the rapid accumulation of large-scale genomics and pharmacological data from diverse model systems: patients, cell lines and drug perturbation studies. We introduce a novel Bayesian modeling framework called the individualized theRapeutic index (iRx) model to integrate high-throughput pharmacogenomic data across model systems. Our iRx model achieves three main goals: first, it exploits the conserved biology between patients and cell lines to calibrate therapeutic response of drugs in patients; second, it finds optimal cell line avatars as proxies for patient(s); and finally, it identifies key genomic drivers explaining cell line-patient similarities. This is achieved through a semi-supervised learning approach that conflates (unsupervised) sparse latent factor models with (supervised) penalized regression techniques. We propose a unified and tractable Bayesian model for estimation, and inference is conducted via efficient posterior sampling schemes. We illustrate and validate our approach using two existing clinical trial data sets in multiple myeloma and breast cancer studies. We show that our iRx model improves prediction accuracy compared to naive alternative approaches, and it consistently outperforms existing methods in literature in both multiple simulation scenarios as well as real clinical examples.},
  archive      = {J_AOAS},
  author       = {Abhisek Saha and Min Jin Ha and Satwik Acharyya and Veerabhadran Baladandayuthapani},
  doi          = {10.1214/21-AOAS1550},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2055-2082},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian precision medicine framework for calibrating individualized therapeutic indices in cancer},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling animal movement with directional persistence and
attractive points. <em>AOAS</em>, <em>16</em>(3), 2030–2053. (<a
href="https://doi.org/10.1214/21-AOAS1584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPS technology is currently easily accessible to researchers, and many animal movement data sets are available. Two of the main features that a model which describes an animal’s path can possess are directional persistence and attraction to a point in space. In this work, we propose a new approach that can have both characteristics. Our proposal is a hidden Markov model with a new emission distribution. The emission distribution models the two aforementioned characteristics, while the latent state of the hidden Markov model is needed to account for the behavioral modes. We show that the model is easy to implement in a Bayesian framework. We estimate our proposal on the motivating data that represent GPS locations of a Maremma Sheepdog recorded in Australia. The obtained results are easily interpretable and we show that our proposal outperforms the main competitive model.},
  archive      = {J_AOAS},
  author       = {Gianluca Mastrantonio},
  doi          = {10.1214/21-AOAS1584},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2030-2053},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling animal movement with directional persistence and attractive points},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured hierarchical models for probabilistic inference
from perturbation screening data. <em>AOAS</em>, <em>16</em>(3),
2010–2029. (<a href="https://doi.org/10.1214/21-AOAS1580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic perturbation screening is an experimental method in biology to study cause and effect relationships between different biological entities. However, knocking out or knocking down genes is a highly error-prone process that complicates estimation of the effect sizes of the interventions. Here, we introduce a family of generative models, called the structured hierarchical model (SHM) for probabilistic inference of causal effects from perturbation screens. SHMs utilize classical hierarchical models to represent heterogeneous data and combine them with categorical Markov random fields to encode biological prior information over functionally related biological entities. The random field induces a clustering of functionally related genes which informs inference of parameters in the hierarchical model. The SHM is designed for extremely noisy data sets for which the true data generating process is difficult to model due to lack of domain knowledge or high stochasticity of the interventions. We apply the SHM to a pan-cancer genetic perturbation screen in order to identify genes that restrict the growth of an entire group of cancer cell lines and show that incorporating prior knowledge in the form of a graph improves inference of parameters.},
  archive      = {J_AOAS},
  author       = {Simon Dirmeier and Niko Beerenwinkel},
  doi          = {10.1214/21-AOAS1580},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {2010-2029},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Structured hierarchical models for probabilistic inference from perturbation screening data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous causal effects with imperfect compliance: A
bayesian machine learning approach. <em>AOAS</em>, <em>16</em>(3),
1986–2009. (<a href="https://doi.org/10.1214/21-AOAS1579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or, less stringently, for the false discovery rate) at leaves’ level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students’ performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students’ performance.},
  archive      = {J_AOAS},
  author       = {Falco J. Bargagli-Stoffi and Kristof De Witte and Giorgio Gnecco},
  doi          = {10.1214/21-AOAS1579},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1986-2009},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Heterogeneous causal effects with imperfect compliance: A bayesian machine learning approach},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference for time-varying treatments in latent
markov models: An application to the effects of remittances on poverty
dynamics. <em>AOAS</em>, <em>16</em>(3), 1962–1985. (<a
href="https://doi.org/10.1214/21-AOAS1578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the effectiveness of remittances on the poverty level of recipient households, we propose a causal inference approach that may be applied with longitudinal data and time-varying treatments. The method relies on the integration of a propensity score based technique, the inverse propensity weighting, with a general latent Markov (LM) framework. It is particularly useful when the outcome of interest is a characteristic that is not directly observable, and the analysis is focused on: (i) clustering units in a finite number of classes according to this latent characteristic and (ii) modelling the evolution of this characteristic across time depending on the received treatment. Parameter estimation is based on a two-step procedure. First, individual propensity score weights are computed accounting for predetermined covariates. Then, a weighted version of the standard LM model likelihood, based on such weights, is maximised by means of an expectation-maximisation algorithm or, alternatively, adopting a stepwise procedure. Finite-sample properties of the proposed estimators are studied by simulation. The application is focused on the effect of remittances on the poverty status of Ugandan households, based on a longitudinal survey spanning the period 2009–2014, and where manifest variables are indicators of deprivation. We find that remittances reduce the probability of falling into poverty, whereas they exert no impact on the probability of moving out of poverty.},
  archive      = {J_AOAS},
  author       = {Federico Tullio and Francesco Bartolucci},
  doi          = {10.1214/21-AOAS1578},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1962-1985},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Causal inference for time-varying treatments in latent markov models: An application to the effects of remittances on poverty dynamics},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ice model calibration using semicontinuous spatial data.
<em>AOAS</em>, <em>16</em>(3), 1937–1961. (<a
href="https://doi.org/10.1214/21-AOAS1577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid changes in Earth’s cryosphere caused by human activity can lead to significant environmental impacts. Computer models provide a useful tool for understanding the behavior and projecting the future of Arctic and Antarctic ice sheets. However, these models are typically subject to large parametric uncertainties, due to poorly constrained model input parameters that govern the behavior of simulated ice sheets. Computer model calibration provides a formal statistical framework to infer parameters, using observational data, and to quantify the uncertainty in projections due to the uncertainty in these parameters. Calibration of ice sheet models is often challenging because the relevant model output and observational data take the form of semicontinuous spatial data with a point mass at zero and a right-skewed continuous distribution for positive values. Current calibration approaches cannot handle such data. Here, we introduce a hierarchical latent variable model that handles binary spatial patterns and positive continuous spatial patterns as separate components. To overcome challenges due to high dimensionality, we use likelihood-based generalized principal component analysis to impose low-dimensional structures on the latent variables for spatial dependence. We apply our methodology to calibrate a physical model for the Antarctic ice sheet and demonstrate that we can overcome the aforementioned modeling and computational challenges. As a result of our calibration, we obtain improved future ice-volume change projections.},
  archive      = {J_AOAS},
  author       = {Won Chang and Bledar A. Konomi and Georgios Karagiannis and Yawen Guan and Murali Haran},
  doi          = {10.1214/21-AOAS1577},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1937-1961},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Ice model calibration using semicontinuous spatial data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial functional data modeling of plant reflectances.
<em>AOAS</em>, <em>16</em>(3), 1919–1936. (<a
href="https://doi.org/10.1214/21-AOAS1576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant reflectance spectra, the profile of light reflected by leaves across different wavelengths, supply the spectral signature for a species at a spatial location to enable estimation of functional and taxonomic diversity for plants. We consider leaf spectra as “responses” to be explained spatially. These reflectance spectra are also functions over wavelength that respond to the environment. Our motivating data are gathered for several plant families from the Greater Cape Floristic Region (GCFR) in South Africa and lead us to develop rich novel spatial models that can explain spectra for genera within families. Wavelength responses for an individual leaf are viewed as a function of wavelength, leading to functional data modeling. Local environmental features become covariates. We introduce a wavelength, covariate interaction, since the response to environmental regressors may vary with wavelength, as may variance. Formal spatial modeling enables prediction of reflectances for genera at unobserved locations with known environmental features. We incorporate spatial dependence, wavelength dependence, and space–wavelength interaction (in the spirit of space–time interaction). We implement out-of-sample validation for model selection, finding that the model features above are informative for the functional data analysis. We supply ecological interpretation of the results under the selected model.},
  archive      = {J_AOAS},
  author       = {Philip A. White and Henry Frye and Michael F. Christensen and Alan E. Gelfand and John A. Silander},
  doi          = {10.1214/21-AOAS1576},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1919-1936},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatial functional data modeling of plant reflectances},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale multivariate sparse regression with applications
to UK biobank. <em>AOAS</em>, <em>16</em>(3), 1891–1918. (<a
href="https://doi.org/10.1214/21-AOAS1575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression problems, often a relatively small subset of the features are relevant for predicting the outcome, and methods that impose sparsity on the solution are popular. When multiple correlated outcomes are available (multitask), reduced rank regression is an effective way to borrow strength and capture latent structures that underlie the data. Our proposal is motivated by the UK Biobank population-based cohort study, where we are faced with large-scale, ultrahigh-dimensional features, and have access to a large number of outcomes (phenotypes)—lifestyle measures, biomarkers, and disease outcomes. We are hence led to fit sparse reduced-rank regression models, using computational strategies that allow us to scale to problems of this size. We use a scheme that alternates between solving the sparse regression problem and solving the reduced rank decomposition. For the sparse regression component we propose a scalable iterative algorithm based on adaptive screening that leverages the sparsity assumption and enables us to focus on solving much smaller subproblems. The full solution is reconstructed and tested via an optimality condition to make sure it is a valid solution for the original problem. We further extend the method to cope with practical issues, such as the inclusion of confounding variables and imputation of missing values among the phenotypes. Experiments on both synthetic data and the UK Biobank data demonstrate the effectiveness of the method and the algorithm. We present multiSnpnet package, available at http://github.com/junyangq/multiSnpnet that works on top of PLINK2 files, which we anticipate to be a valuable tool for generating polygenic risk scores from human genetic studies.},
  archive      = {J_AOAS},
  author       = {Junyang Qian and Yosuke Tanigawa and Ruilin Li and Robert Tibshirani and Manuel A. Rivas and Trevor Hastie},
  doi          = {10.1214/21-AOAS1575},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1891-1918},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Large-scale multivariate sparse regression with applications to UK biobank},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of the marginal effect of antidepressants on body
mass index under confounding and endogenous covariate-driven monitoring
times. <em>AOAS</em>, <em>16</em>(3), 1868–1890. (<a
href="https://doi.org/10.1214/21-AOAS1570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studying the marginal effect of antidepressants on body mass index using electronic health records data, we face several challenges. Patients’ characteristics can affect the exposure (confounding) as well as the timing of routine visits (measurement process), and those characteristics may be altered following a visit which can create dependencies between the monitoring and body mass index when viewed as a stochastic or random processes in time. This may result in a form of selection bias that distorts the estimation of the marginal effect of the antidepressant. Inverse intensity of visit weights have been proposed to adjust for these imbalances, however no approaches have addressed complex settings where the covariate and the monitoring processes affect each other in time so as to induce endogeneity, a situation likely to occur in electronic health records. We review how selection bias due to outcome-dependent follow-up times may arise and propose a new cumulated weight that models a complete monitoring path so as to address the above-mentioned challenges and produce a reliable estimate of the impact of antidepressants on body mass index. More specifically, we do so using data from the Clinical Practice Research Datalink in the United Kingdom, comparing the marginal effect of two commonly used antidepressants, citalopram and fluoxetine, on body mass index. The results are compared to those obtained with simpler methods that do not account for the extent of the dependence due to an endogenous covariate process.},
  archive      = {J_AOAS},
  author       = {Janie Coulombe and Erica E. M. Moodie and Robert W. Platt and Christel Renoux},
  doi          = {10.1214/21-AOAS1570},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1868-1890},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation of the marginal effect of antidepressants on body mass index under confounding and endogenous covariate-driven monitoring times},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of presence-only data via exact bayes, with model
and effects identification. <em>AOAS</em>, <em>16</em>(3), 1848–1867.
(<a href="https://doi.org/10.1214/21-AOAS1569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an exact modeling approach for the analysis of presence-only ecological data. Our proposal is also based on frequently used inhomogeneous Poisson processes but does not rely on model approximations, unlike other approaches. Exactness is achieved via a data augmentation scheme. One of the augmented processes can be interpreted as the unobserved occurrences of the relevant species, and its posterior distribution can be used to make predictions of the species over the region of study beyond the observer bias. The data augmentation also leads to a natural Gibbs sampler to make Bayesian inference through MCMC. The proposal shows better performance than the currently standard method based on Poisson process with intensity function depending log-linearly on the covariates. Additionally, an identification problem that arises in the traditional model does not seem to affect our proposal in the analyses of real ecological data.},
  archive      = {J_AOAS},
  author       = {Guido A. Moreira and Dani Gamerman},
  doi          = {10.1214/21-AOAS1569},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1848-1867},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analysis of presence-only data via exact bayes, with model and effects identification},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric tail dependence modeling, with application to
cryptocurrency market data. <em>AOAS</em>, <em>16</em>(3), 1822–1847.
(<a href="https://doi.org/10.1214/21-AOAS1568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the inception of Bitcoin in 2008, cryptocurrencies have played an increasing role in the world of e-commerce, but the recent turbulence in the cryptocurrency market in 2018 has raised some concerns about their stability and associated risks. For investors it is crucial to uncover the dependence relationships between cryptocurrencies for a more resilient portfolio diversification. Moreover, the stochastic behavior in both tails is important, as long positions are sensitive to a decrease in prices (lower tail), while short positions are sensitive to an increase in prices (upper tail). In order to assess both risk types, we develop in this paper a flexible copula model which is able to distinctively capture asymptotic dependence or independence in its lower and upper tails simultaneously. Our proposed model is parsimonious and smoothly bridges (in each tail) both extremal dependence classes in the interior of the parameter space. Inference is performed using a full or censored likelihood approach, and we investigate by simulation the estimators’ efficiency under three different censoring schemes which reduce the impact of nonextreme observations. We also develop a local likelihood approach to capture the temporal dynamics of extremal dependence among pairs of leading cryptocurrencies. We here apply our model to historical closing prices of five leading cryotocurrencies which share large cryptocurrency market capitalizations. The results show that our proposed copula model outperforms alternative copula models and that the lower-tail dependence level between most pairs of leading cryptocurrencies and, in particular, Bitcoin and Ethereum has become stronger over time, smoothly transitioning from an asymptotic independence regime to an asymptotic dependence regime in recent years, whilst the upper tail has been relatively more stable overall at a weaker dependence level.},
  archive      = {J_AOAS},
  author       = {Yan Gong and Raphaël Huser},
  doi          = {10.1214/21-AOAS1568},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1822-1847},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Asymmetric tail dependence modeling, with application to cryptocurrency market data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter calibration in wake effect simulation model with
stochastic gradient descent and stratified sampling. <em>AOAS</em>,
<em>16</em>(3), 1795–1821. (<a
href="https://doi.org/10.1214/21-AOAS1567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the market share of wind energy has been rapidly growing, wake effect analysis is gaining substantial attention in the wind industry. Wake effects represent a wind shade cast by upstream turbines to the downwind direction, resulting in power deficits in downstream turbines. To quantify the aggregated influence of wake effects on the power generation of a wind farm, various simulation models have been developed, including Jensen’s wake model. These models include parameters that need to be calibrated from field data. Existing calibration methods are based on surrogate models that impute the data under the assumption that physical and/or computer trials are computationally expensive, typically at the design stage. This, however, is not the case where large volumes of data can be collected during the operational stage. Motivated by the wind energy application, we develop a new calibration approach for big data settings without the need for statistical emulators. Specifically, we cast the problem into a stochastic optimization framework and employ stochastic gradient descent to iteratively refine calibration parameters using randomly selected subsets of data. We then propose a stratified sampling scheme that enables choosing more samples from noisy and influential sampling regions and thus reducing the variance of the estimated gradient for improved convergence. Through both theoretical and numerical studies on wind farm data, we highlight the benefits of our variance-conscious calibration approach.},
  archive      = {J_AOAS},
  author       = {Bingjie Liu and Xubo Yue and Eunshin Byon and Raed Al Kontar},
  doi          = {10.1214/21-AOAS1567},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1795-1821},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Parameter calibration in wake effect simulation model with stochastic gradient descent and stratified sampling},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity analysis for evaluating principal surrogate
endpoints relaxing the equal early clinical risk assumption.
<em>AOAS</em>, <em>16</em>(3), 1774–1794. (<a
href="https://doi.org/10.1214/21-AOAS1566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the evaluation of postrandomization immune response biomarkers as principal surrogate endpoints of a vaccine’s protective effect, based on data from randomized vaccine trials. An important metric for quantifying a biomarker’s principal surrogacy in vaccine research is the vaccine efficacy curve, which shows a vaccine’s efficacy as a function of potential biomarker values if receiving vaccine, among an “early-always-at-risk” principal stratum of trial participants who remain disease-free at the time of biomarker measurement whether having received vaccine or placebo. Earlier work in principal surrogate evaluation relied on an “equal-early-clinical-risk” assumption for identifiability of the vaccine curve, based on observed disease status at the time of biomarker measurement. This assumption is violated in the common setting that the vaccine has an early effect on the clinical endpoint before the biomarker is measured. In particular, a vaccine’s early protective effect observed in two phase III dengue vaccine trials (CYD14/CYD15) has motivated our current research development. We relax the “equal-early-clinical-risk” assumption and propose a new sensitivity analysis framework for principal surrogate evaluation allowing for early vaccine efficacy. Under this framework we develop inference procedures for vaccine efficacy curve estimators, based on the estimated maximum likelihood approach. We then use the proposed methodology to assess the surrogacy of postrandomization neutralization titer in the motivating dengue application.},
  archive      = {J_AOAS},
  author       = {Ying Huang and Yingying Zhuang and Peter Gilbert},
  doi          = {10.1214/21-AOAS1566},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1774-1794},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sensitivity analysis for evaluating principal surrogate endpoints relaxing the equal early clinical risk assumption},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measurement error correction in particle tracking
microrheology. <em>AOAS</em>, <em>16</em>(3), 1747–1773. (<a
href="https://doi.org/10.1214/21-AOAS1565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In diverse biological applications, single-particle tracking (SPT) of passive microscopic species has become the experimental measurement of choice, when either the materials are of limited volume or so soft as to deform uncontrollably when manipulated by traditional instruments. In a wide range of SPT experiments, a ubiquitous finding is that of long-range dependence in the particles’ motion. This is characterized by a power-law signature in the mean squared displacement (MSD) of particle positions as a function of time, the parameters of which reveal valuable information about the viscous and elastic properties of various biomaterials. However, MSD measurements are typically contaminated by complex and interacting sources of instrumental noise. As these often affect the high-frequency bandwidth to which MSD estimates are particularly sensitive, inadequate error correction can lead to severe bias in power law estimation and, thereby, the inferred viscoelastic properties. In this article we propose a novel strategy to filter high-frequency noise from SPT measurements. Our filters are shown theoretically to cover a broad spectrum of high-frequency noises and lead to a parametric estimator of MSD power-law coefficients for which an efficient computational implementation is presented. Based on numerous analyses of experimental and simulated data, results suggest our methods perform very well compared to other denoising procedures.},
  archive      = {J_AOAS},
  author       = {Yun Ling and Martin Lysy and Ian Seim and Jay Newby and David B. Hill and Jeremy Cribb and M. Gregory Forest},
  doi          = {10.1214/21-AOAS1565},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1747-1773},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Measurement error correction in particle tracking microrheology},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of two-way outliers in multivariate data and
application to cheating detection in educational tests. <em>AOAS</em>,
<em>16</em>(3), 1718–1746. (<a
href="https://doi.org/10.1214/21-AOAS1564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a new latent variable model for the simultaneous (two-way) detection of outlying individuals and items for item-response-type data. The proposed model is a synergy between a factor model for binary responses and continuous response times that captures normal item response behaviour and a latent class model that captures the outlying individuals and items. A statistical decision framework is developed under the proposed model that provides compound decision rules for controlling local false discovery/nondiscovery rates of outlier detection. Statistical inference is carried out under a Bayesian framework for which a Markov chain Monte Carlo algorithm is developed. The proposed method is applied to the detection of cheating in educational tests, due to item leakage, using a case study of a computer-based nonadaptive licensure assessment. The performance of the proposed method is evaluated by simulation studies.},
  archive      = {J_AOAS},
  author       = {Yunxiao Chen and Yan Lu and Irini Moustaki},
  doi          = {10.1214/21-AOAS1564},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1718-1746},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Detection of two-way outliers in multivariate data and application to cheating detection in educational tests},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint integrative analysis of multiple data sources with
correlated vector outcomes. <em>AOAS</em>, <em>16</em>(3), 1700–1717.
(<a href="https://doi.org/10.1214/21-AOAS1563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distributed quadratic inference function framework to jointly estimate regression parameters from multiple potentially heterogeneous data sources with correlated vector outcomes. The primary goal of this joint integrative analysis is to estimate covariate effects on all outcomes through a marginal regression model in a statistically and computationally efficient way. We develop a data integration procedure for statistical estimation and inference of regression parameters that is implemented in a fully distributed and parallelized computational scheme. To overcome computational and modeling challenges arising from the high-dimensional likelihood of the correlated vector outcomes, we propose to analyze each data source using Qu, Lindsay and Li’s (Biometrika 87 (2000) 823–836) quadratic inference functions and then to jointly reestimate parameters from each data source by accounting for correlation between data sources using a combined meta-estimator in a similar spirit to the generalized method of moments put forward by Hansen (Econometrica 50 (1982) 1029–1054). We show both theoretically and numerically that the proposed method yields efficiency improvements and is computationally fast. We illustrate the proposed methodology with the joint integrative analysis of the association between smoking and metabolites in a large multicohort study and provide an R package for ease of implementation.},
  archive      = {J_AOAS},
  author       = {Emily C. Hector and Peter X.-K. Song},
  doi          = {10.1214/21-AOAS1563},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1700-1717},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint integrative analysis of multiple data sources with correlated vector outcomes},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian functional registration of fMRI activation maps.
<em>AOAS</em>, <em>16</em>(3), 1676–1699. (<a
href="https://doi.org/10.1214/21-AOAS1562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional magnetic resonance imaging (fMRI) has provided invaluable insight into our understanding of human behavior. However, large interindividual differences in both brain anatomy and functional localization after anatomical alignment remain a major limitation in conducting group analyses and performing population level inference. This paper addresses this problem by developing and validating a new computational technique for reducing misalignment across individuals in functional brain systems by spatially transforming each subject’s functional data to a common reference map. Our proposed Bayesian functional registration approach allows us to assess differences in brain function across subjects and individual differences in activation topology. It combines intensity-based and feature-based information into an integrated framework and allows inference to be performed on the transformation via the posterior samples. We evaluate the method in a simulation study and apply it to data from a study of thermal pain. We find that the proposed approach provides increased sensitivity for group-level inference.},
  archive      = {J_AOAS},
  author       = {Guoqing Wang and Abhirup Datta and Martin A. Lindquist},
  doi          = {10.1214/21-AOAS1562},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1676-1699},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian functional registration of fMRI activation maps},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-resolution bayesian mapping of landslide hazard with
unobserved trigger event. <em>AOAS</em>, <em>16</em>(3), 1653–1675. (<a
href="https://doi.org/10.1214/21-AOAS1561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models for landslide hazard enable mapping of risk factors and landslide occurrence intensity by using geomorphological covariates available at high spatial resolution. However, the spatial distribution of the triggering event (e.g., precipitation or earthquakes) is often not directly observed. In this paper we develop Bayesian spatial hierarchical models for point patterns of landslide occurrences using different types of log-Gaussian Cox processes. Starting from a competitive baseline model that captures the unobserved precipitation trigger through a spatial random effect at slope unit resolution, we explore novel complex model structures that take clusters of events arising at small spatial scales into account as well as nonlinear or spatially-varying covariate effects. For a 2009 event of around 5000 precipitation-triggered landslides in Sicily, Italy, we show how to fit our proposed models efficiently, using the integrated nested Laplace approximation (INLA), and rigorously compare the performance of our models both from a statistical and applied perspective. In this context we argue that model comparison should not be based on a single criterion and that different models of various complexity may provide insights into complementary aspects of the same applied problem. In our application our models are found to have mostly the same spatial predictive performance, implying that key to successful prediction is the inclusion of a slope-unit resolved random effect capturing the precipitation trigger. Interestingly, a parsimonious formulation of space-varying slope effects reflects a physical interpretation of the precipitation trigger: in subareas with weak trigger, the slope steepness is shown to be mostly irrelevant.},
  archive      = {J_AOAS},
  author       = {Thomas Opitz and Haakon Bakka and Raphaël Huser and Luigi Lombardo},
  doi          = {10.1214/21-AOAS1561},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1653-1675},
  shortjournal = {Ann. Appl. Stat.},
  title        = {High-resolution bayesian mapping of landslide hazard with unobserved trigger event},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical window variable selection for mixtures: Estimating
the impact of multiple air pollutants on stillbirth. <em>AOAS</em>,
<em>16</em>(3), 1633–1652. (<a
href="https://doi.org/10.1214/21-AOAS1560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the role of time-varying pollution mixtures on human health is critical as people are simultaneously exposed to multiple pollutants during their lives. For vulnerable subpopulations who have well-defined exposure periods (e.g., pregnant women), questions regarding critical windows of exposure to these mixtures are important for mitigating harm. We extend critical window variable selection (CWVS) to the multipollutant setting by introducing CWVS for mixtures (CWVSmix), a hierarchical Bayesian method that combines smoothed variable selection and temporally correlated weight parameters to: (i) identify critical windows of exposure to mixtures of time-varying pollutants, (ii) estimate the time-varying relative importance of each individual pollutant and their first order interactions within the mixture, and (iii) quantify the impact of the mixtures on health. Through simulation we show that CWVSmix offers the best balance of performance in each of these categories in comparison to competing methods. Using these approaches, we investigate the impact of exposure to multiple ambient air pollutants on the risk of stillbirth in New Jersey, 2005–2014. We find consistent elevated risk in gestational weeks 2, 16–17, and 20 for non-Hispanic Black mothers, with pollution mixtures dominated by ammonium (weeks 2, 17, 20), nitrate (weeks 2, 17), nitrogen oxides (weeks 2, 16), PM2.5 (week 2), and sulfate (week 20). The method is available in the R package CWVSmix.},
  archive      = {J_AOAS},
  author       = {Joshua L. Warren and Howard H. Chang and Lauren K. Warren and Matthew J. Strickland and Lyndsey A. Darrow and James A. Mulholland},
  doi          = {10.1214/21-AOAS1560},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1633-1652},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Critical window variable selection for mixtures: Estimating the impact of multiple air pollutants on stillbirth},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric multinomial mixed-effects models: A
university students profiling tool. <em>AOAS</em>, <em>16</em>(3),
1608–1632. (<a href="https://doi.org/10.1214/21-AOAS1559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applicative studies deal with multinomial responses and hierarchical data. Performing clustering at the highest level of grouping, in multilevel multinomial regression, is also often of interest. In this study we analyse Politecnico di Milano data with the aim of profiling students, modelling their probabilities of belonging to different categories and considering their nested structure within engineering degree programmes. In particular, we are interested in clustering degree programmes standing on their effects on different types of student career. To this end, we propose an EM algorithm for implementing semiparametric mixed-effects models dealing with a multinomial response. The novel semiparametric approach assumes the random effects to follow a multivariate discrete distribution with an a priori unknown number of support points, that is, allowed to differ across response categories. The advantage of this modelling is twofold: the discrete distribution on random effects allows, first, to express the marginal density as a weighted sum, avoiding numerical problems in the integration step, typical of the parametric approach, and, second, to identify a latent structure at the highest level of the hierarchy where groups are clustered into subpopulations.},
  archive      = {J_AOAS},
  author       = {Chiara Masci and Francesca Ieva and Anna Maria Paganoni},
  doi          = {10.1214/21-AOAS1559},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1608-1632},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric multinomial mixed-effects models: A university students profiling tool},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring performance for end-of-life care. <em>AOAS</em>,
<em>16</em>(3), 1586–1607. (<a
href="https://doi.org/10.1214/21-AOAS1558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although not without controversy, readmission is entrenched as a hospital quality metric with statistical analyses generally based on fitting a logistic-Normal generalized linear mixed model. Such analyses, however, ignore death as a competing risk, although doing so for clinical conditions with high mortality can have profound effects; a hospital’s seemingly good performance for readmission may be an artifact of it having poor performance for mortality. In this paper we propose novel multivariate hospital-level performance measures for readmission and mortality that derive from framing the analysis as one of cluster-correlated semi-competing risks data. We also consider a number of profiling-related goals, including the identification of extreme performers and a bivariate classification of whether the hospital has higher-/lower-than-expected readmission and mortality rates via a Bayesian decision-theoretic approach that characterizes hospitals on the basis of minimizing the posterior expected loss for an appropriate loss function. In some settings, particularly if the number of hospitals is large, the computational burden may be prohibitive. To resolve this, we propose a series of analysis strategies that will be useful in practice. Throughout, the methods are illustrated with data from CMS on N=17,685 patients diagnosed with pancreatic cancer between 2000–2012 at one of J=264 hospitals in California.},
  archive      = {J_AOAS},
  author       = {Sebastien Haneuse and Deborah Schrag and Francesca Dominici and Sharon-Lise Normand and Kyu Ha Lee},
  doi          = {10.1214/21-AOAS1558},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1586-1607},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Measuring performance for end-of-life care},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating mode effects from a sequential mixed-mode
experiment using structural moment models. <em>AOAS</em>,
<em>16</em>(3), 1563–1585. (<a
href="https://doi.org/10.1214/21-AOAS1557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until recently, the survey mode of the household panel study Understanding Society was mainly face-to-face interview, but it has now adopted a mixed-mode design where individuals can self-complete the questionnaire via the web. As mode is known to affect survey data, a randomized mixed-mode experiment was implemented during the first year of the two-year Wave 8 fieldwork period to assess the impact of this change. The experiment involved a sequential design that permits the identification of mode effects in the presence of nonignorable nonrandom mode selection. While previous studies have used instrumental variables regression to estimate the effects of mode on the means of the survey variables, we describe a more general methodology based on novel structural moment models that characterizes the overall effect of mode on a survey by its effects on the moments of the survey variables’ joint distribution. We adapt our estimation procedure to account for nonresponse and complex sampling designs and to include suitable auxiliary data to improve inference and relax key assumptions. Finally, we demonstrate how to estimate the effects of mode on the parameter estimates of generalized linear models and other exponential family models when both outcomes and predictors are subject to mode effects. This methodology is used to investigate the impact of the move to web mode on Wave 8 of Understanding Society.},
  archive      = {J_AOAS},
  author       = {Paul S. Clarke and Yanchun Bao},
  doi          = {10.1214/21-AOAS1557},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1563-1585},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating mode effects from a sequential mixed-mode experiment using structural moment models},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian hierarchical model for combining multiple data
sources in population size estimation. <em>AOAS</em>, <em>16</em>(3),
1550–1562. (<a href="https://doi.org/10.1214/21-AOAS1556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To combat the HIV/AIDS pandemic effectively, targeted interventions among certain key populations play a critical role. Examples of such key populations include sex workers, people who inject drugs, and men who have sex with men. While having accurate estimates for the size of these key populations is important, any attempt to directly contact or count members of these populations is difficult. As a result, indirect methods are used to produce size estimates. Multiple approaches for estimating the size of such populations have been suggested but often give conflicting results. It is, therefore, necessary to have a principled way to combine and reconcile these estimates. To this end, we present a Bayesian hierarchical model for estimating the size of key populations that combines multiple estimates from different sources of information. The proposed model makes use of multiple years of data and explicitly models the systematic error in the data sources used. We use the model to estimate the size of people who inject drugs in Ukraine. We evaluate the appropriateness of the model and compare the contribution of each data source to the final estimates.},
  archive      = {J_AOAS},
  author       = {Jacob Parsons and Xiaoyue Niu and Le Bao},
  doi          = {10.1214/21-AOAS1556},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1550-1562},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian hierarchical model for combining multiple data sources in population size estimation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved inference on risk measures for univariate extremes.
<em>AOAS</em>, <em>16</em>(3), 1524–1549. (<a
href="https://doi.org/10.1214/21-AOAS1555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss the use of likelihood asymptotics for inference on risk measures in univariate extreme value problems, focusing on estimation of high quantiles and similar summaries of risk for uncertainty quantification. We study whether higher-order approximation, based on the tangent exponential model, can provide improved inferences. We conclude that inference based on maxima is generally robust to mild model misspecification and that profile likelihood-based confidence intervals will often be adequate, whereas inferences based on threshold exceedances can be badly biased but may be improved by higher-order methods, at least for moderate sample sizes. We use the methods to shed light on catastrophic rainfall in Venezuela, flooding in Venice, and the lifetimes of Italian semisupercentenarians.},
  archive      = {J_AOAS},
  author       = {Léo R. Belzile and Anthony C. Davison},
  doi          = {10.1214/21-AOAS1555},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1524-1549},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Improved inference on risk measures for univariate extremes},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric point process modeling of blinking artifacts
in PALM. <em>AOAS</em>, <em>16</em>(3), 1500–1523. (<a
href="https://doi.org/10.1214/21-AOAS1553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photoactivated localization microscopy (PALM) is a powerful imaging technique for characterization of protein organization in biological cells. Due to the stochastic blinking of fluorescent probes and camera discretization effects, each protein gives rise to a cluster of artificial observations. These blinking artifacts are an obstacle for quantitative analysis of PALM data, and tools for their correction are in high demand. We develop the independent blinking cluster point process (IBCpp) family of models, which is suited for modeling of data from single-molecule localization microscopy modalities, and we present results on the mark correlation function. We then construct the PALM-IBCpp, a semiparametric IBCpp tailored for PALM data, and we describe a procedure for estimation of parameters which can be used without parametric assumptions on the spatial organization of proteins. Our model is validated on nuclear pore complex reference data, where the ground truth was accurately recovered, and we demonstrate how the estimated blinking parameters can be used to perform a blinking corrected test for protein clustering in a cell expressing the adaptor protein LAT. Finally, we consider simulations with varying degrees of blinking and protein clustering to shed light on the expected performance in a range of realistic settings.},
  archive      = {J_AOAS},
  author       = {Louis G. Jensen and David J. Williamson and Ute Hahn},
  doi          = {10.1214/21-AOAS1553},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1500-1523},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric point process modeling of blinking artifacts in PALM},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dirichlet-tree multinomial mixtures for clustering
microbiome compositions. <em>AOAS</em>, <em>16</em>(3), 1476–1499. (<a
href="https://doi.org/10.1214/21-AOAS1552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the human microbiome has gained substantial interest in recent years, and a common task in the analysis of these data is to cluster microbiome compositions into subtypes. This subdivision of samples into subgroups serves as an intermediary step in achieving personalized diagnosis and treatment. In applying existing clustering methods to modern microbiome studies, including the American Gut Project (AGP) data, we found that this seemingly standard task, however, is very challenging in the microbiome composition context, due to several key features of such data. Standard distance-based clustering algorithms generally do not produce reliable results, as they do not take into account the heterogeneity of the cross-sample variability among the bacterial taxa, while existing model-based approaches do not allow sufficient flexibility for the identification of complex within-cluster variation from cross-cluster variation. Direct applications of such methods generally lead to overly dispersed clusters in the AGP data, and such a phenomenon is common for other microbiome data. To overcome these challenges, we introduce Dirichlet-tree multinomial mixtures (DTMM) as a Bayesian generative model for clustering amplicon sequencing data in microbiome studies. DTMM models the microbiome population with a mixture of Dirichlet-tree kernels that utilizes the phylogenetic tree to offer a more flexible covariance structure in characterizing within-cluster variation, and it provides a means for identifying a subset of signature taxa that distinguish the clusters. We perform extensive simulation studies to evaluate the performance of DTMM, compare it to state-of-the-art model-based and distance-based clustering methods in the microbiome context and carry out a validation study on a publicly available longitudinal data set to confirm the biological relevance of the clusters. Finally, we report a case study on the fecal data from the AGP to identify compositional clusters among individuals with inflammatory bowel disease and diabetes. Among our most interesting findings is that enterotypes (i.e., gut microbiome clusters) are not always defined by the most dominant taxa, as previous analyses had assumed, but can involve a number of less abundant taxa which cannot be identified with existing distance-based and method-based approaches.},
  archive      = {J_AOAS},
  author       = {Jialiang Mao and Li Ma},
  doi          = {10.1214/21-AOAS1552},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1476-1499},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dirichlet-tree multinomial mixtures for clustering microbiome compositions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian local false discovery rate for sparse count data
with application to the discovery of hotspots in protein domains.
<em>AOAS</em>, <em>16</em>(3), 1459–1475. (<a
href="https://doi.org/10.1214/21-AOAS1551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer research at the molecular level, it is critical to understand which somatic mutations play an important role in the initiation or progression of cancer. Recently, studying cancer somatic variants at the protein domain level is an important area for uncovering functionally related somatic mutations. The main issue is to find the protein domain hotspots which have significantly high frequency of mutations. Multiple testing procedures are commonly used to identify hotspots; however, when data is not large enough, existing methods produce unreliable results with failure in controlling a given type I error rate. We propose multiple testing procedures, based on Bayesian local false discovery rate, for sparse count data and apply it in the identification of clusters of somatic mutations across entire gene families using protein domain models. In multiple testing for count data, it is not clear what kind of the null distribution should be admitted. In our proposed algorithms, we implement the zero assumption in the context of Bayesian methods to identify the null distribution for count data rather than using any theoretical null distribution. Furthermore, we also address different types of modeling of alternative distributions. The proposed fully Bayesian models are efficient when the number of count data is small (50≤N800). We provide numerical studies to show that the proposed fully Bayesian methods can control a given level of false discovery rate for small number of positions while existing approaches based on nonparametric empirical Bayes fail in controlling a false discovery rate. In addition, we present real data examples of protein domain data to select hotspots in protein domain data.},
  archive      = {J_AOAS},
  author       = {Iris Ivy M. Gauran and Junyong Park and Ilia Rattsev and Thomas A. Peterson and Maricel G. Kann and DoHwan Park},
  doi          = {10.1214/21-AOAS1551},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1459-1475},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian local false discovery rate for sparse count data with application to the discovery of hotspots in protein domains},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel framework to estimate multidimensional minimum
effective doses using asymmetric posterior gain and ϵ-tapering.
<em>AOAS</em>, <em>16</em>(3), 1445–1458. (<a
href="https://doi.org/10.1214/21-AOAS1549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we address the problem of estimating minimum effective doses in dose-finding clinical trials of multidimensional treatment. We are motivated by a behavioral intervention trial where we introduce sedentary breaks to subjects with a goal to reduce their glucose level monitored over 8 hours. Each sedentary break regimen is defined by two elements: break frequency and break duration. The trial aims to identify minimum combinations of frequency and duration that shift mean glucose, that is, the minimum effective dose (MED) combinations. The means of glucose reduction associated with the dose combinations are only partially ordered. To circumvent constrained estimation due to partial ordering, we propose estimating the MED by maximizing a weighted product of combinationwise posterior gains. The estimation adopts an asymmetric gain function, indexed by a decision parameter ϵ, which defines the relative gains of a true negative decision and a true positive decision. We also introduce an adaptive ϵ-tapering algorithm to be used in conjunction with the estimation method. Simulation studies show that using asymmetric gain with a carefully chosen ϵ is critical to keeping false discoveries low, while ϵ-tapering adds to the probability of identifying truly effective doses (i.e., true positives). Under an ensemble of scenarios for the sedentary break study, ϵ-tapering yields consistently high true positive rates across scenarios and achieves about 90\% true positive rate, compared to 68\% by a nonadaptive design with comparable false discovery rate.},
  archive      = {J_AOAS},
  author       = {Ying Kuen Cheung and Thevaa Chandereng and Keith M. Diaz},
  doi          = {10.1214/21-AOAS1549},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1445-1458},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A novel framework to estimate multidimensional minimum effective doses using asymmetric posterior gain and ϵ-tapering},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated quantile RAnk test (iQRAT) for gene-level
associations. <em>AOAS</em>, <em>16</em>(3), 1423–1444. (<a
href="https://doi.org/10.1214/21-AOAS1548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene-based testing is a commonly employed strategy in many genetic association studies. Gene-trait associations can be complex due to underlying population heterogeneity, gene-environment interactions, and various other reasons. Existing gene-based tests, such as burden and sequence kernel association tests (SKAT), are mean-based tests and may miss or underestimate higher-order associations that could be scientifically interesting. In this paper we propose a new family of gene-level association tests that integrate quantile rank score process to better accommodate complex associations. The resulting test statistics have multiple advantages: (1) they are almost as efficient as the best existing tests when the associations are homogeneous across quantile levels and have improved efficiency for complex and heterogeneous associations; (2) they provide useful insights into risk stratification; (3) the test statistics are distribution free and could hence accommodate a wide range of underlying distributions, and (4) they are computationally efficient. We established the asymptotic properties of the proposed tests under the null and alternative hypotheses and conducted large-scale simulation studies to investigate their finite sample performance. The performance of the proposed approach is compared with that of conventional mean-based tests, that is, the burden and SKAT tests, through simulation studies and applications to a metabochip dataset on lipid traits and to the genotype-tissue expression data in GTEx to identify eGenes, that is, genes whose expression levels are associated with cis-eQTLs.},
  archive      = {J_AOAS},
  author       = {Tianying Wang and Iuliana Ionita-Laza and Ying Wei},
  doi          = {10.1214/21-AOAS1548},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1423-1444},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrated quantile RAnk test (iQRAT) for gene-level associations},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-clustering of multivariate functional data for the
analysis of air pollution in the south of france. <em>AOAS</em>,
<em>16</em>(3), 1400–1422. (<a
href="https://doi.org/10.1214/21-AOAS1547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, air pollution is a major threat for public health with clear relationships with many diseases, especially cardiovascular ones. The spatiotemporal study of pollution is of great interest for governments and local authorities when deciding for public alerts or new city policies against pollution increase. The aim of this work is to study spatiotemporal profiles of environmental data collected in the south of France (Région Sud) by the public agency AtmoSud. The idea is to better understand the exposition to pollutants of inhabitants on a large territory with important differences in term of geography and urbanism. The data gather the recording of daily measurements of five environmental variables, namely, three pollutants (PM10, NO2, O3) and two meteorological factors (pressure and temperature) over six years. Those data can be seen as multivariate functional data: quantitative entities evolving along time for which there is a growing need of methods to summarize and understand them. For this purpose a novel co-clustering model for multivariate functional data is defined. The model is based on a functional latent block model which assumes for each co-cluster a probabilistic distribution for multivariate functional principal component scores. A stochastic EM algorithm, embedding a Gibbs sampler, is proposed for model inference as well as a model selection criteria for choosing the number of co-clusters. The application of the proposed co-clustering algorithm on environmental data of the Région Sud allowed to divide the region, composed by 357 zones, into six macroareas with common exposure to pollution. We showed that pollution profiles vary accordingly to the seasons, and the patterns are similar during the six years studied. These results can be used by local authorities to develop specific programs to reduce pollution at the macroarea level and to identify specific periods of the year with high pollution peaks in order to set up specific health prevention programs. Overall, the proposed co-clustering approach is a powerful resource to analyse multivariate functional data in order to identify intrinsic data structure and to summarize variables profiles over long periods of time.},
  archive      = {J_AOAS},
  author       = {Charles Bouveyron and Julien Jacques and Amandine Schmutz and Fanny Simões and Silvia Bottini},
  doi          = {10.1214/21-AOAS1547},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1400-1422},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Co-clustering of multivariate functional data for the analysis of air pollution in the south of france},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian semiparametric long memory models for discretized
event data. <em>AOAS</em>, <em>16</em>(3), 1380–1399. (<a
href="https://doi.org/10.1214/21-AOAS1546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of semiparametric latent variable models for long memory discretized event data. The proposed methodology is motivated by a study of bird vocalizations in the Amazon rain forest; the timings of vocalizations exhibit self-similarity and long range dependence. This rules out Poisson process based models where the rate function itself is not long range dependent. The proposed class of FRActional Probit (FRAP) models is based on thresholding, a latent process. This latent process is modeled by a smooth Gaussian process and a fractional Brownian motion by assuming an additive structure. We develop a Bayesian approach to inference using Markov chain Monte Carlo and show good performance in simulation studies. Applying the methods to the Amazon bird vocalization data, we find substantial evidence for self-similarity and non-Markovian/Poisson dynamics. To accommodate the bird vocalization data in which there are many different species of birds exhibiting their own vocalization dynamics, a hierarchical expansion of FRAP is provided in the Supplementary Material.},
  archive      = {J_AOAS},
  author       = {Antik Chakraborty and Otso Ovaskainen and David B. Dunson},
  doi          = {10.1214/21-AOAS1546},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1380-1399},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian semiparametric long memory models for discretized event data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The causal effect of a timeout at stopping an opposing run
in the NBA. <em>AOAS</em>, <em>16</em>(3), 1359–1379. (<a
href="https://doi.org/10.1214/21-AOAS1545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the summer of 2017, the National Basketball Association reduced the number of total timeouts, along with other rule changes, to regulate the flow of the game. With these rule changes it becomes increasingly important for coaches to effectively manage their timeouts. Understanding the utility of a timeout under various game scenarios, for example, during an opposing team’s run, is of the utmost importance. There are two schools of thought when the opposition is on a run: (1) call a timeout and allow your team to rest and regroup, or (2) save a timeout and hope your team can make corrections during play. This paper investigates the credence of these tenets using the Rubin causal model framework to quantify the causal effect of a timeout in the presence of an opposing team’s run. Too often overlooked, we carefully consider the stable unit-treatment-value assumption (SUTVA) in this context and use the SUTVA to motivate our definition of units. To measure the effect of a timeout, we introduce a novel, interpretable outcome based on the score difference to describe broad changes in the scoring dynamics. This outcome is well suited for situations where the quantity of interest fluctuates frequently, a commonality in many sports analytics applications. We conclude from our analysis that, while comebacks frequently occur after a run, it is slightly disadvantageous to call a timeout during a run by the opposing team and further demonstrate that the magnitude of this effect varies by franchise.},
  archive      = {J_AOAS},
  author       = {Connor P. Gibbs and Ryan Elmore and Bailey K. Fosdick},
  doi          = {10.1214/21-AOAS1545},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1359-1379},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The causal effect of a timeout at stopping an opposing run in the NBA},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Matrix completion methods for the total electron content
video reconstruction. <em>AOAS</em>, <em>16</em>(3), 1333–1358. (<a
href="https://doi.org/10.1214/21-AOAS1541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The total electron content (TEC) maps can be used to estimate the signal delay of GPS due to the ionospheric electron content between a receiver and satellite. This delay can result in GPS positioning error. Thus, it is important to monitor the TEC maps. The observed TEC maps have big patches of missingness in the ocean and scattered small areas of missingness on the land. In this paper we propose several extensions of existing matrix completion algorithms to achieve TEC map reconstruction, accounting for spatial smoothness and temporal consistency while preserving important structures of the TEC maps. We call the proposed method video imputation with softImpute, temporal smoothing and auxiliary data (VISTA). Numerical simulations that mimic patterns of real data are given. We show that our proposed method achieves better reconstructed TEC maps, as compared to existing methods in literature. Our proposed computational algorithm is general and can be readily applied for other problems besides TEC map reconstruction.},
  archive      = {J_AOAS},
  author       = {Hu Sun and Zhijun Hua and Jiaen Ren and Shasha Zou and Yuekai Sun and Yang Chen},
  doi          = {10.1214/21-AOAS1541},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1333-1358},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Matrix completion methods for the total electron content video reconstruction},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph link prediction in computer networks using poisson
matrix factorisation. <em>AOAS</em>, <em>16</em>(3), 1313–1332. (<a
href="https://doi.org/10.1214/21-AOAS1540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph link prediction is an important task in cybersecurity: relationships between entities within a computer network, such as users interacting with computers or system libraries and the corresponding processes that use them, can provide key insights into adversary behaviour. Poisson matrix factorisation (PMF) is a popular model for link prediction in large networks, particularly useful for its scalability. In this article PMF is extended to include scenarios that are commonly encountered in cybersecurity applications. Specifically, an extension is proposed to explicitly handle binary adjacency matrices and include known categorical covariates associated with the graph nodes. A seasonal PMF model is also presented to handle seasonal networks. To allow the methods to scale to large graphs, variational methods are discussed for performing fast inference. The results show an improved performance over the standard PMF model and other statistical network models.},
  archive      = {J_AOAS},
  author       = {Francesco Sanna Passino and Melissa J. M. Turcotte and Nicholas A. Heard},
  doi          = {10.1214/21-AOAS1540},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1313-1332},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Graph link prediction in computer networks using poisson matrix factorisation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). B-scaling: A novel nonparametric data fusion method.
<em>AOAS</em>, <em>16</em>(3), 1292–1312. (<a
href="https://doi.org/10.1214/21-AOAS1537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Very often for the same scientific question, there may exist different techniques or experiments that measure the same numerical quantity. Historically, various methods have been developed to exploit the information within each type of data independently. However, statistical data fusion methods that could effectively integrate multisource data under a unified framework are lacking. In this paper we propose a novel data fusion method, called B-scaling, for integrating multisource data. Consider K measurements that are generated from different sources but measure the same latent variable through some linear or nonlinear ways. We seek to find a representation of the latent variable, named B-mean, which captures the common information contained in the K measurements while taking into account the nonlinear mappings between them and the latent variable. We also establish the asymptotic property of the B-mean and apply the proposed method to integrate multiple histone modifications and DNA methylation levels for characterizing epigenomic landscape. Both numerical and empirical studies show that B-scaling is a powerful data fusion method with broad applications.},
  archive      = {J_AOAS},
  author       = {Yiwen Liu and Xiaoxiao Sun and Wenxuan Zhong and Bing Li},
  doi          = {10.1214/21-AOAS1537},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1292-1312},
  shortjournal = {Ann. Appl. Stat.},
  title        = {B-scaling: A novel nonparametric data fusion method},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive latent variable modeling with application to
case-control sequencing experiments. <em>AOAS</em>, <em>16</em>(3),
1268–1291. (<a href="https://doi.org/10.1214/21-AOAS1534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput RNA-sequencing (RNA-seq) technologies are powerful tools for understanding cellular state. Often, it is of interest to quantify and to summarize changes in cell state that occur between experimental or biological conditions. Differential expression is typically assessed using univariate tests to measure genewise shifts in expression. However, these methods largely ignore changes in transcriptional correlation. Furthermore, there is a need to identify the low-dimensional structure of the gene expression shift to identify collections of genes that change between conditions. Here, we propose contrastive latent variable models designed for count data to create a richer portrait of differential expression in sequencing data. These models disentangle the sources of transcriptional variation in different conditions in the context of an explicit model of variation at baseline. Moreover, we develop a model-based hypothesis testing framework that can test for global and gene subset-specific changes in expression. We evaluate our model through extensive simulations and analyses with count-based gene expression data from perturbation and observational sequencing experiments. We find that our methods effectively summarize and quantify complex transcriptional changes in case-control experimental sequencing data.},
  archive      = {J_AOAS},
  author       = {Andrew Jones and F. William Townes and Didong Li and Barbara E. Engelhardt},
  doi          = {10.1214/21-AOAS1534},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1268-1291},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Contrastive latent variable modeling with application to case-control sequencing experiments},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based distance embedding with applications to
chromosomal conformation biology. <em>AOAS</em>, <em>16</em>(3),
1253–1267. (<a href="https://doi.org/10.1214/21-AOAS1479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent development of high-throughput biotechnologies, such as Hi-C, have enabled genome-wide measurement of chromosomal conformation. The interaction signals among genomic loci are contaminated with noises. It remains largely unknown how well the underlying chromosomal conformation can be elucidated, based on massive and noisy measurements. We propose a new model-based distance embedding (MDE) framework, to reveal spatial organizations of chromosomes. The proposed framework is a general methodology, which allows us to link accurate probabilistic models, which characterize biological data properties, to efficiently recovering Euclidean distance matrices from noisy observations. The performance of MDE is shown through numerical experiments inspired by regular helix structure and random movement of chromosomes. The practical merits of MDE are also demonstrated by applications to real Hi-C data from both human and mouse cells which are further validated by gold standard benchmarks.},
  archive      = {J_AOAS},
  author       = {Yuping Zhang and Disheng Mao and Zhengqing Ouyang},
  doi          = {10.1214/21-AOAS1479},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1253-1267},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model-based distance embedding with applications to chromosomal conformation biology},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Higher criticism for discriminating word-frequency tables
and authorship attribution. <em>AOAS</em>, <em>16</em>(2), 1236–1252.
(<a href="https://doi.org/10.1214/21-AOAS1544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We adapt the higher criticism (HC) goodness-of-fit test to measure the closeness between word-frequency tables. We apply this measure to authorship attribution challenges, where the goal is to identify the author of a document using other documents whose authorship is known. The method is simple yet performs well without handcrafting and tuning, reporting accuracy at the state-of-the-art level in various current challenges. As an inherent side effect, the HC calculation identifies a subset of discriminating words. In practice, the identified words have low variance across documents belonging to a corpus of homogeneous authorship. We conclude that in comparing the similarity of a new document and a corpus of a single author, HC is mostly affected by words characteristic of the author and is relatively unaffected by topic structure.},
  archive      = {J_AOAS},
  author       = {Alon Kipnis},
  doi          = {10.1214/21-AOAS1544},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1236-1252},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Higher criticism for discriminating word-frequency tables and authorship attribution},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact evaluation of the LAPD community safety partnership.
<em>AOAS</em>, <em>16</em>(2), 1215–1235. (<a
href="https://doi.org/10.1214/21-AOAS1543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2011, the Los Angeles Police Department (LAPD), in conjunction with other governmental and nonprofit groups, launched the Community Safety Partnership (CSP) in several public housing developments in Los Angeles. Following a relationship-based policing model, officers were assigned to work collaboratively with community members to reduce crime and build trust. However, evaluating the causal impact of this policy intervention is difficult, given the notable differences between communities where CSP was implemented and the surrounding communities in South Los Angeles. In this paper we use a novel data set, based on the LAPD’s reported crime incidents and calls-for-service, to evaluate the effectiveness of this program via augmented synthetic control models, a cutting-edge method for policy evaluation. We perform falsification analyses to evaluate the robustness of the results. In the public housing developments where it was first deployed, we find that CSP exhibited modest but statistically insignificant reductions in reported violent crime incidents, shots fired and violent crime calls-for-service, and Part I reported crime incidents. We do not find evidence of crime displacement from CSP regions to neighboring control regions.},
  archive      = {J_AOAS},
  author       = {Sydney Kahmann and Erin Hartman and Jorja Leap and P. Jeffrey Brantingham},
  doi          = {10.1214/21-AOAS1543},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1215-1235},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Impact evaluation of the LAPD community safety partnership},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional functional clustering for longitudinal data with
heterogeneous nonlinear patterns. <em>AOAS</em>, <em>16</em>(2),
1191–1214. (<a href="https://doi.org/10.1214/21-AOAS1542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies of cognitive aging, it is crucial to distinguish subtypes of longitudinal cognition change while accounting for the effects of given covariates. The longitudinal cognition trajectories and the covariate effects can both be nonlinear with heterogeneous shapes that do not follow a simple parametric form, where flexible functional methods are preferred. However, most functional clustering methods for longitudinal data do not allow controlling for the possible functional effects of covariates. Although traditional mixture-of-experts methods can include covariates and be extended to the functional setting, using nonlinear basis functions, satisfactory parsimonious functional methods required for robust functional coefficient estimation and clustering are still lacking. In this paper we propose a novel latent class functional mixed-effects model in which we assume the covariates have fixed functional effects, and the random curves follow a mixture of Gaussian processes that facilitates a model-based conditional clustering. A transformed penalized B-spline approach is employed for parsimonious modeling and robust model estimation. We propose a new iterative-REML method to choose the penalty parameters in heterogeneous data. The new method is applied to the latest data from the Religious Orders Study and Rush Memory and Aging Project, and four novel subtypes of cognitive changes are identified.},
  archive      = {J_AOAS},
  author       = {Tianhao Wang and Lei Yu and Sue E. Leurgans and Robert S. Wilson and David A. Bennett and Patricia A. Boyle},
  doi          = {10.1214/21-AOAS1542},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1191-1214},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Conditional functional clustering for longitudinal data with heterogeneous nonlinear patterns},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov-modulated hawkes processes for modeling sporadic and
bursty event occurrences in social interactions. <em>AOAS</em>,
<em>16</em>(2), 1171–1190. (<a
href="https://doi.org/10.1214/21-AOAS1539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling event dynamics is central to many disciplines. Patterns in observed social interaction events can be commonly modeled using point processes. Such social interaction event data often exhibit self-exciting, heterogeneous and sporadic trends which is challenging for conventional models. It is reasonable to assume that there exists a hidden state process that drives different event dynamics at different states. In this paper we propose a Markov modulated Hawkes process (MMHP) model for learning such a mixture of social interaction event dynamics and develop corresponding inference algorithms. Numerical experiments using synthetic data demonstrate that MMHP with the proposed estimation algorithms consistently recover the true hidden state process in simulations, while email data from a large university and data from an animal behavior study show that the procedure captures distinct event dynamics that reveal interesting social structures in the real data.},
  archive      = {J_AOAS},
  author       = {Jing Wu and Owen G. Ward and James Curley and Tian Zheng},
  doi          = {10.1214/21-AOAS1539},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1171-1190},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Markov-modulated hawkes processes for modeling sporadic and bursty event occurrences in social interactions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal-textual point processes for crime linkage
detection. <em>AOAS</em>, <em>16</em>(2), 1151–1170. (<a
href="https://doi.org/10.1214/21-AOAS1538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crimes emerge out of complex interactions of human behaviors and situations. Linkages between crime incidents are highly complex. Detecting crime linkage, given a set of incidents, is a highly challenging task since we only have limited information, including text descriptions, incident times, and locations. In practice, there are very few labels. We propose a new statistical modeling framework for spatiotemporal-textual data and demonstrate its usage on crime linkage detection. We capture linkages of crime incidents via multivariate marked spatiotemporal Hawkes processes and treat embedding vectors of the free-text as marks of the incident, inspired by the notion of modus operandi (M.O.) in crime analysis. Numerical results, using real data, demonstrate the good performance of our method as well as reveals interesting patterns in the crime data: the joint modeling of space, time, and text information enhances crime linkage detection, compared with the state-of-the-art, and the learned spatial dependence from data can be useful for police operations.},
  archive      = {J_AOAS},
  author       = {Shixiang Zhu and Yao Xie},
  doi          = {10.1214/21-AOAS1538},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1151-1170},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatiotemporal-textual point processes for crime linkage detection},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical shape analysis of brain arterial networks (BAN).
<em>AOAS</em>, <em>16</em>(2), 1130–1150. (<a
href="https://doi.org/10.1214/21-AOAS1536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arterial networks in the human brain, termed brain arterial networks or BANs, are complex arrangements of individual arteries, branching patterns, and interconnectivity. BANs play an essential role in characterizing and understanding brain physiology, and one would like tools for statistically analyzing the shapes of BANs. These tools include quantifying shape differences, comparing populations of subjects, and studying the effects of covariates on these shapes. This paper mathematically represents and statistically analyzes BAN shapes as elastic shape graphs. Each elastic shape graph consists of nodes, or points in 3D, connected by 3D curves, or edges, with arbitrary shapes. We develop a mathematical representation, a Riemannian metric and other geometrical tools, such as computations of geodesics, means, covariances, and PCA, for helping analyze BANs as elastic graphs. We apply this analysis to BANs after dividing them into four components—top, bottom, left, and right. The framework is then used to generate shape summaries of BANs from 92 subjects and study the effects of age and gender on shapes of BAN components. While gender effects require further investigation, we conclude that age has a clear, quantifiable effect on BAN shapes. Specifically, we find an increased variance in BAN shapes as age increases.},
  archive      = {J_AOAS},
  author       = {Xiaoyang Guo and Aditi Basu Bal and Tom Needham and Anuj Srivastava},
  doi          = {10.1214/21-AOAS1536},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1130-1150},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Statistical shape analysis of brain arterial networks (BAN)},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting heterogeneous treatment effects with instrumental
variables and application to the oregon health insurance experiment.
<em>AOAS</em>, <em>16</em>(2), 1111–1129. (<a
href="https://doi.org/10.1214/21-AOAS1535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing interest in estimating heterogeneity in causal effects in randomized and observational studies. However, little research has been conducted to understand effect heterogeneity in an instrumental variables study. In this work we present a method to estimate heterogeneous causal effects using an instrumental variable with matching. The method has two parts. The first part uses subject-matter knowledge and interpretable machine-learning techniques, such as classification and regression trees, to discover potential effect modifiers. The second part uses closed testing to test for statistical significance of each effect modifier while strongly controlling the familywise error rate. We apply this method on the Oregon Health Insurance Experiment, estimating the effect of Medicaid on the number of days an individual’s health does not impede their usual activities by using a randomized lottery as an instrument. Our method revealed Medicaid’s effect was most impactful among older, English-speaking, non-Asian males and younger, English-speaking individuals with, at most, a high school diploma or General Educational Development.},
  archive      = {J_AOAS},
  author       = {Michael Johnson and Jiongyi Cao and Hyunseung Kang},
  doi          = {10.1214/21-AOAS1535},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1111-1129},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Detecting heterogeneous treatment effects with instrumental variables and application to the oregon health insurance experiment},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kernel machine and distributed lag models for assessing
windows of susceptibility to environmental mixtures in children’s health
studies. <em>AOAS</em>, <em>16</em>(2), 1090–1110. (<a
href="https://doi.org/10.1214/21-AOAS1533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposures to environmental chemicals during gestation can alter health status later in life. Most studies of maternal exposure to chemicals during pregnancy have focused on a single chemical exposure observed at high temporal resolution. Recent research has turned to focus on exposure to mixtures of multiple chemicals, generally observed at a single time point. We consider statistical methods for analyzing data on chemical mixtures that are observed at a high temporal resolution. As motivation, we analyze the association between exposure to four ambient air pollutants observed weekly throughout gestation and birth weight in a Boston-area prospective birth cohort. To explore patterns in the data, we first apply methods for analyzing data on: (1) a single chemical observed at high temporal resolution, and (2) a mixture measured at a single point in time. We highlight the shortcomings of these approaches for temporally-resolved data on exposure to chemical mixtures. Second, we propose a novel method, a Bayesian kernel machine regression distributed lag model (BKMR-DLM) that simultaneously accounts for nonlinear associations and interactions among time-varying measures of exposure to mixtures. BKMR-DLM uses a functional weight for each exposure that parameterizes the window of susceptibility corresponding to that exposure within a kernel machine framework that captures nonlinear and interaction effects of the multivariate exposure on the outcome. In a simulation study we show that the proposed method can better estimate the exposure-response function and, in high signal settings, can identify critical windows in time during which exposure has an increased association with the outcome. Applying the proposed method to the Boston birth cohort data, we find evidence of a negative association between organic carbon and birth weight and that nitrate modifies the organic carbon, elemental carbon, and sulfate exposure-response functions.},
  archive      = {J_AOAS},
  author       = {Ander Wilson and Hsiao-Hsien Leon Hsu and Yueh-Hsiu Mathilda Chiu and Robert O. Wright and Rosalind J. Wright and Brent A. Coull},
  doi          = {10.1214/21-AOAS1533},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1090-1110},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Kernel machine and distributed lag models for assessing windows of susceptibility to environmental mixtures in children’s health studies},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sparse negative binomial classifier with covariate
adjustment for RNA-seq data. <em>AOAS</em>, <em>16</em>(2), 1071–1089.
(<a href="https://doi.org/10.1214/21-AOAS1532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning methods have been increasingly used in biomedical research and clinical practice. In transcriptomic applications, RNA-seq data have become dominating and have gradually replaced traditional microarray, due to their reduced background noise and increased digital precision. Most existing machine learning methods are, however, designed for continuous intensities of microarray and are not suitable for RNA-seq count data. In this paper we develop a negative binomial model via generalized linear model framework with double regularization for gene and covariate sparsity to accommodate three key elements: adequate modeling of count data with overdispersion, gene selection and adjustment for covariate effect. The proposed sparse negative binomial classifier (snbClass) is evaluated in simulations and two real applications of multidisease postmortem brain tissue RNA-seq data and cervical tumor miRNA-seq data to demonstrate its superior performance in prediction accuracy and feature selection.},
  archive      = {J_AOAS},
  author       = {Tanbin Rahman and Hsin-En Huang and Yujia Li and An-Shun Tai and Wen-Ping Hseih and Colleen A. McClung and George Tseng},
  doi          = {10.1214/21-AOAS1532},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1071-1089},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A sparse negative binomial classifier with covariate adjustment for RNA-seq data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust bayesian inference for big data: Combining
sensor-based records with traditional survey data. <em>AOAS</em>,
<em>16</em>(2), 1038–1070. (<a
href="https://doi.org/10.1214/21-AOAS1531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data often presents as massive nonprobability samples. Not only is the selection mechanism often unknown but larger data volume amplifies the relative contribution of selection bias to total error. Existing bias adjustment approaches assume that the conditional mean structures have been correctly specified for the selection indicator or key substantive measures. In the presence of a reference probability sample, these methods rely on a pseudolikelihood method to account for the sampling weights of the reference sample, which is parametric in nature. Under a Bayesian framework, handling the sampling weights is an even bigger hurdle. To further protect against model misspecification, we expand the idea of double robustness such that more flexible nonparametric methods as well as Bayesian models can be used for prediction. In particular, we employ Bayesian additive regression trees which not only capture nonlinear associations automatically but permit direct quantification of the uncertainty of point estimates through its posterior predictive draws. We apply our method to sensor-based naturalistic driving data from the second Strategic Highway Research Program using the 2017 National Household Travel Survey as a benchmark.},
  archive      = {J_AOAS},
  author       = {Ali Rafei and Carol A. C. Flannagan and Brady T. West and Michael R. Elliott},
  doi          = {10.1214/21-AOAS1531},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1038-1070},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Robust bayesian inference for big data: Combining sensor-based records with traditional survey data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible sensitivity analysis approach for unmeasured
confounding with multiple treatments and a binary outcome with
application to SEER-medicare lung cancer data. <em>AOAS</em>,
<em>16</em>(2), 1014–1037. (<a
href="https://doi.org/10.1214/21-AOAS1530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of a randomized experiment, a key assumption for drawing causal inference about treatment effects is the ignorable treatment assignment. Violations of the ignorability assumption may lead to biased treatment effect estimates. Sensitivity analysis helps gauge how causal conclusions will be altered in response to the potential magnitude of departure from the ignorability assumption. However, sensitivity analysis approaches for unmeasured confounding in the context of multiple treatments and binary outcomes are scarce. We propose a flexible Monte Carlo sensitivity analysis approach for causal inference in such settings. We first derive the general form of the bias introduced by unmeasured confounding, with emphasis on theoretical properties uniquely relevant to multiple treatments. We then propose methods to encode the impact of unmeasured confounding on potential outcomes and adjust the estimates of causal effects in which the presumed unmeasured confounding is removed. Our proposed methods embed nested multiple imputation within the Bayesian framework, which allow for seamless integration of the uncertainty about the values of the sensitivity parameters and the sampling variability as well as use of the Bayesian Additive Regression Trees for modeling flexibility. Expansive simulations validate our methods and gain insight into sensitivity analysis with multiple treatments. We use the SEER-Medicare data to demonstrate sensitivity analysis using three treatments for early stage nonsmall cell lung cancer. The methods developed in this work are readily available in the R package SAMTx.},
  archive      = {J_AOAS},
  author       = {Liangyuan Hu and Jungang Zou and Chenyang Gu and Jiayi Ji and Michael Lopez and Minal Kale},
  doi          = {10.1214/21-AOAS1530},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1014-1037},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A flexible sensitivity analysis approach for unmeasured confounding with multiple treatments and a binary outcome with application to SEER-medicare lung cancer data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inverse reinforcement learning for collective
animal movement. <em>AOAS</em>, <em>16</em>(2), 999–1013. (<a
href="https://doi.org/10.1214/21-AOAS1529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based methods allow for defining simple rules that generate complex group behaviors. The governing rules of such models are typically set a priori, and parameters are tuned from observed behavior trajectories. Instead of making simplifying assumptions across all anticipated scenarios, inverse reinforcement learning provides inference on the short-term (local) rules governing long-term behavior policies by using properties of a Markov decision process. We use the computationally efficient linearly-solvable Markov decision process to learn the local rules governing collective movement for a simulation of the selfpropelled-particle (SPP) model and a data application for a captive guppy population. The estimation of the behavioral decision costs is done in a Bayesian framework with basis function smoothing. We recover the true costs in the SPP simulation and find the guppies value collective movement more than targeted movement toward shelter.},
  archive      = {J_AOAS},
  author       = {Toryn L. J. Schafer and Christopher K. Wikle and Mevin B. Hooten},
  doi          = {10.1214/21-AOAS1529},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {999-1013},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian inverse reinforcement learning for collective animal movement},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistate capture–recapture models for irregularly sampled
data. <em>AOAS</em>, <em>16</em>(2), 982–998. (<a
href="https://doi.org/10.1214/21-AOAS1528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate capture-recapture data comprise individual-specific sighting histories, together with information on individuals’ states related, for example, to breeding status, infection level, or geographical location. Such data are often analysed using the Arnason–Schwarz model, where transitions between states are modelled using a discrete-time Markov chain, making the model most easily applicable to regular time series. When time intervals between capture occasions are not of equal length, more complex time-dependent constructions may be required, increasing the number of parameters to estimate, decreasing interpretability, and potentially leading to reduced precision. Here we develop a multi-state model based on a state process operating in continuous time, which can be regarded as an analogue of the discrete-time Arnason–Schwarz model for irregularly sampled data. Statistical inference is carried out by regarding the capture-recapture data as realisations from a continuous-time hidden Markov model, which allows the associated efficient algorithms to be used for maximum likelihood estimation and state decoding. To illustrate the feasibility of the modelling framework, we use a long-term survey of bottlenose dolphins where capture occasions are not regularly spaced through time. Here, we are particularly interested in seasonal effects on the movement rates of the dolphins along the Scottish east coast. The results reveal seasonal movement patterns between two core areas of their range, providing information that will inform conservation management.},
  archive      = {J_AOAS},
  author       = {Sina Mews and Roland Langrock and Ruth King and Nicola Quick},
  doi          = {10.1214/21-AOAS1528},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {982-998},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multistate capture–recapture models for irregularly sampled data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for stochastic kinetic models from multiple data
sources for joint estimation of infection dynamics from aggregate
reports and virological data. <em>AOAS</em>, <em>16</em>(2), 959–981.
(<a href="https://doi.org/10.1214/21-AOAS1527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before the current pandemic, influenza and respiratory syncytial virus (RSV) were the leading etiological agents of seasonal acute respiratory infections (ARI) around the world. In this setting, medical doctors typically based the diagnosis of ARI on patients’ symptoms alone and did not routinely conduct virological tests necessary to identify individual viruses, limiting the ability to study the interaction between multiple pathogens and to make public health recommendations. We consider a stochastic kinetic model (SKM) for two interacting ARI pathogens circulating in a large population and an empirically-motivated background process for infections with other pathogens causing similar symptoms. An extended marginal sampling approach, based on the linear noise approximation to the SKM, integrates multiple data sources and additional model components. We infer the parameters defining the pathogens’ dynamics and interaction within a Bayesian model and explore the posterior trajectories of infections for each illness based on aggregate infection reports from six epidemic seasons collected by the state health department and a subset of virological tests from a sentinel program at a general hospital in San Luis Potosí, México. We interpret the results and make recommendations for future data collection strategies.},
  archive      = {J_AOAS},
  author       = {Oksana A. Chkrebtii and Yury E. García and Marcos A. Capistrán and Daniel E. Noyola},
  doi          = {10.1214/21-AOAS1527},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {959-981},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inference for stochastic kinetic models from multiple data sources for joint estimation of infection dynamics from aggregate reports and virological data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Permutation tests under a rotating sampling plan with
clustered data. <em>AOAS</em>, <em>16</em>(2), 936–958. (<a
href="https://doi.org/10.1214/21-AOAS1526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of lumber strength of any grade may evolve, for example, due to climate change, forest fire, changes in processing methods, and other factors. So, in North America the forest products industry monitors the evolution of their means, percentiles, or other parameters to ensure the wood products meet the industrial standard. For administrative convenience and informativeness, one may adopt a rotating sampling plan by sampling 36 mills in the initial occasion and having six of them replaced in each successive occasion for the next five occasions. The strength data on a specified number, commonly 10 pieces of lumbers from each sampled mills, are obtained. Under such rotating plans the observations on pieces from the same mill are correlated, and the observations on samples from the same mill taken on different occasions are also correlated. Ignoring these correlations may lead to invalid inference procedures. Yet accommodating a cluster structure in parametric models is difficult and entails a high level of misspecification risk. In this paper we explore symmetry in the clustered data collected via a rotating sampling plan to develop a permutation scheme for testing various hypotheses of interest. We also introduce a semiparametric density ratio model to link the distributions of the response variable over time. The combination retains the validity of the inference methods while extracting maximum information from the sampling plan. A simulation study indicates that the proposed permutation tests firmly control the type I error whether or not the data are clustered. The use of the density ratio model improves the power of the tests. We also apply the proposed tests to data from the motivating application. The proposed permutation tests effectively address many real-world issues with trust worth inference conclusions.},
  archive      = {J_AOAS},
  author       = {Jiahua Chen and Yukun Liu and Carilyn G. Taylor and James V. Zidek},
  doi          = {10.1214/21-AOAS1526},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {936-958},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Permutation tests under a rotating sampling plan with clustered data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate bayesian inference for analysis of
spatiotemporal flood frequency data. <em>AOAS</em>, <em>16</em>(2),
905–935. (<a href="https://doi.org/10.1214/21-AOAS1525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme floods cause casualties and widespread damage to property and vital civil infrastructure. Predictions of extreme floods, within gauged and ungauged catchments, is crucial to mitigate these disasters. In this paper a Bayesian framework is proposed for predicting extreme floods, using the generalized extreme-value (GEV) distribution. A major methodological challenge is to find a suitable parametrization for the GEV distribution when multiple covariates and/or latent spatial effects are involved and a time trend is present. Other challenges involve balancing model complexity and parsimony, using an appropriate model selection procedure and making inference based on a reliable and computationally efficient approach. We here propose a latent Gaussian modeling framework with a novel multivariate link function designed to separate the interpretation of the parameters at the latent level and to avoid unreasonable estimates of the shape and time trend parameters. Structured additive regression models, which include catchment descriptors as covariates and spatially correlated model components, are proposed for the four parameters at the latent level. To achieve computational efficiency with large datasets and richly parametrized models, we exploit a highly accurate and fast approximate Bayesian inference approach which can also be used to efficiently select models separately for each of the four regression models at the latent level. We applied our proposed methodology to annual peak river flow data from 554 catchments across the United Kingdom. The framework performed well in terms of flood predictions for both ungauged catchments and future observations at gauged catchments. The results show that the spatial model components for the transformed location and scale parameters as well as the time trend are all important, and none of these should be ignored. Posterior estimates of the time trend parameters correspond to an average increase of about 1.5\% per decade with range 0.1\% to 2.8\% and reveal a spatial structure across the United Kingdom. When the interest lies in estimating return levels for spatial aggregates, we further develop a novel copula-based postprocessing approach of posterior predictive samples in order to mitigate the effect of the conditional independence assumption at the data level, and we demonstrate that our approach indeed provides accurate results.},
  archive      = {J_AOAS},
  author       = {Árni V. Jóhannesson and Stefan Siegert and Raphaël Huser and Haakon Bakka and Birgir Hrafnkelsson},
  doi          = {10.1214/21-AOAS1525},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {905-935},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Approximate bayesian inference for analysis of spatiotemporal flood frequency data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computationally efficient bayesian unit-level models for
non-gaussian data under informative sampling with application to
estimation of health insurance coverage. <em>AOAS</em>, <em>16</em>(2),
887–904. (<a href="https://doi.org/10.1214/21-AOAS1524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical estimates from survey samples have traditionally been obtained via design-based estimators. In many cases these estimators tend to work well for quantities, such as population totals or means, but can fall short as sample sizes become small. In today’s “information age,” there is a strong demand for more granular estimates. To meet this demand, using a Bayesian pseudolikelihood, we propose a computationally efficient unit-level modeling approach for non-Gaussian data collected under informative sampling designs. Specifically, we focus on binary and multinomial data. Our approach is both multivariate and multiscale, incorporating spatial dependence at the area level. We illustrate our approach through an empirical simulation study and through a motivating application to health insurance estimates, using the American Community Survey.},
  archive      = {J_AOAS},
  author       = {Paul A. Parker and Scott H. Holan and Ryan Janicki},
  doi          = {10.1214/21-AOAS1524},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {887-904},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Computationally efficient bayesian unit-level models for non-gaussian data under informative sampling with application to estimation of health insurance coverage},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse block signal detection and identification for shared
cross-trait association analysis. <em>AOAS</em>, <em>16</em>(2),
866–886. (<a href="https://doi.org/10.1214/21-AOAS1523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genome-wide association studies (GWAS) have identified thousands of single nucleotide polymorphisms (SNPs) that are associated with complex traits. GWAS data allows us to investigate the shared genetic etiologies among different traits. However, linkage disequilibrium (LD) between the SNPs complicates the detection and identification of shared genetic effects. In this paper we model the LD by dividing the genome into LD blocks and linking the genetic variants within a block to a possible latent causal variant. An eigenvector-projected score statistic that leverages the set of variants in LD and a maxtype test statistic (Max-block) are proposed to detect the existence of cross-trait genetic association. The Max-block is easy to calculate and is shown to control the genome-wide error rate. After the detection a stepwise procedure is proposed to identify the significant blocks that explain the genetic sharing between two traits. Simulation experiments show that Max-block is more powerful than standard approaches in the sparse settings and is robust to different signal strengths or levels of sparsity. The method is applied to study shared cross-trait associations in 10 pediatric autoimmune diseases and identified several regions that explain the genetic sharing between juvenile idiopathic arthritis (JIA) and ulcerative colitis (UC) and between UC and Crohn’s disease (CD). In addition, our analysis also indicates the genetic sharing in the MHC region among systemic lupus (SLE), celiac disease (CEL) and common variable immunodeficiency (CVID). Results from real data and simulation studies show that Max-block provides an important alternative to commonly used genetic correlation estimation in understanding genetic correlation among complex diseases.},
  archive      = {J_AOAS},
  author       = {Jianqiao Wang and Wanjie Wang and Hongzhe Li},
  doi          = {10.1214/21-AOAS1523},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {866-886},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sparse block signal detection and identification for shared cross-trait association analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intensity estimation on geometric networks with penalized
splines. <em>AOAS</em>, <em>16</em>(2), 843–865. (<a
href="https://doi.org/10.1214/21-AOAS1522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades the growing amount of network data lead to many novel statistical models. In this paper we consider so-called geometric networks. Typical examples are road networks or other infrastructure networks. Nevertheless, the neurons or the blood vessels in a human body can also be interpreted as a geometric network embedded in a three-dimensional space. A network-specific metric, rather than the Euclidean metric, is usually used in all these applications, making the analyses of network data challenging. We consider network-based point processes, and our task is to estimate the intensity (or density) of the process which allows us to detect high- and low-intensity regions of the underlying stochastic processes. Available routines that tackle this problem are commonly based on kernel smoothing methods. This paper uses penalized spline smoothing and extends this toward smooth intensity estimation on geometric networks. Furthermore, our approach easily allows incorporating covariates, enabling us to respect the network geometry in a regression model framework. Several data examples and a simulation study show that penalized spline-based intensity estimation on geometric networks is a numerically stable and efficient tool. Furthermore, it also allows estimating linear and smooth covariate effects, distinguishing our approach from already existing methodologies.},
  archive      = {J_AOAS},
  author       = {Marc Schneble and Göran Kauermann},
  doi          = {10.1214/21-AOAS1522},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {843-865},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Intensity estimation on geometric networks with penalized splines},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch-sequential design and heteroskedastic surrogate
modeling for delta smelt conservation. <em>AOAS</em>, <em>16</em>(2),
816–842. (<a href="https://doi.org/10.1214/21-AOAS1521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delta smelt is an endangered fish species in the San Francisco estuary that have shown an overall population decline over the past 30 years. Researchers have developed a stochastic, agent-based simulator to virtualize the system with the goal of understanding the relative contribution of natural and anthropogenic factors that might play a role in their decline. However, the input configuration space is high dimensional, running the simulator is time-consuming, and its noisy outputs change nonlinearly in both mean and variance. Getting enough runs to effectively learn input–output dynamics requires both a nimble modeling strategy and parallel evaluation. Recent advances in heteroskedastic Gaussian process (HetGP) surrogate modeling helps, but little is known about how to appropriately plan experiments for highly distributed simulation. We propose a batch sequential design scheme, generalizing one-at-a-time variance-based active learning for HetGP, as a means of keeping multicore cluster nodes fully engaged with runs. Our acquisition strategy is carefully engineered to favor selection of replicates which boost statistical and computational efficiency when training surrogates to isolate signal from noise. Design and modeling are illustrated on a range of toy examples before embarking on a large-scale smelt simulation campaign and downstream high-fidelity input sensitivity analysis.},
  archive      = {J_AOAS},
  author       = {Boya Zhang and Robert B. Gramacy and Leah R. Johnson and Kenneth A. Rose and Eric Smith},
  doi          = {10.1214/21-AOAS1521},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {816-842},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Batch-sequential design and heteroskedastic surrogate modeling for delta smelt conservation},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inhomogeneous spatio-temporal point processes on linear
networks for visitors’ stops data. <em>AOAS</em>, <em>16</em>(2),
791–815. (<a href="https://doi.org/10.1214/21-AOAS1519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the spatio-temporal distribution of visitors’ stops by touristic attractions in Palermo (Italy), using theory of stochastic point processes living on linear networks. We first propose an inhomogeneous Poisson point process model with a separable parametric spatio-temporal first-order intensity. We account for the spatial interaction among points on the given network, fitting a Gibbs point process model with mixed effects for the purely spatial component. This allows us to study first-order and second-order properties of the point pattern, accounting both for the spatio-temporal clustering and interaction and for the spatio-temporal scale at which they operate. Due to the strong degree of clustering in the data, we then formulate a more complex model, fitting a spatio-temporal log-Gaussian Cox process to the point process on the linear network, addressing the problem of the choice of the most appropriate distance metric.},
  archive      = {J_AOAS},
  author       = {Nicoletta D’Angelo and Giada Adelfio and Antonino Abbruzzo and Jorge Mateu},
  doi          = {10.1214/21-AOAS1519},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {791-815},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inhomogeneous spatio-temporal point processes on linear networks for visitors’ stops data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composite mixture of log-linear models with application to
psychiatric studies. <em>AOAS</em>, <em>16</em>(2), 765–790. (<a
href="https://doi.org/10.1214/21-AOAS1515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychiatric studies of suicide provide fundamental insights on the evolution of severe psychopathologies, and contribute to the development of early treatment interventions. Our focus is on modelling different traits of psychosis and their interconnections, focusing on a case study on suicide attempt survivors. Such aspects are recorded via multivariate categorical data, involving a large numbers of items for multiple subjects. Current methods for multivariate categorical data—such as penalized log-linear models and latent structure analysis—are either limited to low-dimensional settings or include parameters with difficult interpretation. Motivated by this application, this article proposes a new class of approaches, which we refer to as Mixture of Log Linear models (mills). Combining latent class analysis and log-linear models, mills defines a novel Bayesian approach to model complex multivariate categorical data with flexibility and interpretability, providing interesting insights on the relationship between psychotic diseases and psychological aspects in suicide attempt survivors.},
  archive      = {J_AOAS},
  author       = {Emanuele Aliverti and David B. Dunson},
  doi          = {10.1214/21-AOAS1515},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {765-790},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Composite mixture of log-linear models with application to psychiatric studies},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive design for gaussian process regression under
censoring. <em>AOAS</em>, <em>16</em>(2), 744–764. (<a
href="https://doi.org/10.1214/21-AOAS1512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key objective in engineering problems is to predict an unknown experimental surface over an input domain. In complex physical experiments this may be hampered by response censoring which results in a significant loss of information. For such problems, experimental design is paramount for maximizing predictive power using a small number of expensive experimental runs. To tackle this, we propose a novel adaptive design method, called the integrated censored mean-squared error (ICMSE) method. The ICMSE method first estimates the posterior probability of a new observation being censored, then adaptively chooses design points that minimize predictive uncertainty under censoring. Adopting a Gaussian process regression model with product correlation function, the proposed ICMSE criterion is easy to evaluate which allows for efficient design optimization. We demonstrate the effectiveness of the ICMSE design in two real-world applications on surgical planning and wafer manufacturing.},
  archive      = {J_AOAS},
  author       = {Jialei Chen and Simon Mak and V. Roshan Joseph and Chuck Zhang},
  doi          = {10.1214/21-AOAS1512},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {744-764},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Adaptive design for gaussian process regression under censoring},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable change-point and anomaly detection in
cross-correlated data with an application to condition monitoring.
<em>AOAS</em>, <em>16</em>(2), 721–743. (<a
href="https://doi.org/10.1214/21-AOAS1508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a condition monitoring application arising from subsea engineering, we derive a novel, scalable approach to detecting anomalous mean structure in a subset of correlated multivariate time series. Given the need to analyse such series efficiently, we explore a computationally efficient approximation of the maximum likelihood solution to the resulting modelling framework and develop a new dynamic programming algorithm for solving the resulting binary quadratic programme when the precision matrix of the time series at any given time point is banded. Through a comprehensive simulation study we show that the resulting methods perform favorably compared to competing methods, both in the anomaly and change detection settings, even when the sparsity structure of the precision matrix estimate is misspecified. We also demonstrate its ability to correctly detect faulty time periods of a pump within the motivating application.},
  archive      = {J_AOAS},
  author       = {Martin Tveten and Idris A. Eckley and Paul Fearnhead},
  doi          = {10.1214/21-AOAS1508},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {721-743},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Scalable change-point and anomaly detection in cross-correlated data with an application to condition monitoring},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The assessment of replication success based on relative
effect size. <em>AOAS</em>, <em>16</em>(2), 706–720. (<a
href="https://doi.org/10.1214/21-AOAS1502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replication studies are increasingly conducted in order to confirm original findings. However, there is no established standard how to assess replication success, and, in practice, many different approaches are used. The purpose of this paper is to refine and extend a recently proposed reverse-Bayes approach for the analysis of replication studies. We show how this method is directly related to the relative effect size, the ratio of the replication to the original effect estimate. This perspective leads to a new proposal to recalibrate the assessment of replication success, the golden level. The recalibration ensures that, for borderline significant original studies, replication success can only be achieved if the replication effect estimate is larger than the original one. Conditional power for replication success can then take any desired value if the original study is significant and the replication sample size is large enough. Compared to the standard approach to require statistical significance of both the original and replication study, replication success at the golden level offers uniform gains in project power and controls the type-I error rate if the replication sample size is not smaller than the original one. An application to data from four large replication projects shows that the new approach leads to more appropriate inferences, as it penalizes shrinkage of the replication estimate, compared to the original one, while ensuring that both effect estimates are sufficiently convincing on their own.},
  archive      = {J_AOAS},
  author       = {Leonhard Held and Charlotte Micheloud and Samuel Pawel},
  doi          = {10.1214/21-AOAS1502},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {706-720},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The assessment of replication success based on relative effect size},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bayesian model of dose-response for cancer drug studies.
<em>AOAS</em>, <em>16</em>(2), 680–705. (<a
href="https://doi.org/10.1214/21-AOAS1485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory cancer drug studies test multiple tumor cell lines against multiple candidate drugs. The goal in each paired (cell line, drug) experiment is to map out the dose-response curve of the cell line as the dose level of the drug increases. We propose Bayesian tensor filtering (BTF), a hierarchical Bayesian model for dose-response modeling in multisample, multitreatment cancer drug studies. BTF uses low-dimensional embeddings to share statistical strength between similar drugs and similar cell lines. Structured shrinkage priors in BTF encourage smoothness in the dose-response curves while remaining adaptive to sharp jumps when the data call for it. We focus on a pair of cancer drug studies exhibiting a particular pathology in their experimental design, leading us to a nonconjugate monotone mixture-of-gammas likelihood. To perform posterior inference, we develop a variant of the elliptical slice sampling algorithm for sampling from linearly-constrained multivariate normal priors with nonconjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art methods for covariance regression and dynamic Poisson matrix factorization. On the two cancer drug studies, BTF outperforms the current standard approach in biology and reveals potential new biomarkers of drug sensitivity in cancer. Code is available at https://github.com/tansey/functionalmf.},
  archive      = {J_AOAS},
  author       = {Wesley Tansey and Christopher Tosh and David M. Blei},
  doi          = {10.1214/21-AOAS1485},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {680-705},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian model of dose-response for cancer drug studies},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving exoplanet detection power: Multivariate gaussian
process models for stellar activity. <em>AOAS</em>, <em>16</em>(2),
652–679. (<a href="https://doi.org/10.1214/21-AOAS1471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The radial velocity method is one of the most successful techniques for detecting exoplanets. It works by detecting the velocity of a host star, induced by the gravitational effect of an orbiting planet, specifically, the velocity along our line of sight which is called the radial velocity of the star. Low-mass planets typically cause their host star to move with radial velocities of 1 m/s or less. By analyzing a time series of stellar spectra from a host star, modern astronomical instruments can, in theory, detect such planets. However, in practice, intrinsic stellar variability (e.g., star spots, convective motion, pulsations) affects the spectra and often mimics a radial velocity signal. This signal contamination makes it difficult to reliably detect low-mass planets. A principled approach to recovering planet radial velocity signals in the presence of stellar activity was proposed by Rajpaul et al. (Mon. Not. R. Astron. Soc. 452 (2015) 2269–2291). It uses a multivariate Gaussian process model to jointly capture time series of the apparent radial velocity and multiple indicators of stellar activity. We build on this work in two ways: (i) we propose using dimension reduction techniques to construct new high-information stellar activity indicators; and (ii) we extend the Rajpaul et al. (Mon. Not. R. Astron. Soc. 452 (2015) 2269–2291) model to a larger class of models and use a power-based model comparison procedure to select the best model. Despite significant interest in exoplanets, previous efforts have not performed large-scale stellar activity model selection or attempted to evaluate models based on planet detection power. In the case of main sequence G2V stars, we find that our method substantially improves planet detection power, compared to previous state-of-the-art approaches.},
  archive      = {J_AOAS},
  author       = {David E. Jones and David C. Stenning and Eric B. Ford and Robert L. Wolpert and Thomas J. Loredo and Christian Gilbertson and Xavier Dumusque},
  doi          = {10.1214/21-AOAS1471},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {652-679},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Improving exoplanet detection power: Multivariate gaussian process models for stellar activity},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelled approximations to the ideal filter with application
to GDP and its components. <em>AOAS</em>, <em>16</em>(2), 627–651. (<a
href="https://doi.org/10.1214/21-AOAS1463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines cyclical fluctuations in a comprehensive statistical application, focusing on U.S. macroeconomic indicators related to real gross domestic product (real GDP). While GDP is generally viewed as the most widespread measure of economic activity available, our dataset also encompasses the primary GDP components, such as investment, together with leading (and regularly analyzed) subcomponent series, like residential and inventory investment. Analysis of the cycles in these major sectors provides a more informative perspective on the macroeconomic state and may improve a researcher’s ability to understand and forecast cyclical movements and growth in GDP. Adaptive time series modelling is used for each time series to derive the preferred band-pass filter for computing the optimal cycle. This contrasts with the rigid use of the ideal filter, whose gain function is perfectly sharp. Regarding the ideal filter, we provide an improved implementation compared to current practice. Thus, a set of approximating filters is derived that allow for a more attractive gain profile, a better match to the targeted passband, and a direct statistical way to extract signals near the sample endpoints. Our application study demonstrates that the commonly used ideal filter can perform quite poorly on a routine basis and lead to incorrect conclusions about even the most basic questions about empirical cyclical properties. The amplitude of filtered economic activity can have major distortions and become expanded or diminished (depending on the GDP component under consideration), and many essential divergences in path may occur and affect key signals, such as expansion or contraction in growth. Statistical measures of model performance very strongly favor the adaptive parameter approach. Our statistical analysis reveals diverse dynamic behavior among the series; such results may yield worthwhile insights for output sector analysts and, even for those primarily focused on GDP, may lead to possible modelling improvements by using the finer information content in the GDP-component dynamics.},
  archive      = {J_AOAS},
  author       = {Thomas M. Trimbur and Tucker S. McElroy},
  doi          = {10.1214/21-AOAS1463},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {627-651},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modelled approximations to the ideal filter with application to GDP and its components},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction to: A bayesian model of microbiome data for
simultaneous identification of covariate associations and prediction of
phenotypic outcomes. <em>AOAS</em>, <em>16</em>(1), 625. (<a
href="https://doi.org/10.1214/21-AOAS1573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Matthew D. Koslovsky and Kristi L. Hoffman and Carrie R. Daniel and Marina Vannucci},
  doi          = {10.1214/21-AOAS1573},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {625},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Correction to: A bayesian model of microbiome data for simultaneous identification of covariate associations and prediction of phenotypic outcomes},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood-based bacterial identification approach for
bimicrobial mass spectrometry data. <em>AOAS</em>, <em>16</em>(1),
612–624. (<a href="https://doi.org/10.1214/21-AOAS1520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass spectrometry is a potential diagnostic tool for rapid bacterial detection. However, in order to use this technology in clinical settings, it is important to develop sound statistical algorithms that can accurately analyze polymicrobial mass spectrometry data. Here, we propose a likelihood-based bacterial identification algorithm for bimicrobial mass spectrometry data. Specifically, we introduce a two-component mixture model with partially known labels. This method can model peaks with unknown origins. It also considers errors in mass-to-charge ratios and intensities of peaks between observed and reference mass spectra. Coupled with a decoy strategy, the likelihood is used to identify bacterial species and to measure uncertainty of such identifications. Using two real mass spectrometry datasets, we demonstrate the superior performance of our approach in accurate bacterial identifications, compared to model-free approaches. Example datasets and R codes for the proposed method are freely available under MIT license at https://github.com/soyoungryu/BacID.},
  archive      = {J_AOAS},
  author       = {So Young Ryu},
  doi          = {10.1214/21-AOAS1520},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {612-624},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Likelihood-based bacterial identification approach for bimicrobial mass spectrometry data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accounting for drop-out using inverse probability censoring
weights in longitudinal clustered data with informative cluster size.
<em>AOAS</em>, <em>16</em>(1), 596–611. (<a
href="https://doi.org/10.1214/21-AOAS1518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodontal disease is a serious gum infection impacting half of the U.S. adult population that may lead to loss of teeth. Using standard marginal models to study the association between patient-level predictors and tooth-level outcomes can lead to biased estimates because the independence assumption between the outcome (periodontal disease) and cluster size (number of teeth per patient) is violated. Specifically, the baseline number of teeth of a patient is informative. In this setting a cluster-weighted generalized estimating equations (CWGEE) approach can be used to obtain unbiased marginal inference from data with informative cluster size (ICS). However, in many longitudinal studies of dental health, including the Veterans Affairs Dental Longitudinal Study, the rate of tooth-loss or tooth drop-out over time is also informative, creating a missing at random data mechanism. Here, we propose a novel modeling approach that incorporates the technique of inverse probability censoring weights into CWGEE with binary outcomes to account for ICS and informative drop-out over time. In an extensive simulation study we demonstrate that results obtained from our proposed method yield lower bias and excellent coverage probability, compared to those obtained from traditional methods which do not account for ICS or drop-out.},
  archive      = {J_AOAS},
  author       = {Aya A. Mitani and Elizabeth K. Kaye and Kerrie P. Nelson},
  doi          = {10.1214/21-AOAS1518},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {596-611},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for drop-out using inverse probability censoring weights in longitudinal clustered data with informative cluster size},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian mitigation of spatial coarsening for a hawkes model
applied to gunfire, wildfire and viral contagion. <em>AOAS</em>,
<em>16</em>(1), 573–595. (<a
href="https://doi.org/10.1214/21-AOAS1517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-exciting spatiotemporal Hawkes processes have found increasing use in the study of large-scale public health threats, ranging from gun violence and earthquakes to wildfires and viral contagion. Whereas many such applications feature locational uncertainty, that is, the exact spatial positions of individual events are unknown, most Hawkes model analyses to date have ignored spatial coarsening present in the data. Three particular 21st century public health crises—urban gun violence, rural wildfires and global viral spread—present qualitatively and quantitatively varying uncertainty regimes that exhibit: (a) different collective magnitudes of spatial coarsening, (b) uniform and mixed magnitude coarsening, (c) differently shaped uncertainty regions and—less orthodox—(d) locational data distributed within the “wrong” effective space. We explicitly model such uncertainties in a Bayesian manner and jointly infer unknown locations together with all parameters of a reasonably flexible Hawkes model, obtaining results that are practically and statistically distinct from those obtained while ignoring spatial coarsening. This work also features two different secondary contributions: first, to facilitate Bayesian inference of locations and background rate parameters, we make a subtle yet crucial change to an established kernel-based rate model, and second, to facilitate the same Bayesian inference at scale, we develop a massively parallel implementation of the model’s log-likelihood gradient with respect to locations and thus avoid its quadratic computational cost in the context of Hamiltonian Monte Carlo. Our examples involve thousands of observations and allow us to demonstrate practicality at moderate scales.},
  archive      = {J_AOAS},
  author       = {Andrew J. Holbrook and Xiang Ji and Marc A. Suchard},
  doi          = {10.1214/21-AOAS1517},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {573-595},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian mitigation of spatial coarsening for a hawkes model applied to gunfire, wildfire and viral contagion},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partitioning around medoids clustering and random forest
classification for GIS-informed imputation of fluoride concentration
data. <em>AOAS</em>, <em>16</em>(1), 551–572. (<a
href="https://doi.org/10.1214/21-AOAS1516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community water fluoridation is an important component of oral health promotion, as fluoride exposure is a well-documented dental caries-preventive agent. Direct measurements of domestic water fluoride content provide valuable information regarding individuals’ fluoride exposure and thus caries risk; however, they are logistically challenging to carry out at a large scale in oral health research. This article describes the development and evaluation of a novel method for the imputation of missing domestic water fluoride concentration data informed by spatial autocorrelation. The context is a state-wide epidemiologic study of pediatric oral health in North Carolina, where domestic water fluoride concentration information was missing for approximately 75\% of study participants with clinical data on dental caries. A new machine-learning-based imputation method that combines partitioning around medoids clustering and random forest classification (PAMRF) is developed and implemented. Imputed values are filtered according to allowable error rates or target sample size, depending on the requirements of each application. In leave-one-out cross-validation and simulation studies, PAMRF outperforms four existing imputation approaches—two conventional spatial interpolation methods (i.e., inverse-distance weighting, IDW and universal kriging, UK) and two supervised learning methods (k-nearest neighbors, KNN, and classification and regression trees, CART). The inclusion of multiply imputed values in the estimation of the association between fluoride concentration and dental caries prevalence resulted in essentially no change in PAMRF estimates but substantial gains in precision due to larger effective sample size. PAMRF is a powerful new method for the imputation of missing fluoride values where geographical information exists.},
  archive      = {J_AOAS},
  author       = {Yu Gu and John S. Preisser and Donglin Zeng and Poojan Shrestha and Molina Shah and Miguel A. Simancas-Pallares and Jeannie Ginnis and Kimon Divaris},
  doi          = {10.1214/21-AOAS1516},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {551-572},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Partitioning around medoids clustering and random forest classification for GIS-informed imputation of fluoride concentration data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ordinal probit functional outcome regression with
application to computer-use behavior in rhesus monkeys. <em>AOAS</em>,
<em>16</em>(1), 537–550. (<a
href="https://doi.org/10.1214/21-AOAS1513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in functional regression has made great strides in expanding to non-Gaussian functional outcomes, but exploration of ordinal functional outcomes remains limited. Motivated by a study of computer-use behavior in rhesus macaques (Macaca mulatta), we introduce the ordinal probit functional outcome regression model (OPFOR). OPFOR models can be fit using one of several basis functions including penalized B-splines, wavelets, and O’Sullivan splines—the last of which typically performs best. Simulation using a variety of underlying covariance patterns shows that the model performs reasonably well in estimation under multiple basis functions with near nominal coverage for joint credible intervals. Finally, in application we use Bayesian model selection criteria adapted to functional outcome regression to best characterize the relation between several demographic factors of interest and the monkeys’ computer use over the course of a year. In comparison with a standard ordinal longitudinal analysis, OPFOR outperforms a cumulative-link mixed-effects model in simulation and provides additional and more nuanced information on the nature of the monkeys’ computer-use behavior.},
  archive      = {J_AOAS},
  author       = {Mark J. Meyer and Jeffrey S. Morris and Regina Paxton Gazes and Brent A. Coull},
  doi          = {10.1214/21-AOAS1513},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {537-550},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Ordinal probit functional outcome regression with application to computer-use behavior in rhesus monkeys},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying intergenerational patterns of correlated
methylation sites. <em>AOAS</em>, <em>16</em>(1), 521–536. (<a
href="https://doi.org/10.1214/21-AOAS1511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA methylation can be transmitted through generations. This paper proposes a clustering method to identify the intergenerational patterns from parents to their offspring. Motivated by the potential of correlation between DNA methylation sites, we use the multivariate generalized beta distribution to model the blockwise correlation structure among the sites. A stochastic EM algorithm is implemented to estimate the parameters, and BIC is applied to determine the optimal number of clusters. Simulations demonstrate the feasibility of the proposed method. We further applied the approach to cluster DNA methylation data generated from a cohort study on asthma and allergic conditions.},
  archive      = {J_AOAS},
  author       = {Xichen Mou and Hongmei Zhang and S. Hasan Arshad},
  doi          = {10.1214/21-AOAS1511},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {521-536},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identifying intergenerational patterns of correlated methylation sites},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of hereditary cancers using neural networks.
<em>AOAS</em>, <em>16</em>(1), 495–520. (<a
href="https://doi.org/10.1214/21-AOAS1510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Family history is a major risk factor for many types of cancer. Mendelian risk prediction models translate family histories into cancer risk predictions, based on knowledge of cancer susceptibility genes. These models are widely used in clinical practice to help identify high-risk individuals. Mendelian models leverage the entire family history, but they rely on many assumptions about cancer susceptibility genes that are either unrealistic or challenging to validate, due to low mutation prevalence. Training more flexible models, such as neural networks, on large databases of pedigrees can potentially lead to accuracy gains. In this paper we develop a framework to apply neural networks to family history data and investigate their ability to learn inherited susceptibility to cancer. While there is an extensive literature on neural networks and their state-of-the-art performance in many tasks, there is little work applying them to family history data. We propose adaptations of fully-connected neural networks and convolutional neural networks to pedigrees. In data simulated under Mendelian inheritance, we demonstrate that our proposed neural network models are able to achieve nearly optimal prediction performance. Moreover, when the observed family history includes misreported cancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO model embedding the correct inheritance laws. Using a large dataset of over 200,000 family histories, the Risk Service cohort, we train prediction models for future risk of breast cancer. We validate the models using data from the Cancer Genetics Network.},
  archive      = {J_AOAS},
  author       = {Zoe Guan and Giovanni Parmigiani and Danielle Braun and Lorenzo Trippa},
  doi          = {10.1214/21-AOAS1510},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {495-520},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Prediction of hereditary cancers using neural networks},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting and modeling changes in a time series of
proportions. <em>AOAS</em>, <em>16</em>(1), 477–494. (<a
href="https://doi.org/10.1214/21-AOAS1509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework to detect and model shifts in a time series of continuous proportions, that is, a vector of proportions measuring the parts of a whole. By reparameterizing the shape of a Dirichlet distribution, we can model the location and scale separately through generalized linear models. A hidden Markov model allows the coefficients of the generalized linear models to change, thus allowing for the time series to undergo multiple regimes. This framework allows a practitioner to adequately model seasonality, trends, or include covariate information as well as detect change points. The model’s behavior is studied via simulation and through the analysis of lake phytoplankton data from 1992 through 2012. Our analyses demonstrate that the model can be effective in detecting and modeling changes in a time series of proportions. Pertaining to the phytoplankton data, the overall biomass has grown with some changes to the community level dynamics occurring circa 2000. Specifically, the proportion of cyanobacteria appears to have increased to the detriment of diatoms.},
  archive      = {J_AOAS},
  author       = {Thomas J. Fisher and Jing Zhang and Stephen P. Colegate and Michael J. Vanni},
  doi          = {10.1214/21-AOAS1509},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {477-494},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Detecting and modeling changes in a time series of proportions},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preelectoral polls variability: A hierarchical bayesian
model to assess the role of house effects with application to italian
elections. <em>AOAS</em>, <em>16</em>(1), 460–476. (<a
href="https://doi.org/10.1214/21-AOAS1507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely known that preelectoral polls often suffer from nonsampling errors that pollsters try to compensate for in final estimates by means of diverse ad hoc adjustments, thus leading to well-known house effects. We propose a Bayesian hierarchical model to investigate the role of house effects on the total variability of predictions. To illustrate the model, data from preelectoral polls in Italy in 2006, 2008 and 2013 are considered. Unlike alternative techniques or models, our proposal leads: (i) to correctly decompose the different sources of variability; (ii) to recognize the role of house effects; (iii) to evaluate its dynamics, showing that variability of house effects across pollsters diminishes as the date of election approaches; (iv) to investigate the relationship between house effects and overall prediction errors.},
  archive      = {J_AOAS},
  author       = {Domenico De Stefano and Francesco Pauli and Nicola Torelli},
  doi          = {10.1214/21-AOAS1507},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {460-476},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Preelectoral polls variability: A hierarchical bayesian model to assess the role of house effects with application to italian elections},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian adjustment for preferential testing in estimating
infection fatality rates, as motivated by the COVID-19 pandemic.
<em>AOAS</em>, <em>16</em>(1), 436–459. (<a
href="https://doi.org/10.1214/21-AOAS1499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in estimating the infection fatality rate (IFR), along with its relation with various factors of interest, is determining the total number of cases. The total number of cases is not known not only because not everyone is tested but also, more importantly, because tested individuals are not representative of the population at large. We refer to the phenomenon whereby infected individuals are more likely to be tested than noninfected individuals as “preferential testing.” An open question is whether or not it is possible to reliably estimate the IFR without any specific knowledge about the degree to which the data are biased by preferential testing. In this paper we take a partial identifiability approach, formulating clearly where deliberate prior assumptions can be made and presenting a Bayesian model which pools information from different samples. When the model is fit to European data obtained from seroprevalence studies and national official COVID-19 statistics, we estimate the overall COVID-19 IFR for Europe to be 0.53\%, 95\% C.I.=[0.38\%,0.70\%].},
  archive      = {J_AOAS},
  author       = {Harlan Campbell and Perry de Valpine and Lauren Maxwell and Valentijn M. T. de Jong and Thomas P. A. Debray and Thomas Jaenisch and Paul Gustafson},
  doi          = {10.1214/21-AOAS1499},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {436-459},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian adjustment for preferential testing in estimating infection fatality rates, as motivated by the COVID-19 pandemic},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimating the effectiveness of permanent price reductions
for competing products using multivariate bayesian structural time
series models. <em>AOAS</em>, <em>16</em>(1), 414–435. (<a
href="https://doi.org/10.1214/21-AOAS1498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Florence branch of an Italian supermarket chain recently implemented a strategy that permanently lowered the price of numerous store brands in several product categories. To quantify the impact of such a policy change, researchers often use synthetic control methods for estimating causal effects when a subset of units receive a single persistent treatment and the rest are unaffected by the change. In our applications, however, competitor brands not assigned to treatment are likely impacted by the intervention because of substitution effects; more broadly, this type of interference occurs whenever the treatment assignment of one unit affects the outcome of another. This paper extends the synthetic control methods to accommodate partial interference, allowing interference within predefined groups but not between them. Focusing on a class of causal estimands that capture the effect both on the treated and control units, we develop a multivariate Bayesian structural time series model for generating synthetic controls that would have occurred in the absence of an intervention, enabling us to estimate our novel effects. In a simulation study we explore our Bayesian procedures’ empirical properties and show that it achieves good frequentists coverage, even when the model is misspecified. We use our new methodology to make causal statements about the impact on sales of the affected store brands and their direct competitors. Our proposed approach is implemented in the CausalMBSTS R package.},
  archive      = {J_AOAS},
  author       = {Fiammetta Menchetti and Iavor Bojinov},
  doi          = {10.1214/21-AOAS1498},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {414-435},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the effectiveness of permanent price reductions for competing products using multivariate bayesian structural time series models},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate mixed membership modeling: Inferring
domain-specific risk profiles. <em>AOAS</em>, <em>16</em>(1), 391–413.
(<a href="https://doi.org/10.1214/21-AOAS1496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing the shared memberships of individuals in a classification scheme poses severe interpretability issues, even when using a moderate number of classes (say four). Mixed membership models quantify this phenomenon, but they typically focus on goodness-of-fit more than on interpretable inference. To achieve a good numerical fit, these models may, in fact, require many extreme profiles, making the results difficult to interpret. We introduce a new class of multivariate mixed membership models that, when variables can be partitioned into subject-matter based domains, can provide a good fit to the data using fewer profiles than standard formulations. The proposed model explicitly accounts for the blocks of variables corresponding to the distinct domains along with a cross-domain correlation structure which provides new information about shared membership of individuals in a complex classification scheme. We specify a multivariate logistic normal distribution for the membership vectors which allows easy introduction of auxiliary information leveraging a latent multivariate logistic regression. A Bayesian approach to inference, relying on Pólya gamma data augmentation, facilitates efficient posterior computation via Markov chain Monte Carlo. We apply this methodology to a spatially explicit study of malaria risk over time on the Brazilian Amazon frontier.},
  archive      = {J_AOAS},
  author       = {Massimiliano Russo and Burton H. Singer and David B. Dunson},
  doi          = {10.1214/21-AOAS1496},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {391-413},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multivariate mixed membership modeling: Inferring domain-specific risk profiles},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manifold valued data analysis of samples of networks, with
applications in corpus linguistics. <em>AOAS</em>, <em>16</em>(1),
368–390. (<a href="https://doi.org/10.1214/21-AOAS1480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks arise in many applications, such as in the analysis of text documents, social interactions and brain activity. We develop a general framework for extrinsic statistical analysis of samples of networks, motivated by networks representing text documents in corpus linguistics. We identify networks with their graph Laplacian matrices for which we define metrics, embeddings, tangent spaces and a projection from Euclidean space to the space of graph Laplacians. This framework provides a way of computing means, performing principal component analysis, regression, and carrying out hypothesis tests, such as for testing for equality of means between two samples of networks. We apply the methodology to the set of novels by Jane Austen and Charles Dickens.},
  archive      = {J_AOAS},
  author       = {Katie E. Severn and Ian L. Dryden and Simon P. Preston},
  doi          = {10.1214/21-AOAS1480},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {368-390},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Manifold valued data analysis of samples of networks, with applications in corpus linguistics},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-game win probabilities for the national rugby league.
<em>AOAS</em>, <em>16</em>(1), 349–367. (<a
href="https://doi.org/10.1214/21-AOAS1514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops new methods for providing instantaneous in-game win probabilities for the National Rugby League. Besides the score differential, betting odds, and real-time features extracted from the match event data are also used as inputs to inform the in-game win probabilities. Rugby matches evolve continuously in time, and the circumstances change over the duration of the match. Therefore, the match data are considered as functional data, and the in-game win probability is a function of the time of the match. We express the in-game win probability using a conditional probability formulation, the components of which are evaluated from the perspective of functional data analysis. Specifically, we model the score differential process and functional feature extracted from the match event data as sums of mean functions and noises. The mean functions are approximated by B-spline basis expansions with functional parameters. Since each match is conditional on a unique kickoff win probability of the home team obtained from the betting odds (i.e., the functional data are not independent and identically distributed), we propose a weighted least squares method to estimate the functional parameters by borrowing the information from matches with similar kickoff win probabilities. The variance and covariance elements are obtained by the maximum likelihood estimation method. The proposed method is applicable to other sports when suitable match event data are available.},
  archive      = {J_AOAS},
  author       = {Tianyu Guan and Robert Nguyen and Jiguo Cao and Tim Swartz},
  doi          = {10.1214/21-AOAS1514},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {349-367},
  shortjournal = {Ann. Appl. Stat.},
  title        = {In-game win probabilities for the national rugby league},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The role of intrinsic dimension in high-resolution player
tracking data—insights in basketball. <em>AOAS</em>, <em>16</em>(1),
326–348. (<a href="https://doi.org/10.1214/21-AOAS1506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the introduction of high-resolution player tracking technology, a new range of statistical analysis has emerged in sports, specifically in basketball. However, such high-dimensional data are often challenging for statistical inference and decision making. In this article we employ a state-of-the-art Bayesian mixture model that allows the estimation of heterogeneous intrinsic dimension (ID) within a dataset, and we propose some theoretical enhancements. Informally, the ID can be seen as an indicator of complexity and dependence of the data at hand, and it is usually assumed unique. Our method provides the capacity to reveal valuable insights about the hidden dynamics of sports interactions in space and time which helps to translate complex patterns into more coherent statistics. The application of this technique is illustrated using NBA basketball players’ tracking data, allowing effective classification and clustering. In movement data the analysis identified key stages of offensive actions, such as creating space for passing, preparation/shooting, and following through which are relevant for invasion sports. We found that the ID value spikes, reaching a peak between four and eight seconds in the offensive part of the court, after which it declines. In shot charts we obtained groups of shots that produce substantially higher and lower successes. Overall, game-winners tend to have a larger intrinsic dimension, indicative of greater unpredictability and unique shot placements. Similarly, we found higher ID values in plays when the score margin is smaller rather than larger. The exploitation of these results can bring clear strategic advantages in sports games.},
  archive      = {J_AOAS},
  author       = {Edgar Santos-Fernandez and Francesco Denti and Kerrie Mengersen and Antonietta Mira},
  doi          = {10.1214/21-AOAS1506},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {326-348},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The role of intrinsic dimension in high-resolution player tracking data—Insights in basketball},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential modeling, monitoring, and forecasting of
streaming web traffic data. <em>AOAS</em>, <em>16</em>(1), 300–325. (<a
href="https://doi.org/10.1214/21-AOAS1505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce strategies for modeling, monitoring, and forecasting sequential web traffic data using flows from the Fox News website. In our analysis we consider a family of Poisson-gamma state space (PGSS) models that can accurately quantify the uncertainty exhibited by web traffic data, can provide fast sequential monitoring and prediction mechanisms for high frequency time intervals, and are computationally feasible when structural breaks are present. As such, we extend the family of PGSS models to include the state augmented (sa-)PGSS model whose state evolution structure is flexible and responsive to sudden changes. Such adaptability is achieved by augmenting the state vector of the PGSS model with an additional state variable for a time-varying discount factor. We develop an efficient particle-based estimation procedure that is suitable for sequential analysis, allowing us to estimate dynamic state variables and static parameters via closed-form conditional sufficient statistics. We compare the performance of the PGSS family of models against viable alternatives from the literature and argue that, especially in the presence of structural breaks, our proposed approach yields superior sequential model fit and predictive performance while preserving computational feasibility. We provide additional insights by designing a simulation study that mimics potential web traffic data patterns.},
  archive      = {J_AOAS},
  author       = {Kaoru Irie and Chris Glynn and Tevfik Aktekin},
  doi          = {10.1214/21-AOAS1505},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {300-325},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sequential modeling, monitoring, and forecasting of streaming web traffic data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling nonstationary temperature maxima based on extremal
dependence changing with event magnitude. <em>AOAS</em>, <em>16</em>(1),
272–299. (<a href="https://doi.org/10.1214/21-AOAS1504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modeling of spatiotemporal trends in temperature extremes can help better understand the structure and frequency of heatwaves in a changing climate and assess the environmental, societal, economic and health-related risks they entail. Here, we study annual temperature maxima over Southern Europe using a century-spanning dataset observed at 44 monitoring stations. Extending the spectral representation of max-stable processes, our modeling framework relies on a novel construction of max-infinitely divisible processes which include covariates to capture spatiotemporal nonstationarities. Our new model keeps a popular max-stable process on the boundary of the parameter space, while flexibly capturing weakening extremal dependence at increasing quantile levels and asymptotic independence. This is achieved by linking the overall magnitude of a spatial event to its spatial correlation range in such a way that more extreme events become less spatially dependent, thus more localized. Our model reveals salient features of the spatiotemporal variability of European temperature extremes, and it clearly outperforms natural alternative models. Results show that the spatial extent of heatwaves is smaller for more severe events at higher elevations and that recent heatwaves are moderately wider. Our probabilistic assessment of the 2019 annual maxima confirms the severity of the 2019 heatwaves both spatially and at individual sites, especially when compared to climatic conditions prevailing in 1950–1975. Our results could be exploited in practice to understand the spatiotemporal dynamics, severity and frequency of extreme heatwaves and to design suitable region-specific mitigation measures.},
  archive      = {J_AOAS},
  author       = {Peng Zhong and Raphaël Huser and Thomas Opitz},
  doi          = {10.1214/21-AOAS1504},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {272-299},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling nonstationary temperature maxima based on extremal dependence changing with event magnitude},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast inference for time-varying quantiles via flexible
dynamic models with application to the characterization of atmospheric
rivers. <em>AOAS</em>, <em>16</em>(1), 247–271. (<a
href="https://doi.org/10.1214/21-AOAS1497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric rivers (ARs) are elongated regions of water vapor in the atmosphere that play a key role in global water cycles, particularly in western U.S. precipitation. The primary component of many AR detection schemes is the thresholding of the integrated water vapor transport (IVT) magnitude at a single quantile over time. Utilizing a recently developed family of parametric distributions for quantile regression, this paper develops a flexible dynamic quantile linear model (exDQLM) which enables versatile, structured, and informative estimation of the IVT quantile threshold. A simulation study illustrates our exDQLM to be more robust than the standard Bayesian parametric quantile regression approach for nonstandard distributions, performing better in both quantile estimation and predictive accuracy. In addition to a Markov chain Monte Carlo (MCMC) algorithm, we develop an efficient importance sampling variational Bayes (ISVB) algorithm for fast approximate Bayesian inference which is found to produce comparable results to the MCMC in a fraction of the computation time. Further, we develop a transfer function extension to our exDQLM as a method for quantifying nonlinear relationships between a quantile of a climatological response and an input. The utility of our transfer function exDQLM is demonstrated in capturing both the immediate and lagged effects of El Niño Southern Oscillation Longitude Index on the estimation of the 0.85 quantile IVT.},
  archive      = {J_AOAS},
  author       = {Raquel Barata and Raquel Prado and Bruno Sansó},
  doi          = {10.1214/21-AOAS1497},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {247-271},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Fast inference for time-varying quantiles via flexible dynamic models with application to the characterization of atmospheric rivers},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A functional-data approach to the argo data. <em>AOAS</em>,
<em>16</em>(1), 216–246. (<a
href="https://doi.org/10.1214/21-AOAS1477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Argo data is a modern oceanography dataset that provides unprecedented global coverage of temperature and salinity measurements in the upper 2000 meters of depth of the ocean. We study the Argo data from the perspective of functional data analysis (FDA). We develop spatiotemporal functional kriging methodology for mean and covariance estimation to predict temperature and salinity at a fixed location as a smooth function of depth. By combining tools from FDA and spatial statistics, including smoothing splines, local regression, and multivariate spatial modeling and prediction, our approach provides advantages over current methodology that consider pointwise estimation at fixed depths. Our approach naturally leverages the irregularly-sampled data in space, time, and depth to fit a space-time functional model for temperature and salinity. The developed framework provides new tools to address fundamental scientific problems involving the entire upper water column of the oceans, such as the estimation of ocean heat content, stratification, and thermohaline oscillation. For example, we show that our functional approach yields more accurate ocean heat content estimates than ones based on discrete integral approximations in pressure. Further, using the derivative function estimates, we obtain a new product of a global map of the mixed layer depth, a key component in the study of heat absorption and nutrient circulation in the oceans. The derivative estimates also reveal evidence for density inversions in areas distinguished by mixing of particularly different water masses.},
  archive      = {J_AOAS},
  author       = {Drew Yarger and Stilian Stoev and Tailen Hsing},
  doi          = {10.1214/21-AOAS1477},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {216-246},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A functional-data approach to the argo data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidimensional linked matrix factorization for pan-omics
pan-cancer analysis. <em>AOAS</em>, <em>16</em>(1), 193–215. (<a
href="https://doi.org/10.1214/21-AOAS1495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several modern applications require the integration of multiple large data matrices that have shared rows and/or columns. For example, cancer studies that integrate multiple omics platforms across multiple types of cancer, pan-omics pan-cancer analysis, have extended our knowledge of molecular heterogeneity beyond what was observed in single tumor and single platform studies. However, these studies have been limited by available statistical methodology. We propose a flexible approach to the simultaneous factorization and decomposition of variation across such bidimensionally linked matrices, BIDIFAC+. BIDIFAC+ decomposes variation into a series of low-rank components that may be shared across any number of row sets (e.g., omics platforms) or column sets (e.g., cancer types). This builds on a growing literature for the factorization and decomposition of linked matrices which has primarily focused on multiple matrices that are linked in one dimension (rows or columns) only. Our objective function extends nuclear norm penalization, is motivated by random matrix theory, gives a unique decomposition under relatively mild conditions, and can be shown to give the mode of a Bayesian posterior distribution. We apply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and specific modes of variability across four different omics platforms and 29 different cancer types.},
  archive      = {J_AOAS},
  author       = {Eric F. Lock and Jun Young Park and Katherine A. Hoadley},
  doi          = {10.1214/21-AOAS1495},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {193-215},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bidimensional linked matrix factorization for pan-omics pan-cancer analysis},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse matrix linear models for structured high-throughput
data. <em>AOAS</em>, <em>16</em>(1), 169–192. (<a
href="https://doi.org/10.1214/21-AOAS1444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advancements have led to the rapid generation of high-throughput biological data which can be used to address novel scientific questions in broad areas of research. These data can be thought of as a large matrix with covariates annotating both its rows and columns. Matrix linear models provide a convenient way for modeling such data. In many situations, sparse estimation of these models is desired. We present fast, general methods for fitting sparse matrix linear models to structured high-throughput data. We induce model sparsity using an L1 penalty and consider the case when the response matrix and the covariate matrices are large. Due to data size, standard methods for estimation of these penalized regression models fail if the problem is converted to the corresponding univariate regression scenario. By leveraging matrix properties in the structure of our model, we develop several fast estimation algorithms (coordinate descent, FISTA and ADMM) and discuss their trade-offs. We evaluate our method’s performance on simulated data, E. coli chemical genetic screening data and two Arabidopsis genetic datasets with multivariate responses. Our algorithms have been implemented in the Julia programming language and are available at https://github.com/senresearch/MatrixLMnet.jl.},
  archive      = {J_AOAS},
  author       = {Jane W. Liang and Śaunak Sen},
  doi          = {10.1214/21-AOAS1444},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {169-192},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sparse matrix linear models for structured high-throughput data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian nonparametric multivariate spatial mixture mixed
effects models with application to american community survey special
tabulations. <em>AOAS</em>, <em>16</em>(1), 144–168. (<a
href="https://doi.org/10.1214/21-AOAS1494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging multivariate spatial dependence to improve the precision of estimates using American Community Survey data and other sample survey data has been a topic of recent interest among data users and federal statistical agencies. One strategy is to use a multivariate spatial mixed effects model with a Gaussian observation model and latent Gaussian process model. In practice, this works well for a wide range of tabulations. Nevertheless, in situations in which the data exhibit heterogeneity within or across geographies, and/or there is sparsity in the data, the Gaussian assumptions may be problematic and lead to underperformance. To remedy these situations, we propose a multivariate hierarchical Bayesian nonparametric mixed effects spatial mixture model to increase model flexibility. The number of clusters is chosen automatically in a data-driven manner. The effectiveness of our approach is demonstrated through a simulation study and motivating application of special tabulations for American Community Survey data.},
  archive      = {J_AOAS},
  author       = {Ryan Janicki and Andrew M. Raim and Scott H. Holan and Jerry J. Maples},
  doi          = {10.1214/21-AOAS1494},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {144-168},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian nonparametric multivariate spatial mixture mixed effects models with application to american community survey special tabulations},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible bayesian framework to estimate age- and
cause-specific child mortality over time from sample registration data.
<em>AOAS</em>, <em>16</em>(1), 124–143. (<a
href="https://doi.org/10.1214/21-AOAS1489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to implement disease-specific interventions in young age groups, policy makers in low- and middle-income countries require timely and accurate estimates of age- and cause-specific child mortality. High-quality data is not available in settings where these interventions are most needed, but there is a push to create sample registration systems that collect detailed mortality information. Current methods that estimate mortality from this data employ multistage frameworks without rigorous statistical justification that separately estimate all-cause and cause-specific mortality and are not sufficiently adaptable to capture important features of the data. We propose a flexible Bayesian modeling framework to estimate age- and cause-specific child mortality from sample registration data. We provide a theoretical justification for the framework, explore its properties via simulation, and use it to estimate mortality trends using data from the Maternal and Child Health Surveillance System in China.},
  archive      = {J_AOAS},
  author       = {Austin E. Schumacher and Tyler H. McCormick and Jon Wakefield and Yue Chu and Jamie Perin and Francisco Villavicencio and Noah Simon and Li Liu},
  doi          = {10.1214/21-AOAS1489},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {124-143},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A flexible bayesian framework to estimate age- and cause-specific child mortality over time from sample registration data},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference in bayesian additive vector autoregressive tree
models. <em>AOAS</em>, <em>16</em>(1), 104–123. (<a
href="https://doi.org/10.1214/21-AOAS1488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary nonlinear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. We apply our model to two datasets. The first application shows that the BAVART model yields highly competitive forecasts of the U.S. term structure of interest rates. In a second application we estimate our model using a moderately sized Eurozone dataset to investigate the dynamic effects of uncertainty on the economy.},
  archive      = {J_AOAS},
  author       = {Florian Huber and Luca Rossini},
  doi          = {10.1214/21-AOAS1488},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {104-123},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inference in bayesian additive vector autoregressive tree models},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgroup-effects models for the analysis of personal
treatment effects. <em>AOAS</em>, <em>16</em>(1), 80–103. (<a
href="https://doi.org/10.1214/21-AOAS1503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging field of precision medicine is transforming statistical analysis from the classical paradigm of population-average treatment effects into that of personal treatment effects. This new scientific mission has called for adequate statistical methods to assess heterogeneous covariate effects in regression analysis. This paper focuses on a subgroup analysis that consists of two primary analytic tasks: identification of treatment effect subgroups and individual group memberships, and statistical inference on treatment effects by subgroup. We propose an approach to synergizing supervised clustering analysis via alternating direction method of multipliers (ADMM) algorithm and statistical inference on subgroup effects via expectation-maximization (EM) algorithm. Our proposed procedure, termed as hybrid operation for subgroup analysis (HOSA), enjoys computational speed and numerical stability with interpretability and reproducibility. We establish key theoretical properties for both proposed clustering and inference procedures. Numerical illustration includes extensive simulation studies and analyses of motivating data from two randomized clinical trials to learn subgroup treatment effects.},
  archive      = {J_AOAS},
  author       = {Ling Zhou and Shiquan Sun and Haoda Fu and Peter X.-K. Song},
  doi          = {10.1214/21-AOAS1503},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {80-103},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Subgroup-effects models for the analysis of personal treatment effects},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bounding the local average treatment effect in an
instrumental variable analysis of engagement with a mobile intervention.
<em>AOAS</em>, <em>16</em>(1), 60–79. (<a
href="https://doi.org/10.1214/21-AOAS1476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of local average treatment effects in randomized trials typically relies upon the exclusion restriction assumption in cases where we are unwilling to rule out the possibility of unmeasured confounding. Under this assumption, treatment effects are mediated through the post-randomization variable being conditioned upon and directly attributable to neither the randomization itself nor its latent descendants. Recently, there has been interest in mobile health interventions to provide healthcare support. Mobile health interventions (e.g., the Rapid Encouragement/Education and Communications for Health, or REACH, designed to support self management for adults with type 2 diabetes) often involve both one-way and interactive messages. In practice, it is highly likely that any benefit from the intervention is achieved both through receipt of the intervention content and through engagement with/response to it. Application of an instrumental variable analysis in order to understand the role of engagement with REACH (or a similar intervention) requires the traditional exclusion restriction assumption to be relaxed. We propose a conceptually intuitive sensitivity analysis procedure for the REACH randomized trial that places bounds on local average treatment effects. Simulation studies reveal this approach to have desirable finite-sample behavior and to recover local average treatment effects under correct specification of sensitivity parameters.},
  archive      = {J_AOAS},
  author       = {Andrew J. Spieker and Robert A. Greevy and Lyndsay A. Nelson and Lindsay S. Mayberry},
  doi          = {10.1214/21-AOAS1476},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {60-79},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bounding the local average treatment effect in an instrumental variable analysis of engagement with a mobile intervention},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subgroup identification and variable selection for treatment
decision making. <em>AOAS</em>, <em>16</em>(1), 40–59. (<a
href="https://doi.org/10.1214/21-AOAS1468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When treatment effect heterogeneity exists, identifying the subgroup of patients who would benefit from an active treatment relative to a control is an important question. This article focuses on subgroup identification in the presence of a large dimensional set of covariates, with the number of covariates possibly greater than the sample size. We approach this problem from the perspective of optimal treatment decision rules and propose methods that can simultaneously estimate the treatment decision rule and select prescriptive variables important for treatment decision making and subgroup identification. The proposed methods are built within a robust classification framework based on doubly robust augmented inverse probability weighted estimators (AIPWE), hence sharing the robustness property. An L1 (lasso-type) penalty is used within the classification framework to target selection of prescriptive variables. We further propose a backward elimination process for fine-tuning selection. The methods can be conveniently implemented by taking advantage of standard software for logistic regression and lasso. The methods are evaluated by extensive simulation studies which demonstrated the superior and robust performance of the proposed methods relative to existing ones. In addition, the estimated decision rules from the proposed methods are considerably simpler than other methods. We applied various methods to identify the subgroup of patients suitable for each of the two commonly used anticoagulants in terms of bleeding risk for patients with acute myocardial infarction undergoing percutaneous coronary intervention.},
  archive      = {J_AOAS},
  author       = {Baqun Zhang and Min Zhang},
  doi          = {10.1214/21-AOAS1468},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {40-59},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Subgroup identification and variable selection for treatment decision making},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAGEL: A bayesian graphical model for inferring drug effect
longitudinally on depression in people with HIV. <em>AOAS</em>,
<em>16</em>(1), 21–39. (<a
href="https://doi.org/10.1214/21-AOAS1492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access and adherence to antiretroviral therapy (ART) has transformed the face of HIV infection from a fatal to a chronic disease. However, ART is also known for its side effects. Studies have reported that ART is associated with depressive symptomatology. Large-scale HIV clinical databases with individuals’ longitudinal depression records, ART medications, and clinical characteristics offer researchers unprecedented opportunities to study the effects of ART drugs on depression over time. We develop BAGEL, a Bayesian graphical model, to investigate longitudinal effects of ART drugs on a range of depressive symptoms while adjusting for participants’ demographic, behavior, and clinical characteristics, and taking into account the heterogeneous population through a Bayesian nonparametric prior. We evaluate BAGEL through simulation studies. Application to a dataset from the Women’s Interagency HIV Study yields interpretable and clinically useful results. BAGEL not only can improve our understanding of ART drugs’ effects on disparate depression symptoms but also has clinical utility in guiding informed and effective treatment selection to facilitate precision medicine in HIV.},
  archive      = {J_AOAS},
  author       = {Yuliang Li and Yang Ni and Leah H. Rubin and Amanda B. Spence and Yanxun Xu},
  doi          = {10.1214/21-AOAS1492},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {21-39},
  shortjournal = {Ann. Appl. Stat.},
  title        = {BAGEL: A bayesian graphical model for inferring drug effect longitudinally on depression in people with HIV},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust causal inference for incremental return on ad spend
with randomized paired geo experiments. <em>AOAS</em>, <em>16</em>(1),
1–20. (<a href="https://doi.org/10.1214/21-AOAS1493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the incremental return on ad spend (iROAS) of a prospective online marketing strategy (i.e., the ratio of the strategy’s causal effect on some response metric of interest relative to its causal effect on the ad spend) has become increasingly more important. Although randomized “geo experiments” are frequently employed for this evaluation, obtaining reliable estimates of iROAS can be challenging, as oftentimes only a small number of highly heterogeneous units are used. Moreover, advertisers frequently impose budget constraints on their ad spends which further complicates causal inference by introducing interference between the experimental units. In this paper we formulate a novel statistical framework for inferring the iROAS of online advertising from randomized paired geo experiment, which further motivates and provides new insights into Rosenbaum’s arguments on instrumental variables, and we propose and develop a robust, distribution-free and interpretable estimator “Trimmed Match” as well as a data-driven choice of the tuning parameter which may be of independent interest. We investigate the sensitivity of Trimmed Match to some violations of its assumptions and show that it can be more efficient than some alternative estimators based on simulated data. We then demonstrate its practical utility with real case studies.},
  archive      = {J_AOAS},
  author       = {Aiyou Chen and Timothy C. Au},
  doi          = {10.1214/21-AOAS1493},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Robust causal inference for incremental return on ad spend with randomized paired geo experiments},
  volume       = {16},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
