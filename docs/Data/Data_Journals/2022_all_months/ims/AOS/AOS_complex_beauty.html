<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aos---142">AOS - 142</h2>
<ul>
<li><details>
<summary>
(2022). The stein effect for fréchet means. <em>AOS</em>,
<em>50</em>(6), 3647–3676. (<a
href="https://doi.org/10.1214/22-AOS2245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fréchet mean is a useful description of location for a probability distribution on a metric space that is not necessarily a vector space. This article considers simultaneous estimation of multiple Fréchet means from a decision-theoretic perspective, and in particular, the extent to which the unbiased estimator of a Fréchet mean can be dominated by a generalization of the James–Stein shrinkage estimator. It is shown that if the metric space satisfies a nonpositive curvature condition, then this generalized James–Stein estimator asymptotically dominates the unbiased estimator as the dimension of the space grows. These results hold for a large class of distributions on a variety of spaces, including Hilbert spaces and, therefore, partially extend known results on the applicability of the James–Stein estimator to nonnormal distributions on Euclidean spaces. Simulation studies on phylogenetic trees and symmetric positive definite matrices are presented, numerically demonstrating the efficacy of this generalized James–Stein estimator.},
  archive      = {J_AOS},
  author       = {Andrew McCormack and Peter Hoff},
  doi          = {10.1214/22-AOS2245},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3647-3676},
  shortjournal = {Ann. Statist.},
  title        = {The stein effect for fréchet means},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On optimal block resampling for gaussian-subordinated
long-range dependent processes. <em>AOS</em>, <em>50</em>(6), 3619–3646.
(<a href="https://doi.org/10.1214/22-AOS2242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block-based resampling estimators have been intensively investigated for weakly dependent time processes, which has helped to inform implementation (e.g., best block sizes). However, little is known about resampling performance and block sizes under strong or long-range dependence. To establish guideposts in block selection, we consider a broad class of strongly dependent time processes, formed by a transformation of a stationary long-memory Gaussian series, and examine block-based resampling estimators for the variance of the prototypical sample mean; extensions to general statistical functionals are also considered. Unlike weak dependence, the properties of resampling estimators under strong dependence are shown to depend intricately on the nature of nonlinearity in the time series (beyond Hermite ranks) in addition to the long-memory coefficient and block size. Additionally, the intuition has often been that optimal block sizes should be larger under strong dependence (say O(n1/2) for a sample size n) than the optimal order O(n1/3) known under weak dependence. This intuition turns out to be largely incorrect, though a block order O(n1/2) may be reasonable (and even optimal) in many cases, owing to nonlinearity in a long-memory time series. While optimal block sizes are more complex under long-range dependence compared to short-range, we provide a consistent data-driven rule for block selection. Numerical studies illustrate that the guides for block selection perform well in other block-based problems with long-memory time series, such as distribution estimation and strategies for testing Hermite rank.},
  archive      = {J_AOS},
  author       = {Qihao Zhang and Soumendra N. Lahiri and Daniel J. Nordman},
  doi          = {10.1214/22-AOS2242},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3619-3646},
  shortjournal = {Ann. Statist.},
  title        = {On optimal block resampling for gaussian-subordinated long-range dependent processes},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of markov chains via weak poincaré inequalities
with application to pseudo-marginal MCMC. <em>AOS</em>, <em>50</em>(6),
3592–3618. (<a href="https://doi.org/10.1214/22-AOS2241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of a certain class of functional inequalities known as weak Poincaré inequalities to bound convergence of Markov chains to equilibrium. We show that this enables the straightforward and transparent derivation of subgeometric convergence bounds for methods such as the Independent Metropolis–Hastings sampler and pseudo-marginal methods for intractable likelihoods, the latter being subgeometric in many practical settings. These results rely on novel quantitative comparison theorems between Markov chains. Associated proofs are simpler than those relying on drift/minorisation conditions and the tools developed allow us to recover and further extend known results as particular cases. We are then able to provide new insights into the practical use of pseudo-marginal algorithms, analyse the effect of averaging in Approximate Bayesian Computation (ABC) and the use of products of independent averages and also to study the case of log-normal weights relevant to particle marginal Metropolis–Hastings (PMMH).},
  archive      = {J_AOS},
  author       = {Christophe Andrieu and Anthony Lee and Sam Power and Andi Q. Wang},
  doi          = {10.1214/22-AOS2241},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3592-3618},
  shortjournal = {Ann. Statist.},
  title        = {Comparison of markov chains via weak poincaré inequalities with application to pseudo-marginal MCMC},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The integrated copula spectrum. <em>AOS</em>,
<em>50</em>(6), 3563–3591. (<a
href="https://doi.org/10.1214/22-AOS2240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency domain methods form a ubiquitous part of the statistical toolbox for time-series analysis. In recent years, considerable interest has been given to the development of new spectral methodology and tools capturing dynamics in the entire joint distributions, and thus avoiding the limitations of classical, L2-based spectral methods. Most of the spectral concepts proposed in that literature suffer from one major drawback, though: their estimation requires the choice of a smoothing parameter, which has a considerable impact on estimation quality and poses challenges for statistical inference. In this paper, associated with the concept of a copula-based spectrum, we introduce the notion of a copula spectral distribution function or integrated copula spectrum. This integrated copula spectrum retains the advantages of copula-based spectra but can be estimated without the need for smoothing parameters. We provide such estimators, along with a thorough theoretical analysis, based on a functional central limit theorem, of their asymptotic properties. We leverage these results to test various hypotheses that cannot be addressed by classical spectral methods, such as the lack of time reversibility or asymmetry in tail dynamics.},
  archive      = {J_AOS},
  author       = {Yuichi Goto and Tobias Kley and Ria Van Hecke and Stanislav Volgushev and Holger Dette and Marc Hallin},
  doi          = {10.1214/22-AOS2240},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3563-3591},
  shortjournal = {Ann. Statist.},
  title        = {The integrated copula spectrum},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covariance estimation under one-bit quantization.
<em>AOS</em>, <em>50</em>(6), 3538–3562. (<a
href="https://doi.org/10.1214/22-AOS2239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the classical problem of estimating the covariance matrix of a sub-Gaussian distribution from i.i.d. samples in the novel context of coarse quantization, that is, instead of having full knowledge of the samples, they are quantized to one or two bits per entry. This problem occurs naturally in signal processing applications. We introduce new estimators in two different quantization scenarios and derive nonasymptotic estimation error bounds in terms of the operator norm. In the first scenario, we consider a simple, scale-invariant one-bit quantizer and derive an estimation result for the correlation matrix of a centered Gaussian distribution. In the second scenario, we add random dithering to the quantizer. In this case, we can accurately estimate the full covariance matrix of a general sub-Gaussian distribution by collecting two bits per entry of each sample. In both scenarios, our bounds apply to masked covariance estimation. We demonstrate the near optimality of our error bounds by deriving corresponding (minimax) lower bounds and using numerical simulations.},
  archive      = {J_AOS},
  author       = {Sjoerd Dirksen and Johannes Maly and Holger Rauhut},
  doi          = {10.1214/22-AOS2239},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3538-3562},
  shortjournal = {Ann. Statist.},
  title        = {Covariance estimation under one-bit quantization},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing for the rank of a covariance operator. <em>AOS</em>,
<em>50</em>(6), 3510–3537. (<a
href="https://doi.org/10.1214/22-AOS2238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we discern whether the covariance operator of a stochastic process is of reduced rank, and if so, what its precise rank is? And how can we do so at a given level of confidence? This question is central to a great deal of methods for functional data, which require low-dimensional representations whether by functional PCA or other methods. The difficulty is that the determination is to be made on the basis of i.i.d. replications of the process observed discretely and with measurement error contamination. This adds a ridge to the empirical covariance, obfuscating the underlying dimension. We build a matrix-completion inspired test statistic that circumvents this issue by measuring the best possible least square fit of the empirical covariance’s off-diagonal elements, optimised over covariances of given finite rank. For a fixed grid of sufficiently large size, we determine the statistic’s asymptotic null distribution as the number of replications grows. We then use it to construct a bootstrap implementation of a stepwise testing procedure controlling the familywise error rate corresponding to the collection of hypotheses formalising the question at hand. Under minimal regularity assumptions, we prove that the procedure is consistent and that its bootstrap implementation is valid. The procedure circumvents smoothing and associated smoothing parameters, is indifferent to measurement error heteroskedasticity, and does not assume a low-noise regime. An extensive simulation study reveals an excellent practical performance, stably across a wide range of settings and the procedure is further illustrated by means of two data analyses.},
  archive      = {J_AOS},
  author       = {Anirvan Charkaborty and Victor M. Panaretos},
  doi          = {10.1214/22-AOS2238},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3510-3537},
  shortjournal = {Ann. Statist.},
  title        = {Testing for the rank of a covariance operator},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sup-norm adaptive drift estimation for multivariate
nonreversible diffusions. <em>AOS</em>, <em>50</em>(6), 3484–3509. (<a
href="https://doi.org/10.1214/22-AOS2237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the question of estimating the drift for a large class of ergodic multivariate and possibly nonreversible diffusion processes, based on continuous observations, in sup-norm loss. Nonparametric classes of smooth functions of unknown order are considered, and we suggest an adaptive approach which allows to construct drift estimators attaining optimal sup-norm rates of convergence. Reversibility structures and related functional inequalities are known to be key tools for these estimation problems. We can discard such restrictions by making use of mixing properties which are satisfied for the very general class of processes under consideration. Analysing diffusions, the scalar case is very distinct from the general multivariate setting. Therefore, we treat scalar and multivariate processes separately which leads to in several aspects improved univariate results. While we consider drift estimation on bounded domains for exponentially β-mixing multivariate processes, for scalar diffusion processes we work under minimal assumptions that allow estimation of unbounded drift terms over the entire real line, and we provide classical minimax results (including lower bounds) which cannot be obtained under state-of-the-art conditions in the multivariate case. In addition, we prove a Donsker theorem for the classical kernel estimator of the invariant density in the scalar setting and establish its semiparametric efficiency.},
  archive      = {J_AOS},
  author       = {Cathrine Aeckerle-Willems and Claudia Strauch},
  doi          = {10.1214/22-AOS2237},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3484-3509},
  shortjournal = {Ann. Statist.},
  title        = {Sup-norm adaptive drift estimation for multivariate nonreversible diffusions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Choosing between persistent and stationary volatility.
<em>AOS</em>, <em>50</em>(6), 3466–3483. (<a
href="https://doi.org/10.1214/22-AOS2236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper suggests a multiplicative volatility model where volatility is decomposed into a stationary and a nonstationary persistent part. We provide a testing procedure to determine which type of volatility is prevalent in the data. The persistent part of volatility is associated with a nonstationary persistent process satisfying some smoothness and moment conditions. The stationary part is related to stationary conditional heteroskedasticity. We outline theory and conditions that allow the extraction of the persistent part from the data and enable standard conditional heteroskedasticity tests to detect stationary volatility after persistent volatility is taken into account. Monte Carlo results support the testing strategy in small samples. The empirical application of the theory supports the persistent volatility paradigm, suggesting that stationary conditional heteroskedasticity is considerably less pronounced than previously thought.},
  archive      = {J_AOS},
  author       = {Ilias Chronopoulos and Liudas Giraitis and George Kapetanios},
  doi          = {10.1214/22-AOS2236},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3466-3483},
  shortjournal = {Ann. Statist.},
  title        = {Choosing between persistent and stationary volatility},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rerandomization with diminishing covariate imbalance and
diverging number of covariates. <em>AOS</em>, <em>50</em>(6), 3439–3465.
(<a href="https://doi.org/10.1214/22-AOS2235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Completely randomized experiments have been the gold standard for drawing causal inference because they can balance all potential confounding on average. However, they may suffer from unbalanced covariates for realized treatment assignments. Rerandomization, a design that rerandomizes the treatment assignment until a prespecified covariate balance criterion is met, has recently got attention due to its easy implementation, improved covariate balance and more efficient inference. Researchers have then suggested to use the treatment assignments that minimize the covariate imbalance, namely the optimally balanced design. This has caused again the long-time controversy between two philosophies for designing experiments: randomization versus optimal, and thus almost deterministic designs. Existing literature argued that rerandomization with overly balanced observed covariates can lead to highly imbalanced unobserved covariates, making it vulnerable to model misspecification. On the contrary, rerandomization with properly balanced covariates can provide robust inference for treatment effects while sacrificing some efficiency compared to the ideally optimal design. In this paper, we show it is possible that, by making the covariate imbalance diminishing at a proper rate as the sample size increases, rerandomization can achieve its ideally optimal precision that one can expect with perfectly balanced covariates, while still maintaining its robustness. We further investigate conditions on the number of covariates for achieving the desired optimality. Our results rely on a more delicate asymptotic analysis for rerandomization, allowing both diminishing covariate imbalance threshold (or equivalently the acceptance probability) and diverging number of covariates. The derived theory for rerandomization provides a deeper understanding of its large-sample property and can better guide its practical implementation. Furthermore, it also helps reconcile the controversy between randomized and optimal designs in an asymptotic sense.},
  archive      = {J_AOS},
  author       = {Yuhao Wang and Xinran Li},
  doi          = {10.1214/22-AOS2235},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3439-3465},
  shortjournal = {Ann. Statist.},
  title        = {Rerandomization with diminishing covariate imbalance and diverging number of covariates},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic properties of high-dimensional random forests.
<em>AOS</em>, <em>50</em>(6), 3415–3438. (<a
href="https://doi.org/10.1214/22-AOS2234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a flexible nonparametric learning tool, the random forests algorithm has been widely applied to various real applications with appealing empirical performance, even in the presence of high-dimensional feature space. Unveiling the underlying mechanisms has led to some important recent theoretical results on the consistency of the random forests algorithm and its variants. However, to our knowledge, almost all existing works concerning random forests consistency in a high-dimensional setting were established for various modified random forests models where the splitting rules are independent of the response; a few exceptions assume simple data generating models with binary features. In light of this, in this paper we derive the consistency rates for the random forests algorithm associated with the sample CART splitting criterion, which is the one used in the original version of the algorithm (Mach. Learn. 45 (2001) 5–32), in a general high-dimensional nonparametric regression setting through a bias-variance decomposition analysis. Our new theoretical results show that random forests can indeed adapt to high dimensionality and allow for discontinuous regression function. Our bias analysis characterizes explicitly how the random forests bias depends on the sample size, tree height and column subsampling parameter. Some limitations on our current results are also discussed.},
  archive      = {J_AOS},
  author       = {Chien-Ming Chi and Patrick Vossler and Yingying Fan and Jinchi Lv},
  doi          = {10.1214/22-AOS2234},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3415-3438},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic properties of high-dimensional random forests},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local permutation tests for conditional independence.
<em>AOS</em>, <em>50</em>(6), 3388–3414. (<a
href="https://doi.org/10.1214/22-AOS2233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate local permutation tests for testing conditional independence between two random vectors X and Y given Z. The local permutation test determines the significance of a test statistic by locally shuffling samples, which share similar values of the conditioning variables Z, and it forms a natural extension of the usual permutation approach for unconditional independence testing. Despite its simplicity and empirical support, the theoretical underpinnings of the local permutation test remain unclear. Motivated by this gap, this paper aims to establish theoretical foundations of local permutation tests with a particular focus on binning-based statistics. We start by revisiting the hardness of conditional independence testing and provide an upper bound for the power of any valid conditional independence test, which holds when the probability of observing “collisions” in Z is small. This negative result naturally motivates us to impose additional restrictions on the possible distributions under the null and alternate. To this end, we focus our attention on certain classes of smooth distributions and identify provably tight conditions under which the local permutation method is universally valid, that is, it is valid when applied to any (binning-based) test statistic. To complement this result on type I error control, we also show that in some cases, a binning-based statistic calibrated via the local permutation method can achieve minimax optimal power. We also introduce a double-binning permutation strategy, which yields a valid test over less smooth null distributions than the typical single-binning method without compromising much power. Finally, we present simulation results to support our theoretical findings.},
  archive      = {J_AOS},
  author       = {Ilmun Kim and Matey Neykov and Sivaraman Balakrishnan and Larry Wasserman},
  doi          = {10.1214/22-AOS2233},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3388-3414},
  shortjournal = {Ann. Statist.},
  title        = {Local permutation tests for conditional independence},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Batch policy learning in average reward markov decision
processes. <em>AOS</em>, <em>50</em>(6), 3364–3387. (<a
href="https://doi.org/10.1214/22-AOS2231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the batch (off-line) policy learning problem in the infinite horizon Markov decision process. Motivated by mobile health applications, we focus on learning a policy that maximizes the long-term average reward. We propose a doubly robust estimator for the average reward and show that it achieves semiparametric efficiency. Further, we develop an optimization algorithm to compute the optimal policy in a parameterized stochastic policy class. The performance of the estimated policy is measured by the difference between the optimal average reward in the policy class and the average reward of the estimated policy and we establish a finite-sample regret guarantee. The performance of the method is illustrated by simulation studies and an analysis of a mobile health study promoting physical activity.},
  archive      = {J_AOS},
  author       = {Peng Liao and Zhengling Qi and Runzhe Wan and Predrag Klasnja and Susan A. Murphy},
  doi          = {10.1214/22-AOS2231},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3364-3387},
  shortjournal = {Ann. Statist.},
  title        = {Batch policy learning in average reward markov decision processes},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian fixed-domain asymptotics for covariance parameters
in a gaussian process model. <em>AOS</em>, <em>50</em>(6), 3334–3363.
(<a href="https://doi.org/10.1214/22-AOS2230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process models typically contain finite-dimensional parameters in the covariance function that need to be estimated from the data. We study the Bayesian fixed-domain asymptotics for the covariance parameters in a universal kriging model with an isotropic Matérn covariance function, which has many applications in spatial statistics. We show that when the dimension of domain is less than or equal to three, the joint posterior distribution of the microergodic parameter and the range parameter can be factored independently into the product of their marginal posteriors under fixed-domain asymptotics. The posterior of the microergodic parameter is asymptotically close in total variation distance to a normal distribution with shrinking variance, while the posterior distribution of the range parameter does not converge to any point mass distribution in general. Our theory allows an unbounded prior support for the range parameter and flexible designs of sampling points. We further study the asymptotic efficiency and convergence rates in posterior prediction for the Bayesian kriging predictor with covariance parameters randomly drawn from their posterior distribution. In the special case of one-dimensional Ornstein–Uhlenbeck process, we derive explicitly the limiting posterior of the range parameter and the posterior convergence rate for asymptotic efficiency in posterior prediction. We verify these asymptotic results in numerical experiments.},
  archive      = {J_AOS},
  author       = {Cheng Li},
  doi          = {10.1214/22-AOS2230},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3334-3363},
  shortjournal = {Ann. Statist.},
  title        = {Bayesian fixed-domain asymptotics for covariance parameters in a gaussian process model},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Likelihood estimation of sparse topic distributions in topic
models and its applications to wasserstein document distance
calculations. <em>AOS</em>, <em>50</em>(6), 3307–3333. (<a
href="https://doi.org/10.1214/22-AOS2229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the estimation of high-dimensional, discrete, possibly sparse, mixture models in the context of topic models. The data consists of observed multinomial counts of p words across n independent documents. In topic models, the p×n expected word frequency matrix is assumed to be factorized as a p×K word-topic matrix A and a K×n topic-document matrix T. Since columns of both matrices represent conditional probabilities belonging to probability simplices, columns of A are viewed as p-dimensional mixture components that are common to all documents while columns of T are viewed as the K-dimensional mixture weights that are document specific and are allowed to be sparse. The main interest is to provide sharp, finite sample, ℓ1-norm convergence rates for estimators of the mixture weights T when A is either known or unknown. For known A, we suggest MLE estimation of T. Our nonstandard analysis of the MLE not only establishes its ℓ1 convergence rate, but also reveals a remarkable property: the MLE, with no extra regularization, can be exactly sparse and contain the true zero pattern of T. We further show that the MLE is both minimax optimal and adaptive to the unknown sparsity in a large class of sparse topic distributions. When A is unknown, we estimate T by optimizing the likelihood function corresponding to a plug in, generic, estimator Aˆ of A. For any estimator Aˆ that satisfies carefully detailed conditions for proximity to A, we show that the resulting estimator of T retains the properties established for the MLE. Our theoretical results allow the ambient dimensions K and p to grow with the sample sizes. Our main application is to the estimation of 1-Wasserstein distances between document generating distributions. We propose, estimate and analyze new 1-Wasserstein distances between alternative probabilistic document representations, at the word and topic level, respectively. We derive finite sample bounds on the estimated proposed 1-Wasserstein distances. For word level document-distances, we provide contrast with existing rates on the 1-Wasserstein distance between standard empirical frequency estimates. The effectiveness of the proposed 1-Wasserstein distances is illustrated by an analysis of an IMDB movie reviews data set. Finally, our theoretical results are supported by extensive simulation studies.},
  archive      = {J_AOS},
  author       = {Xin Bing and Florentina Bunea and Seth Strimas-Mackey and Marten Wegkamp},
  doi          = {10.1214/22-AOS2229},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3307-3333},
  shortjournal = {Ann. Statist.},
  title        = {Likelihood estimation of sparse topic distributions in topic models and its applications to wasserstein document distance calculations},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The completion of covariance kernels. <em>AOS</em>,
<em>50</em>(6), 3281–3306. (<a
href="https://doi.org/10.1214/22-AOS2228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of positive-semidefinite continuation: extending a partially specified covariance kernel from a subdomain Ω of a rectangular domain I×I to a covariance kernel on the entire domain I×I. For a broad class of domains Ω called serrated domains, we are able to present a complete theory. Namely, we demonstrate that a canonical completion always exists and can be explicitly constructed. We characterise all possible completions as suitable perturbations of the canonical completion, and determine necessary and sufficient conditions for a unique completion to exist. We interpret the canonical completion via the graphical model structure it induces on the associated Gaussian process. Furthermore, we show how the estimation of the canonical completion reduces to the solution of a system of linear statistical inverse problems in the space of Hilbert–Schmidt operators, and derive rates of convergence. We conclude by providing extensions of our theory to more general forms of domains, and by demonstrating how our results can be used to construct covariance estimators from sample path fragments of the associated stochastic process. Our results are illustrated numerically by way of a simulation study and a real example.},
  archive      = {J_AOS},
  author       = {Kartik G. Waghmare and Victor M. Panaretos},
  doi          = {10.1214/22-AOS2228},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3281-3306},
  shortjournal = {Ann. Statist.},
  title        = {The completion of covariance kernels},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limiting distributions for eigenvalues of sample correlation
matrices from heavy-tailed populations. <em>AOS</em>, <em>50</em>(6),
3249–3280. (<a href="https://doi.org/10.1214/22-AOS2226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a p-dimensional population x∈Rp with i.i.d. coordinates that are regularly varying with index α∈(0,2). Since the variance of x is infinite, the diagonal elements of the sample covariance matrix Sn=n−1 ∑ i=1nxixi′ based on a sample x1,…,xn from the population tend to infinity as n increases and it is of interest to use instead the sample correlation matrix Rn={diag(Sn)}−1/2Sn{diag(Sn)}−1/2. This paper finds the limiting distributions of the eigenvalues of Rn when both the dimension p and the sample size n grow to infinity such that p/n→γ∈(0,∞). The family of limiting distributions {Hα,γ} is new and depends on the two parameters α and γ. The moments of Hα,γ are fully identified as sum of two contributions: the first from the classical Marčenko–Pastur law and a second due to heavy tails. Moreover, the family {Hα,γ} has continuous extensions at the boundaries α=2 and α=0 leading to the Marčenko–Pastur law and a modified Poisson distribution, respectively. Our proofs use the method of moments, the path-shortening algorithm developed in [18] (Stochastic Process. Appl. 128 (2018) 2779–2815) and some novel graph counting combinatorics. As a consequence, the moments of Hα,γ are expressed in terms of combinatorial objects such as Stirling numbers of the second kind. A simulation study on these limiting distributions Hα,γ is also provided for comparison with the Marčenko–Pastur law.},
  archive      = {J_AOS},
  author       = {Johannes Heiny and Jianfeng Yao},
  doi          = {10.1214/22-AOS2226},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3249-3280},
  shortjournal = {Ann. Statist.},
  title        = {Limiting distributions for eigenvalues of sample correlation matrices from heavy-tailed populations},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward theoretical understandings of robust markov decision
processes: Sample complexity and asymptotics. <em>AOS</em>,
<em>50</em>(6), 3223–3248. (<a
href="https://doi.org/10.1214/22-AOS2225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the nonasymptotic and asymptotic performances of the optimal robust policy and value function of robust Markov Decision Processes (MDPs), where the optimal robust policy and value function are estimated from a generative model. While prior work focusing on nonasymptotic performances of robust MDPs is restricted in the setting of the KL uncertainty set and (s,a)-rectangular assumption, we improve their results and also consider other uncertainty sets, including the L1 and χ2 balls. Our results show that when we assume (s,a)-rectangular on uncertainty sets, the sample complexity is about O˜(|S|2|A| ε2ρ2(1−γ)4). In addition, we extend our results from the (s,a)-rectangular assumption to the s-rectangular assumption. In this scenario, the sample complexity varies with the choice of uncertainty sets and is generally larger than the case under the (s,a)-rectangular assumption. Moreover, we also show that the optimal robust value function is asymptotically normal with a typical rate n under the (s,a) and s-rectangular assumptions from both theoretical and empirical perspectives.},
  archive      = {J_AOS},
  author       = {Wenhao Yang and Liangyu Zhang and Zhihua Zhang},
  doi          = {10.1214/22-AOS2225},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3223-3248},
  shortjournal = {Ann. Statist.},
  title        = {Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On resampling schemes for particle filters with weakly
informative observations. <em>AOS</em>, <em>50</em>(6), 3197–3222. (<a
href="https://doi.org/10.1214/22-AOS2222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider particle filters with weakly informative observations (or ‘potentials’) relative to the latent state dynamics. The particular focus of this work is on particle filters to approximate time-discretisations of continuous-time Feynman–Kac path integral models—a scenario that naturally arises when addressing filtering and smoothing problems in continuous time—but our findings are indicative about weakly informative settings beyond this context too. We study the performance of different resampling schemes, such as systematic resampling, SSP (Srinivasan sampling process) and stratified resampling, as the time-discretisation becomes finer and also identify their continuous-time limit, which is expressed as a suitably defined ‘infinitesimal generator.’ By contrasting these generators, we find that (certain modifications of) systematic and SSP resampling ‘dominate’ stratified and independent ‘killing’ resampling in terms of their limiting overall resampling rate. The reduced intensity of resampling manifests itself in lower variance in our numerical experiment. This efficiency result, through an ordering of the resampling rate, is new to the literature. The second major contribution of this work concerns the analysis of the limiting behaviour of the entire population of particles of the particle filter as the time discretisation becomes finer. We provide the first proof, under general conditions, that the particle approximation of the discretised continuous-time Feynman–Kac path integral models converges to a (uniformly weighted) continuous-time particle system.},
  archive      = {J_AOS},
  author       = {Nicolas Chopin and Sumeetpal S. Singh and Tomás Soto and Matti Vihola},
  doi          = {10.1214/22-AOS2222},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3197-3222},
  shortjournal = {Ann. Statist.},
  title        = {On resampling schemes for particle filters with weakly informative observations},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Half-trek criterion for identifiability of latent variable
models. <em>AOS</em>, <em>50</em>(6), 3174–3196. (<a
href="https://doi.org/10.1214/22-AOS2221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider linear structural equation models with latent variables and develop a criterion to certify whether the direct causal effects between the observable variables are identifiable based on the observed covariance matrix. Linear structural equation models assume that both observed and latent variables solve a linear equation system featuring stochastic noise terms. Each model corresponds to a directed graph whose edges represent the direct effects that appear as coefficients in the equation system. Prior research has developed a variety of methods to decide identifiability of direct effects in a latent projection framework, in which the confounding effects of the latent variables are represented by correlation among noise terms. This approach is effective when the confounding is sparse and effects only small subsets of the observed variables. In contrast, the new latent-factor half-trek criterion (LF-HTC) we develop in this paper operates on the original unprojected latent variable model and is able to certify identifiability in settings, where some latent variables may also have dense effects on many or even all of the observables. Our LF-HTC is an effective sufficient criterion for rational identifiability, under which the direct effects can be uniquely recovered as rational functions of the joint covariance matrix of the observed random variables. When restricting the search steps in LF-HTC to consider subsets of latent variables of bounded size, the criterion can be verified in time that is polynomial in the size of the graph.},
  archive      = {J_AOS},
  author       = {Rina Foygel Barber and Mathias Drton and Nils Sturma and Luca Weihs},
  doi          = {10.1214/22-AOS2221},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3174-3196},
  shortjournal = {Ann. Statist.},
  title        = {Half-trek criterion for identifiability of latent variable models},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization hierarchy for fair statistical decision
problems. <em>AOS</em>, <em>50</em>(6), 3144–3173. (<a
href="https://doi.org/10.1214/22-AOS2217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven decision making has drawn scrutiny from policy makers due to fears of potential discrimination, and a growing literature has begun to develop fair statistical techniques. However, these techniques are often specialized to one model context and based on ad hoc arguments, which makes it difficult to perform theoretical analysis. This paper develops an optimization hierarchy, which is a sequence of optimization problems with an increasing number of constraints, for fair statistical decision problems. Because our hierarchy is based on the framework of statistical decision problems, this means it provides a systematic approach for developing and studying fair versions of hypothesis testing, decision making, estimation, regression, and classification. We use the insight that qualitative definitions of fairness are equivalent to statistical independence between the output of a statistical technique and a random variable that measures attributes for which fairness is desired. We use this insight to construct an optimization hierarchy that lends itself to numerical computation, and we use tools from variational analysis and random set theory to prove that higher levels of this hierarchy lead to consistency in the sense that it asymptotically imposes this independence as a constraint in corresponding statistical decision problems. We demonstrate numerical effectiveness of our hierarchy using several data sets, and we use our hierarchy to fairly perform automated dosing of morphine.},
  archive      = {J_AOS},
  author       = {Anil Aswani and Matt Olfat},
  doi          = {10.1214/22-AOS2217},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3144-3173},
  shortjournal = {Ann. Statist.},
  title        = {Optimization hierarchy for fair statistical decision problems},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A no-free-lunch theorem for multitask learning.
<em>AOS</em>, <em>50</em>(6), 3119–3143. (<a
href="https://doi.org/10.1214/22-AOS2189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning and related areas such as multisource domain adaptation address modern settings where data sets from N related distributions {Pt} are to be combined toward improving performance on any single such distribution D. A perplexing fact remains in the evolving theory on the subject: while we would hope for performance bounds that account for the contribution from multiple tasks, the vast majority of analyses result in bounds that improve at best in the number n of samples per task, but most often do not improve in N. As such, it might seem at first that the distributional settings or aggregation procedures considered in such analyses might be somehow unfavorable; however, as we show, the picture happens to be more nuanced, with interestingly hard regimes that might appear otherwise favorable. In particular, we consider a seemingly favorable classification scenario where all tasks Pt share a common optimal classifier h∗, and which can be shown to admit a broad range of regimes with improved oracle rates in terms of N and n. Some of our main results are: ∙ We show that, even though such regimes admit minimax rates accounting for both n and N, no adaptive algorithm exists, that is, without access to distributional information, no algorithm can guarantee rates that improve with large N for n fixed. ∙ With a bit of additional information, namely, a ranking of tasks {Pt} according to their distance to a target D, a simple rank-based procedure can achieve near optimal aggregations of tasks’ data sets, despite a search space exponential in N. Interestingly, the optimal aggregation might exclude certain tasks, even though they all share the same h∗.},
  archive      = {J_AOS},
  author       = {Steve Hanneke and Samory Kpotufe},
  doi          = {10.1214/22-AOS2189},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3119-3143},
  shortjournal = {Ann. Statist.},
  title        = {A no-free-lunch theorem for multitask learning},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional calibration for false discovery rate control
under dependence. <em>AOS</em>, <em>50</em>(6), 3091–3118. (<a
href="https://doi.org/10.1214/21-AOS2137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of methods for finite-sample false discovery rate (FDR) control in multiple testing problems with dependent test statistics where the dependence is known. Our approach separately calibrates a data-dependent p-value rejection threshold for each hypothesis, relaxing or tightening the threshold as appropriate to target exact FDR control. In addition to our general framework, we propose a concrete algorithm, the dependence-adjusted Benjamini–Hochberg (dBH) procedure, which thresholds the BH-adjusted p-value for each hypothesis. Under positive regression dependence, the dBH procedure uniformly dominates the standard BH procedure, and in general it uniformly dominates the Benjamini–Yekutieli (BY) procedure (also known as BH with log correction), which makes a conservative adjustment for worst-case dependence. Simulations and real data examples show substantial power gains over the BY procedure, and competitive performance with knockoffs in settings where both methods are applicable. When the BH procedure empirically controls FDR (as it typically does in practice), the dBH procedure performs comparably.},
  archive      = {J_AOS},
  author       = {William Fithian and Lihua Lei},
  doi          = {10.1214/21-AOS2137},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3091-3118},
  shortjournal = {Ann. Statist.},
  title        = {Conditional calibration for false discovery rate control under dependence},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correction note: “Asymptotic spectral theory for nonlinear
time series.” <em>AOS</em>, <em>50</em>(5), 3088–3089. (<a
href="https://doi.org/10.1214/22-AOS2206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Yi Zhang and Xiaofeng Shao and Weibiao Wu},
  doi          = {10.1214/22-AOS2206},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3088-3089},
  shortjournal = {Ann. Statist.},
  title        = {Correction note: “Asymptotic spectral theory for nonlinear time series”},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rate-optimal cluster-randomized designs for spatial
interference. <em>AOS</em>, <em>50</em>(5), 3064–3087. (<a
href="https://doi.org/10.1214/22-AOS2224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a potential outcomes model in which interference may be present between any two units but the extent of interference diminishes with spatial distance. The causal estimand is the global average treatment effect, which compares outcomes under the counterfactuals that all or no units are treated. We study a class of designs in which space is partitioned into clusters that are randomized into treatment and control. For each design, we estimate the treatment effect using a Horvitz–Thompson estimator that compares the average outcomes of units with all or no neighbors treated, where the neighborhood radius is of the same order as the cluster size dictated by the design. We derive the estimator’s rate of convergence as a function of the design and degree of interference and use this to obtain estimator-design pairs that achieve near-optimal rates of convergence under relatively minimal assumptions on interference. We prove that the estimators are asymptotically normal and provide a variance estimator. For practical implementation of the designs, we suggest partitioning space using clustering algorithms.},
  archive      = {J_AOS},
  author       = {Michael P. Leung},
  doi          = {10.1214/22-AOS2224},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3064-3087},
  shortjournal = {Ann. Statist.},
  title        = {Rate-optimal cluster-randomized designs for spatial interference},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of time series models using residuals dependence
measures. <em>AOS</em>, <em>50</em>(5), 3039–3063. (<a
href="https://doi.org/10.1214/22-AOS2220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new estimation methods for time series models, possibly noncausal and/or noninvertible, using serial dependence information from the characteristic function of model residuals. This allows to impose the i.i.d. or martingale difference assumptions on the model errors to identify the unknown location of the roots of the lag polynomials for ARMA models without resorting to higher order moments or distributional assumptions. We consider generalized spectral density and cumulative distribution functions to measure residuals dependence at an increasing number of lags under both assumptions and discuss robust inference to higher order dependence when only mean independence is imposed on model errors. We study the consistency and asymptotic distribution of parameter estimates and discuss efficiency when different restrictions on error dependence are used simultaneously, including serial uncorrelation. Optimal weighting of continuous moment conditions yields maximum likelihood efficiency under independence for unknown error distribution. We investigate numerical implementation and finite sample properties of the new classes of estimates.},
  archive      = {J_AOS},
  author       = {Carlos Velasco},
  doi          = {10.1214/22-AOS2220},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3039-3063},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of time series models using residuals dependence measures},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locally associated graphical models and mixed convex
exponential families. <em>AOS</em>, <em>50</em>(5), 3009–3038. (<a
href="https://doi.org/10.1214/22-AOS2219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of multivariate total positivity has proved to be useful in finance and psychology but may be too restrictive in other applications. In this paper, we propose a concept of local association, where highly connected components in a graphical model are positively associated and study its properties. Our main motivation comes from gene expression data, where graphical models have become a popular exploratory tool. The models are instances of what we term mixed convex exponential families and we show that a mixed dual likelihood estimator has simple exact properties for such families as well as asymptotic properties similar to the maximum likelihood estimator. We further relax the positivity assumption by penalizing negative partial correlations in what we term the positive graphical lasso. Finally, we develop a GOLAZO algorithm based on block-coordinate descent that applies to a number of optimization procedures that arise in the context of graphical models, including the estimation problems described above. We derive results on existence of the optimum for such problems.},
  archive      = {J_AOS},
  author       = {Steffen Lauritzen and Piotr Zwiernik},
  doi          = {10.1214/22-AOS2219},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3009-3038},
  shortjournal = {Ann. Statist.},
  title        = {Locally associated graphical models and mixed convex exponential families},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric regression on lie groups with measurement
errors. <em>AOS</em>, <em>50</em>(5), 2973–3008. (<a
href="https://doi.org/10.1214/22-AOS2218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a foundation of methodology and theory for nonparametric regression with Lie group-valued predictors contaminated by measurement errors. Our methodology and theory are based on harmonic analysis on Lie groups, which is largely unknown in statistics. We establish a novel deconvolution regression estimator, and study its rate of convergence and asymptotic distribution. We also provide asymptotic confidence intervals based on the asymptotic distribution of the estimator and on the empirical likelihood technique. Several theoretical properties are also studied for a deconvolution density estimator, which is necessary to construct our regression estimator. The case of unknown measurement error distribution is also covered. We present practical details on implementation as well as the results of simulation studies for several Lie groups. A real data example is also provided.},
  archive      = {J_AOS},
  author       = {Jeong Min Jeon and Byeong U. Park and Ingrid Van Keilegom},
  doi          = {10.1214/22-AOS2218},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2973-3008},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric regression on lie groups with measurement errors},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-sample testing of high-dimensional linear regression
coefficients via complementary sketching. <em>AOS</em>, <em>50</em>(5),
2950–2972. (<a href="https://doi.org/10.1214/22-AOS2216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new method for two-sample testing of high-dimensional linear regression coefficients without assuming that those coefficients are individually estimable. The procedure works by first projecting the matrices of covariates and response vectors along directions that are complementary in sign in a subset of the coordinates, a process which we call “complementary sketching.” The resulting projected covariates and responses are aggregated to form two test statistics, which are shown to have essentially optimal asymptotic power under a Gaussian design when the difference between the two regression coefficients is sparse and dense respectively. Simulations confirm that our methods perform well in a broad class of settings and an application to a large single-cell RNA sequencing dataset demonstrates its utility in the real world.},
  archive      = {J_AOS},
  author       = {Fengnan Gao and Tengyao Wang},
  doi          = {10.1214/22-AOS2216},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2950-2972},
  shortjournal = {Ann. Statist.},
  title        = {Two-sample testing of high-dimensional linear regression coefficients via complementary sketching},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study of orthogonal array-based designs under a broad
class of space-filling criteria. <em>AOS</em>, <em>50</em>(5),
2925–2949. (<a href="https://doi.org/10.1214/22-AOS2215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-filling designs based on orthogonal arrays are attractive for computer experiments for they can be easily generated with desirable low-dimensional stratification properties. Nonetheless, it is not very clear how they behave and how to construct good such designs under other space-filling criteria. In this paper, we justify orthogonal array-based designs under a broad class of space-filling criteria, which include commonly used distance-, orthogonality- and discrepancy-based measures. To identify designs with even better space-filling properties, we partition orthogonal array-based designs into classes by allowable level permutations and show that the average performance of each class of designs is determined by two types of stratifications, with one of them being achieved by strong orthogonal arrays of strength 2+. Based on these results, we investigate various new and existing constructions of space-filling orthogonal array-based designs, including some strong orthogonal arrays of strength 2+ and mappable nearly orthogonal arrays.},
  archive      = {J_AOS},
  author       = {Guanzhou Chen and Boxin Tang},
  doi          = {10.1214/22-AOS2215},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2925-2949},
  shortjournal = {Ann. Statist.},
  title        = {A study of orthogonal array-based designs under a broad class of space-filling criteria},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable estimation and inference for censored quantile
regression process. <em>AOS</em>, <em>50</em>(5), 2899–2924. (<a
href="https://doi.org/10.1214/22-AOS2214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censored quantile regression (CQR) has become a valuable tool to study the heterogeneous association between a possibly censored outcome and a set of covariates, yet computation and statistical inference for CQR have remained a challenge for large-scale data with many covariates. In this paper, we focus on a smoothed martingale-based sequential estimating equations approach, to which scalable gradient-based algorithms can be applied. Theoretically, we provide a unified analysis of the smoothed sequential estimator and its penalized counterpart in increasing dimensions. When the covariate dimension grows with the sample size at a sublinear rate, we establish the uniform convergence rate (over a range of quantile indexes) and provide a rigorous justification for the validity of a multiplier bootstrap procedure for inference. In high-dimensional sparse settings, our results considerably improve the existing work on CQR by relaxing an exponential term of sparsity. We also demonstrate the advantage of the smoothed CQR over existing methods with both simulated experiments and data applications.},
  archive      = {J_AOS},
  author       = {Xuming He and Xiaoou Pan and Kean Ming Tan and Wen-Xin Zhou},
  doi          = {10.1214/22-AOS2214},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2899-2924},
  shortjournal = {Ann. Statist.},
  title        = {Scalable estimation and inference for censored quantile regression process},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric bayesian inference for reversible
multidimensional diffusions. <em>AOS</em>, <em>50</em>(5), 2872–2898.
(<a href="https://doi.org/10.1214/22-AOS2213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study nonparametric Bayesian models for reversible multidimensional diffusions with periodic drift. For continuous observation paths, reversibility is exploited to prove a general posterior contraction rate theorem for the drift gradient vector field under approximation-theoretic conditions on the induced prior for the invariant measure. The general theorem is applied to Gaussian priors and p-exponential priors, which are shown to converge to the truth at the optimal nonparametric rate over Sobolev smoothness classes in any dimension.},
  archive      = {J_AOS},
  author       = {Matteo Giordano and Kolyan Ray},
  doi          = {10.1214/22-AOS2213},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2872-2898},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric bayesian inference for reversible multidimensional diffusions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sieve stochastic gradient descent estimator for online
nonparametric regression in sobolev ellipsoids. <em>AOS</em>,
<em>50</em>(5), 2848–2871. (<a
href="https://doi.org/10.1214/22-AOS2212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of regression is to recover an unknown underlying function that best links a set of predictors to an outcome from noisy observations. In nonparametric regression, one assumes that the regression function belongs to a prespecified infinite-dimensional function space (the hypothesis space). In the online setting, when the observations come in a stream, it is computationally-preferable to iteratively update an estimate rather than refitting an entire model repeatedly. Inspired by nonparametric sieve estimation and stochastic approximation methods, we propose a sieve stochastic gradient descent estimator (Sieve-SGD) when the hypothesis space is a Sobolev ellipsoid. We show that Sieve-SGD has rate-optimal mean squared error (MSE) under a set of simple and direct conditions. The proposed estimator can be constructed with a low computational (time and space) expense: We also formally show that Sieve-SGD requires almost minimal memory usage among all statistically rate-optimal estimators.},
  archive      = {J_AOS},
  author       = {Tianyu Zhang and Noah Simon},
  doi          = {10.1214/22-AOS2212},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2848-2871},
  shortjournal = {Ann. Statist.},
  title        = {A sieve stochastic gradient descent estimator for online nonparametric regression in sobolev ellipsoids},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The interpolation phase transition in neural networks:
Memorization and generalization under lazy training. <em>AOS</em>,
<em>50</em>(5), 2816–2847. (<a
href="https://doi.org/10.1214/22-AOS2211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern neural networks are often operated in a strongly overparametrized regime: they comprise so many parameters that they can interpolate the training set, even if actual labels are replaced by purely random ones. Despite this, they achieve good prediction error on unseen data: interpolating the training set does not lead to a large generalization error. Further, overparametrization appears to be beneficial in that it simplifies the optimization landscape. Here, we study these phenomena in the context of two-layers neural networks in the neural tangent (NT) regime. We consider a simple data model, with isotropic covariates vectors in d dimensions, and N hidden neurons. We assume that both the sample size n and the dimension d are large, and they are polynomially related. Our first main result is a characterization of the eigenstructure of the empirical NT kernel in the overparametrized regime Nd≫n. This characterization implies as a corollary that the minimum eigenvalue of the empirical NT kernel is bounded away from zero as soon as Nd≫n and, therefore, the network can exactly interpolate arbitrary labels in the same regime. Our second main result is a characterization of the generalization error of NT ridge regression including, as a special case, min-ℓ2 norm interpolation. We prove that, as soon as Nd≫n, the test error is well approximated by the one of kernel ridge regression with respect to the infinite-width kernel. The latter is in turn well approximated by the error of polynomial ridge regression, whereby the regularization parameter is increased by a “self-induced” term related to the high-degree components of the activation function. The polynomial degree depends on the sample size and the dimension (in particular on logn/logd).},
  archive      = {J_AOS},
  author       = {Andrea Montanari and Yiqiao Zhong},
  doi          = {10.1214/22-AOS2211},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2816-2847},
  shortjournal = {Ann. Statist.},
  title        = {The interpolation phase transition in neural networks: Memorization and generalization under lazy training},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear biomarker combination for constrained classification.
<em>AOS</em>, <em>50</em>(5), 2793–2815. (<a
href="https://doi.org/10.1214/22-AOS2210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple biomarkers are often combined to improve disease diagnosis. The uniformly optimal combination, that is, with respect to all reasonable performance metrics, unfortunately requires excessive distributional modeling, to which the estimation can be sensitive. An alternative strategy is rather to pursue local optimality with respect to a specific performance metric. Nevertheless, existing methods may not target clinical utility of the intended medical test, which usually needs to operate above a certain sensitivity or specificity level, or do not have their statistical properties well studied and understood. In this article, we develop and investigate a linear combination method to maximize the clinical utility empirically for such a constrained classification. The combination coefficient is shown to have cube root asymptotics. The convergence rate and limiting distribution of the predictive performance are subsequently established, exhibiting robustness of the method in comparison with others. An algorithm with sound statistical justification is devised for efficient and high-quality computation. Simulations corroborate the theoretical results, and demonstrate good statistical and computational performance. Illustration with a clinical study on aggressive prostate cancer detection is provided.},
  archive      = {J_AOS},
  author       = {Yijian Huang and Martin G. Sanda},
  doi          = {10.1214/22-AOS2210},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2793-2815},
  shortjournal = {Ann. Statist.},
  title        = {Linear biomarker combination for constrained classification},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric bivariate density estimation for censored
lifetimes. <em>AOS</em>, <em>50</em>(5), 2767–2792. (<a
href="https://doi.org/10.1214/22-AOS2209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that estimation of a bivariate cumulative distribution function of a pair of right censored lifetimes presents challenges unparalleled to the univariate case where a product-limit Kaplan–Meyer’s methodology typically yields optimal estimation, and the literature on optimal estimation of the joint probability density is next to none. The paper, for the first time in the survival analysis literature, develops the theory and methodology of sharp minimax and adaptive nonparametric estimation of the joint density under the mean integrated squared error (MISE) criterion. The theory shows how an underlying joint density, together with the bivariate distribution of censoring variables, affect the estimation, and what and how may or may not be estimated in the presence of censoring. Practical example illustrates the problem.},
  archive      = {J_AOS},
  author       = {Sam Efromovich},
  doi          = {10.1214/22-AOS2209},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2767-2792},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric bivariate density estimation for censored lifetimes},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relaxing the gaussian assumption in shrinkage and SURE in
high dimension. <em>AOS</em>, <em>50</em>(5), 2737–2766. (<a
href="https://doi.org/10.1214/22-AOS2208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shrinkage estimation is a fundamental tool of modern statistics, pioneered by Charles Stein upon his discovery of the famous paradox involving the multivariate Gaussian. A large portion of the subsequent literature only considers the efficiency of shrinkage, and that of an associated procedure known as Stein’s Unbiased Risk Estimate, or SURE, in the Gaussian setting of that original work. We investigate what extensions to the domain of validity of shrinkage and SURE can be made away from the Gaussian through the use of tools developed in the probabilistic area now known as Stein’s method. We show that shrinkage is efficient away from the Gaussian under very mild conditions on the distribution of the noise. SURE is also proved to be adaptive under similar assumptions, and in particular in a way that retains the classical asymptotics of Pinsker’s theorem. Notably, shrinkage and SURE are shown to be efficient under mild distributional assumptions, and particularly for general isotropic log-concave measures.},
  archive      = {J_AOS},
  author       = {Max Fathi and Larry Goldstein and Gesine Reinert and Adrien Saumard},
  doi          = {10.1214/22-AOS2208},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2737-2766},
  shortjournal = {Ann. Statist.},
  title        = {Relaxing the gaussian assumption in shrinkage and SURE in high dimension},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate kernel PCA: Computational versus statistical
trade-off. <em>AOS</em>, <em>50</em>(5), 2713–2736. (<a
href="https://doi.org/10.1214/22-AOS2204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods are powerful learning methodologies that allow to perform nonlinear data analysis. Despite their popularity, they suffer from poor scalability in big data scenarios. Various approximation methods, including random feature approximation, have been proposed to alleviate the problem. However, the statistical consistency of most of these approximate kernel methods is not well understood except for kernel ridge regression wherein it has been shown that the random feature approximation is not only computationally efficient but also statistically consistent with a minimax optimal rate of convergence. In this paper, we investigate the efficacy of random feature approximation in the context of kernel principal component analysis (KPCA) by studying the trade-off between computational and statistical behaviors of approximate KPCA. We show that the approximate KPCA is both computationally and statistically efficient compared to KPCA in terms of the error associated with reconstructing a kernel function based on its projection onto the corresponding eigenspaces. The analysis hinges on Bernstein-type inequalities for the operator and Hilbert–Schmidt norms of a self-adjoint Hilbert–Schmidt operator-valued U-statistics, which are of independent interest.},
  archive      = {J_AOS},
  author       = {Bharath K. Sriperumbudur and Nicholas Sterge},
  doi          = {10.1214/22-AOS2204},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2713-2736},
  shortjournal = {Ann. Statist.},
  title        = {Approximate kernel PCA: Computational versus statistical trade-off},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic analysis of synchrosqueezing transform—toward
statistical inference with nonlinear-type time-frequency analysis.
<em>AOS</em>, <em>50</em>(5), 2694–2712. (<a
href="https://doi.org/10.1214/22-AOS2203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a statistical analysis of a tool in nonlinear-type time-frequency analysis, the synchrosqueezing transform (SST), for both the null and nonnull cases. The intricate nonlinear interaction of different quantities in SST is quantified by carefully analyzing relevant multivariate complex Gaussian random variables. Specifically, we provide the quotient distribution of dependent and improper complex Gaussian random variables. Then a central limit theorem result for SST is established. As an example, we provide a block bootstrap scheme based on the established SST theory to test if a given time series contains oscillatory components.},
  archive      = {J_AOS},
  author       = {Matt Sourisseau and Hau-Tieng Wu and Zhou Zhou},
  doi          = {10.1214/22-AOS2203},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2694-2712},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic analysis of synchrosqueezing transform—toward statistical inference with nonlinear-type time-frequency analysis},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global and individualized community detection in
inhomogeneous multilayer networks. <em>AOS</em>, <em>50</em>(5),
2664–2693. (<a href="https://doi.org/10.1214/22-AOS2202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network applications, it has become increasingly common to obtain datasets in the form of multiple networks observed on the same set of subjects, where each network is obtained in a related but different experiment condition or application scenario. Such datasets can be modeled by multilayer networks where each layer is a separate network itself while different layers are associated and share some common information. The present paper studies community detection in a stylized yet informative inhomogeneous multilayer network model. In our model, layers are generated by different stochastic block models, the community structures of which are (random) perturbations of a common global structure while the connecting probabilities in different layers are not related. Focusing on the symmetric two block case, we establish minimax rates for both global estimation of the common structure and individualized estimation of layerwise community structures. Both minimax rates have sharp exponents. In addition, we provide an efficient algorithm that is simultaneously asymptotic minimax optimal for both estimation tasks under mild conditions. The optimal rates depend on the parity of the number of most informative layers, a phenomenon that is caused by inhomogeneity across layers. The method is extended to handle multiple and potentially asymmetric community cases. We demonstrate its effectiveness on both simulated examples and a real multimodal single-cell dataset.},
  archive      = {J_AOS},
  author       = {Shuxiao Chen and Sifan Liu and Zongming Ma},
  doi          = {10.1214/22-AOS2202},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2664-2693},
  shortjournal = {Ann. Statist.},
  title        = {Global and individualized community detection in inhomogeneous multilayer networks},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Is infinity that far? A bayesian nonparametric perspective
of finite mixture models. <em>AOS</em>, <em>50</em>(5), 2641–2663. (<a
href="https://doi.org/10.1214/22-AOS2201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture models are one of the most widely used statistical tools when dealing with data from heterogeneous populations. Following a Bayesian nonparametric perspective, we introduce a new class of priors: the Normalized Independent Point Process. We investigate the probabilistic properties of this new class and present many special cases. In particular, we provide an explicit formula for the distribution of the implied partition, as well as the posterior characterization of the new process in terms of the superposition of two discrete measures. We also provide consistency results. Moreover, we design both a marginal and a conditional algorithm for finite mixture models with a random number of components. These schemes are based on an auxiliary variable MCMC, which allows handling the otherwise intractable posterior distribution and overcomes the challenges associated with the Reversible Jump algorithm. We illustrate the performance and the potential of our model in a simulation study and on real data applications.},
  archive      = {J_AOS},
  author       = {Raffaele Argiento and Maria De Iorio},
  doi          = {10.1214/22-AOS2201},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2641-2663},
  shortjournal = {Ann. Statist.},
  title        = {Is infinity that far? a bayesian nonparametric perspective of finite mixture models},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affine-equivariant inference for multivariate location under
lp loss functions. <em>AOS</em>, <em>50</em>(5), 2616–2640. (<a
href="https://doi.org/10.1214/22-AOS2199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fundamental problem of estimating the location of a d-variate probability measure under an Lp loss function. The naive estimator, that minimizes the usual empirical Lp risk, has a known asymptotic behavior but suffers from several deficiencies for p≠2, the most important one being the lack of equivariance under general affine transformations. In this work, we introduce a collection of Lp location estimators μˆnp,ℓ that minimize the size of suitable ℓ-dimensional data-based simplices. For ℓ=1, these estimators reduce to the naive ones, whereas, for ℓ=d, they are equivariant under affine transformations. Irrespective of ℓ, these estimators reduce to the sample mean for p=2, whereas for p=1, the estimators provide the well-known spatial median and Oja median for ℓ=1 and ℓ=d, respectively. Under very mild assumptions, we derive an explicit Bahadur representation result for μˆnp,ℓ and establish asymptotic normality. We prove that, quite remarkably, the asymptotic behavior of the estimators does not depend on ℓ under spherical symmetry, so that the affine equivariance for ℓ=d is achieved at no cost in terms of efficiency. To allow for large sample size n and/or large dimension d, we introduce a version of our estimators relying on incomplete U-statistics. Under a centro-symmetry assumption, we also define companion tests ϕnp,ℓ for the problem of testing the null hypothesis that the location μ of the underlying probability measure coincides with a given location μ0. For any p, affine invariance is achieved for ℓ=d. For any ℓ and p, we derive explicit expressions for the asymptotic power of these tests under contiguous local alternatives, which reveals that asymptotic relative efficiencies with respect to traditional parametric Gaussian procedures for hypothesis testing coincide with those obtained for point estimation. We illustrate finite-sample relevance of our asymptotic results through Monte Carlo exercises and also treat a real data example.},
  archive      = {J_AOS},
  author       = {Alexander Dürre and Davy Paindaveine},
  doi          = {10.1214/22-AOS2199},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2616-2640},
  shortjournal = {Ann. Statist.},
  title        = {Affine-equivariant inference for multivariate location under lp loss functions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bounds on the conditional and average treatment effect with
unobserved confounding factors. <em>AOS</em>, <em>50</em>(5), 2587–2615.
(<a href="https://doi.org/10.1214/22-AOS2195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For observational studies, we study the sensitivity of causal inference when treatment assignments may depend on unobserved confounders. We develop a loss minimization approach for estimating bounds on the conditional average treatment effect (CATE) when unobserved confounders have a bounded effect on the odds ratio of treatment selection. Our approach is scalable and allows flexible use of model classes in estimation, including nonparametric and black-box machine learning methods. Based on these bounds for the CATE, we propose a sensitivity analysis for the average treatment effect (ATE). Our semiparametric estimator extends/bounds the augmented inverse propensity weighted (AIPW) estimator for the ATE under bounded unobserved confounding. By constructing a Neyman orthogonal score, our estimator of the bound for the ATE is a regular root-n estimator so long as the nuisance parameters are estimated at the op(n−1/4) rate. We complement our methodology with optimality results showing that our proposed bounds are tight in certain cases. We demonstrate our method on simulated and real data examples, and show accurate coverage of our confidence intervals in practical finite sample regimes with rich covariate information.},
  archive      = {J_AOS},
  author       = {Steve Yadlowsky and Hongseok Namkoong and Sanjay Basu and John Duchi and Lu Tian},
  doi          = {10.1214/22-AOS2195},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2587-2615},
  shortjournal = {Ann. Statist.},
  title        = {Bounds on the conditional and average treatment effect with unobserved confounding factors},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved central limit theorem and bootstrap approximations
in high dimensions. <em>AOS</em>, <em>50</em>(5), 2562–2586. (<a
href="https://doi.org/10.1214/22-AOS2193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the Gaussian and bootstrap approximations to the distribution of the max statistic in high dimensions. This statistic takes the form of the maximum over components of the sum of independent random vectors and its distribution plays a key role in many high-dimensional estimation and testing problems. Using a novel iterative randomized Lindeberg method, the paper derives new bounds for the distributional approximation errors. These new bounds substantially improve upon existing ones and simultaneously allow for a larger class of bootstrap methods.},
  archive      = {J_AOS},
  author       = {Victor Chernozhuokov and Denis Chetverikov and Kengo Kato and Yuta Koike},
  doi          = {10.1214/22-AOS2193},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2562-2586},
  shortjournal = {Ann. Statist.},
  title        = {Improved central limit theorem and bootstrap approximations in high dimensions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New edgeworth-type expansions with finite sample guarantees.
<em>AOS</em>, <em>50</em>(5), 2545–2561. (<a
href="https://doi.org/10.1214/22-AOS2192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish higher-order nonasymptotic expansions for a difference between probability distributions of sums of i.i.d. random vectors in a Euclidean space. The derived bounds are uniform over two classes of sets: the set of all Euclidean balls and the set of all half-spaces. These results allow to account for an impact of higher-order moments or cumulants of the considered distributions; the obtained error terms depend on a sample size and a dimension explicitly. The new inequalities outperform accuracy of the normal approximation in existing Berry–Esseen inequalities under very general conditions. Under some symmetry assumptions on the probability distribution of random summands, the obtained results are optimal in terms of the ratio between the dimension and the sample size. The new technique which we developed for establishing nonasymptotic higher-order expansions can be interesting by itself. Using the new higher-order inequalities, we study accuracy of the nonparametric bootstrap approximation and propose a bootstrap score test under possible model misspecification. The results of the paper also include explicit error bounds for general elliptic confidence regions for an expected value of the random summands, and optimality of the Gaussian anticoncentration inequality over the set of all Euclidean balls.},
  archive      = {J_AOS},
  author       = {Mayya Zhilova},
  doi          = {10.1214/22-AOS2192},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2545-2561},
  shortjournal = {Ann. Statist.},
  title        = {New edgeworth-type expansions with finite sample guarantees},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing goodness-of-fit and conditional independence with
approximate co-sufficient sampling. <em>AOS</em>, <em>50</em>(5),
2514–2544. (<a href="https://doi.org/10.1214/22-AOS2187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goodness-of-fit (GoF) testing is ubiquitous in statistics, with direct ties to model selection, confidence interval construction, conditional independence testing, and multiple testing, just to name a few applications. While testing the GoF of a simple (point) null hypothesis provides an analyst great flexibility in the choice of test statistic while still ensuring validity, most GoF tests for composite null hypotheses are far more constrained, as the test statistic must have a tractable distribution over the entire null model space. A notable exception is co-sufficient sampling (CSS): resampling the data conditional on a sufficient statistic for the null model guarantees valid GoF testing using any test statistic the analyst chooses. But CSS testing requires the null model to have a compact (in an information-theoretic sense) sufficient statistic, which only holds for a very limited class of models; even for a null model as simple as logistic regression, CSS testing is powerless. In this paper, we leverage the concept of approximate sufficiency to generalize CSS testing to essentially any parametric model with an asymptotically efficient estimator; we call our extension “approximate CSS” (aCSS) testing. We quantify the finite-sample Type I error inflation of aCSS testing and show that it is vanishing under standard maximum likelihood asymptotics, for any choice of test statistic. We apply our proposed procedure both theoretically and in simulation to a number of models of interest to demonstrate its finite-sample Type I error and power.},
  archive      = {J_AOS},
  author       = {Rina Foygel Barber and Lucas Janson},
  doi          = {10.1214/22-AOS2187},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2514-2544},
  shortjournal = {Ann. Statist.},
  title        = {Testing goodness-of-fit and conditional independence with approximate co-sufficient sampling},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Central limit theorem and bootstrap approximation in high
dimensions: Near 1/n rates via implicit smoothing. <em>AOS</em>,
<em>50</em>(5), 2492–2513. (<a
href="https://doi.org/10.1214/22-AOS2184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonasymptotic bounds for Gaussian and bootstrap approximation have recently attracted significant interest in high-dimensional statistics. This paper studies Berry–Esseen bounds for such approximations with respect to the multivariate Kolmogorov distance, in the context of a sum of n random vectors that are p-dimensional and i.i.d. Up to now, a growing line of work has established bounds with mild logarithmic dependence on p. However, the problem of developing corresponding bounds with near n−1/2 dependence on n has remained largely unresolved. Within the setting of random vectors that have sub-Gaussian or subexponential entries, this paper establishes bounds with near n−1/2 dependence, for both Gaussian and bootstrap approximation. In addition, the proofs are considerably distinct from other recent approaches, and make use of an “implicit smoothing” operation in the Lindeberg interpolation.},
  archive      = {J_AOS},
  author       = {Miles E. Lopes},
  doi          = {10.1214/22-AOS2184},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2492-2513},
  shortjournal = {Ann. Statist.},
  title        = {Central limit theorem and bootstrap approximation in high dimensions: Near 1/n rates via implicit smoothing},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous-time targeted minimum loss-based estimation of
intervention-specific mean outcomes. <em>AOS</em>, <em>50</em>(5),
2469–2491. (<a href="https://doi.org/10.1214/21-AOS2114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper generalizes the targeted minimum loss-based estimation (TMLE) framework to allow for estimating the effects of time-varying interventions in settings where both interventions, covariates, and outcome can happen at subject-specific time-points on an arbitrarily fine time-scale. TMLE is a general template for constructing asymptotically linear substitution estimators for smooth low-dimensional parameters in infinite-dimensional models. Existing longitudinal TMLE methods are developed for data where observations are made on a discrete time-grid. We consider a continuous-time counting process model where intensity measures track the monitoring of subjects, and focus on a low-dimensional target parameter defined as the intervention-specific mean outcome at the end of follow-up. To construct our TMLE algorithm for the given statistical estimation problem, we derive an expression for the efficient influence curve and represent the target parameter as a functional of intensities and conditional expectations. The high-dimensional nuisance parameters of our model are estimated and updated in an iterative manner according to separate targeting steps for the involved intensities and conditional expectations. The resulting estimator solves the efficient influence curve equation. We state a general efficiency theorem and describe a highly adaptive lasso estimator for nuisance parameters that allows us to establish asymptotic linearity and efficiency of our estimator under minimal conditions on the underlying statistical model.},
  archive      = {J_AOS},
  author       = {Helene C. Rytgaard and Thomas A. Gerds and Mark J. van der Laan},
  doi          = {10.1214/21-AOS2114},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2469-2491},
  shortjournal = {Ann. Statist.},
  title        = {Continuous-time targeted minimum loss-based estimation of intervention-specific mean outcomes},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Erratum: Asymptotic genealogies of interacting particle
systems with an application to sequential monte carlo. <em>AOS</em>,
<em>50</em>(4), 2467–2468. (<a
href="https://doi.org/10.1214/21-AOS2135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Jere Koskela and Paul A. Jenkins and Adam M. Johansen and Dario Spanò},
  doi          = {10.1214/21-AOS2135},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2467-2468},
  shortjournal = {Ann. Statist.},
  title        = {Erratum: Asymptotic genealogies of interacting particle systems with an application to sequential monte carlo},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistency of invariance-based randomization tests.
<em>AOS</em>, <em>50</em>(4), 2443–2466. (<a
href="https://doi.org/10.1214/22-AOS2200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invariance-based randomization tests—such as permutation tests, rotation tests, or sign changes—are an important and widely used class of statistical methods. They allow drawing inferences under weak assumptions on the data distribution. Most work focuses on their type I error control properties, while their consistency properties are much less understood. We develop a general framework to study the consistency of invariance-based randomization tests, assuming the data is drawn from a signal-plus-noise model. We allow the transforms (e.g., permutations or rotations) to be general compact topological groups, such as rotation groups, acting by linear group representations. We study test statistics with a generalized subadditivity property. We apply our framework to a number of fundamental and highly important problems in statistics, including sparse vector detection, testing for low-rank matrices in noise, sparse detection in linear regression, and two-sample testing. Comparing with minimax lower bounds we develop, we find perhaps surprisingly that in some cases, randomization tests detect signals at the minimax optimal rate.},
  archive      = {J_AOS},
  author       = {Edgar Dobriban},
  doi          = {10.1214/22-AOS2200},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2443-2466},
  shortjournal = {Ann. Statist.},
  title        = {Consistency of invariance-based randomization tests},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A minimax framework for quantifying risk-fairness trade-off
in regression. <em>AOS</em>, <em>50</em>(4), 2416–2442. (<a
href="https://doi.org/10.1214/22-AOS2198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a theoretical framework for the problem of learning a real-valued function which meets fairness requirements. This framework is built upon the notion of α-relative (fairness) improvement of the regression function which we introduce using the theory of optimal transport. Setting α=0 corresponds to the regression problem under the Demographic Parity constraint, while α=1 corresponds to the classical regression problem without any constraints. For α∈(0,1) the proposed framework allows to continuously interpolate between these two extreme cases and to study partially fair predictors. Within this framework, we precisely quantify the cost in risk induced by the introduction of the fairness constraint. We put forward a statistical minimax setup and derive a general problem-dependent lower bound on the risk of any estimator satisfying α-relative improvement constraint. We illustrate our framework on a model of linear regression with Gaussian design and systematic group-dependent bias, deriving matching (up to absolute constants) upper and lower bounds on the minimax risk under the introduced constraint. We provide a general post-processing strategy which enjoys fairness, risk guarantees and can be applied on top of any black-box algorithm. Finally, we perform a simulation study of the linear model and numerical experiments of benchmark data, validating our theoretical contributions.},
  archive      = {J_AOS},
  author       = {Evgenii Chzhen and Nicolas Schreuder},
  doi          = {10.1214/22-AOS2198},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2416-2442},
  shortjournal = {Ann. Statist.},
  title        = {A minimax framework for quantifying risk-fairness trade-off in regression},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of smooth functionals in high-dimensional models:
Bootstrap chains and gaussian approximation. <em>AOS</em>,
<em>50</em>(4), 2386–2415. (<a
href="https://doi.org/10.1214/22-AOS2197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let X(n) be an observation sampled from a distribution Pθ(n) with an unknown parameter θ, θ being a vector in a Banach space E (most often, a high-dimensional space of dimension d). We study the problem of estimation of f(θ) for a functional f:E↦R of some smoothness s&gt;0 based on an observation X(n)∼Pθ(n). Assuming that there exists an estimator θˆn=θˆn(X(n)) of parameter θ such that n(θˆ n−θ) is sufficiently close in distribution to a mean zero Gaussian random vector in E, we construct a functional g:E↦R such that g(θˆn) is an asymptotically normal estimator of f(θ) with n rate provided that s&gt;11−α and d≤nα for some α∈(0,1). We also derive general upper bounds on Orlicz norm error rates for estimator g(θˆ) depending on smoothness s, dimension d, sample size n and the accuracy of normal approximation of n(θˆ n−θ). In particular, this approach yields asymptotically efficient estimators in high-dimensional log-concave exponential models.},
  archive      = {J_AOS},
  author       = {Vladimir Koltchinskii},
  doi          = {10.1214/22-AOS2197},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2386-2415},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of smooth functionals in high-dimensional models: Bootstrap chains and gaussian approximation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An ℓp theory of PCA and spectral clustering. <em>AOS</em>,
<em>50</em>(4), 2359–2385. (<a
href="https://doi.org/10.1214/22-AOS2196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal Component Analysis (PCA) is a powerful tool in statistics and machine learning. While existing study of PCA focuses on the recovery of principal components and their associated eigenvalues, there are few precise characterizations of individual principal component scores that yield low-dimensional embedding of samples. That hinders the analysis of various spectral methods. In this paper, we first develop an ℓp perturbation theory for a hollowed version of PCA in Hilbert spaces which provably improves upon the vanilla PCA in the presence of heteroscedastic noises. Through a novel ℓp analysis of eigenvectors, we investigate entrywise behaviors of principal component score vectors and show that they can be approximated by linear functionals of the Gram matrix in ℓp norm, which includes ℓ2 and ℓ∞ as special cases. For sub-Gaussian mixture models, the choice of p giving optimal bounds depends on the signal-to-noise ratio, which further yields optimality guarantees for spectral clustering. For contextual community detection, the ℓp theory leads to simple spectral algorithms that achieve the information threshold for exact recovery and the optimal misclassification rate.},
  archive      = {J_AOS},
  author       = {Emmanuel Abbe and Jianqing Fan and Kaizheng Wang},
  doi          = {10.1214/22-AOS2196},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2359-2385},
  shortjournal = {Ann. Statist.},
  title        = {An ℓp theory of PCA and spectral clustering},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Random graph asymptotics for treatment effect estimation
under network interference. <em>AOS</em>, <em>50</em>(4), 2334–2358. (<a
href="https://doi.org/10.1214/22-AOS2191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network interference model for treatment effect estimation places experimental units at the vertices of an undirected exposure graph, such that treatment assigned to one unit may affect the outcome of another unit if and only if these two units are connected by an edge. This model has recently gained popularity as means of incorporating interference effects into the Neyman–Rubin potential outcomes framework; and several authors have considered estimation of various causal targets, including the direct and indirect effects of treatment. In this paper, we consider large-sample asymptotics for treatment effect estimation under network interference in a setting where the exposure graph is a random draw from a graphon. When targeting the direct effect, we establish a central limit theorem and find that—in our setting—popular estimators are considerably more accurate than existing results suggest. Meanwhile, when targeting the indirect effect, we leverage our generative assumptions to propose a consistent estimator in a setting where no other consistent estimators are currently available. Overall, our results highlight the promise of random graph asymptotics in understanding the practicality and limits of causal inference under network interference.},
  archive      = {J_AOS},
  author       = {Shuangning Li and Stefan Wager},
  doi          = {10.1214/22-AOS2191},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2334-2358},
  shortjournal = {Ann. Statist.},
  title        = {Random graph asymptotics for treatment effect estimation under network interference},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the robustness of minimum norm interpolators and
regularized empirical risk minimizers. <em>AOS</em>, <em>50</em>(4),
2306–2333. (<a href="https://doi.org/10.1214/22-AOS2190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a general theory for minimum norm interpolating estimators and regularized empirical risk minimizers (RERM) in linear models in the presence of additive, potentially adversarial, errors. In particular, no conditions on the errors are imposed. A quantitative bound for the prediction error is given, relating it to the Rademacher complexity of the covariates, the norm of the minimum norm interpolator of the errors and the size of the subdifferential around the true parameter. The general theory is illustrated for Gaussian features and several norms: The ℓ1, ℓ2, group Lasso and nuclear norms. In case of sparsity or low-rank inducing norms, minimum norm interpolators and RERM yield a prediction error of the order of the average noise level, provided that the overparameterization is at least a logarithmic factor larger than the number of samples and that, in case of RERM, the regularization parameter is small enough. Lower bounds that show near optimality of the results complement the analysis.},
  archive      = {J_AOS},
  author       = {Geoffrey Chinot and Matthias Löffler and Sara van de Geer},
  doi          = {10.1214/22-AOS2190},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2306-2333},
  shortjournal = {Ann. Statist.},
  title        = {On the robustness of minimum norm interpolators and regularized empirical risk minimizers},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonregular and minimax estimation of individualized
thresholds in high dimension with binary responses. <em>AOS</em>,
<em>50</em>(4), 2284–2305. (<a
href="https://doi.org/10.1214/22-AOS2188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a large number of covariates Z, we consider the estimation of a high-dimensional parameter θ in an individualized linear threshold θTZ for a continuous variable X, which minimizes the disagreement between sign(X−θTZ) and a binary response Y. While the problem can be formulated into the M-estimation framework, minimizing the corresponding empirical risk function is computationally intractable due to discontinuity of the sign function. Moreover, estimating θ even in the fixed-dimensional setting is known as a nonregular problem leading to nonstandard asymptotic theory. To tackle the computational and theoretical challenges in the estimation of the high-dimensional parameter θ, we propose an empirical risk minimization approach based on a regularized smoothed non-convex loss function. The Fisher consistency of the proposed method is guaranteed as the bandwidth of the smoothed loss is shrunk to 0. Statistically, we show that the finite sample error bound for estimating θ in ℓ2 norm is (slogd/n)β/(2β+1), where d is the dimension of θ, s is the sparsity level, n is the sample size and β is the smoothness of the conditional density of X given the response Y and the covariates Z. The convergence rate is nonstandard and slower than that in the classical Lasso problems. Furthermore, we prove that the resulting estimator is minimax rate optimal up to a logarithmic factor. The Lepski’s method is developed to achieve the adaption to the unknown sparsity s or smoothness β. Computationally, an efficient path-following algorithm is proposed to compute the solution path. We show that this algorithm achieves geometric rate of convergence for computing the whole path. Finally, we evaluate the finite sample performance of the proposed estimator in simulation studies and a real data analysis from the ChAMP (Chondral Lesions And Meniscus Procedures) Trial.},
  archive      = {J_AOS},
  author       = {Huijie Feng and Yang Ning and Jiwei Zhao},
  doi          = {10.1214/22-AOS2188},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2284-2305},
  shortjournal = {Ann. Statist.},
  title        = {Nonregular and minimax estimation of individualized thresholds in high dimension with binary responses},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized resilience and robust statistics. <em>AOS</em>,
<em>50</em>(4), 2256–2283. (<a
href="https://doi.org/10.1214/22-AOS2186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust statistics traditionally focuses on outliers, or perturbations in total variation distance. However, a dataset could be maliciously corrupted in many other ways, such as systematic measurement errors and missing covariates. We consider corruption in either TV or Wasserstein distance, and show that robust estimation is possible whenever the true population distribution satisfies a property called generalized resilience, which holds under moment or hypercontractive conditions. For TV corruption model, our finite-sample analysis improves over previous results for mean estimation with bounded kth moment, linear regression, and joint mean and covariance estimation. For W1 corruption, we provide the first finite-sample guarantees for second moment estimation and linear regression. Technically, our robust estimators are a generalization of minimum distance (MD) functionals, which project the corrupted distribution onto a given set of well-behaved distributions. The error of these MD functionals is bounded by a certain modulus of continuity, and we provide a systematic method for upper bounding this modulus for the class of generalized resilient distributions, which usually gives sharp population-level results and good finite-sample guarantees.},
  archive      = {J_AOS},
  author       = {Banghua Zhu and Jiantao Jiao and Jacob Steinhardt},
  doi          = {10.1214/22-AOS2186},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2256-2283},
  shortjournal = {Ann. Statist.},
  title        = {Generalized resilience and robust statistics},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning mixtures of permutations: Groups of pairwise
comparisons and combinatorial method of moments. <em>AOS</em>,
<em>50</em>(4), 2231–2255. (<a
href="https://doi.org/10.1214/22-AOS2185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications such as rank aggregation, mixture models for permutations are frequently used when the population exhibits heterogeneity. In this work, we study the widely used Mallows mixture model. In the high-dimensional setting, we propose a polynomial-time algorithm that learns a Mallows mixture of permutations on n elements with the optimal sample complexity that is proportional to logn, improving upon previous results that scale polynomially with n. In the high-noise regime, we characterize the optimal dependency of the sample complexity on the noise parameter. Both objectives are accomplished by first studying demixing permutations under a noiseless query model using groups of pairwise comparisons, which can be viewed as moments of the mixing distribution, and then extending these results to the noisy Mallows model by simulating the noiseless oracle.},
  archive      = {J_AOS},
  author       = {Cheng Mao and Yihong Wu},
  doi          = {10.1214/22-AOS2185},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2231-2255},
  shortjournal = {Ann. Statist.},
  title        = {Learning mixtures of permutations: Groups of pairwise comparisons and combinatorial method of moments},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic independence of spiked eigenvalues and linear
spectral statistics for large sample covariance matrices. <em>AOS</em>,
<em>50</em>(4), 2205–2230. (<a
href="https://doi.org/10.1214/22-AOS2183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider general high-dimensional spiked sample covariance models and show that their leading sample spiked eigenvalues and their linear spectral statistics are asymptotically independent when the sample size and dimension are proportional to each other. As a byproduct, we also establish the central limit theorem of the leading sample spiked eigenvalues by removing the block diagonal assumption on the population covariance matrix, which is commonly needed in the literature. Moreover, we propose consistent estimators of the L4 norm of the spiked population eigenvectors. Based on these results, we develop a new statistic to test the equality of two spiked population covariance matrices. Numerical studies show that the new test procedure is more powerful than some existing methods.},
  archive      = {J_AOS},
  author       = {Zhixiang Zhang and Shurong Zheng and Guangming Pan and Ping-Shou Zhong},
  doi          = {10.1214/22-AOS2183},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2205-2230},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic independence of spiked eigenvalues and linear spectral statistics for large sample covariance matrices},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic continuum-armed bandits with additive models:
Minimax regrets and adaptive algorithm. <em>AOS</em>, <em>50</em>(4),
2179–2204. (<a href="https://doi.org/10.1214/22-AOS2182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider d-dimensional stochastic continuum-armed bandits with the expected reward function being additive β-Hölder with sparsity s for 0&lt;β&lt;∞ and 1≤s≤d. The rate of convergence O˜(s·T β+12β+1) for the minimax regret is established where T is the number of rounds. In particular, the minimax regret does not depend on d and is linear in s. A novel algorithm is proposed and is shown to be rate-optimal, up to a logarithmic factor of T. The problem of adaptivity is also studied. A lower bound on the cost of adaptation to the smoothness is obtained and the result implies that adaptation for free is impossible in general without further structural assumptions. We then consider adaptive additive SCAB under an additional self-similarity assumption. An adaptive procedure is constructed and is shown to simultaneously achieve the minimax regret for a range of smoothness levels.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Hongming Pu},
  doi          = {10.1214/22-AOS2182},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2179-2204},
  shortjournal = {Ann. Statist.},
  title        = {Stochastic continuum-armed bandits with additive models: Minimax regrets and adaptive algorithm},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact minimax risk for linear least squares, and the lower
tail of sample covariance matrices. <em>AOS</em>, <em>50</em>(4),
2157–2178. (<a href="https://doi.org/10.1214/22-AOS2181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider random-design linear prediction and related questions on the lower tail of random matrices. It is known that, under boundedness constraints, the minimax risk is of order d/n in dimension d with n samples. Here, we study the minimax expected excess risk over the full linear class, depending on the distribution of covariates. First, the least squares estimator is exactly minimax optimal in the well-specified case, for every distribution of covariates. We express the minimax risk in terms of the distribution of statistical leverage scores of individual samples, and deduce a minimax lower bound of d/(n−d+1) for any covariate distribution, nearly matching the risk for Gaussian design. We then obtain sharp nonasymptotic upper bounds for covariates that satisfy a “small ball”-type regularity condition in both well-specified and misspecified cases. Our main technical contribution is the study of the lower tail of the smallest singular value of empirical covariance matrices at small values. We establish a lower bound on this lower tail, valid for any distribution in dimension d≥2, together with a matching upper bound under a necessary regularity condition. Our proof relies on the PAC-Bayes technique for controlling empirical processes, and extends an analysis of Oliveira devoted to a different part of the lower tail.},
  archive      = {J_AOS},
  author       = {Jaouad Mourtada},
  doi          = {10.1214/22-AOS2181},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2157-2178},
  shortjournal = {Ann. Statist.},
  title        = {Exact minimax risk for linear least squares, and the lower tail of sample covariance matrices},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precise statistical analysis of classification accuracies
for adversarial training. <em>AOS</em>, <em>50</em>(4), 2127–2156. (<a
href="https://doi.org/10.1214/22-AOS2180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the wide empirical success of modern machine learning algorithms and models in a multitude of applications, they are known to be highly susceptible to seemingly small indiscernible perturbations to the input data known as adversarial attacks. A variety of recent adversarial training procedures have been proposed to remedy this issue. Despite the success of such procedures at increasing accuracy on adversarially perturbed inputs or robust accuracy, these techniques often reduce accuracy on natural unperturbed inputs or standard accuracy. Complicating matters further, the effect and trend of adversarial training procedures on standard and robust accuracy is rather counter intuitive and radically dependent on a variety of factors including the perceived form of the perturbation during training, size/quality of data, model overparameterization, etc. In this paper, we focus on binary classification problems where the data is generated according to the mixture of two Gaussians with general anisotropic covariance matrices and derive a precise characterization of the standard and robust accuracy for a class of minimax adversarially trained models. We consider a general norm-based adversarial model, where the adversary can add perturbations of bounded ℓp norm to each input data, for an arbitrary p≥1. Our comprehensive analysis allows us to theoretically explain several intriguing empirical phenomena and provide a precise understanding of the role of different problem parameters on standard and robust accuracies.},
  archive      = {J_AOS},
  author       = {Adel Javanmard and Mahdi Soltanolkotabi},
  doi          = {10.1214/22-AOS2180},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2127-2156},
  shortjournal = {Ann. Statist.},
  title        = {Precise statistical analysis of classification accuracies for adversarial training},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sharp optimal recovery in the two component gaussian mixture
model. <em>AOS</em>, <em>50</em>(4), 2096–2126. (<a
href="https://doi.org/10.1214/22-AOS2178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of clustering in the Two component Gaussian mixture model when the centers are separated by some Δ&gt;0. We present a nonasymptotic lower bound for the corresponding minimax Hamming risk improving on existing results. We also propose an optimal, efficient and adaptive procedure that is minimax rate optimal. The rate optimality is moreover sharp in the asymptotics when the sample size goes to infinity. Our procedure is based on a variant of Lloyd’s iterations initialized by a spectral method. As a consequence of nonasymptotic results, we find a sharp phase transition for the problem of exact recovery in the Gaussian mixture model. We prove that the phase transition occurs around the critical threshold Δ¯ given by Δ¯2=σ2(1+ 1+2p nlogn)logn.},
  archive      = {J_AOS},
  author       = {Mohamed Ndaoud},
  doi          = {10.1214/22-AOS2178},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2096-2126},
  shortjournal = {Ann. Statist.},
  title        = {Sharp optimal recovery in the two component gaussian mixture model},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A CLT for second difference estimators with an application
to volatility and intensity. <em>AOS</em>, <em>50</em>(4), 2072–2095.
(<a href="https://doi.org/10.1214/22-AOS2176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a general method for estimating the quadratic covariation of one or more spot parameter processes associated with continuous time semimartingales, and present a central limit theorem that has this class of estimators as one of its applications. The class of estimators we introduce, that we call Two-Scales Quadratic Covariation (TSQC) estimators, is based on sums of increments of second differences of the observed processes, and the intervals over which the differences are computed are rolling and overlapping. This latter feature lets us take full advantage of the data, and, by sufficiency considerations, ought to outperform estimators that are based on only one partition of the observational window. Moreover, a two-scales approach is employed to deal with asymptotic bias terms in a systematic manner, thus automatically giving consistent estimators without having to work out the form of the bias term on a case-to-case basis. We highlight the versatility of our central limit theorem by applying it to a novel leverage effect estimator that does not belong to the class of TSQC estimators. The principal empirical motivation for the present study is that the discrete times at which a continuous time semimartingale is observed might depend on features of the observable process other than its level, such as its spot-volatility process. As an application of the TSQC estimators, we therefore show how it may be used to estimate the quadratic covariation between the spot-volatility process and the intensity process of the observation times, when both of these are taken to be semimartingales. The finite sample properties of this estimator are studied by way of a simulation experiment, and we also apply this estimator in an empirical analysis of the Apple stock. Our analysis of the Apple stock indicates a rather strong correlation between the spot volatility process of the log-prices process and the times at which this stock is traded and hence observed.},
  archive      = {J_AOS},
  author       = {Emil A. Stoltenberg and Per A. Mykland and Lan Zhang},
  doi          = {10.1214/22-AOS2176},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2072-2095},
  shortjournal = {Ann. Statist.},
  title        = {A CLT for second difference estimators with an application to volatility and intensity},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalization error bounds of dynamic treatment regimes in
penalized regression-based learning. <em>AOS</em>, <em>50</em>(4),
2047–2071. (<a href="https://doi.org/10.1214/22-AOS2171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regime (DTR) is a sequence of decision rules, one per stage of intervention, that maps up-to-date patient information to a recommended treatment. Discovering an appropriate DTR for a given disease is a challenging issue especially when a large set of prognostic variables are observed. To address this problem, we propose penalized regression-based learning methods with l1 penalty to estimate the optimal DTR that would maximize the expected outcome if implemented. We also provide generalization error bounds of the estimated DTR in the setting of finite number of stages with multiple treatment options. We first examine the relationship between value and Q-functions and derive a finite sample upper bound on the difference in values between the optimal and the estimated DTRs. For practical implementation, we develop an algorithm with partial regularization via orthogonality to construct the optimal DTR. The advantages of the proposed methods are demonstrated with extensive simulation studies and data analysis of depression clinical trials.},
  archive      = {J_AOS},
  author       = {Eun Jeong Oh and Min Qian and Ying Kuen Cheung},
  doi          = {10.1214/22-AOS2171},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2047-2071},
  shortjournal = {Ann. Statist.},
  title        = {Generalization error bounds of dynamic treatment regimes in penalized regression-based learning},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic accuracy of the saddlepoint approximation for
maximum likelihood estimation. <em>AOS</em>, <em>50</em>(4), 2021–2046.
(<a href="https://doi.org/10.1214/22-AOS2169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The saddlepoint approximation gives an approximation to the density of a random variable in terms of its moment generating function. When the underlying random variable is itself the sum of n unobserved i.i.d. terms, the basic classical result is that the relative error in the density is of order 1/n. If instead the approximation is interpreted as a likelihood and maximised as a function of model parameters, the result is an approximation to the maximum likelihood estimate (MLE) that can be much faster to compute than the true MLE. This paper proves the analogous basic result for the approximation error between the saddlepoint MLE and the true MLE: subject to certain explicit identifiability conditions, the error has asymptotic size O(1/n2) for some parameters and O(1/n3/2) or O(1/n) for others. In all three cases, the approximation errors are asymptotically negligible compared to the inferential uncertainty. The proof is based on a factorisation of the saddlepoint likelihood into an exact and approximate term, along with an analysis of the approximation error in the gradient of the log-likelihood. This factorisation also gives insight into alternatives to the saddlepoint approximation, including a new and simpler saddlepoint approximation, for which we derive analogous error bounds. As a corollary of our results, we also obtain the asymptotic size of the MLE approximation error when the saddlepoint approximation is replaced by the normal approximation.},
  archive      = {J_AOS},
  author       = {Jesse Goodman},
  doi          = {10.1214/22-AOS2169},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2021-2046},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic accuracy of the saddlepoint approximation for maximum likelihood estimation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Distributed adaptive gaussian mean estimation with unknown
variance: Interactive protocol helps adaptation. <em>AOS</em>,
<em>50</em>(4), 1992–2020. (<a
href="https://doi.org/10.1214/21-AOS2167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed estimation of a Gaussian mean with unknown variance under communication constraints is studied. Necessary and sufficient communication costs under different types of distributed protocols are derived for any estimator that is adaptively rate-optimal over a range of possible values for the variance. Communication-efficient and statistically optimal procedures are developed. The analysis reveals an interesting and important distinction among different types of distributed protocols: compared to the independent protocols, interactive protocols such as the sequential and blackboard protocols require less communication costs for rate-optimal adaptive Gaussian mean estimation. The lower bound techniques developed in the present paper are novel and can be of independent interest.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Hongji Wei},
  doi          = {10.1214/21-AOS2167},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1992-2020},
  shortjournal = {Ann. Statist.},
  title        = {Distributed adaptive gaussian mean estimation with unknown variance: Interactive protocol helps adaptation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limit theorems for distributions invariant under groups of
transformations. <em>AOS</em>, <em>50</em>(4), 1960–1991. (<a
href="https://doi.org/10.1214/21-AOS2165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A distributional symmetry is invariance of a distribution under a group of transformations. Exchangeability and stationarity are examples. We explain that a result of ergodic theory implies a law of large numbers for such invariant distributions: If the group satisfies suitable conditions, expectations can be estimated by averaging over subsets of transformations, and these estimators are strongly consistent. We show that, if a mixing condition holds, the averages also satisfy a central limit theorem, a Berry–Esseen bound, and concentration. These are extended further to apply to triangular arrays, to randomly subsampled averages, and to a generalization of U-statistics. As applications, we obtain a general limit theorem for exchangeable random structures, and new results on stationary random fields, network models, and a class of marked point processes. We also establish asymptotic normality of the empirical entropy for a large class of processes. Some known results are recovered as special cases, and can hence be interpreted as an outcome of symmetry. The proofs adapt Stein’s method.},
  archive      = {J_AOS},
  author       = {Morgane Austern and Peter Orbanz},
  doi          = {10.1214/21-AOS2165},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1960-1991},
  shortjournal = {Ann. Statist.},
  title        = {Limit theorems for distributions invariant under groups of transformations},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On universally consistent and fully distribution-free rank
tests of vector independence. <em>AOS</em>, <em>50</em>(4), 1933–1959.
(<a href="https://doi.org/10.1214/21-AOS2151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank correlations have found many innovative applications in the last decade. In particular, suitable rank correlations have been used for consistent tests of independence between pairs of random variables. Using ranks is especially appealing for continuous data as tests become distribution-free. However, the traditional concept of ranks relies on ordering data and is, thus, tied to univariate observations. As a result, it has long remained unclear how one may construct distribution-free yet consistent tests of independence between random vectors. This is the problem addressed in this paper, in which we lay out a general framework for designing dependence measures that give tests of multivariate independence that are not only consistent and distribution-free but which we also prove to be statistically efficient. Our framework leverages the recently introduced concept of center-outward ranks and signs, a multivariate generalization of traditional ranks, and adopts a common standard form for dependence measures that encompasses many popular examples. In a unified study, we derive a general asymptotic representation of center-outward rank-based test statistics under independence, extending to the multivariate setting the classical Hájek asymptotic representation results. This representation permits direct calculation of limiting null distributions and facilitates a local power analysis that provides strong support for the center-outward approach by establishing, for the first time, the nontrivial power of center-outward rank-based tests over root-n neighborhoods within the class of quadratic mean differentiable alternatives.},
  archive      = {J_AOS},
  author       = {Hongjian Shi and Marc Hallin and Mathias Drton and Fang Han},
  doi          = {10.1214/21-AOS2151},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1933-1959},
  shortjournal = {Ann. Statist.},
  title        = {On universally consistent and fully distribution-free rank tests of vector independence},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal signal detection in some spiked random matrix
models: Likelihood ratio tests and linear spectral statistics.
<em>AOS</em>, <em>50</em>(4), 1910–1932. (<a
href="https://doi.org/10.1214/21-AOS2150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study signal detection by likelihood ratio tests in a number of spiked random matrix models, including but not limited to Gaussian mixtures and spiked Wishart covariance matrices. We work directly with multi-spiked cases in these models and with flexible priors on signal components that allow dependence across spikes. We derive asymptotic normality for the log-likelihood ratios when the signal-to-noise ratios are below certain bounds. In addition, the log-likelihood ratios can be asymptotically decomposed as weighted sums of a collection of statistics which we call bipartite signed cycles. Based on this decomposition, we show that below the bounds we could always achieve the asymptotically optimal powers of likelihood ratio tests via tests based on linear spectral statistics which have polynomial time complexity.},
  archive      = {J_AOS},
  author       = {Debapratim Banerjee and Zongming Ma},
  doi          = {10.1214/21-AOS2150},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1910-1932},
  shortjournal = {Ann. Statist.},
  title        = {Optimal signal detection in some spiked random matrix models: Likelihood ratio tests and linear spectral statistics},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting multiple replicating signals using adaptive
filtering procedures. <em>AOS</em>, <em>50</em>(4), 1890–1909. (<a
href="https://doi.org/10.1214/21-AOS2139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replicability is a fundamental quality of scientific discoveries: we are interested in those signals that are detectable in different laboratories, different populations, across time etc. Unlike meta-analysis which accounts for experimental variability but does not guarantee replicability, testing a partial conjunction (PC) null aims specifically to identify the signals that are discovered in multiple studies. In many contemporary applications, for example, comparing multiple high-throughput genetic experiments, a large number M of PC nulls need to be tested simultaneously, calling for a multiple comparisons correction. However, standard multiple testing adjustments on the M PC p-values can be severely conservative, especially when M is large and the signals are sparse. We introduce AdaFilter, a new multiple testing procedure that increases power by adaptively filtering out unlikely candidates of PC nulls. We prove that AdaFilter can control FWER and FDR as long as data across studies are independent, and has much higher power than other existing methods. We illustrate the application of AdaFilter with three examples: microarray studies of Duchenne muscular dystrophy, single-cell RNA sequencing of T cells in lung cancer tumors and GWAS for metabolomics.},
  archive      = {J_AOS},
  author       = {Jingshu Wang and Lin Gui and Weijie J. Su and Chiara Sabatti and Art B. Owen},
  doi          = {10.1214/21-AOS2139},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1890-1909},
  shortjournal = {Ann. Statist.},
  title        = {Detecting multiple replicating signals using adaptive filtering procedures},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence of de finetti’s mixing measure in latent
structure models for observed exchangeable sequences. <em>AOS</em>,
<em>50</em>(4), 1859–1889. (<a
href="https://doi.org/10.1214/21-AOS2120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of product distributions are a powerful device for learning about heterogeneity within data populations. In this class of latent structure models, de Finetti’s mixing measure plays the central role for describing the uncertainty about the latent parameters representing heterogeneity. In this paper, posterior contraction theorems for de Finetti’s mixing measure arising from finite mixtures of product distributions will be established; under the setting the number of exchangeable sequences of observed variables increases while sequence length(s) may be either fixed or varied. The role of both the number of sequences and the sequence lengths will be carefully examined. In order to obtain concrete rates of convergence, a first-order identifiability theory for finite mixture models and a family of sharp inverse bounds for mixtures of product distributions will be developed via a harmonic analysis of such latent structure models. This theory is applicable to broad classes of probability kernels composing the mixture model of product distributions for both continuous and discrete domain X. Examples of interest include the case the probability kernel is only weakly identifiable in the sense of (Ann. Statist. 44 (2016) 2726–2755), the case where the kernel is itself a mixture distribution as in hierarchical models, and the case the kernel may not have a density with respect to a dominating measure on an abstract domain X, such as Dirichlet processes.},
  archive      = {J_AOS},
  author       = {Yun Wei and XuanLong Nguyen},
  doi          = {10.1214/21-AOS2120},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1859-1889},
  shortjournal = {Ann. Statist.},
  title        = {Convergence of de finetti’s mixing measure in latent structure models for observed exchangeable sequences},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computational barriers to estimation from low-degree
polynomials. <em>AOS</em>, <em>50</em>(3), 1833–1858. (<a
href="https://doi.org/10.1214/22-AOS2179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental goal of high-dimensional statistics is to detect or recover planted structure (such as a low-rank matrix) hidden in noisy data. A growing body of work studies low-degree polynomials as a restricted model of computation for such problems: it has been demonstrated in various settings that low-degree polynomials of the data can match the statistical performance of the best known polynomial-time algorithms. Prior work has studied the power of low-degree polynomials for the task of detecting the presence of hidden structures. In this work, we extend these methods to address problems of estimation and recovery (instead of detection). For a large class of “signal plus noise” problems, we give a user-friendly lower bound for the best possible mean squared error achievable by any degree-D polynomial. To our knowledge, these are the first results to establish low-degree hardness of recovery problems for which the associated detection problem is easy. As applications, we give a tight characterization of the low-degree minimum mean squared error for the planted submatrix and planted dense subgraph problems, resolving (in the low-degree framework) open problems about the computational complexity of recovery in both cases.},
  archive      = {J_AOS},
  author       = {Tselil Schramm and Alexander S. Wein},
  doi          = {10.1214/22-AOS2179},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1833-1858},
  shortjournal = {Ann. Statist.},
  title        = {Computational barriers to estimation from low-degree polynomials},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence regions near singular information and boundary
points with applications to mixed models. <em>AOS</em>, <em>50</em>(3),
1806–1832. (<a href="https://doi.org/10.1214/22-AOS2177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose confidence regions with asymptotically correct uniform coverage probability of parameters whose Fisher information matrix can be singular at important points of the parameter set. Our work is motivated by the need for reliable inference on scale parameters close or equal to zero in mixed models, which is obtained as a special case. The confidence regions are constructed by inverting a continuous extension of the score test statistic standardized by expected information, which we show exists at points of singular information under regularity conditions. Similar results have previously only been obtained for scalar parameters, under conditions stronger than ours, and applications to mixed models have not been considered. In simulations our confidence regions have near-nominal coverage with as few as n=20 observations, regardless of how close to the boundary the true parameter is. It is a corollary of our main results that the proposed test statistic has an asymptotic chi-square distribution with degrees of freedom equal to the number of tested parameters, even if they are on the boundary of the parameter set.},
  archive      = {J_AOS},
  author       = {Karl Oskar Ekvall and Matteo Bottai},
  doi          = {10.1214/22-AOS2177},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1806-1832},
  shortjournal = {Ann. Statist.},
  title        = {Confidence regions near singular information and boundary points with applications to mixed models},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Optimal full ranking from pairwise comparisons.
<em>AOS</em>, <em>50</em>(3), 1775–1805. (<a
href="https://doi.org/10.1214/22-AOS2175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of ranking n players from partial pairwise comparison data under the Bradley–Terry–Luce model. For the first time in the literature, the minimax rate of this ranking problem is derived with respect to the Kendall’s tau distance that measures the difference between two rank vectors by counting the number of inversions. The minimax rate of ranking exhibits a transition between an exponential rate and a polynomial rate depending on the magnitude of the signal-to-noise ratio of the problem. To the best of our knowledge, this phenomenon is unique to full ranking and has not been seen in any other statistical estimation problem. To achieve the minimax rate, we propose a divide-and-conquer ranking algorithm that first divides the n players into groups of similar skills and then computes local MLE within each group. The optimality of the proposed algorithm is established by a careful approximate independence argument between the two steps.},
  archive      = {J_AOS},
  author       = {Pinhan Chen and Chao Gao and Anderson Y. Zhang},
  doi          = {10.1214/22-AOS2175},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1775-1805},
  shortjournal = {Ann. Statist.},
  title        = {Optimal full ranking from pairwise comparisons},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model selection in the space of gaussian models invariant by
symmetry. <em>AOS</em>, <em>50</em>(3), 1747–1774. (<a
href="https://doi.org/10.1214/22-AOS2174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multivariate centered Gaussian models for the random variable Z=(Z1,…,Zp), invariant under the action of a subgroup of the group of permutations on {1,…,p}. Using the representation theory of the symmetric group on the field of reals, we derive the distribution of the maximum likelihood estimate of the covariance parameter Σ and also the analytic expression of the normalizing constant of the Diaconis–Ylvisaker conjugate prior for the precision parameter K=Σ−1. We can thus perform Bayesian model selection in the class of complete Gaussian models invariant by the action of a subgroup of the symmetric group, which we could also call complete RCOP models. We illustrate our results with a toy example of dimension 4 and several examples for selection within cyclic groups, including a high- dimensional example with p=100.},
  archive      = {J_AOS},
  author       = {Piotr Graczyk and Hideyuki Ishi and Bartosz Kołodziejek and Hélène Massam},
  doi          = {10.1214/22-AOS2174},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1747-1774},
  shortjournal = {Ann. Statist.},
  title        = {Model selection in the space of gaussian models invariant by symmetry},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral estimation of hawkes processes from count data.
<em>AOS</em>, <em>50</em>(3), 1722–1746. (<a
href="https://doi.org/10.1214/22-AOS2173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a parametric estimation method for ill-observed linear stationary Hawkes processes. When the exact locations of points are not observed, but only counts over time intervals of fixed size, methods based on the likelihood are not feasible. We show that spectral estimation based on Whittle’s method is adapted to this case and provides consistent and asymptotically normal estimators, provided a mild moment condition on the reproduction function. Simulated data sets and a case-study illustrate the performances of the estimation, notably of the reproduction function even when time intervals are relatively large.},
  archive      = {J_AOS},
  author       = {Felix Cheysson and Gabriel Lang},
  doi          = {10.1214/22-AOS2173},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1722-1746},
  shortjournal = {Ann. Statist.},
  title        = {Spectral estimation of hawkes processes from count data},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsic riemannian functional data analysis for sparse
longitudinal observations. <em>AOS</em>, <em>50</em>(3), 1696–1721. (<a
href="https://doi.org/10.1214/22-AOS2172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new framework is developed to intrinsically analyze sparsely observed Riemannian functional data. It features four innovative components: a frame-independent covariance function, a smooth vector bundle termed covariance vector bundle, a parallel transport and a smooth bundle metric on the covariance vector bundle. The introduced intrinsic covariance function links estimation of covariance structure to smoothing problems that involve raw covariance observations derived from sparsely observed Riemannian functional data, while the covariance vector bundle provides a rigorous mathematical foundation for formulating such smoothing problems. The parallel transport and the bundle metric together make it possible to measure fidelity of fit to the covariance function. They also play a critical role in quantifying the quality of estimators for the covariance function. As an illustration, based on the proposed framework, we develop a local linear smoothing estimator for the covariance function, analyze its theoretical properties and provide numerical demonstration via simulated and real data sets. The intrinsic feature of the framework makes it applicable to not only Euclidean submanifolds but also manifolds without a canonical ambient space.},
  archive      = {J_AOS},
  author       = {Lingxuan Shao and Zhenhua Lin and Fang Yao},
  doi          = {10.1214/22-AOS2172},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1696-1721},
  shortjournal = {Ann. Statist.},
  title        = {Intrinsic riemannian functional data analysis for sparse longitudinal observations},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A precise high-dimensional asymptotic theory for boosting
and minimum-ℓ1-norm interpolated classifiers. <em>AOS</em>,
<em>50</em>(3), 1669–1695. (<a
href="https://doi.org/10.1214/22-AOS2170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes a precise high-dimensional asymptotic theory for boosting on separable data, taking statistical and computational perspectives. We consider a high-dimensional setting where the number of features (weak learners) p scales with the sample size n, in an overparametrized regime. Under a class of statistical models, we provide an exact analysis of the generalization error of boosting when the algorithm interpolates the training data and maximizes the empirical ℓ1-margin. Further, we explicitly pin down the relation between the boosting test error and the optimal Bayes error, as well as the proportion of active features at interpolation (with zero initialization). In turn, these precise characterizations answer certain questions raised in (Neural Comput. 11 (1999) 1493–1517; Ann. Statist. 26 (1998) 1651–1686) surrounding boosting, under assumed data generating processes. At the heart of our theory lies an in-depth study of the maximum-ℓ1-margin, which can be accurately described by a new system of nonlinear equations; to analyze this margin, we rely on Gaussian comparison techniques and develop a novel uniform deviation argument. Our statistical and computational arguments can handle (1) any finite-rank spiked covariance model for the feature distribution and (2) variants of boosting corresponding to general ℓq-geometry, q∈[1,2]. As a final component, via the Lindeberg principle, we establish a universality result showcasing that the scaled ℓ1-margin (asymptotically) remains the same, whether the covariates used for boosting arise from a nonlinear random feature model or an appropriately linearized model with matching moments.},
  archive      = {J_AOS},
  author       = {Tengyuan Liang and Pragya Sur},
  doi          = {10.1214/22-AOS2170},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1669-1695},
  shortjournal = {Ann. Statist.},
  title        = {A precise high-dimensional asymptotic theory for boosting and minimum-ℓ1-norm interpolated classifiers},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data-adaptive method for estimating density level sets
under shape conditions. <em>AOS</em>, <em>50</em>(3), 1653–1668. (<a
href="https://doi.org/10.1214/21-AOS2168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a random sample of points from some unknown density, we propose a method for estimating density level sets, for a positive threshold t, under the r-convexity assumption. This shape condition generalizes the convexity property and allows to consider level sets with more than one connected component. The main problem in practice is that r is an unknown geometric characteristic of the set related to its curvature, which may depend on t. A stochastic algorithm is proposed for selecting its value from data. The resulting reconstruction of the level set is able to achieve minimax rates for Hausdorff metric and distance in measure uniformly on the level t.},
  archive      = {J_AOS},
  author       = {Alberto Rodríguez-Casal and Paula Saavedra-Nieves},
  doi          = {10.1214/21-AOS2168},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1653-1668},
  shortjournal = {Ann. Statist.},
  title        = {A data-adaptive method for estimating density level sets under shape conditions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Partial recovery for top-k ranking: Optimality of MLE and
SubOptimality of the spectral method. <em>AOS</em>, <em>50</em>(3),
1618–1652. (<a href="https://doi.org/10.1214/21-AOS2166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given partially observed pairwise comparison data generated by the Bradley–Terry–Luce (BTL) model, we study the problem of top-k ranking. That is, to optimally identify the set of top-k players. We derive the minimax rate with respect to a normalized Hamming loss. This provides the first result in the literature that characterizes the partial recovery error in terms of the proportion of mistakes for top-k ranking. We also derive the optimal signal to noise ratio condition for the exact recovery of the top-k set. The maximum likelihood estimator (MLE) is shown to achieve both optimal partial recovery and optimal exact recovery. On the other hand, we show another popular algorithm, the spectral method, is in general suboptimal. Our results complement the recent work (Ann. Statist. 47 (2019) 2204–2235) that shows both the MLE and the spectral method achieve the optimal sample complexity for exact recovery. It turns out the leading constants of the sample complexity are different for the two algorithms. Another contribution that may be of independent interest is the analysis of the MLE without any penalty or regularization for the BTL model. This closes an important gap between theory and practice in the literature of ranking.},
  archive      = {J_AOS},
  author       = {Pinhan Chen and Chao Gao and Anderson Y. Zhang},
  doi          = {10.1214/21-AOS2166},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1618-1652},
  shortjournal = {Ann. Statist.},
  title        = {Partial recovery for top-k ranking: Optimality of MLE and SubOptimality of the spectral method},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cointegration in large VARs. <em>AOS</em>, <em>50</em>(3),
1593–1617. (<a href="https://doi.org/10.1214/21-AOS2164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper analyzes cointegration in vector autoregressive processes (VARs) for the cases when both the number of coordinates, N, and the number of time periods, T, are large and of the same order. We propose a way to examine a VAR of order 1 for the presence of cointegration based on a modification of the Johansen likelihood ratio test. The advantage of our procedure over the original Johansen test and its finite sample corrections is that our test does not suffer from overrejection. This is achieved through novel asymptotic theorems for eigenvalues of matrices in the test statistic in the regime of proportionally growing N and T. Our theoretical findings are supported by Monte Carlo simulations and an empirical illustration. Moreover, we find a surprising connection with multivariate analysis of variance (MANOVA) and explain why it emerges.},
  archive      = {J_AOS},
  author       = {Anna Bykhovskaya and Vadim Gorin},
  doi          = {10.1214/21-AOS2164},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1593-1617},
  shortjournal = {Ann. Statist.},
  title        = {Cointegration in large VARs},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uniform convergence of local fréchet regression with
applications to locating extrema and time warping for metric space
valued trajectories. <em>AOS</em>, <em>50</em>(3), 1573–1592. (<a
href="https://doi.org/10.1214/21-AOS2163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local Fréchet regression is a nonparametric regression method for metric space valued responses and Euclidean predictors, which can be utilized to obtain estimates of smooth trajectories taking values in general metric spaces from noisy metric space valued random objects. We derive uniform rates of convergence, which so far have eluded theoretical analysis of this method, for both fixed and random target trajectories, where we utilize tools from empirical processes. These results are shown to be widely applicable in metric space valued data analysis. In addition to simulations, we provide two pertinent examples where these results are important: The consistent estimation of the location of properly defined extrema in metric space valued trajectories, which we illustrate with the problem of locating the age of minimum brain connectivity as obtained from fMRI data; and time warping for metric space valued trajectories, illustrated with yearly age-at-death distributions for different countries.},
  archive      = {J_AOS},
  author       = {Yaqing Chen and Hans-Georg Müller},
  doi          = {10.1214/21-AOS2163},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1573-1592},
  shortjournal = {Ann. Statist.},
  title        = {Uniform convergence of local fréchet regression with applications to locating extrema and time warping for metric space valued trajectories},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale inference with block structure. <em>AOS</em>,
<em>50</em>(3), 1541–1572. (<a
href="https://doi.org/10.1214/21-AOS2162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of weak and rare effects in large amounts of data arises in a number of modern data analysis problems. Known results show that in this situation the potential of statistical inference is severely limited by the large-scale multiple testing that is inherent in these problems. Here, we show that fundamentally more powerful statistical inference is possible when there is some structure in the signal that can be exploited, for example, if the signal is clustered in many small blocks, as is the case in some relevant applications. We derive the detection boundary in such a situation where we allow both the number of blocks and the block length to grow polynomially with sample size. We derive these results both for the univariate and the multivariate settings as well as for the problem of detecting clusters in a network. These results recover as special cases the sparse signal detection problem (Ann. Statist. 32 (2004) 962–994) where there is no structure in the signal, as well as the scan problem (Statist. Sinica 23 (2013) 409–428) where the signal comprises a single interval. We develop methodology that allows optimal adaptive detection in the general setting, thus exploiting the structure if it is present without incurring a relevant penalty in the case where there is no structure. The advantage of this methodology can be considerable, as in the case of no structure the means need to increase at the rate logn to ensure detection, while the presence of structure allows detection even if the means decrease at a polynomial rate.},
  archive      = {J_AOS},
  author       = {Jiyao Kou and Guenther Walther},
  doi          = {10.1214/21-AOS2162},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1541-1572},
  shortjournal = {Ann. Statist.},
  title        = {Large-scale inference with block structure},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimax estimation of smooth densities in wasserstein
distance. <em>AOS</em>, <em>50</em>(3), 1519–1540. (<a
href="https://doi.org/10.1214/21-AOS2161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study nonparametric density estimation problems where error is measured in the Wasserstein distance, a metric on probability distributions popular in many areas of statistics and machine learning. We give the first minimax-optimal rates for this problem for general Wasserstein distances for two classes of densities: smooth probability densities on [0,1]d bounded away from 0, and sub-Gaussian densities lying in the Hölder class Cs, s∈(0,1). Unlike classical nonparametric density estimation, these rates depend on whether the densities in question are bounded below, even in the compactly supported case. Motivated by variational problems involving the Wasserstein distance, we also show how to construct discretely supported measures, suitable for computational purposes, which achieve the minimax rates. Our main technical tool is an inequality giving a nearly tight dual characterization of the Wasserstein distances in terms of Besov norms.},
  archive      = {J_AOS},
  author       = {Jonathan Niles-Weed and Quentin Berthet},
  doi          = {10.1214/21-AOS2161},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1519-1540},
  shortjournal = {Ann. Statist.},
  title        = {Minimax estimation of smooth densities in wasserstein distance},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistency of bayesian inference for multivariate
max-stable distributions. <em>AOS</em>, <em>50</em>(3), 1490–1518. (<a
href="https://doi.org/10.1214/21-AOS2160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting extreme events is important in many applications in risk analysis. Extreme-value theory suggests modelling extremes by max-stable distributions. The Bayesian approach provides a natural framework for statistical prediction. Although various Bayesian inferential procedures have been proposed in the literature of univariate extremes and some for multivariate extremes, the study of their asymptotic properties has been left largely untouched. In this paper we focus on a semiparametric Bayesian method for estimating max-stable distributions in arbitrary dimension. We establish consistency of the pertaining posterior distributions for fairly general, well-specified max-stable models, whose margins can be short-, light- or heavy-tailed. We then extend our consistency results to the case where data are samples of block maxima whose distribution is only approximately a max-stable one, which represents the most realistic inferential setting.},
  archive      = {J_AOS},
  author       = {Simone A. Padoan and Stefano Rizzelli},
  doi          = {10.1214/21-AOS2160},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1490-1518},
  shortjournal = {Ann. Statist.},
  title        = {Consistency of bayesian inference for multivariate max-stable distributions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new and flexible design construction for orthogonal arrays
for modern applications. <em>AOS</em>, <em>50</em>(3), 1473–1489. (<a
href="https://doi.org/10.1214/21-AOS2159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal array, a classical and effective tool for collecting data, has been flourished with its applications in modern computer experiments and engineering statistics. Driven by the wide use of computer experiments with both qualitative and quantitative factors, multiple computer experiments, multifidelity computer experiments, cross-validation and stochastic optimization, orthogonal arrays with certain structures have been introduced. Sliced orthogonal arrays and nested orthogonal arrays are examples of such arrays. This article introduces a flexible, fresh construction method, which uses smaller arrays and a special structure. The method uncovers the hidden structure of many existing fixed-level orthogonal arrays of given run sizes, possibly with more columns. It also allows fixed-level orthogonal arrays of nearly strength three to be constructed, which are useful as there are not many construction methods for fixed-level orthogonal arrays of strength three, and also helpful for generating Latin hypercube designs with desirable low-dimensional projections. Theoretical properties of the proposed method are explored. As by-products, several theoretical results on orthogonal arrays are obtained.},
  archive      = {J_AOS},
  author       = {Yuanzhen He and C. Devon Lin and Fasheng Sun},
  doi          = {10.1214/21-AOS2159},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1473-1489},
  shortjournal = {Ann. Statist.},
  title        = {A new and flexible design construction for orthogonal arrays for modern applications},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Higher criticism to compare two large frequency tables, with
sensitivity to possible rare and weak differences. <em>AOS</em>,
<em>50</em>(3), 1447–1472. (<a
href="https://doi.org/10.1214/21-AOS2158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We adapt Higher Criticism (HC) to the comparison of two frequency tables which may—or may not—exhibit moderate differences between the tables in some unknown, relatively small subset out of a large number of categories. Our analysis of the power of the proposed HC test quantifies the rarity and size of assumed differences and applies moderate deviations-analysis to determine the asymptotic powerfulness/powerlessness of our proposed HC procedure. Our analysis considers the null hypothesis of no difference in underlying generative model against a rare/weak perturbation alternative, in which the frequencies of N1−β out of the N categories are perturbed by r(logN)/2n in the Hellinger distance; here, n is the size of each sample. Our proposed Higher Criticism (HC) test for this setting uses P-values obtained from N exact binomial tests. We characterize the asymptotic performance of the HC-based test in terms of the sparsity parameter β and the perturbation intensity parameter r. Specifically, we derive a region in the (β,r)-plane where the test asymptotically has maximal power, while having asymptotically no power outside this region. Our analysis distinguishes between cases in which the counts in both tables are low, versus cases in which counts are high, corresponding to the cases of sparse and dense frequency tables. The phase transition curve of HC in the high-counts regime matches formally the curve delivered by HC in a two-sample normal means model.},
  archive      = {J_AOS},
  author       = {David L. Donoho and Alon Kipnis},
  doi          = {10.1214/21-AOS2158},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1447-1472},
  shortjournal = {Ann. Statist.},
  title        = {Higher criticism to compare two large frequency tables, with sensitivity to possible rare and weak differences},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cube root weak convergence of empirical estimators of a
density level set. <em>AOS</em>, <em>50</em>(3), 1423–1446. (<a
href="https://doi.org/10.1214/21-AOS2157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given n independent random vectors with common density f on Rd, we study the weak convergence of three empirical-measure based estimators of the convex λ-level set Lλ of f, namely the excess mass set, the minimum volume set and the maximum probability set, all selected from a class of convex sets A that contains Lλ. Since these set-valued estimators approach Lλ, even the formulation of their weak convergence is nonstandard. We identify the joint limiting distribution of the symmetric difference of Lλ and each of the three estimators, at rate n−1/3. It turns out that the minimum volume set and the maximum probability set estimators are asymptotically indistinguishable, whereas the excess mass set estimator exhibits “richer” limit behavior. Arguments rely on the boundary local empirical process, its cylinder representation, dimension-free concentration around the boundary of Lλ, and the set-valued argmax of a drifted Wiener process.},
  archive      = {J_AOS},
  author       = {Philippe Berthet and John H. J. Einmahl},
  doi          = {10.1214/21-AOS2157},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1423-1446},
  shortjournal = {Ann. Statist.},
  title        = {Cube root weak convergence of empirical estimators of a density level set},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ridge regression revisited: Debiasing, thresholding and
bootstrap. <em>AOS</em>, <em>50</em>(3), 1401–1422. (<a
href="https://doi.org/10.1214/21-AOS2156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of the Lasso in the era of high-dimensional data can be attributed to its conducting an implicit model selection, that is, zeroing out regression coefficients that are not significant. By contrast, classical ridge regression cannot reveal a potential sparsity of parameters, and may also introduce a large bias under the high-dimensional setting. Nevertheless, recent work on the Lasso involves debiasing and thresholding, the latter in order to further enhance the model selection. As a consequence, ridge regression may be worth another look since—after debiasing and thresholding—it may offer some advantages over the Lasso, for example, it can be easily computed using a closed-form expression. In this paper, we define a debiased and thresholded ridge regression method, and prove a consistency result and a Gaussian approximation theorem. We further introduce a wild bootstrap algorithm to construct confidence regions and perform hypothesis testing for a linear combination of parameters. In addition to estimation, we consider the problem of prediction, and present a novel, hybrid bootstrap algorithm tailored for prediction intervals. Extensive numerical simulations further show that the debiased and thresholded ridge regression has favorable finite sample performance and may be preferable in some settings.},
  archive      = {J_AOS},
  author       = {Yunyi Zhang and Dimitris N. Politis},
  doi          = {10.1214/21-AOS2156},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1401-1422},
  shortjournal = {Ann. Statist.},
  title        = {Ridge regression revisited: Debiasing, thresholding and bootstrap},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Optimal difference-based variance estimators in time
series: A general framework. <em>AOS</em>, <em>50</em>(3), 1376–1400.
(<a href="https://doi.org/10.1214/21-AOS2154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variance estimation is important for statistical inference. It becomes nontrivial when observations are masked by serial dependence structures and time-varying mean structures. Existing methods either ignore or sub-optimally handle these nuisance structures. This paper develops a general framework for the estimation of the long-run variance for time series with nonconstant means. The building blocks are difference statistics. The proposed class of estimators is general enough to cover many existing estimators. Necessary and sufficient conditions for consistency are investigated. The first asymptotically optimal estimator is derived. Our proposed estimator is theoretically proven to be invariant to arbitrary mean structures, which may include trends and a possibly divergent number of discontinuities.},
  archive      = {J_AOS},
  author       = {Kin Wai Chan},
  doi          = {10.1214/21-AOS2154},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1376-1400},
  shortjournal = {Ann. Statist.},
  title        = {Optimal difference-based variance estimators in time series: A general framework},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for the partially linear cox model.
<em>AOS</em>, <em>50</em>(3), 1348–1375. (<a
href="https://doi.org/10.1214/21-AOS2153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep learning approaches to survival data have demonstrated empirical success in applications, most of these methods are difficult to interpret and mathematical understanding of them is lacking. This paper studies the partially linear Cox model, where the nonlinear component of the model is implemented using a deep neural network. The proposed approach is flexible and able to circumvent the curse of dimensionality, yet it facilitates interpretability of the effects of treatment covariates on survival. We establish asymptotic theories of maximum partial likelihood estimators and show that our nonparametric deep neural network estimator achieves the minimax optimal rate of convergence (up to a polylogarithmic factor). Moreover, we prove that the corresponding finite-dimensional estimator for treatment covariate effects is n-consistent, asymptotically normal and attains semiparametric efficiency. Extensive simulation studies and analyses of two real survival data sets show the proposed estimator produces confidence intervals with superior coverage as well as survival time predictions with superior concordance to actual survival times.},
  archive      = {J_AOS},
  author       = {Qixian Zhong and Jonas Mueller and Jane-Ling Wang},
  doi          = {10.1214/21-AOS2153},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1348-1375},
  shortjournal = {Ann. Statist.},
  title        = {Deep learning for the partially linear cox model},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Doubly debiased lasso: High-dimensional inference under
hidden confounding. <em>AOS</em>, <em>50</em>(3), 1320–1347. (<a
href="https://doi.org/10.1214/21-AOS2152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring causal relationships or related associations from observational data can be invalidated by the existence of hidden confounding. We focus on a high-dimensional linear regression setting, where the measured covariates are affected by hidden confounding and propose the doubly debiased lasso estimator for individual components of the regression coefficient vector. Our advocated method simultaneously corrects both the bias due to estimation of high-dimensional parameters as well as the bias caused by the hidden confounding. We establish its asymptotic normality and also prove that it is efficient in the Gauss–Markov sense. The validity of our methodology relies on a dense confounding assumption, that is, that every confounding variable affects many covariates. The finite sample performance is illustrated with an extensive simulation study and a genomic application. The method is implemented by the DDL package available from CRAN.},
  archive      = {J_AOS},
  author       = {Zijian Guo and Domagoj Ćevid and Peter Bühlmann},
  doi          = {10.1214/21-AOS2152},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1320-1347},
  shortjournal = {Ann. Statist.},
  title        = {Doubly debiased lasso: High-dimensional inference under hidden confounding},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent order selection for ARFIMA processes.
<em>AOS</em>, <em>50</em>(3), 1297–1319. (<a
href="https://doi.org/10.1214/21-AOS2149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the orders of the autoregressive fractionally integrated moving average (ARFIMA) model has been a long-standing problem in time series analysis. This paper tackles this challenge by establishing the consistency of the Bayesian information criterion (BIC) for ARFIMA models with independent errors. Since the memory parameter of the model can be any real number, this consistency result is valid for short memory, long memory and nonstationary time series. This paper further extends the consistency of the BIC to ARFIMA models with conditional heteroscedastic errors, thereby extending its applications to encompass many real-life situations. Finite-sample implications of the theoretical results are illustrated via numerical examples.},
  archive      = {J_AOS},
  author       = {Hsueh-Han Huang and Ngai Hang Chan and Kun Chen and Ching-Kang Ing},
  doi          = {10.1214/21-AOS2149},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1297-1319},
  shortjournal = {Ann. Statist.},
  title        = {Consistent order selection for ARFIMA processes},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evidence factors from multiple, possibly invalid,
instrumental variables. <em>AOS</em>, <em>50</em>(3), 1266–1296. (<a
href="https://doi.org/10.1214/21-AOS2148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valid instrumental variables enable treatment effect inference even when selection into treatment is biased by unobserved confounders. When multiple candidate instruments are available, but some of them are possibly invalid, the previously proposed reinforced design enables one or more nearly independent valid analyses that depend on very different assumptions. That is, we can perform evidence factor analysis. However, the validity of the reinforced design depends crucially on the order in which multiple instrumental variable analyses are conducted. Motivated by the orthogonality of balanced factorial designs, we propose a balanced block design to offset the possible violation of the exclusion restriction by balancing the instruments against each other in the design, and demonstrate its utility for constructing approximate evidence factors under multiple analysis strategies free of the order imposition. We also propose a novel stratification method using multiple, nested candidate instruments, in which case the balanced block design is not applicable. We apply our proposed methods to evaluate (a) the effect of education on future earnings using instrumental variables arising from the disruption of education during World War II via the balanced block design, and (b) the causal effect of malaria on stunting among children in Western Kenya using three nested instruments.},
  archive      = {J_AOS},
  author       = {Anqi Zhao and Youjin Lee and Dylan S. Small and Bikram Karmakar},
  doi          = {10.1214/21-AOS2148},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1266-1296},
  shortjournal = {Ann. Statist.},
  title        = {Evidence factors from multiple, possibly invalid, instrumental variables},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design admissibility and de la garza phenomenon in
multifactor experiments. <em>AOS</em>, <em>50</em>(3), 1247–1265. (<a
href="https://doi.org/10.1214/21-AOS2147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of an optimal design for a given regression problem is an intricate optimization problem, especially for models with multivariate predictors. Design admissibility and invariance are main tools to reduce the complexity of the optimization problem and have been successfully applied for models with univariate predictors. In particular, several authors have developed sufficient conditions for the existence of minimally supported designs in univariate models, where the number of support points of the optimal design equals the number of parameters. These results generalize the celebrated de la Garza phenomenon (Ann. Math. Statistics 25 (1954) 123–130), which states that for a polynomial regression model of degree k−1 any optimal design can be based on k points. This paper provides—for the first time—extensions of these results for models with a multivariate predictor. In particular, we study a geometric characterization of the support points of an optimal design to provide sufficient conditions for the occurrence of the de la Garza phenomenon in models with multivariate predictors and characterize properties of admissible designs in terms of admissibility of designs in conditional univariate regression models.},
  archive      = {J_AOS},
  author       = {Holger Dette and Xin Liu and Rong-Xian Yue},
  doi          = {10.1214/21-AOS2147},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1247-1265},
  shortjournal = {Ann. Statist.},
  title        = {Design admissibility and de la garza phenomenon in multifactor experiments},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for low-rank tensors—no need to debias.
<em>AOS</em>, <em>50</em>(2), 1220–1245. (<a
href="https://doi.org/10.1214/21-AOS2146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the statistical inference for several low-rank tensor models. Specifically, in the Tucker low-rank tensor PCA or regression model, provided with any estimates achieving some attainable error rate, we develop the data-driven confidence regions for the singular subspace of the parameter tensor based on the asymptotic distribution of an updated estimate by two-iteration alternating minimization. The asymptotic distributions are established under some essential conditions on the signal-to-noise ratio (in PCA model) or sample size (in regression model). If the parameter tensor is further orthogonally decomposable, we develop the methods and nonasymptotic theory for inference on each individual singular vector. For the rank-one tensor PCA model, we establish the asymptotic distribution for general linear forms of principal components and confidence interval for each entry of the parameter tensor. Finally, numerical simulations are presented to corroborate our theoretical discoveries. In all of these models, we observe that different from many matrix/vector settings in existing work, debiasing is not required to establish the asymptotic distribution of estimates or to make statistical inference on low-rank tensors. In fact, due to the widely observed statistical-computational-gap for low-rank tensor estimation, one usually requires stronger conditions than the statistical (or information-theoretic) limit to ensure the computationally feasible estimation is achievable. Surprisingly, such conditions “incidentally” render a feasible low-rank tensor inference without debiasing.},
  archive      = {J_AOS},
  author       = {Dong Xia and Anru R. Zhang and Yuchen Zhou},
  doi          = {10.1214/21-AOS2146},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1220-1245},
  shortjournal = {Ann. Statist.},
  title        = {Inference for low-rank tensors—no need to debias},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All-in-one robust estimator of the gaussian mean.
<em>AOS</em>, <em>50</em>(2), 1193–1219. (<a
href="https://doi.org/10.1214/21-AOS2145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to show that a single robust estimator of the mean of a multivariate Gaussian distribution can enjoy five desirable properties. First, it is computationally tractable in the sense that it can be computed in a time, which is at most polynomial in dimension, sample size and the logarithm of the inverse of the contamination rate. Second, it is equivariant by translations, uniform scaling and orthogonal transformations. Third, it has a high breakdown point equal to 0.5, and a nearly minimax rate breakdown point approximately equal to 0.28. Fourth, it is minimax rate optimal, up to a logarithmic factor, when data consists of independent observations corrupted by adversarially chosen outliers. Fifth, it is asymptotically efficient when the rate of contamination tends to zero. The estimator is obtained by an iterative reweighting approach. Each sample point is assigned a weight that is iteratively updated by solving a convex optimization problem. We also establish a dimension-free nonasymptotic risk bound for the expected error of the proposed estimator. It is the first result of this kind in the literature and involves only the effective rank of the covariance matrix. Finally, we show that the obtained results can be extended to sub-Gaussian distributions, as well as to the cases of unknown rate of contamination or unknown covariance matrix.},
  archive      = {J_AOS},
  author       = {Arnak S. Dalalyan and Arshak Minasyan},
  doi          = {10.1214/21-AOS2145},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1193-1219},
  shortjournal = {Ann. Statist.},
  title        = {All-in-one robust estimator of the gaussian mean},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconciling design-based and model-based causal inferences
for split-plot experiments. <em>AOS</em>, <em>50</em>(2), 1170–1192. (<a
href="https://doi.org/10.1214/21-AOS2144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The split-plot design arose from agricultural science with experimental units, also known as the subplots, nested within groups known as the whole plots. It assigns different interventions at the whole-plot and subplot levels, respectively, providing a convenient way to accommodate hard-to-change factors. By design, subplots within the same whole plot receive the same level of the whole-plot intervention, and thereby induce a group structure on the final treatment assignments. A common strategy is to run an ordinary least squares (ols) regression of the outcome on the treatment indicators coupled with the robust standard errors clustered at the whole-plot level. It does not give consistent estimators for the treatment effects of interest when the whole-plot sizes vary. Another common strategy is to fit a linear mixed-effects model of the outcome with normal random effects and errors. It is a purely model-based approach and can be sensitive to violations of the parametric assumptions. In contrast, design-based inference assumes no outcome models and relies solely on the controllable randomization mechanism determined by the physical experiment. We first extend the existing design-based inference based on the Horvitz–Thompson estimator to the Hajek estimator, and establish the finite-population central limit theorem for both under split-plot randomization. We then reconcile the results with those under the model-based approach, and propose two regression strategies, namely (i) the weighted least squares (wls) fit of the unit-level data based on the inverse probability weighting and (ii) the ols fit of the aggregate data based on whole-plot total outcomes, to reproduce the Hajek and Horvitz–Thompson estimators, respectively. This, together with the asymptotic conservativeness of the corresponding cluster-robust covariances for estimating the true design-based covariances as we establish in the process, justifies the validity of the regression estimators for design-based inference. In light of the flexibility of regression formulation for covariate adjustment, we further extend the theory to the case with covariates, and demonstrate the efficiency gain by regression-based covariate adjustment via both asymptotic theory and simulation. Importantly, all our theories are either numeric or design-based, and hold regardless of how well the regression equations represent the true data generating process.},
  archive      = {J_AOS},
  author       = {Anqi Zhao and Peng Ding},
  doi          = {10.1214/21-AOS2144},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1170-1192},
  shortjournal = {Ann. Statist.},
  title        = {Reconciling design-based and model-based causal inferences for split-plot experiments},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical inference for principal components of spiked
covariance matrices. <em>AOS</em>, <em>50</em>(2), 1144–1169. (<a
href="https://doi.org/10.1214/21-AOS2143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the asymptotic behavior of the extreme eigenvalues and eigenvectors of the high-dimensional spiked sample covariance matrices, in the supercritical case when a reliable detection of spikes is possible. In particular, we derive the joint distribution of the extreme eigenvalues and the generalized components of the associated eigenvectors, that is, the projections of the eigenvectors onto arbitrary given direction, assuming that the dimension and sample size are comparably large. In general, the joint distribution is given in terms of linear combinations of finitely many Gaussian and Chi-square variables, with parameters depending on the projection direction and the spikes. Our assumption on the spikes is fully general. First, the strengths of spikes are only required to be slightly above the critical threshold and no upper bound on the strengths is needed. Second, multiple spikes, that is, spikes with the same strength, are allowed. Third, no structural assumption is imposed on the spikes. Thanks to the general setting, we can then apply the results to various high dimensional statistical hypothesis testing problems involving both the eigenvalues and eigenvectors. Specifically, we propose accurate and powerful statistics to conduct hypothesis testing on the principal components. These statistics are data-dependent and adaptive to the underlying true spikes. Numerical simulations also confirm the accuracy and powerfulness of our proposed statistics and illustrate significantly better performance compared to the existing methods in the literature. In particular, our methods are accurate and powerful even when either the spikes are small or the dimension is large.},
  archive      = {J_AOS},
  author       = {Zhigang Bao and Xiucai Ding and Jingming Wang and Ke Wang},
  doi          = {10.1214/21-AOS2143},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1144-1169},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference for principal components of spiked covariance matrices},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Max-sum tests for cross-sectional independence of
high-dimensional panel data. <em>AOS</em>, <em>50</em>(2), 1124–1143.
(<a href="https://doi.org/10.1214/21-AOS2142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a testing problem for cross-sectional independence for high-dimensional panel data, where the number of cross-sectional units is potentially much larger than the number of observations. The cross-sectional independence is described through linear regression models. We study three tests named the sum, the max and the max-sum tests, where the latter two are new. The sum test is initially proposed by Breusch and Pagan (1980). We design the max and sum tests for sparse and nonsparse correlation coefficients of random errors between the linear regression models, respectively. And the max-sum test is devised to compromise both situations on the correlation coefficients. Indeed, our simulation shows that the max-sum test outperforms the previous two tests. This makes the max-sum test very useful in practice where sparsity or not for a set of numbers is usually vague. Toward the theoretical analysis of the three tests, we have settled two conjectures regarding the sum of squares of sample correlation coefficients asked by Pesaran (2004 and 2008). In addition, we establish the asymptotic theory for maxima of sample correlation coefficients appeared in the linear regression model for panel data, which is also the first successful attempt to our knowledge. To study the max-sum test, we create a novel method to show asymptotic independence between maxima and sums of dependent random variables. We expect the method itself is useful for other problems of this nature. Finally, an extensive simulation study as well as a case study are carried out. They demonstrate advantages of our proposed methods in terms of both empirical powers and robustness for correlation coefficients of residuals regardless of sparsity or not.},
  archive      = {J_AOS},
  author       = {Long Feng and Tiefeng Jiang and Binghui Liu and Wei Xiong},
  doi          = {10.1214/21-AOS2142},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1124-1143},
  shortjournal = {Ann. Statist.},
  title        = {Max-sum tests for cross-sectional independence of high-dimensional panel data},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). False discovery rate control with unknown null distribution:
Is it possible to mimic the oracle? <em>AOS</em>, <em>50</em>(2),
1095–1123. (<a href="https://doi.org/10.1214/21-AOS2141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical multiple testing theory prescribes the null distribution, which is often too stringent an assumption for nowadays large scale experiments. This paper presents theoretical foundations to understand the limitations caused by ignoring the null distribution, and how it can be properly learned from the same data set, when possible. We explore this issue in the setting where the null distributions are Gaussian with unknown rescaling parameters (mean and variance) whereas the alternative distributions are let arbitrary. In that case, an oracle procedure is the Benjamini–Hochberg procedure applied with the true (unknown) null distribution and we aim at building a procedure that asymptotically mimics the performances of the oracle (AMO in short). Our main result establishes a phase transition at the sparsity boundary n/log(n): an AMO procedure exists if and only if the number of false nulls is of order less than n/log(n), where n is the total number of tests. Further sparsity boundaries are derived for general location models where the shape of the null distribution is not necessarily Gaussian. In light of our impossibility results, we also pursue the less stringent aim of building a nonparametric confidence region for the null distribution. From a practical perspective, this provides goodness-of-fit tests for the null distribution and allows to assess the reliability of empirical null procedures via novel diagnostic graphs. Our results are illustrated on numerical experiments and real data sets, as detailed in a companion vignette (Roquain and Verzelen (2021)).},
  archive      = {J_AOS},
  author       = {Etienne Roquain and Nicolas Verzelen},
  doi          = {10.1214/21-AOS2141},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1095-1123},
  shortjournal = {Ann. Statist.},
  title        = {False discovery rate control with unknown null distribution: Is it possible to mimic the oracle?},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative algorithm for discrete structure recovery.
<em>AOS</em>, <em>50</em>(2), 1066–1094. (<a
href="https://doi.org/10.1214/21-AOS2140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general modeling and algorithmic framework for discrete structure recovery that can be applied to a wide range of problems. Under this framework, we are able to study the recovery of clustering labels, ranks of players, signs of regression coefficients, cyclic shifts and even group elements from a unified perspective. A simple iterative algorithm is proposed for discrete structure recovery, which generalizes methods including Lloyd’s algorithm and the power method. A linear convergence result for the proposed algorithm is established in this paper under appropriate abstract conditions on stochastic errors and initialization. We illustrate our general theory by applying it on several representative problems: (1) clustering in Gaussian mixture model, (2) approximate ranking, (3) sign recovery in compressed sensing, (4) multireference alignment and (5) group synchronization, and show that minimax rate is achieved in each case.},
  archive      = {J_AOS},
  author       = {Chao Gao and Anderson Y. Zhang},
  doi          = {10.1214/21-AOS2140},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1066-1094},
  shortjournal = {Ann. Statist.},
  title        = {Iterative algorithm for discrete structure recovery},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Necessary and sufficient conditions for asymptotically
optimal linear prediction of random fields on compact metric spaces.
<em>AOS</em>, <em>50</em>(2), 1038–1065. (<a
href="https://doi.org/10.1214/21-AOS2138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal linear prediction (aka. kriging) of a random field {Z(x)}x∈X indexed by a compact metric space (X,dX) can be obtained if the mean value function m:X→R and the covariance function ϱ:X×X→R of Z are known. We consider the problem of predicting the value of Z(x∗) at some location x∗∈X based on observations at locations {xj}j=1n, which accumulate at x∗ as n→∞ (or, more generally, predicting φ(Z) based on {φj(Z)}j=1n for linear functionals φ,φ1,…,φn). Our main result characterizes the asymptotic performance of linear predictors (as n increases) based on an incorrect second-order structure (m˜,ϱ˜), without any restrictive assumptions on ϱ, ϱ˜ such as stationarity. We, for the first time, provide necessary and sufficient conditions on (m˜,ϱ˜) for asymptotic optimality of the corresponding linear predictor holding uniformly with respect to φ. These general results are illustrated by weakly stationary random fields on X⊂Rd with Matérn or periodic covariance functions, and on the sphere X= S2 for the case of two isotropic covariance functions.},
  archive      = {J_AOS},
  author       = {Kristin Kirchner and David Bolin},
  doi          = {10.1214/21-AOS2138},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1038-1065},
  shortjournal = {Ann. Statist.},
  title        = {Necessary and sufficient conditions for asymptotically optimal linear prediction of random fields on compact metric spaces},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate ranks and quantiles using optimal transport:
Consistency, rates and nonparametric testing. <em>AOS</em>,
<em>50</em>(2), 1012–1037. (<a
href="https://doi.org/10.1214/21-AOS2136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study multivariate ranks and quantiles, defined using the theory of optimal transport, and build on the work of Chernozhukov et al. (Ann. Statist. 45 (2017) 223–256) and Hallin et al. (Ann. Statist. 49 (2021) 1139–1165). We study the characterization, computation and properties of the multivariate rank and quantile functions and their empirical counterparts. We derive the uniform consistency of these empirical estimates to their population versions, under certain assumptions. In fact, we prove a Glivenko–Cantelli type theorem that shows the asymptotic stability of the empirical rank map in any direction. Under mild structural assumptions, we provide global and local rates of convergence of the empirical quantile and rank maps. We also provide a sub-Gaussian tail bound for the global L2-loss of the empirical quantile function. Further, we propose tuning parameter-free multivariate nonparametric tests—a two-sample test and a test for mutual independence—based on our notion of multivariate quantiles/ranks. Asymptotic consistency of these tests are shown and the rates of convergence of the associated test statistics are derived, both under the null and alternative hypotheses.},
  archive      = {J_AOS},
  author       = {Promit Ghosal and Bodhisattva Sen},
  doi          = {10.1214/21-AOS2136},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1012-1037},
  shortjournal = {Ann. Statist.},
  title        = {Multivariate ranks and quantiles using optimal transport: Consistency, rates and nonparametric testing},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motif estimation via subgraph sampling: The fourth-moment
phenomenon. <em>AOS</em>, <em>50</em>(2), 987–1011. (<a
href="https://doi.org/10.1214/21-AOS2134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network sampling is an indispensable tool for understanding features of large complex networks where it is practically impossible to search over the entire graph. In this paper, we develop a framework for statistical inference for counting network motifs, such as edges, triangles and wedges, in the widely used subgraph sampling model, where each vertex is sampled independently, and the subgraph induced by the sampled vertices is observed. We derive necessary and sufficient conditions for the consistency and the asymptotic normality of the natural Horvitz–Thompson (HT) estimator, which can be used for constructing confidence intervals and hypothesis testing for the motif counts based on the sampled graph. In particular, we show that the asymptotic normality of the HT estimator exhibits an interesting fourth-moment phenomenon, which asserts that the HT estimator (appropriately centered and rescaled) converges in distribution to the standard normal whenever its fourth-moment converges to 3 (the fourth-moment of the standard normal distribution). As a consequence, we derive the exact thresholds for consistency and asymptotic normality of the HT estimator in various natural graph ensembles, such as sparse graphs with bounded degree, Erdős–Rényi random graphs, random regular graphs and dense graphons.},
  archive      = {J_AOS},
  author       = {Bhaswar B. Bhattacharya and Sayan Das and Sumit Mukherjee},
  doi          = {10.1214/21-AOS2134},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {987-1011},
  shortjournal = {Ann. Statist.},
  title        = {Motif estimation via subgraph sampling: The fourth-moment phenomenon},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surprises in high-dimensional ridgeless least squares
interpolation. <em>AOS</em>, <em>50</em>(2), 949–986. (<a
href="https://doi.org/10.1214/21-AOS2133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpolators—estimators that achieve zero training error—have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum ℓ2 norm (“ridgeless”) interpolation least squares regression, focusing on the high-dimensional regime in which the number of unknown parameters p is of the same order as the number of samples n. We consider two different models for the feature distribution: a linear model, where the feature vectors xi∈Rp are obtained by applying a linear transform to a vector of i.i.d. entries, xi=Σ1/2zi (with zi∈Rp); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, xi=φ(Wzi) (with zi∈Rd, W∈Rp×d a matrix of i.i.d. entries, and φ an activation function acting componentwise on Wzi). We recover—in a precise quantitative way—several phenomena that have been observed in large-scale neural networks and kernel machines, including the “double descent” behavior of the prediction risk, and the potential benefits of overparametrization.},
  archive      = {J_AOS},
  author       = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
  doi          = {10.1214/21-AOS2133},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {949-986},
  shortjournal = {Ann. Statist.},
  title        = {Surprises in high-dimensional ridgeless least squares interpolation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). General and feasible tests with multiply-imputed datasets.
<em>AOS</em>, <em>50</em>(2), 930–948. (<a
href="https://doi.org/10.1214/21-AOS2132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation (MI) is a technique especially designed for handling missing data in public-use datasets. It allows analysts to perform incomplete-data inference straightforwardly by using several already imputed datasets released by the dataset owners. However, the existing MI tests require either a restrictive assumption on the missing-data mechanism, known as equal odds of missing information (EOMI), or an infinite number of imputations. Some of them also require analysts to have access to restrictive or nonstandard computer subroutines. Besides, the existing MI testing procedures cover only Wald’s tests and likelihood ratio tests but not Rao’s score tests, therefore, these MI testing procedures are not general enough. In addition, the MI Wald’s tests and MI likelihood ratio tests are not procedurally identical, so analysts need to resort to distinct algorithms for implementation. In this paper, we propose a general MI procedure, called stacked multiple imputation (SMI), for performing Wald’s tests, likelihood ratio tests and Rao’s score tests by a unified algorithm. SMI requires neither EOMI nor an infinite number of imputations. It is particularly feasible for analysts as they just need to use a complete-data testing device for performing the corresponding incomplete-data test.},
  archive      = {J_AOS},
  author       = {Kin Wai Chan},
  doi          = {10.1214/21-AOS2132},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {930-948},
  shortjournal = {Ann. Statist.},
  title        = {General and feasible tests with multiply-imputed datasets},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Functional sufficient dimension reduction through average
fréchet derivatives. <em>AOS</em>, <em>50</em>(2), 904–929. (<a
href="https://doi.org/10.1214/21-AOS2131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) embodies a family of methods that aim for reduction of dimensionality without loss of information in a regression setting. In this article, we propose a new method for nonparametric function-on-function SDR, where both the response and the predictor are a function. We first develop the notions of functional central mean subspace and functional central subspace, which form the population targets of our functional SDR. We then introduce an average Fréchet derivative estimator, which extends the gradient of the regression function to the operator level and enables us to develop estimators for our functional dimension reduction spaces. We show the resulting functional SDR estimators are unbiased and exhaustive, and more importantly, without imposing any distributional assumptions such as the linearity or the constant variance conditions that are commonly imposed by all existing functional SDR methods. We establish the uniform convergence of the estimators for the functional dimension reduction spaces, while allowing both the number of Karhunen–Loève expansions and the intrinsic dimension to diverge with the sample size. We demonstrate the efficacy of the proposed methods through both simulations and two real data examples.},
  archive      = {J_AOS},
  author       = {Kuang-Yao Lee and Lexin Li},
  doi          = {10.1214/21-AOS2131},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {904-929},
  shortjournal = {Ann. Statist.},
  title        = {Functional sufficient dimension reduction through average fréchet derivatives},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse high-dimensional linear regression. Estimating
squared error and a phase transition. <em>AOS</em>, <em>50</em>(2),
880–903. (<a href="https://doi.org/10.1214/21-AOS2130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a sparse high-dimensional regression model where the goal is to recover a k-sparse unknown binary vector β∗ from n noisy linear observations of the form Y=Xβ∗+W∈Rn where X∈Rn×p has i.i.d. N(0,1) entries and W∈Rn has i.i.d. N(0,σ2) entries. In the high signal-to-noise ratio regime and sublinear sparsity regime, while the order of the sample size needed to recover the unknown vector information-theoretically is known to be n∗:=2klogp/log(k/σ2+1), no polynomial-time algorithm is known to succeed unless n&gt;nalg:=(2k+σ2)logp. In this work, we offer a series of results investigating multiple computational and statistical aspects of the recovery task in the regime n∈[n∗,nalg]. First, we establish a novel information-theoretic property of the MLE of the problem happening around n=n∗ samples, which we coin as an “all-or-nothing behavior”: when n&gt;n∗ it recovers almost perfectly the support of β∗, while if n0 when nCnalg OGP disappears. OGP is a geometric “disconnectivity” property, which initially appeared in the theory of spin glasses and is known to suggest algorithmic hardness when it occurs. Finally, using certain technical results obtained to establish the OGP phase transition, we additionally establish various novel positive and negative algorithmic results for the recovery task of interest, including the failure of LASSO with access to nCnalg samples.},
  archive      = {J_AOS},
  author       = {David Gamarnik and Ilias Zadik},
  doi          = {10.1214/21-AOS2130},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {880-903},
  shortjournal = {Ann. Statist.},
  title        = {Sparse high-dimensional linear regression. estimating squared error and a phase transition},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive test of independence based on HSIC measures.
<em>AOS</em>, <em>50</em>(2), 858–879. (<a
href="https://doi.org/10.1214/21-AOS2129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hilbert–Schmidt Independence Criterion (HSIC) is a dependence measure based on reproducing kernel Hilbert spaces that is widely used to test independence between two random vectors. Remains the delicate choice of the kernel. In this work, we develop a new HSIC-based aggregated procedure which avoids such a kernel choice, and provide theoretical guarantees for this procedure. To achieve this, on the one hand, we introduce non-asymptotic single tests based on Gaussian kernels with a given bandwidth, which are of prescribed level. Then, we aggregate several single tests with different bandwidths, and prove sharp upper bounds for the uniform separation rate of the aggregated procedure over Sobolev balls. On the other hand, we provide a lower bound for the non-asymptotic minimax separation rate of testing over Sobolev balls, and deduce that the aggregated procedure is adaptive in the minimax sense over such regularity spaces. Finally, from a practical point of view, we perform numerical studies in order to assess the efficiency of our aggregated procedure and compare it to existing tests in the literature.},
  archive      = {J_AOS},
  author       = {Mélisande Albert and Béatrice Laurent and Amandine Marrel and Anouar Meynaoui},
  doi          = {10.1214/21-AOS2129},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {858-879},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive test of independence based on HSIC measures},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal false discovery rate control for large scale
multiple testing with auxiliary information. <em>AOS</em>,
<em>50</em>(2), 807–857. (<a
href="https://doi.org/10.1214/21-AOS2128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multiple testing is a fundamental problem in high dimensional statistical inference. It is increasingly common that various types of auxiliary information, reflecting the structural relationship among the hypotheses, are available. Exploiting such auxiliary information can boost statistical power. To this end, we propose a framework based on a two-group mixture model with varying probabilities of being null for different hypotheses a priori, where a shape-constrained relationship is imposed between the auxiliary information and the prior probabilities of being null. An optimal rejection rule is designed to maximize the expected number of true positives when average false discovery rate is controlled. Focusing on the ordered structure, we develop a robust EM algorithm to estimate the prior probabilities of being null and the distribution of p-values under the alternative hypothesis simultaneously. We show that the proposed method has better power than state-of-the-art competitors while controlling the false discovery rate, both empirically and theoretically. Extensive simulations demonstrate the advantage of the proposed method. Datasets from genome-wide association studies are used to illustrate the new methodology.},
  archive      = {J_AOS},
  author       = {Hongyuan Cao and Jun Chen and Xianyang Zhang},
  doi          = {10.1214/21-AOS2128},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {807-857},
  shortjournal = {Ann. Statist.},
  title        = {Optimal false discovery rate control for large scale multiple testing with auxiliary information},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference for change points in high-dimensional data via
selfnormalization. <em>AOS</em>, <em>50</em>(2), 781–806. (<a
href="https://doi.org/10.1214/21-AOS2127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers change-point testing and estimation for a sequence of high-dimensional data. In the case of testing for a mean shift for high-dimensional independent data, we propose a new test which is based on U-statistic in Chen and Qin (Ann. Statist. 38 (2010) 808–835) and utilizes the self-normalization principle (Shao J. R. Stat. Soc. Ser. B. Stat. Methodol. 72 (2010) 343–366; Shao and Zhang J. Amer. Statist. Assoc. 105 (2010) 1228–1240). Our test targets dense alternatives in the high-dimensional setting and involves no tuning parameters. To extend to change-point testing for high-dimensional time series, we introduce a trimming parameter and formulate a self-normalized test statistic with trimming to accommodate the weak temporal dependence. On the theory front we derive the limiting distributions of self-normalized test statistics under both the null and alternatives for both independent and dependent high-dimensional data. At the core of our asymptotic theory, we obtain weak convergence of a sequential U-statistic based process for high-dimensional independent data, and weak convergence of sequential trimmed U-statistic based processes for high-dimensional linear processes, both of which are of independent interests. Additionally, we illustrate how our tests can be used in combination with wild binary segmentation to estimate the number and location of multiple change points. Numerical simulations demonstrate the competitiveness of our proposed testing and estimation procedures in comparison with several existing methods in the literature.},
  archive      = {J_AOS},
  author       = {Runmin Wang and Changbo Zhu and Stanislav Volgushev and Xiaofeng Shao},
  doi          = {10.1214/21-AOS2127},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {781-806},
  shortjournal = {Ann. Statist.},
  title        = {Inference for change points in high-dimensional data via selfnormalization},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parametric copula adjusted for non- and semiparametric
regression. <em>AOS</em>, <em>50</em>(2), 754–780. (<a
href="https://doi.org/10.1214/21-AOS2126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multivariate response regression model where each coordinate is described by a location-scale non- or semiparametric regression and where the dependence structure of the “noise term” is described by a parametric copula. Our goal is to estimate the associated Euclidean copula parameter, given a sample of the response and the covariate. In the absence of the copula sample, the usual oracle ranks are no longer computable. Instead, we study the normal scores estimator for the Gaussian copula and generalized pseudo-likelihood estimation for general parametric copulas, both based on residual ranks calculated from preliminary non- or semiparametric estimators of the location and scale functions. We show that the residual-based estimators are asymptotically equivalent to their oracle counterparts and provide explicit rate of convergence. Partially to serve this objective, we also study weighted convergence of the residual empirical process under the non- or semiparametric regression model.},
  archive      = {J_AOS},
  author       = {Yue Zhao and Irène Gijbels and Ingrid Van Keilegom},
  doi          = {10.1214/21-AOS2126},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {754-780},
  shortjournal = {Ann. Statist.},
  title        = {Parametric copula adjusted for non- and semiparametric regression},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edgeworth expansions for network moments. <em>AOS</em>,
<em>50</em>(2), 726–753. (<a
href="https://doi.org/10.1214/21-AOS2125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network method of moments (Ann. Statist. 39 (2011) 2280–2301) is an important tool for nonparametric network inference. However, there has been little investigation on accurate descriptions of the sampling distributions of network moment statistics. In this paper, we present the first higher-order accurate approximation to the sampling CDF of a studentized network moment by Edgeworth expansion. In sharp contrast to classical literature on noiseless U-statistics, we show that the Edgeworth expansion of a network moment statistic as a noisy U-statistic can achieve higher-order accuracy without nonlattice or smoothness assumptions but just requiring weak regularity conditions. Behind this result is our surprising discovery that the two typically-hated factors in network analysis, namely, sparsity and edgewise observational errors, jointly play a blessing role, contributing a crucial self-smoothing effect in the network moment statistic and making it analytically tractable. Our assumptions match the minimum requirements in related literature. For sparse networks, our theory shows that our empirical Edgeworth expansion and a simple normal approximation both achieve the same gradually depreciating Berry–Esseen-type bound as the network becomes sparser. This result also significantly refines the best previous theoretical result. For practitioners, our empirical Edgeworth expansion is highly accurate and computationally efficient. It is also easy to implement and convenient for parallel computing. We demonstrate the clear advantage of our method by several comprehensive simulation studies. As a byproduct, we also provide a finite-sample analysis of the network jackknife. We showcase three applications of our results in network inference. We prove, to our knowledge, the first theoretical guarantee of higher-order accuracy for some network bootstrap schemes, and moreover, the first theoretical guidance for selecting the subsample size for network subsampling. We also derive a one-sample test and the Cornish–Fisher confidence interval for a given moment with higher-order accurate controls of confidence level and type I error, respectively.},
  archive      = {J_AOS},
  author       = {Yuan Zhang and Dong Xia},
  doi          = {10.1214/21-AOS2125},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {726-753},
  shortjournal = {Ann. Statist.},
  title        = {Edgeworth expansions for network moments},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Distributed nonparametric function estimation: Optimal rate
of convergence and cost of adaptation. <em>AOS</em>, <em>50</em>(2),
698–725. (<a href="https://doi.org/10.1214/21-AOS2124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed minimax estimation and distributed adaptive estimation under communication constraints for Gaussian sequence model and white noise model are studied. The minimax rate of convergence for distributed estimation over a given Besov class, which serves as a benchmark for the cost of adaptation, is established. We then quantify the exact communication cost for adaptation and construct an optimally adaptive procedure for distributed estimation over a range of Besov classes. The results demonstrate significant differences between nonparametric function estimation in the distributed setting and the conventional centralized setting. For global estimation, adaptation in general cannot be achieved for free in the distributed setting. The new technical tools to obtain the exact characterization for the cost of adaptation can be of independent interest.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Hongji Wei},
  doi          = {10.1214/21-AOS2124},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {698-725},
  shortjournal = {Ann. Statist.},
  title        = {Distributed nonparametric function estimation: Optimal rate of convergence and cost of adaptation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refined cramér-type moderate deviation theorems for general
self-normalized sums with applications to dependent random variables and
winsorized mean. <em>AOS</em>, <em>50</em>(2), 673–697. (<a
href="https://doi.org/10.1214/21-AOS2122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let {(Xi,Yi)}i=1n be a sequence of independent bivariate random vectors. In this paper, we establish a refined Cramér-type moderate deviation theorem for the general self-normalized sum ∑ i=1nXi/(∑i=1nYi2)1/2, which unifies and extends the classical Cramér (Actual. Sci. Ind. 736 (1938) 5–23) theorem and the self-normalized Cramér-type moderate deviation theorems by Jing, Shao and Wang (Ann. Probab. 31 (2003) 2167–2215) as well as the further refined version by Wang (J. Theoret. Probab. 24 (2011) 307–329). The advantage of our result is evidenced through successful applications to weakly dependent random variables and self-normalized winsorized mean. Specifically, by applying our new framework on general self-normalized sum, we significantly improve Cramér-type moderate deviation theorems for one-dependent random variables, geometrically β-mixing random variables and causal processes under geometrical moment contraction. As an additional application, we also derive the Cramér-type moderate deviation theorems for self-normalized winsorized mean.},
  archive      = {J_AOS},
  author       = {Lan Gao and Qi-Man Shao and Jiasheng Shi},
  doi          = {10.1214/21-AOS2122},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {673-697},
  shortjournal = {Ann. Statist.},
  title        = {Refined cramér-type moderate deviation theorems for general self-normalized sums with applications to dependent random variables and winsorized mean},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive estimation in multivariate response regression with
hidden variables. <em>AOS</em>, <em>50</em>(2), 640–672. (<a
href="https://doi.org/10.1214/21-AOS2059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prominent concern of scientific investigators is the presence of unobserved hidden variables in association analysis. Ignoring hidden variables often yields biased statistical results and misleading scientific conclusions. Motivated by this practical issue, this paper studies the multivariate response regression with hidden variables, Y=(Ψ∗)TX+(B∗)TZ+E, where Y∈Rm is the response vector, X∈Rp is the observable feature, Z∈RK represents the vector of unobserved hidden variables, possibly correlated with X, and E is an independent error. The number of hidden variables K is unknown and both m and p are allowed, but not required, to grow with the sample size n. Though Ψ∗ is shown to be nonidentifiable due to the presence of hidden variables, we propose to identify the projection of Ψ∗ onto the orthogonal complement of the row space of B∗, denoted by Θ∗. The quantity (Θ∗)TX measures the effect of X on Y that cannot be explained through the hidden variables, and thus Θ∗ is treated as the parameter of interest. Motivated by the identifiability proof, we propose a novel estimation algorithm for Θ∗, called HIVE, under homoscedastic errors. The first step of the algorithm estimates the best linear prediction of Y given X, in which the unknown coefficient matrix exhibits an additive decomposition of Ψ∗ and a dense matrix due to the correlation between X and Z. Under the sparsity assumption on Ψ∗, we propose to minimize a penalized least squares loss by regularizing Ψ∗ and the dense matrix via group-lasso and multivariate ridge, respectively. Nonasymptotic deviation bounds of the in-sample prediction error are established. Our second step estimates the row space of B∗ by leveraging the covariance structure of the residual vector from the first step. In the last step, we estimate Θ∗ via projecting Y onto the orthogonal complement of the estimated row space of B∗ to remove the effect of hidden variables. Nonasymptotic error bounds of our final estimator of Θ∗, which are valid for any m,p,K and n, are established. We further show that, under mild assumptions, the rate of our estimator matches the best possible rate with known B∗ and is adaptive to the unknown sparsity of Θ∗ induced by the sparsity of Ψ∗. The model identifiability, estimation algorithm and statistical guarantees are further extended to the setting with heteroscedastic errors. Thorough numerical simulations and two real data examples are provided to back up our theoretical results.},
  archive      = {J_AOS},
  author       = {Xin Bing and Yang Ning and Yaosheng Xu},
  doi          = {10.1214/21-AOS2059},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {640-672},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive estimation in multivariate response regression with hidden variables},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testability of high-dimensional linear models with nonsparse
structures. <em>AOS</em>, <em>50</em>(2), 615–639. (<a
href="https://doi.org/10.1214/19-AOS1932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding statistical inference under possibly nonsparse high-dimensional models has gained much interest recently. For a given component of the regression coefficient, we show that the difficulty of the problem depends on the sparsity of the corresponding row of the precision matrix of the covariates, not the sparsity of the regression coefficients. We develop new concepts of uniform and essentially uniform nontestability that allow the study of limitations of tests across a broad set of alternatives. Uniform nontestability identifies a collection of alternatives such that the power of any test, against any alternative in the group, is asymptotically at most equal to the nominal size. Implications of the new constructions include new minimax testability results that, in sharp contrast to the current results, do not depend on the sparsity of the regression parameters. We identify new tradeoffs between testability and feature correlation. In particular, we show that, in models with weak feature correlations, minimax lower bound can be attained by a test whose power has the n rate, regardless of the size of the model sparsity.},
  archive      = {J_AOS},
  author       = {Jelena Bradic and Jianqing Fan and Yinchu Zhu},
  doi          = {10.1214/19-AOS1932},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {615-639},
  shortjournal = {Ann. Statist.},
  title        = {Testability of high-dimensional linear models with nonsparse structures},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor clustering with planted structures: Statistical
optimality and computational limits. <em>AOS</em>, <em>50</em>(1),
584–613. (<a href="https://doi.org/10.1214/21-AOS2123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the statistical and computational limits of high-order clustering with planted structures. We focus on two clustering models, constant high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and study the methods and theory for testing whether a cluster exists (detection) and identifying the support of cluster (recovery). Specifically, we identify the sharp boundaries of signal-to-noise ratio for which CHC and ROHC detection/recovery are statistically possible. We also develop the tight computational thresholds: when the signal-to-noise ratio is below these thresholds, we prove that polynomial-time algorithms cannot solve these problems under the computational hardness conjectures of hypergraphic planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS) recovery. We also propose polynomial-time tensor algorithms that achieve reliable detection and recovery when the signal-to-noise ratio is above these thresholds. Both sparsity and tensor structures yield the computational barriers in high-order tensor clustering. The interplay between them results in significant differences between high-order tensor clustering and matrix clustering in literature in aspects of statistical and computational phase transition diagrams, algorithmic approaches, hardness conjecture, and proof techniques. To our best knowledge, we are the first to give a thorough characterization of the statistical and computational trade-off for such a double computational-barrier problem. Finally, we provide evidence for the computational hardness conjectures of HPC detection (via low-degree polynomial and Metropolis methods) and HPDS recovery (via low-degree polynomial method).},
  archive      = {J_AOS},
  author       = {Yuetian Luo and Anru R. Zhang},
  doi          = {10.1214/21-AOS2123},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {584-613},
  shortjournal = {Ann. Statist.},
  title        = {Tensor clustering with planted structures: Statistical optimality and computational limits},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Backfitting for large scale crossed random effects
regressions. <em>AOS</em>, <em>50</em>(1), 560–583. (<a
href="https://doi.org/10.1214/21-AOS2121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression models with crossed random effect errors can be very expensive to compute. The cost of both generalized least squares and Gibbs sampling can easily grow as N3/2 (or worse) for N observations. Papaspiliopoulos, Roberts and Zanella (Biometrika 107 (2020) 25–40) present a collapsed Gibbs sampler that costs O(N), but under an extremely stringent sampling model. We propose a backfitting algorithm to compute a generalized least squares estimate and prove that it costs O(N). A critical part of the proof is in ensuring that the number of iterations required is O(1), which follows from keeping a certain matrix norm below 1−δ for some δ&gt;0. Our conditions are greatly relaxed compared to those for the collapsed Gibbs sampler, though still strict. Empirically, the backfitting algorithm has a norm below 1−δ under conditions that are less strict than those in our assumptions. We illustrate the new algorithm on a ratings data set from Stitch Fix.},
  archive      = {J_AOS},
  author       = {Swarnadip Ghosh and Trevor Hastie and Art B. Owen},
  doi          = {10.1214/21-AOS2121},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {560-583},
  shortjournal = {Ann. Statist.},
  title        = {Backfitting for large scale crossed random effects regressions},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On an extension of the promotion time cure model.
<em>AOS</em>, <em>50</em>(1), 537–559. (<a
href="https://doi.org/10.1214/21-AOS2119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the distribution of time-to-event data that is subject to censoring and for which the event of interest might never occur, that is, some subjects are cured. To model this kind of data in the presence of covariates, one of the leading semiparametric models is the promotion time cure model (Stochastic Models of Tumor Latency and Their Biostatistical Applications (1996) World Scientific), which adapts the Cox model to the presence of cured subjects. Estimating the conditional distribution results in a complicated constrained optimization problem, and inference is difficult as no closed-formula for the variance is available. We propose a new model, inspired by the Cox model, that leads to a simple estimation procedure and that presents a closed formula for the variance. In this paper, we show (i) that the new model contains as a special case the promotion time cure model with an exponential link, and hence we have a simpler way to estimate the latter model than what is done so far in the literature; (ii) that in the latter special case, both estimators are equal to the partial likelihood estimator under the usual Cox model; (iii) that the estimators under the new model have certain asymptotic properties when the model is correct and when it is misspecified; (iv) that the error of LASSO type estimators is of order log(nd)/n in the case of high-dimensional covariates with dimension d and sample size n. We also study the practical behaviour of our estimation procedure by means of simulations, and we apply our model and estimation method to a breast cancer data set.},
  archive      = {J_AOS},
  author       = {Jad Beyhum and Anouar El Ghouch and François Portier and Ingrid Van Keilegom},
  doi          = {10.1214/21-AOS2119},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {537-559},
  shortjournal = {Ann. Statist.},
  title        = {On an extension of the promotion time cure model},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust sub-gaussian estimation of a mean vector in nearly
linear time. <em>AOS</em>, <em>50</em>(1), 511–536. (<a
href="https://doi.org/10.1214/21-AOS2118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct an algorithm for estimating the mean of a heavy-tailed random variable when given an adversarial corrupted sample of N independent observations. The only assumption we make on the distribution of the noncorrupted (or informative) data is the existence of a covariance matrix Σ, unknown to the statistician. Our algorithm outputs μˆ, which is robust to the presence of |O| adversarial outliers and satisfies (1)‖μˆ−μ‖2≲ Tr(Σ) N+ ‖Σ‖opK N with probability at least 1−exp(−c0K)−exp(−c1u), and runtime O˜(Nd+uKd) where K∈{600|O|,…,N} and u∈N∗ are two parameters of the algorithm. The algorithm is fully data-dependent and does not use (1) in its construction, which combines recently developed tools for median-of-means estimators and covering semidefinite programming. We also show that this algorithm can automatically adapt to the number of outliers (adaptive choice of K) and that it satisfies the same bound in expectation.},
  archive      = {J_AOS},
  author       = {Jules Depersin and Guillaume Lecué},
  doi          = {10.1214/21-AOS2118},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {511-536},
  shortjournal = {Ann. Statist.},
  title        = {Robust sub-gaussian estimation of a mean vector in nearly linear time},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semiparametric latent-class models for multivariate
longitudinal and survival data. <em>AOS</em>, <em>50</em>(1), 487–510.
(<a href="https://doi.org/10.1214/21-AOS2117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-term follow-up studies, data are often collected on repeated measures of multivariate response variables as well as on time to the occurrence of a certain event. To jointly analyze such longitudinal data and survival time, we propose a general class of semiparametric latent-class models that accommodates a heterogeneous study population with flexible dependence structures between the longitudinal and survival outcomes. We combine nonparametric maximum likelihood estimation with sieve estimation and devise an efficient EM algorithm to implement the proposed approach. We establish the asymptotic properties of the proposed estimators through novel use of modern empirical process theory, sieve estimation theory and semiparametric efficiency theory. Finally, we demonstrate the advantages of the proposed methods through extensive simulation studies and provide an application to the Atherosclerosis Risk in Communities study.},
  archive      = {J_AOS},
  author       = {Kin Yau Wong and Donglin Zeng and D. Y. Lin},
  doi          = {10.1214/21-AOS2117},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {487-510},
  shortjournal = {Ann. Statist.},
  title        = {Semiparametric latent-class models for multivariate longitudinal and survival data},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Canonical thresholding for nonsparse high-dimensional linear
regression. <em>AOS</em>, <em>50</em>(1), 460–486. (<a
href="https://doi.org/10.1214/21-AOS2116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a high-dimensional linear regression problem. Unlike many papers on the topic, we do not require sparsity of the regression coefficients; instead, our main structural assumption is a decay of eigenvalues of the covariance matrix of the data. We propose a new family of estimators, called the canonical thresholding estimators, which pick largest regression coefficients in the canonical form. The estimators admit an explicit form and can be linked to LASSO and Principal Component Regression (PCR). A theoretical analysis for both fixed design and random design settings is provided. Obtained bounds on the mean squared error and the prediction error of a specific estimator from the family allow to clearly state sufficient conditions on the decay of eigenvalues to ensure convergence. In addition, we promote the use of the relative errors, strongly linked with the out-of-sample R2. The study of these relative errors leads to a new concept of joint effective dimension, which incorporates the covariance of the data and the regression coefficients simultaneously, and describes the complexity of a linear regression problem. Some minimax lower bounds are established to showcase the optimality of our procedure. Numerical simulations confirm good performance of the proposed estimators compared to the previously developed methods.},
  archive      = {J_AOS},
  author       = {Igor Silin and Jianqing Fan},
  doi          = {10.1214/21-AOS2116},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {460-486},
  shortjournal = {Ann. Statist.},
  title        = {Canonical thresholding for nonsparse high-dimensional linear regression},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimax nonparametric estimation of pure quantum states.
<em>AOS</em>, <em>50</em>(1), 430–459. (<a
href="https://doi.org/10.1214/21-AOS2115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical statistics, Pinsker’s theorem provides an exact asymptotic minimax bound in nonparametric estimation, improving upon optimal rates of convergence results. We obtain a quantum version of the theorem by establishing asymptotic minimax results for estimation of the displacement vector in a quantum Gaussian white noise model, given by a sequence of shifted vacuum states. Analogous results are then obtained for estimation of a general pure state from an ensemble of identically prepared, independent quantum systems, using the recently established local asymptotic equivalence to the quantum Gaussian white noise model. Optimality holds with respect to the most fundamental distance measure for quantum states, that is, trace norm distance, and in a true quantum sense, allowing for all possible measurements. Adaptive estimators are also obtained for the above cases. As an application, we obtain asymptotic minimax adaptive estimators for Wigner functions of pure states.},
  archive      = {J_AOS},
  author       = {Samriddha Lahiry and Michael Nussbaum},
  doi          = {10.1214/21-AOS2115},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {430-459},
  shortjournal = {Ann. Statist.},
  title        = {Minimax nonparametric estimation of pure quantum states},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing equivalence of clustering. <em>AOS</em>,
<em>50</em>(1), 407–429. (<a
href="https://doi.org/10.1214/21-AOS2113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we test whether two data sets measured on the same set of subjects share a common clustering structure. As a leading example, we focus on comparing clustering structures in two independent random samples from two deterministic two-component mixtures of multivariate Gaussian distributions. Mean parameters of these Gaussian distributions are treated as potentially unknown nuisance parameters and are allowed to differ. Assuming knowledge of mean parameters, we first determine the phase diagram of the testing problem over the entire range of signal-to-noise ratios by providing both lower bounds and tests that achieve them. When nuisance parameters are unknown, we propose tests that achieve the detection boundary adaptively as long as ambient dimensions of the data sets grow at a sublinear rate with the sample size.},
  archive      = {J_AOS},
  author       = {Chao Gao and Zongming Ma},
  doi          = {10.1214/21-AOS2113},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {407-429},
  shortjournal = {Ann. Statist.},
  title        = {Testing equivalence of clustering},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional asymptotics of likelihood ratio tests in
the gaussian sequence model under convex constraints. <em>AOS</em>,
<em>50</em>(1), 376–406. (<a
href="https://doi.org/10.1214/21-AOS2111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Gaussian sequence model Y=μ+ξ, we study the likelihood ratio test (LRT) for testing H0:μ=μ0 versus H1:μ∈K, where μ0∈K, and K is a closed convex set in Rn. In particular, we show that under the null hypothesis, normal approximation holds for the log-likelihood ratio statistic for a general pair (μ0,K), in the high-dimensional regime where the estimation error of the associated least squares estimator diverges in an appropriate sense. The normal approximation further leads to a precise characterization of the power behavior of the LRT in the high-dimensional regime. These characterizations show that the power behavior of the LRT is in general nonuniform with respect to the Euclidean metric, and illustrate the conservative nature of existing minimax optimality and suboptimality results for the LRT. A variety of examples, including testing in the orthant/circular cone, isotonic regression, Lasso and testing parametric assumptions versus shape-constrained alternatives, are worked out to demonstrate the versatility of the developed theory.},
  archive      = {J_AOS},
  author       = {Qiyang Han and Bodhisattva Sen and Yandi Shen},
  doi          = {10.1214/21-AOS2111},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {376-406},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional asymptotics of likelihood ratio tests in the gaussian sequence model under convex constraints},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Admissible ways of merging p-values under arbitrary
dependence. <em>AOS</em>, <em>50</em>(1), 351–375. (<a
href="https://doi.org/10.1214/21-AOS2109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods of merging several p-values into a single p-value are important in their own right and widely used in multiple hypothesis testing. This paper is the first to systematically study the admissibility (in Wald’s sense) of p-merging functions and their domination structure, without any information on the dependence structure of the input p-values. As a technical tool, we use the notion of e-values, which are alternatives to p-values recently promoted by several authors. We obtain several results on the representation of admissible p-merging functions via e-values and on (in)admissibility of existing p-merging functions. By introducing new admissible p-merging functions, we show that some classic merging methods can be strictly improved to enhance power without compromising validity under arbitrary dependence.},
  archive      = {J_AOS},
  author       = {Vladimir Vovk and Bin Wang and Ruodu Wang},
  doi          = {10.1214/21-AOS2109},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {351-375},
  shortjournal = {Ann. Statist.},
  title        = {Admissible ways of merging p-values under arbitrary dependence},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Isotonic regression with unknown permutations: Statistics,
computation and adaptation. <em>AOS</em>, <em>50</em>(1), 324–350. (<a
href="https://doi.org/10.1214/21-AOS2107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by models for multiway comparison data, we consider the problem of estimating a coordinatewise isotonic function on the domain [0,1]d from noisy observations collected on a uniform lattice, but where the design points have been permuted along each dimension. While the univariate and bivariate versions of this problem have received significant attention, our focus is on the multivariate case d≥3. We study both the minimax risk of estimation (in empirical L2 loss) and the fundamental limits of adaptation (quantified by the adaptivity index) to a family of piecewise constant functions. We provide a computationally efficient Mirsky partition estimator that is minimax optimal while also achieving the smallest adaptivity index possible for polynomial time procedures. Thus, from a worst-case perspective and in sharp contrast to the bivariate case, the latent permutations in the model do not introduce significant computational difficulties over and above vanilla isotonic regression. On the other hand, the fundamental limits of adaptation are significantly different with and without unknown permutations: Assuming a hardness conjecture from average-case complexity theory, a statistical-computational gap manifests in the former case. In a complementary direction, we show that natural modifications of existing estimators fail to satisfy at least one of the desiderata of optimal worst-case statistical performance, computational efficiency and fast adaptation. Along the way to showing our results, we improve adaptation results in the special case d=2 and establish some properties of estimators for vanilla isotonic regression, both of which may be of independent interest.},
  archive      = {J_AOS},
  author       = {Ashwin Pananjady and Richard J. Samworth},
  doi          = {10.1214/21-AOS2107},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {324-350},
  shortjournal = {Ann. Statist.},
  title        = {Isotonic regression with unknown permutations: Statistics, computation and adaptation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deconvolution with unknown noise distribution is possible
for multivariate signals. <em>AOS</em>, <em>50</em>(1), 303–323. (<a
href="https://doi.org/10.1214/21-AOS2106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the deconvolution problem in the case where the target signal is multidimensional and no information is known about the noise distribution. More precisely, no assumption is made on the noise distribution and no samples are available to estimate it: the deconvolution problem is solved based only on observations of the corrupted signal. We establish the identifiability of the model up to translation when the signal has a Laplace transform with an exponential growth ρ smaller than 2 and when it can be decomposed into two dependent components. Then we propose an estimator of the probability density function of the signal, which is consistent for any unknown noise distribution with finite variance. We also prove rates of convergence and, as the estimator depends on ρ which is usually unknown, we propose a model selection procedure to obtain an adaptive estimator with the same rate of convergence as the estimator with a known tail parameter. This rate of convergence is known to be minimax when ρ=1.},
  archive      = {J_AOS},
  author       = {Élisabeth Gassiat and Sylvain Le Corff and Luc Lehéricy},
  doi          = {10.1214/21-AOS2106},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {303-323},
  shortjournal = {Ann. Statist.},
  title        = {Deconvolution with unknown noise distribution is possible for multivariate signals},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On least squares estimation under heteroscedastic and
heavy-tailed errors. <em>AOS</em>, <em>50</em>(1), 277–302. (<a
href="https://doi.org/10.1214/21-AOS2105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider least squares estimation in a general nonparametric regression model where the error is allowed to depend on the covariates. The rate of convergence of the least squares estimator (LSE) for the unknown regression function is well studied when the errors are sub-Gaussian. We find upper bounds on the rates of convergence of the LSE when the error has a uniformly bounded conditional variance and has only finitely many moments. Our upper bound on the rate of convergence of the LSE depends on the moment assumptions on the error, the metric entropy of the class of functions involved and the “local” structure of the function class around the truth. We find sufficient conditions on the error distribution under which the rate of the LSE matches the rate of the LSE under sub-Gaussian error. Our results are finite sample and allow for heteroscedastic and heavy-tailed errors.},
  archive      = {J_AOS},
  author       = {Arun K. Kuchibhotla and Rohit K. Patra},
  doi          = {10.1214/21-AOS2105},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {277-302},
  shortjournal = {Ann. Statist.},
  title        = {On least squares estimation under heteroscedastic and heavy-tailed errors},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Powerful knockoffs via minimizing reconstructability.
<em>AOS</em>, <em>50</em>(1), 252–276. (<a
href="https://doi.org/10.1214/21-AOS2104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-X knockoffs (J. R. Stat. Soc. Ser. B. Stat. Methodol. 80 (2018) 551–577) allows analysts to perform feature selection using almost any machine learning algorithm while provably controlling the expected proportion of false discoveries. This procedure involves constructing synthetic variables, called knockoffs, which effectively act as controls during feature selection. The gold standard for constructing knockoffs has been to minimize the mean absolute correlation (MAC) between features and their knockoffs, but, surprisingly, we prove this procedure can be powerless in extremely easy settings, including Gaussian linear models with correlated exchangeable features. The key problem is that minimizing the MAC creates joint dependencies between the features and knockoffs, which allow machine learning algorithms to reconstruct the effect of the features on the response using the knockoffs. To improve power, we propose generating knockoffs which minimize the reconstructability (MRC) of the features, and we demonstrate our proposal for Gaussian features by showing it is computationally efficient, robust, and powerful. We also prove that certain MRC knockoffs minimize a notion of estimation error in Gaussian linear models. Through extensive simulations, we show MRC knockoffs often dramatically outperform MAC-minimizing knockoffs, and we find no settings in which MAC-minimizing knockoffs outperform MRC knockoffs by more than a slight margin. We implement our methods and many others from the knockoffs literature in a new python package knockpy.},
  archive      = {J_AOS},
  author       = {Asher Spector and Lucas Janson},
  doi          = {10.1214/21-AOS2104},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {252-276},
  shortjournal = {Ann. Statist.},
  title        = {Powerful knockoffs via minimizing reconstructability},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimax optimality of permutation tests. <em>AOS</em>,
<em>50</em>(1), 225–251. (<a
href="https://doi.org/10.1214/21-AOS2103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation tests are widely used in statistics, providing a finite-sample guarantee on the type I error rate whenever the distribution of the samples under the null hypothesis is invariant to some rearrangement. Despite its increasing popularity and empirical success, theoretical properties of the permutation test, especially its power, have not been fully explored beyond simple cases. In this paper, we attempt to partly fill this gap by presenting a general nonasymptotic framework for analyzing the minimax power of the permutation test. The utility of our proposed framework is illustrated in the context of two-sample and independence testing under both discrete and continuous settings. In each setting, we introduce permutation tests based on U-statistics and study their minimax performance. We also develop exponential concentration bounds for permuted U-statistics based on a novel coupling idea, which may be of independent interest. Building on these exponential bounds, we introduce permutation tests, which are adaptive to unknown smoothness parameters without losing much power. The proposed framework is further illustrated using more sophisticated test statistics including weighted U-statistics for multinomial testing and Gaussian kernel-based statistics for density testing. Finally, we provide some simulation results that further justify the permutation approach.},
  archive      = {J_AOS},
  author       = {Ilmun Kim and Sivaraman Balakrishnan and Larry Wasserman},
  doi          = {10.1214/21-AOS2103},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {225-251},
  shortjournal = {Ann. Statist.},
  title        = {Minimax optimality of permutation tests},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate message passing algorithms for rotationally
invariant matrices. <em>AOS</em>, <em>50</em>(1), 197–224. (<a
href="https://doi.org/10.1214/21-AOS2101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Message Passing (AMP) algorithms have seen widespread use across a variety of applications. However, the precise forms for their Onsager corrections and state evolutions depend on properties of the underlying random matrix ensemble, limiting the extent to which AMP algorithms derived for white noise may be applicable to data matrices that arise in practice. In this work, we study more general AMP algorithms for random matrices W that satisfy orthogonal rotational invariance in law, where W may have a spectral distribution that is different from the semicircle and Marcenko–Pastur laws characteristic of white noise. The Onsager corrections and state evolutions in these algorithms are defined by the free cumulants or rectangular free cumulants of the spectral distribution of W. Their forms were derived previously by Opper, Çakmak and Winther using nonrigorous dynamic functional theory techniques, and we provide rigorous proofs. Our motivating application is a Bayes-AMP algorithm for Principal Components Analysis, when there is prior structure for the principal components (PCs) and possibly nonwhite noise. For sufficiently large signal strengths and any non-Gaussian prior distributions for the PCs, we show that this algorithm provably achieves higher estimation accuracy than the sample PCs.},
  archive      = {J_AOS},
  author       = {Zhou Fan},
  doi          = {10.1214/21-AOS2101},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {197-224},
  shortjournal = {Ann. Statist.},
  title        = {Approximate message passing algorithms for rotationally invariant matrices},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fundamental barriers to high-dimensional regression with
convex penalties. <em>AOS</em>, <em>50</em>(1), 170–196. (<a
href="https://doi.org/10.1214/21-AOS2100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression, we attempt to estimate a parameter vector β0∈Rp from n≲p observations {(yi,xi)}i≤n, where xi∈Rp is a vector of predictors and yi is a response variable. A well-established approach uses convex regularizers to promote specific structures (e.g., sparsity) of the estimate βˆ while allowing for practical algorithms. Theoretical analysis implies that convex penalization schemes have nearly optimal estimation properties in certain settings. However, in general the gaps between statistically optimal estimation (with unbounded computational resources) and convex methods are poorly understood. We show that when the statistican has very simple structural information about the distribution of the entries of β0, a large gap frequently exists between the best performance achieved by any convex regularizer satisfying a mild technical condition and either: (i) the optimal statistical error or (ii) the statistical error achieved by optimal approximate message passing algorithms. Remarkably, a gap occurs at high enough signal-to-noise ratio if and only if the distribution of the coordinates of β0 is not log-concave. These conclusions follow from an analysis of standard Gaussian designs. Our lower bounds are expected to be generally tight, and we prove tightness under certain conditions.},
  archive      = {J_AOS},
  author       = {Michael Celentano and Andrea Montanari},
  doi          = {10.1214/21-AOS2100},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {170-196},
  shortjournal = {Ann. Statist.},
  title        = {Fundamental barriers to high-dimensional regression with convex penalties},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing community structure for hypergraphs. <em>AOS</em>,
<em>50</em>(1), 147–169. (<a
href="https://doi.org/10.1214/21-AOS2099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many complex networks in the real world can be formulated as hypergraphs where community detection has been widely used. However, the fundamental question of whether communities exist or not in an observed hypergraph remains unclear. This work aims to tackle this important problem. Specifically, we systematically study when a hypergraph with community structure can be successfully distinguished from its Erdős–Rényi counterpart, and propose concrete test statistics when the models are distinguishable. The main contribution of this paper is threefold. First, we discover a phase transition in the hyperedge probability for distinguishability. Second, in the bounded-degree regime, we derive a sharp signal-to-noise ratio (SNR) threshold for distinguishability in the special two-community 3-uniform hypergraphs, and derive nearly tight SNR thresholds in the general two-community m-uniform hypergraphs. Third, in the dense regime, we propose a computationally feasible test based on sub-hypergraph counts, obtain its asymptotic distribution, and analyze its power. Our results are further extended to nonuniform hypergraphs in which a new test involving both edge and hyperedge information is proposed. The proofs rely on Janson’s contiguity theory (Combin. Probab. Comput. 4 (1995) 369–405), a high-moments driven asymptotic normality result by Gao and Wormald (Probab. Theory Related Fields 130 (2004) 368–376), and a truncation technique for analyzing the likelihood ratio.},
  archive      = {J_AOS},
  author       = {Mingao Yuan and Ruiqi Liu and Yang Feng and Zuofeng Shang},
  doi          = {10.1214/21-AOS2099},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {147-169},
  shortjournal = {Ann. Statist.},
  title        = {Testing community structure for hypergraphs},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pattern graphs: A graphical approach to nonmonotone missing
data. <em>AOS</em>, <em>50</em>(1), 129–146. (<a
href="https://doi.org/10.1214/21-AOS2094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the concept of pattern graphs–directed acyclic graphs representing how response patterns are associated. A pattern graph represents an identifying restriction that is nonparametrically identified/saturated and is often a missing not at random restriction. We introduce a selection model and a pattern mixture model formulations using the pattern graphs and show that they are equivalent. A pattern graph leads to an inverse probability weighting estimator as well as an imputation-based estimator. We also study the semiparametric efficiency theory and derive a multiply-robust estimator using pattern graphs.},
  archive      = {J_AOS},
  author       = {Yen-Chi Chen},
  doi          = {10.1214/21-AOS2094},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {129-146},
  shortjournal = {Ann. Statist.},
  title        = {Pattern graphs: A graphical approach to nonmonotone missing data},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimension reduction for functional data based on weak
conditional moments. <em>AOS</em>, <em>50</em>(1), 107–128. (<a
href="https://doi.org/10.1214/21-AOS2091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general theory and estimation methods for functional linear sufficient dimension reduction, where both the predictor and the response can be random functions, or even vectors of functions. Unlike the existing dimension reduction methods, our approach does not rely on the estimation of conditional mean and conditional variance. Instead, it is based on a new statistical construction—the weak conditional expectation, which is based on Carleman operators and their inducing functions. Weak conditional expectation is a generalization of conditional expectation. Its key advantage is to replace the projection on to an L2-space—which defines conditional expectation—by projection on to an arbitrary Hilbert space, while still maintaining the unbiasedness of the related dimension reduction methods. This flexibility is particularly important for functional data, because attempting to estimate a full-fledged conditional mean or conditional variance by slicing or smoothing over the space of vector-valued functions may be inefficient due to the curse of dimensionality. We evaluated the performances of the our new methods by simulation and in several applied settings.},
  archive      = {J_AOS},
  author       = {Bing Li and Jun Song},
  doi          = {10.1214/21-AOS2091},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {107-128},
  shortjournal = {Ann. Statist.},
  title        = {Dimension reduction for functional data based on weak conditional moments},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On minimax optimality of sparse bayes predictive density
estimates. <em>AOS</em>, <em>50</em>(1), 81–106. (<a
href="https://doi.org/10.1214/21-AOS2086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study predictive density estimation under Kullback–Leibler loss in ℓ0-sparse Gaussian sequence models. We propose proper Bayes predictive density estimates and establish asymptotic minimaxity in sparse models. Fundamental for this is a new risk decomposition for sparse, or spike-and-slab priors. A surprise is the existence of a phase transition in the future-to-past variance ratio r. For r&lt;r0=(√5−1)/4, the natural discrete prior ceases to be asymptotically optimal. Instead, for subcritical r, a ‘bi-grid’ prior with a central region of reduced grid spacing recovers asymptotic minimaxity. This phenomenon seems to have no analog in the otherwise parallel theory of point estimation of a multivariate normal mean under quadratic loss. For spike-and-uniform slab priors to have any prospect of minimaxity, we show that the sparse parameter space needs also to be magnitude constrained. Within a substantial range of magnitudes, such spike-and-slab priors can attain asymptotic minimaxity.},
  archive      = {J_AOS},
  author       = {Gourab Mukherjee and Iain M. Johnstone},
  doi          = {10.1214/21-AOS2086},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {81-106},
  shortjournal = {Ann. Statist.},
  title        = {On minimax optimality of sparse bayes predictive density estimates},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heteroskedastic PCA: Algorithm, optimality, and
applications. <em>AOS</em>, <em>50</em>(1), 53–80. (<a
href="https://doi.org/10.1214/21-AOS2074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general framework for principal component analysis (PCA) in the presence of heteroskedastic noise is introduced. We propose an algorithm called HeteroPCA, which involves iteratively imputing the diagonal entries of the sample covariance matrix to remove estimation bias due to heteroskedasticity. This procedure is computationally efficient and provably optimal under the generalized spiked covariance model. A key technical step is a deterministic robust perturbation analysis on singular subspaces, which can be of independent interest. The effectiveness of the proposed algorithm is demonstrated in a suite of problems in high-dimensional statistics, including singular value decomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for heteroskedastic and incomplete data.},
  archive      = {J_AOS},
  author       = {Anru R. Zhang and T. Tony Cai and Yihong Wu},
  doi          = {10.1214/21-AOS2074},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {53-80},
  shortjournal = {Ann. Statist.},
  title        = {Heteroskedastic PCA: Algorithm, optimality, and applications},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial dependence and space–time trend in extreme events.
<em>AOS</em>, <em>50</em>(1), 30–52. (<a
href="https://doi.org/10.1214/21-AOS2067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical theory of extremes is extended to independent multivariate observations that are non-stationary both over time and across space. The non-stationarity over time and space is controlled via the scedasis (tail scale) in the marginal distributions. Spatial dependence stems from multivariate extreme value theory. We establish asymptotic theory for both the weighted sequential tail empirical process and the weighted tail quantile process based on all observations, taken over time and space. The results yield two statistical tests for homoscedasticity in the tail, one in space and one in time. Further, we show that the common extreme value index can be estimated via a pseudo-maximum likelihood procedure based on pooling all (non-stationary and dependent) observations. Our leading example and application is rainfall in Northern Germany.},
  archive      = {J_AOS},
  author       = {John H. J. Einmahl and Ana Ferreira and Laurens de Haan and Cláudia Neves and Chen Zhou},
  doi          = {10.1214/21-AOS2067},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {30-52},
  shortjournal = {Ann. Statist.},
  title        = {Spatial dependence and space–time trend in extreme events},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal statistical and computational framework for
generalized tensor estimation. <em>AOS</em>, <em>50</em>(1), 1–29. (<a
href="https://doi.org/10.1214/21-AOS2061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a flexible framework for generalized low-rank tensor estimation problems that includes many important instances arising from applications in computational imaging, genomics, and network analysis. The proposed estimator consists of finding a low-rank tensor fit to the data under generalized parametric models. To overcome the difficulty of nonconvexity in these problems, we introduce a unified approach of projected gradient descent that adapts to the underlying low-rank structure. Under mild conditions on the loss function, we establish both an upper bound on statistical error and the linear rate of computational convergence through a general deterministic analysis. Then we further consider a suite of generalized tensor estimation problems, including sub-Gaussian tensor PCA, tensor regression, and Poisson and binomial tensor PCA. We prove that the proposed algorithm achieves the minimax optimal rate of convergence in estimation error. Finally, we demonstrate the superiority of the proposed framework via extensive experiments on both simulated and real data.},
  archive      = {J_AOS},
  author       = {Rungang Han and Rebecca Willett and Anru R. Zhang},
  doi          = {10.1214/21-AOS2061},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Ann. Statist.},
  title        = {An optimal statistical and computational framework for generalized tensor estimation},
  volume       = {50},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
